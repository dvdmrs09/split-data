{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef convert_model_to_external_data(model, all_tensors_to_one_file=True, location=None):\n    # type: (ModelProto, bool, Optional[Text]) -> None\n    \"\"\"\n    call to set all tensors as external data. save_model saves all the tensors data as external data after calling this function.\n    @params\n    model: ModelProto to be converted.\n    all_tensors_to_one_file: If true, save all tensors to one external file specified by location.\n                             If false, save each tensor to a file named with the tensor name.\n    location: specify the external file that all tensors to save to.\n              If not specified, will use the model name.\n    \"\"\"\n    if all_tensors_to_one_file:\n        file_name = Text(uuid.uuid1())\n        if location:\n            file_name = location\n        for tensor in _get_all_tensors(model):\n            set_external_data(tensor, file_name)\n    else:\n        for tensor in _get_all_tensors(model):\n            set_external_data(tensor, tensor.name)", "response": "Convert a model to external data."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert model from external data to internal data.", "response": "def convert_model_from_external_data(model):  # type: (ModelProto) -> None\n    \"\"\"\n    call to set all tensors data as embedded data. save_model saves all the tensors data as embedded data after calling this function.\n    @params\n    model: ModelProto to be converted.\n    \"\"\"\n    for tensor in _get_all_tensors(model):\n        if uses_external_data(tensor):\n            if not tensor.HasField(\"raw_data\"):\n                raise ValueError(\"raw_data field doesn't exist.\")\n            del tensor.external_data[:]\n            tensor.data_location = TensorProto.DEFAULT"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save_external_data(tensor, base_path):  # type: (TensorProto, Text) -> None\n    info = ExternalDataInfo(tensor)\n    external_data_file_path = os.path.join(base_path, info.location)\n\n    # Retrieve the tensor's data from raw_data or load external file\n    if not tensor.HasField(\"raw_data\"):\n        raise ValueError(\"raw_data field doesn't exist.\")\n\n    # Create file if it doesn't exist\n    if not os.path.isfile(external_data_file_path):\n        open(external_data_file_path, 'ab').close()\n\n    # Open file for reading and writing at random locations ('r+b')\n    with open(external_data_file_path, 'r+b') as data_file:\n        data_file.seek(0, 2)\n        if info.offset is not None:\n            # Pad file to required offset if needed\n            file_size = data_file.tell()\n            if info.offset > file_size:\n                data_file.write(b\"\\0\" * (info.offset - file_size))\n\n            data_file.seek(info.offset)\n        offset = data_file.tell()\n        data_file.write(tensor.raw_data)\n        set_external_data(tensor, info.location, offset, data_file.tell() - offset)", "response": "Writes tensor data to an external file according to information in the external_data field."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nyield the tensors from the node attributes of an ONNX model.", "response": "def _get_attribute_tensors(onnx_model_proto):  # type: (ModelProto) -> Iterable[TensorProto]\n    \"\"\"Create an iterator of tensors from node attributes of an ONNX model.\"\"\"\n    for node in onnx_model_proto.graph.node:\n        for attribute in node.attribute:\n            if attribute.HasField(\"t\"):\n                yield attribute.t\n            for tensor in attribute.tensors:\n                yield tensor"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove_external_data_field(tensor, field_key):  # type: (TensorProto, Text) -> None\n    for (i, field) in enumerate(tensor.external_data):\n        if field.key == field_key:\n            del tensor.external_data[i]", "response": "Removes a field from a Tensor s external_data key - value store."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting external data of all tensors to files on disk.", "response": "def write_external_data_tensors(model, filepath):  # type: (ModelProto, Text) -> ModelProto\n    \"\"\"\n    Write external data of all tensors to files on disk.\n\n    Note: This function also strips basepath information from all tensors' external_data fields.\n\n    @params\n    model: Model object which is the source of tensors to serialize.\n    filepath: System path to the directory which should be treated as base path for external data.\n\n    @return\n    The modified model object.\n    \"\"\"\n    for tensor in _get_all_tensors(model):\n        if uses_external_data(tensor):\n            save_external_data(tensor, filepath)\n            tensor.ClearField(str('raw_data'))\n\n    return model"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nimport a stdlib path and returns a handle to it eg. self. imports [ path ] = name", "response": "def _import(self, path, name):\n        # type: (Text, Text) -> Text\n        \"\"\"Imports a stdlib path and returns a handle to it\n        eg. self._import(\"typing\", \"Optional\") -> \"Optional\"\n        \"\"\"\n        imp = path.replace('/', '.')\n        self.imports[imp].add(name)\n        return name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nimport a referenced message and returns a handle", "response": "def _import_message(self, type_name):\n        # type: (d.FieldDescriptorProto) -> Text\n        \"\"\"Import a referenced message and return a handle\"\"\"\n        name = cast(Text, type_name)\n\n        if name[0] == '.' and name[1].isupper() and name[2].islower():\n            # Message defined in this file\n            return name[1:]\n\n        message_fd = self.descriptors.message_to_fd[name]\n        if message_fd.name == self.fd.name:\n            # message defined in this package\n            split = name.split('.')\n            for i, segment in enumerate(split):\n                if segment and segment[0].isupper() and segment[1].islower():\n                    return \".\".join(split[i:])\n\n        # Not in package. Must import\n        split = name.split(\".\")\n        for i, segment in enumerate(split):\n            if segment and segment[0].isupper() and segment[1].islower():\n                assert message_fd.name.endswith('.proto')\n                import_name = self._import(message_fd.name[:-6].replace('-', '_') + \"_pb2\", segment)\n                remains = \".\".join(split[i + 1:])\n                if not remains:\n                    return import_name\n                raise AssertionError(\"Don't support nested imports yet\")\n                # return new_nested_import(import_name, remains)\n\n        raise AssertionError(\"Could not parse local name \" + name)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_node(\n        op_type,  # type: Text\n        inputs,  # type: Sequence[Text]\n        outputs,  # type: Sequence[Text]\n        name=None,  # type: Optional[Text]\n        doc_string=None,  # type: Optional[Text]\n        domain=None,  # type: Optional[Text]\n        **kwargs  # type: Any\n):  # type: (...) -> NodeProto\n    \"\"\"Construct a NodeProto.\n\n    Arguments:\n        op_type (string): The name of the operator to construct\n        inputs (list of string): list of input names\n        outputs (list of string): list of output names\n        name (string, default None): optional unique identifier for NodeProto\n        doc_string (string, default None): optional documentation string for NodeProto\n        domain (string, default None): optional domain for NodeProto.\n            If it's None, we will just use default domain (which is empty)\n        **kwargs (dict): the attributes of the node.  The acceptable values\n            are documented in :func:`make_attribute`.\n    \"\"\"\n\n    node = NodeProto()\n    node.op_type = op_type\n    node.input.extend(inputs)\n    node.output.extend(outputs)\n    if name:\n        node.name = name\n    if doc_string:\n        node.doc_string = doc_string\n    if domain is not None:\n        node.domain = domain\n    if kwargs:\n        node.attribute.extend(\n            make_attribute(key, value)\n            for key, value in sorted(kwargs.items()))\n    return node", "response": "Construct a NodeProto object from the given arguments."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconstructs an OperatorSetIdProto object from the given domain and version.", "response": "def make_operatorsetid(\n        domain,  # type: Text\n        version,  # type: int\n):  # type: (...) -> OperatorSetIdProto\n    \"\"\"Construct an OperatorSetIdProto.\n\n    Arguments:\n        domain (string): The domain of the operator set id\n        version (integer): Version of operator set id\n    \"\"\"\n    operatorsetid = OperatorSetIdProto()\n    operatorsetid.domain = domain\n    operatorsetid.version = version\n    return operatorsetid"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _to_bytes_or_false(val):  # type: (Union[Text, bytes]) -> Union[bytes, bool]\n    if isinstance(val, bytes):\n        return val\n    else:\n        try:\n            return val.encode('utf-8')\n        except AttributeError:\n            return False", "response": "Convert the input to a bytes or to False."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make_attribute(\n        key,  # type: Text\n        value,  # type: Any\n        doc_string=None  # type: Optional[Text]\n):  # type: (...) -> AttributeProto\n    \"\"\"Makes an AttributeProto based on the value type.\"\"\"\n    attr = AttributeProto()\n    attr.name = key\n    if doc_string:\n        attr.doc_string = doc_string\n\n    is_iterable = isinstance(value, collections.Iterable)\n    bytes_or_false = _to_bytes_or_false(value)\n    # First, singular cases\n    # float\n    if isinstance(value, float):\n        attr.f = value\n        attr.type = AttributeProto.FLOAT\n    # integer\n    elif isinstance(value, numbers.Integral):\n        attr.i = cast(int, value)\n        attr.type = AttributeProto.INT\n    # string\n    elif bytes_or_false:\n        assert isinstance(bytes_or_false, bytes)\n        attr.s = bytes_or_false\n        attr.type = AttributeProto.STRING\n    elif isinstance(value, TensorProto):\n        attr.t.CopyFrom(value)\n        attr.type = AttributeProto.TENSOR\n    elif isinstance(value, GraphProto):\n        attr.g.CopyFrom(value)\n        attr.type = AttributeProto.GRAPH\n    # third, iterable cases\n    elif is_iterable:\n        byte_array = [_to_bytes_or_false(v) for v in value]\n        if all(isinstance(v, float) for v in value):\n            attr.floats.extend(value)\n            attr.type = AttributeProto.FLOATS\n        elif all(isinstance(v, numbers.Integral) for v in value):\n            # Turn np.int32/64 into Python built-in int.\n            attr.ints.extend(int(v) for v in value)\n            attr.type = AttributeProto.INTS\n        elif all(byte_array):\n            attr.strings.extend(cast(List[bytes], byte_array))\n            attr.type = AttributeProto.STRINGS\n        elif all(isinstance(v, TensorProto) for v in value):\n            attr.tensors.extend(value)\n            attr.type = AttributeProto.TENSORS\n        elif all(isinstance(v, GraphProto) for v in value):\n            attr.graphs.extend(value)\n            attr.type = AttributeProto.GRAPHS\n        else:\n            raise ValueError(\n                \"You passed in an iterable attribute but I cannot figure out \"\n                \"its applicable type.\")\n    else:\n        raise ValueError(\n            'Value \"{}\" is not valid attribute data type.'.format(value))\n    return attr", "response": "Makes an AttributeProto based on the value type."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes a ValueInfoProto based on the data type and shape.", "response": "def make_tensor_value_info(\n        name,  # type: Text\n        elem_type,  # type: int\n        shape,  # type: Optional[Sequence[Union[Text, int]]]\n        doc_string=\"\",  # type: Text\n        shape_denotation=None,  # type: Optional[List[Text]]\n):  # type: (...) -> ValueInfoProto\n    \"\"\"Makes a ValueInfoProto based on the data type and shape.\"\"\"\n    value_info_proto = ValueInfoProto()\n    value_info_proto.name = name\n    if doc_string:\n        value_info_proto.doc_string = doc_string\n\n    tensor_type_proto = value_info_proto.type.tensor_type\n    tensor_type_proto.elem_type = elem_type\n\n    tensor_shape_proto = tensor_type_proto.shape\n\n    if shape is not None:\n        # You might think this is a no-op (extending a normal Python\n        # list by [] certainly is), but protobuf lists work a little\n        # differently; if a field is never set, it is omitted from the\n        # resulting protobuf; a list that is explicitly set to be\n        # empty will get an (empty) entry in the protobuf. This\n        # difference is visible to our consumers, so make sure we emit\n        # an empty shape!\n        tensor_shape_proto.dim.extend([])\n\n        if shape_denotation:\n            if len(shape_denotation) != len(shape):\n                raise ValueError(\n                    'Invalid shape_denotation. '\n                    'Must be of the same length as shape.')\n\n        for i, d in enumerate(shape):\n            dim = tensor_shape_proto.dim.add()\n            if d is None:\n                pass\n            elif isinstance(d, integer_types):\n                dim.dim_value = d\n            elif isinstance(d, text_type):\n                dim.dim_param = d\n            else:\n                raise ValueError(\n                    'Invalid item in shape: {}. '\n                    'Needs to of integer_types or text_type.'.format(d))\n\n            if shape_denotation:\n                dim.denotation = shape_denotation[i]\n\n    return value_info_proto"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef strip_doc_string(proto):  # type: (google.protobuf.message.Message) -> None\n    assert isinstance(proto, google.protobuf.message.Message)\n    for descriptor in proto.DESCRIPTOR.fields:\n        if descriptor.name == 'doc_string':\n            proto.ClearField(descriptor.name)\n        elif descriptor.type == descriptor.TYPE_MESSAGE:\n            if descriptor.label == descriptor.LABEL_REPEATED:\n                for x in getattr(proto, descriptor.name):\n                    strip_doc_string(x)\n            elif proto.HasField(descriptor.name):\n                strip_doc_string(getattr(proto, descriptor.name))", "response": "Removes the doc_string field from any nested protobuf message."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a TensorProto object to a numpy array.", "response": "def to_array(tensor):  # type: (TensorProto) -> np.ndarray[Any]\n    \"\"\"Converts a tensor def object to a numpy array.\n\n    Inputs:\n        tensor: a TensorProto object.\n    Returns:\n        arr: the converted array.\n    \"\"\"\n    if tensor.HasField(\"segment\"):\n        raise ValueError(\n            \"Currently not supporting loading segments.\")\n    if tensor.data_type == TensorProto.UNDEFINED:\n        raise ValueError(\"The data type is not defined.\")\n\n    tensor_dtype = tensor.data_type\n    np_dtype = mapping.TENSOR_TYPE_TO_NP_TYPE[tensor_dtype]\n    storage_type = mapping.TENSOR_TYPE_TO_STORAGE_TENSOR_TYPE[tensor_dtype]\n    storage_np_dtype = mapping.TENSOR_TYPE_TO_NP_TYPE[storage_type]\n    storage_field = mapping.STORAGE_TENSOR_TYPE_TO_FIELD[storage_type]\n    dims = tensor.dims\n\n    if tensor.data_type == TensorProto.STRING:\n        utf8_strings = getattr(tensor, storage_field)\n        ss = list(s.decode('utf-8') for s in utf8_strings)\n        return np.asarray(ss).astype(np_dtype).reshape(dims)\n\n    if tensor.HasField(\"raw_data\"):\n        # Raw_bytes support: using frombuffer.\n        return np.frombuffer(\n            tensor.raw_data,\n            dtype=np_dtype).reshape(dims)\n    else:\n        data = getattr(tensor, storage_field),  # type: Sequence[np.complex64]\n        if (tensor_dtype == TensorProto.COMPLEX64\n                or tensor_dtype == TensorProto.COMPLEX128):\n            data = combine_pairs_to_complex(data)\n        return (\n            np.asarray(\n                data,\n                dtype=storage_np_dtype)\n            .astype(np_dtype)\n            .reshape(dims)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_array(arr, name=None):  # type: (np.ndarray[Any], Optional[Text]) -> TensorProto\n    tensor = TensorProto()\n    tensor.dims.extend(arr.shape)\n    if name:\n        tensor.name = name\n\n    if arr.dtype == np.object:\n        # Special care for strings.\n        tensor.data_type = mapping.NP_TYPE_TO_TENSOR_TYPE[arr.dtype]\n        # TODO: Introduce full string support.\n        # We flatten the array in case there are 2-D arrays are specified\n        # We throw the error below if we have a 3-D array or some kind of other\n        # object. If you want more complex shapes then follow the below instructions.\n        # Unlike other types where the shape is automatically inferred from\n        # nested arrays of values, the only reliable way now to feed strings\n        # is to put them into a flat array then specify type astype(np.object)\n        # (otherwise all strings may have different types depending on their length)\n        # and then specify shape .reshape([x, y, z])\n        flat_array = arr.flatten()\n        for e in flat_array:\n            if isinstance(e, text_type):\n                tensor.string_data.append(e.encode('utf-8'))\n            elif isinstance(e, np.ndarray):\n                for s in e:\n                    if isinstance(s, text_type):\n                        tensor.string_data.append(s.encode('utf-8'))\n            else:\n                raise NotImplementedError(\n                    \"Unrecognized object in the object array, expect a string, or array of bytes: \", str(type(e)))\n        return tensor\n\n    # For numerical types, directly use numpy raw bytes.\n    try:\n        dtype = mapping.NP_TYPE_TO_TENSOR_TYPE[arr.dtype]\n    except KeyError:\n        raise RuntimeError(\n            \"Numpy data type not understood yet: {}\".format(str(arr.dtype)))\n    tensor.data_type = dtype\n    tensor.raw_data = arr.tobytes()  # note: tobytes() is only after 1.9.\n\n    return tensor", "response": "Converts a numpy array to a tensor def."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _serialize(proto):  # type: (Union[bytes, google.protobuf.message.Message]) -> bytes\n    '''\n    Serialize a in-memory proto to bytes\n\n    @params\n    proto is a in-memory proto, such as a ModelProto, TensorProto, etc\n\n    @return\n    Serialized proto in bytes\n    '''\n    if isinstance(proto, bytes):\n        return proto\n    elif hasattr(proto, 'SerializeToString') and callable(proto.SerializeToString):\n        result = proto.SerializeToString()\n        return result\n    else:\n        raise ValueError('No SerializeToString method is detected. '\n                         'neither proto is a str.\\ntype is {}'.format(type(proto)))", "response": "Serialize a in - memory proto to bytes\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _deserialize(s, proto):  # type: (bytes, _Proto) -> _Proto\n    '''\n    Parse bytes into a in-memory proto\n\n    @params\n    s is bytes containing serialized proto\n    proto is a in-memory proto object\n\n    @return\n    The proto instance filled in by s\n    '''\n    if not isinstance(s, bytes):\n        raise ValueError('Parameter s must be bytes, but got type: {}'.format(type(s)))\n\n    if not (hasattr(proto, 'ParseFromString') and callable(proto.ParseFromString)):\n        raise ValueError('No ParseFromString method is detected. '\n                         '\\ntype is {}'.format(type(proto)))\n\n    decoded = cast(Optional[int], proto.ParseFromString(s))\n    if decoded is not None and decoded != len(s):\n        raise google.protobuf.message.DecodeError(\n            \"Protobuf decoding consumed too few bytes: {} out of {}\".format(\n                decoded, len(s)))\n    return proto", "response": "Deserialize bytes into a in - memory proto object returning the resulting proto object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_model(f, format=None, load_external_data=True):  # type: (Union[IO[bytes], Text], Optional[Any], bool) -> ModelProto\n    '''\n    Loads a serialized ModelProto into memory\n\n    @params\n    f can be a file-like object (has \"read\" function) or a string containing a file name\n    format is for future use\n\n    @return\n    Loaded in-memory ModelProto\n    '''\n    s = _load_bytes(f)\n    model = load_model_from_string(s, format=format)\n\n    if load_external_data:\n        model_filepath = _get_file_path(f)\n        if model_filepath:\n            base_dir = os.path.dirname(model_filepath)\n            load_external_data_for_model(model, base_dir)\n\n    return model", "response": "Loads a serialized ModelProto into memory\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads a serialized TensorProto into memory", "response": "def load_tensor(f, format=None):  # type: (Union[IO[bytes], Text], Optional[Any]) -> TensorProto\n    '''\n    Loads a serialized TensorProto into memory\n\n    @params\n    f can be a file-like object (has \"read\" function) or a string containing a file name\n    format is for future use\n\n    @return\n    Loaded in-memory TensorProto\n    '''\n    s = _load_bytes(f)\n    return load_tensor_from_string(s, format=format)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save_model(proto, f, format=None):  # type: (Union[ModelProto, bytes], Union[IO[bytes], Text], Optional[Any]) -> None\n    '''\n    Saves the ModelProto to the specified path.\n\n    @params\n    proto should be a in-memory ModelProto\n    f can be a file-like object (has \"write\" function) or a string containing a file name\n    format is for future use\n    '''\n    if isinstance(proto, bytes):\n        proto = _deserialize(proto, ModelProto())\n\n    model_filepath = _get_file_path(f)\n    if model_filepath:\n        basepath = os.path.dirname(model_filepath)\n        proto = write_external_data_tensors(proto, basepath)\n\n    s = _serialize(proto)\n    _save_bytes(s, f)", "response": "Save the given ModelProto to the specified file - like object f."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nunrolls an RNN cell across time steps.", "response": "def dynamic_unroll(cell, inputs, begin_state, drop_inputs=0, drop_outputs=0,\n                   layout='TNC', valid_length=None):\n    \"\"\"Unrolls an RNN cell across time steps.\n\n    Currently, 'TNC' is a preferred layout. unroll on the input of this layout\n    runs much faster.\n\n    Parameters\n    ----------\n    cell : an object whose base class is RNNCell.\n        The RNN cell to run on the input sequence.\n    inputs : Symbol\n        It should have shape (batch_size, length, ...) if `layout` is 'NTC',\n        or (length, batch_size, ...) if `layout` is 'TNC'.\n    begin_state : nested list of Symbol\n        The initial states of the RNN sequence.\n    drop_inputs : float, default 0.\n        The dropout rate for inputs. Won't apply dropout if it equals 0.\n    drop_outputs : float, default 0.\n        The dropout rate for outputs. Won't apply dropout if it equals 0.\n    layout : str, optional\n        `layout` of input symbol. Only used if inputs\n        is a single Symbol.\n    valid_length : Symbol, NDArray or None\n        `valid_length` specifies the length of the sequences in the batch without padding.\n        This option is especially useful for building sequence-to-sequence models where\n        the input and output sequences would potentially be padded.\n        If `valid_length` is None, all sequences are assumed to have the same length.\n        If `valid_length` is a Symbol or NDArray, it should have shape (batch_size,).\n        The ith element will be the length of the ith sequence in the batch.\n        The last valid state will be return and the padded outputs will be masked with 0.\n        Note that `valid_length` must be smaller or equal to `length`.\n\n    Returns\n    -------\n    outputs : Symbol\n        the output of the RNN from this unrolling.\n\n    states : list of Symbol\n        The new state of this RNN after this unrolling.\n        The type of this symbol is same as the output of `begin_state`.\n\n    Examples\n    --------\n    >>> seq_len = 3\n    >>> batch_size = 2\n    >>> input_size = 5\n    >>> cell = mx.gluon.rnn.LSTMCell(input_size, prefix='rnn_')\n    >>> cell.initialize(ctx=mx.cpu())\n    >>> rnn_data = mx.nd.normal(loc=0, scale=1, shape=(seq_len, batch_size, input_size))\n    >>> state_shape = (batch_size, input_size)\n    >>> states = [mx.nd.normal(loc=0, scale=1, shape=state_shape) for i in range(2)]\n    >>> valid_length = mx.nd.array([2, 3])\n    >>> output, states = mx.gluon.contrib.rnn.rnn_cell.dynamic_unroll(cell, rnn_data, states,\n                                                                      valid_length=valid_length,\n                                                                      layout='TNC')\n    >>> print(output)\n    [[[ 0.00767238  0.00023103  0.03973929 -0.00925503 -0.05660512]\n      [ 0.00881535  0.05428379 -0.02493718 -0.01834097  0.02189514]]\n     [[-0.00676967  0.01447039  0.01287002 -0.00574152 -0.05734247]\n      [ 0.01568508  0.02650866 -0.04270559 -0.04328435  0.00904011]]\n     [[ 0.          0.          0.          0.          0.        ]\n      [ 0.01055336  0.02734251 -0.03153727 -0.03742751 -0.01378113]]]\n     <NDArray 3x2x5 @cpu(0)>\n    \"\"\"\n\n    # Merge is always True, so we don't need length.\n    inputs, axis, F, _ = _format_sequence(0, inputs, layout, True)\n    if axis != 0:\n        axes = list(range(len(layout)))\n        tmp = axes[0]\n        axes[0] = axes[axis]\n        axes[axis] = tmp\n        inputs = F.transpose(inputs, axes=axes)\n    states = begin_state\n\n    if drop_inputs:\n        inputs = F.Dropout(inputs, p=drop_inputs, axes=(axis,))\n\n    if valid_length is None:\n        def loop_body(inputs, states):\n            return cell(inputs, states)\n    else:\n        zeros = []\n        for s in states:\n            zeros.append(F.zeros_like(s))\n        states = list(_as_list(states))\n        states.append(F.zeros((1)))\n        def loop_body(inputs, states):\n            cell_states = states[:-1]\n            iter_no = states[-1]\n            out, new_states = cell(inputs, cell_states)\n            for i, state in enumerate(cell_states):\n                new_states[i] = F.where(F.broadcast_greater(valid_length, iter_no),\n                                        new_states[i], state)\n            new_states.append(iter_no + 1)\n            return out, new_states\n\n    outputs, states = F.contrib.foreach(loop_body, inputs, states)\n    if drop_outputs:\n        outputs = F.Dropout(outputs, p=drop_outputs, axes=(axis,))\n    if valid_length is not None:\n        if axis != 0:\n            outputs = F.transpose(outputs, axes)\n        outputs = F.SequenceMask(outputs, sequence_length=valid_length,\n                                 use_sequence_length=True, axis=axis)\n        # the last state is the iteration number. We don't need it.\n        return outputs, states[:-1]\n    else:\n        if axis != 0:\n            outputs = F.transpose(outputs, axes)\n        return outputs, states"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nunrolling an RNN cell across time steps.", "response": "def unroll(self, length, inputs, begin_state=None, layout='NTC', merge_outputs=None,\n               valid_length=None):\n        \"\"\"Unrolls an RNN cell across time steps.\n\n        Parameters\n        ----------\n        length : int\n            Number of steps to unroll.\n        inputs : Symbol, list of Symbol, or None\n            If `inputs` is a single Symbol (usually the output\n            of Embedding symbol), it should have shape\n            (batch_size, length, ...) if `layout` is 'NTC',\n            or (length, batch_size, ...) if `layout` is 'TNC'.\n\n            If `inputs` is a list of symbols (usually output of\n            previous unroll), they should all have shape\n            (batch_size, ...).\n        begin_state : nested list of Symbol, optional\n            Input states created by `begin_state()`\n            or output state of another cell.\n            Created from `begin_state()` if `None`.\n        layout : str, optional\n            `layout` of input symbol. Only used if inputs\n            is a single Symbol.\n        merge_outputs : bool, optional\n            If `False`, returns outputs as a list of Symbols.\n            If `True`, concatenates output across time steps\n            and returns a single symbol with shape\n            (batch_size, length, ...) if layout is 'NTC',\n            or (length, batch_size, ...) if layout is 'TNC'.\n            If `None`, output whatever is faster.\n        valid_length : Symbol, NDArray or None\n            `valid_length` specifies the length of the sequences in the batch without padding.\n            This option is especially useful for building sequence-to-sequence models where\n            the input and output sequences would potentially be padded.\n            If `valid_length` is None, all sequences are assumed to have the same length.\n            If `valid_length` is a Symbol or NDArray, it should have shape (batch_size,).\n            The ith element will be the length of the ith sequence in the batch.\n            The last valid state will be return and the padded outputs will be masked with 0.\n            Note that `valid_length` must be smaller or equal to `length`.\n\n        Returns\n        -------\n        outputs : list of Symbol or Symbol\n            Symbol (if `merge_outputs` is True) or list of Symbols\n            (if `merge_outputs` is False) corresponding to the output from\n            the RNN from this unrolling.\n\n        states : list of Symbol\n            The new state of this RNN after this unrolling.\n            The type of this symbol is same as the output of `begin_state()`.\n        \"\"\"\n\n        # Dropout on inputs and outputs can be performed on the whole sequence\n        # only when state dropout is not present.\n        if self.drop_states:\n            return super(VariationalDropoutCell, self).unroll(length, inputs, begin_state,\n                                                              layout, merge_outputs,\n                                                              valid_length=valid_length)\n\n        self.reset()\n\n        inputs, axis, F, batch_size = _format_sequence(length, inputs, layout, True)\n        states = _get_begin_state(self, F, begin_state, inputs, batch_size)\n\n        if self.drop_inputs:\n            inputs = F.Dropout(inputs, p=self.drop_inputs, axes=(axis,))\n\n        outputs, states = self.base_cell.unroll(length, inputs, states, layout, merge_outputs=True,\n                                                valid_length=valid_length)\n        if self.drop_outputs:\n            outputs = F.Dropout(outputs, p=self.drop_outputs, axes=(axis,))\n        merge_outputs = isinstance(outputs, tensor_types) if merge_outputs is None else \\\n            merge_outputs\n        outputs, _, _, _ = _format_sequence(length, outputs, layout, merge_outputs)\n        if valid_length is not None:\n            outputs = _mask_sequence_variable_length(F, outputs, length, valid_length, axis,\n                                                     merge_outputs)\n        return outputs, states"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchanging attribute names as per values in change_map dictionary.", "response": "def _fix_attribute_names(attrs, change_map):\n    \"\"\"\n    Change attribute names as per values in change_map dictionary.\n    Parameters\n    ----------\n    :param attrs : dict Dict of operator attributes\n    :param change_map : dict Dict of onnx attribute name to mxnet attribute names.\n\n    Returns\n    -------\n    :return new_attr : dict Converted dict of operator attributes.\n    \"\"\"\n    new_attr = {}\n    for k in attrs.keys():\n        if k in change_map:\n            new_attr[change_map[k]] = attrs[k]\n        else:\n            new_attr[k] = attrs[k]\n    return new_attr"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nremove attributes in the remove list from the input attribute dict.", "response": "def _remove_attributes(attrs, remove_list):\n    \"\"\"\n    Removes attributes in the remove list from the input attribute dict\n    :param attrs : Dict of operator attributes\n    :param remove_list : list of attributes to be removed\n\n    :return new_attr : Dict of operator attributes without the listed attributes.\n    \"\"\"\n    new_attrs = {}\n    for attr in attrs.keys():\n        if attr not in remove_list:\n            new_attrs[attr] = attrs[attr]\n    return new_attrs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _add_extra_attributes(attrs, extra_attr_map):\n    for attr in extra_attr_map:\n        if attr not in attrs:\n            attrs[attr] = extra_attr_map[attr]\n    return attrs", "response": "Adds extra attributes to the current attribute list."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfix padding sequence to match with mxnet s pad_width .", "response": "def _pad_sequence_fix(attr, kernel_dim=None):\n    \"\"\"Changing onnx's pads sequence to match with mxnet's pad_width\n    mxnet: (x1_begin, x1_end, ... , xn_begin, xn_end)\n    onnx: (x1_begin, x2_begin, ... , xn_end, xn_end)\"\"\"\n    new_attr = ()\n    if len(attr) % 2 == 0:\n        for index in range(int(len(attr) / 2)):\n            new_attr = new_attr + attr[index::int(len(attr) / 2)]\n        # Making sure pad values  are in the attr for all axes.\n        if kernel_dim is not None:\n            while len(new_attr) < kernel_dim*2:\n                new_attr = new_attr + (0, 0)\n\n    return new_attr"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _fix_pooling(pool_type, inputs, new_attr):\n    stride = new_attr.get('stride')\n    kernel = new_attr.get('kernel')\n    padding = new_attr.get('pad')\n    p_value = new_attr.get('p_value')\n\n    # Adding default stride.\n    if stride is None:\n        stride = (1,) * len(kernel)\n\n    # Add padding attr if not provided.\n    if padding is None:\n        padding = (0,) * len(kernel) * 2\n\n    # Mxnet Pad operator supports only 4D/5D tensors.\n    # For 1D case, these are the steps:\n    #    Step 1. Add extra dummy dimension to make it 4D. Adding to  axis = 2\n    #    Step 2. Apply padding to this changed tensor\n    #    Step 3. Remove the extra dimension added in step 1.\n    if len(kernel) == 1:\n        dummy_axis = 2\n        # setting 0 padding to the new dim to be added.\n        padding = (0, padding[0], 0, padding[1])\n        pad_width = (0, 0, 0, 0) + _pad_sequence_fix(padding, kernel_dim=2)\n\n        # Step 1.\n        curr_sym = symbol.expand_dims(inputs[0], axis=dummy_axis)\n\n        # Step 2. Common for all tensor sizes\n        new_pad_op = symbol.pad(curr_sym, mode='edge', pad_width=pad_width)\n\n        # Step 3: Removing extra dim added.\n        new_pad_op = symbol.split(new_pad_op, axis=dummy_axis, num_outputs=1, squeeze_axis=1)\n    else:\n        # For 2D/3D cases:\n        # Apply padding\n        pad_width = (0, 0, 0, 0) + _pad_sequence_fix(padding, kernel_dim=len(kernel))\n        curr_sym = inputs[0]\n\n        if pool_type == 'max':\n            # For max pool : mode = 'edge', we should replicate the\n            # edge values to pad, so that we only include  input data values\n            # for calculating 'max'\n            new_pad_op = symbol.pad(curr_sym, mode='edge', pad_width=pad_width)\n        else:\n            # For avg pool, we should add 'zeros' for padding  so mode='constant'\n            new_pad_op = symbol.pad(curr_sym, mode='constant', pad_width=pad_width)\n\n    # Apply pooling without pads.\n    if pool_type == 'lp':\n        new_pooling_op = symbol.Pooling(new_pad_op, pool_type=pool_type, stride=stride, kernel=kernel, p_value=p_value)\n    else:\n        new_pooling_op = symbol.Pooling(new_pad_op, pool_type=pool_type, stride=stride, kernel=kernel)\n    return new_pooling_op", "response": "This function handles the pooling operator."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _fix_bias(op_name, attrs, num_inputs):\n    if num_inputs == 3:\n        attrs['no_bias'] = False\n    elif num_inputs == 2:\n        attrs['no_bias'] = True\n    else:\n        raise ValueError(\"Unexpected number of inputs for: {}\".format(op_name))\n    return attrs", "response": "A workaround for use_bias attribute since onnx don t provide this attribute."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _fix_broadcast(op_name, inputs, broadcast_axis, proto_obj):\n    if int(len(proto_obj._params)) > 0:\n        assert len(list(inputs)) == 2\n\n        input0_shape = get_input_shape(inputs[0], proto_obj)\n        #creating reshape shape\n        reshape_shape = list(len(input0_shape) * (1,))\n        reshape_shape[broadcast_axis] = -1\n        reshape_shape = tuple(reshape_shape)\n        reshape_op_sym = symbol.reshape(inputs[1], shape=reshape_shape)\n        op_sym = getattr(symbol, op_name)(inputs[0], reshape_op_sym)\n    else:\n        op_sym = op_name\n    return op_sym", "response": "A workaround to reshape bias term to num_channel."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _fix_gemm(op_name, inputs, old_attr, proto_obj):\n    op_sym = getattr(symbol, op_name, None)\n    alpha = float(old_attr.get('alpha', 1.0))\n    beta = float(old_attr.get('beta', 1.0))\n    trans_a = int(old_attr.get('transA', 0))\n    trans_b = int(old_attr.get('transB', 0))\n    if trans_a:\n        inputs[0] = symbol.transpose(inputs[0], axes=(1, 0))\n    if not trans_b:\n        inputs[1] = symbol.transpose(inputs[1], axes=(1, 0))\n    new_inputs = [alpha*inputs[0], inputs[1], beta*inputs[2]]\n    new_attr = {'num_hidden' : proto_obj._params[inputs[2].name].shape[0]}\n    return op_sym, new_attr, new_inputs", "response": "Using FullyConnected operator in place of linalg_gemm to perform same operation"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef imresize(src, w, h, *args, **kwargs):\n    return _internal._cvimresize(src, w, h, *args, **kwargs)", "response": "r Resize image with OpenCV."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndecodes an image to an NDArray.", "response": "def imdecode(buf, *args, **kwargs):\n    \"\"\"Decode an image to an NDArray.\n\n    .. note:: `imdecode` uses OpenCV (not the CV2 Python library).\n       MXNet must have been built with USE_OPENCV=1 for `imdecode` to work.\n\n    Parameters\n    ----------\n    buf : str/bytes/bytearray or numpy.ndarray\n        Binary image data as string or numpy ndarray.\n    flag : int, optional, default=1\n        1 for three channel color output. 0 for grayscale output.\n    to_rgb : int, optional, default=1\n        1 for RGB formatted output (MXNet default). 0 for BGR formatted output (OpenCV default).\n    out : NDArray, optional\n        Output buffer. Use `None` for automatic allocation.\n\n    Returns\n    -------\n    NDArray\n        An `NDArray` containing the image.\n\n    Example\n    -------\n    >>> with open(\"flower.jpg\", 'rb') as fp:\n    ...     str_image = fp.read()\n    ...\n    >>> image = mx.img.imdecode(str_image)\n    >>> image\n    <NDArray 224x224x3 @cpu(0)>\n\n    Set `flag` parameter to 0 to get grayscale output\n\n    >>> with open(\"flower.jpg\", 'rb') as fp:\n    ...     str_image = fp.read()\n    ...\n    >>> image = mx.img.imdecode(str_image, flag=0)\n    >>> image\n    <NDArray 224x224x1 @cpu(0)>\n\n    Set `to_rgb` parameter to 0 to get output in OpenCV format (BGR)\n\n    >>> with open(\"flower.jpg\", 'rb') as fp:\n    ...     str_image = fp.read()\n    ...\n    >>> image = mx.img.imdecode(str_image, to_rgb=0)\n    >>> image\n    <NDArray 224x224x3 @cpu(0)>\n    \"\"\"\n    if not isinstance(buf, nd.NDArray):\n        if sys.version_info[0] == 3 and not isinstance(buf, (bytes, bytearray, np.ndarray)):\n            raise ValueError('buf must be of type bytes, bytearray or numpy.ndarray,'\n                             'if you would like to input type str, please convert to bytes')\n        buf = nd.array(np.frombuffer(buf, dtype=np.uint8), dtype=np.uint8)\n\n    return _internal._cvimdecode(buf, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nscales down the image size if it s larger than image size.", "response": "def scale_down(src_size, size):\n    \"\"\"Scales down crop size if it's larger than image size.\n\n    If width/height of the crop is larger than the width/height of the image,\n    sets the width/height to the width/height of the image.\n\n    Parameters\n    ----------\n    src_size : tuple of int\n        Size of the image in (width, height) format.\n    size : tuple of int\n        Size of the crop in (width, height) format.\n\n    Returns\n    -------\n    tuple of int\n        A tuple containing the scaled crop size in (width, height) format.\n\n    Example\n    --------\n    >>> src_size = (640,480)\n    >>> size = (720,120)\n    >>> new_size = mx.img.scale_down(src_size, size)\n    >>> new_size\n    (640,106)\n    \"\"\"\n    w, h = size\n    sw, sh = src_size\n    if sh < h:\n        w, h = float(w * sh) / h, sh\n    if sw < w:\n        w, h = sw, float(h * sw) / w\n    return int(w), int(h)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npads image border with OpenCV. Parameters ---------- src : NDArray source image top : int, required Top margin. bot : int, required Bottom margin. left : int, required Left margin. right : int, required Right margin. type : int, optional, default='0' Filling type (default=cv2.BORDER_CONSTANT). 0 - cv2.BORDER_CONSTANT - Adds a constant colored border. 1 - cv2.BORDER_REFLECT - Border will be mirror reflection of the border elements, like this : fedcba|abcdefgh|hgfedcb 2 - cv2.BORDER_REFLECT_101 or cv.BORDER_DEFAULT - Same as above, but with a slight change, like this : gfedcb|abcdefgh|gfedcba 3 - cv2.BORDER_REPLICATE - Last element is replicated throughout, like this: aaaaaa|abcdefgh|hhhhhhh 4 - cv2.BORDER_WRAP - it will look like this : cdefgh|abcdefgh|abcdefg value : double, optional, default=0 (Deprecated! Use ``values`` instead.) Fill with single value. values : tuple of <double>, optional, default=[] Fill with value(RGB[A] or gray), up to 4 channels. out : NDArray, optional The output NDArray to hold the result. Returns ------- out : NDArray or list of NDArrays The output of this function. Example -------- >>> with open(\"flower.jpeg\", 'rb') as fp: ... str_image = fp.read() ... >>> image = mx.img.imdecode(str_image) >>> image <NDArray 2321x3482x3 @cpu(0)> >>> new_image = mx_border = mx.image.copyMakeBorder(mx_img, 1, 2, 3, 4, type=0) >>> new_image <NDArray 2324x3489x3 @cpu(0)>", "response": "def copyMakeBorder(src, top, bot, left, right, *args, **kwargs):\n    \"\"\"Pad image border with OpenCV.\n\n    Parameters\n    ----------\n    src : NDArray\n        source image\n    top : int, required\n        Top margin.\n    bot : int, required\n        Bottom margin.\n    left : int, required\n        Left margin.\n    right : int, required\n        Right margin.\n    type : int, optional, default='0'\n        Filling type (default=cv2.BORDER_CONSTANT).\n        0 - cv2.BORDER_CONSTANT - Adds a constant colored border.\n        1 - cv2.BORDER_REFLECT - Border will be mirror reflection of the\n        border elements, like this : fedcba|abcdefgh|hgfedcb\n        2 - cv2.BORDER_REFLECT_101 or cv.BORDER_DEFAULT - Same as above,\n        but with a slight change, like this : gfedcb|abcdefgh|gfedcba\n        3 - cv2.BORDER_REPLICATE - Last element is replicated throughout,\n        like this: aaaaaa|abcdefgh|hhhhhhh\n        4 - cv2.BORDER_WRAP - it will look like this : cdefgh|abcdefgh|abcdefg\n    value : double, optional, default=0\n        (Deprecated! Use ``values`` instead.) Fill with single value.\n    values : tuple of <double>, optional, default=[]\n        Fill with value(RGB[A] or gray), up to 4 channels.\n\n    out : NDArray, optional\n        The output NDArray to hold the result.\n\n    Returns\n    -------\n    out : NDArray or list of NDArrays\n        The output of this function.\n\n    Example\n    --------\n    >>> with open(\"flower.jpeg\", 'rb') as fp:\n    ...     str_image = fp.read()\n    ...\n    >>> image = mx.img.imdecode(str_image)\n    >>> image\n    <NDArray 2321x3482x3 @cpu(0)>\n    >>> new_image = mx_border = mx.image.copyMakeBorder(mx_img, 1, 2, 3, 4, type=0)\n    >>> new_image\n    <NDArray 2324x3489x3 @cpu(0)>\n    \"\"\"\n    return _internal._cvcopyMakeBorder(src, top, bot, left, right, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the interpolation method for resizing a single image.", "response": "def _get_interp_method(interp, sizes=()):\n    \"\"\"Get the interpolation method for resize functions.\n    The major purpose of this function is to wrap a random interp method selection\n    and a auto-estimation method.\n\n    Parameters\n    ----------\n    interp : int\n        interpolation method for all resizing operations\n\n        Possible values:\n        0: Nearest Neighbors Interpolation.\n        1: Bilinear interpolation.\n        2: Area-based (resampling using pixel area relation). It may be a\n        preferred method for image decimation, as it gives moire-free\n        results. But when the image is zoomed, it is similar to the Nearest\n        Neighbors method. (used by default).\n        3: Bicubic interpolation over 4x4 pixel neighborhood.\n        4: Lanczos interpolation over 8x8 pixel neighborhood.\n        9: Cubic for enlarge, area for shrink, bilinear for others\n        10: Random select from interpolation method metioned above.\n        Note:\n        When shrinking an image, it will generally look best with AREA-based\n        interpolation, whereas, when enlarging an image, it will generally look best\n        with Bicubic (slow) or Bilinear (faster but still looks OK).\n        More details can be found in the documentation of OpenCV, please refer to\n        http://docs.opencv.org/master/da/d54/group__imgproc__transform.html.\n    sizes : tuple of int\n        (old_height, old_width, new_height, new_width), if None provided, auto(9)\n        will return Area(2) anyway.\n\n    Returns\n    -------\n    int\n        interp method from 0 to 4\n    \"\"\"\n    if interp == 9:\n        if sizes:\n            assert len(sizes) == 4\n            oh, ow, nh, nw = sizes\n            if nh > oh and nw > ow:\n                return 2\n            elif nh < oh and nw < ow:\n                return 3\n            else:\n                return 1\n        else:\n            return 2\n    if interp == 10:\n        return random.randint(0, 4)\n    if interp not in (0, 1, 2, 3, 4):\n        raise ValueError('Unknown interp method %d' % interp)\n    return interp"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef resize_short(src, size, interp=2):\n    h, w, _ = src.shape\n    if h > w:\n        new_h, new_w = size * h // w, size\n    else:\n        new_h, new_w = size, size * w // h\n    return imresize(src, new_w, new_h, interp=_get_interp_method(interp, (h, w, new_h, new_w)))", "response": "Resizes the shorter edge to size."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncrops src at fixed location and resize it to size.", "response": "def fixed_crop(src, x0, y0, w, h, size=None, interp=2):\n    \"\"\"Crop src at fixed location, and (optionally) resize it to size.\n\n    Parameters\n    ----------\n    src : NDArray\n        Input image\n    x0 : int\n        Left boundary of the cropping area\n    y0 : int\n        Top boundary of the cropping area\n    w : int\n        Width of the cropping area\n    h : int\n        Height of the cropping area\n    size : tuple of (w, h)\n        Optional, resize to new size after cropping\n    interp : int, optional, default=2\n        Interpolation method. See resize_short for details.\n\n    Returns\n    -------\n    NDArray\n        An `NDArray` containing the cropped image.\n    \"\"\"\n    out = nd.slice(src, begin=(y0, x0, 0), end=(y0 + h, x0 + w, int(src.shape[2])))\n    if size is not None and (w, h) != size:\n        sizes = (h, w, size[1], size[0])\n        out = imresize(out, *size, interp=_get_interp_method(interp, sizes))\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef center_crop(src, size, interp=2):\n\n    h, w, _ = src.shape\n    new_w, new_h = scale_down((w, h), size)\n\n    x0 = int((w - new_w) / 2)\n    y0 = int((h - new_h) / 2)\n\n    out = fixed_crop(src, x0, y0, new_w, new_h, size, interp)\n    return out, (x0, y0, new_w, new_h)", "response": "Crops the image src to the given size and resizes it to the center of the image."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nnormalizes src with mean and std.", "response": "def color_normalize(src, mean, std=None):\n    \"\"\"Normalize src with mean and std.\n\n    Parameters\n    ----------\n    src : NDArray\n        Input image\n    mean : NDArray\n        RGB mean to be subtracted\n    std : NDArray\n        RGB standard deviation to be divided\n\n    Returns\n    -------\n    NDArray\n        An `NDArray` containing the normalized image.\n    \"\"\"\n    if mean is not None:\n        src -= mean\n    if std is not None:\n        src /= std\n    return src"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate an augmenter list for a given set of data.", "response": "def CreateAugmenter(data_shape, resize=0, rand_crop=False, rand_resize=False, rand_mirror=False,\n                    mean=None, std=None, brightness=0, contrast=0, saturation=0, hue=0,\n                    pca_noise=0, rand_gray=0, inter_method=2):\n    \"\"\"Creates an augmenter list.\n\n    Parameters\n    ----------\n    data_shape : tuple of int\n        Shape for output data\n    resize : int\n        Resize shorter edge if larger than 0 at the begining\n    rand_crop : bool\n        Whether to enable random cropping other than center crop\n    rand_resize : bool\n        Whether to enable random sized cropping, require rand_crop to be enabled\n    rand_gray : float\n        [0, 1], probability to convert to grayscale for all channels, the number\n        of channels will not be reduced to 1\n    rand_mirror : bool\n        Whether to apply horizontal flip to image with probability 0.5\n    mean : np.ndarray or None\n        Mean pixel values for [r, g, b]\n    std : np.ndarray or None\n        Standard deviations for [r, g, b]\n    brightness : float\n        Brightness jittering range (percent)\n    contrast : float\n        Contrast jittering range (percent)\n    saturation : float\n        Saturation jittering range (percent)\n    hue : float\n        Hue jittering range (percent)\n    pca_noise : float\n        Pca noise level (percent)\n    inter_method : int, default=2(Area-based)\n        Interpolation method for all resizing operations\n\n        Possible values:\n        0: Nearest Neighbors Interpolation.\n        1: Bilinear interpolation.\n        2: Area-based (resampling using pixel area relation). It may be a\n        preferred method for image decimation, as it gives moire-free\n        results. But when the image is zoomed, it is similar to the Nearest\n        Neighbors method. (used by default).\n        3: Bicubic interpolation over 4x4 pixel neighborhood.\n        4: Lanczos interpolation over 8x8 pixel neighborhood.\n        9: Cubic for enlarge, area for shrink, bilinear for others\n        10: Random select from interpolation method metioned above.\n        Note:\n        When shrinking an image, it will generally look best with AREA-based\n        interpolation, whereas, when enlarging an image, it will generally look best\n        with Bicubic (slow) or Bilinear (faster but still looks OK).\n\n    Examples\n    --------\n    >>> # An example of creating multiple augmenters\n    >>> augs = mx.image.CreateAugmenter(data_shape=(3, 300, 300), rand_mirror=True,\n    ...    mean=True, brightness=0.125, contrast=0.125, rand_gray=0.05,\n    ...    saturation=0.125, pca_noise=0.05, inter_method=10)\n    >>> # dump the details\n    >>> for aug in augs:\n    ...    aug.dumps()\n    \"\"\"\n    auglist = []\n\n    if resize > 0:\n        auglist.append(ResizeAug(resize, inter_method))\n\n    crop_size = (data_shape[2], data_shape[1])\n    if rand_resize:\n        assert rand_crop\n        auglist.append(RandomSizedCropAug(crop_size, 0.08, (3.0 / 4.0, 4.0 / 3.0), inter_method))\n    elif rand_crop:\n        auglist.append(RandomCropAug(crop_size, inter_method))\n    else:\n        auglist.append(CenterCropAug(crop_size, inter_method))\n\n    if rand_mirror:\n        auglist.append(HorizontalFlipAug(0.5))\n\n    auglist.append(CastAug())\n\n    if brightness or contrast or saturation:\n        auglist.append(ColorJitterAug(brightness, contrast, saturation))\n\n    if hue:\n        auglist.append(HueJitterAug(hue))\n\n    if pca_noise > 0:\n        eigval = np.array([55.46, 4.794, 1.148])\n        eigvec = np.array([[-0.5675, 0.7192, 0.4009],\n                           [-0.5808, -0.0045, -0.8140],\n                           [-0.5836, -0.6948, 0.4203]])\n        auglist.append(LightingAug(pca_noise, eigval, eigvec))\n\n    if rand_gray > 0:\n        auglist.append(RandomGrayAug(rand_gray))\n\n    if mean is True:\n        mean = nd.array([123.68, 116.28, 103.53])\n    elif mean is not None:\n        assert isinstance(mean, (np.ndarray, nd.NDArray)) and mean.shape[0] in [1, 3]\n\n    if std is True:\n        std = nd.array([58.395, 57.12, 57.375])\n    elif std is not None:\n        assert isinstance(std, (np.ndarray, nd.NDArray)) and std.shape[0] in [1, 3]\n\n    if mean is not None or std is not None:\n        auglist.append(ColorNormalizeAug(mean, std))\n\n    return auglist"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsaves the Augmenter to string returning the str JSON formatted string that describes the Augmenter.", "response": "def dumps(self):\n        \"\"\"Saves the Augmenter to string\n\n        Returns\n        -------\n        str\n            JSON formatted string that describes the Augmenter.\n        \"\"\"\n        return json.dumps([self.__class__.__name__.lower(), self._kwargs])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\noverride the default to avoid duplicate dump.", "response": "def dumps(self):\n        \"\"\"Override the default to avoid duplicate dump.\"\"\"\n        return [self.__class__.__name__.lower(), [x.dumps() for x in self.ts]]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nresets the iterator to the beginning of the data.", "response": "def reset(self):\n        \"\"\"Resets the iterator to the beginning of the data.\"\"\"\n        if self.seq is not None and self.shuffle:\n            random.shuffle(self.seq)\n        if self.last_batch_handle != 'roll_over' or \\\n            self._cache_data is None:\n            if self.imgrec is not None:\n                self.imgrec.reset()\n            self.cur = 0\n            if self._allow_read is False:\n                self._allow_read = True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreset the iterator and ignore roll over data", "response": "def hard_reset(self):\n        \"\"\"Resets the iterator and ignore roll over data\"\"\"\n        if self.seq is not None and self.shuffle:\n            random.shuffle(self.seq)\n        if self.imgrec is not None:\n            self.imgrec.reset()\n        self.cur = 0\n        self._allow_read = True\n        self._cache_data = None\n        self._cache_label = None\n        self._cache_idx = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef imdecode(self, s):\n        def locate():\n            \"\"\"Locate the image file/index if decode fails.\"\"\"\n            if self.seq is not None:\n                idx = self.seq[(self.cur % self.num_image) - 1]\n            else:\n                idx = (self.cur % self.num_image) - 1\n            if self.imglist is not None:\n                _, fname = self.imglist[idx]\n                msg = \"filename: {}\".format(fname)\n            else:\n                msg = \"index: {}\".format(idx)\n            return \"Broken image \" + msg\n        try:\n            img = imdecode(s)\n        except Exception as e:\n            raise RuntimeError(\"{}, {}\".format(locate(), e))\n        return img", "response": "Decodes a string or byte string to an NDArray."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading an input image fname and returns the decoded raw bytes.", "response": "def read_image(self, fname):\n        \"\"\"Reads an input image `fname` and returns the decoded raw bytes.\n        Examples\n        --------\n        >>> dataIter.read_image('Face.jpg') # returns decoded raw bytes.\n        \"\"\"\n        with open(os.path.join(self.path_root, fname), 'rb') as fin:\n            img = fin.read()\n        return img"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting a word to a list of integer vectors.", "response": "def word_to_vector(word):\n    \"\"\"\n    Convert character vectors to integer vectors.\n    \"\"\"\n    vector = []\n    for char in list(word):\n        vector.append(char2int(char))\n    return vector"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef vector_to_word(vector):\n    word = \"\"\n    for vec in vector:\n        word = word + int2char(vec)\n    return word", "response": "Convert integer vectors to character vectors."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef char_conv(out):\n    out_conv = list()\n    for i in range(out.shape[0]):\n        tmp_str = ''\n        for j in range(out.shape[1]):\n            if int(out[i][j]) >= 0:\n                tmp_char = int2char(int(out[i][j]))\n                if int(out[i][j]) == 27:\n                    tmp_char = ''\n                tmp_str = tmp_str + tmp_char\n        out_conv.append(tmp_str)\n    return out_conv", "response": "Convert integer vectors to character vectors for batch."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_pooling_with_padding_types(builder, name, height, width, stride_height, stride_width,\r\n        layer_type, padding_type, input_name, output_name,\r\n        padding_top = 0, padding_bottom = 0, padding_left = 0, padding_right = 0,\r\n        same_padding_asymmetry_mode = 'BOTTOM_RIGHT_HEAVY',\r\n        exclude_pad_area = True, is_global = False):\r\n    \"\"\"\r\n    Add a pooling layer to the model.\r\n\r\n    This is our own implementation of add_pooling since current CoreML's version (0.5.0) of builder\r\n    doesn't provide support for padding types apart from valid. This support will be added in the\r\n    next release of coremltools. When that happens, this can be removed.\r\n\r\n    Parameters\r\n\r\n    ----------\r\n    builder: NeuralNetworkBuilder\r\n        A neural network builder object.\r\n    name: str\r\n        The name of this layer.\r\n    height: int\r\n        Height of pooling region.\r\n    width: int\r\n        Number of elements to be padded on the right side of the input blob.\r\n    stride_height: int\r\n        Stride along the height direction.\r\n    stride_width: int\r\n        Stride along the height direction.\r\n    layer_type: str\r\n        Type of pooling performed. Can either be 'MAX', 'AVERAGE' or 'L2'.\r\n    padding_type: str\r\n        Option for the output blob shape. Can be either 'VALID' , 'SAME' or 'INCLUDE_LAST_PIXEL'. Kindly look at NeuralNetwork.proto for details.\r\n    input_name: str\r\n        The input blob name of this layer.\r\n    output_name: str\r\n        The output blob name of this layer.\r\n\r\n    padding_top, padding_bottom, padding_left, padding_right: int\r\n        values of height (top, bottom) and width (left, right) padding to be used if padding type is \"VALID\" or \"INCLUDE_LAST_PIXEL\"\r\n\r\n    same_padding_asymmetry_mode : str.\r\n        Type of asymmetric padding to be used when  padding_type = 'SAME'. Kindly look at NeuralNetwork.proto for details. Can be either 'BOTTOM_RIGHT_HEAVY' or  'TOP_LEFT_HEAVY'.\r\n\r\n    exclude_pad_area: boolean\r\n        Whether to exclude padded area in the pooling operation. Defaults to True.\r\n\r\n        - If True, the value of the padded area will be excluded.\r\n        - If False, the padded area will be included.\r\n        This flag is only used with average pooling.\r\n    is_global: boolean\r\n        Whether the pooling operation is global. Defaults to False.\r\n\r\n        - If True, the pooling operation is global -- the pooling region is of the same size of the input blob.\r\n        Parameters height, width, stride_height, stride_width will be ignored.\r\n\r\n        - If False, the pooling operation is not global.\r\n\r\n    See Also\r\n    --------\r\n    add_convolution, add_pooling, add_activation\r\n    \"\"\"\r\n\r\n    spec = builder.spec\r\n    nn_spec = builder.nn_spec\r\n\r\n    # Add a new layer\r\n    spec_layer = nn_spec.layers.add()\r\n    spec_layer.name = name\r\n    spec_layer.input.append(input_name)\r\n    spec_layer.output.append(output_name)\r\n    spec_layer_params = spec_layer.pooling\r\n\r\n    # Set the parameters\r\n    spec_layer_params.type = \\\r\n                _NeuralNetwork_pb2.PoolingLayerParams.PoolingType.Value(layer_type)\r\n\r\n    if padding_type == 'VALID':\r\n        height_border = spec_layer_params.valid.paddingAmounts.borderAmounts.add()\r\n        height_border.startEdgeSize = padding_top\r\n        height_border.endEdgeSize = padding_bottom\r\n        width_border = spec_layer_params.valid.paddingAmounts.borderAmounts.add()\r\n        width_border.startEdgeSize = padding_left\r\n        width_border.endEdgeSize = padding_right\r\n    elif padding_type == 'SAME':\r\n        if not (same_padding_asymmetry_mode == 'BOTTOM_RIGHT_HEAVY' or  same_padding_asymmetry_mode == 'TOP_LEFT_HEAVY'):\r\n            raise ValueError(\"Invalid value %d of same_padding_asymmetry_mode parameter\" % same_padding_asymmetry_mode)\r\n        spec_layer_params.same.asymmetryMode = _NeuralNetwork_pb2.SamePadding.SamePaddingMode.Value(same_padding_asymmetry_mode)\r\n    elif padding_type == 'INCLUDE_LAST_PIXEL':\r\n        if padding_top != padding_bottom or padding_left != padding_right:\r\n            raise ValueError(\"Only symmetric padding is supported with the INCLUDE_LAST_PIXEL padding type\")\r\n        spec_layer_params.includeLastPixel.paddingAmounts.append(padding_top)\r\n        spec_layer_params.includeLastPixel.paddingAmounts.append(padding_left)\r\n\r\n    spec_layer_params.kernelSize.append(height)\r\n    spec_layer_params.kernelSize.append(width)\r\n    spec_layer_params.stride.append(stride_height)\r\n    spec_layer_params.stride.append(stride_width)\r\n    spec_layer_params.avgPoolExcludePadding = exclude_pad_area\r\n    spec_layer_params.globalPooling = is_global", "response": "Add a pooling layer with optional padding types."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_frames(root_path):\n   ret = []\n   for root, _, files in os.walk(root_path):\n       root=root.replace('\\\\','/')\n       files=[s for s in files if \".dcm\" in s]\n       if len(files) == 0 or not files[0].endswith(\".dcm\") or root.find(\"sax\") == -1:\n           continue\n       prefix = files[0].rsplit('-', 1)[0]\n       fileset = set(files)\n       expected = [\"%s-%04d.dcm\" % (prefix, i + 1) for i in range(30)]\n       if all(x in fileset for x in expected):\n           ret.append([root + \"/\" + x for x in expected])\n   # sort for reproduciblity\n   return sorted(ret, key = lambda x: x[0])", "response": "Get all the frame in view SAX and contain complete frames"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write_data_csv(fname, frames, preproc):\n   fdata = open(fname, \"w\")\n   dr = Parallel()(delayed(get_data)(lst,preproc) for lst in frames)\n   data,result = zip(*dr)\n   for entry in data:\n      fdata.write(','.join(entry)+'\\r\\n')\n   print(\"All finished, %d slices in total\" % len(data))\n   fdata.close()\n   result = np.ravel(result)\n   return result", "response": "Write data to csv file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef crop_resize(img, size):\n   if img.shape[0] < img.shape[1]:\n       img = img.T\n   # we crop image from center\n   short_egde = min(img.shape[:2])\n   yy = int((img.shape[0] - short_egde) / 2)\n   xx = int((img.shape[1] - short_egde) / 2)\n   crop_img = img[yy : yy + short_egde, xx : xx + short_egde]\n   # resize to 64, 64\n   resized_img = transform.resize(crop_img, (size, size))\n   resized_img *= 255\n   return resized_img.astype(\"uint8\")", "response": "crop center and resize"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconstructing and return a generator for the current ISO - 8601 version of the current ISO - 8601 version", "response": "def get_generator():\n    \"\"\" construct and return generator \"\"\"\n    g_net = gluon.nn.Sequential()\n    with g_net.name_scope():\n\n        g_net.add(gluon.nn.Conv2DTranspose(\n            channels=512, kernel_size=4, strides=1, padding=0, use_bias=False))\n        g_net.add(gluon.nn.BatchNorm())\n        g_net.add(gluon.nn.LeakyReLU(0.2))\n\n        g_net.add(gluon.nn.Conv2DTranspose(\n            channels=256, kernel_size=4, strides=2, padding=1, use_bias=False))\n        g_net.add(gluon.nn.BatchNorm())\n        g_net.add(gluon.nn.LeakyReLU(0.2))\n\n        g_net.add(gluon.nn.Conv2DTranspose(\n            channels=128, kernel_size=4, strides=2, padding=1, use_bias=False))\n        g_net.add(gluon.nn.BatchNorm())\n        g_net.add(gluon.nn.LeakyReLU(0.2))\n\n        g_net.add(gluon.nn.Conv2DTranspose(\n            channels=64, kernel_size=4, strides=2, padding=1, use_bias=False))\n        g_net.add(gluon.nn.BatchNorm())\n        g_net.add(gluon.nn.LeakyReLU(0.2))\n\n        g_net.add(gluon.nn.Conv2DTranspose(channels=3, kernel_size=4, strides=2, padding=1, use_bias=False))\n        g_net.add(gluon.nn.Activation('tanh'))\n\n    return g_net"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_descriptor(ctx):\n    d_net = gluon.nn.Sequential()\n    with d_net.name_scope():\n\n        d_net.add(SNConv2D(num_filter=64, kernel_size=4, strides=2, padding=1, in_channels=3, ctx=ctx))\n        d_net.add(gluon.nn.LeakyReLU(0.2))\n\n        d_net.add(SNConv2D(num_filter=128, kernel_size=4, strides=2, padding=1, in_channels=64, ctx=ctx))\n        d_net.add(gluon.nn.LeakyReLU(0.2))\n\n        d_net.add(SNConv2D(num_filter=256, kernel_size=4, strides=2, padding=1, in_channels=128, ctx=ctx))\n        d_net.add(gluon.nn.LeakyReLU(0.2))\n\n        d_net.add(SNConv2D(num_filter=512, kernel_size=4, strides=2, padding=1, in_channels=256, ctx=ctx))\n        d_net.add(gluon.nn.LeakyReLU(0.2))\n\n        d_net.add(SNConv2D(num_filter=1, kernel_size=4, strides=1, padding=0, in_channels=512, ctx=ctx))\n\n    return d_net", "response": "construct and return a new descriptor"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the length of the output sequence after 1D convolution along the Keras module.", "response": "def conv_output_length(input_length, filter_size, border_mode, stride,\n                       dilation=1):\n    \"\"\" Compute the length of the output sequence after 1D convolution along\n        time. Note that this function is in line with the function used in\n        Convolution1D class from Keras.\n    Params:\n        input_length (int): Length of the input sequence.\n        filter_size (int): Width of the convolution kernel.\n        border_mode (str): Only support `same` or `valid`.\n        stride (int): Stride size used in 1D convolution.\n        dilation (int)\n    \"\"\"\n    if input_length is None:\n        return None\n    assert border_mode in {'same', 'valid'}\n    dilated_filter_size = filter_size + (filter_size - 1) * (dilation - 1)\n    if border_mode == 'same':\n        output_length = input_length\n    elif border_mode == 'valid':\n        output_length = input_length - dilated_filter_size + 1\n    return (output_length + stride - 1) // stride"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef spectrogram(samples, fft_length=256, sample_rate=2, hop_length=128):\n    assert not np.iscomplexobj(samples), \"Must not pass in complex numbers\"\n\n    window = np.hanning(fft_length)[:, None]\n    window_norm = np.sum(window ** 2)\n\n    # The scaling below follows the convention of\n    # matplotlib.mlab.specgram which is the same as\n    # matlabs specgram.\n    scale = window_norm * sample_rate\n\n    trunc = (len(samples) - fft_length) % hop_length\n    x = samples[:len(samples) - trunc]\n\n    # \"stride trick\" reshape to include overlap\n    nshape = (fft_length, (len(x) - fft_length) // hop_length + 1)\n    nstrides = (x.strides[0], x.strides[0] * hop_length)\n    x = as_strided(x, shape=nshape, strides=nstrides)\n\n    # window stride sanity check\n    assert np.all(x[:, 1] == samples[hop_length:(hop_length + fft_length)])\n\n    # broadcast window, compute fft over columns and square mod\n    # This function computes the one-dimensional n-point discrete Fourier Transform (DFT) of a real-valued array by means of an efficient algorithm called the Fast Fourier Transform (FFT).\n    x = np.fft.rfft(x * window, axis=0)\n    x = np.absolute(x) ** 2\n\n    # scale, 2.0 for everything except dc and fft_length/2\n    x[1:-1, :] *= (2.0 / scale)\n    x[(0, -1), :] /= scale\n\n    freqs = float(sample_rate) / fft_length * np.arange(x.shape[0])\n\n    return x, freqs", "response": "Compute the spectrogram for a real signal."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the log of linear spectrogram from the audio file.", "response": "def spectrogram_from_file(filename, step=10, window=20, max_freq=None,\n                          eps=1e-14, overwrite=False, save_feature_as_csvfile=False):\n    \"\"\" Calculate the log of linear spectrogram from FFT energy\n    Params:\n        filename (str): Path to the audio file\n        step (int): Step size in milliseconds between windows\n        window (int): FFT window size in milliseconds\n        max_freq (int): Only FFT bins corresponding to frequencies between\n            [0, max_freq] are returned\n        eps (float): Small value to ensure numerical stability (for ln(x))\n    \"\"\"\n\n    csvfilename = filename.replace(\".wav\", \".csv\")\n    if (os.path.isfile(csvfilename) is False) or overwrite:\n        with soundfile.SoundFile(filename) as sound_file:\n            audio = sound_file.read(dtype='float32')\n            sample_rate = sound_file.samplerate\n            if audio.ndim >= 2:\n                audio = np.mean(audio, 1)\n            if max_freq is None:\n                max_freq = sample_rate / 2\n            if max_freq > sample_rate / 2:\n                raise ValueError(\"max_freq must not be greater than half of \"\n                                 \" sample rate\")\n            if step > window:\n                raise ValueError(\"step size must not be greater than window size\")\n            hop_length = int(0.001 * step * sample_rate)\n            fft_length = int(0.001 * window * sample_rate)\n\n            pxx, freqs = spectrogram(\n                audio, fft_length=fft_length, sample_rate=sample_rate,\n                hop_length=hop_length)\n\n            ind = np.where(freqs <= max_freq)[0][-1] + 1\n            res = np.transpose(np.log(pxx[:ind, :] + eps))\n            if save_feature_as_csvfile:\n                np.savetxt(csvfilename, res)\n            return res\n    else:\n        return np.loadtxt(csvfilename)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates random cropping boxes according to parameters .", "response": "def sample(self, label):\n        \"\"\"\n        generate random cropping boxes according to parameters\n        if satifactory crops generated, apply to ground-truth as well\n\n        Parameters:\n        ----------\n        label : numpy.array (n x 5 matrix)\n            ground-truths\n\n        Returns:\n        ----------\n        list of (crop_box, label) tuples, if failed, return empty list []\n        \"\"\"\n        samples = []\n        count = 0\n        for trial in range(self.max_trials):\n            if count >= self.max_sample:\n                return samples\n            scale = np.random.uniform(self.min_scale, self.max_scale)\n            min_ratio = max(self.min_aspect_ratio, scale * scale)\n            max_ratio = min(self.max_aspect_ratio, 1. / scale / scale)\n            ratio = math.sqrt(np.random.uniform(min_ratio, max_ratio))\n            width = scale * ratio\n            height = scale / ratio\n            left = np.random.uniform(0., 1 - width)\n            top = np.random.uniform(0., 1 - height)\n            rand_box = (left, top, left + width, top + height)\n            valid_mask = np.where(label[:, 0] > -1)[0]\n            gt = label[valid_mask, :]\n            ious = self._check_satisfy(rand_box, gt)\n            if ious is not None:\n                # transform gt labels after crop, discard bad ones\n                l, t, r, b = rand_box\n                new_gt_boxes = []\n                new_width = r - l\n                new_height = b - t\n                for i in range(valid_mask.size):\n                    if ious[i] > 0:\n                        xmin = max(0., (gt[i, 1] - l) / new_width)\n                        ymin = max(0., (gt[i, 2] - t) / new_height)\n                        xmax = min(1., (gt[i, 3] - l) / new_width)\n                        ymax = min(1., (gt[i, 4] - t) / new_height)\n                        new_gt_boxes.append([gt[i, 0], xmin, ymin, xmax, ymax])\n                if not new_gt_boxes:\n                    continue\n                new_gt_boxes = np.array(new_gt_boxes)\n                label = np.lib.pad(new_gt_boxes,\n                    ((0, label.shape[0]-new_gt_boxes.shape[0]), (0,0)), \\\n                    'constant', constant_values=(-1, -1))\n                samples.append((rand_box, label))\n                count += 1\n        return samples"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _check_satisfy(self, rand_box, gt_boxes):\n        l, t, r, b = rand_box\n        num_gt = gt_boxes.shape[0]\n        ls = np.ones(num_gt) * l\n        ts = np.ones(num_gt) * t\n        rs = np.ones(num_gt) * r\n        bs = np.ones(num_gt) * b\n        mask = np.where(ls < gt_boxes[:, 1])[0]\n        ls[mask] = gt_boxes[mask, 1]\n        mask = np.where(ts < gt_boxes[:, 2])[0]\n        ts[mask] = gt_boxes[mask, 2]\n        mask = np.where(rs > gt_boxes[:, 3])[0]\n        rs[mask] = gt_boxes[mask, 3]\n        mask = np.where(bs > gt_boxes[:, 4])[0]\n        bs[mask] = gt_boxes[mask, 4]\n        w = rs - ls\n        w[w < 0] = 0\n        h = bs - ts\n        h[h < 0] = 0\n        inter_area = h * w\n        union_area = np.ones(num_gt) * max(0, r - l) * max(0, b - t)\n        union_area += (gt_boxes[:, 3] - gt_boxes[:, 1]) * (gt_boxes[:, 4] - gt_boxes[:, 2])\n        union_area -= inter_area\n        ious = inter_area / union_area\n        ious[union_area <= 0] = 0\n        max_iou = np.amax(ious)\n        if max_iou < self.min_overlap:\n            return None\n        # check ground-truth constraint\n        if self.config['gt_constraint'] == 'center':\n            for i in range(ious.shape[0]):\n                if ious[i] > 0:\n                    gt_x = (gt_boxes[i, 1] + gt_boxes[i, 3]) / 2.0\n                    gt_y = (gt_boxes[i, 2] + gt_boxes[i, 4]) / 2.0\n                    if gt_x < l or gt_x > r or gt_y < t or gt_y > b:\n                        return None\n        elif self.config['gt_constraint'] == 'corner':\n            for i in range(ious.shape[0]):\n                if ious[i] > 0:\n                    if gt_boxes[i, 1] < l or gt_boxes[i, 3] > r \\\n                        or gt_boxes[i, 2] < t or gt_boxes[i, 4] > b:\n                        return None\n        return ious", "response": "check if any of the random_box is larger than threshold"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating random padding boxes according to parameters .", "response": "def sample(self, label):\n        \"\"\"\n        generate random padding boxes according to parameters\n        if satifactory padding generated, apply to ground-truth as well\n\n        Parameters:\n        ----------\n        label : numpy.array (n x 5 matrix)\n            ground-truths\n\n        Returns:\n        ----------\n        list of (crop_box, label) tuples, if failed, return empty list []\n        \"\"\"\n        samples = []\n        count = 0\n        for trial in range(self.max_trials):\n            if count >= self.max_sample:\n                return samples\n            scale = np.random.uniform(self.min_scale, self.max_scale)\n            min_ratio = max(self.min_aspect_ratio, scale * scale)\n            max_ratio = min(self.max_aspect_ratio, 1. / scale / scale)\n            ratio = math.sqrt(np.random.uniform(min_ratio, max_ratio))\n            width = scale * ratio\n            if width < 1:\n                continue\n            height = scale / ratio\n            if height < 1:\n                continue\n            left = np.random.uniform(0., 1 - width)\n            top = np.random.uniform(0., 1 - height)\n            right = left + width\n            bot = top + height\n            rand_box = (left, top, right, bot)\n            valid_mask = np.where(label[:, 0] > -1)[0]\n            gt = label[valid_mask, :]\n            new_gt_boxes = []\n            for i in range(gt.shape[0]):\n                xmin = (gt[i, 1] - left) / width\n                ymin = (gt[i, 2] - top) / height\n                xmax = (gt[i, 3] - left) / width\n                ymax = (gt[i, 4] - top) / height\n                new_size = min(xmax - xmin, ymax - ymin)\n                if new_size < self.min_gt_scale:\n                    new_gt_boxes = []\n                    break\n                new_gt_boxes.append([gt[i, 0], xmin, ymin, xmax, ymax])\n            if not new_gt_boxes:\n                continue\n            new_gt_boxes = np.array(new_gt_boxes)\n            label = np.lib.pad(new_gt_boxes,\n                ((0, label.shape[0]-new_gt_boxes.shape[0]), (0,0)), \\\n                'constant', constant_values=(-1, -1))\n            samples.append((rand_box, label))\n            count += 1\n        return samples"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmeasure time cost of running a function in a random order", "response": "def measure_cost(repeat, scipy_trans_lhs, scipy_dns_lhs, func_name, *args, **kwargs):\n    \"\"\"Measure time cost of running a function\n    \"\"\"\n    mx.nd.waitall()\n    args_list = []\n    for arg in args:\n        args_list.append(arg)\n    start = time.time()\n    if scipy_trans_lhs:\n        args_list[0] = np.transpose(args_list[0]) if scipy_dns_lhs else sp.spmatrix.transpose(args_list[0])\n    for _ in range(repeat):\n        func_name(*args_list, **kwargs)\n    mx.nd.waitall()\n    end = time.time()\n    diff = end - start\n    return diff / repeat"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef info(self):\n        for key, value in self.dataset['info'].items():\n            print('{}: {}'.format(key, value))", "response": "Print information about the annotation file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the ids of the given categories", "response": "def getCatIds(self, catNms=[], supNms=[], catIds=[]):\n        \"\"\"\n        filtering parameters. default skips that filter.\n        :param catNms (str array)  : get cats for given cat names\n        :param supNms (str array)  : get cats for given supercategory names\n        :param catIds (int array)  : get cats for given cat ids\n        :return: ids (int array)   : integer array of cat ids\n        \"\"\"\n        catNms = catNms if type(catNms) == list else [catNms]\n        supNms = supNms if type(supNms) == list else [supNms]\n        catIds = catIds if type(catIds) == list else [catIds]\n\n        if len(catNms) == len(supNms) == len(catIds) == 0:\n            cats = self.dataset['categories']\n        else:\n            cats = self.dataset['categories']\n            cats = cats if len(catNms) == 0 else [cat for cat in cats if cat['name']          in catNms]\n            cats = cats if len(supNms) == 0 else [cat for cat in cats if cat['supercategory'] in supNms]\n            cats = cats if len(catIds) == 0 else [cat for cat in cats if cat['id']            in catIds]\n        ids = [cat['id'] for cat in cats]\n        return ids"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloads anns with the specified ids.", "response": "def loadAnns(self, ids=[]):\n        \"\"\"\n        Load anns with the specified ids.\n        :param ids (int array)       : integer ids specifying anns\n        :return: anns (object array) : loaded ann objects\n        \"\"\"\n        if type(ids) == list:\n            return [self.anns[id] for id in ids]\n        elif type(ids) == int:\n            return [self.anns[ids]]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload cats with the specified ids.", "response": "def loadCats(self, ids=[]):\n        \"\"\"\n        Load cats with the specified ids.\n        :param ids (int array)       : integer ids specifying cats\n        :return: cats (object array) : loaded cat objects\n        \"\"\"\n        if type(ids) == list:\n            return [self.cats[id] for id in ids]\n        elif type(ids) == int:\n            return [self.cats[ids]]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload an img objects with the specified ids.", "response": "def loadImgs(self, ids=[]):\n        \"\"\"\n        Load anns with the specified ids.\n        :param ids (int array)       : integer ids specifying img\n        :return: imgs (object array) : loaded img objects\n        \"\"\"\n        if type(ids) == list:\n            return [self.imgs[id] for id in ids]\n        elif type(ids) == int:\n            return [self.imgs[ids]]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndisplay the specified annotations.", "response": "def showAnns(self, anns):\n        \"\"\"\n        Display the specified annotations.\n        :param anns (array of object): annotations to display\n        :return: None\n        \"\"\"\n        if len(anns) == 0:\n            return 0\n        if 'segmentation' in anns[0] or 'keypoints' in anns[0]:\n            datasetType = 'instances'\n        elif 'caption' in anns[0]:\n            datasetType = 'captions'\n        else:\n            raise Exception('datasetType not supported')\n        if datasetType == 'instances':\n            ax = plt.gca()\n            ax.set_autoscale_on(False)\n            polygons = []\n            color = []\n            for ann in anns:\n                c = (np.random.random((1, 3))*0.6+0.4).tolist()[0]\n                if 'segmentation' in ann:\n                    if type(ann['segmentation']) == list:\n                        # polygon\n                        for seg in ann['segmentation']:\n                            poly = np.array(seg).reshape((int(len(seg)/2), 2))\n                            polygons.append(Polygon(poly))\n                            color.append(c)\n                    else:\n                        # mask\n                        raise NotImplementedError(\"maskUtils disabled!\")\n                if 'keypoints' in ann and type(ann['keypoints']) == list:\n                    # turn skeleton into zero-based index\n                    sks = np.array(self.loadCats(ann['category_id'])[0]['skeleton'])-1\n                    kp = np.array(ann['keypoints'])\n                    x = kp[0::3]\n                    y = kp[1::3]\n                    v = kp[2::3]\n                    for sk in sks:\n                        if np.all(v[sk]>0):\n                            plt.plot(x[sk],y[sk], linewidth=3, color=c)\n                    plt.plot(x[v>0], y[v>0],'o',markersize=8, markerfacecolor=c, markeredgecolor='k',markeredgewidth=2)\n                    plt.plot(x[v>1], y[v>1],'o',markersize=8, markerfacecolor=c, markeredgecolor=c, markeredgewidth=2)\n            p = PatchCollection(polygons, facecolor=color, linewidths=0, alpha=0.4)\n            ax.add_collection(p)\n            p = PatchCollection(polygons, facecolor='none', edgecolors=color, linewidths=2)\n            ax.add_collection(p)\n        elif datasetType == 'captions':\n            for ann in anns:\n                print(ann['caption'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef download(self, tarDir = None, imgIds = [] ):\n        '''\n        Download COCO images from mscoco.org server.\n        :param tarDir (str): COCO results directory name\n               imgIds (list): images to be downloaded\n        :return:\n        '''\n        if tarDir is None:\n            print('Please specify target directory')\n            return -1\n        if len(imgIds) == 0:\n            imgs = self.imgs.values()\n        else:\n            imgs = self.loadImgs(imgIds)\n        N = len(imgs)\n        if not os.path.exists(tarDir):\n            os.makedirs(tarDir)\n        for i, img in enumerate(imgs):\n            tic = time.time()\n            fname = os.path.join(tarDir, img['file_name'])\n            if not os.path.exists(fname):\n                urlretrieve(img['coco_url'], fname)\n            print('downloaded {}/{} images (t={:0.1f}s)'.format(i, N, time.time()- tic))", "response": "Download all images from mscoco. org server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert result data from a numpy array [ Nx7 ] where each row contains imageID x1 y1 w h score class", "response": "def loadNumpyAnnotations(self, data):\n        \"\"\"\n        Convert result data from a numpy array [Nx7] where each row contains {imageID,x1,y1,w,h,score,class}\n        :param  data (numpy.ndarray)\n        :return: annotations (python nested list)\n        \"\"\"\n        print('Converting ndarray to lists...')\n        assert(type(data) == np.ndarray)\n        print(data.shape)\n        assert(data.shape[1] == 7)\n        N = data.shape[0]\n        ann = []\n        for i in range(N):\n            if i % 1000000 == 0:\n                print('{}/{}'.format(i,N))\n            ann += [{\n                'image_id'  : int(data[i, 0]),\n                'bbox'  : [ data[i, 1], data[i, 2], data[i, 3], data[i, 4] ],\n                'score' : data[i, 5],\n                'category_id': int(data[i, 6]),\n                }]\n        return ann"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef annToRLE(self, ann):\n        t = self.imgs[ann['image_id']]\n        h, w = t['height'], t['width']\n        segm = ann['segmentation']\n        if type(segm) == list:\n            # polygon -- a single object might consist of multiple parts\n            # we merge all parts into one mask rle code\n            # rles = maskUtils.frPyObjects(segm, h, w)\n            # rle = maskUtils.merge(rles)\n            raise NotImplementedError(\"maskUtils disabled!\")\n        elif type(segm['counts']) == list:\n            # uncompressed RLE\n            # rle = maskUtils.frPyObjects(segm, h, w)\n            raise NotImplementedError(\"maskUtils disabled!\")\n        else:\n            # rle\n            rle = ann['segmentation']\n        return rle", "response": "Convert an annotation which can be polygons uncompressed RLE to binary mask."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsaves cnn model Returns-----------", "response": "def save_model():\n    \"\"\"Save cnn model\n    Returns\n    ----------\n    callback: A callback function that can be passed as epoch_end_callback to fit\n    \"\"\"\n    if not os.path.exists(\"checkpoint\"):\n        os.mkdir(\"checkpoint\")\n    return mx.callback.do_checkpoint(\"checkpoint/checkpoint\", args.save_period)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef highway(data):\n    _data = data\n    high_weight = mx.sym.Variable('high_weight')\n    high_bias = mx.sym.Variable('high_bias')\n    high_fc = mx.sym.FullyConnected(data=data, weight=high_weight, bias=high_bias, num_hidden=300, name='high_fc')\n    high_relu = mx.sym.Activation(high_fc, act_type='relu')\n\n    high_trans_weight = mx.sym.Variable('high_trans_weight')\n    high_trans_bias = mx.sym.Variable('high_trans_bias')\n    high_trans_fc = mx.sym.FullyConnected(data=_data, weight=high_trans_weight, bias=high_trans_bias, num_hidden=300,\n                                          name='high_trans_sigmoid')\n    high_trans_sigmoid = mx.sym.Activation(high_trans_fc, act_type='sigmoid')\n\n    return high_relu * high_trans_sigmoid + _data * (1 - high_trans_sigmoid)", "response": "Construct highway net\n    Parameters\n    ----------\n    data:\n    Returns\n    ----------\n    Highway Networks"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntrain cnn model on a set of data.", "response": "def train(symbol_data, train_iterator, valid_iterator, data_column_names, target_names):\n    \"\"\"Train cnn model\n    Parameters\n    ----------\n    symbol_data: symbol\n    train_iterator: DataIter\n                    Train DataIter\n    valid_iterator: DataIter\n                    Valid DataIter\n    data_column_names: list of str\n                       Defaults to ('data') for a typical model used in image classification\n    target_names: list of str\n                  Defaults to ('softmax_label') for a typical model used in image classification\n    \"\"\"\n    devs = mx.cpu()  # default setting\n    if args.gpus is not None:\n        for i in args.gpus.split(','):\n            mx.gpu(int(i))\n        devs = mx.gpu()\n    module = mx.mod.Module(symbol_data, data_names=data_column_names, label_names=target_names, context=devs)\n\n    init_params = {\n        'vocab_embed_weight': {'uniform': 0.1},\n        'convolution0_weight': {'uniform': 0.1}, 'convolution0_bias': {'costant': 0},\n        'convolution1_weight': {'uniform': 0.1}, 'convolution1_bias': {'costant': 0},\n        'convolution2_weight': {'uniform': 0.1}, 'convolution2_bias': {'costant': 0},\n        'high_weight': {'uniform': 0.1}, 'high_bias': {'costant': 0},\n        'high_trans_weight': {'uniform': 0.1}, 'high_trans_bias': {'costant': -2},\n        'cls_weight': {'uniform': 0.1}, 'cls_bias': {'costant': 0},\n    }\n    # custom init_params\n    module.bind(data_shapes=train_iterator.provide_data, label_shapes=train_iterator.provide_label)\n    module.init_params(CustomInit(init_params))\n    lr_sch = mx.lr_scheduler.FactorScheduler(step=25000, factor=0.999)\n    module.init_optimizer(\n        optimizer='rmsprop', optimizer_params={'learning_rate': 0.0005, 'lr_scheduler': lr_sch})\n\n    def norm_stat(d):\n        return mx.nd.norm(d) / np.sqrt(d.size)\n    mon = mx.mon.Monitor(25000, norm_stat)\n\n    module.fit(train_data=train_iterator,\n               eval_data=valid_iterator,\n               eval_metric='acc',\n               kvstore=args.kv_store,\n               monitor=mon,\n               num_epoch=args.num_epochs,\n               batch_end_callback=mx.callback.Speedometer(args.batch_size, args.disp_batches),\n               epoch_end_callback=save_model())"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef default_batchify_fn(data):\n    if isinstance(data[0], nd.NDArray):\n        return nd.stack(*data)\n    elif isinstance(data[0], tuple):\n        data = zip(*data)\n        return [default_batchify_fn(i) for i in data]\n    else:\n        data = np.asarray(data)\n        return nd.array(data, dtype=data.dtype)", "response": "Collate data into batch."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef default_mp_batchify_fn(data):\n    if isinstance(data[0], nd.NDArray):\n        out = nd.empty((len(data),) + data[0].shape, dtype=data[0].dtype,\n                       ctx=context.Context('cpu_shared', 0))\n        return nd.stack(*data, out=out)\n    elif isinstance(data[0], tuple):\n        data = zip(*data)\n        return [default_mp_batchify_fn(i) for i in data]\n    else:\n        data = np.asarray(data)\n        return nd.array(data, dtype=data.dtype,\n                        ctx=context.Context('cpu_shared', 0))", "response": "Collate data into batch. Use shared memory for stacking."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _as_in_context(data, ctx):\n    if isinstance(data, nd.NDArray):\n        return data.as_in_context(ctx)\n    elif isinstance(data, (list, tuple)):\n        return [_as_in_context(d, ctx) for d in data]\n    return data", "response": "Move data into new context."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef worker_loop_v1(dataset, key_queue, data_queue, batchify_fn):\n    while True:\n        idx, samples = key_queue.get()\n        if idx is None:\n            break\n        batch = batchify_fn([dataset[i] for i in samples])\n        data_queue.put((idx, batch))", "response": "Worker loop for multiprocessing DataLoader v1."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fetcher_loop_v1(data_queue, data_buffer, pin_memory=False,\n                    pin_device_id=0, data_buffer_lock=None):\n    \"\"\"Fetcher loop for fetching data from queue and put in reorder dict.\"\"\"\n    while True:\n        idx, batch = data_queue.get()\n        if idx is None:\n            break\n        if pin_memory:\n            batch = _as_in_context(batch, context.cpu_pinned(pin_device_id))\n        else:\n            batch = _as_in_context(batch, context.cpu())\n        if data_buffer_lock is not None:\n            with data_buffer_lock:\n                data_buffer[idx] = batch\n        else:\n            data_buffer[idx] = batch", "response": "Fetcher loop for fetching data from queue and put in reorder dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfunctions for processing data in worker process.", "response": "def _worker_fn(samples, batchify_fn, dataset=None):\n    \"\"\"Function for processing data in worker process.\"\"\"\n    # pylint: disable=unused-argument\n    # it is required that each worker process has to fork a new MXIndexedRecordIO handle\n    # preserving dataset as global variable can save tons of overhead and is safe in new process\n    global _worker_dataset\n    batch = batchify_fn([_worker_dataset[i] for i in samples])\n    buf = io.BytesIO()\n    ForkingPickler(buf, pickle.HIGHEST_PROTOCOL).dump(batch)\n    return buf.getvalue()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsending object to the socket.", "response": "def send(self, obj):\n        \"\"\"Send object\"\"\"\n        buf = io.BytesIO()\n        ForkingPickler(buf, pickle.HIGHEST_PROTOCOL).dump(obj)\n        self.send_bytes(buf.getvalue())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nassign next batch workload to workers.", "response": "def _push_next(self):\n        \"\"\"Assign next batch workload to workers.\"\"\"\n        r = next(self._iter, None)\n        if r is None:\n            return\n        self._key_queue.put((self._sent_idx, r))\n        self._sent_idx += 1"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nassign next batch workload to workers.", "response": "def _push_next(self):\n        \"\"\"Assign next batch workload to workers.\"\"\"\n        r = next(self._iter, None)\n        if r is None:\n            return\n        async_ret = self._worker_pool.apply_async(\n            self._worker_fn, (r, self._batchify_fn, self._dataset))\n        self._data_buffer[self._sent_idx] = async_ret\n        self._sent_idx += 1"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning ctype arrays for the key - value args and whether string keys are used.", "response": "def _ctype_key_value(keys, vals):\n    \"\"\"\n    Returns ctype arrays for the key-value args, and the whether string keys are used.\n    For internal use only.\n    \"\"\"\n    if isinstance(keys, (tuple, list)):\n        assert(len(keys) == len(vals))\n        c_keys = []\n        c_vals = []\n        use_str_keys = None\n        for key, val in zip(keys, vals):\n            c_key_i, c_val_i, str_keys_i = _ctype_key_value(key, val)\n            c_keys += c_key_i\n            c_vals += c_val_i\n            use_str_keys = str_keys_i if use_str_keys is None else use_str_keys\n            assert(use_str_keys == str_keys_i), \"inconsistent types of keys detected.\"\n        c_keys_arr = c_array(ctypes.c_char_p, c_keys) if use_str_keys \\\n                     else c_array(ctypes.c_int, c_keys)\n        c_vals_arr = c_array(ctypes.c_void_p, c_vals)\n        return (c_keys_arr, c_vals_arr, use_str_keys)\n\n    assert(isinstance(keys, (int,) + string_types)), \\\n           \"unexpected type for keys: \" + str(type(keys))\n    use_str_keys = isinstance(keys, string_types)\n    if isinstance(vals, NDArray):\n        c_keys = c_str_array([keys]) if use_str_keys \\\n                 else c_array_buf(ctypes.c_int, array('i', [keys]))\n        return (c_keys, c_handle_array([vals]), use_str_keys)\n    else:\n        for value in vals:\n            assert(isinstance(value, NDArray))\n        c_keys = c_str_array([keys] * len(vals)) if use_str_keys \\\n                 else c_array_buf(ctypes.c_int, array('i', [keys] * len(vals)))\n        return (c_keys, c_handle_array(vals), use_str_keys)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _ctype_dict(param_dict):\n    assert(isinstance(param_dict, dict)), \\\n        \"unexpected type for param_dict: \" + str(type(param_dict))\n    c_keys = c_array(ctypes.c_char_p, [c_str(k) for k in param_dict.keys()])\n    c_vals = c_array(ctypes.c_char_p, [c_str(str(v)) for v in param_dict.values()])\n    return (c_keys, c_vals)", "response": "Returns ctype arrays for keys and values in a dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create(name='local'):\n    if not isinstance(name, string_types):\n        raise TypeError('name must be a string')\n    handle = KVStoreHandle()\n    check_call(_LIB.MXKVStoreCreate(c_str(name),\n                                    ctypes.byref(handle)))\n    kv = KVStore(handle)\n    set_kvstore_handle(kv.handle)\n    return kv", "response": "Creates a new KVStore."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninitializing a single or a sequence of key - value pairs into the store.", "response": "def init(self, key, value):\n        \"\"\" Initializes a single or a sequence of key-value pairs into the store.\n\n        For each key, one must `init` it before calling `push` or `pull`.\n        When multiple workers invoke `init` for the same key, only\n        the value supplied by worker with rank `0` is used. This function returns\n        after data has been initialized successfully.\n\n        Parameters\n        ----------\n        key : str, int, or sequence of str or int\n            The keys.\n        value : NDArray, RowSparseNDArray or sequence of NDArray or RowSparseNDArray\n            Values corresponding to the keys.\n\n        Examples\n        --------\n        >>> # init a single key-value pair\n        >>> shape = (2,3)\n        >>> kv = mx.kv.create('local')\n        >>> kv.init('3', mx.nd.ones(shape)*2)\n        >>> a = mx.nd.zeros(shape)\n        >>> kv.pull('3', out=a)\n        >>> print a.asnumpy()\n        [[ 2.  2.  2.]\n        [ 2.  2.  2.]]\n\n        >>> # init a list of key-value pairs\n        >>> keys = ['5', '7', '9']\n        >>> kv.init(keys, [mx.nd.ones(shape)]*len(keys))\n\n        >>> # init a row_sparse value\n        >>> kv.init('4', mx.nd.ones(shape).tostype('row_sparse'))\n        >>> b = mx.nd.sparse.zeros('row_sparse', shape)\n        >>> kv.row_sparse_pull('4', row_ids=mx.nd.array([0, 1]), out=b)\n        >>> print b\n        <RowSparseNDArray 2x3 @cpu(0)>\n        \"\"\"\n        ckeys, cvals, use_str_keys = _ctype_key_value(key, value)\n        if use_str_keys:\n            check_call(_LIB.MXKVStoreInitEx(self.handle, mx_uint(len(ckeys)), ckeys, cvals))\n        else:\n            check_call(_LIB.MXKVStoreInit(self.handle, mx_uint(len(ckeys)), ckeys, cvals))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef push(self, key, value, priority=0):\n        ckeys, cvals, use_str_keys = _ctype_key_value(key, value)\n        if use_str_keys:\n            check_call(_LIB.MXKVStorePushEx(\n                self.handle, mx_uint(len(ckeys)), ckeys, cvals, ctypes.c_int(priority)))\n        else:\n            check_call(_LIB.MXKVStorePush(\n                self.handle, mx_uint(len(ckeys)), ckeys, cvals, ctypes.c_int(priority)))", "response": "Pushes a single or a sequence of key - value pairs into the store."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pull(self, key, out=None, priority=0, ignore_sparse=True):\n        assert(out is not None)\n        ckeys, cvals, use_str_keys = _ctype_key_value(key, out)\n        if use_str_keys:\n            check_call(_LIB.MXKVStorePullWithSparseEx(self.handle, mx_uint(len(ckeys)), ckeys,\n                                                      cvals, ctypes.c_int(priority),\n                                                      ctypes.c_bool(ignore_sparse)))\n        else:\n            check_call(_LIB.MXKVStorePullWithSparse(self.handle, mx_uint(len(ckeys)), ckeys,\n                                                    cvals, ctypes.c_int(priority),\n                                                    ctypes.c_bool(ignore_sparse)))", "response": "Pulls a single value or a sequence of values from the store."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef row_sparse_pull(self, key, out=None, priority=0, row_ids=None):\n        assert(out is not None)\n        assert(row_ids is not None)\n        if isinstance(row_ids, NDArray):\n            row_ids = [row_ids]\n        assert(isinstance(row_ids, list)), \\\n            \"row_ids should be NDArray or list of NDArray\"\n        first_out = out\n        # whether row_ids are the same\n        single_rowid = False\n        if len(row_ids) == 1 and isinstance(out, list):\n            single_rowid = True\n            first_out = [out[0]]\n        ckeys, cvals, use_str_keys = _ctype_key_value(key, first_out)\n        _, crow_ids, _ = _ctype_key_value(key, row_ids)\n        assert(len(crow_ids) == len(cvals)), \\\n               \"the number of row_ids doesn't match the number of values\"\n        if use_str_keys:\n            check_call(_LIB.MXKVStorePullRowSparseEx(\n                self.handle, mx_uint(len(ckeys)), ckeys, cvals, crow_ids, ctypes.c_int(priority)))\n        else:\n            check_call(_LIB.MXKVStorePullRowSparse(\n                self.handle, mx_uint(len(ckeys)), ckeys, cvals, crow_ids, ctypes.c_int(priority)))\n        # the result can be copied to other devices without invoking row_sparse_pull\n        # if the indices are the same\n        if single_rowid:\n            for out_i in out[1:]:\n                out[0].copyto(out_i)", "response": "Pulls a single RowSparseNDArray value or a sequence of RowSparseNDArray values from the store with specified row_ids."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the gradient compression for the given key - value pairs.", "response": "def set_gradient_compression(self, compression_params):\n        \"\"\" Specifies type of low-bit quantization for gradient compression \\\n         and additional arguments depending on the type of compression being used.\n\n        2bit Gradient Compression takes a positive float `threshold`.\n        The technique works by thresholding values such that positive values in the\n        gradient above threshold will be set to threshold. Negative values whose absolute\n        values are higher than threshold, will be set to the negative of threshold.\n        Values whose absolute values are less than threshold will be set to 0.\n        By doing so, each value in the gradient is in one of three states. 2bits are\n        used to represent these states, and every 16 float values in the original\n        gradient can be represented using one float. This compressed representation\n        can reduce communication costs. The difference between these thresholded values and\n        original values is stored at the sender's end as residual and added to the\n        gradient in the next iteration.\n\n        When kvstore is 'local', gradient compression is used to reduce communication\n        between multiple devices (gpus). Gradient is quantized on each GPU which\n        computed the gradients, then sent to the GPU which merges the gradients. This\n        receiving GPU dequantizes the gradients and merges them. Note that this\n        increases memory usage on each GPU because of the residual array stored.\n\n        When kvstore is 'dist', gradient compression is used to reduce communication\n        from worker to sender. Gradient is quantized on each worker which\n        computed the gradients, then sent to the server which dequantizes\n        this data and merges the gradients from each worker. Note that this\n        increases CPU memory usage on each worker because of the residual array stored.\n        Only worker to server communication is compressed in this setting.\n        If each machine has multiple GPUs, currently this GPU to GPU or GPU to CPU communication\n        is not compressed. Server to worker communication (in the case of pull)\n        is also not compressed.\n\n        To use 2bit compression, we need to specify `type` as `2bit`.\n        Only specifying `type` would use default value for the threshold.\n        To completely specify the arguments for 2bit compression, we would need to pass\n        a dictionary which includes `threshold` like:\n        {'type': '2bit', 'threshold': 0.5}\n\n        Parameters\n        ----------\n        compression_params : dict\n            A dictionary specifying the type and parameters for gradient compression.\n            The key `type` in this dictionary is a\n            required string argument and specifies the type of gradient compression.\n            Currently `type` can be only `2bit`\n            Other keys in this dictionary are optional and specific to the type\n            of gradient compression.\n        \"\"\"\n        if ('device' in self.type) or ('dist' in self.type): # pylint: disable=unsupported-membership-test\n            ckeys, cvals = _ctype_dict(compression_params)\n            check_call(_LIB.MXKVStoreSetGradientCompression(self.handle,\n                                                            mx_uint(len(compression_params)),\n                                                            ckeys, cvals))\n        else:\n            raise Exception('Gradient compression is not supported for this type of kvstore')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nregistering an optimizer for the store and sends it to all servers.", "response": "def set_optimizer(self, optimizer):\n        \"\"\" Registers an optimizer with the kvstore.\n\n        When using a single machine, this function updates the local optimizer.\n        If using multiple machines and this operation is invoked from a worker node,\n        it will serialized the optimizer with pickle and send it to all servers.\n        The function returns after all servers have been updated.\n\n        Parameters\n        ----------\n        optimizer : Optimizer\n            The new optimizer for the store\n\n        Examples\n        --------\n\n        >>> kv = mx.kv.create()\n        >>> shape = (2, 2)\n        >>> weight = mx.nd.zeros(shape)\n        >>> kv.init(3, weight)\n        >>> # set the optimizer for kvstore as the default SGD optimizer\n        >>> kv.set_optimizer(mx.optimizer.SGD())\n        >>> grad = mx.nd.ones(shape)\n        >>> kv.push(3, grad)\n        >>> kv.pull(3, out = weight)\n        >>> # weight is updated via gradient descent\n        >>> weight.asnumpy()\n        array([[-0.01, -0.01],\n               [-0.01, -0.01]], dtype=float32)\n        \"\"\"\n        is_worker = ctypes.c_int()\n        check_call(_LIB.MXKVStoreIsWorkerNode(ctypes.byref(is_worker)))\n\n        # pylint: disable=invalid-name\n        if 'dist' in self.type and is_worker.value: # pylint: disable=unsupported-membership-test\n            # send the optimizer to server\n            try:\n                # use ASCII protocol 0, might be slower, but not a big ideal\n                optim_str = py_str(pickle.dumps(optimizer, 0))\n            except:\n                raise\n            cmd = _get_kvstore_server_command_type('kController')\n            self._send_command_to_servers(cmd, optim_str)\n            if optimizer.multi_precision:\n                cmd = _get_kvstore_server_command_type('kSetMultiPrecision')\n                self._send_command_to_servers(cmd, '')\n        else:\n            self._set_updater(opt.get_updater(optimizer))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the type of this kvstore.", "response": "def type(self):\n        \"\"\" Returns the type of this kvstore.\n\n        Returns\n        -------\n        type : str\n            the string type\n        \"\"\"\n        kv_type = ctypes.c_char_p()\n        check_call(_LIB.MXKVStoreGetType(self.handle, ctypes.byref(kv_type)))\n        return py_str(kv_type.value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the rank of this worker node.", "response": "def rank(self):\n        \"\"\" Returns the rank of this worker node.\n\n        Returns\n        -------\n        rank : int\n            The rank of this node, which is in range [0, num_workers())\n        \"\"\"\n        rank = ctypes.c_int()\n        check_call(_LIB.MXKVStoreGetRank(self.handle, ctypes.byref(rank)))\n        return rank.value"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the number of worker nodes in the group.", "response": "def num_workers(self):\n        \"\"\"Returns the number of worker nodes.\n\n        Returns\n        -------\n        size :int\n            The number of worker nodes.\n        \"\"\"\n        size = ctypes.c_int()\n        check_call(_LIB.MXKVStoreGetGroupSize(self.handle, ctypes.byref(size)))\n        return size.value"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save_optimizer_states(self, fname, dump_optimizer=False):\n        assert self._updater is not None, \"Cannot save states for distributed training\"\n        with open(fname, 'wb') as fout:\n            fout.write(self._updater.get_states(dump_optimizer))", "response": "Saves the optimizer state to a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_optimizer_states(self, fname):\n        assert self._updater is not None, \"Cannot load states for distributed training\"\n        self._updater.set_states(open(fname, 'rb').read())", "response": "Loads the optimizer state from the file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _set_updater(self, updater):\n        self._updater = updater\n        # set updater with int keys\n        _updater_proto = ctypes.CFUNCTYPE(\n            None, ctypes.c_int, NDArrayHandle, NDArrayHandle, ctypes.c_void_p)\n        self._updater_func = _updater_proto(_updater_wrapper(updater))\n        # set updater with str keys\n        _str_updater_proto = ctypes.CFUNCTYPE(\n            None, ctypes.c_char_p, NDArrayHandle, NDArrayHandle, ctypes.c_void_p)\n        self._str_updater_func = _str_updater_proto(_updater_wrapper(updater))\n        check_call(_LIB.MXKVStoreSetUpdaterEx(self.handle, self._updater_func,\n                                              self._str_updater_func, None))", "response": "Sets a push updater into the store."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsends a command to all server nodes.", "response": "def _send_command_to_servers(self, head, body):\n        \"\"\"Sends a command to all server nodes.\n\n        Sending command to a server node will cause that server node to invoke\n        ``KVStoreServer.controller`` to execute the command.\n\n        This function returns after the command has been executed on all server\n        nodes.\n\n        Parameters\n        ----------\n        head : int\n            the head of the command.\n        body : str\n            the body of the command.\n        \"\"\"\n        check_call(_LIB.MXKVStoreSendCommmandToServers(\n            self.handle, mx_uint(head), c_str(body)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a module to the chain.", "response": "def add(self, module, **kwargs):\n        \"\"\"Add a module to the chain.\n\n        Parameters\n        ----------\n        module : BaseModule\n            The new module to add.\n        kwargs : ``**keywords``\n            All the keyword arguments are saved as meta information\n            for the added module. The currently known meta includes\n\n            - `take_labels`: indicating whether the module expect to\n                take labels when doing computation. Note any module in\n                the chain can take labels (not necessarily only the top\n                most one), and they all take the same labels passed\n                from the original data batch for the `SequentialModule`.\n\n\n        Returns\n        -------\n        self\n            This function returns `self` to allow us to easily chain a\n            series of `add` calls.\n        Examples\n        --------\n        >>> # An example of addinging two modules to a chain.\n        >>> seq_mod = mx.mod.SequentialModule()\n        >>> seq_mod.add(mod1)\n        >>> seq_mod.add(mod2)\n\n        \"\"\"\n        self._modules.append(module)\n\n        # a sanity check to avoid typo\n        for key in kwargs:\n            assert key in self._meta_keys, ('Unknown meta \"%s\", a typo?' % key)\n\n        self._metas.append(kwargs)\n\n        # after adding new modules, we are reset back to raw states, needs\n        # to bind, init_params, etc.\n        self.binded = False\n        self.params_initialized = False\n        self.optimizer_initialized = False\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the current parameters.", "response": "def get_params(self):\n        \"\"\"Gets current parameters.\n\n        Returns\n        -------\n        (arg_params, aux_params)\n            A pair of dictionaries each mapping parameter names to NDArray values. This\n            is a merged dictionary of all the parameters in the modules.\n        \"\"\"\n        assert self.binded and self.params_initialized\n\n        arg_params = dict()\n        aux_params = dict()\n\n        for module in self._modules:\n            arg, aux = module.get_params()\n            arg_params.update(arg)\n            aux_params.update(aux)\n\n        return (arg_params, aux_params)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninitialize the parameters of the modules and their auxiliary states.", "response": "def init_params(self, initializer=Uniform(0.01), arg_params=None, aux_params=None,\n                    allow_missing=False, force_init=False, allow_extra=False):\n        \"\"\"Initializes parameters.\n\n        Parameters\n        ----------\n        initializer : Initializer\n        arg_params : dict\n            Default ``None``. Existing parameters. This has higher priority\n            than `initializer`.\n        aux_params : dict\n            Default ``None``. Existing auxiliary states. This has higher priority\n            than `initializer`.\n        allow_missing : bool\n            Allow missing values in `arg_params` and `aux_params` (if not ``None``).\n            In this case, missing values will be filled with `initializer`.\n        force_init : bool\n            Default ``False``.\n        allow_extra : boolean, optional\n            Whether allow extra parameters that are not needed by symbol.\n            If this is True, no error will be thrown when arg_params or aux_params\n            contain extra parameters that is not needed by the executor.\n        \"\"\"\n        if self.params_initialized and not force_init:\n            return\n        assert self.binded, 'call bind before initializing the parameters'\n\n        for module in self._modules:\n            module.init_params(initializer=initializer, arg_params=arg_params,\n                               aux_params=aux_params, allow_missing=allow_missing,\n                               force_init=force_init, allow_extra=allow_extra)\n\n        # make sure we do not have duplicated parameter names\n        def _check_name(known_names, new_names, modules, i):\n            \"\"\"Internal function to help checking duplicated names.\"\"\"\n            for name in new_names:\n                assert not name in known_names, \"Duplicated parameter names: \" + \\\n                    ('name \"%s\" in layer %d (%s) is already ' % (name, i, type(modules[i]))) + \\\n                    ('used in layer %d (%s).' % (known_names[name],\n                                                 type(modules[known_names[name]])))\n                known_names[name] = i\n\n        arg_names = dict()\n        aux_names = dict()\n        for i_layer, module in enumerate(self._modules):\n            arg_params, aux_params = module.get_params()\n            _check_name(arg_names, arg_params.keys(), self._modules, i_layer)\n            _check_name(aux_names, aux_params.keys(), self._modules, i_layer)\n\n        self.params_initialized = True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef bind(self, data_shapes, label_shapes=None, for_training=True,\n             inputs_need_grad=False, force_rebind=False, shared_module=None,\n             grad_req='write'):\n        \"\"\"Binds the symbols to construct executors. This is necessary before one\n        can perform computation with the module.\n\n        Parameters\n        ----------\n        data_shapes : list of (str, tuple)\n            Typically is `data_iter.provide_data`.\n        label_shapes : list of (str, tuple)\n            Typically is `data_iter.provide_label`.\n        for_training : bool\n            Default is ``True``. Whether the executors should be bind for training.\n        inputs_need_grad : bool\n            Default is ``False``. Whether the gradients to the input data need to be computed.\n            Typically this is not needed. But this might be needed when implementing composition\n            of modules.\n        force_rebind : bool\n            Default is ``False``. This function does nothing if the executors are already\n            bound. But with this ``True``, the executors will be forced to rebind.\n        shared_module : Module\n            Default is ``None``. Currently shared module is not supported for `SequentialModule`.\n        grad_req : str, list of str, dict of str to str\n            Requirement for gradient accumulation. Can be 'write', 'add', or 'null'\n            (default to 'write').\n            Can be specified globally (str) or for each argument (list, dict).\n        \"\"\"\n        if self.binded and not force_rebind:\n            self.logger.warning('Already bound, ignoring bind()')\n            return\n\n        if inputs_need_grad:\n            assert for_training is True\n        assert shared_module is None, 'Shared module is not supported'\n        assert len(self._modules) > 0, 'Attempting to bind an empty SequentialModule'\n\n        self.binded = True\n\n        # the same label shapes are used for all chained modules\n        self._label_shapes = label_shapes\n\n        my_data_shapes = data_shapes\n        anybody_ever_needs_label = False\n        for i_layer, module in enumerate(self._modules):\n            meta = self._metas[i_layer]\n            if SequentialModule.META_TAKE_LABELS in meta and \\\n                    meta[SequentialModule.META_TAKE_LABELS]:\n                my_label_shapes = label_shapes\n                anybody_ever_needs_label = True\n            else:\n                my_label_shapes = None\n\n            my_inputs_need_grad = bool(inputs_need_grad or\n                                       (for_training and i_layer > 0))\n\n            if meta.get(SequentialModule.META_AUTO_WIRING, False):\n                data_names = module.data_names\n                assert len(data_names) == len(my_data_shapes)\n                my_data_shapes = [(new_name, shape) for (new_name, (_, shape))\n                                  in zip(data_names, my_data_shapes)]\n\n            module.bind(data_shapes=my_data_shapes, label_shapes=my_label_shapes,\n                        for_training=for_training, inputs_need_grad=my_inputs_need_grad,\n                        force_rebind=force_rebind, shared_module=None, grad_req=grad_req)\n\n            # the output of the previous module is the data of the next module\n            my_data_shapes = module.output_shapes\n\n        if not anybody_ever_needs_label:\n            # then I do not need label either\n            self._label_shapes = None", "response": "Binds the executors to construct executors."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef init_optimizer(self, kvstore='local', optimizer='sgd',\n                       optimizer_params=(('learning_rate', 0.01),),\n                       force_init=False):\n        \"\"\"Installs and initializes optimizers.\n\n        Parameters\n        ----------\n        kvstore : str or KVStore\n            Default `'local'`.\n        optimizer : str or Optimizer\n            Default `'sgd'`\n        optimizer_params : dict\n            Default ``(('learning_rate', 0.01),)``. The default value is not a dictionary,\n            just to avoid pylint warning of dangerous default values.\n        force_init : bool\n            Default ``False``, indicating whether we should force re-initializing the\n            optimizer in the case an optimizer is already installed.\n        \"\"\"\n        assert self.binded and self.params_initialized\n        if self.optimizer_initialized and not force_init:\n            self.logger.warning('optimizer already initialized, ignoring.')\n            return\n\n        for module in self._modules:\n            module.init_optimizer(kvstore=kvstore, optimizer=optimizer,\n                                  optimizer_params=optimizer_params, force_init=force_init)\n\n        self.optimizer_initialized = True", "response": "Installs and initializes optimizers."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef forward(self, data_batch, is_train=None):\n        assert self.binded and self.params_initialized\n\n        # make a shallow copy, just to maintain necessary properties (if any) like\n        # bucket_key, pad, etc.\n        data_batch = copy.copy(data_batch)\n\n        for i_layer, module in enumerate(self._modules):\n            module.forward(data_batch, is_train=is_train)\n\n            if i_layer+1 == len(self._modules):\n                # the last layer, do not need to do the followings\n                break\n\n            data_batch.data = module.get_outputs()\n            if hasattr(data_batch, 'provide_data'):\n                # need to update this, in case the internal module is using bucketing\n                # or whatever\n                data_names = [x[0] for x in module.output_shapes]\n                assert len(data_names) == len(data_batch.data)\n                data_batch.provide_data = [(name, x.shape) for name, x in\n                                           zip(data_names, data_batch.data)]", "response": "Forward computation.\n\n        Parameters\n        ----------\n        data_batch : DataBatch\n        is_train : bool\n            Default is ``None``, in which case `is_train` is take as ``self.for_training``."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update(self):\n        assert self.binded and self.params_initialized and self.optimizer_initialized\n\n        for module in self._modules:\n            module.update()", "response": "Updates the parameters according to the installed optimizer and the gradient computed\n        in the previous forward - backward cycle."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting outputs from a previous forward computation.", "response": "def get_outputs(self, merge_multi_context=True):\n        \"\"\"Gets outputs from a previous forward computation.\n\n        Parameters\n        ----------\n        merge_multi_context : bool\n            Default is ``True``. In the case when data-parallelism is used, the outputs\n            will be collected from multiple devices. A ``True`` value indicate that we\n            should merge the collected results so that they look like from a single\n            executor.\n\n        Returns\n        -------\n        list of NDArray or list of list of NDArray\n            If `merge_multi_context` is ``True``, it is like ``[out1,\n            out2]``. Otherwise, it is like ``[[out1_dev1, out1_dev2], [out2_dev1,\n            out2_dev2]]``. All the output elements are numpy arrays.\n        \"\"\"\n        assert self.binded and self.params_initialized\n        return self._modules[-1].get_outputs(merge_multi_context=merge_multi_context)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the gradients with respect to the inputs of the module.", "response": "def get_input_grads(self, merge_multi_context=True):\n        \"\"\"Gets the gradients with respect to the inputs of the module.\n\n        Parameters\n        ----------\n        merge_multi_context : bool\n            Default is ``True``. In the case when data-parallelism is used, the outputs\n            will be collected from multiple devices. A ``True`` value indicate that we\n            should merge the collected results so that they look like from a single\n            executor.\n\n        Returns\n        -------\n        list of NDArrays or list of list of NDArrays\n            If `merge_multi_context` is ``True``, it is like ``[grad1, grad2]``. Otherwise, it\n            is like ``[[grad1_dev1, grad1_dev2], [grad2_dev1, grad2_dev2]]``. All the output\n            elements are `NDArray`.\n        \"\"\"\n        assert self.binded and self.params_initialized and self.inputs_need_grad\n        return self._modules[0].get_input_grads(merge_multi_context=merge_multi_context)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nevaluate and accumulates evaluation metric on outputs of the last forward computation.", "response": "def update_metric(self, eval_metric, labels, pre_sliced=False):\n        \"\"\"Evaluates and accumulates evaluation metric on outputs of the last forward computation.\n\n        Parameters\n        ----------\n        eval_metric : EvalMetric\n        labels : list of NDArray\n            Typically ``data_batch.label``.\n        \"\"\"\n        assert self.binded and self.params_initialized\n\n        for meta, module in zip(self._metas, self._modules):\n            if SequentialModule.META_TAKE_LABELS in meta and \\\n                    meta[SequentialModule.META_TAKE_LABELS]:\n                module.update_metric(eval_metric, labels, pre_sliced)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef install_monitor(self, mon):\n        assert self.binded\n        for module in self._modules:\n            module.install_monitor(mon)", "response": "Installs monitor on all executors."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating the iterator of mnist dataset", "response": "def get_iterator(data_shape, use_caffe_data):\n    \"\"\"Generate the iterator of mnist dataset\"\"\"\n    def get_iterator_impl_mnist(args, kv):\n        \"\"\"return train and val iterators for mnist\"\"\"\n        # download data\n        get_mnist_ubyte()\n        flat = False if len(data_shape) != 1 else True\n\n        train = mx.io.MNISTIter(\n            image=\"data/train-images-idx3-ubyte\",\n            label=\"data/train-labels-idx1-ubyte\",\n            input_shape=data_shape,\n            batch_size=args.batch_size,\n            shuffle=True,\n            flat=flat,\n            num_parts=kv.num_workers,\n            part_index=kv.rank)\n\n        val = mx.io.MNISTIter(\n            image=\"data/t10k-images-idx3-ubyte\",\n            label=\"data/t10k-labels-idx1-ubyte\",\n            input_shape=data_shape,\n            batch_size=args.batch_size,\n            flat=flat,\n            num_parts=kv.num_workers,\n            part_index=kv.rank)\n\n        return (train, val)\n\n    def get_iterator_impl_caffe(args, kv):\n        flat = False if len(data_shape) != 1 else True\n        train = mx.io.CaffeDataIter(\n            prototxt=\n            'layer { \\\n                name: \"mnist\" \\\n                type: \"Data\" \\\n                top: \"data\" \\\n                top: \"label\" \\\n                include { \\\n                    phase: TRAIN \\\n                } \\\n                transform_param { \\\n                    scale: 0.00390625 \\\n                } \\\n                data_param { \\\n                    source: \"mnist_train_lmdb\" \\\n                    batch_size: 64 \\\n                    backend: LMDB \\\n                } \\\n            }',\n            flat=flat,\n            num_examples=60000\n            # float32 is the default, so left out here in order to illustrate\n        )\n\n        val = mx.io.CaffeDataIter(\n            prototxt=\n            'layer { \\\n                name: \"mnist\" \\\n                type: \"Data\" \\\n                top: \"data\" \\\n                top: \"label\" \\\n                include { \\\n                    phase: TEST \\\n                } \\\n                transform_param { \\\n                    scale: 0.00390625 \\\n                } \\\n                data_param { \\\n                    source: \"mnist_test_lmdb\" \\\n                    batch_size: 100 \\\n                    backend: LMDB \\\n                } \\\n            }',\n            flat=flat,\n            num_examples=10000,\n            dtype=\"float32\"  # float32 is the default\n        )\n\n        return train, val\n\n    if use_caffe_data:\n        return get_iterator_impl_caffe\n    else:\n        return get_iterator_impl_mnist"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfunctioning that runs predictions on the audio files in the prediction_dir.", "response": "def predict(prediction_dir='./Test'):\n    \"\"\"The function is used to run predictions on the audio files in the directory `pred_directory`.\n\n    Parameters\n    ----------\n    net:\n        The model that has been trained.\n    prediction_dir: string, default ./Test\n        The directory that contains the audio files on which predictions are to be made\n\n    \"\"\"\n\n    if not os.path.exists(prediction_dir):\n        warnings.warn(\"The directory on which predictions are to be made is not found!\")\n        return\n\n    if len(os.listdir(prediction_dir)) == 0:\n        warnings.warn(\"The directory on which predictions are to be made is empty! Exiting...\")\n        return\n\n    # Loading synsets\n    if not os.path.exists('./synset.txt'):\n        warnings.warn(\"The synset or labels for the dataset do not exist. Please run the training script first.\")\n        return\n\n    with open(\"./synset.txt\", \"r\") as f:\n        synset = [l.rstrip() for l in f]\n    net = get_net(len(synset))\n    print(\"Trying to load the model with the saved parameters...\")\n    if not os.path.exists(\"./net.params\"):\n        warnings.warn(\"The model does not have any saved parameters... Cannot proceed! Train the model first\")\n        return\n\n    net.load_parameters(\"./net.params\")\n    file_names = os.listdir(prediction_dir)\n    full_file_names = [os.path.join(prediction_dir, item) for item in file_names]\n    from transforms import MFCC\n    mfcc = MFCC()\n    print(\"\\nStarting predictions for audio files in \", prediction_dir, \" ....\\n\")\n    for filename in full_file_names:\n        # Argument kaiser_fast to res_type is faster than 'kaiser_best'. To reduce the load time, passing kaiser_fast.\n        X1, _ = librosa.load(filename, res_type='kaiser_fast')\n        transformed_test_data = mfcc(mx.nd.array(X1))\n        output = net(transformed_test_data.reshape((1, -1)))\n        prediction = nd.argmax(output, axis=1)\n        print(filename, \" -> \", synset[(int)(prediction.asscalar())])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _proc_loop(proc_id, alive, queue, fn):\n        print(\"proc {} started\".format(proc_id))\n        try:\n            while alive.value:\n                data = fn()\n                put_success = False\n                while alive.value and not put_success:\n                    try:\n                        queue.put(data, timeout=0.5)\n                        put_success = True\n                    except QFullExcept:\n                        # print(\"Queue Full\")\n                        pass\n        except KeyboardInterrupt:\n            print(\"W: interrupt received, stopping process {} ...\".format(proc_id))\n        print(\"Closing process {}\".format(proc_id))\n        queue.close()", "response": "This function is a thread loop that generates data from the process."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstart processes if not already started", "response": "def _init_proc(self):\n        \"\"\"Start processes if not already started\"\"\"\n        if not self.proc:\n            self.proc = [\n                mp.Process(target=self._proc_loop, args=(i, self.alive, self.queue, self.fn))\n                for i in range(self.num_proc)\n            ]\n            self.alive.value = True\n            for p in self.proc:\n                p.start()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef reset(self):\n        self.alive.value = False\n        qsize = 0\n        try:\n            while True:\n                self.queue.get(timeout=0.1)\n                qsize += 1\n        except QEmptyExcept:\n            pass\n        print(\"Queue size on reset: {}\".format(qsize))\n        for i, p in enumerate(self.proc):\n            p.join()\n        self.proc.clear()", "response": "Resets the generator by stopping all processes"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef with_metaclass(meta, *bases):\n    # This requires a bit of explanation: the basic idea is to make a dummy\n    # metaclass for one level of class instantiation that replaces itself with\n    # the actual metaclass.\n    class metaclass(type):\n\n        def __new__(cls, name, this_bases, d):\n            return meta(name, bases, d)\n\n        @classmethod\n        def __prepare__(cls, name, this_bases):\n            return meta.__prepare__(name, bases)\n    return type.__new__(metaclass, 'temporary_class', (), {})", "response": "Create a base class with a metaclass."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _load_lib():\n    lib_path = libinfo.find_lib_path()\n    lib = ctypes.CDLL(lib_path[0], ctypes.RTLD_LOCAL)\n    # DMatrix functions\n    lib.MXGetLastError.restype = ctypes.c_char_p\n    return lib", "response": "Load library by searching possible path."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef c_array(ctype, values):\n    out = (ctype * len(values))()\n    out[:] = values\n    return out", "response": "Create ctypes array from a Python array."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate ctypes const void ** from a list of MXNet objects with handles.", "response": "def c_handle_array(objs):\n    \"\"\"Create ctypes const void ** from a list of MXNet objects with handles.\n\n    Parameters\n    ----------\n    objs : list of NDArray/Symbol.\n        MXNet objects.\n\n    Returns\n    -------\n    (ctypes.c_void_p * len(objs))\n        A void ** pointer that can be passed to C API.\n    \"\"\"\n    arr = (ctypes.c_void_p * len(objs))()\n    arr[:] = [o.handle for o in objs]\n    return arr"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a ctypes pointer to a numpy array.", "response": "def ctypes2numpy_shared(cptr, shape):\n    \"\"\"Convert a ctypes pointer to a numpy array.\n\n    The resulting NumPy array shares the memory with the pointer.\n\n    Parameters\n    ----------\n    cptr : ctypes.POINTER(mx_float)\n        pointer to the memory region\n\n    shape : tuple\n        Shape of target `NDArray`.\n\n    Returns\n    -------\n    out : numpy_array\n        A numpy array : numpy array.\n    \"\"\"\n    if not isinstance(cptr, ctypes.POINTER(mx_float)):\n        raise RuntimeError('expected float pointer')\n    size = 1\n    for s in shape:\n        size *= s\n    dbuffer = (mx_float * size).from_address(ctypes.addressof(cptr.contents))\n    return _np.frombuffer(dbuffer, dtype=_np.float32).reshape(shape)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build_param_doc(arg_names, arg_types, arg_descs, remove_dup=True):\n    param_keys = set()\n    param_str = []\n    for key, type_info, desc in zip(arg_names, arg_types, arg_descs):\n        if key in param_keys and remove_dup:\n            continue\n        if key == 'num_args':\n            continue\n        param_keys.add(key)\n        ret = '%s : %s' % (key, type_info)\n        if len(desc) != 0:\n            ret += '\\n    ' + desc\n        param_str.append(ret)\n    doc_str = ('Parameters\\n' +\n               '----------\\n' +\n               '%s\\n')\n    doc_str = doc_str % ('\\n'.join(param_str))\n    return doc_str", "response": "Build a string that contains the parameter documentation for the given arguments."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_fileline_to_docstring(module, incursive=True):\n\n    def _add_fileline(obj):\n        \"\"\"Add fileinto to a object.\n        \"\"\"\n        if obj.__doc__ is None or 'From:' in obj.__doc__:\n            return\n        fname = inspect.getsourcefile(obj)\n        if fname is None:\n            return\n        try:\n            line = inspect.getsourcelines(obj)[-1]\n        except IOError:\n            return\n        obj.__doc__ += '\\n\\nFrom:%s:%d' % (fname, line)\n\n    if isinstance(module, str):\n        module = sys.modules[module]\n    for _, obj in inspect.getmembers(module):\n        if inspect.isbuiltin(obj):\n            continue\n        if inspect.isfunction(obj):\n            _add_fileline(obj)\n        if inspect.ismethod(obj):\n            _add_fileline(obj.__func__)\n        if inspect.isclass(obj) and incursive:\n            add_fileline_to_docstring(obj, False)", "response": "Adds a function to a module s docstring."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninitialize the op module with the given name.", "response": "def _init_op_module(root_namespace, module_name, make_op_func):\n    \"\"\"\n    Registers op functions created by `make_op_func` under\n    `root_namespace.module_name.[submodule_name]`,\n    where `submodule_name` is one of `_OP_SUBMODULE_NAME_LIST`.\n\n    Parameters\n    ----------\n    root_namespace : str\n        Top level module name, `mxnet` in the current cases.\n    module_name : str\n        Second level module name, `ndarray` and `symbol` in the current cases.\n    make_op_func : function\n        Function for creating op functions for `ndarray` and `symbol` modules.\n    \"\"\"\n    plist = ctypes.POINTER(ctypes.c_char_p)()\n    size = ctypes.c_uint()\n\n    check_call(_LIB.MXListAllOpNames(ctypes.byref(size),\n                                     ctypes.byref(plist)))\n    op_names = []\n    for i in range(size.value):\n        op_names.append(py_str(plist[i]))\n\n    module_op = sys.modules[\"%s.%s.op\" % (root_namespace, module_name)]\n    module_internal = sys.modules[\"%s.%s._internal\" % (root_namespace, module_name)]\n    # contrib module in the old format (deprecated)\n    # kept here for backward compatibility\n    # use mx.nd.contrib or mx.sym.contrib from now on\n    contrib_module_name_old = \"%s.contrib.%s\" % (root_namespace, module_name)\n    contrib_module_old = sys.modules[contrib_module_name_old]\n    submodule_dict = {}\n    for op_name_prefix in _OP_NAME_PREFIX_LIST:\n        submodule_dict[op_name_prefix] =\\\n            sys.modules[\"%s.%s.%s\" % (root_namespace, module_name, op_name_prefix[1:-1])]\n    for name in op_names:\n        hdl = OpHandle()\n        check_call(_LIB.NNGetOpHandle(c_str(name), ctypes.byref(hdl)))\n        op_name_prefix = _get_op_name_prefix(name)\n        module_name_local = module_name\n        if len(op_name_prefix) > 0:\n            if op_name_prefix != '_random_' or name.endswith('_like'):\n                func_name = name[len(op_name_prefix):]\n                cur_module = submodule_dict[op_name_prefix]\n                module_name_local = \"%s.%s.%s\" % (root_namespace, module_name, op_name_prefix[1:-1])\n            else:\n                func_name = name\n                cur_module = module_internal\n        elif name.startswith('_'):\n            func_name = name\n            cur_module = module_internal\n        else:\n            func_name = name\n            cur_module = module_op\n\n        function = make_op_func(hdl, name, func_name)\n        function.__module__ = module_name_local\n        setattr(cur_module, function.__name__, function)\n        cur_module.__all__.append(function.__name__)\n\n        if op_name_prefix == '_contrib_':\n            hdl = OpHandle()\n            check_call(_LIB.NNGetOpHandle(c_str(name), ctypes.byref(hdl)))\n            func_name = name[len(op_name_prefix):]\n\n            function = make_op_func(hdl, name, func_name)\n            function.__module__ = contrib_module_name_old\n            setattr(contrib_module_old, function.__name__, function)\n            contrib_module_old.__all__.append(function.__name__)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _generate_op_module_signature(root_namespace, module_name, op_code_gen_func):\n    def get_module_file(module_name):\n        \"\"\"Return the generated module file based on module name.\"\"\"\n        path = os.path.dirname(__file__)\n        module_path = module_name.split('.')\n        module_path[-1] = 'gen_' + module_path[-1]\n        file_name = os.path.join(path, '..', *module_path) + '.py'\n        module_file = open(file_name, 'w')\n        dependencies = {'symbol': ['from ._internal import SymbolBase',\n                                   'from ..base import _Null'],\n                        'ndarray': ['from ._internal import NDArrayBase',\n                                    'from ..base import _Null']}\n        module_file.write('# File content is auto-generated. Do not modify.' + os.linesep)\n        module_file.write('# pylint: skip-file' + os.linesep)\n        module_file.write(os.linesep.join(dependencies[module_name.split('.')[1]]))\n        return module_file\n\n    def write_all_str(module_file, module_all_list):\n        \"\"\"Write the proper __all__ based on available operators.\"\"\"\n        module_file.write(os.linesep)\n        module_file.write(os.linesep)\n        all_str = '__all__ = [' + ', '.join([\"'%s'\"%s for s in module_all_list]) + ']'\n        module_file.write(all_str)\n\n    plist = ctypes.POINTER(ctypes.c_char_p)()\n    size = ctypes.c_uint()\n\n    check_call(_LIB.MXListAllOpNames(ctypes.byref(size),\n                                     ctypes.byref(plist)))\n    op_names = []\n    for i in range(size.value):\n        op_names.append(py_str(plist[i]))\n\n    module_op_file = get_module_file(\"%s.%s.op\" % (root_namespace, module_name))\n    module_op_all = []\n    module_internal_file = get_module_file(\"%s.%s._internal\"%(root_namespace, module_name))\n    module_internal_all = []\n    submodule_dict = {}\n    for op_name_prefix in _OP_NAME_PREFIX_LIST:\n        submodule_dict[op_name_prefix] =\\\n            (get_module_file(\"%s.%s.%s\" % (root_namespace, module_name,\n                                           op_name_prefix[1:-1])), [])\n    for name in op_names:\n        hdl = OpHandle()\n        check_call(_LIB.NNGetOpHandle(c_str(name), ctypes.byref(hdl)))\n        op_name_prefix = _get_op_name_prefix(name)\n        if len(op_name_prefix) > 0:\n            func_name = name[len(op_name_prefix):]\n            cur_module_file, cur_module_all = submodule_dict[op_name_prefix]\n        elif name.startswith('_'):\n            func_name = name\n            cur_module_file = module_internal_file\n            cur_module_all = module_internal_all\n        else:\n            func_name = name\n            cur_module_file = module_op_file\n            cur_module_all = module_op_all\n\n        code, _ = op_code_gen_func(hdl, name, func_name, True)\n        cur_module_file.write(os.linesep)\n        cur_module_file.write(code)\n        cur_module_all.append(func_name)\n\n    for (submodule_f, submodule_all) in submodule_dict.values():\n        write_all_str(submodule_f, submodule_all)\n        submodule_f.close()\n    write_all_str(module_op_file, module_op_all)\n    module_op_file.close()\n    write_all_str(module_internal_file, module_internal_all)\n    module_internal_file.close()", "response": "Generate the signature of the op module."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nturns on NumPy - compatibility. NumPy - compatibility is turned off by default in backend.", "response": "def set_np_compat(active):\n    \"\"\"\n    Turns on/off NumPy compatibility. NumPy-compatibility is turned off by default in backend.\n\n    Parameters\n    ----------\n    active : bool\n        Indicates whether to turn on/off NumPy compatibility.\n\n    Returns\n    -------\n        A bool value indicating the previous state of NumPy compatibility.\n    \"\"\"\n    prev = ctypes.c_int()\n    check_call(_LIB.MXSetIsNumpyCompatible(ctypes.c_int(active), ctypes.byref(prev)))\n    return bool(prev.value)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_np_compat():\n    curr = ctypes.c_bool()\n    check_call(_LIB.MXIsNumpyCompatible(ctypes.byref(curr)))\n    return curr.value", "response": "Checks whether NumPy - compatibility is currently on."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef use_np_compat(func):\n    @wraps(func)\n    def _with_np_compat(*args, **kwargs):\n        with np_compat(active=True):\n            return func(*args, **kwargs)\n\n    return _with_np_compat", "response": "Wraps a function with an activated NumPy - compatibility scope."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the root relative squared error", "response": "def rse(label, pred):\n    \"\"\"computes the root relative squared error (condensed using standard deviation formula)\"\"\"\n    numerator = np.sqrt(np.mean(np.square(label - pred), axis = None))\n    denominator = np.std(label, axis = None)\n    return numerator / denominator"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rae(label, pred):\n    numerator = np.mean(np.abs(label - pred), axis=None)\n    denominator = np.mean(np.abs(label - np.mean(label, axis=None)), axis=None)\n    return numerator / denominator", "response": "computes the relative absolute error"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef corr(label, pred):\n    numerator1 = label - np.mean(label, axis=0)\n    numerator2 = pred - np.mean(pred, axis = 0)\n    numerator = np.mean(numerator1 * numerator2, axis=0)\n    denominator = np.std(label, axis=0) * np.std(pred, axis=0)\n    return np.mean(numerator / denominator)", "response": "computes the empirical correlation coefficient"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_custom_metrics():\n    _rse = mx.metric.create(rse)\n    _rae = mx.metric.create(rae)\n    _corr = mx.metric.create(corr)\n    return mx.metric.create([_rae, _rse, _corr])", "response": "Returns mxnet metric object for custom metrics"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert convolution layer parameter from Caffe to MXNet", "response": "def _convert_conv_param(param):\n    \"\"\"\n    Convert convolution layer parameter from Caffe to MXNet\n    \"\"\"\n    param_string = \"num_filter=%d\" % param.num_output\n\n    pad_w = 0\n    pad_h = 0\n    if isinstance(param.pad, int):\n        pad = param.pad\n        param_string += \", pad=(%d, %d)\" % (pad, pad)\n    else:\n        if len(param.pad) > 0:\n            pad = param.pad[0]\n            param_string += \", pad=(%d, %d)\" % (pad, pad)\n        else:\n            if isinstance(param.pad_w, int):\n                pad_w = param.pad_w\n            if isinstance(param.pad_h, int):\n                pad_h = param.pad_h\n            param_string += \", pad=(%d, %d)\" % (pad_h, pad_w)\n\n    if isinstance(param.kernel_size, int):\n        kernel_size = param.kernel_size\n        param_string += \", kernel=(%d,%d)\" % (kernel_size, kernel_size)\n    else:\n        if len(param.kernel_size) > 0:\n            kernel_size = param.kernel_size[0]\n            param_string += \", kernel=(%d,%d)\" % (kernel_size, kernel_size)\n        else:\n            assert isinstance(param.kernel_w, int)\n            kernel_w = param.kernel_w\n            assert isinstance(param.kernel_h, int)\n            kernel_h = param.kernel_h\n            param_string += \", kernel=(%d,%d)\" % (kernel_h, kernel_w)\n\n    stride = 1\n    if isinstance(param.stride, int):\n        stride = param.stride\n    else:\n        stride = 1 if len(param.stride) == 0 else param.stride[0]\n\n    param_string += \", stride=(%d,%d)\" % (stride, stride)\n\n    dilate = 1\n    if hasattr(param, 'dilation'):\n        if isinstance(param.dilation, int):\n            dilate = param.dilation\n        else:\n            dilate = 1 if len(param.dilation) == 0 else param.dilation[0]\n\n    param_string += \", no_bias=%s\" % (not param.bias_term)\n\n    # deal with dilation. Won't be in deconvolution\n    if dilate > 1:\n        param_string += \", dilate=(%d, %d)\" % (dilate, dilate)\n\n    if isinstance(param.group, int):\n        if param.group != 1:\n            param_string += \", num_group=%d\" % param.group\n\n    return param_string"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _convert_pooling_param(param):\n    param_string = \"pooling_convention='full', \"\n    if param.global_pooling:\n        param_string += \"global_pool=True, kernel=(1,1)\"\n    else:\n        param_string += \"pad=(%d,%d), kernel=(%d,%d), stride=(%d,%d)\" % (\n            param.pad, param.pad, param.kernel_size, param.kernel_size,\n            param.stride, param.stride)\n    if param.pool == 0:\n        param_string += \", pool_type='max'\"\n    elif param.pool == 1:\n        param_string += \", pool_type='avg'\"\n    else:\n        raise ValueError(\"Unknown Pooling Method!\")\n    return param_string", "response": "Convert the pooling layer parameter into a string."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _parse_proto(prototxt_fname):\n    proto = caffe_parser.read_prototxt(prototxt_fname)\n\n    # process data layer\n    input_name, input_dim, layers = _get_input(proto)\n    # only support single input, so always use `data` as the input data\n    mapping = {input_name: 'data'}\n    need_flatten = {input_name: False}\n    symbol_string = \"import mxnet as mx\\ndata = mx.symbol.Variable(name='data')\\n\"\n\n    flatten_count = 0\n    output_name = \"\"\n    prev_name = None\n\n    # convert reset layers one by one\n    for i, layer in enumerate(layers):\n        type_string = ''\n        param_string = ''\n        skip_layer = False\n        bottom_order = []\n        name = re.sub('[-/]', '_', layer.name)\n        if layer.type == 'Convolution' or layer.type == 4:\n            type_string = 'mx.symbol.Convolution'\n            param_string = _convert_conv_param(layer.convolution_param)\n            need_flatten[name] = True\n        if layer.type == 'Deconvolution' or layer.type == 39:\n            type_string = 'mx.symbol.Deconvolution'\n            param_string = _convert_conv_param(layer.convolution_param)\n            need_flatten[name] = True\n        if layer.type == 'Pooling' or layer.type == 17:\n            type_string = 'mx.symbol.Pooling'\n            param_string = _convert_pooling_param(layer.pooling_param)\n            need_flatten[name] = True\n        if layer.type == 'ReLU' or layer.type == 18:\n            type_string = 'mx.symbol.Activation'\n            param_string = \"act_type='relu'\"\n            need_flatten[name] = need_flatten[mapping[layer.bottom[0]]]\n        if layer.type == 'TanH' or layer.type == 23:\n            type_string = 'mx.symbol.Activation'\n            param_string = \"act_type='tanh'\"\n            need_flatten[name] = need_flatten[mapping[layer.bottom[0]]]\n        if layer.type == 'Sigmoid' or layer.type == 19:\n            type_string = 'mx.symbol.Activation'\n            param_string = \"act_type='sigmoid'\"\n            need_flatten[name] = need_flatten[mapping[layer.bottom[0]]]\n        if layer.type == 'LRN' or layer.type == 15:\n            type_string = 'mx.symbol.LRN'\n            param = layer.lrn_param\n            param_string = \"alpha=%f, beta=%f, knorm=%f, nsize=%d\" % (\n                param.alpha, param.beta, param.k, param.local_size)\n            need_flatten[name] = True\n        if layer.type == 'InnerProduct' or layer.type == 14:\n            type_string = 'mx.symbol.FullyConnected'\n            param = layer.inner_product_param\n            param_string = \"num_hidden=%d, no_bias=%s\" % (\n                param.num_output, not param.bias_term)\n            need_flatten[name] = False\n        if layer.type == 'Dropout' or layer.type == 6:\n            type_string = 'mx.symbol.Dropout'\n            param = layer.dropout_param\n            param_string = \"p=%f\" % param.dropout_ratio\n            need_flatten[name] = need_flatten[mapping[layer.bottom[0]]]\n        if layer.type == 'Softmax' or layer.type == 20:\n            if layer.softmax_param.axis == 2:\n                symbol_string += \"%s = mx.symbol.transpose(%s, axes=(0,2,1))\\n\" %\\\n                    (mapping[layer.bottom[0]], mapping[layer.bottom[0]])\n                type_string = 'mx.symbol.SoftmaxActivation'\n                param_string = \"mode='channel'\"\n                need_flatten[name] = False\n            else:\n                type_string = 'mx.symbol.SoftmaxOutput'\n        if layer.type == 'Flatten' or layer.type == 8:\n            if 'softmax' in layer.bottom[0]:\n                prev_name = re.sub('[-/]', '_', layers[i-1].name)\n                skip_layer = True\n            else:\n                type_string = 'mx.symbol.Flatten'\n            need_flatten[name] = False\n        if layer.type == 'Split' or layer.type == 22:\n            type_string = 'split'  # will process later\n        if layer.type == 'Concat' or layer.type == 3:\n            type_string = 'mx.symbol.Concat'\n            need_flatten[name] = True\n        if layer.type == 'Crop':\n            type_string = 'mx.symbol.Crop'\n            need_flatten[name] = True\n            param_string = 'center_crop=True'\n        if layer.type == 'BatchNorm':\n            type_string = 'mx.symbol.BatchNorm'\n            param = layer.batch_norm_param\n            # CuDNN requires eps to be greater than 1e-05\n            # We compensate for this change in convert_model\n            epsilon = param.eps\n            if (epsilon <= 1e-05):\n                epsilon = 1e-04\n            # if next layer is scale, don't fix gamma\n            fix_gamma = layers[i+1].type != 'Scale'\n            param_string = 'use_global_stats=%s, fix_gamma=%s, eps=%f' % (\n                param.use_global_stats, fix_gamma, epsilon)\n            need_flatten[name] = need_flatten[mapping[layer.bottom[0]]]\n        if layer.type == 'Scale':\n            assert layers[i-1].type == 'BatchNorm'\n            need_flatten[name] = need_flatten[mapping[layer.bottom[0]]]\n            skip_layer = True\n            prev_name = re.sub('[-/]', '_', layers[i-1].name)\n        if layer.type == 'PReLU':\n            type_string = 'mx.symbol.LeakyReLU'\n            param = layer.prelu_param\n            param_string = \"act_type='prelu', slope=%f\" % param.filler.value\n            need_flatten[name] = need_flatten[mapping[layer.bottom[0]]]\n        if layer.type == 'Eltwise':\n            type_string = 'mx.symbol.broadcast_add'\n            param_string = \"\"\n            need_flatten[name] = False\n        if layer.type == 'Reshape':\n            type_string = 'mx.symbol.Reshape'\n            param = layer.reshape_param\n            param_string = 'shape=(' + ','.join([str(x) for x in list(param.shape.dim)]) + ')'\n            need_flatten[name] = True\n        if layer.type == 'AbsVal':\n            type_string = 'mx.symbol.abs'\n            need_flatten[name] = need_flatten[mapping[layer.bottom[0]]]\n        if layer.type == 'Normalize':\n            bottom = re.sub('[-/]', '_', layer.bottom[0])\n            conv_layer = _find_layer(layers, bottom)\n            assert conv_layer is not None\n            param = layer.norm_param\n            assert not param.across_spatial and not param.channel_shared\n            assert param.scale_filler.type == 'constant'\n            if conv_layer.type == 'Convolution':\n                scale_name = \"%s_scale\" % name\n                symbol_string += \"%s=mx.sym.Variable(name='%s', shape=(1, %d, 1, 1), init=mx.init.Constant(%f))\\n\" % \\\n                    (scale_name, scale_name, conv_layer.convolution_param.num_output,\n                    param.scale_filler.value)\n                symbol_string += \"%s=mx.symbol.L2Normalization(name='%s', data=%s, mode='channel')\\n\" %\\\n                    (name, name, mapping[layer.bottom[0]])\n                symbol_string += \"%s=mx.symbol.broadcast_mul(lhs=%s, rhs=%s)\\n\" %\\\n                    (name, scale_name, name)\n                type_string = 'split'\n                need_flatten[name] = True\n            else:\n                raise ValueError('Unknown/Invalid normalize layer!')\n        if layer.type == 'Permute':\n            type_string = 'mx.symbol.transpose'\n            param_string = \"axes=(%s)\" % (','.join([str(x) for x in layer.permute_param.order]))\n            need_flatten[name] = True\n            from_name = ''\n        if layer.type == 'PriorBox':\n            param = layer.prior_box_param\n            if layer.bottom[0] == 'data':\n                bottom_order = [1]\n            else:\n                bottom_order = [0]\n            try:\n                import math\n                min_size = param.min_size[0] / input_dim[2]\n                max_size = math.sqrt(param.min_size[0] * param.max_size[0]) / input_dim[2]\n                sizes = '(%f, %f)' %(min_size, max_size)\n            except AttributeError:\n                min_size = param.min_size[0] / input_dim[2]\n                sizes = '(%f)' %(min_size)\n            ars = list(param.aspect_ratio)\n            ratios = [1.]\n            for ar in ars:\n                ratios.append(ar)\n                if param.flip:\n                    ratios.append(1. / ar)\n            ratios_string = '(' + ','.join(str(x) for x in ratios) + ')'\n            clip = param.clip\n            if (param.step_h > 0 or param.step_w > 0):\n                step_h = param.step_h\n                step_w = param.step_w\n            elif param.step > 0:\n                step_h = param.step\n                step_w = param.step\n            else:\n                step_h = -1\n                step_w = -1\n            finput_dimh = float(input_dim[2])\n            finput_dimw = float(input_dim[3])\n            step = '(%f, %f)' % (step_h / finput_dimh, step_w / finput_dimw)\n            assert param.offset == 0.5, \"currently only support offset = 0.5\"\n            symbol_string += '%s = mx.contrib.symbol.MultiBoxPrior(%s, sizes=%s, ratios=%s, clip=%s, steps=%s, name=\"%s\")\\n' % \\\n                (name, mapping[layer.bottom[0]], sizes, ratios_string, clip, step, name)\n            symbol_string += '%s = mx.symbol.Flatten(data=%s)\\n' % (name, name)\n            type_string = 'split'\n            need_flatten[name] = False\n        if layer.type == 'DetectionOutput':\n            bottom_order = [1, 0, 2]\n            param = layer.detection_output_param\n            assert param.share_location == True\n            assert param.background_label_id == 0\n            nms_param = param.nms_param\n            type_string = 'mx.contrib.symbol.MultiBoxDetection'\n            param_string = \"nms_threshold=%f, nms_topk=%d, clip=False\" % \\\n                (nms_param.nms_threshold, nms_param.top_k)\n        if skip_layer:\n            assert len(layer.bottom) == 1\n            symbol_string += \"%s = %s\\n\" % (name, prev_name)\n        elif type_string == '':\n            raise ValueError('Unknown layer %s!' % layer.type)\n        elif type_string != 'split':\n            bottom = layer.bottom\n            if param_string != \"\":\n                param_string = \", \" + param_string\n            if len(bottom) == 1:\n                # print(need_flatten)\n                if need_flatten[mapping[bottom[0]]] and type_string == 'mx.symbol.FullyConnected':\n                    flatten_name = \"flatten_%d\" % flatten_count\n                    symbol_string += \"%s=mx.symbol.Flatten(name='%s', data=%s)\\n\" % (\n                        flatten_name, flatten_name, mapping[bottom[0]])\n                    flatten_count += 1\n                    need_flatten[flatten_name] = False\n                    bottom[0] = flatten_name\n                    mapping[bottom[0]] = bottom[0]\n                symbol_string += \"%s = %s(name='%s', data=%s %s)\\n\" % (\n                    name, type_string, name, mapping[bottom[0]], param_string)\n            else:\n                if not bottom_order:\n                    bottom_order = range(len(bottom))\n                symbol_string += \"%s = %s(name='%s', *[%s] %s)\\n\" % \\\n                                 (name, type_string, name, ','.join([mapping[bottom[x]] for x in bottom_order]), param_string)\n                if layer.type == 'Concat' and layer.concat_param.axis == 2:\n                    symbol_string += \"%s = mx.symbol.Reshape(data=%s, shape=(0, -1, 4), name='%s')\\n\" %\\\n                        (name, name, name)\n        for j in range(len(layer.top)):\n            mapping[layer.top[j]] = name\n        output_name = name\n    return symbol_string, output_name, input_dim", "response": "Parse Caffe prototxt into symbol string"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef convert_symbol(prototxt_fname):\n    sym, output_name, input_dim = _parse_proto(prototxt_fname)\n    exec(sym)                   # pylint: disable=exec-used\n    _locals = locals()\n    exec(\"ret = \" + output_name, globals(), _locals)  # pylint: disable=exec-used\n    ret = _locals['ret']\n    return ret, input_dim", "response": "Convert caffe model definition into a sequence of Symbols."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntrain an episode for each environment.", "response": "def train_episode(agent, envs, preprocessors, t_max, render):\n    \"\"\"Complete an episode's worth of training for each environment.\"\"\"\n    num_envs = len(envs)\n\n    # Buffers to hold trajectories, e.g. `env_xs[i]` will hold the observations\n    # for environment `i`.\n    env_xs, env_as = _2d_list(num_envs), _2d_list(num_envs)\n    env_rs, env_vs = _2d_list(num_envs), _2d_list(num_envs)\n    episode_rs = np.zeros(num_envs, dtype=np.float)\n\n    for p in preprocessors:\n        p.reset()\n\n    observations = [p.preprocess(e.reset())\n                    for p, e in zip(preprocessors, envs)]\n\n    done = np.array([False for _ in range(num_envs)])\n    all_done = False\n    t = 1\n\n    while not all_done:\n        if render:\n            envs[0].render()\n\n        # NOTE(reed): Reshape to set the data shape.\n        agent.model.reshape([('data', (num_envs, preprocessors[0].obs_size))])\n        step_xs = np.vstack([o.ravel() for o in observations])\n\n        # Get actions and values for all environments in a single forward pass.\n        step_xs_nd = mx.nd.array(step_xs, ctx=agent.ctx)\n        data_batch = mx.io.DataBatch(data=[step_xs_nd], label=None)\n        agent.model.forward(data_batch, is_train=False)\n        _, step_vs, _, step_ps = agent.model.get_outputs()\n\n        step_ps = step_ps.asnumpy()\n        step_vs = step_vs.asnumpy()\n        step_as = agent.act(step_ps)\n\n        # Step each environment whose episode has not completed.\n        for i, env in enumerate(envs):\n            if not done[i]:\n                obs, r, done[i], _ = env.step(step_as[i])\n\n                # Record the observation, action, value, and reward in the\n                # buffers.\n                env_xs[i].append(step_xs[i].ravel())\n                env_as[i].append(step_as[i])\n                env_vs[i].append(step_vs[i][0])\n                env_rs[i].append(r)\n                episode_rs[i] += r\n\n                # Add 0 as the state value when done.\n                if done[i]:\n                    env_vs[i].append(0.0)\n                else:\n                    observations[i] = preprocessors[i].preprocess(obs)\n\n        # Perform an update every `t_max` steps.\n        if t == t_max:\n            # If the episode has not finished, add current state's value. This\n            # will be used to 'bootstrap' the final return (see Algorithm S3\n            # in A3C paper).\n            step_xs = np.vstack([o.ravel() for o in observations])\n            step_xs_nd = mx.nd.array(step_xs, ctx=agent.ctx)\n            data_batch = mx.io.DataBatch(data=[step_xs_nd], label=None)\n            agent.model.forward(data_batch, is_train=False)\n            _, extra_vs, _, _ = agent.model.get_outputs()\n            extra_vs = extra_vs.asnumpy()\n            for i in range(num_envs):\n                if not done[i]:\n                    env_vs[i].append(extra_vs[i][0])\n\n            # Perform update and clear buffers.\n            env_xs = np.vstack(list(chain.from_iterable(env_xs)))\n            agent.train_step(env_xs, env_as, env_rs, env_vs)\n            env_xs, env_as = _2d_list(num_envs), _2d_list(num_envs)\n            env_rs, env_vs = _2d_list(num_envs), _2d_list(num_envs)\n            t = 0\n\n        all_done = np.all(done)\n        t += 1\n\n    return episode_rs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing the trained. caffemodel file and returns a list of layers", "response": "def parse_caffemodel(file_path):\n    \"\"\"\n    parses the trained .caffemodel file\n\n    filepath: /path/to/trained-model.caffemodel\n\n    returns: layers\n    \"\"\"\n    f = open(file_path, 'rb')\n    contents = f.read()\n\n    net_param = caffe_pb2.NetParameter()\n    net_param.ParseFromString(contents)\n\n    layers = find_layers(net_param)\n    return layers"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the log of the Fourier Transform of the audio clip.", "response": "def featurize(self, audio_clip, overwrite=False, save_feature_as_csvfile=False):\n        \"\"\" For a given audio clip, calculate the log of its Fourier Transform\n        Params:\n            audio_clip(str): Path to the audio clip\n        \"\"\"\n        return spectrogram_from_file(\n            audio_clip, step=self.step, window=self.window,\n            max_freq=self.max_freq, overwrite=overwrite,\n            save_feature_as_csvfile=save_feature_as_csvfile)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading metadata from the description file.", "response": "def load_metadata_from_desc_file(self, desc_file, partition='train',\n                                     max_duration=16.0,):\n        \"\"\" Read metadata from the description file\n            (possibly takes long, depending on the filesize)\n        Params:\n            desc_file (str):  Path to a JSON-line file that contains labels and\n                paths to the audio files\n            partition (str): One of 'train', 'validation' or 'test'\n            max_duration (float): In seconds, the maximum duration of\n                utterances to train or test on\n        \"\"\"\n        logger = logUtil.getlogger()\n        logger.info('Reading description file: {} for partition: {}'\n                    .format(desc_file, partition))\n        audio_paths, durations, texts = [], [], []\n        with open(desc_file) as json_line_file:\n            for line_num, json_line in enumerate(json_line_file):\n                try:\n                    spec = json.loads(json_line)\n                    if float(spec['duration']) > max_duration:\n                        continue\n                    audio_paths.append(spec['key'])\n                    durations.append(float(spec['duration']))\n                    texts.append(spec['text'])\n                except Exception as e:\n                    # Change to (KeyError, ValueError) or\n                    # (KeyError,json.decoder.JSONDecodeError), depending on\n                    # json module version\n                    logger.warn('Error reading line #{}: {}'\n                                .format(line_num, json_line))\n                    logger.warn(str(e))\n\n        if partition == 'train':\n            self.count = len(audio_paths)\n            self.train_audio_paths = audio_paths\n            self.train_durations = durations\n            self.train_texts = texts\n        elif partition == 'validation':\n            self.val_audio_paths = audio_paths\n            self.val_durations = durations\n            self.val_texts = texts\n            self.val_count = len(audio_paths)\n        elif partition == 'test':\n            self.test_audio_paths = audio_paths\n            self.test_durations = durations\n            self.test_texts = texts\n        else:\n            raise Exception(\"Invalid partition to load metadata. \"\n                            \"Must be train/validation/test\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef prepare_minibatch(self, audio_paths, texts, overwrite=False,\n                          is_bi_graphemes=False, seq_length=-1, save_feature_as_csvfile=False):\n        \"\"\" Featurize a minibatch of audio, zero pad them and return a dictionary\n        Params:\n            audio_paths (list(str)): List of paths to audio files\n            texts (list(str)): List of texts corresponding to the audio files\n        Returns:\n            dict: See below for contents\n        \"\"\"\n        assert len(audio_paths) == len(texts),\\\n            \"Inputs and outputs to the network must be of the same number\"\n        # Features is a list of (timesteps, feature_dim) arrays\n        # Calculate the features for each audio clip, as the log of the\n        # Fourier Transform of the audio\n        features = [self.featurize(a, overwrite=overwrite, save_feature_as_csvfile=save_feature_as_csvfile) for a in audio_paths]\n        input_lengths = [f.shape[0] for f in features]\n        feature_dim = features[0].shape[1]\n        mb_size = len(features)\n        # Pad all the inputs so that they are all the same length\n        if seq_length == -1:\n            x = np.zeros((mb_size, self.max_seq_length, feature_dim))\n        else:\n            x = np.zeros((mb_size, seq_length, feature_dim))\n        y = np.zeros((mb_size, self.max_label_length))\n        labelUtil = LabelUtil.getInstance()\n        label_lengths = []\n        for i in range(mb_size):\n            feat = features[i]\n            feat = self.normalize(feat)  # Center using means and std\n            x[i, :feat.shape[0], :] = feat\n            if is_bi_graphemes:\n                label = generate_bi_graphemes_label(texts[i])\n                label = labelUtil.convert_bi_graphemes_to_num(label)\n                y[i, :len(label)] = label\n            else:\n                label = labelUtil.convert_word_to_num(texts[i])\n                y[i, :len(texts[i])] = label\n            label_lengths.append(len(label))\n        return {\n            'x': x,  # (0-padded features of shape(mb_size,timesteps,feat_dim)\n            'y': y,  # list(int) Flattened labels (integer sequences)\n            'texts': texts,  # list(str) Original texts\n            'input_lengths': input_lengths,  # list(int) Length of each input\n            'label_lengths': label_lengths,  # list(int) Length of each label\n        }", "response": "Prepares a minibatch of audio files and returns a dictionary of the audio files that can be used to generate the minibatch."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sample_normalize(self, k_samples=1000, overwrite=False):\n        log = logUtil.getlogger()\n        log.info(\"Calculating mean and std from samples\")\n        # if k_samples is negative then it goes through total dataset\n        if k_samples < 0:\n            audio_paths = self.audio_paths\n\n        # using sample\n        else:\n            k_samples = min(k_samples, len(self.train_audio_paths))\n            samples = self.rng.sample(self.train_audio_paths, k_samples)\n            audio_paths = samples\n        manager = Manager()\n        return_dict = manager.dict()\n        jobs = []\n        for threadIndex in range(cpu_count()):\n            proc = Process(target=self.preprocess_sample_normalize, args=(threadIndex, audio_paths, overwrite, return_dict))\n            jobs.append(proc)\n            proc.start()\n        for proc in jobs:\n            proc.join()\n\n        feat = np.sum(np.vstack([item['feat'] for item in return_dict.values()]), axis=0)\n        count = sum([item['count'] for item in return_dict.values()])\n        feat_squared = np.sum(np.vstack([item['feat_squared'] for item in return_dict.values()]), axis=0)\n\n        self.feats_mean = feat / float(count)\n        self.feats_std = np.sqrt(feat_squared / float(count) - np.square(self.feats_mean))\n        np.savetxt(\n            generate_file_path(self.save_dir, self.model_name, 'feats_mean'), self.feats_mean)\n        np.savetxt(\n            generate_file_path(self.save_dir, self.model_name, 'feats_std'), self.feats_std)\n        log.info(\"End calculating mean and std from samples\")", "response": "This function calculates the mean and std of the features from the training set and stores them in the self. train_train_files attribute."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsaves image to file", "response": "def save_image(data, epoch, image_size, batch_size, output_dir, padding=2):\n    \"\"\" save image \"\"\"\n    data = data.asnumpy().transpose((0, 2, 3, 1))\n    datanp = np.clip(\n        (data - np.min(data))*(255.0/(np.max(data) - np.min(data))), 0, 255).astype(np.uint8)\n    x_dim = min(8, batch_size)\n    y_dim = int(math.ceil(float(batch_size) / x_dim))\n    height, width = int(image_size + padding), int(image_size + padding)\n    grid = np.zeros((height * y_dim + 1 + padding // 2, width *\n                     x_dim + 1 + padding // 2, 3), dtype=np.uint8)\n    k = 0\n    for y in range(y_dim):\n        for x in range(x_dim):\n            if k >= batch_size:\n                break\n            start_y = y * height + 1 + padding // 2\n            end_y = start_y + height - padding\n            start_x = x * width + 1 + padding // 2\n            end_x = start_x + width - padding\n            np.copyto(grid[start_y:end_y, start_x:end_x, :], datanp[k])\n            k += 1\n    imageio.imwrite(\n        '{}/fake_samples_epoch_{}.png'.format(output_dir, epoch), grid)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef list_image(root, recursive, exts):\n\n    i = 0\n    if recursive:\n        cat = {}\n        for path, dirs, files in os.walk(root, followlinks=True):\n            dirs.sort()\n            files.sort()\n            for fname in files:\n                fpath = os.path.join(path, fname)\n                suffix = os.path.splitext(fname)[1].lower()\n                if os.path.isfile(fpath) and (suffix in exts):\n                    if path not in cat:\n                        cat[path] = len(cat)\n                    yield (i, os.path.relpath(fpath, root), cat[path])\n                    i += 1\n        for k, v in sorted(cat.items(), key=lambda x: x[1]):\n            print(os.path.relpath(k, root), v)\n    else:\n        for fname in sorted(os.listdir(root)):\n            fpath = os.path.join(root, fname)\n            suffix = os.path.splitext(fname)[1].lower()\n            if os.path.isfile(fpath) and (suffix in exts):\n                yield (i, os.path.relpath(fpath, root), 0)\n                i += 1", "response": "Traverses the root of directory that contains images and generates image list iterator."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_list(args):\n    image_list = list_image(args.root, args.recursive, args.exts)\n    image_list = list(image_list)\n    if args.shuffle is True:\n        random.seed(100)\n        random.shuffle(image_list)\n    N = len(image_list)\n    chunk_size = (N + args.chunks - 1) // args.chunks\n    for i in range(args.chunks):\n        chunk = image_list[i * chunk_size:(i + 1) * chunk_size]\n        if args.chunks > 1:\n            str_chunk = '_%d' % i\n        else:\n            str_chunk = ''\n        sep = int(chunk_size * args.train_ratio)\n        sep_test = int(chunk_size * args.test_ratio)\n        if args.train_ratio == 1.0:\n            write_list(args.prefix + str_chunk + '.lst', chunk)\n        else:\n            if args.test_ratio:\n                write_list(args.prefix + str_chunk + '_test.lst', chunk[:sep_test])\n            if args.train_ratio + args.test_ratio < 1.0:\n                write_list(args.prefix + str_chunk + '_val.lst', chunk[sep_test + sep:])\n            write_list(args.prefix + str_chunk + '_train.lst', chunk[sep_test:sep_test + sep])", "response": "Generates .lst file.\n    Parameters\n    ----------\n    args: object that contains all the arguments"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread the. lst file and generates corresponding iterator.", "response": "def read_list(path_in):\n    \"\"\"Reads the .lst file and generates corresponding iterator.\n    Parameters\n    ----------\n    path_in: string\n    Returns\n    -------\n    item iterator that contains information in .lst file\n    \"\"\"\n    with open(path_in) as fin:\n        while True:\n            line = fin.readline()\n            if not line:\n                break\n            line = [i.strip() for i in line.strip().split('\\t')]\n            line_len = len(line)\n            # check the data format of .lst file\n            if line_len < 3:\n                print('lst should have at least has three parts, but only has %s parts for %s' % (line_len, line))\n                continue\n            try:\n                item = [int(line[0])] + [line[-1]] + [float(i) for i in line[1:-1]]\n            except Exception as e:\n                print('Parsing lst met error for %s, detail: %s' % (line, e))\n                continue\n            yield item"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading preprocesses packs the image and puts it back in output queue.", "response": "def image_encode(args, i, item, q_out):\n    \"\"\"Reads, preprocesses, packs the image and put it back in output queue.\n    Parameters\n    ----------\n    args: object\n    i: int\n    item: list\n    q_out: queue\n    \"\"\"\n    fullpath = os.path.join(args.root, item[1])\n\n    if len(item) > 3 and args.pack_label:\n        header = mx.recordio.IRHeader(0, item[2:], item[0], 0)\n    else:\n        header = mx.recordio.IRHeader(0, item[2], item[0], 0)\n\n    if args.pass_through:\n        try:\n            with open(fullpath, 'rb') as fin:\n                img = fin.read()\n            s = mx.recordio.pack(header, img)\n            q_out.put((i, s, item))\n        except Exception as e:\n            traceback.print_exc()\n            print('pack_img error:', item[1], e)\n            q_out.put((i, None, item))\n        return\n\n    try:\n        img = cv2.imread(fullpath, args.color)\n    except:\n        traceback.print_exc()\n        print('imread error trying to load file: %s ' % fullpath)\n        q_out.put((i, None, item))\n        return\n    if img is None:\n        print('imread read blank (None) image for file: %s' % fullpath)\n        q_out.put((i, None, item))\n        return\n    if args.center_crop:\n        if img.shape[0] > img.shape[1]:\n            margin = (img.shape[0] - img.shape[1]) // 2\n            img = img[margin:margin + img.shape[1], :]\n        else:\n            margin = (img.shape[1] - img.shape[0]) // 2\n            img = img[:, margin:margin + img.shape[0]]\n    if args.resize:\n        if img.shape[0] > img.shape[1]:\n            newsize = (args.resize, img.shape[0] * args.resize // img.shape[1])\n        else:\n            newsize = (img.shape[1] * args.resize // img.shape[0], args.resize)\n        img = cv2.resize(img, newsize)\n\n    try:\n        s = mx.recordio.pack_img(header, img, quality=args.quality, img_fmt=args.encoding)\n        q_out.put((i, s, item))\n    except Exception as e:\n        traceback.print_exc()\n        print('pack_img error on file: %s' % fullpath, e)\n        q_out.put((i, None, item))\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read_worker(args, q_in, q_out):\n    while True:\n        deq = q_in.get()\n        if deq is None:\n            break\n        i, item = deq\n        image_encode(args, i, item, q_out)", "response": "Function that will be spawned to fetch the image\n    from the input queue and put it back to the output queue."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfunction that will be spawned to fetch processed image from the output queue and write to the. rec file.", "response": "def write_worker(q_out, fname, working_dir):\n    \"\"\"Function that will be spawned to fetch processed image\n    from the output queue and write to the .rec file.\n    Parameters\n    ----------\n    q_out: queue\n    fname: string\n    working_dir: string\n    \"\"\"\n    pre_time = time.time()\n    count = 0\n    fname = os.path.basename(fname)\n    fname_rec = os.path.splitext(fname)[0] + '.rec'\n    fname_idx = os.path.splitext(fname)[0] + '.idx'\n    record = mx.recordio.MXIndexedRecordIO(os.path.join(working_dir, fname_idx),\n                                           os.path.join(working_dir, fname_rec), 'w')\n    buf = {}\n    more = True\n    while more:\n        deq = q_out.get()\n        if deq is not None:\n            i, s, item = deq\n            buf[i] = (s, item)\n        else:\n            more = False\n        while count in buf:\n            s, item = buf[count]\n            del buf[count]\n            if s is not None:\n                record.write_idx(item[0], s)\n\n            if count % 1000 == 0:\n                cur_time = time.time()\n                print('time:', cur_time - pre_time, ' count:', count)\n                pre_time = cur_time\n            count += 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndefines all the arguments.", "response": "def parse_args():\n    \"\"\"Defines all arguments.\n    Returns\n    -------\n    args object that contains all the params\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n        description='Create an image list or \\\n        make a record database by reading from an image list')\n    parser.add_argument('prefix', help='prefix of input/output lst and rec files.')\n    parser.add_argument('root', help='path to folder containing images.')\n\n    cgroup = parser.add_argument_group('Options for creating image lists')\n    cgroup.add_argument('--list', action='store_true',\n                        help='If this is set im2rec will create image list(s) by traversing root folder\\\n        and output to <prefix>.lst.\\\n        Otherwise im2rec will read <prefix>.lst and create a database at <prefix>.rec')\n    cgroup.add_argument('--exts', nargs='+', default=['.jpeg', '.jpg', '.png'],\n                        help='list of acceptable image extensions.')\n    cgroup.add_argument('--chunks', type=int, default=1, help='number of chunks.')\n    cgroup.add_argument('--train-ratio', type=float, default=1.0,\n                        help='Ratio of images to use for training.')\n    cgroup.add_argument('--test-ratio', type=float, default=0,\n                        help='Ratio of images to use for testing.')\n    cgroup.add_argument('--recursive', action='store_true',\n                        help='If true recursively walk through subdirs and assign an unique label\\\n        to images in each folder. Otherwise only include images in the root folder\\\n        and give them label 0.')\n    cgroup.add_argument('--no-shuffle', dest='shuffle', action='store_false',\n                        help='If this is passed, \\\n        im2rec will not randomize the image order in <prefix>.lst')\n    rgroup = parser.add_argument_group('Options for creating database')\n    rgroup.add_argument('--pass-through', action='store_true',\n                        help='whether to skip transformation and save image as is')\n    rgroup.add_argument('--resize', type=int, default=0,\n                        help='resize the shorter edge of image to the newsize, original images will\\\n        be packed by default.')\n    rgroup.add_argument('--center-crop', action='store_true',\n                        help='specify whether to crop the center image to make it rectangular.')\n    rgroup.add_argument('--quality', type=int, default=95,\n                        help='JPEG quality for encoding, 1-100; or PNG compression for encoding, 1-9')\n    rgroup.add_argument('--num-thread', type=int, default=1,\n                        help='number of thread to use for encoding. order of images will be different\\\n        from the input list if >1. the input list will be modified to match the\\\n        resulting order.')\n    rgroup.add_argument('--color', type=int, default=1, choices=[-1, 0, 1],\n                        help='specify the color mode of the loaded image.\\\n        1: Loads a color image. Any transparency of image will be neglected. It is the default flag.\\\n        0: Loads image in grayscale mode.\\\n        -1:Loads image as such including alpha channel.')\n    rgroup.add_argument('--encoding', type=str, default='.jpg', choices=['.jpg', '.png'],\n                        help='specify the encoding of the images.')\n    rgroup.add_argument('--pack-label', action='store_true',\n        help='Whether to also pack multi dimensional label in the record file')\n    args = parser.parse_args()\n    args.prefix = os.path.abspath(args.prefix)\n    args.root = os.path.abspath(args.root)\n    return args"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncrop and normnalize an image nd array.", "response": "def transform(data, target_wd, target_ht, is_train, box):\n    \"\"\"Crop and normnalize an image nd array.\"\"\"\n    if box is not None:\n        x, y, w, h = box\n        data = data[y:min(y+h, data.shape[0]), x:min(x+w, data.shape[1])]\n\n    # Resize to target_wd * target_ht.\n    data = mx.image.imresize(data, target_wd, target_ht)\n\n    # Normalize in the same way as the pre-trained model.\n    data = data.astype(np.float32) / 255.0\n    data = (data - mx.nd.array([0.485, 0.456, 0.406])) / mx.nd.array([0.229, 0.224, 0.225])\n\n    if is_train:\n        if random.random() < 0.5:\n            data = nd.flip(data, axis=1)\n        data, _ = mx.image.random_crop(data, (224, 224))\n    else:\n        data, _ = mx.image.center_crop(data, (224, 224))\n\n    # Transpose from (target_wd, target_ht, 3)\n    # to (3, target_wd, target_ht).\n    data = nd.transpose(data, (2, 0, 1))\n\n    # If image is greyscale, repeat 3 times to get RGB image.\n    if data.shape[0] == 1:\n        data = nd.tile(data, (3, 1, 1))\n    return data.reshape((1,) + data.shape)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cub200_iterator(data_path, batch_k, batch_size, data_shape):\n    return (CUB200Iter(data_path, batch_k, batch_size, data_shape, is_train=True),\n            CUB200Iter(data_path, batch_k, batch_size, data_shape, is_train=False))", "response": "Return training and testing iterator for the CUB200 -2011 dataset."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload and transform an image.", "response": "def get_image(self, img, is_train):\n        \"\"\"Load and transform an image.\"\"\"\n        img_arr = mx.image.imread(img)\n        img_arr = transform(img_arr, 256, 256, is_train, self.boxes[img])\n        return img_arr"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sample_train_batch(self):\n        batch = []\n        labels = []\n        num_groups = self.batch_size // self.batch_k\n\n        # For CUB200, we use the first 100 classes for training.\n        sampled_classes = np.random.choice(100, num_groups, replace=False)\n        for i in range(num_groups):\n            img_fnames = np.random.choice(self.train_image_files[sampled_classes[i]],\n                                          self.batch_k, replace=False)\n            batch += [self.get_image(img_fname, is_train=True) for img_fname in img_fnames]\n            labels += [sampled_classes[i] for _ in range(self.batch_k)]\n\n        return nd.concatenate(batch, axis=0), labels", "response": "Sample a training batch."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef feature_list():\n    lib_features_c_array = ctypes.POINTER(Feature)()\n    lib_features_size = ctypes.c_size_t()\n    check_call(_LIB.MXLibInfoFeatures(ctypes.byref(lib_features_c_array), ctypes.byref(lib_features_size)))\n    features = [lib_features_c_array[i] for i in range(lib_features_size.value)]\n    return features", "response": "Check the library for compile - time features."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_enabled(self, feature_name):\n        feature_name = feature_name.upper()\n        if feature_name not in self:\n            raise RuntimeError(\"Feature '{}' is unknown, known features are: {}\".format(\n                feature_name, list(self.keys())))\n        return self[feature_name].enabled", "response": "Check if a particular feature is enabled."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmake a directory to store all caches", "response": "def cache_path(self):\n        \"\"\"\n        make a directory to store all caches\n\n        Returns:\n        ---------\n            cache path\n        \"\"\"\n        cache_path = os.path.join(os.path.dirname(__file__), '..', 'cache')\n        if not os.path.exists(cache_path):\n            os.mkdir(cache_path)\n        return cache_path"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _load_image_set_index(self, shuffle):\n        image_set_index_file = os.path.join(self.data_path, 'ImageSets', 'Main', self.image_set + '.txt')\n        assert os.path.exists(image_set_index_file), 'Path does not exist: {}'.format(image_set_index_file)\n        with open(image_set_index_file) as f:\n            image_set_index = [x.strip() for x in f.readlines()]\n        if shuffle:\n            np.random.shuffle(image_set_index)\n        return image_set_index", "response": "Load the image set index file and return the list of images that correspond to the given image set."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind out full path of the image file given the index", "response": "def image_path_from_index(self, index):\n        \"\"\"\n        given image index, find out full path\n\n        Parameters:\n        ----------\n        index: int\n            index of a specific image\n        Returns:\n        ----------\n        full path of this image\n        \"\"\"\n        assert self.image_set_index is not None, \"Dataset not initialized\"\n        name = self.image_set_index[index]\n        image_file = os.path.join(self.data_path, 'JPEGImages', name + self.extension)\n        assert os.path.exists(image_file), 'Path does not exist: {}'.format(image_file)\n        return image_file"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _label_path_from_index(self, index):\n        label_file = os.path.join(self.data_path, 'Annotations', index + '.xml')\n        assert os.path.exists(label_file), 'Path does not exist: {}'.format(label_file)\n        return label_file", "response": "Given an index find out the full path of the annotation file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _load_image_labels(self):\n        temp = []\n\n        # load ground-truth from xml annotations\n        for idx in self.image_set_index:\n            label_file = self._label_path_from_index(idx)\n            tree = ET.parse(label_file)\n            root = tree.getroot()\n            size = root.find('size')\n            width = float(size.find('width').text)\n            height = float(size.find('height').text)\n            label = []\n\n            for obj in root.iter('object'):\n                difficult = int(obj.find('difficult').text)\n                # if not self.config['use_difficult'] and difficult == 1:\n                #     continue\n                cls_name = obj.find('name').text\n                if cls_name not in self.classes:\n                    continue\n                cls_id = self.classes.index(cls_name)\n                xml_box = obj.find('bndbox')\n                xmin = float(xml_box.find('xmin').text) / width\n                ymin = float(xml_box.find('ymin').text) / height\n                xmax = float(xml_box.find('xmax').text) / width\n                ymax = float(xml_box.find('ymax').text) / height\n                label.append([cls_id, xmin, ymin, xmax, ymax, difficult])\n            temp.append(np.array(label))\n        return temp", "response": "Load all ground - truths from xml annotations and return a list of numpy arrays."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nevaluates the given list of detections and write the results to the results folder.", "response": "def evaluate_detections(self, detections):\n        \"\"\"\n        top level evaluations\n        Parameters:\n        ----------\n        detections: list\n            result list, each entry is a matrix of detections\n        Returns:\n        ----------\n            None\n        \"\"\"\n        # make all these folders for results\n        result_dir = os.path.join(self.devkit_path, 'results')\n        if not os.path.exists(result_dir):\n            os.mkdir(result_dir)\n        year_folder = os.path.join(self.devkit_path, 'results', 'VOC' + self.year)\n        if not os.path.exists(year_folder):\n            os.mkdir(year_folder)\n        res_file_folder = os.path.join(self.devkit_path, 'results', 'VOC' + self.year, 'Main')\n        if not os.path.exists(res_file_folder):\n            os.mkdir(res_file_folder)\n\n        self.write_pascal_results(detections)\n        self.do_python_eval()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_result_file_template(self):\n        res_file_folder = os.path.join(self.devkit_path, 'results', 'VOC' + self.year, 'Main')\n        comp_id = self.config['comp_id']\n        filename = comp_id + '_det_' + self.image_set + '_{:s}.txt'\n        path = os.path.join(res_file_folder, filename)\n        return path", "response": "returns the template file name for the result set"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_pascal_results(self, all_boxes):\n        for cls_ind, cls in enumerate(self.classes):\n            print('Writing {} VOC results file'.format(cls))\n            filename = self.get_result_file_template().format(cls)\n            with open(filename, 'wt') as f:\n                for im_ind, index in enumerate(self.image_set_index):\n                    dets = all_boxes[im_ind]\n                    if dets.shape[0] < 1:\n                        continue\n                    h, w = self._get_imsize(self.image_path_from_index(im_ind))\n                    # the VOCdevkit expects 1-based indices\n                    for k in range(dets.shape[0]):\n                        if (int(dets[k, 0]) == cls_ind):\n                            f.write('{:s} {:.3f} {:.1f} {:.1f} {:.1f} {:.1f}\\n'.\n                                    format(index, dets[k, 1],\n                                           int(dets[k, 2] * w) + 1, int(dets[k, 3] * h) + 1,\n                                           int(dets[k, 4] * w) + 1, int(dets[k, 5] * h) + 1))", "response": "Writes the pascal results files in pascal devkit path"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_imsize(self, im_name):\n        img = cv2.imread(im_name)\n        return (img.shape[0], img.shape[1])", "response": "get image size info"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds the args required by fit to the parser", "response": "def add_fit_args(parser):\n    \"\"\"\n    parser : argparse.ArgumentParser\n    return a parser added with args required by fit\n    \"\"\"\n    train = parser.add_argument_group('Training', 'model training')\n    train.add_argument('--network', type=str,\n                       help='the neural network to use')\n    train.add_argument('--num-layers', type=int,\n                       help='number of layers in the neural network, \\\n                             required by some networks such as resnet')\n    train.add_argument('--gpus', type=str,\n                       help='list of gpus to run, e.g. 0 or 0,2,5. empty means using cpu')\n    train.add_argument('--kv-store', type=str, default='device',\n                       help='key-value store type')\n    train.add_argument('--num-epochs', type=int, default=100,\n                       help='max num of epochs')\n    train.add_argument('--lr', type=float, default=0.1,\n                       help='initial learning rate')\n    train.add_argument('--lr-factor', type=float, default=0.1,\n                       help='the ratio to reduce lr on each step')\n    train.add_argument('--lr-step-epochs', type=str,\n                       help='the epochs to reduce the lr, e.g. 30,60')\n    train.add_argument('--initializer', type=str, default='default',\n                       help='the initializer type')\n    train.add_argument('--optimizer', type=str, default='sgd',\n                       help='the optimizer type')\n    train.add_argument('--mom', type=float, default=0.9,\n                       help='momentum for sgd')\n    train.add_argument('--wd', type=float, default=0.0001,\n                       help='weight decay for sgd')\n    train.add_argument('--batch-size', type=int, default=128,\n                       help='the batch size')\n    train.add_argument('--disp-batches', type=int, default=20,\n                       help='show progress for every n batches')\n    train.add_argument('--model-prefix', type=str,\n                       help='model prefix')\n    train.add_argument('--save-period', type=int, default=1, help='params saving period')\n    parser.add_argument('--monitor', dest='monitor', type=int, default=0,\n                        help='log network parameters every N iters if larger than 0')\n    train.add_argument('--load-epoch', type=int,\n                       help='load the model on an epoch using the model-load-prefix')\n    train.add_argument('--top-k', type=int, default=0,\n                       help='report the top-k accuracy. 0 means no report.')\n    train.add_argument('--loss', type=str, default='',\n                       help='show the cross-entropy or nll loss. ce strands for cross-entropy, nll-loss stands for likelihood loss')\n    train.add_argument('--test-io', type=int, default=0,\n                       help='1 means test reading speed without training')\n    train.add_argument('--dtype', type=str, default='float32',\n                       help='precision: float32 or float16')\n    train.add_argument('--gc-type', type=str, default='none',\n                       help='type of gradient compression to use, \\\n                             takes `2bit` or `none` for now')\n    train.add_argument('--gc-threshold', type=float, default=0.5,\n                       help='threshold for 2bit gradient compression')\n    # additional parameters for large batch sgd\n    train.add_argument('--macrobatch-size', type=int, default=0,\n                       help='distributed effective batch size')\n    train.add_argument('--warmup-epochs', type=int, default=5,\n                       help='the epochs to ramp-up lr to scaled large-batch value')\n    train.add_argument('--warmup-strategy', type=str, default='linear',\n                       help='the ramping-up strategy for large batch sgd')\n    train.add_argument('--profile-worker-suffix', type=str, default='',\n                       help='profile workers actions into this file. During distributed training\\\n                             filename saved will be rank1_ followed by this suffix')\n    train.add_argument('--profile-server-suffix', type=str, default='',\n                       help='profile server actions into a file with name like rank1_ followed by this suffix \\\n                             during distributed training')\n    return train"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntraining a nerual network and return the training and validation data iterators", "response": "def fit(args, network, data_loader, **kwargs):\n    \"\"\"\n    train a model\n    args : argparse returns\n    network : the symbol definition of the nerual network\n    data_loader : function that returns the train and val data iterators\n    \"\"\"\n    # kvstore\n    kv = mx.kvstore.create(args.kv_store)\n    if args.gc_type != 'none':\n        kv.set_gradient_compression({'type': args.gc_type,\n                                     'threshold': args.gc_threshold})\n    if args.profile_server_suffix:\n        mx.profiler.set_config(filename=args.profile_server_suffix, profile_all=True, profile_process='server')\n        mx.profiler.set_state(state='run', profile_process='server')\n\n    if args.profile_worker_suffix:\n        if kv.num_workers > 1:\n            filename = 'rank' + str(kv.rank) + '_' + args.profile_worker_suffix\n        else:\n            filename = args.profile_worker_suffix\n        mx.profiler.set_config(filename=filename, profile_all=True, profile_process='worker')\n        mx.profiler.set_state(state='run', profile_process='worker')\n\n    # logging\n    head = '%(asctime)-15s Node[' + str(kv.rank) + '] %(message)s'\n    logging.basicConfig(level=logging.DEBUG, format=head)\n    logging.info('start with arguments %s', args)\n    \n    epoch_size = get_epoch_size(args, kv)\n\n    # data iterators\n    (train, val) = data_loader(args, kv)\n    if 'dist' in args.kv_store and not 'async' in args.kv_store:\n        logging.info('Resizing training data to %d batches per machine', epoch_size)\n        # resize train iter to ensure each machine has same number of batches per epoch\n        # if not, dist_sync can hang at the end with one machine waiting for other machines\n        train = mx.io.ResizeIter(train, epoch_size)\n\n    if args.test_io:\n        tic = time.time()\n        for i, batch in enumerate(train):\n            if isinstance(batch, list):\n                for b in batch:\n                    for j in b.data:\n                        j.wait_to_read()\n            else:\n                for j in batch.data:\n                    j.wait_to_read()\n            if (i + 1) % args.disp_batches == 0:\n                logging.info('Batch [%d]\\tSpeed: %.2f samples/sec', i,\n                             args.disp_batches * args.batch_size / (time.time() - tic))\n                tic = time.time()\n        return\n\n    # load model\n    if 'arg_params' in kwargs and 'aux_params' in kwargs:\n        arg_params = kwargs['arg_params']\n        aux_params = kwargs['aux_params']\n    else:\n        sym, arg_params, aux_params = _load_model(args, kv.rank)\n        if sym is not None:\n            assert sym.tojson() == network.tojson()\n\n    # save model\n    checkpoint = _save_model(args, kv.rank)\n\n    # devices for training\n    devs = mx.cpu() if args.gpus is None or args.gpus == \"\" else [\n        mx.gpu(int(i)) for i in args.gpus.split(',')]\n\n    # learning rate\n    lr, lr_scheduler = _get_lr_scheduler(args, kv)\n\n    # create model\n    model = mx.mod.Module(\n        context=devs,\n        symbol=network\n    )\n\n    lr_scheduler = lr_scheduler\n    optimizer_params = {\n        'learning_rate': lr,\n        'wd': args.wd,\n        'lr_scheduler': lr_scheduler,\n        'multi_precision': True}\n\n    # Only a limited number of optimizers have 'momentum' property\n    has_momentum = {'sgd', 'dcasgd', 'nag', 'signum', 'lbsgd'}\n    if args.optimizer in has_momentum:\n        optimizer_params['momentum'] = args.mom\n\n    monitor = mx.mon.Monitor(\n        args.monitor, pattern=\".*\") if args.monitor > 0 else None\n\n    # A limited number of optimizers have a warmup period\n    has_warmup = {'lbsgd', 'lbnag'}\n    if args.optimizer in has_warmup:\n        nworkers = kv.num_workers\n        if epoch_size < 1:\n            epoch_size = 1\n        macrobatch_size = args.macrobatch_size\n        if macrobatch_size < args.batch_size * nworkers:\n            macrobatch_size = args.batch_size * nworkers\n        #batch_scale = round(float(macrobatch_size) / args.batch_size / nworkers +0.4999)\n        batch_scale = math.ceil(\n            float(macrobatch_size) / args.batch_size / nworkers)\n        optimizer_params['updates_per_epoch'] = epoch_size\n        optimizer_params['begin_epoch'] = args.load_epoch if args.load_epoch else 0\n        optimizer_params['batch_scale'] = batch_scale\n        optimizer_params['warmup_strategy'] = args.warmup_strategy\n        optimizer_params['warmup_epochs'] = args.warmup_epochs\n        optimizer_params['num_epochs'] = args.num_epochs\n\n    if args.initializer == 'default':\n        if args.network == 'alexnet':\n            # AlexNet will not converge using Xavier\n            initializer = mx.init.Normal()\n            # VGG will not trend to converge using Xavier-Gaussian\n        elif args.network and 'vgg' in args.network:\n            initializer = mx.init.Xavier()\n        else:\n            initializer = mx.init.Xavier(\n                rnd_type='gaussian', factor_type=\"in\", magnitude=2)\n    # initializer   = mx.init.Xavier(factor_type=\"in\", magnitude=2.34),\n    elif args.initializer == 'xavier':\n        initializer = mx.init.Xavier()\n    elif args.initializer == 'msra':\n        initializer = mx.init.MSRAPrelu()\n    elif args.initializer == 'orthogonal':\n        initializer = mx.init.Orthogonal()\n    elif args.initializer == 'normal':\n        initializer = mx.init.Normal()\n    elif args.initializer == 'uniform':\n        initializer = mx.init.Uniform()\n    elif args.initializer == 'one':\n        initializer = mx.init.One()\n    elif args.initializer == 'zero':\n        initializer = mx.init.Zero()\n\n    # evaluation metrices\n    eval_metrics = ['accuracy']\n    if args.top_k > 0:\n        eval_metrics.append(mx.metric.create(\n            'top_k_accuracy', top_k=args.top_k))\n\n    supported_loss = ['ce', 'nll_loss']\n    if len(args.loss) > 0:\n        # ce or nll loss is only applicable to softmax output\n        loss_type_list = args.loss.split(',')\n        if 'softmax_output' in network.list_outputs():\n            for loss_type in loss_type_list:\n                loss_type = loss_type.strip()\n                if loss_type == 'nll':\n                    loss_type = 'nll_loss'\n                if loss_type not in supported_loss:\n                    logging.warning(loss_type + ' is not an valid loss type, only cross-entropy or ' \\\n                                    'negative likelihood loss is supported!')\n                else:\n                    eval_metrics.append(mx.metric.create(loss_type))\n        else:\n            logging.warning(\"The output is not softmax_output, loss argument will be skipped!\")\n\n    # callbacks that run after each batch\n    batch_end_callbacks = [mx.callback.Speedometer(\n        args.batch_size, args.disp_batches)]\n    if 'batch_end_callback' in kwargs:\n        cbs = kwargs['batch_end_callback']\n        batch_end_callbacks += cbs if isinstance(cbs, list) else [cbs]\n\n    # run\n    model.fit(train,\n              begin_epoch=args.load_epoch if args.load_epoch else 0,\n              num_epoch=args.num_epochs,\n              eval_data=val,\n              eval_metric=eval_metrics,\n              kvstore=kv,\n              optimizer=args.optimizer,\n              optimizer_params=optimizer_params,\n              initializer=initializer,\n              arg_params=arg_params,\n              aux_params=aux_params,\n              batch_end_callback=batch_end_callbacks,\n              epoch_end_callback=checkpoint,\n              allow_missing=True,\n              monitor=monitor)\n\n    if args.profile_server_suffix:\n        mx.profiler.set_state(state='run', profile_process='server')\n    if args.profile_worker_suffix:\n        mx.profiler.set_state(state='run', profile_process='worker')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef CreateMultiRandCropAugmenter(min_object_covered=0.1, aspect_ratio_range=(0.75, 1.33),\n                                 area_range=(0.05, 1.0), min_eject_coverage=0.3,\n                                 max_attempts=50, skip_prob=0):\n    \"\"\"Helper function to create multiple random crop augmenters.\n\n    Parameters\n    ----------\n    min_object_covered : float or list of float, default=0.1\n        The cropped area of the image must contain at least this fraction of\n        any bounding box supplied. The value of this parameter should be non-negative.\n        In the case of 0, the cropped area does not need to overlap any of the\n        bounding boxes supplied.\n    min_eject_coverage : float or list of float, default=0.3\n        The minimum coverage of cropped sample w.r.t its original size. With this\n        constraint, objects that have marginal area after crop will be discarded.\n    aspect_ratio_range : tuple of floats or list of tuple of floats, default=(0.75, 1.33)\n        The cropped area of the image must have an aspect ratio = width / height\n        within this range.\n    area_range : tuple of floats or list of tuple of floats, default=(0.05, 1.0)\n        The cropped area of the image must contain a fraction of the supplied\n        image within in this range.\n    max_attempts : int or list of int, default=50\n        Number of attempts at generating a cropped/padded region of the image of the\n        specified constraints. After max_attempts failures, return the original image.\n\n    Examples\n    --------\n    >>> # An example of creating multiple random crop augmenters\n    >>> min_object_covered = [0.1, 0.3, 0.5, 0.7, 0.9]  # use 5 augmenters\n    >>> aspect_ratio_range = (0.75, 1.33)  # use same range for all augmenters\n    >>> area_range = [(0.1, 1.0), (0.2, 1.0), (0.2, 1.0), (0.3, 0.9), (0.5, 1.0)]\n    >>> min_eject_coverage = 0.3\n    >>> max_attempts = 50\n    >>> aug = mx.image.det.CreateMultiRandCropAugmenter(min_object_covered=min_object_covered,\n            aspect_ratio_range=aspect_ratio_range, area_range=area_range,\n            min_eject_coverage=min_eject_coverage, max_attempts=max_attempts,\n            skip_prob=0)\n    >>> aug.dumps()  # show some details\n\n    \"\"\"\n    def align_parameters(params):\n        \"\"\"Align parameters as pairs\"\"\"\n        out_params = []\n        num = 1\n        for p in params:\n            if not isinstance(p, list):\n                p = [p]\n            out_params.append(p)\n            num = max(num, len(p))\n        # align for each param\n        for k, p in enumerate(out_params):\n            if len(p) != num:\n                assert len(p) == 1\n                out_params[k] = p * num\n        return out_params\n\n    aligned_params = align_parameters([min_object_covered, aspect_ratio_range, area_range,\n                                       min_eject_coverage, max_attempts])\n    augs = []\n    for moc, arr, ar, mec, ma in zip(*aligned_params):\n        augs.append(DetRandomCropAug(min_object_covered=moc, aspect_ratio_range=arr,\n                                     area_range=ar, min_eject_coverage=mec, max_attempts=ma))\n    return DetRandomSelectAug(augs, skip_prob=skip_prob)", "response": "This function creates a random crop augmenter for a single resource."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef CreateDetAugmenter(data_shape, resize=0, rand_crop=0, rand_pad=0, rand_gray=0,\n                       rand_mirror=False, mean=None, std=None, brightness=0, contrast=0,\n                       saturation=0, pca_noise=0, hue=0, inter_method=2, min_object_covered=0.1,\n                       aspect_ratio_range=(0.75, 1.33), area_range=(0.05, 3.0),\n                       min_eject_coverage=0.3, max_attempts=50, pad_val=(127, 127, 127)):\n    \"\"\"Create augmenters for detection.\n\n    Parameters\n    ----------\n    data_shape : tuple of int\n        Shape for output data\n    resize : int\n        Resize shorter edge if larger than 0 at the begining\n    rand_crop : float\n        [0, 1], probability to apply random cropping\n    rand_pad : float\n        [0, 1], probability to apply random padding\n    rand_gray : float\n        [0, 1], probability to convert to grayscale for all channels\n    rand_mirror : bool\n        Whether to apply horizontal flip to image with probability 0.5\n    mean : np.ndarray or None\n        Mean pixel values for [r, g, b]\n    std : np.ndarray or None\n        Standard deviations for [r, g, b]\n    brightness : float\n        Brightness jittering range (percent)\n    contrast : float\n        Contrast jittering range (percent)\n    saturation : float\n        Saturation jittering range (percent)\n    hue : float\n        Hue jittering range (percent)\n    pca_noise : float\n        Pca noise level (percent)\n    inter_method : int, default=2(Area-based)\n        Interpolation method for all resizing operations\n\n        Possible values:\n        0: Nearest Neighbors Interpolation.\n        1: Bilinear interpolation.\n        2: Area-based (resampling using pixel area relation). It may be a\n        preferred method for image decimation, as it gives moire-free\n        results. But when the image is zoomed, it is similar to the Nearest\n        Neighbors method. (used by default).\n        3: Bicubic interpolation over 4x4 pixel neighborhood.\n        4: Lanczos interpolation over 8x8 pixel neighborhood.\n        9: Cubic for enlarge, area for shrink, bilinear for others\n        10: Random select from interpolation method metioned above.\n        Note:\n        When shrinking an image, it will generally look best with AREA-based\n        interpolation, whereas, when enlarging an image, it will generally look best\n        with Bicubic (slow) or Bilinear (faster but still looks OK).\n    min_object_covered : float\n        The cropped area of the image must contain at least this fraction of\n        any bounding box supplied. The value of this parameter should be non-negative.\n        In the case of 0, the cropped area does not need to overlap any of the\n        bounding boxes supplied.\n    min_eject_coverage : float\n        The minimum coverage of cropped sample w.r.t its original size. With this\n        constraint, objects that have marginal area after crop will be discarded.\n    aspect_ratio_range : tuple of floats\n        The cropped area of the image must have an aspect ratio = width / height\n        within this range.\n    area_range : tuple of floats\n        The cropped area of the image must contain a fraction of the supplied\n        image within in this range.\n    max_attempts : int\n        Number of attempts at generating a cropped/padded region of the image of the\n        specified constraints. After max_attempts failures, return the original image.\n    pad_val: float\n        Pixel value to be filled when padding is enabled. pad_val will automatically\n        be subtracted by mean and divided by std if applicable.\n\n    Examples\n    --------\n    >>> # An example of creating multiple augmenters\n    >>> augs = mx.image.CreateDetAugmenter(data_shape=(3, 300, 300), rand_crop=0.5,\n    ...    rand_pad=0.5, rand_mirror=True, mean=True, brightness=0.125, contrast=0.125,\n    ...    saturation=0.125, pca_noise=0.05, inter_method=10, min_object_covered=[0.3, 0.5, 0.9],\n    ...    area_range=(0.3, 3.0))\n    >>> # dump the details\n    >>> for aug in augs:\n    ...    aug.dumps()\n    \"\"\"\n    auglist = []\n\n    if resize > 0:\n        auglist.append(DetBorrowAug(ResizeAug(resize, inter_method)))\n\n    if rand_crop > 0:\n        crop_augs = CreateMultiRandCropAugmenter(min_object_covered, aspect_ratio_range,\n                                                 area_range, min_eject_coverage,\n                                                 max_attempts, skip_prob=(1 - rand_crop))\n        auglist.append(crop_augs)\n\n    if rand_mirror > 0:\n        auglist.append(DetHorizontalFlipAug(0.5))\n\n    # apply random padding as late as possible to save computation\n    if rand_pad > 0:\n        pad_aug = DetRandomPadAug(aspect_ratio_range,\n                                  (1.0, area_range[1]), max_attempts, pad_val)\n        auglist.append(DetRandomSelectAug([pad_aug], 1 - rand_pad))\n\n    # force resize\n    auglist.append(DetBorrowAug(ForceResizeAug((data_shape[2], data_shape[1]), inter_method)))\n\n    auglist.append(DetBorrowAug(CastAug()))\n\n    if brightness or contrast or saturation:\n        auglist.append(DetBorrowAug(ColorJitterAug(brightness, contrast, saturation)))\n\n    if hue:\n        auglist.append(DetBorrowAug(HueJitterAug(hue)))\n\n    if pca_noise > 0:\n        eigval = np.array([55.46, 4.794, 1.148])\n        eigvec = np.array([[-0.5675, 0.7192, 0.4009],\n                           [-0.5808, -0.0045, -0.8140],\n                           [-0.5836, -0.6948, 0.4203]])\n        auglist.append(DetBorrowAug(LightingAug(pca_noise, eigval, eigvec)))\n\n    if rand_gray > 0:\n        auglist.append(DetBorrowAug(RandomGrayAug(rand_gray)))\n\n    if mean is True:\n        mean = np.array([123.68, 116.28, 103.53])\n    elif mean is not None:\n        assert isinstance(mean, np.ndarray) and mean.shape[0] in [1, 3]\n\n    if std is True:\n        std = np.array([58.395, 57.12, 57.375])\n    elif std is not None:\n        assert isinstance(std, np.ndarray) and std.shape[0] in [1, 3]\n\n    if mean is not None or std is not None:\n        auglist.append(DetBorrowAug(ColorNormalizeAug(mean, std)))\n\n    return auglist", "response": "Create an augmenter for detection."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _calculate_areas(self, label):\n        heights = np.maximum(0, label[:, 3] - label[:, 1])\n        widths = np.maximum(0, label[:, 2] - label[:, 0])\n        return heights * widths", "response": "Calculate areas for multiple labels"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _intersect(self, label, xmin, ymin, xmax, ymax):\n        left = np.maximum(label[:, 0], xmin)\n        right = np.minimum(label[:, 2], xmax)\n        top = np.maximum(label[:, 1], ymin)\n        bot = np.minimum(label[:, 3], ymax)\n        invalid = np.where(np.logical_or(left >= right, top >= bot))[0]\n        out = label.copy()\n        out[:, 0] = left\n        out[:, 1] = top\n        out[:, 2] = right\n        out[:, 3] = bot\n        out[invalid, :] = 0\n        return out", "response": "Calculate intersect areas normalized."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if constrains are satisfied", "response": "def _check_satisfy_constraints(self, label, xmin, ymin, xmax, ymax, width, height):\n        \"\"\"Check if constrains are satisfied\"\"\"\n        if (xmax - xmin) * (ymax - ymin) < 2:\n            return False  # only 1 pixel\n        x1 = float(xmin) / width\n        y1 = float(ymin) / height\n        x2 = float(xmax) / width\n        y2 = float(ymax) / height\n        object_areas = self._calculate_areas(label[:, 1:])\n        valid_objects = np.where(object_areas * width * height > 2)[0]\n        if valid_objects.size < 1:\n            return False\n        intersects = self._intersect(label[valid_objects, 1:], x1, y1, x2, y2)\n        coverages = self._calculate_areas(intersects) / object_areas[valid_objects]\n        coverages = coverages[np.where(coverages > 0)[0]]\n        return coverages.size > 0 and np.amin(coverages) > self.min_object_covered"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert labels according to crop box", "response": "def _update_labels(self, label, crop_box, height, width):\n        \"\"\"Convert labels according to crop box\"\"\"\n        xmin = float(crop_box[0]) / width\n        ymin = float(crop_box[1]) / height\n        w = float(crop_box[2]) / width\n        h = float(crop_box[3]) / height\n        out = label.copy()\n        out[:, (1, 3)] -= xmin\n        out[:, (2, 4)] -= ymin\n        out[:, (1, 3)] /= w\n        out[:, (2, 4)] /= h\n        out[:, 1:5] = np.maximum(0, out[:, 1:5])\n        out[:, 1:5] = np.minimum(1, out[:, 1:5])\n        coverage = self._calculate_areas(out[:, 1:]) * w * h / self._calculate_areas(label[:, 1:])\n        valid = np.logical_and(out[:, 3] > out[:, 1], out[:, 4] > out[:, 2])\n        valid = np.logical_and(valid, coverage > self.min_eject_coverage)\n        valid = np.where(valid)[0]\n        if valid.size < 1:\n            return None\n        out = out[valid, :]\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates the label according to padding region", "response": "def _update_labels(self, label, pad_box, height, width):\n        \"\"\"Update label according to padding region\"\"\"\n        out = label.copy()\n        out[:, (1, 3)] = (out[:, (1, 3)] * width + pad_box[0]) / pad_box[2]\n        out[:, (2, 4)] = (out[:, (2, 4)] * height + pad_box[1]) / pad_box[3]\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating a random padding region", "response": "def _random_pad_proposal(self, label, height, width):\n        \"\"\"Generate random padding region\"\"\"\n        from math import sqrt\n        if not self.enabled or height <= 0 or width <= 0:\n            return ()\n        min_area = self.area_range[0] * height * width\n        max_area = self.area_range[1] * height * width\n        for _ in range(self.max_attempts):\n            ratio = random.uniform(*self.aspect_ratio_range)\n            if ratio <= 0:\n                continue\n            h = int(round(sqrt(min_area / ratio)))\n            max_h = int(round(sqrt(max_area / ratio)))\n            if round(h * ratio) < width:\n                h = int((width + 0.499999) / ratio)\n            if h < height:\n                h = height\n            if h > max_h:\n                h = max_h\n            if h < max_h:\n                h = random.randint(h, max_h)\n            w = int(round(h * ratio))\n            if (h - height) < 2 or (w - width) < 2:\n                continue  # marginal padding is not helpful\n\n            y = random.randint(0, max(0, h - height))\n            x = random.randint(0, max(0, w - width))\n            new_label = self._update_labels(label, (x, y, w, h), height, width)\n            return (x, y, w, h, new_label)\n        return ()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nvalidating label and its shape.", "response": "def _check_valid_label(self, label):\n        \"\"\"Validate label and its shape.\"\"\"\n        if len(label.shape) != 2 or label.shape[1] < 5:\n            msg = \"Label with shape (1+, 5+) required, %s received.\" % str(label)\n            raise RuntimeError(msg)\n        valid_label = np.where(np.logical_and(label[:, 0] >= 0, label[:, 3] > label[:, 1],\n                                              label[:, 4] > label[:, 2]))[0]\n        if valid_label.size < 1:\n            raise RuntimeError('Invalid label occurs.')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _parse_label(self, label):\n        if isinstance(label, nd.NDArray):\n            label = label.asnumpy()\n        raw = label.ravel()\n        if raw.size < 7:\n            raise RuntimeError(\"Label shape is invalid: \" + str(raw.shape))\n        header_width = int(raw[0])\n        obj_width = int(raw[1])\n        if (raw.size - header_width) % obj_width != 0:\n            msg = \"Label shape %s inconsistent with annotation width %d.\" \\\n                %(str(raw.shape), obj_width)\n            raise RuntimeError(msg)\n        out = np.reshape(raw[header_width:], (-1, obj_width))\n        # remove bad ground-truths\n        valid = np.where(np.logical_and(out[:, 3] > out[:, 1], out[:, 4] > out[:, 2]))[0]\n        if valid.size < 1:\n            raise RuntimeError('Encounter sample with no valid label.')\n        return out[valid, :]", "response": "Helper function to parse object detection label."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reshape(self, data_shape=None, label_shape=None):\n        if data_shape is not None:\n            self.check_data_shape(data_shape)\n            self.provide_data = [(self.provide_data[0][0], (self.batch_size,) + data_shape)]\n            self.data_shape = data_shape\n        if label_shape is not None:\n            self.check_label_shape(label_shape)\n            self.provide_label = [(self.provide_label[0][0], (self.batch_size,) + label_shape)]\n            self.label_shape = label_shape", "response": "Reshape the data_shape or label_shape of the containing class."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\noverride the helper function for batchifying data", "response": "def _batchify(self, batch_data, batch_label, start=0):\n        \"\"\"Override the helper function for batchifying data\"\"\"\n        i = start\n        batch_size = self.batch_size\n        try:\n            while i < batch_size:\n                label, s = self.next_sample()\n                data = self.imdecode(s)\n                try:\n                    self.check_valid_image([data])\n                    label = self._parse_label(label)\n                    data, label = self.augmentation_transform(data, label)\n                    self._check_valid_label(label)\n                except RuntimeError as e:\n                    logging.debug('Invalid image, skipping:  %s', str(e))\n                    continue\n                for datum in [data]:\n                    assert i < batch_size, 'Batch size must be multiples of augmenter output length'\n                    batch_data[i] = self.postprocess_data(datum)\n                    num_object = label.shape[0]\n                    batch_label[i][0:num_object] = nd.array(label)\n                    if num_object < batch_label[i].shape[0]:\n                        batch_label[i][num_object:] = -1\n                    i += 1\n        except StopIteration:\n            if not i:\n                raise StopIteration\n\n        return i"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\noverriding the function for returning next batch.", "response": "def next(self):\n        \"\"\"Override the function for returning next batch.\"\"\"\n        batch_size = self.batch_size\n        c, h, w = self.data_shape\n        # if last batch data is rolled over\n        if self._cache_data is not None:\n            # check both the data and label have values\n            assert self._cache_label is not None, \"_cache_label didn't have values\"\n            assert self._cache_idx is not None, \"_cache_idx didn't have values\"\n            batch_data = self._cache_data\n            batch_label = self._cache_label\n            i = self._cache_idx\n        else:\n            batch_data = nd.zeros((batch_size, c, h, w))\n            batch_label = nd.empty(self.provide_label[0][1])\n            batch_label[:] = -1\n            i = self._batchify(batch_data, batch_label)\n        # calculate the padding\n        pad = batch_size - i\n        # handle padding for the last batch\n        if pad != 0:\n            if self.last_batch_handle == 'discard':\n                raise StopIteration\n            # if the option is 'roll_over', throw StopIteration and cache the data\n            elif self.last_batch_handle == 'roll_over' and \\\n                self._cache_data is None:\n                self._cache_data = batch_data\n                self._cache_label = batch_label\n                self._cache_idx = i\n                raise StopIteration\n            else:\n                _ = self._batchify(batch_data, batch_label, i)\n                if self.last_batch_handle == 'pad':\n                    self._allow_read = False\n                else:\n                    self._cache_data = None\n                    self._cache_label = None\n                    self._cache_idx = None\n\n        return io.DataBatch([batch_data], [batch_label], pad=pad)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\noverrides Transforms input data with specified augmentations.", "response": "def augmentation_transform(self, data, label):  # pylint: disable=arguments-differ\n        \"\"\"Override Transforms input data with specified augmentations.\"\"\"\n        for aug in self.auglist:\n            data, label = aug(data, label)\n        return (data, label)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if the new label shape is valid", "response": "def check_label_shape(self, label_shape):\n        \"\"\"Checks if the new label shape is valid\"\"\"\n        if not len(label_shape) == 2:\n            raise ValueError('label_shape should have length 2')\n        if label_shape[0] < self.label_shape[0]:\n            msg = 'Attempts to reduce label count from %d to %d, not allowed.' \\\n                % (self.label_shape[0], label_shape[0])\n            raise ValueError(msg)\n        if label_shape[1] != self.provide_label[0][1][2]:\n            msg = 'label_shape object width inconsistent: %d vs %d.' \\\n                % (self.provide_label[0][1][2], label_shape[1])\n            raise ValueError(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef draw_next(self, color=None, thickness=2, mean=None, std=None, clip=True,\n                  waitKey=None, window_name='draw_next', id2labels=None):\n        \"\"\"Display next image with bounding boxes drawn.\n\n        Parameters\n        ----------\n        color : tuple\n            Bounding box color in RGB, use None for random color\n        thickness : int\n            Bounding box border thickness\n        mean : True or numpy.ndarray\n            Compensate for the mean to have better visual effect\n        std : True or numpy.ndarray\n            Revert standard deviations\n        clip : bool\n            If true, clip to [0, 255] for better visual effect\n        waitKey : None or int\n            Hold the window for waitKey milliseconds if set, skip ploting if None\n        window_name : str\n            Plot window name if waitKey is set.\n        id2labels : dict\n            Mapping of labels id to labels name.\n\n        Returns\n        -------\n            numpy.ndarray\n\n        Examples\n        --------\n        >>> # use draw_next to get images with bounding boxes drawn\n        >>> iterator = mx.image.ImageDetIter(1, (3, 600, 600), path_imgrec='train.rec')\n        >>> for image in iterator.draw_next(waitKey=None):\n        ...     # display image\n        >>> # or let draw_next display using cv2 module\n        >>> for image in iterator.draw_next(waitKey=0, window_name='disp'):\n        ...     pass\n        \"\"\"\n        try:\n            import cv2\n        except ImportError as e:\n            warnings.warn('Unable to import cv2, skip drawing: %s', str(e))\n            return\n        count = 0\n        try:\n            while True:\n                label, s = self.next_sample()\n                data = self.imdecode(s)\n                try:\n                    self.check_valid_image([data])\n                    label = self._parse_label(label)\n                except RuntimeError as e:\n                    logging.debug('Invalid image, skipping:  %s', str(e))\n                    continue\n                count += 1\n                data, label = self.augmentation_transform(data, label)\n                image = data.asnumpy()\n\n                # revert color_normalize\n                if std is True:\n                    std = np.array([58.395, 57.12, 57.375])\n                elif std is not None:\n                    assert isinstance(std, np.ndarray) and std.shape[0] in [1, 3]\n                if std is not None:\n                    image *= std\n\n                if mean is True:\n                    mean = np.array([123.68, 116.28, 103.53])\n                elif mean is not None:\n                    assert isinstance(mean, np.ndarray) and mean.shape[0] in [1, 3]\n                if mean is not None:\n                    image += mean\n\n                # swap RGB\n                image[:, :, (0, 1, 2)] = image[:, :, (2, 1, 0)]\n                if clip:\n                    image = np.maximum(0, np.minimum(255, image))\n                if color:\n                    color = color[::-1]\n                image = image.astype(np.uint8)\n                height, width, _ = image.shape\n                for i in range(label.shape[0]):\n                    x1 = int(label[i, 1] * width)\n                    if x1 < 0:\n                        continue\n                    y1 = int(label[i, 2] * height)\n                    x2 = int(label[i, 3] * width)\n                    y2 = int(label[i, 4] * height)\n                    bc = np.random.rand(3) * 255 if not color else color\n                    cv2.rectangle(image, (x1, y1), (x2, y2), bc, thickness)\n                    if id2labels is not None:\n                        cls_id = int(label[i, 0])\n                        if cls_id in id2labels:\n                            cls_name = id2labels[cls_id]\n                            text = \"{:s}\".format(cls_name)\n                            font = cv2.FONT_HERSHEY_SIMPLEX\n                            font_scale = 0.5\n                            text_height = cv2.getTextSize(text, font, font_scale, 2)[0][1]\n                            tc = (255, 255, 255)\n                            tpos = (x1 + 5, y1 + text_height + 5)\n                            cv2.putText(image, text, tpos, font, font_scale, tc, 2)\n                if waitKey is not None:\n                    cv2.imshow(window_name, image)\n                    cv2.waitKey(waitKey)\n                yield image\n        except StopIteration:\n            if not count:\n                return", "response": "Draw next image with bounding boxes drawn."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsynchronizing label shape with the input iterator.", "response": "def sync_label_shape(self, it, verbose=False):\n        \"\"\"Synchronize label shape with the input iterator. This is useful when\n        train/validation iterators have different label padding.\n\n        Parameters\n        ----------\n        it : ImageDetIter\n            The other iterator to synchronize\n        verbose : bool\n            Print verbose log if true\n\n        Returns\n        -------\n        ImageDetIter\n            The synchronized other iterator, the internal label shape is updated as well.\n\n        Examples\n        --------\n        >>> train_iter = mx.image.ImageDetIter(32, (3, 300, 300), path_imgrec='train.rec')\n        >>> val_iter = mx.image.ImageDetIter(32, (3, 300, 300), path.imgrec='val.rec')\n        >>> train_iter.label_shape\n        (30, 6)\n        >>> val_iter.label_shape\n        (25, 6)\n        >>> val_iter = train_iter.sync_label_shape(val_iter, verbose=False)\n        >>> train_iter.label_shape\n        (30, 6)\n        >>> val_iter.label_shape\n        (30, 6)\n        \"\"\"\n        assert isinstance(it, ImageDetIter), 'Synchronize with invalid iterator.'\n        train_label_shape = self.label_shape\n        val_label_shape = it.label_shape\n        assert train_label_shape[1] == val_label_shape[1], \"object width mismatch.\"\n        max_count = max(train_label_shape[0], val_label_shape[0])\n        if max_count > train_label_shape[0]:\n            self.reshape(None, (max_count, train_label_shape[1]))\n        if max_count > val_label_shape[0]:\n            it.reshape(None, (max_count, val_label_shape[1]))\n        if verbose and max_count > min(train_label_shape[0], val_label_shape[0]):\n            logging.info('Resized label_shape to (%d, %d).', max_count, train_label_shape[1])\n        return it"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _generate_base_anchors(base_size, scales, ratios):\n        base_anchor = np.array([1, 1, base_size, base_size]) - 1\n        ratio_anchors = AnchorGenerator._ratio_enum(base_anchor, ratios)\n        anchors = np.vstack([AnchorGenerator._scale_enum(ratio_anchors[i, :], scales)\n                             for i in range(ratio_anchors.shape[0])])\n        return anchors", "response": "Generate anchor windows by enumerating aspect ratios X\n scales wrt a reference ( 0 15 ) window."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning width height x center y center for an anchor.", "response": "def _whctrs(anchor):\n        \"\"\"\n        Return width, height, x center, and y center for an anchor (window).\n        \"\"\"\n        w = anchor[2] - anchor[0] + 1\n        h = anchor[3] - anchor[1] + 1\n        x_ctr = anchor[0] + 0.5 * (w - 1)\n        y_ctr = anchor[1] + 0.5 * (h - 1)\n        return w, h, x_ctr, y_ctr"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a set of anchors around a center of a set of windows.", "response": "def _mkanchors(ws, hs, x_ctr, y_ctr):\n        \"\"\"\n        Given a vector of widths (ws) and heights (hs) around a center\n        (x_ctr, y_ctr), output a set of anchors (windows).\n        \"\"\"\n        ws = ws[:, np.newaxis]\n        hs = hs[:, np.newaxis]\n        anchors = np.hstack((x_ctr - 0.5 * (ws - 1),\n                             y_ctr - 0.5 * (hs - 1),\n                             x_ctr + 0.5 * (ws - 1),\n                             y_ctr + 0.5 * (hs - 1)))\n        return anchors"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nenumerating a set of anchors for each aspect ratio wrt an anchor.", "response": "def _ratio_enum(anchor, ratios):\n        \"\"\"\n        Enumerate a set of anchors for each aspect ratio wrt an anchor.\n        \"\"\"\n        w, h, x_ctr, y_ctr = AnchorGenerator._whctrs(anchor)\n        size = w * h\n        size_ratios = size / ratios\n        ws = np.round(np.sqrt(size_ratios))\n        hs = np.round(ws * ratios)\n        anchors = AnchorGenerator._mkanchors(ws, hs, x_ctr, y_ctr)\n        return anchors"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _scale_enum(anchor, scales):\n        w, h, x_ctr, y_ctr = AnchorGenerator._whctrs(anchor)\n        ws = w * scales\n        hs = h * scales\n        anchors = AnchorGenerator._mkanchors(ws, hs, x_ctr, y_ctr)\n        return anchors", "response": "Enumerate a set of anchors for each scale wrt an anchor."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef prepare_data(args):\n    rnn_type = args.config.get(\"arch\", \"rnn_type\")\n    num_rnn_layer = args.config.getint(\"arch\", \"num_rnn_layer\")\n    num_hidden_rnn_list = json.loads(args.config.get(\"arch\", \"num_hidden_rnn_list\"))\n\n    batch_size = args.config.getint(\"common\", \"batch_size\")\n\n    if rnn_type == 'lstm':\n        init_c = [('l%d_init_c' % l, (batch_size, num_hidden_rnn_list[l]))\n                  for l in range(num_rnn_layer)]\n        init_h = [('l%d_init_h' % l, (batch_size, num_hidden_rnn_list[l]))\n                  for l in range(num_rnn_layer)]\n    elif rnn_type == 'bilstm':\n        forward_init_c = [('forward_l%d_init_c' % l, (batch_size, num_hidden_rnn_list[l]))\n                          for l in range(num_rnn_layer)]\n        backward_init_c = [('backward_l%d_init_c' % l, (batch_size, num_hidden_rnn_list[l]))\n                           for l in range(num_rnn_layer)]\n        init_c = forward_init_c + backward_init_c\n        forward_init_h = [('forward_l%d_init_h' % l, (batch_size, num_hidden_rnn_list[l]))\n                          for l in range(num_rnn_layer)]\n        backward_init_h = [('backward_l%d_init_h' % l, (batch_size, num_hidden_rnn_list[l]))\n                           for l in range(num_rnn_layer)]\n        init_h = forward_init_h + backward_init_h\n    elif rnn_type == 'gru':\n        init_h = [('l%d_init_h' % l, (batch_size, num_hidden_rnn_list[l]))\n                  for l in range(num_rnn_layer)]\n    elif rnn_type == 'bigru':\n        forward_init_h = [('forward_l%d_init_h' % l, (batch_size, num_hidden_rnn_list[l]))\n                          for l in range(num_rnn_layer)]\n        backward_init_h = [('backward_l%d_init_h' % l, (batch_size, num_hidden_rnn_list[l]))\n                           for l in range(num_rnn_layer)]\n        init_h = forward_init_h + backward_init_h\n    else:\n        raise Exception('network type should be one of the lstm,bilstm,gru,bigru')\n\n    if rnn_type == 'lstm' or rnn_type == 'bilstm':\n        init_states = init_c + init_h\n    elif rnn_type == 'gru' or rnn_type == 'bigru':\n        init_states = init_h\n    return init_states", "response": "Prepare the data for the current rnn."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef arch(args, seq_len=None):\n    if isinstance(args, argparse.Namespace):\n        mode = args.config.get(\"common\", \"mode\")\n        is_bucketing = args.config.getboolean(\"arch\", \"is_bucketing\")\n        if mode == \"train\" or is_bucketing:\n            channel_num = args.config.getint(\"arch\", \"channel_num\")\n            conv_layer1_filter_dim = \\\n                tuple(json.loads(args.config.get(\"arch\", \"conv_layer1_filter_dim\")))\n            conv_layer1_stride = tuple(json.loads(args.config.get(\"arch\", \"conv_layer1_stride\")))\n            conv_layer2_filter_dim = \\\n                tuple(json.loads(args.config.get(\"arch\", \"conv_layer2_filter_dim\")))\n            conv_layer2_stride = tuple(json.loads(args.config.get(\"arch\", \"conv_layer2_stride\")))\n\n            rnn_type = args.config.get(\"arch\", \"rnn_type\")\n            num_rnn_layer = args.config.getint(\"arch\", \"num_rnn_layer\")\n            num_hidden_rnn_list = json.loads(args.config.get(\"arch\", \"num_hidden_rnn_list\"))\n\n            is_batchnorm = args.config.getboolean(\"arch\", \"is_batchnorm\")\n\n            if seq_len is None:\n                seq_len = args.config.getint('arch', 'max_t_count')\n\n            num_label = args.config.getint('arch', 'max_label_length')\n\n            num_rear_fc_layers = args.config.getint(\"arch\", \"num_rear_fc_layers\")\n            num_hidden_rear_fc_list = json.loads(args.config.get(\"arch\", \"num_hidden_rear_fc_list\"))\n            act_type_rear_fc_list = json.loads(args.config.get(\"arch\", \"act_type_rear_fc_list\"))\n            # model symbol generation\n            # input preparation\n            data = mx.sym.Variable('data')\n            label = mx.sym.Variable('label')\n\n            net = mx.sym.Reshape(data=data, shape=(-4, -1, 1, 0, 0))\n            net = conv(net=net,\n                       channels=channel_num,\n                       filter_dimension=conv_layer1_filter_dim,\n                       stride=conv_layer1_stride,\n                       no_bias=is_batchnorm,\n                       name='conv1')\n            if is_batchnorm:\n               # batch norm normalizes axis 1\n               net = batchnorm(net, name=\"conv1_batchnorm\")\n\n            net = conv(net=net,\n                       channels=channel_num,\n                       filter_dimension=conv_layer2_filter_dim,\n                       stride=conv_layer2_stride,\n                       no_bias=is_batchnorm,\n                       name='conv2')\n            if is_batchnorm:\n                # batch norm normalizes axis 1\n                net = batchnorm(net, name=\"conv2_batchnorm\")\n\n            net = mx.sym.transpose(data=net, axes=(0, 2, 1, 3))\n            net = mx.sym.Reshape(data=net, shape=(0, 0, -3))\n            seq_len_after_conv_layer1 = int(\n                math.floor((seq_len - conv_layer1_filter_dim[0]) / conv_layer1_stride[0])) + 1\n            seq_len_after_conv_layer2 = int(\n                math.floor((seq_len_after_conv_layer1 - conv_layer2_filter_dim[0])\n                           / conv_layer2_stride[0])) + 1\n            net = slice_symbol_to_seq_symobls(net=net, seq_len=seq_len_after_conv_layer2, axis=1)\n            if rnn_type == \"bilstm\":\n                net = bi_lstm_unroll(net=net,\n                                     seq_len=seq_len_after_conv_layer2,\n                                     num_hidden_lstm_list=num_hidden_rnn_list,\n                                     num_lstm_layer=num_rnn_layer,\n                                     dropout=0.,\n                                     is_batchnorm=is_batchnorm,\n                                     is_bucketing=is_bucketing)\n            elif rnn_type == \"gru\":\n                net = gru_unroll(net=net,\n                                 seq_len=seq_len_after_conv_layer2,\n                                 num_hidden_gru_list=num_hidden_rnn_list,\n                                 num_gru_layer=num_rnn_layer,\n                                 dropout=0.,\n                                 is_batchnorm=is_batchnorm,\n                                 is_bucketing=is_bucketing)\n            elif rnn_type == \"bigru\":\n                net = bi_gru_unroll(net=net,\n                                    seq_len=seq_len_after_conv_layer2,\n                                    num_hidden_gru_list=num_hidden_rnn_list,\n                                    num_gru_layer=num_rnn_layer,\n                                    dropout=0.,\n                                    is_batchnorm=is_batchnorm,\n                                    is_bucketing=is_bucketing)\n            else:\n                raise Exception('rnn_type should be one of the followings, bilstm,gru,bigru')\n\n            # rear fc layers\n            net = sequence_fc(net=net, seq_len=seq_len_after_conv_layer2,\n                              num_layer=num_rear_fc_layers, prefix=\"rear\",\n                              num_hidden_list=num_hidden_rear_fc_list,\n                              act_type_list=act_type_rear_fc_list,\n                              is_batchnorm=is_batchnorm)\n            # warpctc layer\n            net = warpctc_layer(net=net,\n                                seq_len=seq_len_after_conv_layer2,\n                                label=label,\n                                num_label=num_label,\n                                character_classes_count=\n                                (args.config.getint('arch', 'n_classes') + 1))\n            args.config.set('arch', 'max_t_count', str(seq_len_after_conv_layer2))\n            return net\n        elif mode == 'load' or mode == 'predict':\n            conv_layer1_filter_dim = \\\n                tuple(json.loads(args.config.get(\"arch\", \"conv_layer1_filter_dim\")))\n            conv_layer1_stride = tuple(json.loads(args.config.get(\"arch\", \"conv_layer1_stride\")))\n            conv_layer2_filter_dim = \\\n                tuple(json.loads(args.config.get(\"arch\", \"conv_layer2_filter_dim\")))\n            conv_layer2_stride = tuple(json.loads(args.config.get(\"arch\", \"conv_layer2_stride\")))\n            if seq_len is None:\n                seq_len = args.config.getint('arch', 'max_t_count')\n            seq_len_after_conv_layer1 = int(\n                math.floor((seq_len - conv_layer1_filter_dim[0]) / conv_layer1_stride[0])) + 1\n            seq_len_after_conv_layer2 = int(\n                math.floor((seq_len_after_conv_layer1 - conv_layer2_filter_dim[0])\n                           / conv_layer2_stride[0])) + 1\n\n            args.config.set('arch', 'max_t_count', str(seq_len_after_conv_layer2))\n        else:\n            raise Exception('mode must be the one of the followings - train,predict,load')", "response": "Define deep speech 2 network"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--batch_size', type=int, default=64)\n    parser.add_argument('--epochs', type=int, default=100)\n    parser.add_argument('--image_path', type=str, default='./data/datasets/')\n    parser.add_argument('--align_path', type=str, default='./data/align/')\n    parser.add_argument('--dr_rate', type=float, default=0.5)\n    parser.add_argument('--num_gpus', type=int, default=1)\n    parser.add_argument('--num_workers', type=int, default=0)\n    parser.add_argument('--model_path', type=str, default=None)\n    config = parser.parse_args()\n    trainer = Train(config)\n    trainer.build_model(dr_rate=config.dr_rate, path=config.model_path)\n    trainer.load_dataloader()\n    trainer.run(epochs=config.epochs)", "response": "Description : run lipnet training code using argument info"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef vis_detection(im_orig, detections, class_names, thresh=0.7):\n    import matplotlib.pyplot as plt\n    import random\n    plt.imshow(im_orig)\n    colors = [(random.random(), random.random(), random.random()) for _ in class_names]\n    for [cls, conf, x1, y1, x2, y2] in detections:\n        cls = int(cls)\n        if cls > 0 and conf > thresh:\n            rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1,\n                                 fill=False, edgecolor=colors[cls], linewidth=3.5)\n            plt.gca().add_patch(rect)\n            plt.gca().text(x1, y1 - 2, '{:s} {:.3f}'.format(class_names[cls], conf),\n                           bbox=dict(facecolor=colors[cls], alpha=0.5), fontsize=12, color='white')\n    plt.show()", "response": "visualize [cls, conf, x1, y1, x2, y2]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_error(model, path, shapes, output = 'softmax_output', verbose = True):\n    coreml_model = _coremltools.models.MLModel(path)\n    input_data = {}\n    input_data_copy = {}\n    for ip in shapes:\n        input_data[ip] = _np.random.rand(*shapes[ip]).astype('f')\n        input_data_copy[ip] = _np.copy(input_data[ip])\n\n    dataIter = _mxnet.io.NDArrayIter(input_data_copy)\n    mx_out = model.predict(dataIter).flatten()\n\n    e_out_dict = coreml_model.predict(_mxnet_remove_batch(input_data))\n    e_out = e_out_dict[output].flatten()\n    error = _np.linalg.norm(e_out - mx_out)\n\n    if verbose:\n        print(\"First few predictions from CoreML : %s\" % e_out[0:10])\n        print(\"First few predictions from MXNet  : %s\" % e_out[0:10])\n        print(\"L2 Error on random data %s\" % error)\n    return error", "response": "Check the difference between predictions from MXNet and CoreML."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef char_beam_search(out):\n    out_conv = list()\n    for idx in range(out.shape[0]):\n        probs = out[idx]\n        prob = probs.softmax().asnumpy()\n        line_string_proposals = ctcBeamSearch(prob, ALPHABET, None, k=4, beamWidth=25)\n        out_conv.append(line_string_proposals[0])\n    return out_conv", "response": "Description : apply beam search for prediction result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build_model(self, dr_rate=0, path=None):\n        #set network\n        self.net = LipNet(dr_rate)\n        self.net.hybridize()\n        self.net.initialize(ctx=self.ctx)\n\n        if path is not None:\n            self.load_model(path)\n\n        #set optimizer\n        self.loss_fn = gluon.loss.CTCLoss()\n        self.trainer = gluon.Trainer(self.net.collect_params(), \\\n                                     optimizer='SGD')", "response": "Description : build network and optimizer"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef save_model(self, epoch, loss):\n        prefix = 'checkpoint/epoches'\n        file_name = \"{prefix}_{epoch}_loss_{l:.4f}\".format(prefix=prefix,\n                                                           epoch=str(epoch),\n                                                           l=loss)\n        self.net.save_parameters(file_name)", "response": "Description : save parameter of network weight"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef train(self, data, label, batch_size):\n        # pylint: disable=no-member\n        sum_losses = 0\n        len_losses = 0\n        with autograd.record():\n            losses = [self.loss_fn(self.net(X), Y) for X, Y in zip(data, label)]\n        for loss in losses:\n            sum_losses += mx.nd.array(loss).sum().asscalar()\n            len_losses += len(loss)\n            loss.backward()\n        self.trainer.step(batch_size)\n        return sum_losses, len_losses", "response": "Train LipNet with data and label."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef train_batch(self, dataloader):\n        sum_losses = 0\n        len_losses = 0\n        for input_data, input_label in tqdm(dataloader):\n            data = gluon.utils.split_and_load(input_data, self.ctx, even_split=False)\n            label = gluon.utils.split_and_load(input_label, self.ctx, even_split=False)\n            batch_size = input_data.shape[0]\n            sum_losses, len_losses = self.train(data, label, batch_size)\n            sum_losses += sum_losses\n            len_losses += len_losses\n\n        return sum_losses, len_losses", "response": "Train LipNet with batched data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef infer_batch(self, dataloader):\n        sum_losses = 0\n        len_losses = 0\n        for input_data, input_label in dataloader:\n            data = gluon.utils.split_and_load(input_data, self.ctx, even_split=False)\n            label = gluon.utils.split_and_load(input_label, self.ctx, even_split=False)\n            sum_losses, len_losses = self.infer(data, label)\n            sum_losses += sum_losses\n            len_losses += len_losses\n\n        return sum_losses, len_losses", "response": "Infer the batch of LipNet data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run(self, epochs):\n        best_loss = sys.maxsize\n        for epoch in trange(epochs):\n            iter_no = 0\n\n            ## train\n            sum_losses, len_losses = self.train_batch(self.train_dataloader)\n\n            if iter_no % 20 == 0:\n                current_loss = sum_losses / len_losses\n                print(\"[Train] epoch:{e} iter:{i} loss:{l:.4f}\".format(e=epoch,\n                                                                       i=iter_no,\n                                                                       l=current_loss))\n\n            ## validating\n            sum_val_losses, len_val_losses = self.infer_batch(self.valid_dataloader)\n\n            current_val_loss = sum_val_losses / len_val_losses\n            print(\"[Vaild] epoch:{e} iter:{i} loss:{l:.4f}\".format(e=epoch,\n                                                                   i=iter_no,\n                                                                   l=current_val_loss))\n\n            if best_loss > current_val_loss:\n                self.save_model(epoch, current_val_loss)\n                best_loss = current_val_loss\n\n            iter_no += 1", "response": "Run training and validation for LipNet\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsamples from independent categorical distributions Each batch is an independent categorical distribution.", "response": "def sample_categorical(prob, rng):\n    \"\"\"Sample from independent categorical distributions\n\n    Each batch is an independent categorical distribution.\n\n    Parameters\n    ----------\n    prob : numpy.ndarray\n      Probability of the categorical distribution. Shape --> (batch_num, category_num)\n    rng : numpy.random.RandomState\n\n    Returns\n    -------\n    ret : numpy.ndarray\n      Sampling result. Shape --> (batch_num,)\n    \"\"\"\n    ret = numpy.empty(prob.shape[0], dtype=numpy.float32)\n    for ind in range(prob.shape[0]):\n        ret[ind] = numpy.searchsorted(numpy.cumsum(prob[ind]), rng.rand()).clip(min=0.0,\n                                                                                max=prob.shape[\n                                                                                        1] - 0.5)\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsampling from independent normal distributions Each element is an independent normal distribution.", "response": "def sample_normal(mean, var, rng):\n    \"\"\"Sample from independent normal distributions\n\n    Each element is an independent normal distribution.\n\n    Parameters\n    ----------\n    mean : numpy.ndarray\n      Means of the normal distribution. Shape --> (batch_num, sample_dim)\n    var : numpy.ndarray\n      Variance of the normal distribution. Shape --> (batch_num, sample_dim)\n    rng : numpy.random.RandomState\n\n    Returns\n    -------\n    ret : numpy.ndarray\n       The sampling result. Shape --> (batch_num, sample_dim)\n    \"\"\"\n    ret = numpy.sqrt(var) * rng.randn(*mean.shape) + mean\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsample from independent mixture of gaussian distributions", "response": "def sample_mog(prob, mean, var, rng):\n    \"\"\"Sample from independent mixture of gaussian (MoG) distributions\n\n    Each batch is an independent MoG distribution.\n\n    Parameters\n    ----------\n    prob : numpy.ndarray\n      mixture probability of each gaussian. Shape --> (batch_num, center_num)\n    mean : numpy.ndarray\n      mean of each gaussian. Shape --> (batch_num, center_num, sample_dim)\n    var : numpy.ndarray\n      variance of each gaussian. Shape --> (batch_num, center_num, sample_dim)\n    rng : numpy.random.RandomState\n\n    Returns\n    -------\n    ret : numpy.ndarray\n      sampling result. Shape --> (batch_num, sample_dim)\n    \"\"\"\n    gaussian_inds = sample_categorical(prob, rng).astype(numpy.int32)\n    mean = mean[numpy.arange(mean.shape[0]), gaussian_inds, :]\n    var = var[numpy.arange(mean.shape[0]), gaussian_inds, :]\n    ret = sample_normal(mean=mean, var=var, rng=rng)\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef nce_loss_subwords(\n        data, label, label_mask, label_weight, embed_weight, vocab_size, num_hidden):\n    \"\"\"NCE-Loss layer under subword-units input.\n    \"\"\"\n    # get subword-units embedding.\n    label_units_embed = mx.sym.Embedding(data=label,\n                                         input_dim=vocab_size,\n                                         weight=embed_weight,\n                                         output_dim=num_hidden)\n    # get valid subword-units embedding with the help of label_mask\n    # it's achieved by multiplying zeros to useless units in order to handle variable-length input.\n    label_units_embed = mx.sym.broadcast_mul(lhs=label_units_embed,\n                                             rhs=label_mask,\n                                             name='label_units_embed')\n    # sum over them to get label word embedding.\n    label_embed = mx.sym.sum(label_units_embed, axis=2, name='label_embed')\n\n    # by boardcast_mul and sum you can get prediction scores in all label_embed inputs,\n    # which is easy to feed into LogisticRegressionOutput and make your code more concise.\n    data = mx.sym.Reshape(data=data, shape=(-1, 1, num_hidden))\n    pred = mx.sym.broadcast_mul(data, label_embed)\n    pred = mx.sym.sum(data=pred, axis=2)\n\n    return mx.sym.LogisticRegressionOutput(data=pred,\n                                           label=label_weight)", "response": "NCE - Loss layer under subword - units input."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndownloading the BSDS500 dataset and return train and test iters.", "response": "def get_dataset(prefetch=False):\n    \"\"\"Download the BSDS500 dataset and return train and test iters.\"\"\"\n\n    if path.exists(data_dir):\n        print(\n            \"Directory {} already exists, skipping.\\n\"\n            \"To force download and extraction, delete the directory and re-run.\"\n            \"\".format(data_dir),\n            file=sys.stderr,\n        )\n    else:\n        print(\"Downloading dataset...\", file=sys.stderr)\n        downloaded_file = download(dataset_url, dirname=datasets_tmpdir)\n        print(\"done\", file=sys.stderr)\n\n        print(\"Extracting files...\", end=\"\", file=sys.stderr)\n        os.makedirs(data_dir)\n        os.makedirs(tmp_dir)\n        with zipfile.ZipFile(downloaded_file) as archive:\n            archive.extractall(tmp_dir)\n        shutil.rmtree(datasets_tmpdir)\n\n        shutil.copytree(\n            path.join(tmp_dir, \"BSDS500-master\", \"BSDS500\", \"data\", \"images\"),\n            path.join(data_dir, \"images\"),\n        )\n        shutil.copytree(\n            path.join(tmp_dir, \"BSDS500-master\", \"BSDS500\", \"data\", \"groundTruth\"),\n            path.join(data_dir, \"groundTruth\"),\n        )\n        shutil.rmtree(tmp_dir)\n        print(\"done\", file=sys.stderr)\n\n    crop_size = 256\n    crop_size -= crop_size % upscale_factor\n    input_crop_size = crop_size // upscale_factor\n\n    input_transform = [CenterCropAug((crop_size, crop_size)), ResizeAug(input_crop_size)]\n    target_transform = [CenterCropAug((crop_size, crop_size))]\n\n    iters = (\n        ImagePairIter(\n            path.join(data_dir, \"images\", \"train\"),\n            (input_crop_size, input_crop_size),\n            (crop_size, crop_size),\n            batch_size,\n            color_flag,\n            input_transform,\n            target_transform,\n        ),\n        ImagePairIter(\n            path.join(data_dir, \"images\", \"test\"),\n            (input_crop_size, input_crop_size),\n            (crop_size, crop_size),\n            test_batch_size,\n            color_flag,\n            input_transform,\n            target_transform,\n        ),\n    )\n\n    return [PrefetchingIter(i) for i in iters] if prefetch else iters"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nevaluate the model on the given data iterator.", "response": "def evaluate(mod, data_iter, epoch, log_interval):\n    \"\"\" Run evaluation on cpu. \"\"\"\n    start = time.time()\n    total_L = 0.0\n    nbatch = 0\n    density = 0\n    mod.set_states(value=0)\n    for batch in data_iter:\n        mod.forward(batch, is_train=False)\n        outputs = mod.get_outputs(merge_multi_context=False)\n        states = outputs[:-1]\n        total_L += outputs[-1][0]\n        mod.set_states(states=states)\n        nbatch += 1\n        # don't include padding data in the test perplexity\n        density += batch.data[1].mean()\n        if (nbatch + 1) % log_interval == 0:\n            logging.info(\"Eval batch %d loss : %.7f\" % (nbatch, (total_L / density).asscalar()))\n    data_iter.reset()\n    loss = (total_L / density).asscalar()\n    ppl = math.exp(loss) if loss < 100 else 1e37\n    end = time.time()\n    logging.info('Iter[%d]\\t\\t CE loss %.7f, ppl %.7f. Eval duration = %.2f seconds ' % \\\n                 (epoch, loss, ppl, end - start))\n    return loss"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _read(self):\n        _, data_img_name, label_img_name = self.f.readline().strip('\\n').split(\"\\t\")\n        data = {}\n        label = {}\n        data[self.data_name], label[self.label_name] = self._read_img(data_img_name, label_img_name)\n        return list(data.items()), list(label.items())", "response": "get two list each list contains two elements name nd. array value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef next(self):\n        if self.iter_next():\n            self.data, self.label = self._read()\n            return {self.data_name  :  self.data[0][1],\n                    self.label_name :  self.label[0][1]}\n        else:\n            raise StopIteration", "response": "return one dict which contains data and label"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting from onnx operator to mxnet operator.", "response": "def _convert_operator(self, node_name, op_name, attrs, inputs):\n        \"\"\"Convert from onnx operator to mxnet operator.\n        The converter must specify conversions explicitly for incompatible name, and\n        apply handlers to operator attributes.\n\n        Parameters\n        ----------\n        :param node_name : str\n            name of the node to be translated.\n        :param op_name : str\n            Operator name, such as Convolution, FullyConnected\n        :param attrs : dict\n            Dict of operator attributes\n        :param inputs: list\n            list of inputs to the operator\n        Returns\n        -------\n        :return mxnet_sym\n            Converted mxnet symbol\n        \"\"\"\n        if op_name in convert_map:\n            op_name, new_attrs, inputs = convert_map[op_name](attrs, inputs, self)\n        else:\n            raise NotImplementedError(\"Operator {} not implemented.\".format(op_name))\n        if isinstance(op_name, string_types):\n            new_op = getattr(symbol, op_name, None)\n            if not new_op:\n                raise RuntimeError(\"Unable to map op_name {} to sym\".format(op_name))\n            if node_name is None:\n                mxnet_sym = new_op(*inputs, **new_attrs)\n            else:\n                mxnet_sym = new_op(name=node_name, *inputs, **new_attrs)\n            return mxnet_sym\n        return op_name"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconstruct a new symbol from onnx protobuf object.", "response": "def from_onnx(self, graph):\n        \"\"\"Construct symbol from onnx graph.\n\n        Parameters\n        ----------\n        graph : onnx protobuf object\n            The loaded onnx graph\n\n        Returns\n        -------\n        sym :symbol.Symbol\n            The returned mxnet symbol\n        params : dict\n            A dict of name: nd.array pairs, used as pretrained weights\n        \"\"\"\n        # get input, output shapes\n        self.model_metadata = self.get_graph_metadata(graph)\n        # parse network inputs, aka parameters\n        for init_tensor in graph.initializer:\n            if not init_tensor.name.strip():\n                raise ValueError(\"Tensor's name is required.\")\n            self._params[init_tensor.name] = self._parse_array(init_tensor)\n\n        # converting GraphProto message\n        for i in graph.input:\n            if i.name in self._params:\n                # i is a param instead of input\n                self._nodes[i.name] = symbol.Variable(name=i.name,\n                                                      shape=self._params[i.name].shape)\n            else:\n                self._nodes[i.name] = symbol.Variable(name=i.name)\n\n        # constructing nodes, nodes are stored as directed acyclic graph\n        # converting NodeProto message\n        for node in graph.node:\n            op_name = node.op_type\n            node_name = node.name.strip()\n            node_name = node_name if node_name else None\n            onnx_attr = self._parse_attr(node.attribute)\n            inputs = [self._nodes[i] for i in node.input]\n            mxnet_sym = self._convert_operator(node_name, op_name, onnx_attr, inputs)\n\n            for k, i in zip(list(node.output), range(len(mxnet_sym.list_outputs()))):\n                self._nodes[k] = mxnet_sym[i]\n\n            # splitting params into args and aux params\n            for args in mxnet_sym.list_arguments():\n                if args in self._params:\n                    self.arg_dict.update({args: nd.array(self._params[args])})\n            for aux in mxnet_sym.list_auxiliary_states():\n                if aux in self._params:\n                    self.aux_dict.update({aux: nd.array(self._params[aux])})\n\n        # now return the outputs\n        out = [self._nodes[i.name] for i in graph.output]\n        if len(out) > 1:\n            out = symbol.Group(out)\n        else:\n            out = out[0]\n        return out, self.arg_dict, self.aux_dict"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the model metadata from a given onnx graph.", "response": "def get_graph_metadata(self, graph):\n        \"\"\"\n        Get the model metadata from a given onnx graph.\n        \"\"\"\n        _params = set()\n        for tensor_vals in graph.initializer:\n            _params.add(tensor_vals.name)\n\n        input_data = []\n        for graph_input in graph.input:\n            if graph_input.name not in _params:\n                shape = [val.dim_value for val in graph_input.type.tensor_type.shape.dim]\n                input_data.append((graph_input.name, tuple(shape)))\n\n        output_data = []\n        for graph_out in graph.output:\n            shape = [val.dim_value for val in graph_out.type.tensor_type.shape.dim]\n            output_data.append((graph_out.name, tuple(shape)))\n        metadata = {'input_tensor_data' : input_data,\n                    'output_tensor_data' : output_data\n                   }\n        return metadata"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef graph_to_gluon(self, graph, ctx):\n        sym, arg_params, aux_params = self.from_onnx(graph)\n        metadata = self.get_graph_metadata(graph)\n        data_names = [input_tensor[0] for input_tensor in metadata['input_tensor_data']]\n        data_inputs = [symbol.var(data_name) for data_name in data_names]\n\n        from ....gluon import SymbolBlock\n        net = SymbolBlock(outputs=sym, inputs=data_inputs)\n        net_params = net.collect_params()\n        for param in arg_params:\n            if param in net_params:\n                net_params[param].shape = arg_params[param].shape\n                net_params[param]._load_init(arg_params[param], ctx=ctx)\n        for param in aux_params:\n            if param in net_params:\n                net_params[param].shape = aux_params[param].shape\n                net_params[param]._load_init(aux_params[param], ctx=ctx)\n        return net", "response": "Construct a symbol block from onnx protobuf graph."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _parse_array(self, tensor_proto):\n        try:\n            from onnx.numpy_helper import to_array\n        except ImportError:\n            raise ImportError(\"Onnx and protobuf need to be installed. \"\n                              + \"Instructions to install - https://github.com/onnx/onnx\")\n        if len(tuple(tensor_proto.dims)) > 0:\n            np_array = to_array(tensor_proto).reshape(tuple(tensor_proto.dims))\n        else:\n            # If onnx's params are scalar values without dims mentioned.\n            np_array = np.array([to_array(tensor_proto)])\n        return nd.array(np_array)", "response": "Grab data in TensorProto and convert to numpy array."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a list of AttributeProto to a dict with names as keys.", "response": "def _parse_attr(self, attr_proto):\n        \"\"\"Convert a list of AttributeProto to a dict, with names as keys.\"\"\"\n        attrs = {}\n        for a in attr_proto:\n            for f in ['f', 'i', 's']:\n                if a.HasField(f):\n                    attrs[a.name] = getattr(a, f)\n                    # Needed for supporting python version  > 3.5\n                    if isinstance(attrs[a.name], bytes):\n                        attrs[a.name] = attrs[a.name].decode(encoding='utf-8')\n            for f in ['floats', 'ints', 'strings']:\n                if list(getattr(a, f)):\n                    assert a.name not in attrs, \"Only one type of attr is allowed\"\n                    attrs[a.name] = tuple(getattr(a, f))\n            for f in ['t', 'g']:\n                if a.HasField(f):\n                    attrs[a.name] = getattr(a, f)\n            for f in ['tensors', 'graphs']:\n                if list(getattr(a, f)):\n                    raise NotImplementedError(\"Filed {} is not supported in mxnet.\".format(f))\n            if a.name not in attrs:\n                raise ValueError(\"Cannot parse attribute: \\n{}\\n.\".format(a))\n        return attrs"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reshape(self, data_shapes, label_shapes=None):\n        super(SVRGModule, self).reshape(data_shapes, label_shapes=label_shapes)\n        self._mod_aux.reshape(data_shapes, label_shapes=label_shapes)", "response": "Reshapes both modules for new input shapes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninstalling and initializes SVRGOptimizer.", "response": "def init_optimizer(self, kvstore='local', optimizer='sgd',\n                       optimizer_params=(('learning_rate', 0.01),), force_init=False):\n        \"\"\"Installs and initializes SVRGOptimizer. The SVRGOptimizer is a wrapper class for a regular optimizer that is\n        passed in and a special AssignmentOptimizer to accumulate the full gradients.  If KVStore is 'local' or None,\n        the full gradients will be accumulated locally without pushing to the KVStore. Otherwise, additional keys will\n        be pushed to accumulate the full gradients in the KVStore.\n\n        Parameters\n        ----------\n        kvstore : str or KVStore\n            Default `'local'`.\n        optimizer : str or Optimizer\n            Default `'sgd'`\n        optimizer_params : dict\n            Default `(('learning_rate', 0.01),)`. The default value is not a dictionary,\n            just to avoid pylint warning of dangerous default values.\n        force_init : bool\n            Default ``False``, indicating whether we should force re-initializing the\n            optimizer in the case an optimizer is already installed.\n        \"\"\"\n\n        # Init dict for storing average of full gradients for each device\n        self._param_dict = [{key: mx.nd.zeros(shape=value.shape, ctx=self._context[i])\n                             for key, value in self.get_params()[0].items()} for i in range(self._ctx_len)]\n\n        svrg_optimizer = self._create_optimizer(_SVRGOptimizer.__name__, default_opt=optimizer,\n                                                kvstore=kvstore, optimizer_params=optimizer_params)\n\n        super(SVRGModule, self).init_optimizer(kvstore=kvstore, optimizer=svrg_optimizer,\n                                               optimizer_params=optimizer_params, force_init=force_init)\n\n        # Init additional keys for accumulating full grads in KVStore\n        if self._kvstore:\n            for idx, param_on_devs in enumerate(self._exec_group.param_arrays):\n                name = self._exec_group.param_names[idx]\n                self._kvstore.init(name + \"_full\", mx.nd.zeros(shape=self._arg_params[name].shape))\n                if self._update_on_kvstore:\n                    self._kvstore.pull(name + \"_full\", param_on_devs, priority=-idx)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbind the symbols to construct executors for both two modules.", "response": "def bind(self, data_shapes, label_shapes=None, for_training=True,\n             inputs_need_grad=False, force_rebind=False, shared_module=None, grad_req='write'):\n        \"\"\"Binds the symbols to construct executors for both two modules. This is necessary before one\n        can perform computation with the SVRGModule.\n\n        Parameters\n        ----------\n        data_shapes : list of (str, tuple)\n            Typically is ``data_iter.provide_data``.\n        label_shapes : list of (str, tuple)\n            Typically is ``data_iter.provide_label``.\n        for_training : bool\n            Default is ``True``. Whether the executors should be bound for training.\n        inputs_need_grad : bool\n            Default is ``False``. Whether the gradients to the input data need to be computed.\n            Typically this is not needed. But this might be needed when implementing composition\n            of modules.\n        force_rebind : bool\n            Default is ``False``. This function does nothing if the executors are already\n            bound. But with this ``True``, the executors will be forced to rebind.\n        shared_module : Module\n            Default is ``None``. This is used in bucketing. When not ``None``, the shared module\n            essentially corresponds to a different bucket -- a module with different symbol\n            but with the same sets of parameters (e.g. unrolled RNNs with different lengths).\n        \"\"\"\n        # force rebinding is typically used when one want to switch from\n        # training to prediction phase.\n        super(SVRGModule, self).bind(data_shapes, label_shapes, for_training, inputs_need_grad, force_rebind,\n                                     shared_module, grad_req)\n\n        if for_training:\n            self._mod_aux.bind(data_shapes, label_shapes, for_training, inputs_need_grad, force_rebind, shared_module,\n                               grad_req)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nforward computation for both two modules.", "response": "def forward(self, data_batch, is_train=None):\n        \"\"\"Forward computation for both two modules. It supports data batches with different shapes, such as\n        different batch sizes or different image sizes.\n        If reshaping of data batch relates to modification of symbol or module, such as\n        changing image layout ordering or switching from training to predicting, module\n        rebinding is required.\n\n        See Also\n        ----------\n        :meth:`BaseModule.forward`.\n\n        Parameters\n        ----------\n        data_batch : DataBatch\n            Could be anything with similar API implemented.\n        is_train : bool\n            Default is ``None``, which means ``is_train`` takes the value of ``self.for_training``.\n        \"\"\"\n        super(SVRGModule, self).forward(data_batch, is_train)\n\n        if is_train:\n            self._mod_aux.forward(data_batch, is_train)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates the gradients of all data w. r. t weights of past m epochs.", "response": "def update_full_grads(self, train_data):\n        \"\"\"Computes the gradients over all data w.r.t weights of past\n        m epochs. For distributed env, it will accumulate full grads in the kvstore.\n\n        Parameters\n        ----------\n        train_data: DataIter\n            Train data iterator\n        \"\"\"\n        param_names = self._exec_group.param_names\n        arg, aux = self.get_params()\n        self._mod_aux.set_params(arg_params=arg, aux_params=aux)\n        train_data.reset()\n        nbatch = 0\n        padding = 0\n        for batch in train_data:\n            self._mod_aux.forward(batch, is_train=True)\n            self._mod_aux.backward()\n            nbatch += 1\n            for ctx in range(self._ctx_len):\n                for index, name in enumerate(param_names):\n                    grads = self._mod_aux._exec_group.grad_arrays[index][ctx]\n                    self._param_dict[ctx][name] = mx.nd.broadcast_add(self._param_dict[ctx][name], grads, axis=0)\n            padding = batch.pad\n\n        true_num_batch = nbatch - padding / train_data.batch_size\n        for name in param_names:\n            grad_list = []\n            for i in range(self._ctx_len):\n                self._param_dict[i][name] /= true_num_batch\n                grad_list.append(self._param_dict[i][name])\n            if self._kvstore:\n                # If in distributed mode, push a list of gradients from each worker/device to the KVStore\n                self._accumulate_kvstore(name, grad_list)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _accumulate_kvstore(self, key, value):\n        # Accumulate full gradients for current epochs\n        self._kvstore.push(key + \"_full\", value)\n        self._kvstore._barrier()\n        self._kvstore.pull(key + \"_full\", value)\n\n        self._allocate_gradients(key, value)", "response": "Accumulate gradients over all data in the KVStore."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _allocate_gradients(self, key, value):\n        for i in range(self._ctx_len):\n            self._param_dict[i][key] = value[i] / self._ctx_len", "response": "Allocate average of full gradients accumulated in the KVStore to each device."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate the gradient of the SVRG update rule.", "response": "def _svrg_grads_update_rule(self, g_curr_batch_curr_weight, g_curr_batch_special_weight,\n                                g_special_weight_all_batch):\n        \"\"\"Calculates the gradient based on the SVRG update rule.\n        Parameters\n        ----------\n        g_curr_batch_curr_weight : NDArray\n            gradients of current weight of self.mod w.r.t current batch of data\n        g_curr_batch_special_weight: NDArray\n            gradients of the weight of past m epochs of self._mod_special w.r.t current batch of data\n        g_special_weight_all_batch: NDArray\n            average of full gradients over full pass of data\n\n        Returns\n        ----------\n        Gradients calculated using SVRG update rule:\n        grads = g_curr_batch_curr_weight - g_curr_batch_special_weight + g_special_weight_all_batch\n        \"\"\"\n        for index, grad in enumerate(g_curr_batch_curr_weight):\n            grad -= g_curr_batch_special_weight[index]\n            grad += g_special_weight_all_batch[index]\n        return g_curr_batch_curr_weight"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _update_svrg_gradients(self):\n        param_names = self._exec_group.param_names\n        for ctx in range(self._ctx_len):\n            for index, name in enumerate(param_names):\n                g_curr_batch_reg = self._exec_group.grad_arrays[index][ctx]\n                g_curr_batch_special = self._mod_aux._exec_group.grad_arrays[index][ctx]\n                g_special_weight_all_batch = self._param_dict[ctx][name]\n                g_svrg = self._svrg_grads_update_rule(g_curr_batch_reg, g_curr_batch_special,\n                                                      g_special_weight_all_batch)\n                self._exec_group.grad_arrays[index][ctx] = g_svrg", "response": "Calculates gradients based on the SVRG update rule."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfits the module to the data.", "response": "def fit(self, train_data, eval_data=None, eval_metric='acc',\n            epoch_end_callback=None, batch_end_callback=None, kvstore='local',\n            optimizer='sgd', optimizer_params=(('learning_rate', 0.01),),\n            eval_end_callback=None,\n            eval_batch_end_callback=None, initializer=mx.init.Uniform(0.01),\n            arg_params=None, aux_params=None, allow_missing=False,\n            force_rebind=False, force_init=False, begin_epoch=0, num_epoch=None,\n            validation_metric=None, monitor=None, sparse_row_id_fn=None):\n        \"\"\"Trains the module parameters.\n\n        Parameters\n        ----------\n        train_data : DataIter\n            Train DataIter.\n        eval_data : DataIter\n            If not ``None``, will be used as validation set and the performance\n            after each epoch will be evaluated.\n        eval_metric : str or EvalMetric\n            Defaults to 'accuracy'. The performance measure used to display during training.\n            Other possible predefined metrics are:\n            'ce' (CrossEntropy), 'f1', 'mae', 'mse', 'rmse', 'top_k_accuracy'.\n        epoch_end_callback : function or list of functions\n            Each callback will be called with the current `epoch`, `symbol`, `arg_params`\n            and `aux_params`.\n        batch_end_callback : function or list of function\n            Each callback will be called with a `BatchEndParam`.\n        kvstore : str or KVStore\n            Defaults to 'local'.\n        optimizer : str or Optimizer\n            Defaults to 'sgd'.\n        optimizer_params : dict\n            Defaults to ``(('learning_rate', 0.01),)``. The parameters for\n            the optimizer constructor.\n            The default value is not a dict, just to avoid pylint warning on dangerous\n            default values.\n        eval_end_callback : function or list of function\n            These will be called at the end of each full evaluation, with the metrics over\n            the entire evaluation set.\n        eval_batch_end_callback : function or list of function\n            These will be called at the end of each mini-batch during evaluation.\n        initializer : Initializer\n            The initializer is called to initialize the module parameters when they are\n            not already initialized.\n        arg_params : dict\n            Defaults to ``None``, if not ``None``, should be existing parameters from a trained\n            model or loaded from a checkpoint (previously saved model). In this case,\n            the value here will be used to initialize the module parameters, unless they\n            are already initialized by the user via a call to `init_params` or `fit`.\n            `arg_params` has a higher priority than `initializer`.\n        aux_params : dict\n            Defaults to ``None``. Similar to `arg_params`, except for auxiliary states.\n        allow_missing : bool\n            Defaults to ``False``. Indicates whether to allow missing parameters when `arg_params`\n            and `aux_params` are not ``None``. If this is ``True``, then the missing parameters\n            will be initialized via the `initializer`.\n        force_rebind : bool\n            Defaults to ``False``. Whether to force rebinding the executors if already bound.\n        force_init : bool\n            Defaults to ``False``. Indicates whether to force initialization even if the\n            parameters are already initialized.\n        begin_epoch : int\n            Defaults to 0. Indicates the starting epoch. Usually, if resumed from a\n            checkpoint saved at a previous training phase at epoch N, then this value should be\n            N+1.\n        num_epoch : int\n            Number of epochs for training.\n        sparse_row_id_fn : A callback function\n            The function  takes `data_batch` as an input and returns a dict of\n            str -> NDArray. The resulting dict is used for pulling row_sparse\n            parameters from the kvstore, where the str key is the name of the param,\n            and the value is the row id of the param to pull.\n        validation_metric: str or EvalMetric\n            The performance measure used to display during validation.\n        \"\"\"\n        assert num_epoch is not None, 'please specify number of epochs'\n\n        self.bind(data_shapes=train_data.provide_data, label_shapes=train_data.provide_label,\n                  for_training=True, force_rebind=force_rebind)\n        if monitor is not None:\n            self.install_monitor(monitor)\n        self.init_params(initializer=initializer, arg_params=arg_params, aux_params=aux_params,\n                         allow_missing=allow_missing, force_init=force_init)\n        self.init_optimizer(kvstore=kvstore, optimizer=optimizer, optimizer_params=optimizer_params)\n\n        if validation_metric is None:\n            validation_metric = eval_metric\n        if not isinstance(eval_metric, mx.metric.EvalMetric):\n            eval_metric = mx.metric.create(eval_metric)\n\n        ################################################################################\n        # training loop\n        ################################################################################\n        for epoch in range(begin_epoch, num_epoch):\n            eval_metric.reset()\n            tic = time.time()\n            if epoch % self.update_freq == 0:\n                self.update_full_grads(train_data)\n\n            train_data.reset()\n            data_iter = iter(train_data)\n            end_of_batch = False\n            nbatch = 0\n            next_data_batch = next(data_iter)\n\n            while not end_of_batch:\n                data_batch = next_data_batch\n                if monitor is not None:\n                    monitor.tic()\n\n                self.forward_backward(data_batch)\n                self.update()\n\n                if isinstance(data_batch, list):\n                    self.update_metric(eval_metric, [db.label for db in data_batch], pre_sliced=True)\n                else:\n                    self.update_metric(eval_metric, data_batch.label)\n\n                try:\n                    # pre fetch next batch\n                    next_data_batch = next(data_iter)\n                    self.prepare(next_data_batch, sparse_row_id_fn=sparse_row_id_fn)\n                except StopIteration:\n                    end_of_batch = True\n\n                if monitor is not None:\n                    monitor.toc_print()\n\n                if end_of_batch:\n                    eval_name_vals = eval_metric.get_name_value()\n\n                if batch_end_callback is not None:\n                    batch_end_params = mx.model.BatchEndParam(epoch=epoch, nbatch=nbatch,\n                                                              eval_metric=eval_metric, locals=locals())\n                    for callback in mx.base._as_list(batch_end_callback):\n                        callback(batch_end_params)\n\n                nbatch += 1\n            for name, val in eval_name_vals:\n                self.logger.info('Epoch[%d] Train-%s=%f', epoch, name, val)\n            toc = time.time()\n            self.logger.info('Epoch[%d] Time cost=%.3f', epoch, (toc - tic))\n\n            # sync aux params across devices\n            arg_params, aux_params = self.get_params()\n            self.set_params(arg_params, aux_params)\n\n            if epoch_end_callback is not None:\n                for callback in mx.base._as_list(epoch_end_callback):\n                    callback(epoch, self.symbol, arg_params, aux_params)\n\n            # ----------------------------------------\n            # evaluation on validation set\n            if eval_data:\n                res = self.score(eval_data, validation_metric,\n                                 score_end_callback=eval_end_callback,\n                                 batch_end_callback=eval_batch_end_callback, epoch=epoch)\n                for name, val in res:\n                    self.logger.info('Epoch[%d] Validation-%s=%f', epoch, name, val)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprepare two modules for processing a data batch.", "response": "def prepare(self, data_batch, sparse_row_id_fn=None):\n        \"\"\"Prepares two modules for processing a data batch.\n\n        Usually involves switching bucket and reshaping.\n        For modules that contain `row_sparse` parameters in KVStore,\n        it prepares the `row_sparse` parameters based on the sparse_row_id_fn.\n\n        When KVStore is used to update parameters for multi-device or multi-machine training,\n        a copy of the parameters are stored in KVStore. Note that for `row_sparse` parameters,\n        the `update()` updates the copy of parameters in KVStore, but doesn't broadcast\n        the updated parameters to all devices / machines. The `prepare` function is used to\n        broadcast `row_sparse` parameters with the next batch of data.\n\n        Parameters\n        ----------\n        data_batch : DataBatch\n            The current batch of data for forward computation.\n\n        sparse_row_id_fn : A callback function\n            The function  takes `data_batch` as an input and returns a dict of\n            str -> NDArray. The resulting dict is used for pulling row_sparse\n            parameters from the kvstore, where the str key is the name of the param,\n            and the value is the row id of the param to pull.\n        \"\"\"\n        super(SVRGModule, self).prepare(data_batch, sparse_row_id_fn=sparse_row_id_fn)\n        self._mod_aux.prepare(data_batch, sparse_row_id_fn=sparse_row_id_fn)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading the image set index from the file.", "response": "def _load_image_set_index(self, shuffle):\n        \"\"\"\n        find out which indexes correspond to given image set (train or val)\n\n        Parameters:\n        ----------\n        shuffle : boolean\n            whether to shuffle the image list\n        Returns:\n        ----------\n        entire list of images specified in the setting\n        \"\"\"\n        assert os.path.exists(self.list_file), 'Path does not exists: {}'.format(self.list_file)\n        with open(self.list_file, 'r') as f:\n            image_set_index = [x.strip() for x in f.readlines()]\n        if shuffle:\n            np.random.shuffle(image_set_index)\n        return image_set_index"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives an index find out the full path of the label file.", "response": "def _label_path_from_index(self, index):\n        \"\"\"\n        given image index, find out annotation path\n\n        Parameters:\n        ----------\n        index: int\n            index of a specific image\n\n        Returns:\n        ----------\n        full path of annotation file\n        \"\"\"\n        label_file = os.path.join(self.label_dir, index + self.label_extension)\n        assert os.path.exists(label_file), 'Path does not exist: {}'.format(label_file)\n        return label_file"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _load_image_labels(self):\n        temp = []\n\n        # load ground-truths\n        for idx in self.image_set_index:\n            label_file = self._label_path_from_index(idx)\n            with open(label_file, 'r') as f:\n                label = []\n                for line in f.readlines():\n                    temp_label = line.strip().split()\n                    assert len(temp_label) == 5, \"Invalid label file\" + label_file\n                    cls_id = int(temp_label[0])\n                    x = float(temp_label[1])\n                    y = float(temp_label[2])\n                    half_width = float(temp_label[3]) / 2\n                    half_height = float(temp_label[4]) / 2\n                    xmin = x - half_width\n                    ymin = y - half_height\n                    xmax = x + half_width\n                    ymax = y + half_height\n                    label.append([cls_id, xmin, ymin, xmax, ymax])\n                temp.append(np.array(label))\n        return temp", "response": "Load all ground - truths and labels into a list of numpy arrays."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting registrator function. Parameters ---------- base_class : type base class for classes that will be reigstered nickname : str nickname of base_class for logging Returns ------- a registrator function", "response": "def get_register_func(base_class, nickname):\n    \"\"\"Get registrator function.\n\n    Parameters\n    ----------\n    base_class : type\n        base class for classes that will be reigstered\n    nickname : str\n        nickname of base_class for logging\n\n    Returns\n    -------\n    a registrator function\n    \"\"\"\n    if base_class not in _REGISTRY:\n        _REGISTRY[base_class] = {}\n    registry = _REGISTRY[base_class]\n\n    def register(klass, name=None):\n        \"\"\"Register functions\"\"\"\n        assert issubclass(klass, base_class), \\\n            \"Can only register subclass of %s\"%base_class.__name__\n        if name is None:\n            name = klass.__name__\n        name = name.lower()\n        if name in registry:\n            warnings.warn(\n                \"\\033[91mNew %s %s.%s registered with name %s is\"\n                \"overriding existing %s %s.%s\\033[0m\"%(\n                    nickname, klass.__module__, klass.__name__, name,\n                    nickname, registry[name].__module__, registry[name].__name__),\n                UserWarning, stacklevel=2)\n        registry[name] = klass\n        return klass\n\n    register.__doc__ = \"Register %s to the %s factory\"%(nickname, nickname)\n    return register"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_alias_func(base_class, nickname):\n    register = get_register_func(base_class, nickname)\n\n    def alias(*aliases):\n        \"\"\"alias registrator\"\"\"\n        def reg(klass):\n            \"\"\"registrator function\"\"\"\n            for name in aliases:\n                register(klass, name)\n            return klass\n        return reg\n    return alias", "response": "Get a registrator function that allows aliases."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a function that creates a new object of base_class with the given name.", "response": "def get_create_func(base_class, nickname):\n    \"\"\"Get creator function\n\n    Parameters\n    ----------\n    base_class : type\n        base class for classes that will be reigstered\n    nickname : str\n        nickname of base_class for logging\n\n    Returns\n    -------\n    a creator function\n    \"\"\"\n    if base_class not in _REGISTRY:\n        _REGISTRY[base_class] = {}\n    registry = _REGISTRY[base_class]\n\n    def create(*args, **kwargs):\n        \"\"\"Create instance from config\"\"\"\n        if len(args):\n            name = args[0]\n            args = args[1:]\n        else:\n            name = kwargs.pop(nickname)\n\n        if isinstance(name, base_class):\n            assert len(args) == 0 and len(kwargs) == 0, \\\n                \"%s is already an instance. Additional arguments are invalid\"%(nickname)\n            return name\n\n        if isinstance(name, dict):\n            return create(**name)\n\n        assert isinstance(name, string_types), \"%s must be of string type\"%nickname\n\n        if name.startswith('['):\n            assert not args and not kwargs\n            name, kwargs = json.loads(name)\n            return create(name, **kwargs)\n        elif name.startswith('{'):\n            assert not args and not kwargs\n            kwargs = json.loads(name)\n            return create(**kwargs)\n\n        name = name.lower()\n        assert name in registry, \\\n            \"%s is not registered. Please register with %s.register first\"%(\n                str(name), nickname)\n        return registry[name](*args, **kwargs)\n\n    create.__doc__ = \"\"\"Create a %s instance from config.\n\nParameters\n----------\n%s : str or %s instance\n    class name of desired instance. If is a instance,\n    it will be returned directly.\n**kwargs : dict\n    arguments to be passed to constructor\"\"\"%(nickname, nickname, base_class.__name__)\n\n    return create"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse command line arguments.", "response": "def parse_args():\n    \"\"\"Parse arguments.\"\"\"\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n        description='Diagnose script for checking the current system.')\n    choices = ['python', 'pip', 'mxnet', 'os', 'hardware', 'network']\n    for choice in choices:\n        parser.add_argument('--' + choice, default=1, type=int,\n                            help='Diagnose {}.'.format(choice))\n    parser.add_argument('--region', default='', type=str,\n                        help=\"Additional sites in which region(s) to test. \\\n                        Specify 'cn' for example to test mirror sites in China.\")\n    parser.add_argument('--timeout', default=10, type=int,\n                        help=\"Connection test timeout threshold, 0 to disable.\")\n    args = parser.parse_args()\n    return args"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clean_str(string):\n    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n    string = re.sub(r\"\\'s\", \" \\'s\", string)\n    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n    string = re.sub(r\"\\'re\", \" \\'re\", string)\n    string = re.sub(r\"\\'d\", \" \\'d\", string)\n    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n    string = re.sub(r\",\", \" , \", string)\n    string = re.sub(r\"!\", \" ! \", string)\n    string = re.sub(r\"\\(\", r\" \\( \", string)\n    string = re.sub(r\"\\)\", r\" \\) \", string)\n    string = re.sub(r\"\\?\", r\" \\? \", string)\n    string = re.sub(r\"\\s{2,}\", \" \", string)\n    return string.strip().lower()", "response": "Tokenization and string cleaning for all datasets except for SST."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_data_and_labels():\n    # Load data from files\n    pos_path = \"./data/rt-polaritydata/rt-polarity.pos\"\n    neg_path = \"./data/rt-polaritydata/rt-polarity.neg\"\n    if not os.path.exists(pos_path):\n        os.system(\"git clone https://github.com/dennybritz/cnn-text-classification-tf.git\")\n        os.system('mv cnn-text-classification-tf/data .')\n        os.system('rm -rf cnn-text-classification-tf')\n    positive_examples = list(open(pos_path).readlines())\n    positive_examples = [s.strip() for s in positive_examples]\n    negative_examples = list(open(neg_path).readlines())\n    negative_examples = [s.strip() for s in negative_examples]\n    # Split by words\n    x_text = positive_examples + negative_examples\n    x_text = [clean_str(sent) for sent in x_text]\n    x_text = [s.split(\" \") for s in x_text]\n    # Generate labels\n    positive_labels = [1 for _ in positive_examples]\n    negative_labels = [0 for _ in negative_examples]\n    y = np.concatenate([positive_labels, negative_labels], 0)\n    return [x_text, y]", "response": "Loads MR polarity data from files splits the data into words and generates labels. Returns split sentences and labels."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npads all sentences to the same length.", "response": "def pad_sentences(sentences, padding_word=\"</s>\"):\n    \"\"\"Pads all sentences to the same length. The length is defined by the longest sentence.\n    Returns padded sentences.\n    \"\"\"\n    sequence_length = max(len(x) for x in sentences)\n    padded_sentences = []\n    for i, sentence in enumerate(sentences):\n        num_padding = sequence_length - len(sentence)\n        new_sentence = sentence + [padding_word] * num_padding\n        padded_sentences.append(new_sentence)\n    return padded_sentences"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmap sentencs and labels to vectors based on a vocabulary.", "response": "def build_input_data(sentences, labels, vocabulary):\n    \"\"\"Maps sentencs and labels to vectors based on a vocabulary.\"\"\"\n    x = np.array([[vocabulary[word] for word in sentence] for sentence in sentences])\n    y = np.array(labels)\n    return [x, y]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef build_input_data_with_word2vec(sentences, labels, word2vec_list):\n    x_vec = []\n    for sent in sentences:\n        vec = []\n        for word in sent:\n            if word in word2vec_list:\n                vec.append(word2vec_list[word])\n            else:\n                vec.append(word2vec_list['</s>'])\n        x_vec.append(vec)\n    x_vec = np.array(x_vec)\n    y_vec = np.array(labels)\n    return [x_vec, y_vec]", "response": "Build input data for a single language."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_data_with_word2vec(word2vec_list):\n    # Load and preprocess data\n    sentences, labels = load_data_and_labels()\n    sentences_padded = pad_sentences(sentences)\n    # vocabulary, vocabulary_inv = build_vocab(sentences_padded)\n    return build_input_data_with_word2vec(sentences_padded, labels, word2vec_list)", "response": "Loads and preprocessed data for the MR dataset.\n    Returns input vectors labels vocabulary inverse vocabulary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_data():\n    # Load and preprocess data\n    sentences, labels = load_data_and_labels()\n    sentences_padded = pad_sentences(sentences)\n    vocabulary, vocabulary_inv = build_vocab(sentences_padded)\n    x, y = build_input_data(sentences_padded, labels, vocabulary)\n    return [x, y, vocabulary, vocabulary_inv]", "response": "Loads and preprocessed data for the MR dataset."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate a batch iterator for a dataset.", "response": "def batch_iter(data, batch_size, num_epochs):\n    \"\"\"Generates a batch iterator for a dataset.\"\"\"\n    data = np.array(data)\n    data_size = len(data)\n    num_batches_per_epoch = int(len(data)/batch_size) + 1\n    for epoch in range(num_epochs):\n        # Shuffle the data at each epoch\n        shuffle_indices = np.random.permutation(np.arange(data_size))\n        shuffled_data = data[shuffle_indices]\n        for batch_num in range(num_batches_per_epoch):\n            start_index = batch_num * batch_size\n            end_index = min((batch_num + 1) * batch_size, data_size)\n            yield shuffled_data[start_index:end_index]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads the pre - trained word2vec from file.", "response": "def load_pretrained_word2vec(infile):\n    \"\"\"Load the pre-trained word2vec from file.\"\"\"\n    if isinstance(infile, str):\n        infile = open(infile)\n\n    word2vec_list = {}\n    for idx, line in enumerate(infile):\n        if idx == 0:\n            vocab_size, dim = line.strip().split()\n        else:\n            tks = line.strip().split()\n            word2vec_list[tks[0]] = map(float, tks[1:])\n\n    return word2vec_list"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_batch(im_tensor, im_info):\n    data = [im_tensor, im_info]\n    data_shapes = [('data', im_tensor.shape), ('im_info', im_info.shape)]\n    data_batch = mx.io.DataBatch(data=data, label=None, provide_data=data_shapes, provide_label=None)\n    return data_batch", "response": "Generate a batch of data from the given image."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_symbol(num_classes=1000, **kwargs):\n    data = mx.symbol.Variable(name=\"data\")\n    label = mx.symbol.Variable(name=\"label\")\n\n    # group 1\n    conv1_1 = mx.symbol.Convolution(\n        data=data, kernel=(3, 3), pad=(1, 1), num_filter=64, name=\"conv1_1\")\n    relu1_1 = mx.symbol.Activation(data=conv1_1, act_type=\"relu\", name=\"relu1_1\")\n    conv1_2 = mx.symbol.Convolution(\n        data=relu1_1, kernel=(3, 3), pad=(1, 1), num_filter=64, name=\"conv1_2\")\n    relu1_2 = mx.symbol.Activation(data=conv1_2, act_type=\"relu\", name=\"relu1_2\")\n    pool1 = mx.symbol.Pooling(\n        data=relu1_2, pool_type=\"max\", kernel=(2, 2), stride=(2, 2), name=\"pool1\")\n    # group 2\n    conv2_1 = mx.symbol.Convolution(\n        data=pool1, kernel=(3, 3), pad=(1, 1), num_filter=128, name=\"conv2_1\")\n    relu2_1 = mx.symbol.Activation(data=conv2_1, act_type=\"relu\", name=\"relu2_1\")\n    conv2_2 = mx.symbol.Convolution(\n        data=relu2_1, kernel=(3, 3), pad=(1, 1), num_filter=128, name=\"conv2_2\")\n    relu2_2 = mx.symbol.Activation(data=conv2_2, act_type=\"relu\", name=\"relu2_2\")\n    pool2 = mx.symbol.Pooling(\n        data=relu2_2, pool_type=\"max\", kernel=(2, 2), stride=(2, 2), name=\"pool2\")\n    # group 3\n    conv3_1 = mx.symbol.Convolution(\n        data=pool2, kernel=(3, 3), pad=(1, 1), num_filter=256, name=\"conv3_1\")\n    relu3_1 = mx.symbol.Activation(data=conv3_1, act_type=\"relu\", name=\"relu3_1\")\n    conv3_2 = mx.symbol.Convolution(\n        data=relu3_1, kernel=(3, 3), pad=(1, 1), num_filter=256, name=\"conv3_2\")\n    relu3_2 = mx.symbol.Activation(data=conv3_2, act_type=\"relu\", name=\"relu3_2\")\n    conv3_3 = mx.symbol.Convolution(\n        data=relu3_2, kernel=(3, 3), pad=(1, 1), num_filter=256, name=\"conv3_3\")\n    relu3_3 = mx.symbol.Activation(data=conv3_3, act_type=\"relu\", name=\"relu3_3\")\n    pool3 = mx.symbol.Pooling(\n        data=relu3_3, pool_type=\"max\", kernel=(2, 2), stride=(2, 2), \\\n        pooling_convention=\"full\", name=\"pool3\")\n    # group 4\n    conv4_1 = mx.symbol.Convolution(\n        data=pool3, kernel=(3, 3), pad=(1, 1), num_filter=512, name=\"conv4_1\")\n    relu4_1 = mx.symbol.Activation(data=conv4_1, act_type=\"relu\", name=\"relu4_1\")\n    conv4_2 = mx.symbol.Convolution(\n        data=relu4_1, kernel=(3, 3), pad=(1, 1), num_filter=512, name=\"conv4_2\")\n    relu4_2 = mx.symbol.Activation(data=conv4_2, act_type=\"relu\", name=\"relu4_2\")\n    conv4_3 = mx.symbol.Convolution(\n        data=relu4_2, kernel=(3, 3), pad=(1, 1), num_filter=512, name=\"conv4_3\")\n    relu4_3 = mx.symbol.Activation(data=conv4_3, act_type=\"relu\", name=\"relu4_3\")\n    pool4 = mx.symbol.Pooling(\n        data=relu4_3, pool_type=\"max\", kernel=(2, 2), stride=(2, 2), name=\"pool4\")\n    # group 5\n    conv5_1 = mx.symbol.Convolution(\n        data=pool4, kernel=(3, 3), pad=(1, 1), num_filter=512, name=\"conv5_1\")\n    relu5_1 = mx.symbol.Activation(data=conv5_1, act_type=\"relu\", name=\"relu5_1\")\n    conv5_2 = mx.symbol.Convolution(\n        data=relu5_1, kernel=(3, 3), pad=(1, 1), num_filter=512, name=\"conv5_2\")\n    relu5_2 = mx.symbol.Activation(data=conv5_2, act_type=\"relu\", name=\"relu5_2\")\n    conv5_3 = mx.symbol.Convolution(\n        data=relu5_2, kernel=(3, 3), pad=(1, 1), num_filter=512, name=\"conv5_3\")\n    relu5_3 = mx.symbol.Activation(data=conv5_3, act_type=\"relu\", name=\"relu5_3\")\n    pool5 = mx.symbol.Pooling(\n        data=relu5_3, pool_type=\"max\", kernel=(3, 3), stride=(1, 1),\n        pad=(1,1), name=\"pool5\")\n    # group 6\n    conv6 = mx.symbol.Convolution(\n        data=pool5, kernel=(3, 3), pad=(6, 6), dilate=(6, 6),\n        num_filter=1024, name=\"fc6\")\n    relu6 = mx.symbol.Activation(data=conv6, act_type=\"relu\", name=\"relu6\")\n    # drop6 = mx.symbol.Dropout(data=relu6, p=0.5, name=\"drop6\")\n    # group 7\n    conv7 = mx.symbol.Convolution(\n        data=relu6, kernel=(1, 1), pad=(0, 0), num_filter=1024, name=\"fc7\")\n    relu7 = mx.symbol.Activation(data=conv7, act_type=\"relu\", name=\"relu7\")\n    # drop7 = mx.symbol.Dropout(data=relu7, p=0.5, name=\"drop7\")\n\n    gpool = mx.symbol.Pooling(data=relu7, pool_type='avg', kernel=(7, 7),\n        global_pool=True, name='global_pool')\n    conv8 = mx.symbol.Convolution(data=gpool, num_filter=num_classes, kernel=(1, 1),\n        name='fc8')\n    flat = mx.symbol.Flatten(data=conv8)\n    softmax = mx.symbol.SoftmaxOutput(data=flat, name='softmax')\n    return softmax", "response": "Returns a symbol that can be used to generate VGG 16 network."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget multi - layer perceptron", "response": "def get_mlp():\n    \"\"\"Get multi-layer perceptron\"\"\"\n    data = mx.symbol.Variable('data')\n    fc1 = mx.symbol.CaffeOp(data_0=data, num_weight=2, name='fc1',\n                            prototxt=\"layer{type:\\\"InnerProduct\\\" inner_product_param{num_output: 128} }\")\n    act1 = mx.symbol.CaffeOp(data_0=fc1, prototxt=\"layer{type:\\\"TanH\\\"}\")\n    fc2 = mx.symbol.CaffeOp(data_0=act1, num_weight=2, name='fc2',\n                            prototxt=\"layer{type:\\\"InnerProduct\\\" inner_product_param{num_output: 64} }\")\n    act2 = mx.symbol.CaffeOp(data_0=fc2, prototxt=\"layer{type:\\\"TanH\\\"}\")\n    fc3 = mx.symbol.CaffeOp(data_0=act2, num_weight=2, name='fc3',\n                            prototxt=\"layer{type:\\\"InnerProduct\\\" inner_product_param{num_output: 10}}\")\n    if use_caffe_loss:\n        label = mx.symbol.Variable('softmax_label')\n        mlp = mx.symbol.CaffeLoss(data=fc3, label=label, grad_scale=1, name='softmax',\n                                  prototxt=\"layer{type:\\\"SoftmaxWithLoss\\\"}\")\n    else:\n        mlp = mx.symbol.SoftmaxOutput(data=fc3, name='softmax')\n    return mlp"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_args():\n    parser = argparse.ArgumentParser(description='train an image classifier on mnist')\n    parser.add_argument('--network', type=str, default='lenet',\n                        help='the cnn to use (mlp | lenet | <path to network json file>')\n    parser.add_argument('--caffe-loss', type=int, default=0,\n                        help='Use CaffeLoss symbol')\n    parser.add_argument('--caffe-data', action='store_true',\n                        help='Use Caffe input-data layer only if specified')\n    parser.add_argument('--data-dir', type=str, default='mnist/',\n                        help='the input data directory')\n    parser.add_argument('--gpus', type=str,\n                        help='the gpus will be used, e.g \"0,1,2,3\"')\n    parser.add_argument('--num-examples', type=int, default=60000,\n                        help='the number of training examples')\n    parser.add_argument('--batch-size', type=int, default=128,\n                        help='the batch size')\n    parser.add_argument('--lr', type=float, default=.1,\n                        help='the initial learning rate')\n    parser.add_argument('--model-prefix', type=str,\n                        help='the prefix of the model to load/save')\n    parser.add_argument('--save-model-prefix', type=str,\n                        help='the prefix of the model to save')\n    parser.add_argument('--num-epochs', type=int, default=10,\n                        help='the number of training epochs')\n    parser.add_argument('--load-epoch', type=int,\n                        help=\"load the model on an epoch using the model-prefix\")\n    parser.add_argument('--kv-store', type=str, default='local',\n                        help='the kvstore type')\n    parser.add_argument('--lr-factor', type=float, default=1,\n                        help='times the lr with a factor for every lr-factor-epoch epoch')\n    parser.add_argument('--lr-factor-epoch', type=float, default=1,\n                        help='the number of epoch to factor the lr, could be .5')\n    return parser.parse_args()", "response": "Parse the arguments for the\n    command line"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nimplements forward computation. is_train : bool, whether forwarding for training or testing. req : list of {'null', 'write', 'inplace', 'add'}, how to assign to out_data. 'null' means skip assignment, etc. in_data : list of NDArray, input data. out_data : list of NDArray, pre-allocated output buffers. aux : list of NDArray, mutable auxiliary states. Usually not used.", "response": "def forward(self, is_train, req, in_data, out_data, aux):\n        \"\"\"Implements forward computation.\n\n        is_train : bool, whether forwarding for training or testing.\n        req : list of {'null', 'write', 'inplace', 'add'}, how to assign to out_data. 'null' means skip assignment, etc.\n        in_data : list of NDArray, input data.\n        out_data : list of NDArray, pre-allocated output buffers.\n        aux : list of NDArray, mutable auxiliary states. Usually not used.\n        \"\"\"\n        data = in_data[0]\n        label = in_data[1]\n        pred = mx.nd.SoftmaxOutput(data, label)\n        self.assign(out_data[0], req[0], pred)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef backward(self, req, out_grad, in_data, out_data, in_grad, aux):\n        label = in_data[1]\n        pred = out_data[0]\n        dx = pred - mx.nd.one_hot(label, 2)\n        pos_cls_weight = self.positive_cls_weight\n        scale_factor = ((1 + label * pos_cls_weight) / pos_cls_weight).reshape((pred.shape[0],1))\n        rescaled_dx = scale_factor * dx\n        self.assign(in_grad[0], req[0], rescaled_dx)", "response": "Implements backward computation of the cluster class."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _reset_bind(self):\n        self.binded = False\n        self._buckets = {}\n        self._curr_module = None\n        self._curr_bucket_key = None", "response": "Internal utility function to reset the bind state."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef data_names(self):\n        if self.binded:\n            return self._curr_module.data_names\n        else:\n            _, data_names, _ = self._call_sym_gen(self._default_bucket_key)\n            return data_names", "response": "A list of names for data required by this module."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef output_names(self):\n        if self.binded:\n            return self._curr_module.output_names\n        else:\n            symbol, _, _ = self._call_sym_gen(self._default_bucket_key)\n            return symbol.list_outputs()", "response": "A list of names for the outputs of this module."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the current parameters.", "response": "def get_params(self):\n        \"\"\"Gets current parameters.\n\n        Returns\n        -------\n        `(arg_params, aux_params)`\n            A pair of dictionaries each mapping parameter names to NDArray values.\n        \"\"\"\n        assert self.binded and self.params_initialized\n        self._curr_module._params_dirty = self._params_dirty\n        params = self._curr_module.get_params()\n        self._params_dirty = False\n        return params"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef init_params(self, initializer=Uniform(0.01), arg_params=None, aux_params=None,\n                    allow_missing=False, force_init=False, allow_extra=False):\n        \"\"\"Initializes parameters.\n\n        Parameters\n        ----------\n        initializer : Initializer\n        arg_params : dict\n            Defaults to ``None``. Existing parameters. This has higher priority\n            than `initializer`.\n        aux_params : dict\n            Defaults to ``None``. Existing auxiliary states. This has higher priority\n            than `initializer`.\n        allow_missing : bool\n            Allow missing values in `arg_params` and `aux_params` (if not ``None``).\n            In this case, missing values will be filled with `initializer`.\n        force_init : bool\n            Defaults to ``False``.\n        allow_extra : boolean, optional\n            Whether allow extra parameters that are not needed by symbol.\n            If this is True, no error will be thrown when arg_params or aux_params\n            contain extra parameters that is not needed by the executor.\n        \"\"\"\n        if self.params_initialized and not force_init:\n            return\n        assert self.binded, 'call bind before initializing the parameters'\n        self._curr_module.init_params(initializer=initializer, arg_params=arg_params,\n                                      aux_params=aux_params, allow_missing=allow_missing,\n                                      force_init=force_init, allow_extra=allow_extra)\n        self._params_dirty = False\n        self.params_initialized = True", "response": "Initializes the parameters of the current module."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets states from all devices.", "response": "def get_states(self, merge_multi_context=True):\n        \"\"\"Gets states from all devices.\n\n        Parameters\n        ----------\n        merge_multi_context : bool\n            Default is `True`. In the case when data-parallelism is used, the states\n            will be collected from multiple devices. A `True` value indicate that we\n            should merge the collected results so that they look like from a single\n            executor.\n\n        Returns\n        -------\n        list of NDArrays or list of list of NDArrays\n            If `merge_multi_context` is ``True``, it is like ``[out1, out2]``. Otherwise, it\n            is like ``[[out1_dev1, out1_dev2], [out2_dev1, out2_dev2]]``. All the output\n            elements are `NDArray`.\n        \"\"\"\n        assert self.binded and self.params_initialized\n        return self._curr_module.get_states(merge_multi_context=merge_multi_context)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting value for states. Only one of states & values can be specified.", "response": "def set_states(self, states=None, value=None):\n        \"\"\"Sets value for states. Only one of states & values can be specified.\n\n        Parameters\n        ----------\n        states : list of list of NDArrays\n            Source states arrays formatted like ``[[state1_dev1, state1_dev2],\n            [state2_dev1, state2_dev2]]``.\n        value : number\n            A single scalar value for all state arrays.\n        \"\"\"\n        assert self.binded and self.params_initialized\n        self._curr_module.set_states(states, value)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbind the parameters of a bucketing module to the corresponding executor.", "response": "def bind(self, data_shapes, label_shapes=None, for_training=True,\n             inputs_need_grad=False, force_rebind=False, shared_module=None,\n             grad_req='write'):\n        \"\"\"Binding for a `BucketingModule` means setting up the buckets and binding the\n        executor for the default bucket key. Executors corresponding to other keys are\n        bound afterwards with `switch_bucket`.\n\n        Parameters\n        ----------\n        data_shapes : list of (str, tuple)\n            This should correspond to the symbol for the default bucket.\n        label_shapes : list of (str, tuple)\n            This should correspond to the symbol for the default bucket.\n        for_training : bool\n            Default is ``True``.\n        inputs_need_grad : bool\n            Default is ``False``.\n        force_rebind : bool\n            Default is ``False``.\n        shared_module : BucketingModule\n            Default is ``None``. This value is currently not used.\n        grad_req : str, list of str, dict of str to str\n            Requirement for gradient accumulation. Can be 'write', 'add', or 'null'\n            (default to 'write').\n            Can be specified globally (str) or for each argument (list, dict).\n        bucket_key : str (or any python object)\n            bucket key for binding. by default use the default_bucket_key\n        \"\"\"\n        # in case we already initialized params, keep it\n        if self.params_initialized:\n            arg_params, aux_params = self.get_params()\n\n        # force rebinding is typically used when one want to switch from\n        # training to prediction phase.\n        if force_rebind:\n            self._reset_bind()\n\n        if self.binded:\n            self.logger.warning('Already bound, ignoring bind()')\n            return\n\n        assert shared_module is None, 'shared_module for BucketingModule is not supported'\n\n        self.for_training = for_training\n        self.inputs_need_grad = inputs_need_grad\n        self.binded = True\n        self._grad_req = grad_req\n\n        symbol, data_names, label_names = self._call_sym_gen(self._default_bucket_key)\n        module = Module(symbol, data_names, label_names, logger=self.logger,\n                        context=self._context, work_load_list=self._work_load_list,\n                        fixed_param_names=self._fixed_param_names,\n                        state_names=self._state_names,\n                        group2ctxs=self._group2ctxs,\n                        compression_params=self._compression_params)\n        module.bind(data_shapes, label_shapes, for_training, inputs_need_grad,\n                    force_rebind=False, shared_module=None, grad_req=self._grad_req)\n        self._curr_module = module\n        self._curr_bucket_key = self._default_bucket_key\n        self._buckets[self._default_bucket_key] = module\n\n        # copy back saved params, if already initialized\n        if self.params_initialized:\n            self.set_params(arg_params, aux_params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef switch_bucket(self, bucket_key, data_shapes, label_shapes=None):\n        assert self.binded, 'call bind before switching bucket'\n        if not bucket_key in self._buckets:\n            symbol, data_names, label_names = self._call_sym_gen(bucket_key)\n            module = Module(symbol, data_names, label_names,\n                            logger=self.logger, context=self._context,\n                            work_load_list=self._work_load_list,\n                            fixed_param_names=self._fixed_param_names,\n                            state_names=self._state_names,\n                            group2ctxs=self._group2ctxs,\n                            compression_params=self._compression_params)\n            module.bind(data_shapes, label_shapes, self._curr_module.for_training,\n                        self._curr_module.inputs_need_grad,\n                        force_rebind=False, shared_module=self._buckets[self._default_bucket_key],\n                        grad_req=self._grad_req)\n            if self._monitor is not None:\n                module.install_monitor(self._monitor)\n            self._buckets[bucket_key] = module\n\n        self._curr_module = self._buckets[bucket_key]\n        self._curr_bucket_key = bucket_key", "response": "Switches to a different bucket. This will change self. curr_module."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninstalling and initializes the optimizer for the current object.", "response": "def init_optimizer(self, kvstore='local', optimizer='sgd',\n                       optimizer_params=(('learning_rate', 0.01),),\n                       force_init=False):\n        \"\"\"Installs and initializes optimizers.\n\n        Parameters\n        ----------\n        kvstore : str or KVStore\n            Defaults to `'local'`.\n        optimizer : str or Optimizer\n            Defaults to `'sgd'`\n        optimizer_params : dict\n            Defaults to `(('learning_rate', 0.01),)`. The default value is not a dictionary,\n            just to avoid pylint warning of dangerous default values.\n        force_init : bool\n            Defaults to ``False``, indicating whether we should force re-initializing the\n            optimizer in the case an optimizer is already installed.\n        \"\"\"\n        assert self.binded and self.params_initialized\n        if self.optimizer_initialized and not force_init:\n            self.logger.warning('optimizer already initialized, ignoring.')\n            return\n\n        self._curr_module.init_optimizer(kvstore, optimizer, optimizer_params,\n                                         force_init=force_init)\n        for mod in self._buckets.values():\n            if mod is not self._curr_module:\n                mod.borrow_optimizer(self._curr_module)\n\n        self.optimizer_initialized = True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npreparing the module for processing a data batch.", "response": "def prepare(self, data_batch, sparse_row_id_fn=None):\n        '''Prepares the module for processing a data batch.\n\n        Usually involves switching bucket and reshaping.\n        For modules that contain `row_sparse` parameters in KVStore,\n        it prepares the `row_sparse` parameters based on the sparse_row_id_fn.\n\n        Parameters\n        ----------\n        data_batch : DataBatch\n            The current batch of data for forward computation.\n\n        sparse_row_id_fn : A callback function\n            The function  takes `data_batch` as an input and returns a dict of\n            str -> NDArray. The resulting dict is used for pulling row_sparse\n            parameters from the kvstore, where the str key is the name of the param,\n            and the value is the row id of the param to pull.\n        '''\n        # perform bind if haven't done so\n        assert self.binded and self.params_initialized\n        bucket_key = data_batch.bucket_key\n        original_bucket_key = self._curr_bucket_key\n        data_shapes = data_batch.provide_data\n        label_shapes = data_batch.provide_label\n        self.switch_bucket(bucket_key, data_shapes, label_shapes)\n        self._curr_module.prepare(data_batch, sparse_row_id_fn=sparse_row_id_fn)\n        # switch back\n        self.switch_bucket(original_bucket_key, None, None)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef forward(self, data_batch, is_train=None):\n        assert self.binded and self.params_initialized\n        self.switch_bucket(data_batch.bucket_key, data_batch.provide_data,\n                           data_batch.provide_label)\n        self._curr_module.forward(data_batch, is_train=is_train)", "response": "Forward computation.\n\n        Parameters\n        ----------\n        data_batch : DataBatch\n        is_train : bool\n            Defaults to ``None``, in which case `is_train` is take as ``self.for_training``."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the parameters according to the current optimizer and gradient computed in the previous forward - backward cycle.", "response": "def update(self):\n        \"\"\"Updates parameters according to installed optimizer and the gradient computed\n        in the previous forward-backward cycle.\n\n        When KVStore is used to update parameters for multi-device or multi-machine training,\n        a copy of the parameters are stored in KVStore. Note that for `row_sparse` parameters,\n        this function does update the copy of parameters in KVStore, but doesn't broadcast the\n        updated parameters to all devices / machines. Please call `prepare` to broadcast\n        `row_sparse` parameters with the next batch of data.\n\n        \"\"\"\n        assert self.binded and self.params_initialized and self.optimizer_initialized\n        self._params_dirty = True\n        self._curr_module.update()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_outputs(self, merge_multi_context=True):\n        assert self.binded and self.params_initialized\n        return self._curr_module.get_outputs(merge_multi_context=merge_multi_context)", "response": "Gets outputs from a previous forward computation."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the gradients with respect to the inputs of the module.", "response": "def get_input_grads(self, merge_multi_context=True):\n        \"\"\"Gets the gradients with respect to the inputs of the module.\n\n        Parameters\n        ----------\n        merge_multi_context : bool\n            Defaults to ``True``. In the case when data-parallelism is used, the outputs\n            will be collected from multiple devices. A ``True`` value indicate that we\n            should merge the collected results so that they look like from a single\n            executor.\n\n        Returns\n        -------\n        list of NDArrays or list of list of NDArrays\n            If `merge_multi_context` is ``True``, it is like ``[grad1, grad2]``. Otherwise, it\n            is like ``[[grad1_dev1, grad1_dev2], [grad2_dev1, grad2_dev2]]``. All the output\n            elements are `NDArray`.\n        \"\"\"\n        assert self.binded and self.params_initialized and self.inputs_need_grad\n        return self._curr_module.get_input_grads(merge_multi_context=merge_multi_context)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nevaluate and accumulates evaluation metric on outputs of the last forward computation.", "response": "def update_metric(self, eval_metric, labels, pre_sliced=False):\n        \"\"\"Evaluates and accumulates evaluation metric on outputs of the last forward computation.\n\n        Parameters\n        ----------\n        eval_metric : EvalMetric\n        labels : list of NDArray\n            Typically ``data_batch.label``.\n        \"\"\"\n        assert self.binded and self.params_initialized\n        self._curr_module.update_metric(eval_metric, labels, pre_sliced)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef install_monitor(self, mon):\n        assert self.binded\n        self._monitor = mon\n        for mod in self._buckets.values():\n            mod.install_monitor(mon)", "response": "Installs monitor on all executors."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_recording(is_recording): #pylint: disable=redefined-outer-name\n    prev = ctypes.c_int()\n    check_call(_LIB.MXAutogradSetIsRecording(\n        ctypes.c_int(is_recording), ctypes.byref(prev)))\n    return bool(prev.value)", "response": "Sets the status of the\n    to recording or not recording."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_training(train_mode): #pylint: disable=redefined-outer-name\n    prev = ctypes.c_int()\n    check_call(_LIB.MXAutogradSetIsTraining(\n        ctypes.c_int(train_mode), ctypes.byref(prev)))\n    return bool(prev.value)", "response": "Sets the status of the\n    to training or predicting."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_recording():\n    curr = ctypes.c_bool()\n    check_call(_LIB.MXAutogradIsRecording(ctypes.byref(curr)))\n    return curr.value", "response": "Get status on recording or not recording."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets status of training or predicting.", "response": "def is_training():\n    \"\"\"Get status on training/predicting.\n\n    Returns\n    -------\n    Current state of training/predicting.\n    \"\"\"\n    curr = ctypes.c_bool()\n    check_call(_LIB.MXAutogradIsTraining(ctypes.byref(curr)))\n    return curr.value"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef mark_variables(variables, gradients, grad_reqs='write'):\n    if isinstance(variables, NDArray):\n        assert isinstance(gradients, NDArray)\n        variables = [variables]\n        gradients = [gradients]\n\n    if isinstance(grad_reqs, string_types):\n        grad_reqs = [_GRAD_REQ_MAP[grad_reqs]]*len(variables)\n    else:\n        grad_reqs = [_GRAD_REQ_MAP[i] for i in grad_reqs]\n\n    check_call(_LIB.MXAutogradMarkVariables(\n        len(variables),\n        c_handle_array(variables),\n        c_array_buf(mx_uint, array('I', grad_reqs)),\n        c_handle_array(gradients)))", "response": "Mark NDArrays as variables to compute gradient for autograd."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse head gradient for backward and grad.", "response": "def _parse_head(heads, head_grads):\n    \"\"\"parse head gradient for backward and grad.\"\"\"\n    if isinstance(heads, NDArray):\n        heads = [heads]\n    if isinstance(head_grads, NDArray):\n        head_grads = [head_grads]\n\n    head_handles = c_handle_array(heads)\n\n    if head_grads is None:\n        hgrad_handles = ctypes.c_void_p(0)\n    else:\n        assert len(heads) == len(head_grads), \\\n            \"heads and head_grads must be lists of the same length\"\n        hgrad_handles = c_array(NDArrayHandle,\n                                [i.handle if i is not None else NDArrayHandle(0)\n                                 for i in head_grads])\n    return head_handles, hgrad_handles"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef backward(heads, head_grads=None, retain_graph=False, train_mode=True): #pylint: disable=redefined-outer-name\n    head_handles, hgrad_handles = _parse_head(heads, head_grads)\n\n    check_call(_LIB.MXAutogradBackwardEx(\n        len(head_handles),\n        head_handles,\n        hgrad_handles,\n        0,\n        ctypes.c_void_p(0),\n        ctypes.c_int(retain_graph),\n        ctypes.c_int(0),\n        ctypes.c_int(train_mode),\n        ctypes.c_void_p(0),\n        ctypes.c_void_p(0)))", "response": "Compute the gradients of the given heads w. r. t previously marked variables."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef grad(heads, variables, head_grads=None, retain_graph=None, create_graph=False,\n         train_mode=True):  #pylint: disable=redefined-outer-name\n    \"\"\"Compute the gradients of heads w.r.t variables. Gradients will be\n    returned as new NDArrays instead of stored into `variable.grad`.\n    Supports recording gradient graph for computing higher order gradients.\n\n    .. note::\n\n      Currently only a very limited set of operators support higher order \\\n      gradients.\n\n    Parameters\n    ----------\n    heads: NDArray or list of NDArray\n        Output NDArray(s)\n    variables: NDArray or list of NDArray\n        Input variables to compute gradients for.\n    head_grads: NDArray or list of NDArray or None\n        Gradients with respect to heads.\n    retain_graph: bool\n        Whether to keep computation graph to differentiate again, instead\n        of clearing history and release memory. Defaults to the same value\n        as create_graph.\n    create_graph: bool\n        Whether to record gradient graph for computing higher order\n    train_mode: bool, optional\n        Whether to do backward for training or prediction.\n\n    Returns\n    -------\n    NDArray or list of NDArray:\n        Gradients with respect to variables.\n\n    Examples\n    --------\n    >>> x = mx.nd.ones((1,))\n    >>> x.attach_grad()\n    >>> with mx.autograd.record():\n    ...     z = mx.nd.elemwise_add(mx.nd.exp(x), x)\n    >>> dx = mx.autograd.grad(z, [x], create_graph=True)\n    >>> print(dx)\n    [\n    [ 3.71828175]\n    <NDArray 1 @cpu(0)>]\n    \"\"\"\n    head_handles, hgrad_handles = _parse_head(heads, head_grads)\n\n    if isinstance(variables, NDArray):\n        variables = [variables]\n    else:\n        assert len(variables), \"variables cannot be an empty list.\"\n    var_handles = c_handle_array(variables)\n\n    retain_graph = retain_graph if retain_graph is not None else create_graph\n    grad_vars = ctypes.POINTER(NDArrayHandle)()\n    grad_stypes = ctypes.POINTER(ctypes.c_int)()\n\n    check_call(_LIB.MXAutogradBackwardEx(\n        len(head_handles),\n        head_handles,\n        hgrad_handles,\n        len(var_handles),\n        var_handles,\n        ctypes.c_int(retain_graph),\n        ctypes.c_int(create_graph),\n        ctypes.c_int(train_mode),\n        ctypes.byref(grad_vars),\n        ctypes.byref(grad_stypes)))\n\n    ret = [_ndarray_cls(ctypes.cast(grad_vars[i], NDArrayHandle),\n                        stype=grad_stypes[i])\n           for i in range(len(var_handles))]\n    if isinstance(variables, NDArray):\n        return ret[0]\n    return ret", "response": "Compute the gradients of variables w. r. t variables."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_symbol(x):\n    hdl = SymbolHandle()\n    check_call(_LIB.MXAutogradGetSymbol(x.handle, ctypes.byref(hdl)))\n    return Symbol(hdl)", "response": "Retrieve recorded computation history as Symbol."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfunction parses all the operations and returns a single element list.", "response": "def ParseAllOps():\n    \"\"\"\n    MXNET_DLL int MXSymbolListAtomicSymbolCreators(mx_uint *out_size,\n                                                   AtomicSymbolCreator **out_array);\n\n    MXNET_DLL int MXSymbolGetAtomicSymbolInfo(AtomicSymbolCreator creator,\n                                              const char **name,\n                                              const char **description,\n                                              mx_uint *num_args,\n                                              const char ***arg_names,\n                                              const char ***arg_type_infos,\n                                              const char ***arg_descriptions,\n                                              const char **key_var_num_args);\n    \"\"\"\n    cdll.libmxnet = cdll.LoadLibrary(sys.argv[1])\n    ListOP = cdll.libmxnet.MXSymbolListAtomicSymbolCreators\n    GetOpInfo = cdll.libmxnet.MXSymbolGetAtomicSymbolInfo\n    ListOP.argtypes=[POINTER(c_int), POINTER(POINTER(c_void_p))]\n    GetOpInfo.argtypes=[c_void_p, \\\n        POINTER(c_char_p), \\\n        POINTER(c_char_p), \\\n        POINTER(c_int), \\\n        POINTER(POINTER(c_char_p)), \\\n        POINTER(POINTER(c_char_p)), \\\n        POINTER(POINTER(c_char_p)), \\\n        POINTER(c_char_p), \\\n        POINTER(c_char_p)\n        ]\n\n    nOps = c_int()\n    opHandlers = POINTER(c_void_p)()\n    r = ListOP(byref(nOps), byref(opHandlers))\n    ret = ''\n    ret2 = ''\n    for i in range(0, nOps.value):\n        handler = opHandlers[i]\n        name = c_char_p()\n        description = c_char_p()\n        nArgs = c_int()\n        argNames = POINTER(c_char_p)()\n        argTypes = POINTER(c_char_p)()\n        argDescs = POINTER(c_char_p)()\n        varArgName = c_char_p()\n        return_type = c_char_p()\n\n        GetOpInfo(handler, byref(name), byref(description), \\\n            byref(nArgs), byref(argNames), byref(argTypes), \\\n            byref(argDescs), byref(varArgName), byref(return_type))\n\n        if name.value.decode('utf-8').startswith('_'):     # get rid of functions like __init__\n            continue\n\n        args = []\n\n        for i in range(0, nArgs.value):\n            arg = Arg(name.value.decode('utf-8'),\n                      argNames[i].decode('utf-8'),\n                      argTypes[i].decode('utf-8'),\n                      argDescs[i].decode('utf-8'))\n            args.append(arg)\n\n        op = Op(name.value.decode('utf-8'), description.value.decode('utf-8'), args)\n\n        ret = ret + op.GetOpDefinitionString(True) + \"\\n\"\n        ret2 = ret2 + op.GetOpDefinitionString(False) + \"\\n\"\n    return ret + ret2"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef main():\n    parser = argparse.ArgumentParser(description='.caffemodel to MXNet .params converter.')\n    parser.add_argument('caffemodel', help='Path to the .caffemodel file to convert.')\n    parser.add_argument('output_file_name', help='Name of the output .params file.')\n\n    args = parser.parse_args()\n\n    converter = CaffeModelConverter()\n    converter.convert(args.caffemodel, args.output_file_name)", "response": "Main function for the caffe - param conversion script."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_param(self, param_name, layer_index, blob_index):\n        blobs = self.layers[layer_index].blobs\n        self.dict_param[param_name] = mx.nd.array(caffe.io.blobproto_to_array(blobs[blob_index]))", "response": "Add a param to the. params file"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding an arg param to. params file. Example", "response": "def add_arg_param(self, param_name, layer_index, blob_index):\n        \"\"\"Add an arg param to .params file. Example: weights of a fully connected layer.\"\"\"\n        self.add_param('arg:%s' % param_name, layer_index, blob_index)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds an aux param to. params file. Example", "response": "def add_aux_param(self, param_name, layer_index, blob_index):\n        \"\"\"Add an aux param to .params file. Example: moving_mean in BatchNorm layer \"\"\"\n        self.add_param('aux:%s' % param_name, layer_index, blob_index)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds an optional arg param.", "response": "def add_optional_arg_param(self, param_name, layer_index, blob_index):\n        \"\"\"Add an arg param. If there is no such param in .caffemodel fie, silently ignore it.\"\"\"\n        blobs = self.layers[layer_index].blobs\n        if blob_index < len(blobs):\n            self.add_arg_param(param_name, layer_index, blob_index)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a Caffe. caffemodel file to MXNet. params file", "response": "def convert(self, caffemodel_path, outmodel_path):\n        \"\"\"Convert a Caffe .caffemodel file to MXNet .params file\"\"\"\n        net_param = caffe_pb2.NetParameter()\n        with open(caffemodel_path, 'rb') as caffe_model_file:\n            net_param.ParseFromString(caffe_model_file.read())\n\n        layers = net_param.layer\n        self.layers = layers\n\n        for idx, layer in enumerate(layers):\n            layer_name = str(layer.name)\n\n            if layer.blobs:\n\n                # If this is a layer that has only weight and bias as parameter\n                if layer.type == 'Convolution' or layer.type == 'InnerProduct' \\\n                        or layer.type == 'Deconvolution':\n\n                    # Add weight and bias to the dictionary\n                    self.add_arg_param('%s_weight' % layer_name, layer_index=idx, blob_index=0)\n                    self.add_optional_arg_param('%s_bias' % layer_name, layer_index=idx,\n                                                blob_index=1)\n\n                elif layer.type == 'BatchNorm':\n\n                    gamma_param_name = '%s_gamma' % layer_name\n                    beta_param_name = '%s_beta' % layer_name\n\n                    next_layer = layers[idx + 1]\n\n                    if next_layer.type == 'Scale':\n                        # If next layer is scale layer, get gamma and beta from there\n                        self.add_arg_param(gamma_param_name, layer_index=idx+1, blob_index=0)\n                        self.add_arg_param(beta_param_name, layer_index=idx+1, blob_index=1)\n\n                    mean_param_name = '%s_moving_mean' % layer_name\n                    var_param_name = '%s_moving_var' % layer_name\n\n                    self.add_aux_param(mean_param_name, layer_index=idx, blob_index=0)\n                    self.add_aux_param(var_param_name, layer_index=idx, blob_index=1)\n\n                elif layer.type == 'Scale':\n\n                    prev_layer = layers[idx - 1]\n\n                    if prev_layer.type == 'BatchNorm':\n                        continue\n                    else:\n                        # Use the naming convention used by CaffeOp\n                        self.add_arg_param('%s_0_weight' % layer_name, layer_index=idx,\n                                           blob_index=0)\n                        self.add_optional_arg_param('%s_1_bias' % layer_name,\n                                                    layer_index=idx, blob_index=1)\n\n        mx.nd.save(outmodel_path, self.dict_param)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sample_rois(rois, gt_boxes, num_classes, rois_per_image, fg_rois_per_image, fg_overlap, box_stds):\n    overlaps = bbox_overlaps(rois[:, 1:], gt_boxes[:, :4])\n    gt_assignment = overlaps.argmax(axis=1)\n    labels = gt_boxes[gt_assignment, 4]\n    max_overlaps = overlaps.max(axis=1)\n\n    # select foreground RoI with FG_THRESH overlap\n    fg_indexes = np.where(max_overlaps >= fg_overlap)[0]\n    # guard against the case when an image has fewer than fg_rois_per_image foreground RoIs\n    fg_rois_this_image = min(fg_rois_per_image, len(fg_indexes))\n    # sample foreground regions without replacement\n    if len(fg_indexes) > fg_rois_this_image:\n        fg_indexes = np.random.choice(fg_indexes, size=fg_rois_this_image, replace=False)\n\n    # select background RoIs as those within [0, FG_THRESH)\n    bg_indexes = np.where(max_overlaps < fg_overlap)[0]\n    # compute number of background RoIs to take from this image (guarding against there being fewer than desired)\n    bg_rois_this_image = rois_per_image - fg_rois_this_image\n    bg_rois_this_image = min(bg_rois_this_image, len(bg_indexes))\n    # sample bg rois without replacement\n    if len(bg_indexes) > bg_rois_this_image:\n        bg_indexes = np.random.choice(bg_indexes, size=bg_rois_this_image, replace=False)\n\n    # indexes selected\n    keep_indexes = np.append(fg_indexes, bg_indexes)\n    # pad more bg rois to ensure a fixed minibatch size\n    while len(keep_indexes) < rois_per_image:\n        gap = min(len(bg_indexes), rois_per_image - len(keep_indexes))\n        gap_indexes = np.random.choice(range(len(bg_indexes)), size=gap, replace=False)\n        keep_indexes = np.append(keep_indexes, bg_indexes[gap_indexes])\n\n    # sample rois and labels\n    rois = rois[keep_indexes]\n    labels = labels[keep_indexes]\n    # set labels of bg rois to be 0\n    labels[fg_rois_this_image:] = 0\n\n    # load or compute bbox_target\n    targets = bbox_transform(rois[:, 1:], gt_boxes[gt_assignment[keep_indexes], :4], box_stds=box_stds)\n    bbox_targets = np.zeros((rois_per_image, 4 * num_classes), dtype=np.float32)\n    bbox_weights = np.zeros((rois_per_image, 4 * num_classes), dtype=np.float32)\n    for i in range(fg_rois_this_image):\n        cls_ind = int(labels[i])\n        bbox_targets[i, cls_ind * 4:(cls_ind + 1) * 4] = targets[i]\n        bbox_weights[i, cls_ind * 4:(cls_ind + 1) * 4] = 1\n\n    return rois, labels, bbox_targets, bbox_weights", "response": "generate random sample of ROIs comprising foreground and background examples"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef register(reg_name):\n    def do_register(prop_cls):\n        \"\"\"Register a subclass of CustomOpProp to the registry.\"\"\"\n        fb_functype = CFUNCTYPE(c_int, c_int, POINTER(c_void_p), POINTER(c_int),\n                                POINTER(c_int), c_int, c_void_p)\n        del_functype = CFUNCTYPE(c_int, c_void_p)\n\n        infershape_functype = CFUNCTYPE(c_int, c_int, POINTER(c_int),\n                                        POINTER(POINTER(mx_int)), c_void_p)\n        infertype_functype = CFUNCTYPE(c_int, c_int, POINTER(c_int), c_void_p)\n        inferstorage_functype = CFUNCTYPE(c_int, c_int, POINTER(c_int), c_void_p)\n        inferstorage_backward_functype = CFUNCTYPE(c_int, c_int, POINTER(c_int), \\\n                                                   POINTER(c_int), c_void_p)\n        list_functype = CFUNCTYPE(c_int, POINTER(POINTER(POINTER(c_char))), c_void_p)\n        deps_functype = CFUNCTYPE(c_int, c_int_p, c_int_p, c_int_p,\n                                  c_int_p, POINTER(c_int_p), c_void_p)\n        createop_functype = CFUNCTYPE(c_int, c_char_p, c_int, POINTER(POINTER(mx_uint)),\n                                      POINTER(c_int), POINTER(c_int),\n                                      POINTER(MXCallbackList), c_void_p)\n        req_enum = ('null', 'write', 'inplace', 'add')\n\n        def creator(op_type, argc, keys, vals, ret):\n            \"\"\"internal function\"\"\"\n            assert py_str(op_type) == reg_name\n            kwargs = dict([(py_str(keys[i]), py_str(vals[i])) for i in range(argc)])\n            op_prop = prop_cls(**kwargs)\n\n            def infer_shape_entry(num_tensor, tensor_dims,\n                                  tensor_shapes, _):\n                \"\"\"C Callback for ``CustomOpProp::InferShape``.\"\"\"\n                try:\n                    n_in = len(op_prop.list_arguments())\n                    n_out = len(op_prop.list_outputs())\n                    n_aux = len(op_prop.list_auxiliary_states())\n                    assert num_tensor == n_in + n_out + n_aux\n\n                    shapes = [[tensor_shapes[i][j] for j in range(tensor_dims[i])]\n                              for i in range(n_in)]\n                    ret = op_prop.infer_shape(shapes)\n                    if len(ret) == 2:\n                        ishape, oshape = ret\n                        ashape = []\n                    elif len(ret) == 3:\n                        ishape, oshape, ashape = ret\n                    else:\n                        raise AssertionError(\"infer_shape must return 2 or 3 lists\")\n                    assert len(oshape) == n_out, \\\n                        \"InferShape Error: expecting %d entries in returned output \" \\\n                        \"shapes, got %d.\"%(n_out, len(oshape))\n                    assert len(ishape) == n_in, \\\n                        \"InferShape Error: expecting %d entries in returned input \" \\\n                        \"shapes, got %d.\"%(n_in, len(ishape))\n                    assert len(ashape) == n_aux, \\\n                        \"InferShape Error: expecting %d entries in returned aux state \" \\\n                        \"shapes, got %d.\"%(n_aux, len(ashape))\n                    rshape = list(ishape) + list(oshape) + list(ashape)\n                    for i in range(n_in+n_out+n_aux):\n                        tensor_shapes[i] = cast(c_array_buf(mx_int,\n                                                            array('i', rshape[i])),\n                                                POINTER(mx_int))\n                        tensor_dims[i] = len(rshape[i])\n\n                    infer_shape_entry._ref_holder = [tensor_shapes]\n                except Exception:\n                    print('Error in %s.infer_shape: %s' % (reg_name, traceback.format_exc()))\n                    return False\n                return True\n\n\n            def infer_storage_type_backward_entry(num_tensor, tensor_stypes, tags, _):\n                # pylint: disable=C0301\n                \"\"\"C Callback for CustomOpProp::InferStorageTypeBackward\"\"\"\n                try:\n                    tensors = [[] for i in range(5)]\n                    for i in range(num_tensor):\n                        tensors[tags[i]].append(_STORAGE_TYPE_ID_TO_STR[tensor_stypes[i]])\n                    # Ordering of stypes: ograd, input, output, igrad, aux\n                    tensors = [tensors[3], tensors[0], tensors[1], tensors[2], tensors[4]]\n                    ret = op_prop.infer_storage_type_backward(tensors[0],\n                                                              tensors[1],\n                                                              tensors[2],\n                                                              tensors[3],\n                                                              tensors[4])\n                    if len(ret) == 4:\n                        ret += []\n                    elif len(ret) == 5:\n                        pass\n                    else:\n                        raise AssertionError(\"infer_storage_type_backward must return 4 or 5 lists\")\n                    assert len(ret[0]) == len(tensors[0]), \\\n                        \"InferStorageTypeBackward Error: expecting == %d \" \\\n                        \"entries in returned output gradient \" \\\n                        \"stypes, got %d.\"%(len(tensors[0]), len(ret[0]))\n                    assert len(ret[1]) == len(tensors[1]), \\\n                        \"InferStorageTypeBackward Error: expecting == %d \" \\\n                        \"entries in returned input stypes, \" \\\n                        \"got %d.\"%(len(tensors[1]), len(ret[1]))\n                    assert len(ret[2]) == len(tensors[2]), \\\n                        \"InferStorageTypeBackward Error: expecting == %d \" \\\n                        \"entries in returned output stypes, \" \\\n                        \"got %d.\"%(len(tensors[2]), len(ret[2]))\n                    assert len(ret[3]) == len(tensors[3]), \\\n                        \"InferStorageTypeBackward Error: expecting == %d \" \\\n                        \"entries in returned input gradient stypes, \" \\\n                        \"got %d.\"%(len(tensors[3]), len(ret[3]))\n                    assert len(ret[4]) == len(tensors[4]), \\\n                        \"InferStorageTypeBackward Error: expecting == %d \" \\\n                        \"entries in returned aux stypes, \" \\\n                        \"got %d.\"%(len(tensors[4]), len(ret[4]))\n                    rstype = []\n                    for i, ret_list in enumerate(ret):\n                        rstype.extend(ret_list)\n\n                    for i, stype in enumerate(rstype):\n                        assert stype != _STORAGE_TYPE_ID_TO_STR[_STORAGE_TYPE_UNDEFINED], \\\n                            \"stype should not be undefined\"\n                        assert stype in _STORAGE_TYPE_STR_TO_ID, \\\n                            \"Provided stype: %s is not valid \" \\\n                            \"valid stypes are %s, %s, %s\"%(stype,\n                                                           _STORAGE_TYPE_ID_TO_STR[_STORAGE_TYPE_DEFAULT],\n                                                           _STORAGE_TYPE_ID_TO_STR[_STORAGE_TYPE_ROW_SPARSE],\n                                                           _STORAGE_TYPE_ID_TO_STR[_STORAGE_TYPE_CSR])\n                        tensor_stypes[i] = _STORAGE_TYPE_STR_TO_ID[stype]\n\n                    infer_storage_type_backward_entry._ref_holder = [tensor_stypes]\n                except Exception:\n                    print('Error in %s.infer_type: %s' % (reg_name, traceback.format_exc()))\n                    return False\n                return True\n\n            def infer_storage_type_entry(num_tensor, tensor_stypes, _):\n                \"\"\"C Callback for CustomOpProp::InferStorageType\"\"\"\n                try:\n                    n_in = len(op_prop.list_arguments())\n                    n_out = len(op_prop.list_outputs())\n                    n_aux = len(op_prop.list_auxiliary_states())\n                    assert num_tensor == n_in + n_out + n_aux\n\n                    stypes = [_STORAGE_TYPE_ID_TO_STR[tensor_stypes[i]] for i in range(n_in)]\n                    ret = op_prop.infer_storage_type(stypes)\n                    if len(ret) == 2:\n                        istype, ostype = ret\n                        astype = []\n                    elif len(ret) == 3:\n                        istype, ostype, astype = ret\n                    else:\n                        raise AssertionError(\"infer_storage_type must return 2 or 3 lists\")\n\n                    assert len(ostype) == n_out, \\\n                        \"InferStorageType Error: expecting %d entries in returned output \" \\\n                        \"stypes, got %d.\"%(n_out, len(ostype))\n                    assert len(istype) == n_in, \\\n                        \"InferStorageType Error: expecting %d entries in returned input \" \\\n                        \"stypes, got %d.\"%(n_in, len(istype))\n                    assert len(astype) == n_aux, \\\n                        \"InferStorageType Error: expecting %d entries in returned aux state \" \\\n                        \"stypes, got %d.\"%(n_aux, len(astype))\n                    rtype = list(istype) + list(ostype) + list(astype)\n                    for i, dtype in enumerate(rtype):\n                        tensor_stypes[i] = _STORAGE_TYPE_STR_TO_ID[dtype]\n                    infer_storage_type_entry._ref_holder = [tensor_stypes]\n                except Exception:\n                    print('Error in %s.infer_type: %s' % (reg_name, traceback.format_exc()))\n                    return False\n                return True\n\n            def infer_type_entry(num_tensor, tensor_types, _):\n                \"\"\"C Callback for CustomOpProp::InferType\"\"\"\n                try:\n                    n_in = len(op_prop.list_arguments())\n                    n_out = len(op_prop.list_outputs())\n                    n_aux = len(op_prop.list_auxiliary_states())\n                    assert num_tensor == n_in + n_out + n_aux\n\n                    types = [_DTYPE_MX_TO_NP[tensor_types[i]] for i in range(n_in)]\n                    ret = op_prop.infer_type(types)\n                    if len(ret) == 2:\n                        itype, otype = ret\n                        atype = []\n                    elif len(ret) == 3:\n                        itype, otype, atype = ret\n                    else:\n                        raise AssertionError(\"infer_type must return 2 or 3 lists\")\n                    assert len(otype) == n_out, \\\n                        \"InferType Error: expecting %d entries in returned output \" \\\n                        \"types, got %d.\"%(n_out, len(otype))\n                    assert len(itype) == n_in, \\\n                        \"InferType Error: expecting %d entries in returned input \" \\\n                        \"types, got %d.\"%(n_in, len(itype))\n                    assert len(atype) == n_aux, \\\n                        \"InferType Error: expecting %d entries in returned aux state \" \\\n                        \"types, got %d.\"%(n_aux, len(atype))\n                    rtype = list(itype) + list(otype) + list(atype)\n                    for i, dtype in enumerate(rtype):\n                        tensor_types[i] = _DTYPE_NP_TO_MX[dtype]\n\n                    infer_type_entry._ref_holder = [tensor_types]\n                except Exception:\n                    print('Error in %s.infer_type: %s' % (reg_name, traceback.format_exc()))\n                    return False\n                return True\n\n            def list_outputs_entry(out, _):\n                \"\"\"C Callback for CustomOpProp::ListOutputs\"\"\"\n                try:\n                    ret = op_prop.list_outputs()\n                    ret = [c_str(i) for i in ret] + [c_char_p(0)]\n                    ret = c_array(c_char_p, ret)\n                    out[0] = cast(ret, POINTER(POINTER(c_char)))\n\n                    list_outputs_entry._ref_holder = [out]\n                except Exception:\n                    print('Error in %s.list_outputs: %s' % (reg_name, traceback.format_exc()))\n                    return False\n                return True\n\n            def list_arguments_entry(out, _):\n                \"\"\"C Callback for CustomOpProp::ListArguments\"\"\"\n                try:\n                    ret = op_prop.list_arguments()\n                    ret = [c_str(i) for i in ret] + [c_char_p(0)]\n                    ret = c_array(c_char_p, ret)\n                    out[0] = cast(ret, POINTER(POINTER(c_char)))\n\n                    list_arguments_entry._ref_holder = [out]\n                except Exception:\n                    print('Error in %s.list_arguments: %s' % (reg_name, traceback.format_exc()))\n                    return False\n                return True\n\n            def list_auxiliary_states_entry(out, _):\n                \"\"\"C Callback for CustomOpProp::ListAuxiliaryStates\"\"\"\n                try:\n                    ret = op_prop.list_auxiliary_states()\n                    ret = [c_str(i) for i in ret] + [c_char_p(0)]\n                    ret = c_array(c_char_p, ret)\n                    out[0] = cast(ret, POINTER(POINTER(c_char)))\n\n                    list_auxiliary_states_entry._ref_holder = [out]\n                except Exception:\n                    tb = traceback.format_exc()\n                    print('Error in %s.list_auxiliary_states: %s' % (reg_name, tb))\n                    return False\n                return True\n\n            def declare_backward_dependency_entry(out_grad, in_data, out_data, num_dep, deps, _):\n                \"\"\"C Callback for CustomOpProp::DeclareBacwardDependency\"\"\"\n                try:\n                    out_grad = [out_grad[i] for i in range(len(op_prop.list_outputs()))]\n                    in_data = [in_data[i] for i in range(len(op_prop.list_arguments()))]\n                    out_data = [out_data[i] for i in range(len(op_prop.list_outputs()))]\n                    rdeps = op_prop.declare_backward_dependency(out_grad, in_data, out_data)\n                    num_dep[0] = len(rdeps)\n                    _registry.result_deps = set()\n                    for dep in rdeps:\n                        _registry.result_deps.add(dep)\n                    rdeps = cast(c_array_buf(c_int, array('i', rdeps)), c_int_p)\n                    deps[0] = rdeps\n\n                    declare_backward_dependency_entry._ref_holder = [deps]\n                except Exception:\n                    tb = traceback.format_exc()\n                    print('Error in %s.declare_backward_dependency: %s' % (reg_name, tb))\n                    return False\n                return True\n\n            def create_operator_entry(ctx, num_inputs, shapes, ndims, dtypes, ret, _):\n                \"\"\"C Callback for CustomOpProp::CreateOperator\"\"\"\n                try:\n                    ctx = py_str(ctx)\n                    sep = ctx.find('(')\n                    ctx = context.Context(ctx[:sep], int(ctx[sep+1:-1]))\n                    ndims = [ndims[i] for i in range(num_inputs)]\n                    shapes = [[shapes[i][j] for j in range(ndims[i])] for i in range(num_inputs)]\n                    dtypes = [dtypes[i] for i in range(num_inputs)]\n                    op = op_prop.create_operator(ctx, shapes, dtypes)\n\n                    def forward_entry(num_ndarray, ndarraies, tags, reqs, is_train, _):\n                        \"\"\"C Callback for CustomOp::Forward\"\"\"\n                        try:\n                            tensors = [[] for i in range(5)]\n                            for i in range(num_ndarray):\n                                if tags[i] == 1 or tags[i] == 4:\n                                    tensors[tags[i]].append(_ndarray_cls(cast(ndarraies[i],\n                                                                              NDArrayHandle),\n                                                                         writable=True))\n                                else:\n                                    tensors[tags[i]].append(_ndarray_cls(cast(ndarraies[i],\n                                                                              NDArrayHandle),\n                                                                         writable=False))\n                            reqs = [req_enum[reqs[i]] for i in range(len(tensors[1]))]\n                            with ctx:\n                                op.forward(is_train=is_train, req=reqs,\n                                           in_data=tensors[0], out_data=tensors[1],\n                                           aux=tensors[4])\n                        except Exception:\n                            print('Error in CustomOp.forward: %s' % traceback.format_exc())\n                            return False\n                        return True\n\n                    def backward_entry(num_ndarray, ndarraies, tags, reqs, is_train, _):\n                        \"\"\"C Callback for CustomOp::Backward\"\"\"\n                        # pylint: disable=W0613\n                        try:\n                            tensors = [[] for i in range(5)]\n                            num_outputs = len(op_prop.list_outputs())\n                            num_args = len(op_prop.list_arguments())\n                            for i in range(num_ndarray):\n                                if i in _registry.result_deps or i >= (num_outputs * 2 + num_args):\n                                    # If it is a backward dependency or output or aux:\n                                    # Set stype as undefined so that it returns\n                                    # ndarray based on existing stype\n                                    stype = _STORAGE_TYPE_UNDEFINED\n                                else:\n                                    # If it is some input, output or out grad ndarray not part of\n                                    # backward dependency it is empty and thus the ndarray should\n                                    # be set to default\n                                    stype = _STORAGE_TYPE_DEFAULT\n                                if tags[i] == 2 or tags[i] == 4:\n                                    tensors[tags[i]].append(_ndarray_cls(cast(ndarraies[i],\n                                                                              NDArrayHandle),\n                                                                         writable=True,\n                                                                         stype=stype))\n                                else:\n                                    tensors[tags[i]].append(_ndarray_cls(cast(ndarraies[i],\n                                                                              NDArrayHandle),\n                                                                         writable=False,\n                                                                         stype=stype))\n                            reqs = [req_enum[reqs[i]] for i in range(len(tensors[2]))]\n                            with ctx:\n                                op.backward(req=reqs,\n                                            in_data=tensors[0], out_data=tensors[1],\n                                            in_grad=tensors[2], out_grad=tensors[3],\n                                            aux=tensors[4])\n                        except Exception:\n                            print('Error in CustomOp.backward: %s' % traceback.format_exc())\n                            return False\n                        return True\n\n                    cur = _registry.inc()\n\n                    def delete_entry(_):\n                        \"\"\"C Callback for CustomOp::del\"\"\"\n                        try:\n                            del _registry.ref_holder[cur]\n                        except Exception:\n                            print('Error in CustomOp.delete: %s' % traceback.format_exc())\n                            return False\n                        return True\n\n                    callbacks = [del_functype(delete_entry),\n                                 fb_functype(forward_entry),\n                                 fb_functype(backward_entry)]\n                    callbacks = [cast(i, CFUNCTYPE(c_int)) for i in callbacks]\n                    contexts = [None, None, None]\n                    ret[0] = MXCallbackList(c_int(len(callbacks)),\n                                            cast(c_array(CFUNCTYPE(c_int), callbacks),\n                                                 POINTER(CFUNCTYPE(c_int))),\n                                            cast(c_array(c_void_p, contexts),\n                                                 POINTER(c_void_p)))\n                    op._ref_holder = [ret]\n                    _registry.ref_holder[cur] = op\n                except Exception:\n                    print('Error in %s.create_operator: %s' % (reg_name, traceback.format_exc()))\n                    return False\n                return True\n\n            cur = _registry.inc()\n\n            def delete_entry(_):\n                \"\"\"C Callback for CustomOpProp::del\"\"\"\n                try:\n                    del _registry.ref_holder[cur]\n                except Exception:\n                    print('Error in CustomOpProp.delete: %s' % traceback.format_exc())\n                    return False\n                return True\n\n            callbacks = [del_functype(delete_entry),\n                         list_functype(list_arguments_entry),\n                         list_functype(list_outputs_entry),\n                         list_functype(list_auxiliary_states_entry),\n                         infershape_functype(infer_shape_entry),\n                         deps_functype(declare_backward_dependency_entry),\n                         createop_functype(create_operator_entry),\n                         infertype_functype(infer_type_entry),\n                         inferstorage_functype(infer_storage_type_entry),\n                         inferstorage_backward_functype(infer_storage_type_backward_entry)]\n            callbacks = [cast(i, CFUNCTYPE(c_int)) for i in callbacks]\n            contexts = [None]*len(callbacks)\n            ret[0] = MXCallbackList(c_int(len(callbacks)),\n                                    cast(c_array(CFUNCTYPE(c_int), callbacks),\n                                         POINTER(CFUNCTYPE(c_int))),\n                                    cast(c_array(c_void_p, contexts),\n                                         POINTER(c_void_p)))\n            op_prop._ref_holder = [ret]\n            _registry.ref_holder[cur] = op_prop\n            return True\n\n        creator_functype = CFUNCTYPE(c_int, c_char_p, c_int, POINTER(c_char_p),\n                                     POINTER(c_char_p), POINTER(MXCallbackList))\n        creator_func = creator_functype(creator)\n        check_call(_LIB.MXCustomOpRegister(c_str(reg_name), creator_func))\n        cur = _registry.inc()\n        _registry.ref_holder[cur] = creator_func\n        return prop_cls\n    return do_register", "response": "Register a subclass of CustomOpProp to the registry with name reg_name."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndeclare dependencies of this operator for backward pass.", "response": "def declare_backward_dependency(self, out_grad, in_data, out_data):\n        \"\"\"Declare dependencies of this operator for backward pass.\n\n        Parameters\n        ----------\n        out_grad : list of int\n            ids of out_grad blobs.\n        in_data : list of int\n            ids of in_data blobs.\n        out_data: list of int\n            ids of out_data blobs.\n\n        Returns\n        -------\n        deps : list of int\n            ids of the needed blobs.\n        \"\"\"\n        deps = []\n        if self.need_top_grad():\n            deps.extend(out_grad)\n        deps.extend(in_data)\n        deps.extend(out_data)\n        return deps"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nassigning a value to a key in a dict.", "response": "def assign(self, dst, req, src):\n        \"\"\"Helper function for assigning into dst depending on requirements.\"\"\"\n        if req == 'null':\n            return\n        elif req in ('write', 'inplace'):\n            dst[:] = src\n        elif req == 'add':\n            dst[:] += src"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef infer_type(self, in_type):\n        return in_type, [in_type[0]]*len(self.list_outputs()), \\\n            [in_type[0]]*len(self.list_auxiliary_states())", "response": "infer_type interface. override to create new operators\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef infer_storage_type(self, in_stype):\n        for i, stype in enumerate(in_stype):\n            assert stype == _STORAGE_TYPE_ID_TO_STR[_STORAGE_TYPE_DEFAULT], \\\n            \"Default infer_storage_type implementation doesnt allow non default stypes: \" \\\n            \"found non default stype '%s' for in_stype[%d]. Please implement \" \\\n            \"infer_storage_type and infer_storage_type_backward interface \" \\\n            \"in your custom operator if you have non-default input/output stypes\" % (stype, i)\n        return in_stype, \\\n               [_STORAGE_TYPE_ID_TO_STR[_STORAGE_TYPE_DEFAULT]]*len(self.list_outputs()), \\\n               [_STORAGE_TYPE_ID_TO_STR[_STORAGE_TYPE_DEFAULT]]*len(self.list_auxiliary_states())", "response": "Infer storage type of a set of variables."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninfers storage type of inputs and outputs in the backward pass.", "response": "def infer_storage_type_backward(self, ograd_stype, in_stype, out_stype, igrad_stype, aux_stype):\n        \"\"\"infer_storage_type_backward interface. Used to infer storage\n        type of inputs and outputs in the backward pass.\n\n        Will raise an error if undefined storage type is returned.\n        Returned lists have to be the same size as the input lists to infer_storage_type_backward,\n        otherwise an exception will be thrown. When this interface is not implemented,\n        all stypes will be inferred as default.\n\n        Parameters\n        ----------\n        ograd_stype : list\n            list of output gradient storage types\n        in_stype : list\n            list of input storage types\n        out_stype : list\n            list of output storage types\n        igrad_stype : list\n            list of input gradient storage types\n        aux_stype : list\n            list of auxiliary storage types\n\n        Returns\n        -------\n        ograd_stype : list\n            list of inferred output gradient storage types\n        in_stype : list\n            list of inferred input storage types\n        out_stype : list\n            list of inferred output storage types\n        igrad_stype : list\n            list of inferred input gradient storage types\n        aux_stype : list\n            list of inferred storage types for auxiliary states\n        \"\"\"\n        for i, stype in enumerate(ograd_stype):\n            assert stype == _STORAGE_TYPE_ID_TO_STR[_STORAGE_TYPE_DEFAULT], \\\n            \"Default infer_storage_type_backward implementation doesnt allow non default stypes: \" \\\n             \"found non default stype '%s' for ograd_stype[%d]. Please implement \" \\\n             \"infer_storage_type and infer_storage_type_backward interface \" \\\n             \"in your custom operator if you have non-default output gradient stypes\" % (stype, i)\n        for i, stype in enumerate(igrad_stype):\n            if stype == _STORAGE_TYPE_ID_TO_STR[_STORAGE_TYPE_UNDEFINED]:\n                stype = _STORAGE_TYPE_ID_TO_STR[_STORAGE_TYPE_DEFAULT]\n            assert stype == _STORAGE_TYPE_ID_TO_STR[_STORAGE_TYPE_DEFAULT], \\\n            \"Default infer_storage_type_backward implementation doesnt allow non default stypes: \" \\\n            \"found non default stype '%s' for igrad_stype[%d]. Please implement \" \\\n            \"infer_storage_type and infer_storage_type_backward interface \" \\\n            \"in your custom operator if you have non-default input gradient stypes\" % (stype, i)\n        stype_lists = [ograd_stype, in_stype, out_stype, igrad_stype, aux_stype]\n        for stype_list in stype_lists:\n            stype_list[:] = len(stype_list) * [_STORAGE_TYPE_ID_TO_STR[_STORAGE_TYPE_DEFAULT]]\n        return stype_lists[0], stype_lists[1], stype_lists[2], stype_lists[3], stype_lists[4]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeclares dependencies of this operator for backward pass.", "response": "def declare_backward_dependency(self, out_grad, in_data, out_data):\n        \"\"\"Declare dependencies of this operator for backward pass.\n\n        Parameters\n        ----------\n        out_grad : list of int\n            ids of out_grad blobs.\n        in_data : list of int\n            ids of in_data blobs.\n        out_data: list of int\n            ids of out_data blobs.\n\n        Returns\n        -------\n        deps : list of int\n            ids of the needed blobs.\n        \"\"\"\n        deps = []\n        if self.need_top_grad_:\n            deps.extend(out_grad)\n        deps.extend(in_data)\n        deps.extend(out_data)\n        return deps"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets index for new entry.", "response": "def inc(self):\n        \"\"\"Get index for new entry.\"\"\"\n        self.lock.acquire()\n        cur = self.counter\n        self.counter += 1\n        self.lock.release()\n        return cur"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef close(self):\n        if not self.is_open:\n            return\n        super(IndexCreator, self).close()\n        self.fidx.close()", "response": "Closes the record and index files."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef tell(self):\n        pos = ctypes.c_size_t()\n        check_call(_LIB.MXRecordIOReaderTell(self.handle, ctypes.byref(pos)))\n        return pos.value", "response": "Returns the current position of the record header."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate the index file from open record file", "response": "def create_index(self):\n        \"\"\"Creates the index file from open record file\n        \"\"\"\n        self.reset()\n        counter = 0\n        pre_time = time.time()\n        while True:\n            if counter % 1000 == 0:\n                cur_time = time.time()\n                print('time:', cur_time - pre_time, ' count:', counter)\n            pos = self.tell()\n            cont = self.read()\n            if cont is None:\n                break\n            key = self.key_type(counter)\n            self.fidx.write('%s\\t%d\\n'%(str(key), pos))\n            counter = counter + 1"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns commands and raise exception if failed", "response": "def _run_cmd(cmds):\n    \"\"\"Run commands, raise exception if failed\"\"\"\n    if not isinstance(cmds, str):\n        cmds = \"\".join(cmds)\n    print(\"Execute \\\"%s\\\"\" % cmds)\n    try:\n        subprocess.check_call(cmds, shell=True)\n    except subprocess.CalledProcessError as err:\n        print(err)\n        raise err"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef generate_doxygen(app):\n    _run_cmd(\"cd %s/.. && make doxygen\" % app.builder.srcdir)\n    _run_cmd(\"cp -rf doxygen/html %s/doxygen\" % app.builder.outdir)", "response": "Run the doxygen make commands"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build_mxnet(app):\n    if not os.path.exists(os.path.join(app.builder.srcdir, '..', 'config.mk')):\n        _run_cmd(\"cd %s/.. && cp make/config.mk config.mk && make -j$(nproc) USE_MKLDNN=0 USE_CPP_PACKAGE=1 \" %\n                app.builder.srcdir)\n    else:\n        _run_cmd(\"cd %s/.. && make -j$(nproc) USE_MKLDNN=0 USE_CPP_PACKAGE=1 \" %\n                app.builder.srcdir)", "response": "Build mxnet. so lib"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build_scala(app):\n    if any(v in _BUILD_VER for v in ['1.2.', '1.3.', '1.4.']):\n        _run_cmd(\"cd %s/.. && make scalapkg\" % app.builder.srcdir)\n        _run_cmd(\"cd %s/.. && make scalainstall\" % app.builder.srcdir)\n    else:\n        _run_cmd(\"cd %s/../scala-package && mvn -B install -DskipTests\" % app.builder.srcdir)", "response": "build scala for scala docs java docs clojure docs to use"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build_scala_docs(app):\n    scala_path = app.builder.srcdir + '/../scala-package'\n    scala_doc_sources = 'find . -type f -name \"*.scala\" | egrep \\\"\\.\\/core|\\.\\/infer\\\" | egrep -v \\\"\\/javaapi\\\"  | egrep -v \\\"Suite\\\"'\n    scala_doc_classpath = ':'.join([\n        '`find native -name \"*.jar\" | grep \"target/lib/\" | tr \"\\\\n\" \":\" `',\n        '`find macros -name \"*.jar\" | tr \"\\\\n\" \":\" `',\n        '`find core -name \"*.jar\" | tr \"\\\\n\" \":\" `',\n        '`find infer -name \"*.jar\" | tr \"\\\\n\" \":\" `'\n    ])\n    # There are unresolvable errors on mxnet 1.2.x. We are ignoring those errors while aborting the ci on newer versions\n    scala_ignore_errors = '; exit 0' if any(v in _BUILD_VER for v in ['1.2.', '1.3.']) else ''\n    _run_cmd('cd {}; scaladoc `{}` -classpath {} -feature -deprecation {}'\n             .format(scala_path, scala_doc_sources, scala_doc_classpath, scala_ignore_errors))\n    dest_path = app.builder.outdir + '/api/scala/docs'\n    _run_cmd('rm -rf ' + dest_path)\n    _run_cmd('mkdir -p ' + dest_path)\n    # 'index' and 'package.html' do not exist in later versions of scala; delete these after upgrading scala>2.12.x\n    scaladocs = ['index', 'index.html', 'org', 'lib', 'index.js', 'package.html']\n    for doc_file in scaladocs:\n        _run_cmd('cd ' + scala_path + ' && mv -f ' + doc_file + ' ' + dest_path + '; exit 0')", "response": "build scala doc and then move the outdir"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef build_java_docs(app):\n    java_path = app.builder.srcdir + '/../scala-package'\n    java_doc_sources = 'find . -type f -name \"*.scala\" | egrep \\\"\\.\\/core|\\.\\/infer\\\" | egrep \\\"\\/javaapi\\\" | egrep -v \\\"Suite\\\"'\n    java_doc_classpath = ':'.join([\n        '`find native -name \"*.jar\" | grep \"target/lib/\" | tr \"\\\\n\" \":\" `',\n        '`find macros -name \"*.jar\" | tr \"\\\\n\" \":\" `',\n        '`find core -name \"*.jar\" | tr \"\\\\n\" \":\" `',\n        '`find infer -name \"*.jar\" | tr \"\\\\n\" \":\" `'\n    ])\n    _run_cmd('cd {}; scaladoc `{}` -classpath {} -feature -deprecation'\n             .format(java_path, java_doc_sources, java_doc_classpath))\n    dest_path = app.builder.outdir + '/api/java/docs'\n    _run_cmd('rm -rf ' + dest_path)\n    _run_cmd('mkdir -p ' + dest_path)\n    javadocs = ['index', 'index.html', 'org', 'lib', 'index.js', 'package.html']\n    for doc_file in javadocs:\n        _run_cmd('cd ' + java_path + ' && mv -f ' + doc_file + ' ' + dest_path + '; exit 0')", "response": "build java docs and then move the outdir"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef build_clojure_docs(app):\n    clojure_path = app.builder.srcdir + '/../contrib/clojure-package'\n    _run_cmd('cd ' + clojure_path + '; lein codox')\n    dest_path = app.builder.outdir + '/api/clojure/docs'\n    _run_cmd('rm -rf ' + dest_path)\n    _run_cmd('mkdir -p ' + dest_path)\n    clojure_doc_path = app.builder.srcdir + '/../contrib/clojure-package/target/doc'\n    _run_cmd('cd ' + clojure_doc_path + ' && cp -r *  ' + dest_path + '; exit 0')", "response": "build clojure doc and then move the outdir"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _convert_md_table_to_rst(table):\n    if len(table) < 3:\n        return ''\n    out = '```eval_rst\\n.. list-table::\\n   :header-rows: 1\\n\\n'\n    for i,l in enumerate(table):\n        cols = l.split('|')[1:-1]\n        if i == 0:\n            ncol = len(cols)\n        else:\n            if len(cols) != ncol:\n                return ''\n        if i == 1:\n            for c in cols:\n                if len(c) is not 0 and '---' not in c:\n                    return ''\n        else:\n            for j,c in enumerate(cols):\n                out += '   * - ' if j == 0 else '     - '\n                out += pypandoc.convert_text(\n                    c, 'rst', format='md').replace('\\n', ' ').replace('\\r', '') + '\\n'\n    out += '```\\n'\n    return out", "response": "Convert a markdown table to rst format"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts the tables in a markdown file into rst format", "response": "def convert_table(app, docname, source):\n    \"\"\"Find tables in a markdown and then convert them into the rst format\"\"\"\n    num_tables = 0\n    for i,j in enumerate(source):\n        table = []\n        output = ''\n        in_table = False\n        for l in j.split('\\n'):\n            r = l.strip()\n            if r.startswith('|'):\n                table.append(r)\n                in_table = True\n            else:\n                if in_table is True:\n                    converted = _convert_md_table_to_rst(table)\n                    if converted is '':\n                        print(\"Failed to convert the markdown table\")\n                        print(table)\n                    else:\n                        num_tables += 1\n                    output += converted\n                    in_table = False\n                    table = []\n                output += l + '\\n'\n        source[i] = output\n    if num_tables > 0:\n        print('Converted %d tables in %s' % (num_tables, docname))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_blocks(lines):\n    cur_block = []\n    pre_lang = None\n    pre_in_code = None\n    for (l, in_code, cur_lang, _) in _parse_code_lines(lines):\n        if in_code != pre_in_code:\n            if pre_in_code and len(cur_block) >= 2:\n                cur_block = cur_block[1:-1] # remove ```\n            # remove empty lines at head\n            while len(cur_block) > 0:\n                if len(cur_block[0]) == 0:\n                    cur_block.pop(0)\n                else:\n                    break\n            # remove empty lines at tail\n            while len(cur_block) > 0:\n                if len(cur_block[-1]) == 0:\n                    cur_block.pop()\n                else:\n                    break\n            if len(cur_block):\n                yield (pre_in_code, pre_lang, cur_block)\n            cur_block = []\n        cur_block.append(l)\n        pre_lang = cur_lang\n        pre_in_code = in_code\n    if len(cur_block):\n        yield (pre_in_code, pre_lang, cur_block)", "response": "split lines into code and non - code blocks and yield them as lists of tuples"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_python_block_output(src, global_dict, local_dict):\n    src = '\\n'.join([l for l in src.split('\\n')\n                     if not l.startswith('%') and not 'plt.show()' in l])\n    ret_status = True\n    err = ''\n    with _string_io() as s:\n        try:\n            exec(src, global_dict, global_dict)\n        except Exception as e:\n            err = str(e)\n            ret_status = False\n    return (ret_status, s.getvalue()+err)", "response": "Evaluate python source codes\nElems returns True if success otherwise False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef copy_artifacts(app):\n    dest_path = app.builder.outdir + '/error'\n    source_path = app.builder.srcdir + '/build_version_doc/artifacts'\n    _run_cmd('cd ' + app.builder.srcdir)\n    _run_cmd('rm -rf ' + dest_path)\n    _run_cmd('mkdir -p ' + dest_path)\n    _run_cmd('cp ' + source_path + '/404.html ' + dest_path)\n    _run_cmd('cp ' + source_path + '/api.html ' + dest_path)\n    dest_path = app.builder.outdir + '/_static'\n    _run_cmd('rm -rf ' + dest_path)\n    _run_cmd('mkdir -p ' + dest_path)\n    _run_cmd('cp ' + app.builder.srcdir + '/_static/mxnet.css ' + dest_path)", "response": "Copies artifacts needed for website presentation"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef download_caffe_model(model_name, meta_info, dst_dir='./model'):\n    if not os.path.isdir(dst_dir):\n        os.mkdir(dst_dir)\n    model_name = os.path.join(dst_dir, model_name)\n\n    assert 'prototxt' in meta_info, \"missing prototxt url\"\n    proto_url, proto_sha1 = meta_info['prototxt']\n    prototxt = mx.gluon.utils.download(proto_url,\n                                       model_name+'_deploy.prototxt',\n                                       sha1_hash=proto_sha1)\n\n    assert 'caffemodel' in meta_info, \"mssing caffemodel url\"\n    caffemodel_url, caffemodel_sha1 = meta_info['caffemodel']\n    caffemodel = mx.gluon.utils.download(caffemodel_url,\n                                         model_name+'.caffemodel',\n                                         sha1_hash=caffemodel_sha1)\n    assert 'mean' in meta_info, 'no mean info'\n    mean = meta_info['mean']\n    if isinstance(mean[0], str):\n        mean_url, mean_sha1 = mean\n        mean = mx.gluon.utils.download(mean_url,\n                                       model_name+'_mean.binaryproto',\n                                       sha1_hash=mean_sha1)\n    return (prototxt, caffemodel, mean)", "response": "Download the caffe model into disk by the given meta info"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndownloading and convert a caffe model and save it to dst_dir", "response": "def convert_caffe_model(model_name, meta_info, dst_dir='./model'):\n    \"\"\"Download, convert and save a caffe model\"\"\"\n\n    (prototxt, caffemodel, mean) = download_caffe_model(model_name, meta_info, dst_dir)\n    model_name = os.path.join(dst_dir, model_name)\n    convert_model(prototxt, caffemodel, model_name)\n    if isinstance(mean, str):\n        mx_mean = model_name + '-mean.nd'\n        convert_mean(mean, mx_mean)\n        mean = mx_mean\n    return (model_name, mean)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrun _func with multi - process using params.", "response": "def multi_p_run(tot_num, _func, worker, params, n_process):\n    \"\"\"\n    Run _func with multi-process using params.\n    \"\"\"\n    from multiprocessing import Process, Queue\n    out_q = Queue()\n    procs = []\n\n    split_num = split_seq(list(range(0, tot_num)), n_process)\n\n    print(tot_num, \">>\", split_num)\n\n    split_len = len(split_num)\n    if n_process > split_len:\n        n_process = split_len\n\n    for i in range(n_process):\n        _p = Process(target=_func,\n                     args=(worker, split_num[i][0], split_num[i][1],\n                           params, out_q))\n        _p.daemon = True\n        procs.append(_p)\n        _p.start()\n\n    try:\n        result = []\n        for i in range(n_process):\n            result.append(out_q.get())\n        for i in procs:\n            i.join()\n    except KeyboardInterrupt:\n        print('Killing all the children in the pool.')\n        for i in procs:\n            i.terminate()\n            i.join()\n        return -1\n\n    while not out_q.empty():\n        print(out_q.get(block=False))\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef split_seq(sam_num, n_tile):\n    import math\n    print(sam_num)\n    print(n_tile)\n    start_num = sam_num[0::int(math.ceil(len(sam_num) / (n_tile)))]\n    end_num = start_num[1::]\n    end_num.append(len(sam_num))\n    return [[i, j] for i, j in zip(start_num, end_num)]", "response": "Split the number sam_num into n_tile sequence."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a namedtuple with default values", "response": "def namedtuple_with_defaults(typename, field_names, default_values=()):\n    \"\"\" create a namedtuple with default values \"\"\"\n    T = collections.namedtuple(typename, field_names)\n    T.__new__.__defaults__ = (None, ) * len(T._fields)\n    if isinstance(default_values, collections.Mapping):\n        prototype = T(**default_values)\n    else:\n        prototype = T(*default_values)\n    T.__new__.__defaults__ = tuple(prototype)\n    return T"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmerge dict a b with b overriding keys in a", "response": "def merge_dict(a, b):\n    \"\"\" merge dict a, b, with b overriding keys in a \"\"\"\n    c = a.copy()\n    c.update(b)\n    return c"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef zip_namedtuple(nt_list):\n    if not nt_list:\n        return dict()\n    if not isinstance(nt_list, list):\n        nt_list = [nt_list]\n    for nt in nt_list:\n        assert type(nt) == type(nt_list[0])\n    ret = {k : [v] for k, v in nt_list[0]._asdict().items()}\n    for nt in nt_list[1:]:\n        for k, v in nt._asdict().items():\n            ret[k].append(v)\n    return ret", "response": "accept list of namedtuple return a dict of zipped fields"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef config_as_dict(cfg):\n    ret = cfg.__dict__.copy()\n    # random cropping params\n    del ret['rand_crop_samplers']\n    assert isinstance(cfg.rand_crop_samplers, list)\n    ret = merge_dict(ret, zip_namedtuple(cfg.rand_crop_samplers))\n    num_crop_sampler = len(cfg.rand_crop_samplers)\n    ret['num_crop_sampler'] = num_crop_sampler  # must specify the #\n    ret['rand_crop_prob'] = 1.0 / (num_crop_sampler + 1) * num_crop_sampler\n    # random padding params\n    del ret['rand_pad']\n    ret = merge_dict(ret, cfg.rand_pad._asdict())\n    # color jitter\n    del ret['color_jitter']\n    ret = merge_dict(ret, cfg.color_jitter._asdict())\n    return ret", "response": "convert raw configuration to unified dictionary"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nimport the ONNX model file passed as a parameter into MXNet symbol and parameters.", "response": "def import_model(model_file):\n    \"\"\"Imports the ONNX model file, passed as a parameter, into MXNet symbol and parameters.\n    Operator support and coverage -\n    https://cwiki.apache.org/confluence/display/MXNET/MXNet-ONNX+Integration\n\n    Parameters\n    ----------\n    model_file : str\n        ONNX model file name\n\n    Returns\n    -------\n    sym : :class:`~mxnet.symbol.Symbol`\n        MXNet symbol object\n\n    arg_params : dict of ``str`` to :class:`~mxnet.ndarray.NDArray`\n        Dict of converted parameters stored in ``mxnet.ndarray.NDArray`` format\n\n    aux_params : dict of ``str`` to :class:`~mxnet.ndarray.NDArray`\n        Dict of converted parameters stored in ``mxnet.ndarray.NDArray`` format\n\n    Notes\n    -----\n    This method is available when you ``import mxnet.contrib.onnx``\n\n    \"\"\"\n    graph = GraphProto()\n\n    try:\n        import onnx\n    except ImportError:\n        raise ImportError(\"Onnx and protobuf need to be installed. \"\n                          + \"Instructions to install - https://github.com/onnx/onnx\")\n    # loads model file and returns ONNX protobuf object\n    model_proto = onnx.load_model(model_file)\n    sym, arg_params, aux_params = graph.from_onnx(model_proto.graph)\n    return sym, arg_params, aux_params"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the name and shape information of input and output tensors of the given ONNX model file.", "response": "def get_model_metadata(model_file):\n    \"\"\"\n    Returns the name and shape information of input and output tensors of the given ONNX model file.\n\n    Notes\n    -----\n    This method is available when you ``import mxnet.contrib.onnx``\n\n    Parameters\n    ----------\n    model_file : str\n        ONNX model file name\n\n    Returns\n    -------\n    model_metadata : dict\n        A dictionary object mapping various metadata to its corresponding value.\n        The dictionary will have the following template::\n\n          'input_tensor_data' : list of tuples representing the shape of the input paramters\n          'output_tensor_data' : list of tuples representing the shape of the output of the model\n    \"\"\"\n    graph = GraphProto()\n\n    try:\n        import onnx\n    except ImportError:\n        raise ImportError(\"Onnx and protobuf need to be installed. \"\n                          + \"Instructions to install - https://github.com/onnx/onnx\")\n    model_proto = onnx.load_model(model_file)\n    metadata = graph.get_graph_metadata(model_proto.graph)\n    return metadata"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwraps for a small Convolution group", "response": "def conv_act_layer(from_layer, name, num_filter, kernel=(1,1), pad=(0,0), \\\n    stride=(1,1), act_type=\"relu\", use_batchnorm=False):\n    \"\"\"\n    wrapper for a small Convolution group\n\n    Parameters:\n    ----------\n    from_layer : mx.symbol\n        continue on which layer\n    name : str\n        base name of the new layers\n    num_filter : int\n        how many filters to use in Convolution layer\n    kernel : tuple (int, int)\n        kernel size (h, w)\n    pad : tuple (int, int)\n        padding size (h, w)\n    stride : tuple (int, int)\n        stride size (h, w)\n    act_type : str\n        activation type, can be relu...\n    use_batchnorm : bool\n        whether to use batch normalization\n\n    Returns:\n    ----------\n    (conv, relu) mx.Symbols\n    \"\"\"\n    conv = mx.symbol.Convolution(data=from_layer, kernel=kernel, pad=pad, \\\n        stride=stride, num_filter=num_filter, name=\"{}_conv\".format(name))\n    if use_batchnorm:\n        conv = mx.symbol.BatchNorm(data=conv, name=\"{}_bn\".format(name))\n    relu = mx.symbol.Activation(data=conv, act_type=act_type, \\\n        name=\"{}_{}\".format(name, act_type))\n    return relu"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwrapping for a small Convolution group Parameters: ---------- from_layer : mx.symbol continue on which layer name : str base name of the new layers num_filter : int how many filters to use in Convolution layer kernel : tuple (int, int) kernel size (h, w) pad : tuple (int, int) padding size (h, w) stride : tuple (int, int) stride size (h, w) act_type : str activation type, can be relu... use_batchnorm : bool whether to use batch normalization Returns: ---------- (conv, relu) mx.Symbols", "response": "def legacy_conv_act_layer(from_layer, name, num_filter, kernel=(1,1), pad=(0,0), \\\n    stride=(1,1), act_type=\"relu\", use_batchnorm=False):\n    \"\"\"\n    wrapper for a small Convolution group\n\n    Parameters:\n    ----------\n    from_layer : mx.symbol\n        continue on which layer\n    name : str\n        base name of the new layers\n    num_filter : int\n        how many filters to use in Convolution layer\n    kernel : tuple (int, int)\n        kernel size (h, w)\n    pad : tuple (int, int)\n        padding size (h, w)\n    stride : tuple (int, int)\n        stride size (h, w)\n    act_type : str\n        activation type, can be relu...\n    use_batchnorm : bool\n        whether to use batch normalization\n\n    Returns:\n    ----------\n    (conv, relu) mx.Symbols\n    \"\"\"\n    assert not use_batchnorm, \"batchnorm not yet supported\"\n    bias = mx.symbol.Variable(name=\"conv{}_bias\".format(name),\n        init=mx.init.Constant(0.0), attr={'__lr_mult__': '2.0'})\n    conv = mx.symbol.Convolution(data=from_layer, bias=bias, kernel=kernel, pad=pad, \\\n        stride=stride, num_filter=num_filter, name=\"conv{}\".format(name))\n    relu = mx.symbol.Activation(data=conv, act_type=act_type, \\\n        name=\"{}{}\".format(act_type, name))\n    if use_batchnorm:\n        relu = mx.symbol.BatchNorm(data=relu, name=\"bn{}\".format(name))\n    return conv, relu"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwraps function to extract features from base network and SSD specific layers and add extra layers to the base network", "response": "def multi_layer_feature(body, from_layers, num_filters, strides, pads, min_filter=128):\n    \"\"\"Wrapper function to extract features from base network, attaching extra\n    layers and SSD specific layers\n\n    Parameters\n    ----------\n    from_layers : list of str\n        feature extraction layers, use '' for add extra layers\n        For example:\n        from_layers = ['relu4_3', 'fc7', '', '', '', '']\n        which means extract feature from relu4_3 and fc7, adding 4 extra layers\n        on top of fc7\n    num_filters : list of int\n        number of filters for extra layers, you can use -1 for extracted features,\n        however, if normalization and scale is applied, the number of filter for\n        that layer must be provided.\n        For example:\n        num_filters = [512, -1, 512, 256, 256, 256]\n    strides : list of int\n        strides for the 3x3 convolution appended, -1 can be used for extracted\n        feature layers\n    pads : list of int\n        paddings for the 3x3 convolution, -1 can be used for extracted layers\n    min_filter : int\n        minimum number of filters used in 1x1 convolution\n\n    Returns\n    -------\n    list of mx.Symbols\n\n    \"\"\"\n    # arguments check\n    assert len(from_layers) > 0\n    assert isinstance(from_layers[0], str) and len(from_layers[0].strip()) > 0\n    assert len(from_layers) == len(num_filters) == len(strides) == len(pads)\n\n    internals = body.get_internals()\n    layers = []\n    for k, params in enumerate(zip(from_layers, num_filters, strides, pads)):\n        from_layer, num_filter, s, p = params\n        if from_layer.strip():\n            # extract from base network\n            layer = internals[from_layer.strip() + '_output']\n            layers.append(layer)\n        else:\n            # attach from last feature layer\n            assert len(layers) > 0\n            assert num_filter > 0\n            layer = layers[-1]\n            num_1x1 = max(min_filter, num_filter // 2)\n            conv_1x1 = conv_act_layer(layer, 'multi_feat_%d_conv_1x1' % (k),\n                num_1x1, kernel=(1, 1), pad=(0, 0), stride=(1, 1), act_type='relu')\n            conv_3x3 = conv_act_layer(conv_1x1, 'multi_feat_%d_conv_3x3' % (k),\n                num_filter, kernel=(3, 3), pad=(p, p), stride=(s, s), act_type='relu')\n            layers.append(conv_3x3)\n    return layers"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates multibox layer from a list of mx. symbol objects.", "response": "def multibox_layer(from_layers, num_classes, sizes=[.2, .95],\n                    ratios=[1], normalization=-1, num_channels=[],\n                    clip=False, interm_layer=0, steps=[]):\n    \"\"\"\n    the basic aggregation module for SSD detection. Takes in multiple layers,\n    generate multiple object detection targets by customized layers\n\n    Parameters:\n    ----------\n    from_layers : list of mx.symbol\n        generate multibox detection from layers\n    num_classes : int\n        number of classes excluding background, will automatically handle\n        background in this function\n    sizes : list or list of list\n        [min_size, max_size] for all layers or [[], [], []...] for specific layers\n    ratios : list or list of list\n        [ratio1, ratio2...] for all layers or [[], [], ...] for specific layers\n    normalizations : int or list of int\n        use normalizations value for all layers or [...] for specific layers,\n        -1 indicate no normalizations and scales\n    num_channels : list of int\n        number of input layer channels, used when normalization is enabled, the\n        length of list should equals to number of normalization layers\n    clip : bool\n        whether to clip out-of-image boxes\n    interm_layer : int\n        if > 0, will add a intermediate Convolution layer\n    steps : list\n        specify steps for each MultiBoxPrior layer, leave empty, it will calculate\n        according to layer dimensions\n\n    Returns:\n    ----------\n    list of outputs, as [loc_preds, cls_preds, anchor_boxes]\n    loc_preds : localization regression prediction\n    cls_preds : classification prediction\n    anchor_boxes : generated anchor boxes\n    \"\"\"\n    assert len(from_layers) > 0, \"from_layers must not be empty list\"\n    assert num_classes > 0, \\\n        \"num_classes {} must be larger than 0\".format(num_classes)\n\n    assert len(ratios) > 0, \"aspect ratios must not be empty list\"\n    if not isinstance(ratios[0], list):\n        # provided only one ratio list, broadcast to all from_layers\n        ratios = [ratios] * len(from_layers)\n    assert len(ratios) == len(from_layers), \\\n        \"ratios and from_layers must have same length\"\n\n    assert len(sizes) > 0, \"sizes must not be empty list\"\n    if len(sizes) == 2 and not isinstance(sizes[0], list):\n        # provided size range, we need to compute the sizes for each layer\n         assert sizes[0] > 0 and sizes[0] < 1\n         assert sizes[1] > 0 and sizes[1] < 1 and sizes[1] > sizes[0]\n         tmp = np.linspace(sizes[0], sizes[1], num=(len(from_layers)-1))\n         # Ref for start_offset value:\n         # https://arxiv.org/abs/1512.02325\n         start_offset = 0.1\n         min_sizes = [start_offset] + tmp.tolist()\n         max_sizes = tmp.tolist() + [tmp[-1]+start_offset]\n         sizes = zip(min_sizes, max_sizes)\n    assert len(sizes) == len(from_layers), \\\n        \"sizes and from_layers must have same length\"\n\n    if not isinstance(normalization, list):\n        normalization = [normalization] * len(from_layers)\n    assert len(normalization) == len(from_layers)\n\n    assert sum(x > 0 for x in normalization) <= len(num_channels), \\\n        \"must provide number of channels for each normalized layer\"\n\n    if steps:\n        assert len(steps) == len(from_layers), \"provide steps for all layers or leave empty\"\n\n    loc_pred_layers = []\n    cls_pred_layers = []\n    anchor_layers = []\n    num_classes += 1 # always use background as label 0\n\n    for k, from_layer in enumerate(from_layers):\n        from_name = from_layer.name\n        # normalize\n        if normalization[k] > 0:\n            from_layer = mx.symbol.L2Normalization(data=from_layer, \\\n                mode=\"channel\", name=\"{}_norm\".format(from_name))\n            scale = mx.symbol.Variable(name=\"{}_scale\".format(from_name),\n                shape=(1, num_channels.pop(0), 1, 1),\n                init=mx.init.Constant(normalization[k]),\n                attr={'__wd_mult__': '0.1'})\n            from_layer = mx.symbol.broadcast_mul(lhs=scale, rhs=from_layer)\n        if interm_layer > 0:\n            from_layer = mx.symbol.Convolution(data=from_layer, kernel=(3,3), \\\n                stride=(1,1), pad=(1,1), num_filter=interm_layer, \\\n                name=\"{}_inter_conv\".format(from_name))\n            from_layer = mx.symbol.Activation(data=from_layer, act_type=\"relu\", \\\n                name=\"{}_inter_relu\".format(from_name))\n\n        # estimate number of anchors per location\n        # here I follow the original version in caffe\n        # TODO: better way to shape the anchors??\n        size = sizes[k]\n        assert len(size) > 0, \"must provide at least one size\"\n        size_str = \"(\" + \",\".join([str(x) for x in size]) + \")\"\n        ratio = ratios[k]\n        assert len(ratio) > 0, \"must provide at least one ratio\"\n        ratio_str = \"(\" + \",\".join([str(x) for x in ratio]) + \")\"\n        num_anchors = len(size) -1 + len(ratio)\n\n        # create location prediction layer\n        num_loc_pred = num_anchors * 4\n        bias = mx.symbol.Variable(name=\"{}_loc_pred_conv_bias\".format(from_name),\n            init=mx.init.Constant(0.0), attr={'__lr_mult__': '2.0'})\n        loc_pred = mx.symbol.Convolution(data=from_layer, bias=bias, kernel=(3,3), \\\n            stride=(1,1), pad=(1,1), num_filter=num_loc_pred, \\\n            name=\"{}_loc_pred_conv\".format(from_name))\n        loc_pred = mx.symbol.transpose(loc_pred, axes=(0,2,3,1))\n        loc_pred = mx.symbol.Flatten(data=loc_pred)\n        loc_pred_layers.append(loc_pred)\n\n        # create class prediction layer\n        num_cls_pred = num_anchors * num_classes\n        bias = mx.symbol.Variable(name=\"{}_cls_pred_conv_bias\".format(from_name),\n            init=mx.init.Constant(0.0), attr={'__lr_mult__': '2.0'})\n        cls_pred = mx.symbol.Convolution(data=from_layer, bias=bias, kernel=(3,3), \\\n            stride=(1,1), pad=(1,1), num_filter=num_cls_pred, \\\n            name=\"{}_cls_pred_conv\".format(from_name))\n        cls_pred = mx.symbol.transpose(cls_pred, axes=(0,2,3,1))\n        cls_pred = mx.symbol.Flatten(data=cls_pred)\n        cls_pred_layers.append(cls_pred)\n\n        # create anchor generation layer\n        if steps:\n            step = (steps[k], steps[k])\n        else:\n            step = '(-1.0, -1.0)'\n        anchors = mx.symbol.contrib.MultiBoxPrior(from_layer, sizes=size_str, ratios=ratio_str,\n                                                  clip=clip, name=\"{}_anchors\".format(from_name),\n                                                  steps=step)\n        anchors = mx.symbol.Flatten(data=anchors)\n        anchor_layers.append(anchors)\n\n    loc_preds = mx.symbol.Concat(*loc_pred_layers, num_args=len(loc_pred_layers), \\\n        dim=1, name=\"multibox_loc_pred\")\n    cls_preds = mx.symbol.Concat(*cls_pred_layers, num_args=len(cls_pred_layers), \\\n        dim=1)\n    cls_preds = mx.symbol.Reshape(data=cls_preds, shape=(0, -1, num_classes))\n    cls_preds = mx.symbol.transpose(cls_preds, axes=(0, 2, 1), name=\"multibox_cls_pred\")\n    anchor_boxes = mx.symbol.Concat(*anchor_layers, \\\n        num_args=len(anchor_layers), dim=1)\n    anchor_boxes = mx.symbol.Reshape(data=anchor_boxes, shape=(0, -1, 4), name=\"multibox_anchors\")\n    return [loc_preds, cls_preds, anchor_boxes]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\napplies weighting to loss.", "response": "def _apply_weighting(F, loss, weight=None, sample_weight=None):\n    \"\"\"Apply weighting to loss.\n\n    Parameters\n    ----------\n    loss : Symbol\n        The loss to be weighted.\n    weight : float or None\n        Global scalar weight for loss.\n    sample_weight : Symbol or None\n        Per sample weighting. Must be broadcastable to\n        the same shape as loss. For example, if loss has\n        shape (64, 10) and you want to weight each sample\n        in the batch separately, `sample_weight` should have\n        shape (64, 1).\n\n    Returns\n    -------\n    loss : Symbol\n        Weighted loss\n    \"\"\"\n    if sample_weight is not None:\n        loss = F.broadcast_mul(loss, sample_weight)\n\n    if weight is not None:\n        assert isinstance(weight, numeric_types), \"weight must be a number\"\n        loss = loss * weight\n\n    return loss"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _reshape_like(F, x, y):\n    return x.reshape(y.shape) if F is ndarray else F.reshape_like(x, y)", "response": "Reshapes x to the same shape as y."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_tv_grad_executor(img, ctx, tv_weight):\n    if tv_weight <= 0.0:\n        return None\n    nchannel = img.shape[1]\n    simg = mx.sym.Variable(\"img\")\n    skernel = mx.sym.Variable(\"kernel\")\n    channels = mx.sym.SliceChannel(simg, num_outputs=nchannel)\n    out = mx.sym.Concat(*[\n        mx.sym.Convolution(data=channels[i], weight=skernel,\n                           num_filter=1,\n                           kernel=(3, 3), pad=(1,1),\n                           no_bias=True, stride=(1,1))\n        for i in range(nchannel)])\n    kernel = mx.nd.array(np.array([[0, -1, 0],\n                                   [-1, 4, -1],\n                                   [0, -1, 0]])\n                         .reshape((1, 1, 3, 3)),\n                         ctx) / 8.0\n    out = out * tv_weight\n    return out.bind(ctx, args={\"img\": img,\n                               \"kernel\": kernel})", "response": "create TV gradient executor with input binded on img"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef train_nstyle(args, callback=None):\n    # input\n    dev = mx.gpu(args.gpu) if args.gpu >= 0 else mx.cpu()\n    content_np = PreprocessContentImage(args.content_image, args.max_long_edge)\n    style_np = PreprocessStyleImage(args.style_image, shape=content_np.shape)\n    size = content_np.shape[2:]\n\n    # model\n    Executor = namedtuple('Executor', ['executor', 'data', 'data_grad'])\n\n    model_module =  importlib.import_module('model_' + args.model)\n    style, content = model_module.get_symbol()\n    gram, gscale = style_gram_symbol(size, style)\n    model_executor = model_module.get_executor(gram, content, size, dev)\n    model_executor.data[:] = style_np\n    model_executor.executor.forward()\n    style_array = []\n    for i in range(len(model_executor.style)):\n        style_array.append(model_executor.style[i].copyto(mx.cpu()))\n\n    model_executor.data[:] = content_np\n    model_executor.executor.forward()\n    content_array = model_executor.content.copyto(mx.cpu())\n\n    # delete the executor\n    del model_executor\n\n    style_loss, content_loss = get_loss(gram, content)\n    model_executor = model_module.get_executor(\n        style_loss, content_loss, size, dev)\n\n    grad_array = []\n    for i in range(len(style_array)):\n        style_array[i].copyto(model_executor.arg_dict[\"target_gram_%d\" % i])\n        grad_array.append(mx.nd.ones((1,), dev) * (float(args.style_weight) / gscale[i]))\n    grad_array.append(mx.nd.ones((1,), dev) * (float(args.content_weight)))\n\n    print([x.asscalar() for x in grad_array])\n    content_array.copyto(model_executor.arg_dict[\"target_content\"])\n\n    # train\n    # initialize img with random noise\n    img = mx.nd.zeros(content_np.shape, ctx=dev)\n    img[:] = mx.rnd.uniform(-0.1, 0.1, img.shape)\n\n    lr = mx.lr_scheduler.FactorScheduler(step=args.lr_sched_delay,\n            factor=args.lr_sched_factor)\n\n    optimizer = mx.optimizer.NAG(\n        learning_rate = args.lr,\n        wd = 0.0001,\n        momentum=0.95,\n        lr_scheduler = lr)\n    optim_state = optimizer.create_state(0, img)\n\n    logging.info('start training arguments %s', args)\n    old_img = img.copyto(dev)\n    clip_norm = 1 * np.prod(img.shape)\n    tv_grad_executor = get_tv_grad_executor(img, dev, args.tv_weight)\n\n    for e in range(args.max_num_epochs):\n        img.copyto(model_executor.data)\n        model_executor.executor.forward()\n        model_executor.executor.backward(grad_array)\n        gnorm = mx.nd.norm(model_executor.data_grad).asscalar()\n        if gnorm > clip_norm:\n            model_executor.data_grad[:] *= clip_norm / gnorm\n\n        if tv_grad_executor is not None:\n            tv_grad_executor.forward()\n            optimizer.update(0, img,\n                             model_executor.data_grad + tv_grad_executor.outputs[0],\n                             optim_state)\n        else:\n            optimizer.update(0, img, model_executor.data_grad, optim_state)\n        new_img = img\n        eps = (mx.nd.norm(old_img - new_img) / mx.nd.norm(new_img)).asscalar()\n\n        old_img = new_img.copyto(dev)\n        logging.info('epoch %d, relative change %f', e, eps)\n        if eps < args.stop_eps:\n            logging.info('eps < args.stop_eps, training finished')\n            break\n\n        if callback:\n            cbdata = {\n                'eps': eps,\n                'epoch': e+1,\n            }\n        if (e+1) % args.save_epochs == 0:\n            outfn = args.output_dir + 'e_'+str(e+1)+'.jpg'\n            npimg = new_img.asnumpy()\n            SaveImage(npimg, outfn, args.remove_noise)\n            if callback:\n                cbdata['filename'] = outfn\n                cbdata['img'] = npimg\n        if callback:\n            callback(cbdata)\n\n    final_fn = args.output_dir + '/final.jpg'\n    SaveImage(new_img.asnumpy(), final_fn)", "response": "Train a neural style network."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload data and label from dataset", "response": "def _get_batch(self):\n        \"\"\"\n        Load data/label from dataset\n        \"\"\"\n        batch_data = mx.nd.zeros((self.batch_size, 3, self._data_shape[0], self._data_shape[1]))\n        batch_label = []\n        for i in range(self.batch_size):\n            if (self._current + i) >= self._size:\n                if not self.is_train:\n                    continue\n                # use padding from middle in each epoch\n                idx = (self._current + i + self._size // 2) % self._size\n                index = self._index[idx]\n            else:\n                index = self._index[self._current + i]\n            # index = self.debug_index\n            im_path = self._imdb.image_path_from_index(index)\n            with open(im_path, 'rb') as fp:\n                img_content = fp.read()\n            img = mx.img.imdecode(img_content)\n            gt = self._imdb.label_from_index(index).copy() if self.is_train else None\n            data, label = self._data_augmentation(img, gt)\n            batch_data[i] = data\n            if self.is_train:\n                batch_label.append(label)\n        self._data = {'data': batch_data}\n        if self.is_train:\n            self._label = {'label': mx.nd.array(np.array(batch_label))}\n        else:\n            self._label = {'label': None}"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nperforms data augmentations for the current image.", "response": "def _data_augmentation(self, data, label):\n        \"\"\"\n        perform data augmentations: crop, mirror, resize, sub mean, swap channels...\n        \"\"\"\n        if self.is_train and self._rand_samplers:\n            rand_crops = []\n            for rs in self._rand_samplers:\n                rand_crops += rs.sample(label)\n            num_rand_crops = len(rand_crops)\n            # randomly pick up one as input data\n            if num_rand_crops > 0:\n                index = int(np.random.uniform(0, 1) * num_rand_crops)\n                width = data.shape[1]\n                height = data.shape[0]\n                crop = rand_crops[index][0]\n                xmin = int(crop[0] * width)\n                ymin = int(crop[1] * height)\n                xmax = int(crop[2] * width)\n                ymax = int(crop[3] * height)\n                if xmin >= 0 and ymin >= 0 and xmax <= width and ymax <= height:\n                    data = mx.img.fixed_crop(data, xmin, ymin, xmax-xmin, ymax-ymin)\n                else:\n                    # padding mode\n                    new_width = xmax - xmin\n                    new_height = ymax - ymin\n                    offset_x = 0 - xmin\n                    offset_y = 0 - ymin\n                    data_bak = data\n                    data = mx.nd.full((new_height, new_width, 3), 128, dtype='uint8')\n                    data[offset_y:offset_y+height, offset_x:offset_x + width, :] = data_bak\n                label = rand_crops[index][1]\n        if self.is_train:\n            interp_methods = [cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, \\\n                              cv2.INTER_NEAREST, cv2.INTER_LANCZOS4]\n        else:\n            interp_methods = [cv2.INTER_LINEAR]\n        interp_method = interp_methods[int(np.random.uniform(0, 1) * len(interp_methods))]\n        data = mx.img.imresize(data, self._data_shape[1], self._data_shape[0], interp_method)\n        if self.is_train and self._rand_mirror:\n            if np.random.uniform(0, 1) > 0.5:\n                data = mx.nd.flip(data, axis=1)\n                valid_mask = np.where(label[:, 0] > -1)[0]\n                tmp = 1.0 - label[valid_mask, 1]\n                label[valid_mask, 1] = 1.0 - label[valid_mask, 3]\n                label[valid_mask, 3] = tmp\n        data = mx.nd.transpose(data, (2,0,1))\n        data = data.astype('float32')\n        data = data - self._mean_pixels\n        return data, label"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _split_input_slice(batch_size, work_load_list):\n    total_work_load = sum(work_load_list)\n    batch_num_list = [round(work_load * batch_size / total_work_load)\n                      for work_load in work_load_list]\n    batch_num_sum = sum(batch_num_list)\n    if batch_num_sum < batch_size:\n        batch_num_list[-1] += batch_size - batch_num_sum\n    slices = []\n    end = 0\n    for batch_num in batch_num_list:\n        begin = int(min((end, batch_size)))\n        end = int(min((begin + batch_num, batch_size)))\n        if begin >= end:\n            raise ValueError('Too many slices. Some splits are empty.')\n        slices.append(slice(begin, end))\n    return slices", "response": "Get a list of slice objects from the input shape."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _check_arguments(symbol):\n    arg_set = set()\n    arg_names = symbol.list_arguments()\n    for name in arg_names:\n        if name in arg_set:\n            raise ValueError(('Find duplicated argument name \\\"%s\\\", ' +\n                              'please make the weight name non-duplicated(using name arguments), ' +\n                              'arguments are %s') % (name, str(arg_names)))\n        arg_set.add(name)\n\n    aux_set = set()\n    aux_names = symbol.list_auxiliary_states()\n    for name in aux_names:\n        if name in aux_set:\n            raise ValueError(\n                ('Find duplicated auxiliary param name \\\"%s\\\", ' +\n                 'please make the weight name non-duplicated(using name arguments), ' +\n                 'arguments are %s, auxiliary params are %s'\n                ) % (name, str(arg_names), str(aux_names)))\n        aux_set.add(name)", "response": "Checks the argument names of the symbol."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading a list of arrays into a list of arrays specified by slices.", "response": "def _load_general(data, targets):\n    \"\"\"Load a list of arrays into a list of arrays specified by slices.\"\"\"\n    for d_src, d_targets in zip(data, targets):\n        if isinstance(d_targets, nd.NDArray):\n            d_src.copyto(d_targets)\n        else:\n            assert d_targets[-1][0].stop == d_src.shape[0], \\\n                \"Batch size miss match. Expected %d, got %d\"%( \\\n                    d_targets[-1][0].stop, d_src.shape[0])\n            for slice_idx, d_dst in d_targets:\n                d_src[slice_idx].copyto(d_dst)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _bind_exec(sym, ctx, input_shapes, param_names, need_grad=False,\n               base_exec=None, shared_data_arrays=None, input_types=None, logger=logging):\n    \"\"\"bind executor for bucketing, potentially sharing data with an existing executor.\"\"\"\n    arg_shape, _, aux_shape = sym.infer_shape(**input_shapes)\n    assert(arg_shape is not None)\n    if input_types is None:\n        input_types = {k: mx_real_t for k in input_shapes.keys()}\n    arg_types, _, aux_types = sym.infer_type(**input_types)\n    assert(arg_types is not None)\n\n    arg_arrays = []\n    grad_arrays = {} if need_grad is not False else None\n\n    arg_names = sym.list_arguments()\n\n    if need_grad is False:\n        need_grad = set()\n    elif need_grad is True:\n        need_grad = set(arg_names) - set(input_shapes.keys())\n    elif isinstance(need_grad, set):\n        pass\n    else:\n        raise AssertionError(\"need_grad must be boolean or set.\")\n    grad_req = {name:('write' if name in need_grad else 'null') for name in arg_names}\n\n\n    # create or borrow arguments and gradients\n    for i, name in enumerate(arg_names):\n        if not name in param_names:\n            # data or label\n            if shared_data_arrays is not None and \\\n                    name in shared_data_arrays:\n                arg_arr = shared_data_arrays[name]\n\n                if np.prod(arg_arr.shape) >= np.prod(arg_shape[i]):\n                    # good, we can share this memory\n                    assert(arg_types[i] == arg_arr.dtype)\n                    arg_arr = arg_arr.reshape(arg_shape[i])\n                else:\n                    logger.warning(('bucketing: data \"%s\" has a shape %s' % (name, arg_shape[i])) +\n                                   (', which is larger than already allocated ') +\n                                   ('shape %s' % (arg_arr.shape,)) +\n                                   ('. Need to re-allocate. Consider putting ') +\n                                   ('default_bucket_key to be the bucket taking the largest ') +\n                                   ('input for better memory sharing.'))\n                    arg_arr = nd.zeros(arg_shape[i], ctx, dtype=arg_types[i])\n\n                    # replace existing shared array because the new one is bigger\n                    shared_data_arrays[name] = arg_arr\n            else:\n                arg_arr = nd.zeros(arg_shape[i], ctx, dtype=arg_types[i])\n                if shared_data_arrays is not None:\n                    shared_data_arrays[name] = arg_arr\n\n            arg_arrays.append(arg_arr)\n        else:\n            # model parameter\n            if base_exec is None:\n                arg_arr = nd.zeros(arg_shape[i], ctx, dtype=arg_types[i])\n                if name in need_grad:\n                    grad_arr = nd.zeros(arg_shape[i], ctx, dtype=arg_types[i])\n                    grad_arrays[name] = grad_arr\n            else:\n                arg_arr = base_exec.arg_dict[name]\n                assert arg_arr.shape == arg_shape[i]\n                assert arg_arr.dtype == arg_types[i]\n                if name in need_grad:\n                    grad_arrays[name] = base_exec.grad_dict[name]\n            arg_arrays.append(arg_arr)\n\n    # create or borrow aux variables\n    if base_exec is None:\n        aux_arrays = [nd.zeros(s, ctx, dtype=t) for s, t in zip(aux_shape, aux_types)]\n    else:\n        for i, a in enumerate(base_exec.aux_arrays):\n            assert aux_shape[i] == a.shape\n            assert aux_types[i] == a.dtype\n\n        aux_arrays = [a for a in base_exec.aux_arrays]\n\n    executor = sym.bind(ctx=ctx, args=arg_arrays, args_grad=grad_arrays,\n                        aux_states=aux_arrays,\n                        grad_req=grad_req, shared_exec=base_exec)\n    return executor", "response": "bind executor for bucketing potentially sharing data with an existing executor."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_data_batch(self, data_batch):\n        _load_data(data_batch, self.data_arrays)\n        _load_label(data_batch, self.label_arrays)", "response": "Load data and labels into arrays."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef forward(self, is_train=False):\n        for texec in self.train_execs:\n            texec.forward(is_train=is_train)", "response": "Perform a forward pass on each executor."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_metric(self, metric, labels, pre_sliced=False):\n        for current_exec, (texec, islice) in enumerate(zip(self.train_execs, self.slices)):\n            if not pre_sliced:\n                labels_slice = [label[islice] for label in labels]\n            else:\n                labels_slice = labels[current_exec]\n            metric.update(labels_slice, texec.outputs)", "response": "Update evaluation metric with label and current outputs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef install_monitor(self, monitor):\n        if self.sym_gen is not None:\n            raise NotImplementedError(\"Monitoring is not implemented for bucketing\")\n\n        for train_exec in self.execgrp.train_execs:\n            monitor.install(train_exec)", "response": "Install monitor on all executors."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_params(self, arg_params, aux_params):\n\n        for texec in self.execgrp.train_execs:\n            texec.copy_params_from(arg_params, aux_params)", "response": "Set parameter and aux values."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_data_batch(self, data_batch):\n        if self.sym_gen is not None:\n            key = data_batch.bucket_key\n            if key not in self.execgrp_bucket:\n                # create new bucket entry\n                symbol = self.sym_gen(key)\n                execgrp = DataParallelExecutorGroup(symbol, self.arg_names,\n                                                    self.param_names, self.ctx,\n                                                    self.slices, data_batch,\n                                                    shared_group=self.execgrp)\n                self.execgrp_bucket[key] = execgrp\n\n            self.curr_execgrp = self.execgrp_bucket[key]\n        else:\n            self.curr_execgrp = self.execgrp\n\n        self.curr_execgrp.load_data_batch(data_batch)", "response": "Load data and labels into arrays."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates the metric with the current executor.", "response": "def update_metric(self, metric, labels, pre_sliced=False):\n        \"\"\"Update metric with the current executor.\"\"\"\n        self.curr_execgrp.update_metric(metric, labels, pre_sliced)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef clear(self):\n        self.states[:] = 0\n        self.actions[:] = 0\n        self.rewards[:] = 0\n        self.terminate_flags[:] = 0\n        self.top = 0\n        self.size = 0", "response": "Clear all contents of the relay memory."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_header_guard_dmlc(filename):\n    fileinfo = cpplint.FileInfo(filename)\n    file_path_from_root = fileinfo.RepositoryName()\n    inc_list = ['include', 'api', 'wrapper']\n\n    if file_path_from_root.find('src/') != -1 and _HELPER.project_name is not None:\n        idx = file_path_from_root.find('src/')\n        file_path_from_root = _HELPER.project_name +  file_path_from_root[idx + 3:]\n    else:\n        for spath in inc_list:\n            prefix = spath + os.sep\n            if file_path_from_root.startswith(prefix):\n                file_path_from_root = re.sub('^' + prefix, '', file_path_from_root)\n                break\n    return re.sub(r'[-./\\s]', '_', file_path_from_root).upper() + '_'", "response": "Get Header Guard Convention for DMLC Projects."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprocess a file and return a tuple of the n - tuple of n - tuple of n - tuple of n - tuple of n - tuple of n - tuple of n - tuple of n - tuple of n - tuple of n - tuple of n - tuple of n - tuple of n - tuple of n - tuple of n - tuple of n - tuple of n - tuple of n - tuple of n - tuple of n - tuple of n - tuple of n - tuple of n", "response": "def process(fname, allow_type):\n    \"\"\"Process a file.\"\"\"\n    fname = str(fname)\n    # HACK: ignore op.h which is automatically generated\n    if fname.endswith('op.h'):\n      return\n    arr = fname.rsplit('.', 1)\n    if fname.find('#') != -1 or arr[-1] not in allow_type:\n        return\n    if arr[-1] in CXX_SUFFIX:\n        _HELPER.process_cpp(fname, arr[-1])\n    if arr[-1] in PYTHON_SUFFIX:\n        _HELPER.process_python(fname)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _print_summary_map(strm, result_map, ftype):\n        if len(result_map) == 0:\n            return 0\n        npass = len([x for k, x in result_map.iteritems() if len(x) == 0])\n        strm.write('=====%d/%d %s files passed check=====\\n' % (npass, len(result_map), ftype))\n        for fname, emap in result_map.iteritems():\n            if len(emap) == 0:\n                continue\n            strm.write('%s: %d Errors of %d Categories map=%s\\n' % (\n                fname, sum(emap.values()), len(emap), str(emap)))\n        return len(result_map) - npass", "response": "Print summary of certain result map."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process_cpp(self, path, suffix):\n        _cpplint_state.ResetErrorCounts()\n        cpplint.ProcessFile(str(path), _cpplint_state.verbose_level)\n        _cpplint_state.PrintErrorCounts()\n        errors = _cpplint_state.errors_by_category.copy()\n\n        if suffix == 'h':\n            self.cpp_header_map[str(path)] = errors\n        else:\n            self.cpp_src_map[str(path)] = errors", "response": "Process a cpp file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef process_python(self, path):\n        (pylint_stdout, pylint_stderr) = epylint.py_run(\n            ' '.join([str(path)] + self.pylint_opts), return_std=True)\n        emap = {}\n        print(pylint_stderr.read())\n        for line in pylint_stdout:\n            sys.stderr.write(line)\n            key = line.split(':')[-1].split('(')[0].strip()\n            if key not in self.pylint_cats:\n                continue\n            if key not in emap:\n                emap[key] = 1\n            else:\n                emap[key] += 1\n        sys.stderr.write('\\n')\n        self.python_map[str(path)] = emap", "response": "Process a python file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprinting summary of lint.", "response": "def print_summary(self, strm):\n        \"\"\"Print summary of lint.\"\"\"\n        nerr = 0\n        nerr += LintHelper._print_summary_map(strm, self.cpp_header_map, 'cpp-header')\n        nerr += LintHelper._print_summary_map(strm, self.cpp_src_map, 'cpp-soruce')\n        nerr += LintHelper._print_summary_map(strm, self.python_map, 'python')\n        if nerr == 0:\n            strm.write('All passed!\\n')\n        else:\n            strm.write('%d files failed lint\\n' % nerr)\n        return nerr"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the server controller function.", "response": "def _controller(self):\n        \"\"\"Return the server controller.\"\"\"\n        def server_controller(cmd_id, cmd_body, _):\n            \"\"\"Server controler.\"\"\"\n            if not self.init_logginig:\n                # the reason put the codes here is because we cannot get\n                # kvstore.rank earlier\n                head = '%(asctime)-15s Server[' + str(\n                    self.kvstore.rank) + '] %(message)s'\n                logging.basicConfig(level=logging.DEBUG, format=head)\n                self.init_logginig = True\n\n            if cmd_id == 0:\n                try:\n                    optimizer = pickle.loads(cmd_body)\n                except:\n                    raise\n                self.kvstore.set_optimizer(optimizer)\n            else:\n                print(\"server %d, unknown command (%d, %s)\" % (\n                    self.kvstore.rank, cmd_id, cmd_body))\n        return server_controller"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun the server, whose behavior is like. >>> while receive(x): ... if is_command x: controller(x) ... else if is_key_value x: updater(x)", "response": "def run(self):\n        \"\"\"Run the server, whose behavior is like.\n\n\n        >>> while receive(x):\n        ...     if is_command x: controller(x)\n        ...     else if is_key_value x: updater(x)\n        \"\"\"\n        _ctrl_proto = ctypes.CFUNCTYPE(None, ctypes.c_int, ctypes.c_char_p, ctypes.c_void_p)\n        check_call(_LIB.MXKVStoreRunServer(self.handle, _ctrl_proto(self._controller()), None))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates function for ndarray op by handle and function name.", "response": "def _generate_ndarray_function_code(handle, name, func_name, signature_only=False):\n    \"\"\"Generate function for ndarray op by handle and function name.\"\"\"\n    real_name = ctypes.c_char_p()\n    desc = ctypes.c_char_p()\n    num_args = mx_uint()\n    arg_names = ctypes.POINTER(ctypes.c_char_p)()\n    arg_types = ctypes.POINTER(ctypes.c_char_p)()\n    arg_descs = ctypes.POINTER(ctypes.c_char_p)()\n    key_var_num_args = ctypes.c_char_p()\n    ret_type = ctypes.c_char_p()\n\n    check_call(_LIB.MXSymbolGetAtomicSymbolInfo(\n        handle, ctypes.byref(real_name), ctypes.byref(desc),\n        ctypes.byref(num_args),\n        ctypes.byref(arg_names),\n        ctypes.byref(arg_types),\n        ctypes.byref(arg_descs),\n        ctypes.byref(key_var_num_args),\n        ctypes.byref(ret_type)))\n    narg = int(num_args.value)\n    arg_names = [py_str(arg_names[i]) for i in range(narg)]\n    arg_types = [py_str(arg_types[i]) for i in range(narg)]\n    key_var_num_args = py_str(key_var_num_args.value)\n    ret_type = py_str(ret_type.value) if ret_type.value is not None else ''\n    doc_str = _build_doc(name,\n                         py_str(desc.value),\n                         arg_names,\n                         arg_types,\n                         [py_str(arg_descs[i]) for i in range(narg)],\n                         key_var_num_args,\n                         ret_type)\n\n    dtype_name = None\n    arr_name = None\n    ndsignature = []\n    signature = []\n    ndarg_names = []\n    kwarg_names = []\n    for i in range(narg):\n        name, atype = arg_names[i], arg_types[i]\n        if name == 'dtype':\n            dtype_name = name\n            signature.append('%s=_Null'%name)\n        elif atype.startswith('NDArray') or atype.startswith('Symbol'):\n            assert not arr_name, \\\n                \"Op can only have one argument with variable \" \\\n                \"size and it must be the last argument.\"\n            if atype.endswith('[]'):\n                ndsignature.append('*%s'%name)\n                arr_name = name\n            else:\n                ndsignature.append('%s=None'%name)\n                ndarg_names.append(name)\n        else:\n            signature.append('%s=_Null'%name)\n            kwarg_names.append(name)\n    signature.append('out=None')\n    signature.append('name=None')\n    signature.append('**kwargs')\n    signature = ndsignature + signature\n\n    code = []\n    if arr_name:\n        code.append(\"\"\"\ndef %s(*%s, **kwargs):\"\"\"%(func_name, arr_name))\n        if not signature_only:\n            code.append(\"\"\"\n    ndargs = []\n    for i in {}:\n        assert isinstance(i, NDArrayBase), \\\\\n            \"Positional arguments must have NDArray type, \" \\\\\n            \"but got %s\"%str(i)\n        ndargs.append(i)\"\"\".format(arr_name))\n            if dtype_name is not None:\n                code.append(\"\"\"\n    if '%s' in kwargs:\n        kwargs['%s'] = _np.dtype(kwargs['%s']).name\"\"\"%(\n            dtype_name, dtype_name, dtype_name))\n            code.append(\"\"\"\n    _ = kwargs.pop('name', None)\n    out = kwargs.pop('out', None)\n    keys = list(kwargs.keys())\n    vals = list(kwargs.values())\"\"\")\n    else:\n        code.append(\"\"\"\ndef %s(%s):\"\"\"%(func_name, ', '.join(signature)))\n        if not signature_only:\n            code.append(\"\"\"\n    ndargs = []\n    keys = list(kwargs.keys())\n    vals = list(kwargs.values())\"\"\")\n            # NDArray args\n            for name in ndarg_names: # pylint: disable=redefined-argument-from-local\n                code.append(\"\"\"\n    if {name} is not None:\n        assert isinstance({name}, NDArrayBase), \\\\\n            \"Argument {name} must have NDArray type, but got %s\"%str({name})\n        ndargs.append({name})\"\"\".format(name=name))\n            # kwargs\n            for name in kwarg_names: # pylint: disable=redefined-argument-from-local\n                code.append(\"\"\"\n    if %s is not _Null:\n        keys.append('%s')\n        vals.append(%s)\"\"\"%(name, name, name))\n            # dtype\n            if dtype_name is not None:\n                code.append(\"\"\"\n    if %s is not _Null:\n        keys.append('%s')\n        vals.append(_np.dtype(%s).name)\"\"\"%(dtype_name, dtype_name, dtype_name))\n\n    if not signature_only:\n        code.append(\"\"\"\n    return _imperative_invoke(%d, ndargs, keys, vals, out)\"\"\"%(\n        handle.value))\n    else:\n        code.append(\"\"\"\n    return (0,)\"\"\")\n\n    doc_str_lines = _os.linesep+''.join(['    '+s if s.strip() else s\n                                         for s in 'r\"\"\"{doc_str}\"\"\"'.format(doc_str=doc_str)\n                                         .splitlines(True)])\n    code.insert(1, doc_str_lines)\n    return ''.join(code), doc_str"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _make_ndarray_function(handle, name, func_name):\n    code, doc_str = _generate_ndarray_function_code(handle, name, func_name)\n\n    local = {}\n    exec(code, None, local)  # pylint: disable=exec-used\n    ndarray_function = local[func_name]\n    ndarray_function.__name__ = func_name\n    ndarray_function.__doc__ = doc_str\n    ndarray_function.__module__ = 'mxnet.ndarray'\n    return ndarray_function", "response": "Create a NDArray function from the FunctionHandle."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncount the number of tokens in the specified string.", "response": "def count_tokens_from_str(source_str, token_delim=' ', seq_delim='\\n',\n                          to_lower=False, counter_to_update=None):\n    \"\"\"Counts tokens in the specified string.\n\n    For token_delim=\\'<td>\\' and seq_delim=\\'<sd>\\', a specified string of two sequences of\n    tokens may look like::\n\n    <td>token1<td>token2<td>token3<td><sd><td>token4<td>token5<td><sd>\n\n    <td> and <sd> are regular expressions. Make use of \\\\\\\\ to allow special characters as\n    delimiters. The list of\n    special characters can be found at https://docs.python.org/3/library/re.html.\n\n    Parameters\n    ----------\n    source_str : str\n        A source string of tokens.\n    token_delim : str, default ' '\n        A token delimiter.\n    seq_delim : str, default '\\\\\\\\n'\n        A sequence delimiter.\n    to_lower : bool, default False\n        Whether to convert the source source_str to the lower case.\n    counter_to_update : collections.Counter or None, default None\n        The collections.Counter instance to be updated with the token counts of `source_str`. If\n        None, return a new collections.Counter instance counting tokens from `source_str`.\n\n\n    Returns\n    -------\n    collections.Counter\n        The `counter_to_update` collections.Counter instance after being updated with the token\n        counts of `source_str`. If `counter_to_update` is None, return a new collections.Counter\n        instance counting tokens from `source_str`.\n\n\n    Examples\n    --------\n    >>> source_str = ' Life is great ! \\\\n life is good . \\\\n'\n    >>> count_tokens_from_str(token_line, ' ', '\\\\n', True)\n    Counter({'!': 1, '.': 1, 'good': 1, 'great': 1, 'is': 2, 'life': 2})\n\n\n    >>> source_str = '*Life*is*great*!*\\\\n*life*is*good*.*\\\\n'\n    >>> count_tokens_from_str(token_line, '\\\\*', '\\\\n', True)\n    Counter({'is': 2, 'life': 2, '!': 1, 'great': 1, 'good': 1, '.': 1})\n    \"\"\"\n\n    source_str = filter(None,\n                        re.split(token_delim + '|' + seq_delim, source_str))\n    if to_lower:\n        source_str = [t.lower() for t in source_str]\n\n    if counter_to_update is None:\n        return collections.Counter(source_str)\n    else:\n        counter_to_update.update(source_str)\n        return counter_to_update"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a new array filled with zeros.", "response": "def zeros(shape, ctx=None, dtype=None, stype=None, **kwargs):\n    \"\"\"Return a new array of given shape and type, filled with zeros.\n\n    Parameters\n    ----------\n    shape : int or tuple of int\n        The shape of the empty array\n    ctx : Context, optional\n        An optional device context (default is the current default context)\n    dtype : str or numpy.dtype, optional\n        An optional value type (default is `float32`)\n    stype: string, optional\n        The storage type of the empty array, such as 'row_sparse', 'csr', etc.\n\n    Returns\n    -------\n    NDArray, CSRNDArray or RowSparseNDArray\n        A created array\n    Examples\n    --------\n    >>> mx.nd.zeros((1,2), mx.cpu(), stype='csr')\n    <CSRNDArray 1x2 @cpu(0)>\n    >>> mx.nd.zeros((1,2), mx.cpu(), 'float16', stype='row_sparse').asnumpy()\n    array([[ 0.,  0.]], dtype=float16)\n    \"\"\"\n\n    if stype is None or stype == 'default':\n        return _zeros_ndarray(shape, ctx, dtype, **kwargs)\n    else:\n        return _zeros_sparse_ndarray(stype, shape, ctx, dtype, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning an empty array with given shape and type.", "response": "def empty(shape, ctx=None, dtype=None, stype=None):\n    \"\"\"Returns a new array of given shape and type, without initializing entries.\n\n    Parameters\n    ----------\n    shape : int or tuple of int\n        The shape of the empty array.\n    ctx : Context, optional\n        An optional device context (default is the current default context).\n    dtype : str or numpy.dtype, optional\n        An optional value type (default is `float32`).\n    stype : str, optional\n        An optional storage type (default is `default`).\n\n    Returns\n    -------\n    NDArray, CSRNDArray or RowSparseNDArray\n        A created array.\n\n    Examples\n    --------\n    >>> mx.nd.empty(1)\n    <NDArray 1 @cpu(0)>\n    >>> mx.nd.empty((1,2), mx.gpu(0))\n    <NDArray 1x2 @gpu(0)>\n    >>> mx.nd.empty((1,2), mx.gpu(0), 'float16')\n    <NDArray 1x2 @gpu(0)>\n    >>> mx.nd.empty((1,2), stype='csr')\n    <CSRNDArray 1x2 @cpu(0)>\n    \"\"\"\n    if stype is None or stype == 'default':\n        return _empty_ndarray(shape, ctx, dtype)\n    else:\n        return _empty_sparse_ndarray(stype, shape, ctx, dtype)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating an array from any object exposing the array interface.", "response": "def array(source_array, ctx=None, dtype=None):\n    \"\"\"Creates an array from any object exposing the array interface.\n\n    Parameters\n    ----------\n    source_array : array_like\n        An object exposing the array interface, an object whose `__array__`\n        method returns an array, or any (nested) sequence.\n    ctx : Context, optional\n        Device context (default is the current default context).\n    dtype : str or numpy.dtype, optional\n        The data type of the output array. The default dtype is ``source_array.dtype``\n        if `source_array` is an `NDArray`, `float32` otherwise.\n\n    Returns\n    -------\n    NDArray, RowSparseNDArray or CSRNDArray\n        An array with the same contents as the `source_array`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> mx.nd.array([1, 2, 3])\n    <NDArray 3 @cpu(0)>\n    >>> mx.nd.array([[1, 2], [3, 4]])\n    <NDArray 2x2 @cpu(0)>\n    >>> mx.nd.array(np.zeros((3, 2)))\n    <NDArray 3x2 @cpu(0)>\n    >>> mx.nd.array(np.zeros((3, 2)), mx.gpu(0))\n    <NDArray 3x2 @gpu(0)>\n    >>> mx.nd.array(mx.nd.zeros((3, 2), stype='row_sparse'))\n    <RowSparseNDArray 3x2 @cpu(0)>\n    \"\"\"\n    if spsp is not None and isinstance(source_array, spsp.csr.csr_matrix):\n        return _sparse_array(source_array, ctx=ctx, dtype=dtype)\n    elif isinstance(source_array, NDArray) and source_array.stype != 'default':\n        return _sparse_array(source_array, ctx=ctx, dtype=dtype)\n    else:\n        return _array(source_array, ctx=ctx, dtype=dtype)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading an array from file.", "response": "def load(fname):\n    \"\"\"Loads an array from file.\n\n    See more details in ``save``.\n\n    Parameters\n    ----------\n    fname : str\n        The filename.\n\n    Returns\n    -------\n    list of NDArray, RowSparseNDArray or CSRNDArray, or \\\n    dict of str to NDArray, RowSparseNDArray or CSRNDArray\n        Loaded data.\n    \"\"\"\n    if not isinstance(fname, string_types):\n        raise TypeError('fname required to be a string')\n    out_size = mx_uint()\n    out_name_size = mx_uint()\n    handles = ctypes.POINTER(NDArrayHandle)()\n    names = ctypes.POINTER(ctypes.c_char_p)()\n    check_call(_LIB.MXNDArrayLoad(c_str(fname),\n                                  ctypes.byref(out_size),\n                                  ctypes.byref(handles),\n                                  ctypes.byref(out_name_size),\n                                  ctypes.byref(names)))\n    if out_name_size.value == 0:\n        return [_ndarray_cls(NDArrayHandle(handles[i])) for i in range(out_size.value)]\n    else:\n        assert out_name_size.value == out_size.value\n        return dict(\n            (py_str(names[i]), _ndarray_cls(NDArrayHandle(handles[i])))\n            for i in range(out_size.value))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads an array dictionary or list from a buffer.", "response": "def load_frombuffer(buf):\n    \"\"\"Loads an array dictionary or list from a buffer\n\n    See more details in ``save``.\n\n    Parameters\n    ----------\n    buf : str\n        Buffer containing contents of a file as a string or bytes.\n\n    Returns\n    -------\n    list of NDArray, RowSparseNDArray or CSRNDArray, or \\\n    dict of str to NDArray, RowSparseNDArray or CSRNDArray\n        Loaded data.\n    \"\"\"\n    if not isinstance(buf, string_types + tuple([bytes])):\n        raise TypeError('buf required to be a string or bytes')\n    out_size = mx_uint()\n    out_name_size = mx_uint()\n    handles = ctypes.POINTER(NDArrayHandle)()\n    names = ctypes.POINTER(ctypes.c_char_p)()\n    check_call(_LIB.MXNDArrayLoadFromBuffer(buf,\n                                            mx_uint(len(buf)),\n                                            ctypes.byref(out_size),\n                                            ctypes.byref(handles),\n                                            ctypes.byref(out_name_size),\n                                            ctypes.byref(names)))\n    if out_name_size.value == 0:\n        return [_ndarray_cls(NDArrayHandle(handles[i])) for i in range(out_size.value)]\n    else:\n        assert out_name_size.value == out_size.value\n        return dict(\n            (py_str(names[i]), _ndarray_cls(NDArrayHandle(handles[i])))\n            for i in range(out_size.value))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef save(fname, data):\n    if isinstance(data, NDArray):\n        data = [data]\n        handles = c_array(NDArrayHandle, [])\n    if isinstance(data, dict):\n        str_keys = data.keys()\n        nd_vals = data.values()\n        if any(not isinstance(k, string_types) for k in str_keys) or \\\n           any(not isinstance(v, NDArray) for v in nd_vals):\n            raise TypeError('save only accept dict str->NDArray or list of NDArray')\n        keys = c_str_array(str_keys)\n        handles = c_handle_array(nd_vals)\n    elif isinstance(data, list):\n        if any(not isinstance(v, NDArray) for v in data):\n            raise TypeError('save only accept dict str->NDArray or list of NDArray')\n        keys = None\n        handles = c_handle_array(data)\n    else:\n        raise ValueError(\"data needs to either be a NDArray, dict of str, NDArray pairs \"\n                         \"or a list of NDarrays.\")\n    check_call(_LIB.MXNDArraySave(c_str(fname),\n                                  mx_uint(len(handles)),\n                                  handles,\n                                  keys))", "response": "Saves a list of arrays or a dict of str - > array to file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the common prefix for all names", "response": "def _common_prefix(names):\n    \"\"\"Get the common prefix for all names\"\"\"\n    if not names:\n        return ''\n    prefix = names[0]\n    for name in names:\n        i = 0\n        while i < len(prefix) and i < len(name) and prefix[i] == name[i]:\n            i += 1\n        prefix = prefix[:i]\n    return prefix"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _infer_param_types(in_params, out_params, arg_params, aux_params, default_dtype=mx_real_t):\n    arg_types = None\n    aux_types = None\n\n    # Get Input symbol details. This will be used to infer types of\n    # other parameters.\n    input_sym_names = [in_param.name for in_param in in_params]\n\n    # Try to infer input types. If not successful, we will set default dtype.\n    # If successful, we will try to infer other params in the graph.\n    input_sym_arg_types = []\n    can_infer_input_type = True\n    for in_param in in_params:\n        input_sym_arg_type = in_param.infer_type()[0]\n        if not input_sym_arg_type or len(input_sym_arg_type) < 1:\n            can_infer_input_type = False\n            break\n        else:\n            input_sym_arg_types.append(in_param.infer_type()[0][0])\n\n    # Try to infer types of other parameters.\n    if can_infer_input_type:\n        params = {k:v for k, v in zip(input_sym_names, input_sym_arg_types)}\n        arg_types, _, aux_types = out_params.infer_type(**params)\n\n    if arg_types is None or len(arg_types) != len(arg_params):\n        arg_types = []\n        for _ in arg_params:\n            arg_types.append(default_dtype)\n\n    if aux_types is None or len(aux_types) != len(aux_params):\n        aux_types = []\n        for _ in aux_params:\n            aux_types.append(default_dtype)\n\n    return (arg_types, aux_types)", "response": "Utility function that helps in inferring DType of args and auxs params from given input param."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates prefix and params for new Block.", "response": "def create(prefix, params, hint):\n        \"\"\"Creates prefix and params for new `Block`.\"\"\"\n        current = getattr(_BlockScope._current, \"value\", None)\n        if current is None:\n            if prefix is None:\n                if not hasattr(_name.NameManager._current, \"value\"):\n                    _name.NameManager._current.value = _name.NameManager()\n                prefix = _name.NameManager._current.value.get(None, hint) + '_'\n            if params is None:\n                params = ParameterDict(prefix)\n            else:\n                params = ParameterDict(params.prefix, params)\n            return prefix, params\n\n        if prefix is None:\n            count = current._counter.get(hint, 0)\n            prefix = '%s%d_'%(hint, count)\n            current._counter[hint] = count + 1\n        if params is None:\n            parent = current._block.params\n            params = ParameterDict(parent.prefix+prefix, parent._shared)\n        else:\n            params = ParameterDict(params.prefix, params)\n        return current._block.prefix+prefix, params"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a dictionary containing this block and all of its its s Parameters and all of its children s Parameters.", "response": "def collect_params(self, select=None):\n        \"\"\"Returns a :py:class:`ParameterDict` containing this :py:class:`Block` and all of its\n        children's Parameters(default), also can returns the select :py:class:`ParameterDict`\n        which match some given regular expressions.\n\n        For example, collect the specified parameters in ['conv1_weight', 'conv1_bias', 'fc_weight',\n        'fc_bias']::\n\n            model.collect_params('conv1_weight|conv1_bias|fc_weight|fc_bias')\n\n        or collect all parameters whose names end with 'weight' or 'bias', this can be done\n        using regular expressions::\n\n            model.collect_params('.*weight|.*bias')\n\n        Parameters\n        ----------\n        select : str\n            regular expressions\n\n        Returns\n        -------\n        The selected :py:class:`ParameterDict`\n        \"\"\"\n        # We need to check here because blocks inside containers are not supported.\n        self._check_container_with_block()\n        ret = ParameterDict(self._params.prefix)\n        if not select:\n            ret.update(self.params)\n        else:\n            pattern = re.compile(select)\n            ret.update({name:value for name, value in self.params.items() if pattern.match(name)})\n        for cld in self._children.values():\n            ret.update(cld.collect_params(select=select))\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsaves parameters to file.", "response": "def save_params(self, filename):\n        \"\"\"[Deprecated] Please use save_parameters. Note that if you want load\n        from SymbolBlock later, please use export instead.\n\n        Save parameters to file.\n\n        filename : str\n            Path to file.\n        \"\"\"\n        warnings.warn(\"save_params is deprecated. Please use save_parameters. \"\n                      \"Note that if you want load from SymbolBlock later, please \"\n                      \"use export instead. For details, see \"\n                      \"https://mxnet.incubator.apache.org/tutorials/gluon/save_lo\"\n                      \"ad_params.html\")\n        try:\n            self.collect_params().save(filename, strip_prefix=self.prefix)\n        except ValueError as e:\n            raise ValueError('%s\\nsave_params is deprecated. Using ' \\\n                              'save_parameters may resolve this error.'%e.message)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload parameters from file.", "response": "def load_parameters(self, filename, ctx=None, allow_missing=False,\n                        ignore_extra=False):\n        \"\"\"Load parameters from file previously saved by `save_parameters`.\n\n        Parameters\n        ----------\n        filename : str\n            Path to parameter file.\n        ctx : Context or list of Context, default cpu()\n            Context(s) to initialize loaded parameters on.\n        allow_missing : bool, default False\n            Whether to silently skip loading parameters not represents in the file.\n        ignore_extra : bool, default False\n            Whether to silently ignore parameters from the file that are not\n            present in this Block.\n\n        References\n        ----------\n        `Saving and Loading Gluon Models \\\n        <https://mxnet.incubator.apache.org/tutorials/gluon/save_load_params.html>`_\n        \"\"\"\n        loaded = ndarray.load(filename)\n        params = self._collect_params_with_prefix()\n        if not loaded and not params:\n            return\n\n        if not any('.' in i for i in loaded.keys()):\n            # legacy loading\n            del loaded\n            self.collect_params().load(\n                filename, ctx, allow_missing, ignore_extra, self.prefix)\n            return\n\n        if not allow_missing:\n            for name in params.keys():\n                assert name in loaded, \\\n                    \"Parameter '%s' is missing in file '%s', which contains parameters: %s. \" \\\n                    \"Set allow_missing=True to ignore missing parameters.\"%(\n                        name, filename, _brief_print_list(loaded.keys()))\n        for name in loaded:\n            if not ignore_extra and name not in params:\n                raise ValueError(\n                    \"Parameter '%s' loaded from file '%s' is not present in ParameterDict, \" \\\n                    \"which contains parameters %s. Set ignore_extra=True to ignore. \"%(\n                        name, filename, _brief_print_list(self._params.keys())))\n            if name in params:\n                params[name]._load_init(loaded[name], ctx)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading parameters from file.", "response": "def load_params(self, filename, ctx=None, allow_missing=False,\n                    ignore_extra=False):\n        \"\"\"[Deprecated] Please use load_parameters.\n\n        Load parameters from file.\n\n        filename : str\n            Path to parameter file.\n        ctx : Context or list of Context, default cpu()\n            Context(s) to initialize loaded parameters on.\n        allow_missing : bool, default False\n            Whether to silently skip loading parameters not represents in the file.\n        ignore_extra : bool, default False\n            Whether to silently ignore parameters from the file that are not\n            present in this Block.\n        \"\"\"\n        warnings.warn(\"load_params is deprecated. Please use load_parameters.\")\n        self.load_parameters(filename, ctx, allow_missing, ignore_extra)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nregister a block as a child of self.", "response": "def register_child(self, block, name=None):\n        \"\"\"Registers block as a child of self. :py:class:`Block` s assigned to self as\n        attributes will be registered automatically.\"\"\"\n        if name is None:\n            name = str(len(self._children))\n        self._children[name] = block"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nregistering a forward pre - hook on the block.", "response": "def register_forward_pre_hook(self, hook):\n        r\"\"\"Registers a forward pre-hook on the block.\n\n        The hook function is called immediately before :func:`forward`.\n        It should not modify the input or output.\n\n        Parameters\n        ----------\n        hook : callable\n            The forward hook function of form `hook(block, input) -> None`.\n\n        Returns\n        -------\n        :class:`mxnet.gluon.utils.HookHandle`\n        \"\"\"\n        handle = HookHandle()\n        handle.attach(self._forward_pre_hooks, hook)\n        return handle"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nregisters a forward hook on the block.", "response": "def register_forward_hook(self, hook):\n        r\"\"\"Registers a forward hook on the block.\n\n        The hook function is called immediately after :func:`forward`.\n        It should not modify the input or output.\n\n        Parameters\n        ----------\n        hook : callable\n            The forward hook function of form `hook(block, input, output) -> None`.\n\n        Returns\n        -------\n        :class:`mxnet.gluon.utils.HookHandle`\n        \"\"\"\n        handle = HookHandle()\n        handle.attach(self._forward_hooks, hook)\n        return handle"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninitialize all parameters of this block and its children.", "response": "def initialize(self, init=initializer.Uniform(), ctx=None, verbose=False,\n                   force_reinit=False):\n        \"\"\"Initializes :py:class:`Parameter` s of this :py:class:`Block` and its children.\n        Equivalent to ``block.collect_params().initialize(...)``\n\n        Parameters\n        ----------\n        init : Initializer\n            Global default Initializer to be used when :py:meth:`Parameter.init` is ``None``.\n            Otherwise, :py:meth:`Parameter.init` takes precedence.\n        ctx : Context or list of Context\n            Keeps a copy of Parameters on one or many context(s).\n        verbose : bool, default False\n            Whether to verbosely print out details on initialization.\n        force_reinit : bool, default False\n            Whether to force re-initialization if parameter is already initialized.\n        \"\"\"\n        self.collect_params().initialize(init, ctx, verbose, force_reinit)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef hybridize(self, active=True, **kwargs):\n        for cld in self._children.values():\n            cld.hybridize(active, **kwargs)", "response": "Activates or deactivates a : py : class : HybridBlock s recursively. Has no effect on\n            non - hybrid children."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cast(self, dtype):\n        for child in self._children.values():\n            child.cast(dtype)\n        for _, param in self.params.items():\n            param.cast(dtype)", "response": "Cast this Block to use another data type."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef summary(self, *inputs):\n        summary = OrderedDict()\n        seen = set()\n        hooks = []\n\n        def _get_shape_str(args):\n            def flatten(args):\n                if not isinstance(args, (list, tuple)):\n                    return [args], int(0)\n                flat = []\n                fmts = []\n                for i in args:\n                    arg, fmt = flatten(i)\n                    flat.extend(arg)\n                    fmts.append(fmt)\n                return flat, fmts\n\n            def regroup(args, fmt):\n                if isinstance(fmt, int):\n                    if fmt == 0:\n                        return args[0], args[1:]\n                    return args[:fmt], args[fmt:]\n                ret = []\n                for i in fmt:\n                    res, args = regroup(args, i)\n                    ret.append(res)\n                return ret, args\n\n            flat_args, fmts = flatten(args)\n            flat_arg_shapes = [x.shape if isinstance(x, ndarray.NDArray) else x\n                               for x in flat_args]\n            shapes = regroup(flat_arg_shapes, fmts)[0]\n            if isinstance(shapes, list):\n                shape_str = str(shapes)[1:-1]\n            else:\n                shape_str = str(shapes)\n            return shape_str.replace('L', '')\n\n        def _register_summary_hook(block):\n            assert not isinstance(block, HybridBlock) or not block._active, \\\n                    '\"{}\" must not be hybridized to print summary.'.format(block.name)\n            def _summary_hook(block, _, outputs):\n                class_name = block.__class__.__name__\n                block_idx = len(summary) - 1\n\n                m_key = '%s-%i' % (class_name, block_idx+1)\n                summary[m_key] = OrderedDict()\n                summary[m_key]['output_shape'] = _get_shape_str(outputs)\n\n                params = 0\n                summary[m_key]['trainable'] = 0\n                summary[m_key]['shared'] = 0\n                for p in block.params.values():\n                    params += p.data().size\n                    summary[m_key]['trainable'] += 0 if p.grad_req == 'null' else p.data().size\n                    if p in seen:\n                        summary[m_key]['shared'] += p.data().size\n                    else:\n                        seen.add(p)\n                summary[m_key]['n_params'] = params\n\n            from .nn.basic_layers import Sequential, HybridSequential\n            if not isinstance(block, (Sequential, HybridSequential)):\n                hooks.append(block.register_forward_hook(_summary_hook))\n\n        summary['Input'] = OrderedDict()\n        summary['Input']['output_shape'] = _get_shape_str(inputs)\n        summary['Input']['n_params'] = 0\n        summary['Input']['trainable'] = 0\n        summary['Input']['shared'] = 0\n\n        try:\n            self.apply(_register_summary_hook)\n            self(*inputs)\n\n            line_format = '{:>20}  {:>42} {:>15}'\n            print('-'*80)\n            print(line_format.format('Layer (type)', 'Output Shape', 'Param #'))\n            print('='*80)\n            total_params = 0\n            trainable_params = 0\n            shared_params = 0\n            for layer in summary:\n                print(line_format.format(layer,\n                                         str(summary[layer]['output_shape']),\n                                         summary[layer]['n_params']))\n                total_params += summary[layer]['n_params']\n                trainable_params += summary[layer]['trainable']\n                shared_params += summary[layer]['shared']\n            print('='*80)\n            print('Parameters in forward computation graph, duplicate included')\n            print('   Total params: ' + str(total_params))\n            print('   Trainable params: ' + str(trainable_params))\n            print('   Non-trainable params: ' + str(total_params - trainable_params))\n            print('Shared params in forward computation graph: ' + str(shared_params))\n            print('Unique parameters in model: ' + str(total_params - shared_params))\n            print('-'*80)\n        finally:\n            for h in hooks:\n                h.detach()", "response": "Print the summary of the model s output and parameters."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nexporting the HybridBlock to json format that can be loaded by C ++ interface or mxnet. mod. Module.", "response": "def export(self, path, epoch=0):\n        \"\"\"Export HybridBlock to json format that can be loaded by\n        `SymbolBlock.imports`, `mxnet.mod.Module` or the C++ interface.\n\n        .. note:: When there are only one input, it will have name `data`. When there\n                  Are more than one inputs, they will be named as `data0`, `data1`, etc.\n\n        Parameters\n        ----------\n        path : str\n            Path to save model. Two files `path-symbol.json` and `path-xxxx.params`\n            will be created, where xxxx is the 4 digits epoch number.\n        epoch : int\n            Epoch number of saved model.\n        \"\"\"\n        if not self._cached_graph:\n            raise RuntimeError(\n                \"Please first call block.hybridize() and then run forward with \"\n                \"this block at least once before calling export.\")\n        sym = self._cached_graph[1]\n        sym.save('%s-symbol.json'%path)\n\n        arg_names = set(sym.list_arguments())\n        aux_names = set(sym.list_auxiliary_states())\n        arg_dict = {}\n        for name, param in self.collect_params().items():\n            if name in arg_names:\n                arg_dict['arg:%s'%name] = param._reduce()\n            else:\n                assert name in aux_names\n                arg_dict['aux:%s'%name] = param._reduce()\n        ndarray.save('%s-%04d.params'%(path, epoch), arg_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef forward(self, x, *args):\n        if isinstance(x, NDArray):\n            with x.context as ctx:\n                if self._active:\n                    return self._call_cached_op(x, *args)\n\n                try:\n                    params = {i: j.data(ctx) for i, j in self._reg_params.items()}\n                except DeferredInitializationError:\n                    self._deferred_infer_shape(x, *args)\n                    for _, i in self.params.items():\n                        i._finish_deferred_init()\n                    params = {i: j.data(ctx) for i, j in self._reg_params.items()}\n\n                return self.hybrid_forward(ndarray, x, *args, **params)\n\n        assert isinstance(x, Symbol), \\\n            \"HybridBlock requires the first argument to forward be either \" \\\n            \"Symbol or NDArray, but got %s\"%type(x)\n        params = {i: j.var() for i, j in self._reg_params.items()}\n        with self.name_scope():\n            return self.hybrid_forward(symbol, x, *args, **params)", "response": "Defines the forward computation. Arguments can be either NDArray or Symbol."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nimporting a model previously saved by HybridBlock. export or HybridBlock. save_checkpoint as a SymbolBlock.", "response": "def imports(symbol_file, input_names, param_file=None, ctx=None):\n        \"\"\"Import model previously saved by `HybridBlock.export` or\n        `Module.save_checkpoint` as a SymbolBlock for use in Gluon.\n\n        Parameters\n        ----------\n        symbol_file : str\n            Path to symbol file.\n        input_names : list of str\n            List of input variable names\n        param_file : str, optional\n            Path to parameter file.\n        ctx : Context, default None\n            The context to initialize SymbolBlock on.\n\n        Returns\n        -------\n        SymbolBlock\n            SymbolBlock loaded from symbol and parameter files.\n\n        Examples\n        --------\n        >>> net1 = gluon.model_zoo.vision.resnet18_v1(\n        ...     prefix='resnet', pretrained=True)\n        >>> net1.hybridize()\n        >>> x = mx.nd.random.normal(shape=(1, 3, 32, 32))\n        >>> out1 = net1(x)\n        >>> net1.export('net1', epoch=1)\n        >>>\n        >>> net2 = gluon.SymbolBlock.imports(\n        ...     'net1-symbol.json', ['data'], 'net1-0001.params')\n        >>> out2 = net2(x)\n        \"\"\"\n        sym = symbol.load(symbol_file)\n        if isinstance(input_names, str):\n            input_names = [input_names]\n        inputs = [symbol.var(i) for i in input_names]\n        ret = SymbolBlock(sym, inputs)\n        if param_file is not None:\n            ret.collect_params().load(param_file, ctx=ctx)\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the expectation of the gradients per epoch for each parameter w. r. t number of batches", "response": "def calc_expectation(grad_dict, num_batches):\n    \"\"\"Calculates the expectation of the gradients per epoch for each parameter w.r.t number of batches\n\n    Parameters\n    ----------\n    grad_dict: dict\n        dictionary that maps parameter name to gradients in the mod executor group\n    num_batches: int\n        number of batches\n\n    Returns\n    ----------\n    grad_dict: dict\n        dictionary with new keys mapping to gradients expectations\n\n    \"\"\"\n    for key in grad_dict.keys():\n        grad_dict[str.format(key+\"_expectation\")] = mx.ndarray.sum(grad_dict[key], axis=0) / num_batches\n\n    return grad_dict"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef calc_variance(grad_dict, num_batches, param_names):\n    for i in range(len(param_names)):\n        diff_sqr = mx.ndarray.square(mx.nd.subtract(grad_dict[param_names[i]],\n                                                    grad_dict[str.format(param_names[i]+\"_expectation\")]))\n        grad_dict[str.format(param_names[i] + \"_variance\")] = mx.ndarray.sum(diff_sqr, axis=0) / num_batches", "response": "Calculates the variance of the gradients per epoch for each parameter w. r. t number of batches"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating directories recursively if they don t exist.", "response": "def makedirs(d):\n    \"\"\"Create directories recursively if they don't exist. os.makedirs(exist_ok=True) is not\n    available in Python2\"\"\"\n    if sys.version_info[0] < 3:\n        from distutils.dir_util import mkpath\n        mkpath(d)\n    else:\n        os.makedirs(d, exist_ok=True)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing precision and recall on the entity class", "response": "def classifer_metrics(label, pred):\n    \"\"\"\n    computes f1, precision and recall on the entity class\n    \"\"\"\n    prediction = np.argmax(pred, axis=1)\n    label = label.astype(int)\n\n    pred_is_entity = prediction != not_entity_index\n    label_is_entity = label != not_entity_index\n\n    corr_pred = (prediction == label) == (pred_is_entity == True)\n\n    #how many entities are there?\n    num_entities = np.sum(label_is_entity)\n    entity_preds = np.sum(pred_is_entity)\n\n    #how many times did we correctly predict an entity?\n    correct_entitites = np.sum(corr_pred[pred_is_entity])\n\n    #precision: when we predict entity, how often are we right?\n    precision = correct_entitites/entity_preds\n    if entity_preds == 0:\n        precision = np.nan\n\n    #recall: of the things that were an entity, how many did we catch?\n    recall = correct_entitites / num_entities\n    if num_entities == 0:\n        recall = np.nan\n    f1 = 2 * precision * recall / (precision + recall)\n    return precision, recall, f1"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconstruct data iter for the next batch of data.", "response": "def data_iter(batch_size, num_embed, pre_trained_word2vec=False):\n    \"\"\"Construct data iter\n\n    Parameters\n    ----------\n    batch_size: int\n    num_embed: int\n    pre_trained_word2vec: boolean\n                        identify the pre-trained layers or not\n    Returns\n    ----------\n    train_set: DataIter\n                Train DataIter\n    valid: DataIter\n                Valid DataIter\n    sentences_size: int\n                array dimensions\n    embedded_size: int\n                array dimensions\n    vocab_size: int\n                array dimensions\n    \"\"\"\n    print('Loading data...')\n    if pre_trained_word2vec:\n        word2vec = data_helpers.load_pretrained_word2vec('data/rt.vec')\n        x, y = data_helpers.load_data_with_word2vec(word2vec)\n        # reshape for convolution input\n        x = np.reshape(x, (x.shape[0], 1, x.shape[1], x.shape[2]))\n        embedded_size = x.shape[-1]\n        sentences_size = x.shape[2]\n        vocabulary_size = -1\n    else:\n        x, y, vocab, vocab_inv = data_helpers.load_data()\n        embedded_size = num_embed\n        sentences_size = x.shape[1]\n        vocabulary_size = len(vocab)\n\n    # randomly shuffle data\n    np.random.seed(10)\n    shuffle_indices = np.random.permutation(np.arange(len(y)))\n    x_shuffled = x[shuffle_indices]\n    y_shuffled = y[shuffle_indices]\n\n    # split train/valid set\n    x_train, x_dev = x_shuffled[:-1000], x_shuffled[-1000:]\n    y_train, y_dev = y_shuffled[:-1000], y_shuffled[-1000:]\n    print('Train/Valid split: %d/%d' % (len(y_train), len(y_dev)))\n    print('train shape:', x_train.shape)\n    print('valid shape:', x_dev.shape)\n    print('sentence max words', sentences_size)\n    print('embedding size', embedded_size)\n    print('vocab size', vocabulary_size)\n\n    train_set = mx.io.NDArrayIter(\n        x_train, y_train, batch_size, shuffle=True)\n    valid = mx.io.NDArrayIter(\n        x_dev, y_dev, batch_size)\n\n    return train_set, valid, sentences_size, embedded_size, vocabulary_size"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sym_gen(batch_size, sentences_size, num_embed, vocabulary_size,\n            num_label=2, filter_list=None, num_filter=100,\n            dropout=0.0, pre_trained_word2vec=False):\n    \"\"\"Generate network symbol\n\n    Parameters\n    ----------\n    batch_size: int\n    sentences_size: int\n    num_embed: int\n    vocabulary_size: int\n    num_label: int\n    filter_list: list\n    num_filter: int\n    dropout: int\n    pre_trained_word2vec: boolean\n                        identify the pre-trained layers or not\n    Returns\n    ----------\n    sm: symbol\n    data: list of str\n        data names\n    softmax_label: list of str\n        label names\n    \"\"\"\n    input_x = mx.sym.Variable('data')\n    input_y = mx.sym.Variable('softmax_label')\n\n    # embedding layer\n    if not pre_trained_word2vec:\n        embed_layer = mx.sym.Embedding(data=input_x,\n                                       input_dim=vocabulary_size,\n                                       output_dim=num_embed,\n                                       name='vocab_embed')\n        conv_input = mx.sym.Reshape(data=embed_layer, target_shape=(batch_size, 1, sentences_size, num_embed))\n    else:\n        conv_input = input_x\n\n    # create convolution + (max) pooling layer for each filter operation\n    pooled_outputs = []\n    for i, filter_size in enumerate(filter_list):\n        convi = mx.sym.Convolution(data=conv_input, kernel=(filter_size, num_embed), num_filter=num_filter)\n        relui = mx.sym.Activation(data=convi, act_type='relu')\n        pooli = mx.sym.Pooling(data=relui, pool_type='max', kernel=(sentences_size - filter_size + 1, 1), stride=(1, 1))\n        pooled_outputs.append(pooli)\n\n    # combine all pooled outputs\n    total_filters = num_filter * len(filter_list)\n    concat = mx.sym.Concat(*pooled_outputs, dim=1)\n    h_pool = mx.sym.Reshape(data=concat, target_shape=(batch_size, total_filters))\n\n    # dropout layer\n    if dropout > 0.0:\n        h_drop = mx.sym.Dropout(data=h_pool, p=dropout)\n    else:\n        h_drop = h_pool\n\n    # fully connected\n    cls_weight = mx.sym.Variable('cls_weight')\n    cls_bias = mx.sym.Variable('cls_bias')\n\n    fc = mx.sym.FullyConnected(data=h_drop, weight=cls_weight, bias=cls_bias, num_hidden=num_label)\n\n    # softmax output\n    sm = mx.sym.SoftmaxOutput(data=fc, label=input_y, name='softmax')\n\n    return sm, ('data',), ('softmax_label',)", "response": "Generate network symbol for the given batch size sentences_size num_embed vocabulary_size num_label num_filter num_embed dropout is optional."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntrains cnn model on a set of data.", "response": "def train(symbol_data, train_iterator, valid_iterator, data_column_names, target_names):\n    \"\"\"Train cnn model\n\n    Parameters\n    ----------\n    symbol_data: symbol\n    train_iterator: DataIter\n                    Train DataIter\n    valid_iterator: DataIter\n                    Valid DataIter\n    data_column_names: list of str\n                       Defaults to ('data') for a typical model used in image classification\n    target_names: list of str\n                  Defaults to ('softmax_label') for a typical model used in image classification\n    \"\"\"\n    devs = mx.cpu()  # default setting\n    if args.gpus is not None:\n        for i in args.gpus.split(','):\n            mx.gpu(int(i))\n        devs = mx.gpu()\n    module = mx.mod.Module(symbol_data, data_names=data_column_names, label_names=target_names, context=devs)\n    module.fit(train_data=train_iterator,\n               eval_data=valid_iterator,\n               eval_metric='acc',\n               kvstore=args.kv_store,\n               optimizer=args.optimizer,\n               optimizer_params={'learning_rate': args.lr},\n               initializer=mx.initializer.Uniform(0.1),\n               num_epoch=args.num_epochs,\n               batch_end_callback=mx.callback.Speedometer(args.batch_size, args.disp_batches),\n               epoch_end_callback=save_model())"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting the caltech101 mat file to images Examples -------- python convert_data. py", "response": "def convert_mat_to_images(args):\n    '''convert the caltech101 mat file to images\n    Examples\n    --------\n    python convert_data.py --dataset /home/ubuntu/datasets/caltech101/data/caltech101_silhouettes_28.mat --save_path /home/ubuntu/datasets/caltech101/data/ --invert --height 32 --width 32\n    '''\n    dataset = scipy.io.loadmat(\"{}/{}\".format(args.save_path, args.dataset))\n\n    # image pixel data\n    X = dataset['X']\n\n    # image class labels (not used in this project)\n    Y = dataset['Y']\n\n    total_image = X.shape[0]\n\n    h=args.height\n    w=args.width\n\n    for i in range(total_image):\n        img = X[i]\n        img = np.reshape(img, (28, 28))\n        if args.invert:\n            img = (1-img)*255\n        else:\n            img = img*255\n        img = Image.fromarray(img, 'L')\n        img = img.rotate(-90)\n        img = img.resize([h, w], Image.BILINEAR)\n        img.save(args.save_path + '/img' + str(i) + '.png')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbuilding a new virtual environment using CMake.", "response": "def build(args) -> None:\n    \"\"\"Build using CMake\"\"\"\n    venv_exe = shutil.which('virtualenv')\n    pyexe = shutil.which(args.pyexe)\n    if not venv_exe:\n        logging.warn(\"virtualenv wasn't found in path, it's recommended to install virtualenv to manage python environments\")\n    if not pyexe:\n        logging.warn(\"Python executable %s not found in path\", args.pyexe)\n    if args.cmake_options:\n        cmake = CMake(args.cmake_options)\n    else:\n        cmake = CMake()\n    cmake()\n    create_virtualenv(venv_exe, pyexe, args.venv)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a linear regression network for performing SVRG optimization.", "response": "def create_network(batch_size, update_freq):\n    \"\"\"Create a linear regression network for performing SVRG optimization.\n    Parameters\n    ----------\n    batch_size: int\n        Size of data split\n    update_freq: int\n        Update Frequency for calculating full gradients\n\n    Returns\n    ----------\n    di: mx.io.NDArrayIter\n        Data iterator\n    update_freq: SVRGModule\n        An instance of SVRGModule for performing SVRG optimization\n    \"\"\"\n    import logging\n    head = '%(asctime)-15s %(message)s'\n    logging.basicConfig(level=logging.INFO, format=head)\n\n    train_data = np.random.randint(1, 5, [1000, 2])\n    weights = np.array([1.0, 2.0])\n    train_label = train_data.dot(weights)\n\n    di = mx.io.NDArrayIter(train_data, train_label, batch_size=batch_size, shuffle=True, label_name='lin_reg_label')\n    X = mx.sym.Variable('data')\n    Y = mx.symbol.Variable('lin_reg_label')\n    fully_connected_layer = mx.sym.FullyConnected(data=X, name='fc1', num_hidden=1)\n    lro = mx.sym.LinearRegressionOutput(data=fully_connected_layer, label=Y, name=\"lro\")\n\n    mod = SVRGModule(\n        symbol=lro,\n        data_names=['data'],\n        label_names=['lin_reg_label'], update_freq=update_freq, logger=logging\n    )\n\n    return di, mod"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_squeezenet(version, pretrained=False, ctx=cpu(),\n                   root=os.path.join(base.data_dir(), 'models'), **kwargs):\n    r\"\"\"SqueezeNet model from the `\"SqueezeNet: AlexNet-level accuracy with 50x fewer parameters\n    and <0.5MB model size\" <https://arxiv.org/abs/1602.07360>`_ paper.\n    SqueezeNet 1.1 model from the `official SqueezeNet repo\n    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n    than SqueezeNet 1.0, without sacrificing accuracy.\n\n    Parameters\n    ----------\n    version : str\n        Version of squeezenet. Options are '1.0', '1.1'.\n    pretrained : bool, default False\n        Whether to load the pretrained weights for model.\n    ctx : Context, default CPU\n        The context in which to load the pretrained weights.\n    root : str, default $MXNET_HOME/models\n        Location for keeping the model parameters.\n    \"\"\"\n    net = SqueezeNet(version, **kwargs)\n    if pretrained:\n        from ..model_store import get_model_file\n        net.load_parameters(get_model_file('squeezenet%s'%version, root=root), ctx=ctx)\n    return net", "response": "r Returns a new SqueezeNet model with the given version."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_helper(attrs, attrs_name, alt_value=None):\n    tuple_re = re.compile('\\([0-9L|,| ]+\\)')\n    if not attrs:\n        return alt_value\n    attrs_str = None if attrs.get(attrs_name) is None else str(attrs.get(attrs_name))\n    if attrs_str is None:\n        return alt_value\n    attrs_match = tuple_re.search(attrs_str)\n    if attrs_match is not None:\n        if attrs_match.span() == (0, len(attrs_str)):\n            dims = eval(attrs_str)\n            return dims\n        else:\n            raise AttributeError(\"Malformed %s dimensions: %s\" % (attrs_name, str(attrs_str)))\n    return alt_value", "response": "Helper function to parse operator attributes in required format."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef transform_padding(pad_width):\n    num_pad_values = len(pad_width)\n    onnx_pad_width = [0]*num_pad_values\n\n    start_index = 0\n    # num_pad_values will always be multiple of 2\n    end_index = int(num_pad_values/2)\n    for idx in range(0, num_pad_values):\n        if idx % 2 == 0:\n            onnx_pad_width[start_index] = pad_width[idx]\n            start_index += 1\n        else:\n            onnx_pad_width[end_index] = pad_width[idx]\n            end_index += 1\n\n    return onnx_pad_width", "response": "Helper function to convert padding format for pad operator."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef convert_weights_and_inputs(node, **kwargs):\n    name, _, _ = get_inputs(node, kwargs)\n\n    if kwargs[\"is_input\"] is False:\n        weights = kwargs[\"weights\"]\n        initializer = kwargs[\"initializer\"]\n        np_arr = weights[name]\n        data_type = onnx.mapping.NP_TYPE_TO_TENSOR_TYPE[np_arr.dtype]\n        dims = np.shape(np_arr)\n\n        tensor_node = onnx.helper.make_tensor_value_info(name, data_type, dims)\n\n        initializer.append(\n            onnx.helper.make_tensor(\n                name=name,\n                data_type=data_type,\n                dims=dims,\n                vals=np_arr.flatten().tolist(),\n                raw=False,\n            )\n        )\n\n        return [tensor_node]\n    else:\n        tval_node = onnx.helper.make_tensor_value_info(name, kwargs[\"in_type\"], kwargs[\"in_shape\"])\n        return [tval_node]", "response": "Helper function to convert weights and inputs."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef convert_convolution(node, **kwargs):\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    kernel_dims = list(parse_helper(attrs, \"kernel\"))\n    stride_dims = list(parse_helper(attrs, \"stride\", [1, 1]))\n    pad_dims = list(parse_helper(attrs, \"pad\", [0, 0]))\n    num_group = int(attrs.get(\"num_group\", 1))\n    dilations = list(parse_helper(attrs, \"dilate\", [1, 1]))\n\n    pad_dims = pad_dims + pad_dims\n\n    conv_node = onnx.helper.make_node(\n        \"Conv\",\n        inputs=input_nodes,\n        outputs=[name],\n        kernel_shape=kernel_dims,\n        strides=stride_dims,\n        dilations=dilations,\n        pads=pad_dims,\n        group=num_group,\n        name=name\n    )\n\n    return [conv_node]", "response": "Map MXNet s convolution operator attributes to onnx s Conv operator\n    and return the created node."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef convert_deconvolution(node, **kwargs):\n    name, inputs, attrs = get_inputs(node, kwargs)\n\n    kernel_dims = list(parse_helper(attrs, \"kernel\"))\n    stride_dims = list(parse_helper(attrs, \"stride\", [1, 1]))\n    pad_dims = list(parse_helper(attrs, \"pad\", [0, 0]))\n    num_group = int(attrs.get(\"num_group\", 1))\n    dilations = list(parse_helper(attrs, \"dilate\", [1, 1]))\n    adj_dims = list(parse_helper(attrs, \"adj\", [0, 0]))\n\n    pad_dims = pad_dims + pad_dims\n\n    deconv_node = onnx.helper.make_node(\n        \"ConvTranspose\",\n        inputs=inputs,\n        outputs=[name],\n        kernel_shape=kernel_dims,\n        strides=stride_dims,\n        dilations=dilations,\n        output_padding=adj_dims,\n        pads=pad_dims,\n        group=num_group,\n        name=name\n    )\n\n    return [deconv_node]", "response": "Map MXNet s deconvolution operator attributes to onnx s ConvTranspose operator\n    and return the created node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmapping MXNet s crop operator attributes to onnx s Crop operator and return the created node.", "response": "def convert_crop(node, **kwargs):\n    \"\"\"Map MXNet's crop operator attributes to onnx's Crop operator\n    and return the created node.\n    \"\"\"\n    name, inputs, attrs = get_inputs(node, kwargs)\n    num_inputs = len(inputs)\n\n    y, x = list(parse_helper(attrs, \"offset\", [0, 0]))\n    h, w = list(parse_helper(attrs, \"h_w\", [0, 0]))\n    if num_inputs > 1:\n        h, w = kwargs[\"out_shape\"][-2:]\n    border = [x, y, x + w, y + h]\n\n    crop_node = onnx.helper.make_node(\n        \"Crop\",\n        inputs=[inputs[0]],\n        outputs=[name],\n        border=border,\n        scale=[1, 1],\n        name=name\n    )\n\n    logging.warning(\n        \"Using an experimental ONNX operator: Crop. \" \\\n        \"Its definition can change.\")\n\n    return [crop_node]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef convert_fully_connected(node, **kwargs):\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    initializer = kwargs[\"initializer\"]\n\n    no_bias = get_boolean_attribute_value(attrs, \"no_bias\")\n\n    fcnode = []\n\n    op_name = \"flatten_\" + str(kwargs[\"idx\"])\n    flatten_node = onnx.helper.make_node(\n        'Flatten',\n        inputs=[input_nodes[0]],\n        outputs=[op_name],\n        name=op_name\n    )\n\n    input_nodes[0] = op_name\n    fcnode.append(flatten_node)\n\n    if no_bias:\n        data_type = onnx.mapping.NP_TYPE_TO_TENSOR_TYPE[np.dtype('int64')]\n        bias_name = \"bias\" + str(kwargs[\"idx\"])\n        tensor_node = onnx.helper.make_tensor_value_info(bias_name, data_type, (1,))\n        initializer.append(\n            onnx.helper.make_tensor(\n                name=bias_name,\n                data_type=data_type,\n                dims=(1,),\n                vals=[0],\n                raw=False,\n            )\n        )\n        input_nodes.append(bias_name)\n        fcnode.append(tensor_node)\n\n    node = onnx.helper.make_node(\n        \"Gemm\",\n        input_nodes,  # input (A, B, C) - C can be in place\n        [name],  # output\n        alpha=1.0,\n        beta=1.0,\n        transA=False,\n        transB=True,\n        name=name\n    )\n\n    fcnode.append(node)\n\n    return fcnode", "response": "Map MXNet s FullyConnected operator attributes to onnx s Gemm operator and return the created node."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef convert_batchnorm(node, **kwargs):\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    momentum = float(attrs.get(\"momentum\", 0.9))\n    eps = float(attrs.get(\"eps\", 0.001))\n\n    bn_node = onnx.helper.make_node(\n        \"BatchNormalization\",\n        input_nodes,\n        [name],\n        name=name,\n        epsilon=eps,\n        momentum=momentum,\n        # MXNet computes mean and variance per feature for batchnorm\n        # Default for onnx is across all spatial features. So disabling the parameter.\n        spatial=0\n    )\n    return [bn_node]", "response": "Map MXNet s BatchNorm operator attributes to onnx s BatchNormalization operator and return the created node."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmaps MXNet s Activation operator attributes to onnx s Tanh and Relu operator and return the created node.", "response": "def convert_activation(node, **kwargs):\n    \"\"\"Map MXNet's Activation operator attributes to onnx's Tanh/Relu operator\n    and return the created node.\n    \"\"\"\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    act_type = attrs[\"act_type\"]\n\n    # Creating a dictionary here, but if this titlecase pattern\n    # mxnet_name.title()\n    act_types = {\n        \"tanh\": \"Tanh\",\n        \"relu\": \"Relu\",\n        \"sigmoid\": \"Sigmoid\",\n        \"softrelu\": \"Softplus\",\n        \"softsign\": \"Softsign\"\n    }\n\n    act_name = act_types.get(act_type)\n    if act_name:\n        node = onnx.helper.make_node(\n            act_name,\n            input_nodes,\n            [name],\n            name=name\n        )\n    else:\n        raise AttributeError(\n            \"Activation %s not implemented or recognized in the converter\" % act_type\n        )\n\n    return [node]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmapping MXNet s pad operator attributes to onnx s Pad operator and return the created node.", "response": "def convert_pad(node, **kwargs):\n    \"\"\"Map MXNet's pad operator attributes to onnx's Pad operator\n    and return the created node.\n    \"\"\"\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    mxnet_pad_width = convert_string_to_list(attrs.get(\"pad_width\"))\n    onnx_pad_width = transform_padding(mxnet_pad_width)\n\n    pad_mode = attrs.get(\"mode\")\n\n    if pad_mode == \"constant\":\n        pad_value = float(attrs.get(\"constant_value\")) \\\n            if \"constant_value\" in attrs else 0.0\n        node = onnx.helper.make_node(\n            'Pad',\n            inputs=input_nodes,\n            outputs=[name],\n            mode='constant',\n            value=pad_value,\n            pads=onnx_pad_width,\n            name=name\n        )\n    else:\n        node = onnx.helper.make_node(\n            'Pad',\n            inputs=input_nodes,\n            outputs=[name],\n            mode=pad_mode,\n            pads=onnx_pad_width,\n            name=name\n        )\n\n    return [node]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate extra transpose node for dot operator", "response": "def create_helper_trans_node(op_name, input_node, node_name):\n    \"\"\"create extra transpose node for dot operator\"\"\"\n    node_name = op_name + \"_\" + node_name\n    trans_node = onnx.helper.make_node(\n        'Transpose',\n        inputs=[input_node],\n        outputs=[node_name],\n        name=node_name\n    )\n    return trans_node"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef convert_dot(node, **kwargs):\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n    input_node_a = input_nodes[0]\n    input_node_b = input_nodes[1]\n\n    trans_a_node = None\n    trans_b_node = None\n\n    trans_a = get_boolean_attribute_value(attrs, \"transpose_a\")\n    trans_b = get_boolean_attribute_value(attrs, \"transpose_b\")\n\n    op_name = \"transpose\" + str(kwargs[\"idx\"])\n\n    if trans_a:\n        trans_a_node = create_helper_trans_node(op_name, input_nodes[0], 'a')\n        input_node_a = op_name+\"_a\"\n    if trans_b:\n        trans_b_node = create_helper_trans_node(op_name, input_nodes[1], 'b')\n        input_node_b = op_name+\"_b\"\n\n    matmul_node = onnx.helper.make_node(\n        'MatMul',\n        inputs=[input_node_a, input_node_b],\n        outputs=[name],\n        name=name\n    )\n\n    if not trans_a and not trans_b:\n        return [matmul_node]\n    elif trans_a and not trans_b:\n        return [trans_a_node, matmul_node]\n    elif trans_b and not trans_a:\n        return [trans_b_node, matmul_node]\n    else:\n        return [trans_a_node, trans_b_node, matmul_node]", "response": "Map MXNet s dot operator attributes to onnx s MatMul and Transpose operators based on the values set for transpose_a transpose_b attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmaps MXNet s _linalg_gemm2 operator attributes to onnx s _matmul and Transpose operators based on the values set for transpose_a transpose_b attributes.", "response": "def convert_linalg_gemm2(node, **kwargs):\n    \"\"\"Map MXNet's _linalg_gemm2 operator attributes to onnx's\n    MatMul and Transpose operators based on the values set for\n    transpose_a, transpose_b attributes.\n    Return multiple nodes created.\n    \"\"\"\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    # Getting the attributes and assigning default values.\n    alpha = float(attrs.get(\"alpha\", 1.0))\n    trans_a = get_boolean_attribute_value(attrs, \"transpose_a\")\n    trans_b = get_boolean_attribute_value(attrs, \"transpose_b\")\n\n    op_name = \"transpose\" + str(kwargs[\"idx\"])\n\n    if alpha == 1.0 and trans_a == 0 and trans_b == 0:\n        matmul_node = onnx.helper.make_node(\n            'MatMul',\n            inputs=input_nodes,\n            outputs=[name],\n            name=name\n        )\n        return [matmul_node]\n    elif trans_a == 1 and trans_b == 0:\n        op_name = \"transpose\" + str(kwargs[\"idx\"])\n        node_name = op_name+\"_a\"\n        trans_a_node = onnx.helper.make_node(\n            'Transpose',\n            inputs=[input_nodes[0]],\n            outputs=[op_name+\"_a\"],\n            name=node_name\n        )\n\n        matmul_node = onnx.helper.make_node(\n            'MatMul',\n            inputs=[node_name, input_nodes[1]],\n            outputs=[name],\n            name=name\n        )\n        return [trans_a_node, matmul_node]\n\n    elif trans_a == 0 and trans_b == 1:\n        node_name = op_name + \"_b\"\n        trans_b_node = onnx.helper.make_node(\n            'Transpose',\n            inputs=[input_nodes[1]],\n            outputs=[op_name+\"_b\"],\n            name=node_name\n        )\n\n        matmul_node = onnx.helper.make_node(\n            'MatMul',\n            inputs=[input_nodes[0], node_name],\n            outputs=[name],\n            name=name\n        )\n\n        return [trans_b_node, matmul_node]\n    else:\n        node_name_a = op_name+\"_a\"\n        trans_a_node = onnx.helper.make_node(\n            'Transpose',\n            inputs=[input_nodes[0]],\n            outputs=[op_name+\"_a\"],\n            name=node_name_a\n        )\n\n        node_name_b = op_name + \"_b\"\n        trans_b_node = onnx.helper.make_node(\n            'Transpose',\n            inputs=[input_nodes[1]],\n            outputs=[op_name+\"_b\"],\n            name=node_name_b\n        )\n\n        matmul_node = onnx.helper.make_node(\n            'MatMul',\n            inputs=input_nodes,\n            outputs=[name],\n            name=name\n        )\n\n        return [trans_a_node, trans_b_node, matmul_node]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef convert_pooling(node, **kwargs):\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    kernel = eval(attrs[\"kernel\"])\n    pool_type = attrs[\"pool_type\"] if attrs.get(\"pool_type\") else \"max\"\n    stride = eval(attrs[\"stride\"]) if attrs.get(\"stride\") else (1, 1)\n    global_pool = get_boolean_attribute_value(attrs, \"global_pool\")\n    p_value = attrs.get('p_value', 'None')\n\n    pooling_convention = attrs.get('pooling_convention', 'valid')\n\n    if pooling_convention == 'full':\n        pooling_warning = \"Pooling: ONNX currently doesn't support pooling_convention. \" \\\n                          \"This might lead to shape or accuracy issues. \" \\\n                          \"https://github.com/onnx/onnx/issues/549\"\n\n        logging.warning(pooling_warning)\n\n    pad_dims = list(parse_helper(attrs, \"pad\", [0, 0]))\n    pad_dims = pad_dims + pad_dims\n    pool_types = {\"max\": \"MaxPool\", \"avg\": \"AveragePool\", \"lp\": \"LpPool\"}\n    global_pool_types = {\"max\": \"GlobalMaxPool\", \"avg\": \"GlobalAveragePool\",\n                         \"lp\": \"GlobalLpPool\"}\n\n    if pool_type == 'lp' and p_value == 'None':\n        raise AttributeError('ONNX requires a p value for LpPool and GlobalLpPool')\n\n    if global_pool:\n        if pool_type == 'lp':\n            node = onnx.helper.make_node(\n                global_pool_types[pool_type],\n                input_nodes,  # input\n                [name],\n                p=int(p_value),\n                name=name\n            )\n        else:\n            node = onnx.helper.make_node(\n                global_pool_types[pool_type],\n                input_nodes,  # input\n                [name],\n                name=name\n            )\n    else:\n        if pool_type == 'lp':\n            node = onnx.helper.make_node(\n                pool_types[pool_type],\n                input_nodes,  # input\n                [name],\n                p=int(p_value),\n                kernel_shape=kernel,\n                pads=pad_dims,\n                strides=stride,\n                name=name\n            )\n        else:\n            node = onnx.helper.make_node(\n                pool_types[pool_type],\n                input_nodes,  # input\n                [name],\n                kernel_shape=kernel,\n                pads=pad_dims,\n                strides=stride,\n                name=name\n            )\n\n    return [node]", "response": "Map MXNet s Pooling operator attributes to onnx s MaxPool AveragePool MaxPool and GlobalAveragePool operators based on the input node s attributes and return the created node."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmaps MXNet s InstanceNorm operator attributes to onnx s InstanceNormalization operator based on the input node s attributes and return the created node.", "response": "def convert_instancenorm(node, **kwargs):\n    \"\"\"Map MXNet's InstanceNorm operator attributes to onnx's InstanceNormalization operator\n    based on the input node's attributes and return the created node.\n    \"\"\"\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    eps = float(attrs.get(\"eps\", 0.001))\n\n    node = onnx.helper.make_node(\n        'InstanceNormalization',\n        inputs=input_nodes,\n        outputs=[name],\n        name=name,\n        epsilon=eps)\n\n    return [node]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmaps MXNet s LeakyReLU operator attributes to onnx s Elu and LeakyRelu operator attributes and return the created node.", "response": "def convert_leakyrelu(node, **kwargs):\n    \"\"\"Map MXNet's LeakyReLU operator attributes to onnx's Elu/LeakyRelu/PRelu operators\n    based on the input node's attributes and return the created node.\n    \"\"\"\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    act_type = attrs.get(\"act_type\", \"leaky\")\n    alpha = float(attrs.get(\"slope\", 0.25))\n\n    act_name = {\"elu\": \"Elu\", \"leaky\": \"LeakyRelu\", \"prelu\": \"PRelu\",\n                \"selu\": \"Selu\"}\n\n    if act_type == \"prelu\" or act_type == \"selu\":\n        node = onnx.helper.make_node(\n            act_name[act_type],\n            inputs=input_nodes,\n            outputs=[name],\n            name=name)\n    else:\n        node = onnx.helper.make_node(\n            act_name[act_type],\n            inputs=input_nodes,\n            outputs=[name],\n            name=name,\n            alpha=alpha)\n\n    return [node]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmaps MXNet s softmax operator attributes to onnx s Softmax operator and return the created node.", "response": "def convert_softmax(node, **kwargs):\n    \"\"\"Map MXNet's softmax operator attributes to onnx's Softmax operator\n    and return the created node.\n    \"\"\"\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    axis = int(attrs.get(\"axis\", -1))\n\n    softmax_node = onnx.helper.make_node(\n        \"Softmax\",\n        input_nodes,\n        [name],\n        axis=axis,\n        name=name\n    )\n\n    return [softmax_node]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmapping MXNet s SoftmaxOutput operator attributes to onnx s Softmax operator and return the created node.", "response": "def convert_softmax_output(node, **kwargs):\n    \"\"\"Map MXNet's SoftmaxOutput operator attributes to onnx's Softmax operator\n    and return the created node.\n    \"\"\"\n    name = node[\"name\"]\n\n    input1_idx = kwargs[\"index_lookup\"][node[\"inputs\"][0][0]]\n    input1 = kwargs[\"proc_nodes\"][input1_idx]\n\n    softmax_node = onnx.helper.make_node(\n        \"Softmax\",\n        [input1.name],\n        [name],\n        axis=1,\n        name=name\n    )\n\n    return [softmax_node]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmapping MXNet s SoftmaxOutput operator attributes to onnx s SoftmaxOutput operator and return the created node.", "response": "def convert_logistic_regression_output(node, **kwargs):\n    \"\"\"Map MXNet's SoftmaxOutput operator attributes to onnx's Softmax operator\n    and return the created node.\n    \"\"\"\n    name = node[\"name\"]\n    input1_idx = kwargs[\"index_lookup\"][node[\"inputs\"][0][0]]\n    input1 = kwargs[\"proc_nodes\"][input1_idx]\n    sigmoid_node = onnx.helper.make_node(\n        \"Sigmoid\",\n        [input1.name],\n        [name],\n        name=name\n    )\n    return [sigmoid_node]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmap MXNet s Concat operator attributes to onnx s Concat operator and return the created node.", "response": "def convert_concat(node, **kwargs):\n    \"\"\"Map MXNet's Concat operator attributes to onnx's Concat operator\n    and return the created node.\n    \"\"\"\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    axis = int(attrs.get(\"dim\", 1))\n    concat_node = onnx.helper.make_node(\n        \"Concat\",\n        input_nodes,\n        [name],\n        axis=axis,\n        name=name\n    )\n    return [concat_node]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef convert_transpose(node, **kwargs):\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    axes = attrs.get(\"axes\", ())\n    if axes:\n        axes = tuple(map(int, re.findall(r'\\d+', axes)))\n\n        transpose_node = onnx.helper.make_node(\n            \"Transpose\",\n            input_nodes,\n            [name],\n            perm=axes,\n            name=name\n        )\n    else:\n        transpose_node = onnx.helper.make_node(\n            \"Transpose\",\n            input_nodes,\n            [name],\n            name=name\n        )\n\n    return [transpose_node]", "response": "Map MXNet s transpose operator attributes to onnx s Transpose operator and return the created node."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef convert_lrn(node, **kwargs):\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    alpha = float(attrs.get(\"alpha\", 0.0001))\n    beta = float(attrs.get(\"beta\", 0.75))\n    bias = float(attrs.get(\"knorm\", 1.0))\n    size = int(attrs.get(\"nsize\"))\n\n    lrn_node = onnx.helper.make_node(\n        \"LRN\",\n        inputs=input_nodes,\n        outputs=[name],\n        name=name,\n        alpha=alpha,\n        beta=beta,\n        bias=bias,\n        size=size\n    )\n\n    return [lrn_node]", "response": "Map MXNet s LRN operator attributes to onnx s LRN operator and return the created node."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmaps MXNet s L2Normalization operator attributes to onnx s LpNormalization operator and return the created node.", "response": "def convert_l2normalization(node, **kwargs):\n    \"\"\"Map MXNet's L2Normalization operator attributes to onnx's LpNormalization operator\n    and return the created node.\n    \"\"\"\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    mode = attrs.get(\"mode\", \"instance\")\n\n    if mode != \"channel\":\n        raise AttributeError(\"L2Normalization: ONNX currently supports channel mode only\")\n\n    l2norm_node = onnx.helper.make_node(\n        \"LpNormalization\",\n        input_nodes,\n        [name],\n        axis=1,  # channel only\n        name=name\n    )\n    return [l2norm_node]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmap MXNet s Dropout operator attributes to onnx s Dropout operator and return the created node.", "response": "def convert_dropout(node, **kwargs):\n    \"\"\"Map MXNet's Dropout operator attributes to onnx's Dropout operator\n    and return the created node.\n    \"\"\"\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    probability = float(attrs.get(\"p\", 0.5))\n\n    dropout_node = onnx.helper.make_node(\n        \"Dropout\",\n        input_nodes,\n        [name],\n        ratio=probability,\n        name=name\n    )\n    return [dropout_node]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmap MXNet s Clip operator attributes to onnx s Clip operator and return the created node.", "response": "def convert_clip(node, **kwargs):\n    \"\"\"Map MXNet's Clip operator attributes to onnx's Clip operator\n    and return the created node.\n    \"\"\"\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    a_min = np.float(attrs.get('a_min', -np.inf))\n    a_max = np.float(attrs.get('a_max', np.inf))\n\n    clip_node = onnx.helper.make_node(\n        \"Clip\",\n        input_nodes,\n        [name],\n        name=name,\n        min=a_min,\n        max=a_max\n    )\n    return [clip_node]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmaps MXNet s argmax operator attributes to onnx s ArgMax operator and return the created node.", "response": "def convert_argmax(node, **kwargs):\n    \"\"\"Map MXNet's argmax operator attributes to onnx's ArgMax operator\n    and return the created node.\n    \"\"\"\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    axis = int(attrs.get(\"axis\"))\n    keepdims = get_boolean_attribute_value(attrs, \"keepdims\")\n\n    node = onnx.helper.make_node(\n        'ArgMax',\n        inputs=input_nodes,\n        axis=axis,\n        keepdims=keepdims,\n        outputs=[name],\n        name=name\n    )\n    return [node]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef convert_reshape(node, **kwargs):\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    output_shape_list = convert_string_to_list(attrs[\"shape\"])\n\n    initializer = kwargs[\"initializer\"]\n    output_shape_np = np.array(output_shape_list, dtype='int64')\n    data_type = onnx.mapping.NP_TYPE_TO_TENSOR_TYPE[output_shape_np.dtype]\n    dims = np.shape(output_shape_np)\n\n    output_shape_name = \"reshape_attr_tensor\" + str(kwargs[\"idx\"])\n    tensor_node = onnx.helper.make_tensor_value_info(output_shape_name, data_type, dims)\n\n    initializer.append(\n        onnx.helper.make_tensor(\n            name=output_shape_name,\n            data_type=data_type,\n            dims=dims,\n            vals=output_shape_list,\n            raw=False,\n        )\n    )\n\n    input_nodes.append(output_shape_name)\n\n    not_supported_shape = [-2, -3, -4]\n\n    for val in output_shape_list:\n        if val in not_supported_shape:\n            raise AttributeError(\"Reshape: Shape value not supported in ONNX\", val)\n\n    reshape_node = onnx.helper.make_node(\n        \"Reshape\",\n        input_nodes,\n        [name],\n        name=name\n    )\n\n    return [tensor_node, reshape_node]", "response": "Map MXNet s Reshape operator attributes to onnx s Reshape operator and return multiple created nodes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef convert_cast(node, **kwargs):\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    dtype = attrs[\"dtype\"]\n\n    # dtype can be mapped only with types from TensorProto\n    # float32 is mapped to float and float64 to double in onnx\n    # following tensorproto mapping https://github.com/onnx/onnx/blob/master/onnx/mapping.py\n    if dtype == 'float32':\n        dtype = 'float'\n    elif dtype == 'float64':\n        dtype = 'double'\n\n    node = onnx.helper.make_node(\n        \"Cast\",\n        input_nodes,\n        [name],\n        to=getattr(onnx.TensorProto, dtype.upper()),\n        name=name,\n    )\n    return [node]", "response": "Map MXNet s Cast operator attributes to onnx s Cast operator\n    and return the created node."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmaps MXNet s slice_axis operator attributes to onnx s Slice operator and return the created node.", "response": "def convert_slice_axis(node, **kwargs):\n    \"\"\"Map MXNet's slice_axis operator attributes to onnx's Slice operator\n    and return the created node.\n    \"\"\"\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    axes = int(attrs.get(\"axis\"))\n    starts = int(attrs.get(\"begin\"))\n    ends = int(attrs.get(\"end\", None))\n    if not ends:\n        raise ValueError(\"Slice: ONNX doesnt't support 'None' in 'end' attribute\")\n\n    node = onnx.helper.make_node(\n        \"Slice\",\n        input_nodes,\n        [name],\n        axes=[axes],\n        starts=[starts],\n        ends=[ends],\n        name=name,\n    )\n    return [node]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmap MXNet s SliceChannel operator attributes to onnx s Squeeze or Split operator based on squeeze_axis attribute and return the created node.", "response": "def convert_slice_channel(node, **kwargs):\n    \"\"\"Map MXNet's SliceChannel operator attributes to onnx's Squeeze or Split\n    operator based on squeeze_axis attribute\n    and return the created node.\n    \"\"\"\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    num_outputs = int(attrs.get(\"num_outputs\"))\n    axis = int(attrs.get(\"axis\", 1))\n    squeeze_axis = int(attrs.get(\"squeeze_axis\", 0))\n\n    if squeeze_axis == 1 and num_outputs == 1:\n        node = onnx.helper.make_node(\n            \"Squeeze\",\n            input_nodes,\n            [name],\n            axes=[axis],\n            name=name,\n        )\n        return [node]\n    elif squeeze_axis == 0 and num_outputs > 1:\n        in_shape = kwargs.get('in_shape')[0]\n        split = in_shape[axis] // num_outputs\n        node = onnx.helper.make_node(\n            \"Split\",\n            input_nodes,\n            [name+'_output'+str(i) for i in range(num_outputs)],\n            axis=axis,\n            split=[split for _ in range(num_outputs)],\n            name=name,\n        )\n        return [node]\n    else:\n        raise NotImplementedError(\"SliceChannel operator with num_outputs>1 and\"\n                                  \"squeeze_axis true is not implemented.\")"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef convert_expand_dims(node, **kwargs):\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    axis = int(attrs.get(\"axis\"))\n\n    node = onnx.helper.make_node(\n        \"Unsqueeze\",\n        input_nodes,\n        [name],\n        axes=[axis],\n        name=name,\n    )\n    return [node]", "response": "Map MXNet s expand_dims operator attributes to onnx s Unsqueeze operator and return the created node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef convert_squeeze(node, **kwargs):\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    axis = attrs.get(\"axis\", None)\n    if not axis:\n        raise AttributeError(\"Squeeze: Missing axis attribute: ONNX currently requires axis to \"\n                             \"be specified for squeeze operator\")\n    axis = convert_string_to_list(axis)\n\n    node = onnx.helper.make_node(\n        \"Squeeze\",\n        input_nodes,\n        [name],\n        axes=axis,\n        name=name,\n    )\n    return [node]", "response": "Map MXNet s squeeze operator attributes to onnx s squeeze operator\n    and return the created node."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef convert_depthtospace(node, **kwargs):\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    blksize = int(attrs.get(\"block_size\", 0))\n\n    node = onnx.helper.make_node(\n        \"DepthToSpace\",\n        input_nodes,\n        [name],\n        blocksize=blksize,\n        name=name,\n    )\n    return [node]", "response": "Map MXNet s DepthToSpace operator attributes to onnx s DepthToSpace operator and return the created node."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef convert_square(node, **kwargs):\n    name, input_nodes, _ = get_inputs(node, kwargs)\n\n    initializer = kwargs[\"initializer\"]\n    data_type = onnx.mapping.NP_TYPE_TO_TENSOR_TYPE[np.dtype('int64')]\n\n    power2_name = \"square_tensor\" + str(kwargs[\"idx\"])\n    tensor_node = onnx.helper.make_tensor_value_info(power2_name, data_type, (1,))\n    initializer.append(\n        onnx.helper.make_tensor(\n            name=power2_name,\n            data_type=data_type,\n            dims=(1,),\n            vals=[2],\n            raw=False,\n        )\n    )\n\n    input_nodes.append(power2_name)\n\n    node = onnx.helper.make_node(\n        \"Pow\",\n        input_nodes,\n        [name],\n        name=name\n    )\n    return [tensor_node, node]", "response": "Map MXNet s square operator attributes to onnx s Pow operator\n    and return the created node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmap MXNet s sum operator attributes to onnx s ReduceSum operator and return the created node.", "response": "def convert_sum(node, **kwargs):\n    \"\"\"Map MXNet's sum operator attributes to onnx's ReduceSum operator\n    and return the created node.\n    \"\"\"\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    mx_axis = attrs.get(\"axis\", None)\n    axes = convert_string_to_list(str(mx_axis)) if mx_axis is not None else None\n\n    keepdims = get_boolean_attribute_value(attrs, \"keepdims\")\n\n    if axes:\n        node = onnx.helper.make_node(\n            'ReduceSum',\n            inputs=input_nodes,\n            outputs=[name],\n            axes=axes,\n            keepdims=keepdims,\n            name=name\n        )\n    else:\n        node = onnx.helper.make_node(\n            'ReduceSum',\n            inputs=input_nodes,\n            outputs=[name],\n            keepdims=keepdims,\n            name=name\n        )\n    return [node]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef convert_hardsigmoid(node, **kwargs):\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    # Converting to float32\n    alpha = float(attrs.get(\"alpha\", 0.2))\n    beta = float(attrs.get(\"beta\", 0.5))\n\n    node = onnx.helper.make_node(\n        'HardSigmoid',\n        input_nodes,\n        [name],\n        alpha=alpha,\n        beta=beta,\n        name=name\n    )\n    return [node]", "response": "Map MXNet s hard_sigmoid operator attributes to onnx s HardSigmoid operator and return the created node."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmaps MXNet s log_softmax operator attributes to onnx s LogSoftMax operator and return the created node.", "response": "def convert_logsoftmax(node, **kwargs):\n    \"\"\"Map MXNet's log_softmax operator attributes to onnx's LogSoftMax operator\n    and return the created node.\n    \"\"\"\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    # Converting to int\n    axis = int(attrs.get(\"axis\", -1))\n    temp = attrs.get(\"temperature\", 'None')\n    if temp != 'None':\n        raise AttributeError(\"LogSoftMax: ONNX supports only temperature=None\")\n\n    node = onnx.helper.make_node(\n        'LogSoftmax',\n        input_nodes,\n        [name],\n        axis=axis,\n        name=name\n    )\n    return [node]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmap MXNet s norm operator attributes to onnx s ReduceL1 and ReduceL2 operators and return the created node.", "response": "def convert_norm(node, **kwargs):\n    \"\"\"Map MXNet's norm operator attributes to onnx's ReduceL1 and ReduceL2 operators\n    and return the created node.\n    \"\"\"\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    mx_axis = attrs.get(\"axis\", None)\n    axes = convert_string_to_list(str(mx_axis)) if mx_axis else None\n\n    keepdims = get_boolean_attribute_value(attrs, \"keepdims\")\n    ord = int(attrs.get(\"ord\", 2))\n\n    onnx_op_name = \"ReduceL1\" if ord == 1 else \"ReduceL2\"\n\n    if axes:\n        reduce_node = onnx.helper.make_node(\n            onnx_op_name,\n            input_nodes,\n            [name],\n            axes=axes,\n            keepdims=keepdims,\n            name=name\n        )\n        return [reduce_node]\n    else:\n        reduce_node = onnx.helper.make_node(\n            onnx_op_name,\n            input_nodes,\n            [name],\n            keepdims=keepdims,\n            name=name\n        )\n        return [reduce_node]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmap MXNet s multinomial operator attributes to onnx s Multinomial operator and return the created node.", "response": "def convert_multinomial(node, **kwargs):\n    \"\"\"Map MXNet's multinomial operator attributes to onnx's\n    Multinomial operator and return the created node.\n    \"\"\"\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n    dtype = onnx.mapping.NP_TYPE_TO_TENSOR_TYPE[np.dtype(attrs.get(\"dtype\", 'int32'))]\n    sample_size = convert_string_to_list(attrs.get(\"shape\", '1'))\n    if len(sample_size) < 2:\n        sample_size = sample_size[-1]\n    else:\n        raise AttributeError(\"ONNX currently supports integer sample_size only\")\n    node = onnx.helper.make_node(\n        \"Multinomial\",\n        input_nodes,\n        [name],\n        dtype=dtype,\n        sample_size=sample_size,\n        name=name,\n    )\n    return [node]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmapping MXNet s random_uniform operator attributes to onnx s RandomUniform operator and return the created node.", "response": "def convert_random_uniform(node, **kwargs):\n    \"\"\"Map MXNet's random_uniform operator attributes to onnx's RandomUniform\n    operator and return the created node.\n    \"\"\"\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    # Converting to float32\n    low = float(attrs.get(\"low\", 0))\n    high = float(attrs.get(\"high\", 1.0))\n    shape = convert_string_to_list(attrs.get('shape', '[]'))\n    dtype = onnx.mapping.NP_TYPE_TO_TENSOR_TYPE[np.dtype(attrs.get('dtype', 'float32'))]\n\n    node = onnx.helper.make_node(\n        'RandomUniform',\n        input_nodes,\n        [name],\n        low=low,\n        high=high,\n        dtype=dtype,\n        shape=shape,\n        name=name\n    )\n    return [node]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef convert_random_normal(node, **kwargs):\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    # Converting to float32\n    mean = float(attrs.get(\"loc\", 0))\n    scale = float(attrs.get(\"scale\", 1.0))\n    shape = convert_string_to_list(attrs.get('shape', '[]'))\n    dtype = onnx.mapping.NP_TYPE_TO_TENSOR_TYPE[np.dtype(attrs.get('dtype', 'float32'))]\n\n    node = onnx.helper.make_node(\n        'RandomNormal',\n        input_nodes,\n        [name],\n        mean=mean,\n        scale=scale,\n        dtype=dtype,\n        shape=shape,\n        name=name\n    )\n    return [node]", "response": "Map MXNet s RandomNormal operator attributes to onnx s RandomNormal\n    operator and return the created node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmap MXNet s ROIPooling operator attributes to onnx s MaxRoiPool operator and return the created node.", "response": "def convert_roipooling(node, **kwargs):\n    \"\"\"Map MXNet's ROIPooling operator attributes to onnx's MaxRoiPool\n    operator and return the created node.\n    \"\"\"\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    pooled_shape = convert_string_to_list(attrs.get('pooled_size'))\n    scale = float(attrs.get(\"spatial_scale\"))\n\n    node = onnx.helper.make_node(\n        'MaxRoiPool',\n        input_nodes,\n        [name],\n        pooled_shape=pooled_shape,\n        spatial_scale=scale,\n        name=name\n    )\n    return [node]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef convert_tile(node, **kwargs):\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    reps_list = convert_string_to_list(attrs[\"reps\"])\n\n    initializer = kwargs[\"initializer\"]\n    reps_shape_np = np.array(reps_list, dtype='int64')\n    data_type = onnx.mapping.NP_TYPE_TO_TENSOR_TYPE[reps_shape_np.dtype]\n    dims = np.shape(reps_shape_np)\n\n    output_shape_name = \"reps_attr_tensor\" + str(kwargs[\"idx\"])\n    tensor_node = onnx.helper.make_tensor_value_info(output_shape_name, data_type, dims)\n\n    initializer.append(\n        onnx.helper.make_tensor(\n            name=output_shape_name,\n            data_type=data_type,\n            dims=dims,\n            vals=reps_list,\n            raw=False,\n        )\n    )\n\n    input_nodes.append(output_shape_name)\n    tile_node = onnx.helper.make_node(\n        \"Tile\",\n        input_nodes,\n        [name],\n        name=name\n    )\n\n    return [tensor_node, tile_node]", "response": "Map MXNet s Tile operator attributes to onnx s Tile node and return the created node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef convert_broadcast_to(node, **kwargs):\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    shape_list = convert_string_to_list(attrs[\"shape\"])\n\n    initializer = kwargs[\"initializer\"]\n    output_shape_np = np.array(shape_list, dtype='int64')\n    data_type = onnx.mapping.NP_TYPE_TO_TENSOR_TYPE[output_shape_np.dtype]\n    dims = np.shape(output_shape_np)\n\n    output_shape_name = \"expand_attr_tensor\" + str(kwargs[\"idx\"])\n    tensor_node = onnx.helper.make_tensor_value_info(output_shape_name, data_type, dims)\n\n    initializer.append(\n        onnx.helper.make_tensor(\n            name=output_shape_name,\n            data_type=data_type,\n            dims=dims,\n            vals=shape_list,\n            raw=False,\n        )\n    )\n\n    input_nodes.append(output_shape_name)\n    expand_node = onnx.helper.make_node(\n        \"Expand\",\n        input_nodes,\n        [name],\n        name=name\n    )\n\n    return [tensor_node, expand_node]", "response": "Map MXNet s broadcast_to operator attributes to onnx s Expand\n    operator and return the created node."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the current executor", "response": "def exe(self):\n        \"\"\"Get the current executor\n\n        Returns\n        -------\n        exe : mxnet.executor.Executor\n        \"\"\"\n        return self._buckets[self.curr_bucket_key]['exe'][tuple(self.data_shapes.items())]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compute_internal(self, sym_name, bucket_kwargs=None, **arg_dict):\n        data_shapes = {k: v.shape for k, v in arg_dict.items()}\n        self.switch_bucket(bucket_kwargs=bucket_kwargs,\n                           data_shapes=data_shapes)\n        internal_sym = self.sym.get_internals()[sym_name]\n        data_inputs = {k: mx.nd.empty(v, ctx=self.ctx)\n                       for k, v in self.data_shapes.items()\n                       if k in internal_sym.list_arguments()}\n        params = {k: v for k, v in self.params.items() if\n                  k in internal_sym.list_arguments()}\n        aux_states = {k: v for k, v in self.aux_states.items()\n                      if k in internal_sym.list_auxiliary_states()}\n        exe = internal_sym.bind(ctx=self.ctx,\n                                args=dict(params, **data_inputs),\n                                args_grad=None,\n                                grad_req='null',\n                                aux_states=aux_states,\n                                shared_exec=self.exe)\n        for k, v in arg_dict.items():\n            exe.arg_dict[k][:] = v\n        exe.forward(is_train=False)\n        assert 1 == len(exe.outputs)\n        for output in exe.outputs:\n            output.wait_to_read()\n        return exe.outputs[0]", "response": "View the internal symbols using the forward function."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef init_from_fcnxs(ctx, fcnxs_symbol, fcnxs_args_from, fcnxs_auxs_from):\n    fcnxs_args = fcnxs_args_from.copy()\n    fcnxs_auxs = fcnxs_auxs_from.copy()\n    for k,v in fcnxs_args.items():\n        if(v.context != ctx):\n            fcnxs_args[k] = mx.nd.zeros(v.shape, ctx)\n            v.copyto(fcnxs_args[k])\n    for k,v in fcnxs_auxs.items():\n        if(v.context != ctx):\n            fcnxs_auxs[k] = mx.nd.zeros(v.shape, ctx)\n            v.copyto(fcnxs_auxs[k])\n    data_shape=(1,3,500,500)\n    arg_names = fcnxs_symbol.list_arguments()\n    arg_shapes, _, _ = fcnxs_symbol.infer_shape(data=data_shape)\n    rest_params = {}\n    deconv_params = {}\n    # this is fcn8s init from fcn16s\n    if 'score_pool3_weight' in arg_names:\n        rest_params = dict([(x[0], mx.nd.zeros(x[1], ctx)) for x in zip(arg_names, arg_shapes)\n            if x[0] in ['score_pool3_bias', 'score_pool3_weight']])\n        deconv_params = dict([(x[0], x[1]) for x in zip(arg_names, arg_shapes) if x[0] \\\n            in [\"bigscore_weight\", 'score4_weight']])\n    # this is fcn16s init from fcn32s\n    elif 'score_pool4_weight' in arg_names:\n        rest_params = dict([(x[0], mx.nd.zeros(x[1], ctx)) for x in zip(arg_names, arg_shapes)\n            if x[0] in ['score_pool4_weight', 'score_pool4_bias']])\n        deconv_params = dict([(x[0], x[1]) for x in zip(arg_names, arg_shapes) if x[0] \\\n            in [\"bigscore_weight\", 'score2_weight']])\n    # this is fcn32s init\n    else:\n        logging.error(\"you are init the fcn32s model, so you should use init_from_vgg16()\")\n        sys.exit()\n    fcnxs_args.update(rest_params)\n    for k, v in deconv_params.items():\n        filt = upsample_filt(v[3])\n        initw = np.zeros(v)\n        initw[range(v[0]), range(v[1]), :, :] = filt  # becareful here is the slice assing\n        fcnxs_args[k] = mx.nd.array(initw, ctx)\n    return fcnxs_args, fcnxs_auxs", "response": "initialize the internal state of the FCNS module from the given arguments and auxs."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef residual_unit(data, num_filter, stride, dim_match, name, bottle_neck=True, num_group=32, bn_mom=0.9, workspace=256, memonger=False):\n    if bottle_neck:\n        # the same as https://github.com/facebook/fb.resnet.torch#notes, a bit difference with origin paper\n\n        conv1 = mx.sym.Convolution(data=data, num_filter=int(num_filter*0.5), kernel=(1,1), stride=(1,1), pad=(0,0),\n                                      no_bias=True, workspace=workspace, name=name + '_conv1')\n        bn1 = mx.sym.BatchNorm(data=conv1, fix_gamma=False, eps=2e-5, momentum=bn_mom, name=name + '_bn1')\n        act1 = mx.sym.Activation(data=bn1, act_type='relu', name=name + '_relu1')\n\n\n        conv2 = mx.sym.Convolution(data=act1, num_filter=int(num_filter*0.5), num_group=num_group, kernel=(3,3), stride=stride, pad=(1,1),\n                                      no_bias=True, workspace=workspace, name=name + '_conv2')\n        bn2 = mx.sym.BatchNorm(data=conv2, fix_gamma=False, eps=2e-5, momentum=bn_mom, name=name + '_bn2')\n        act2 = mx.sym.Activation(data=bn2, act_type='relu', name=name + '_relu2')\n\n\n        conv3 = mx.sym.Convolution(data=act2, num_filter=num_filter, kernel=(1,1), stride=(1,1), pad=(0,0), no_bias=True,\n                                   workspace=workspace, name=name + '_conv3')\n        bn3 = mx.sym.BatchNorm(data=conv3, fix_gamma=False, eps=2e-5, momentum=bn_mom, name=name + '_bn3')\n\n        if dim_match:\n            shortcut = data\n        else:\n            shortcut_conv = mx.sym.Convolution(data=data, num_filter=num_filter, kernel=(1,1), stride=stride, no_bias=True,\n                                            workspace=workspace, name=name+'_sc')\n            shortcut = mx.sym.BatchNorm(data=shortcut_conv, fix_gamma=False, eps=2e-5, momentum=bn_mom, name=name + '_sc_bn')\n\n        if memonger:\n            shortcut._set_attr(mirror_stage='True')\n        eltwise =  bn3 + shortcut\n        return mx.sym.Activation(data=eltwise, act_type='relu', name=name + '_relu')\n    else:\n\n        conv1 = mx.sym.Convolution(data=data, num_filter=num_filter, kernel=(3,3), stride=stride, pad=(1,1),\n                                      no_bias=True, workspace=workspace, name=name + '_conv1')\n        bn1 = mx.sym.BatchNorm(data=conv1, fix_gamma=False, momentum=bn_mom, eps=2e-5, name=name + '_bn1')\n        act1 = mx.sym.Activation(data=bn1, act_type='relu', name=name + '_relu1')\n\n\n        conv2 = mx.sym.Convolution(data=act1, num_filter=num_filter, kernel=(3,3), stride=(1,1), pad=(1,1),\n                                      no_bias=True, workspace=workspace, name=name + '_conv2')\n        bn2 = mx.sym.BatchNorm(data=conv2, fix_gamma=False, momentum=bn_mom, eps=2e-5, name=name + '_bn2')\n\n        if dim_match:\n            shortcut = data\n        else:\n            shortcut_conv = mx.sym.Convolution(data=data, num_filter=num_filter, kernel=(1,1), stride=stride, no_bias=True,\n                                            workspace=workspace, name=name+'_sc')\n            shortcut = mx.sym.BatchNorm(data=shortcut_conv, fix_gamma=False, eps=2e-5, momentum=bn_mom, name=name + '_sc_bn')\n\n        if memonger:\n            shortcut._set_attr(mirror_stage='True')\n        eltwise = bn2 + shortcut\n        return mx.sym.Activation(data=eltwise, act_type='relu', name=name + '_relu')", "response": "Builds ResNet Unit symbol for a single resource set."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn ResNeXt symbol of the next stage.", "response": "def resnext(units, num_stages, filter_list, num_classes, num_group, image_shape, bottle_neck=True, bn_mom=0.9, workspace=256, dtype='float32', memonger=False):\n    \"\"\"Return ResNeXt symbol of\n    Parameters\n    ----------\n    units : list\n        Number of units in each stage\n    num_stages : int\n        Number of stage\n    filter_list : list\n        Channel size of each stage\n    num_classes : int\n        Ouput size of symbol\n    num_groupes: int\n    Number of conv groups\n    dataset : str\n        Dataset type, only cifar10 and imagenet supports\n    workspace : int\n        Workspace used in convolution operator\n    dtype : str\n        Precision (float32 or float16)\n    \"\"\"\n    num_unit = len(units)\n    assert(num_unit == num_stages)\n    data = mx.sym.Variable(name='data')\n    if dtype == 'float32':\n        data = mx.sym.identity(data=data, name='id')\n    else:\n        if dtype == 'float16':\n            data = mx.sym.Cast(data=data, dtype=np.float16)\n    data = mx.sym.BatchNorm(data=data, fix_gamma=True, eps=2e-5, momentum=bn_mom, name='bn_data')\n    (nchannel, height, width) = image_shape\n    if height <= 32:            # such as cifar10\n        body = mx.sym.Convolution(data=data, num_filter=filter_list[0], kernel=(3, 3), stride=(1,1), pad=(1, 1),\n                                  no_bias=True, name=\"conv0\", workspace=workspace)\n    else:                       # often expected to be 224 such as imagenet\n        body = mx.sym.Convolution(data=data, num_filter=filter_list[0], kernel=(7, 7), stride=(2,2), pad=(3, 3),\n                                  no_bias=True, name=\"conv0\", workspace=workspace)\n        body = mx.sym.BatchNorm(data=body, fix_gamma=False, eps=2e-5, momentum=bn_mom, name='bn0')\n        body = mx.sym.Activation(data=body, act_type='relu', name='relu0')\n        body = mx.sym.Pooling(data=body, kernel=(3, 3), stride=(2,2), pad=(1,1), pool_type='max')\n\n    for i in range(num_stages):\n        body = residual_unit(body, filter_list[i+1], (1 if i==0 else 2, 1 if i==0 else 2), False,\n                             name='stage%d_unit%d' % (i + 1, 1), bottle_neck=bottle_neck, num_group=num_group,\n                             bn_mom=bn_mom, workspace=workspace, memonger=memonger)\n        for j in range(units[i]-1):\n            body = residual_unit(body, filter_list[i+1], (1,1), True, name='stage%d_unit%d' % (i + 1, j + 2),\n                                 bottle_neck=bottle_neck, num_group=num_group, bn_mom=bn_mom, workspace=workspace, memonger=memonger)\n\n    pool1 = mx.sym.Pooling(data=body, global_pool=True, kernel=(7, 7), pool_type='avg', name='pool1')\n    flat = mx.sym.Flatten(data=pool1)\n    fc1 = mx.sym.FullyConnected(data=flat, num_hidden=num_classes, name='fc1')\n    if dtype == 'float16':\n        fc1 = mx.sym.Cast(data=fc1, dtype=np.float32)\n    return mx.sym.SoftmaxOutput(data=fc1, name='softmax')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_symbol(num_classes, num_layers, image_shape, num_group=32, conv_workspace=256, dtype='float32', **kwargs):\n    image_shape = [int(l) for l in image_shape.split(',')]\n    (nchannel, height, width) = image_shape\n    if height <= 32:\n        num_stages = 3\n        if (num_layers-2) % 9 == 0 and num_layers >= 164:\n            per_unit = [(num_layers-2)//9]\n            filter_list = [16, 64, 128, 256]\n            bottle_neck = True\n        elif (num_layers-2) % 6 == 0 and num_layers < 164:\n            per_unit = [(num_layers-2)//6]\n            filter_list = [16, 16, 32, 64]\n            bottle_neck = False\n        else:\n            raise ValueError(\"no experiments done on num_layers {}, you can do it yourself\".format(num_layers))\n        units = per_unit * num_stages\n    else:\n        if num_layers >= 50:\n            filter_list = [64, 256, 512, 1024, 2048]\n            bottle_neck = True\n        else:\n            filter_list = [64, 64, 128, 256, 512]\n            bottle_neck = False\n        num_stages = 4\n        if num_layers == 18:\n            units = [2, 2, 2, 2]\n        elif num_layers == 34:\n            units = [3, 4, 6, 3]\n        elif num_layers == 50:\n            units = [3, 4, 6, 3]\n        elif num_layers == 101:\n            units = [3, 4, 23, 3]\n        elif num_layers == 152:\n            units = [3, 8, 36, 3]\n        elif num_layers == 200:\n            units = [3, 24, 36, 3]\n        elif num_layers == 269:\n            units = [3, 30, 48, 8]\n        else:\n            raise ValueError(\"no experiments done on num_layers {}, you can do it yourself\".format(num_layers))\n\n    return resnext(units      = units,\n                  num_stages  = num_stages,\n                  filter_list = filter_list,\n                  num_classes = num_classes,\n                  num_group   = num_group,\n                  image_shape = image_shape,\n                  bottle_neck = bottle_neck,\n                  workspace   = conv_workspace,\n                  dtype       = dtype)", "response": "This function returns a symbol from the image."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef var(name, attr=None, shape=None, lr_mult=None, wd_mult=None, dtype=None,\n        init=None, stype=None, **kwargs):\n    \"\"\"Creates a symbolic variable with specified name.\n\n    Example\n    -------\n    >>> data = mx.sym.Variable('data', attr={'a': 'b'})\n    >>> data\n    <Symbol data>\n    >>> csr_data = mx.sym.Variable('csr_data', stype='csr')\n    >>> csr_data\n    <Symbol csr_data>\n    >>> row_sparse_weight = mx.sym.Variable('weight', stype='row_sparse')\n    >>> row_sparse_weight\n    <Symbol weight>\n\n    Parameters\n    ----------\n    name : str\n        Variable name.\n    attr : Dict of strings\n        Additional attributes to set on the variable. Format {string : string}.\n    shape : tuple\n        The shape of a variable. If specified, this will be used during the shape inference.\n        If one has specified a different shape for this variable using\n        a keyword argument when calling shape inference, this shape information will be ignored.\n    lr_mult : float\n        The learning rate multiplier for input variable.\n    wd_mult : float\n        Weight decay multiplier for input variable.\n    dtype : str or numpy.dtype\n        The dtype for input variable. If not specified, this value will be inferred.\n    init : initializer (mxnet.init.*)\n        Initializer for this variable to (optionally) override the default initializer.\n    stype : str\n        The storage type of the variable, such as 'row_sparse', 'csr', 'default', etc\n    kwargs : Additional attribute variables\n        Additional attributes must start and end with double underscores.\n\n    Returns\n    -------\n    variable : Symbol\n        A symbol corresponding to an input to the computation graph.\n    \"\"\"\n    if not isinstance(name, string_types):\n        raise TypeError('Expect a string for variable `name`')\n    handle = SymbolHandle()\n    check_call(_LIB.MXSymbolCreateVariable(c_str(name), ctypes.byref(handle)))\n    ret = Symbol(handle)\n    if not hasattr(AttrScope._current, \"value\"):\n        AttrScope._current.value = AttrScope()\n    attr = AttrScope._current.value.get(attr)\n    attr = {} if attr is None else attr\n    if shape is not None:\n        attr['__shape__'] = str(shape)\n    if lr_mult is not None:\n        attr['__lr_mult__'] = str(lr_mult)\n    if wd_mult is not None:\n        attr['__wd_mult__'] = str(wd_mult)\n    if dtype is not None:\n        attr['__dtype__'] = str(_DTYPE_NP_TO_MX[_numpy.dtype(dtype).type])\n    if init is not None:\n        if not isinstance(init, string_types):\n            init = init.dumps()\n        attr['__init__'] = init\n    if stype is not None:\n        attr['__storage_type__'] = str(_STORAGE_TYPE_STR_TO_ID[stype])\n    for k, v in kwargs.items():\n        if k.startswith('__') and k.endswith('__'):\n            attr[k] = str(v)\n        else:\n            raise ValueError('Attribute name=%s is not supported.'\n                             ' Additional attributes must start and end with double underscores,'\n                             ' e.g, __yourattr__' % k)\n    ret._set_attr(**attr)\n    return ret", "response": "Returns a symbolic variable with the specified name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a new symbol that contains a collection of other symbols grouped together.", "response": "def Group(symbols):\n    \"\"\"Creates a symbol that contains a collection of other symbols, grouped together.\n\n    Example\n    -------\n    >>> a = mx.sym.Variable('a')\n    >>> b = mx.sym.Variable('b')\n    >>> mx.sym.Group([a,b])\n    <Symbol Grouped>\n\n    Parameters\n    ----------\n    symbols : list\n        List of symbols to be grouped.\n\n    Returns\n    -------\n    sym : Symbol\n        A group symbol.\n     \"\"\"\n    if not symbols or any(not isinstance(sym, Symbol) for sym in symbols):\n        raise TypeError('Expected a list of symbols as input')\n    handle = SymbolHandle()\n    check_call(_LIB.MXSymbolCreateGroup(\n        mx_uint(len(symbols)),\n        c_handle_array(symbols), ctypes.byref(handle)))\n    return Symbol(handle)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads a symbol from a JSON file.", "response": "def load(fname):\n    \"\"\"Loads symbol from a JSON file.\n\n    You can also use pickle to do the job if you only work on python.\n    The advantage of load/save is the file is language agnostic.\n    This means the file saved using save can be loaded by other language binding of mxnet.\n    You also get the benefit being able to directly load/save from cloud storage(S3, HDFS).\n\n    Parameters\n    ----------\n    fname : str\n        The name of the file, examples:\n\n        - `s3://my-bucket/path/my-s3-symbol`\n        - `hdfs://my-bucket/path/my-hdfs-symbol`\n        - `/path-to/my-local-symbol`\n\n    Returns\n    -------\n    sym : Symbol\n        The loaded symbol.\n\n    See Also\n    --------\n    Symbol.save : Used to save symbol into file.\n    \"\"\"\n    if not isinstance(fname, string_types):\n        raise TypeError('fname need to be string')\n    handle = SymbolHandle()\n    check_call(_LIB.MXSymbolCreateFromFile(c_str(fname), ctypes.byref(handle)))\n    return Symbol(handle)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload a single symbol from a json string.", "response": "def load_json(json_str):\n    \"\"\"Loads symbol from json string.\n\n    Parameters\n    ----------\n    json_str : str\n        A JSON string.\n\n    Returns\n    -------\n    sym : Symbol\n        The loaded symbol.\n\n    See Also\n    --------\n    Symbol.tojson : Used to save symbol into json string.\n    \"\"\"\n    if not isinstance(json_str, string_types):\n        raise TypeError('fname required to be string')\n    handle = SymbolHandle()\n    check_call(_LIB.MXSymbolCreateFromJSON(c_str(json_str), ctypes.byref(handle)))\n    return Symbol(handle)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn element - wise result of base element raised to powers from exp element.", "response": "def pow(base, exp):\n    \"\"\"Returns element-wise result of base element raised to powers from exp element.\n\n    Both inputs can be Symbol or scalar number.\n    Broadcasting is not supported. Use `broadcast_pow` instead.\n\n    `sym.pow` is being deprecated, please use `sym.power` instead.\n\n    Parameters\n    ---------\n    base : Symbol or scalar\n        The base symbol\n    exp : Symbol or scalar\n        The exponent symbol\n\n    Returns\n    -------\n    Symbol or scalar\n        The bases in x raised to the exponents in y.\n\n    Examples\n    --------\n    >>> mx.sym.pow(2, 3)\n    8\n    >>> x = mx.sym.Variable('x')\n    >>> y = mx.sym.Variable('y')\n    >>> z = mx.sym.pow(x, 2)\n    >>> z.eval(x=mx.nd.array([1,2]))[0].asnumpy()\n    array([ 1.,  4.], dtype=float32)\n    >>> z = mx.sym.pow(3, y)\n    >>> z.eval(y=mx.nd.array([2,3]))[0].asnumpy()\n    array([  9.,  27.], dtype=float32)\n    >>> z = mx.sym.pow(x, y)\n    >>> z.eval(x=mx.nd.array([3,4]), y=mx.nd.array([2,3]))[0].asnumpy()\n    array([  9.,  64.], dtype=float32)\n    \"\"\"\n    if isinstance(base, Symbol) and isinstance(exp, Symbol):\n        return _internal._Power(base, exp)\n    if isinstance(base, Symbol) and isinstance(exp, Number):\n        return _internal._PowerScalar(base, scalar=exp)\n    if isinstance(base, Number) and isinstance(exp, Symbol):\n        return _internal._RPowerScalar(exp, scalar=base)\n    if isinstance(base, Number) and isinstance(exp, Number):\n        return base**exp\n    else:\n        raise TypeError('types (%s, %s) not supported' % (str(type(base)), str(type(exp))))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef maximum(left, right):\n    if isinstance(left, Symbol) and isinstance(right, Symbol):\n        return _internal._Maximum(left, right)\n    if isinstance(left, Symbol) and isinstance(right, Number):\n        return _internal._MaximumScalar(left, scalar=right)\n    if isinstance(left, Number) and isinstance(right, Symbol):\n        return _internal._MaximumScalar(right, scalar=left)\n    if isinstance(left, Number) and isinstance(right, Number):\n        return left if left > right else right\n    else:\n        raise TypeError('types (%s, %s) not supported' % (str(type(left)), str(type(right))))", "response": "Returns element - wise maximum of the input elements."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns element - wise minimum of the input elements.", "response": "def minimum(left, right):\n    \"\"\"Returns element-wise minimum of the input elements.\n\n    Both inputs can be Symbol or scalar number. Broadcasting is not supported.\n\n    Parameters\n    ---------\n    left : Symbol or scalar\n        First symbol to be compared.\n    right : Symbol or scalar\n        Second symbol to be compared.\n\n    Returns\n    -------\n    Symbol or scalar\n        The element-wise minimum of the input symbols.\n\n    Examples\n    --------\n    >>> mx.sym.minimum(2, 3.5)\n    2\n    >>> x = mx.sym.Variable('x')\n    >>> y = mx.sym.Variable('y')\n    >>> z = mx.sym.minimum(x, 4)\n    >>> z.eval(x=mx.nd.array([3,5,2,10]))[0].asnumpy()\n    array([ 3.,  4.,  2.,  4.], dtype=float32)\n    >>> z = mx.sym.minimum(x, y)\n    >>> z.eval(x=mx.nd.array([3,4]), y=mx.nd.array([10,2]))[0].asnumpy()\n    array([ 3.,  2.], dtype=float32)\n    \"\"\"\n    if isinstance(left, Symbol) and isinstance(right, Symbol):\n        return _internal._Minimum(left, right)\n    if isinstance(left, Symbol) and isinstance(right, Number):\n        return _internal._MinimumScalar(left, scalar=right)\n    if isinstance(left, Number) and isinstance(right, Symbol):\n        return _internal._MinimumScalar(right, scalar=left)\n    if isinstance(left, Number) and isinstance(right, Number):\n        return left if left < right else right\n    else:\n        raise TypeError('types (%s, %s) not supported' % (str(type(left)), str(type(right))))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef hypot(left, right):\n    if isinstance(left, Symbol) and isinstance(right, Symbol):\n        return _internal._Hypot(left, right)\n    if isinstance(left, Symbol) and isinstance(right, Number):\n        return _internal._HypotScalar(left, scalar=right)\n    if isinstance(left, Number) and isinstance(right, Symbol):\n        return _internal._HypotScalar(right, scalar=left)\n    if isinstance(left, Number) and isinstance(right, Number):\n        return _numpy.hypot(left, right)\n    else:\n        raise TypeError('types (%s, %s) not supported' % (str(type(left)), str(type(right))))", "response": "Given the legs of a left triangle returns its hypotenuse."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a new 2 - D shpae filled with ones on the diagonal and zeros elsewhere.", "response": "def eye(N, M=0, k=0, dtype=None, **kwargs):\n    \"\"\"Returns a new symbol of 2-D shpae, filled with ones on the diagonal and zeros elsewhere.\n\n    Parameters\n    ----------\n    N: int\n        Number of rows in the output.\n    M: int, optional\n        Number of columns in the output. If 0, defaults to N.\n    k: int, optional\n        Index of the diagonal: 0 (the default) refers to the main diagonal,\n        a positive value refers to an upper diagonal,\n        and a negative value to a lower diagonal.\n    dtype : str or numpy.dtype, optional\n        The value type of the inner value, default to ``np.float32``.\n\n    Returns\n    -------\n    out : Symbol\n        The created Symbol.\n    \"\"\"\n    if dtype is None:\n        dtype = _numpy.float32\n    return _internal._eye(N, M, k, dtype=dtype, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a new symbol filled with zeros.", "response": "def zeros(shape, dtype=None, **kwargs):\n    \"\"\"Returns a new symbol of given shape and type, filled with zeros.\n\n    Parameters\n    ----------\n    shape :  int or sequence of ints\n        Shape of the new array.\n    dtype : str or numpy.dtype, optional\n        The value type of the inner value, default to ``np.float32``.\n\n    Returns\n    -------\n    out : Symbol\n        The created Symbol.\n    \"\"\"\n    if dtype is None:\n        dtype = _numpy.float32\n    return _internal._zeros(shape=shape, dtype=dtype, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a new array filled with ones.", "response": "def ones(shape, dtype=None, **kwargs):\n    \"\"\"Returns a new symbol of given shape and type, filled with ones.\n\n    Parameters\n    ----------\n    shape :  int or sequence of ints\n        Shape of the new array.\n    dtype : str or numpy.dtype, optional\n        The value type of the inner value, default to ``np.float32``.\n\n    Returns\n    -------\n    out : Symbol\n        The created Symbol\n    \"\"\"\n    if dtype is None:\n        dtype = _numpy.float32\n    return _internal._ones(shape=shape, dtype=dtype, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a new array filled with the given value val.", "response": "def full(shape, val, dtype=None, **kwargs):\n    \"\"\"Returns a new array of given shape and type, filled with the given value `val`.\n\n    Parameters\n    ----------\n    shape :  int or sequence of ints\n        Shape of the new array.\n    val : scalar\n        Fill value.\n    dtype : str or numpy.dtype, optional\n        The value type of the inner value, default to ``np.float32``.\n\n    Returns\n    -------\n    out : Symbol\n        The created Symbol\n    \"\"\"\n    if dtype is None:\n        dtype = _numpy.float32\n    return _internal._full(shape=shape, dtype=dtype, value=float(val), **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef arange(start, stop=None, step=1.0, repeat=1, infer_range=False, name=None, dtype=None):\n    if dtype is None:\n        dtype = _numpy.float32\n    return _internal._arange(start=start, stop=stop, step=step, repeat=repeat,\n                             infer_range=infer_range, name=name, dtype=dtype)", "response": "Returns evenly spaced values within a given interval."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute the histogram over the input data.", "response": "def histogram(a, bins=10, range=None, **kwargs):\n    \"\"\"Compute the histogram of the input data.\n\n    Parameters\n    ----------\n    a : NDArray\n        Input data. The histogram is computed over the flattened array.\n    bins : int or sequence of scalars\n        If bins is an int, it defines the number of equal-width bins in the\n        given range (10, by default). If bins is a sequence, it defines the bin edges,\n        including the rightmost edge, allowing for non-uniform bin widths.\n    range : (float, float), required if bins is an integer\n        The lower and upper range of the bins. If not provided, range is simply (a.min(), a.max()).\n        Values outside the range are ignored. The first element of the range must be less than or\n        equal to the second. range affects the automatic bin computation as well, the range will\n        be equally divided by the number of bins.\n\n    Returns\n    -------\n    out : Symbol\n        The created Symbol\n    \"\"\"\n    if isinstance(bins, Symbol):\n        return _internal._histogram(data=a, bins=bins, **kwargs)\n    elif isinstance(bins, integer_types):\n        if range is None:\n            raise ValueError(\"null range is not supported in symbol mode\")\n        return _internal._histogram(data=a, bin_cnt=bins, range=range, **kwargs)\n    raise ValueError(\"bins argument should be either an integer or an NDArray\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef split_v2(ary, indices_or_sections, axis=0, squeeze_axis=False):\n    indices = []\n    sections = 0\n    if isinstance(indices_or_sections, int):\n        sections = indices_or_sections\n    elif isinstance(indices_or_sections, tuple):\n        indices = [0] + list(indices_or_sections)\n    else:\n        raise ValueError('indices_or_sections must either int or tuple of ints')\n    return _internal._split_v2(ary, indices, axis, squeeze_axis, sections)", "response": "Splits an array into multiple sub - arrays along a given axis."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the name of the current symbol.", "response": "def name(self):\n        \"\"\"Gets name string from the symbol, this function only works for non-grouped symbol.\n\n        Returns\n        -------\n        value : str\n            The name of this symbol, returns ``None`` for grouped symbol.\n        \"\"\"\n        ret = ctypes.c_char_p()\n        success = ctypes.c_int()\n        check_call(_LIB.MXSymbolGetName(\n            self.handle, ctypes.byref(ret), ctypes.byref(success)))\n        if success.value != 0:\n            return py_str(ret.value)\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef attr(self, key):\n        ret = ctypes.c_char_p()\n        success = ctypes.c_int()\n        check_call(_LIB.MXSymbolGetAttr(\n            self.handle, c_str(key), ctypes.byref(ret), ctypes.byref(success)))\n        if success.value != 0:\n            return py_str(ret.value)\n        else:\n            return None", "response": "Returns the attribute string for corresponding input key from the symbol."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a dictionary mapping attribute names to values.", "response": "def list_attr(self, recursive=False):\n        \"\"\"Gets all attributes from the symbol.\n\n        Example\n        -------\n        >>> data = mx.sym.Variable('data', attr={'mood': 'angry'})\n        >>> data.list_attr()\n        {'mood': 'angry'}\n\n        Returns\n        -------\n        ret : Dict of str to str\n            A dictionary mapping attribute keys to values.\n        \"\"\"\n        if recursive:\n            raise DeprecationWarning(\"Symbol.list_attr with recursive=True has been deprecated. \"\n                                     \"Please use attr_dict instead.\")\n        size = mx_uint()\n        pairs = ctypes.POINTER(ctypes.c_char_p)()\n        f_handle = _LIB.MXSymbolListAttrShallow\n        check_call(f_handle(self.handle, ctypes.byref(size), ctypes.byref(pairs)))\n        return {py_str(pairs[i * 2]): py_str(pairs[i * 2 + 1]) for i in range(size.value)}"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef attr_dict(self):\n        size = mx_uint()\n        pairs = ctypes.POINTER(ctypes.c_char_p)()\n        f_handle = _LIB.MXSymbolListAttr\n        check_call(f_handle(self.handle, ctypes.byref(size), ctypes.byref(pairs)))\n        ret = {}\n        for i in range(size.value):\n            name, key = py_str(pairs[i * 2]).split('$')\n            val = py_str(pairs[i * 2 + 1])\n            if name not in ret:\n                ret[name] = {}\n            ret[name][key] = val\n        return ret", "response": "Recursively gets all attributes from the symbol and its children."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _set_attr(self, **kwargs):\n        for key, value in kwargs.items():\n            if not isinstance(value, string_types):\n                raise ValueError(\"Set Attr only accepts string values\")\n            check_call(_LIB.MXSymbolSetAttr(\n                self.handle, c_str(key), c_str(str(value))))", "response": "Sets an attribute of the symbol."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_internals(self):\n        handle = SymbolHandle()\n        check_call(_LIB.MXSymbolGetInternals(\n            self.handle, ctypes.byref(handle)))\n        return Symbol(handle=handle)", "response": "Gets a new grouped symbol group."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_children(self):\n        handle = SymbolHandle()\n        check_call(_LIB.MXSymbolGetChildren(\n            self.handle, ctypes.byref(handle)))\n        ret = Symbol(handle=handle)\n        if len(ret.list_outputs()) == 0:\n            return None\n        return ret", "response": "Gets a new grouped symbol whose output contains\n        inputs to output nodes of the original symbol."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef list_arguments(self):\n        size = ctypes.c_uint()\n        sarr = ctypes.POINTER(ctypes.c_char_p)()\n        check_call(_LIB.MXSymbolListArguments(\n            self.handle, ctypes.byref(size), ctypes.byref(sarr)))\n        return [py_str(sarr[i]) for i in range(size.value)]", "response": "Lists all the arguments in the symbol."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef list_outputs(self):\n        size = ctypes.c_uint()\n        sarr = ctypes.POINTER(ctypes.c_char_p)()\n        check_call(_LIB.MXSymbolListOutputs(\n            self.handle, ctypes.byref(size), ctypes.byref(sarr)))\n        return [py_str(sarr[i]) for i in range(size.value)]", "response": "Lists all the outputs in the symbol."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlists all the auxiliary states in the input symbol.", "response": "def list_auxiliary_states(self):\n        \"\"\"Lists all the auxiliary states in the symbol.\n\n        Example\n        -------\n        >>> a = mx.sym.var('a')\n        >>> b = mx.sym.var('b')\n        >>> c = a + b\n        >>> c.list_auxiliary_states()\n        []\n\n        Example of auxiliary states in `BatchNorm`.\n\n        >>> data = mx.symbol.Variable('data')\n        >>> weight = mx.sym.Variable(name='fc1_weight')\n        >>> fc1  = mx.symbol.FullyConnected(data = data, weight=weight, name='fc1', num_hidden=128)\n        >>> fc2 = mx.symbol.BatchNorm(fc1, name='batchnorm0')\n        >>> fc2.list_auxiliary_states()\n        ['batchnorm0_moving_mean', 'batchnorm0_moving_var']\n\n        Returns\n        -------\n        aux_states : list of str\n            List of the auxiliary states in input symbol.\n\n        Notes\n        -----\n        Auxiliary states are special states of symbols that do not correspond to an argument,\n        and are not updated by gradient descent. Common examples of auxiliary states\n        include the `moving_mean` and `moving_variance` in `BatchNorm`.\n        Most operators do not have auxiliary states.\n        \"\"\"\n        size = ctypes.c_uint()\n        sarr = ctypes.POINTER(ctypes.c_char_p)()\n        check_call(_LIB.MXSymbolListAuxiliaryStates(\n            self.handle, ctypes.byref(size), ctypes.byref(sarr)))\n        return [py_str(sarr[i]) for i in range(size.value)]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlist all arguments and auxiliary states of this Symbol.", "response": "def list_inputs(self):\n        \"\"\"Lists all arguments and auxiliary states of this Symbol.\n\n        Returns\n        -------\n        inputs : list of str\n            List of all inputs.\n\n        Examples\n        --------\n        >>> bn = mx.sym.BatchNorm(name='bn')\n        >>> bn.list_arguments()\n        ['bn_data', 'bn_gamma', 'bn_beta']\n        >>> bn.list_auxiliary_states()\n        ['bn_moving_mean', 'bn_moving_var']\n        >>> bn.list_inputs()\n        ['bn_data', 'bn_gamma', 'bn_beta', 'bn_moving_mean', 'bn_moving_var']\n        \"\"\"\n        size = ctypes.c_uint()\n        sarr = ctypes.POINTER(ctypes.c_char_p)()\n        check_call(_LIB.NNSymbolListInputNames(\n            self.handle, 0, ctypes.byref(size), ctypes.byref(sarr)))\n        return [py_str(sarr[i]) for i in range(size.value)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninfers the type of all arguments and all outputs given the known types of some arguments.", "response": "def infer_type(self, *args, **kwargs):\n        \"\"\"Infers the type of all arguments and all outputs, given the known types\n        for some arguments.\n\n        This function takes the known types of some arguments in either positional way\n        or keyword argument way as input. It returns a tuple of `None` values\n        if there is not enough information to deduce the missing types.\n\n        Inconsistencies in the known types will cause an error to be raised.\n\n        Example\n        -------\n        >>> a = mx.sym.var('a')\n        >>> b = mx.sym.var('b')\n        >>> c = a + b\n        >>> arg_types, out_types, aux_types = c.infer_type(a='float32')\n        >>> arg_types\n        [<type 'numpy.float32'>, <type 'numpy.float32'>]\n        >>> out_types\n        [<type 'numpy.float32'>]\n        >>> aux_types\n        []\n\n        Parameters\n        ----------\n        *args :\n            Type of known arguments in a positional way.\n            Unknown type can be marked as None.\n\n        **kwargs :\n            Keyword arguments of known types.\n\n        Returns\n        -------\n        arg_types : list of numpy.dtype or None\n            List of argument types.\n            The order is same as the order of list_arguments().\n        out_types : list of numpy.dtype or None\n            List of output types.\n            The order is same as the order of list_outputs().\n        aux_types : list of numpy.dtype or None\n            List of auxiliary state types.\n            The order is same as the order of list_auxiliary_states().\n        \"\"\"\n        try:\n            res = self._infer_type_impl(False, *args, **kwargs)\n            if res[1] is None:\n                arg_shapes, _, _ = self._infer_type_impl(True, *args, **kwargs)\n                arg_names = self.list_arguments()\n                unknowns = []\n                for name, dtype in zip(arg_names, arg_shapes):\n                    if not dtype:\n                        if len(unknowns) >= 10:\n                            unknowns.append('...')\n                            break\n                        unknowns.append('%s: %s' % (name, str(dtype)))\n                warnings.warn(\n                    \"Cannot decide type for the following arguments. \" +\n                    \"Consider providing them as input:\\n\\t\" +\n                    \"\\n\\t\".join(unknowns), stacklevel=2)\n            return res\n        except MXNetError:\n            print(\"infer_type error. Arguments:\")\n            for i, arg in enumerate(args):\n                print(\"  #%d: %s\" % (i, arg))\n            for k, v in kwargs.items():\n                print(\"  %s: %s\" % (k, v))\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninferring the shape of all arguments and all outputs given the known shapes of some arguments.", "response": "def infer_shape(self, *args, **kwargs):\n        \"\"\"Infers the shapes of all arguments and all outputs given the known shapes of\n        some arguments.\n\n        This function takes the known shapes of some arguments in either positional way\n        or keyword argument way as input. It returns a tuple of `None` values\n        if there is not enough information to deduce the missing shapes.\n\n        Example\n        -------\n        >>> a = mx.sym.var('a')\n        >>> b = mx.sym.var('b')\n        >>> c = a + b\n        >>> arg_shapes, out_shapes, aux_shapes = c.infer_shape(a=(3,3))\n        >>> arg_shapes\n        [(3L, 3L), (3L, 3L)]\n        >>> out_shapes\n        [(3L, 3L)]\n        >>> aux_shapes\n        []\n        >>> c.infer_shape(a=(0,3)) # 0s in shape means unknown dimensions. So, returns None.\n        (None, None, None)\n\n        Inconsistencies in the known shapes will cause an error to be raised.\n        See the following example:\n\n        >>> data = mx.sym.Variable('data')\n        >>> out = mx.sym.FullyConnected(data=data, name='fc1', num_hidden=1000)\n        >>> out = mx.sym.Activation(data=out, act_type='relu')\n        >>> out = mx.sym.FullyConnected(data=out, name='fc2', num_hidden=10)\n        >>> weight_shape= (1, 100)\n        >>> data_shape = (100, 100)\n        >>> out.infer_shape(data=data_shape, fc1_weight=weight_shape)\n        Error in operator fc1: Shape inconsistent, Provided=(1,100), inferred shape=(1000,100)\n\n        Parameters\n        ----------\n        *args :\n            Shape of arguments in a positional way.\n            Unknown shape can be marked as None.\n\n        **kwargs :\n            Keyword arguments of the known shapes.\n\n        Returns\n        -------\n        arg_shapes : list of tuple or None\n            List of argument shapes.\n            The order is same as the order of list_arguments().\n        out_shapes : list of tuple or None\n            List of output shapes.\n            The order is same as the order of list_outputs().\n        aux_shapes : list of tuple or None\n            List of auxiliary state shapes.\n            The order is same as the order of list_auxiliary_states().\n        \"\"\"\n        try:\n            res = self._infer_shape_impl(False, *args, **kwargs)\n            if res[1] is None:\n                arg_shapes, _, _ = self._infer_shape_impl(True, *args, **kwargs)\n                arg_names = self.list_arguments()\n                unknowns = []\n                for name, shape in zip(arg_names, arg_shapes):\n                    if is_np_compat():\n                        shape_is_none = not shape or -1 in shape\n                    else:\n                        shape_is_none = not shape or 0 in shape\n                    if shape_is_none:\n                        if len(unknowns) >= 10:\n                            unknowns.append('...')\n                            break\n                        unknowns.append('%s: %s' % (name, str(shape)))\n                warnings.warn(\n                    \"Cannot decide shape for the following arguments \" +\n                    \"(0s in shape means unknown dimensions). \" +\n                    \"Consider providing them as input:\\n\\t\" +\n                    \"\\n\\t\".join(unknowns), stacklevel=2)\n            return res\n        except MXNetError:\n            print(\"infer_shape error. Arguments:\")\n            for i, arg in enumerate(args):\n                print(\"  #%d: %s\" % (i, arg))\n            for k, v in kwargs.items():\n                print(\"  %s: %s\" % (k, v))\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _infer_shape_impl(self, partial, *args, **kwargs):\n        # pylint: disable=too-many-locals\n        if len(args) != 0 and len(kwargs) != 0:\n            raise ValueError('Can only specify known argument \\\n                    shapes either by positional or kwargs way.')\n        sdata = []\n        indptr = [0]\n        if len(args) != 0:\n            keys = c_array(ctypes.c_char_p, [])\n            for i, s in enumerate(args):\n                if s is not None:\n                    if not isinstance(s, tuple):\n                        raise TypeError(\"Arguments need to be shapes (tuple), \"\n                                        \"but argument %d is %s.\" % (i, type(s)))\n                    sdata.extend(s)\n                indptr.append(len(sdata))\n        else:\n            str_keys = []\n            for k, v in kwargs.items():\n                if not isinstance(v, tuple):\n                    raise TypeError(\"Arguments need to be shapes (tuple), \"\n                                    \"but '%s' is %s.\" % (k, type(v)))\n                str_keys.append(k)\n                sdata.extend(v)\n                indptr.append(len(sdata))\n            keys = c_str_array(str_keys)\n        arg_shape_size = mx_uint()\n        arg_shape_ndim = ctypes.POINTER(mx_int)()\n        arg_shape_data = ctypes.POINTER(ctypes.POINTER(mx_int))()\n        out_shape_size = mx_uint()\n        out_shape_ndim = ctypes.POINTER(mx_int)()\n        out_shape_data = ctypes.POINTER(ctypes.POINTER(mx_int))()\n        aux_shape_size = mx_uint()\n        aux_shape_ndim = ctypes.POINTER(mx_int)()\n        aux_shape_data = ctypes.POINTER(ctypes.POINTER(mx_int))()\n        complete = ctypes.c_int()\n        if partial:\n            infer_func = _LIB.MXSymbolInferShapePartialEx\n        else:\n            infer_func = _LIB.MXSymbolInferShapeEx\n        check_call(infer_func(\n            self.handle,\n            mx_uint(len(indptr) - 1),\n            keys,\n            c_array_buf(mx_uint, array('I', indptr)),\n            c_array_buf(mx_int, array('i', sdata)),\n            ctypes.byref(arg_shape_size),\n            ctypes.byref(arg_shape_ndim),\n            ctypes.byref(arg_shape_data),\n            ctypes.byref(out_shape_size),\n            ctypes.byref(out_shape_ndim),\n            ctypes.byref(out_shape_data),\n            ctypes.byref(aux_shape_size),\n            ctypes.byref(aux_shape_ndim),\n            ctypes.byref(aux_shape_data),\n            ctypes.byref(complete)))\n        if complete.value != 0:\n            arg_shapes = [tuple(arg_shape_data[i][:arg_shape_ndim[i]])\n                          if arg_shape_ndim[i] >= 0 else None\n                          for i in range(arg_shape_size.value)]\n            out_shapes = [tuple(out_shape_data[i][:out_shape_ndim[i]])\n                          if out_shape_ndim[i] >= 0 else None\n                          for i in range(out_shape_size.value)]\n            aux_shapes = [tuple(aux_shape_data[i][:aux_shape_ndim[i]])\n                          if aux_shape_ndim[i] >= 0 else None\n                          for i in range(aux_shape_size.value)]\n            return (arg_shapes, out_shapes, aux_shapes)\n        else:\n            return (None, None, None)", "response": "The actual implementation for calling shape inference API."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef save(self, fname):\n        if not isinstance(fname, string_types):\n            raise TypeError('fname need to be string')\n        check_call(_LIB.MXSymbolSaveToFile(self.handle, c_str(fname)))", "response": "Saves the current state of the symbol to a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef tojson(self):\n        json_str = ctypes.c_char_p()\n        check_call(_LIB.MXSymbolSaveToJSON(self.handle, ctypes.byref(json_str)))\n        return py_str(json_str.value)", "response": "Saves symbol to a JSON string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_ndarray_inputs(arg_key, args, arg_names, allow_missing):\n        # setup args\n        arg_handles = []\n        arg_arrays = []\n        if isinstance(args, list):\n            if len(args) != len(arg_names):\n                raise ValueError('Length of %s does not match the number of arguments' % arg_key)\n            for narr in args:\n                if narr is None and allow_missing:\n                    arg_handles.append(None)\n                elif not isinstance(narr, NDArray):\n                    raise TypeError('Only accept list of NDArrays or dict of str to NDArray')\n                else:\n                    arg_handles.append(narr.handle)\n            arg_arrays = args\n        elif isinstance(args, dict):\n            for name in arg_names:\n                if name in args:\n                    narr = args[name]\n                    if not isinstance(narr, NDArray):\n                        raise TypeError('Only accept list of NDArrays or dict of str to NDArray')\n                    arg_handles.append(narr.handle)\n                    arg_arrays.append(narr)\n                else:\n                    if allow_missing:\n                        arg_handles.append(None)\n                        arg_arrays.append(None)\n                    else:\n                        raise ValueError('key `%s` is missing in `%s`' % (name, arg_key))\n        else:\n            raise TypeError('Only accept list of NDArrays or dict of str to NDArray')\n        return c_array(NDArrayHandle, arg_handles), arg_arrays", "response": "Helper function to get NDArrays handles from various inputs."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbinds the current symbol to an executor.", "response": "def simple_bind(self, ctx, grad_req='write', type_dict=None, stype_dict=None,\n                    group2ctx=None, shared_arg_names=None, shared_exec=None,\n                    shared_buffer=None, **kwargs):\n        \"\"\"Bind current symbol to get an executor, allocate all the arguments needed.\n        Allows specifying data types.\n\n        This function simplifies the binding procedure. You need to specify only input data shapes.\n        Before binding the executor, the function allocates arguments and auxiliary states\n        that were not explicitly specified. Allows specifying data types.\n\n        Example\n        -------\n        >>> x = mx.sym.Variable('x')\n        >>> y = mx.sym.FullyConnected(x, num_hidden=4)\n        >>> exe = y.simple_bind(mx.cpu(), x=(5,4), grad_req='null')\n        >>> exe.forward()\n        [<NDArray 5x4 @cpu(0)>]\n        >>> exe.outputs[0].asnumpy()\n        array([[ 0.,  0.,  0.,  0.],\n               [ 0.,  0.,  0.,  0.],\n               [ 0.,  0.,  0.,  0.],\n               [ 0.,  0.,  0.,  0.],\n               [ 0.,  0.,  0.,  0.]], dtype=float32)\n        >>> exe.arg_arrays\n        [<NDArray 5x4 @cpu(0)>, <NDArray 4x4 @cpu(0)>, <NDArray 4 @cpu(0)>]\n        >>> exe.grad_arrays\n        [<NDArray 5x4 @cpu(0)>, <NDArray 4x4 @cpu(0)>, <NDArray 4 @cpu(0)>]\n\n        Parameters\n        ----------\n        ctx : Context\n            The device context the generated executor to run on.\n\n        grad_req: string\n            {'write', 'add', 'null'}, or list of str or dict of str to str, optional\n            To specify how we should update the gradient to the `args_grad`.\n\n            - 'write' means every time gradient is written to specified `args_grad` NDArray.\n            - 'add' means every time gradient is added to the specified NDArray.\n            - 'null' means no action is taken, the gradient may not be calculated.\n\n        type_dict  : Dict of str->numpy.dtype\n            Input type dictionary, name->dtype\n\n        stype_dict  : Dict of str->str\n            Input storage type dictionary, name->storage_type\n\n        group2ctx : Dict of string to mx.Context\n            The dict mapping the `ctx_group` attribute to the context assignment.\n\n        shared_arg_names : List of string\n            The argument names whose `NDArray` of shared_exec can be reused for initializing\n            the current executor.\n\n        shared_exec : Executor\n            The executor whose arg_arrays, arg_arrays, grad_arrays, and aux_arrays can be\n            reused for initializing the current executor.\n\n        shared_buffer : Dict of string to `NDArray`\n            The dict mapping argument names to the `NDArray` that can be reused for initializing\n            the current executor. This buffer will be checked for reuse if one argument name\n            of the current executor is not found in `shared_arg_names`. The `NDArray` s are\n            expected have default storage type.\n\n        kwargs : Dict of str->shape\n            Input shape dictionary, name->shape\n\n        Returns\n        -------\n        executor : mxnet.Executor\n            The generated executor\n        \"\"\"\n        # data types\n        num_provided_arg_types = 0\n        provided_arg_type_names = ctypes.POINTER(ctypes.c_char_p)()  # provided type argument names\n        provided_arg_type_data = ctypes.POINTER(mx_uint)()  # provided types\n        if type_dict is not None:\n            provided_arg_type_names = []\n            provided_arg_type_data = []\n            for k, v in type_dict.items():\n                v = _numpy.dtype(v).type\n                if v in _DTYPE_NP_TO_MX:\n                    provided_arg_type_names.append(k)\n                    provided_arg_type_data.append(_DTYPE_NP_TO_MX[v])\n            num_provided_arg_types = mx_uint(len(provided_arg_type_names))\n            provided_arg_type_names = c_str_array(provided_arg_type_names)\n            provided_arg_type_data = c_array_buf(ctypes.c_int, array('i', provided_arg_type_data))\n\n        # storage types\n        num_provided_arg_stypes = 0\n        # provided storage type argument names\n        provided_arg_stype_names = ctypes.POINTER(ctypes.c_char_p)()\n        provided_arg_stype_data = ctypes.POINTER(mx_uint)()  # provided storage types\n        if stype_dict is not None:\n            provided_arg_stype_names = []\n            provided_arg_stype_data = []\n            for k, v in stype_dict.items():\n                if v in _STORAGE_TYPE_STR_TO_ID:\n                    provided_arg_stype_names.append(k)\n                    provided_arg_stype_data.append(_STORAGE_TYPE_STR_TO_ID[v])\n            num_provided_arg_stypes = mx_uint(len(provided_arg_stype_names))\n            provided_arg_stype_names = c_str_array(provided_arg_stype_names)\n            provided_arg_stype_data = c_array_buf(ctypes.c_int, array('i', provided_arg_stype_data))\n\n        provided_arg_shape_data = []  # shape data\n        # argument shape index in sdata,\n        # e.g. [sdata[indptr[0]], sdata[indptr[1]]) is the shape of the first arg\n        provided_arg_shape_idx = [0]\n        provided_arg_shape_names = []  # provided argument names\n        for k, v in kwargs.items():\n            # if k not in listed_arguments and k not in listed_aux_states:\n            #   raise ValueError('arg name %s is not valid', k)\n            if isinstance(v, tuple):\n                provided_arg_shape_names.append(k)\n                provided_arg_shape_data.extend(v)\n                provided_arg_shape_idx.append(len(provided_arg_shape_data))\n\n        provided_req_type_list_len = 0\n        provided_grad_req_types = ctypes.POINTER(ctypes.c_char_p)()\n        provided_grad_req_names = ctypes.POINTER(ctypes.c_char_p)()\n        if grad_req is not None:\n            if isinstance(grad_req, string_types):\n                # use provided_req_type_list_len = 0 to indicate this situation\n                provided_req_type_list_len = 0\n                provided_grad_req_types = [grad_req]\n            elif isinstance(grad_req, list):\n                if len(grad_req) == 0:\n                    raise RuntimeError('grad_req in simple_bind cannot be an empty list')\n                provided_grad_req_types = grad_req\n                provided_req_type_list_len = len(provided_grad_req_types)\n            elif isinstance(grad_req, dict):\n                if len(grad_req) == 0:\n                    raise RuntimeError('grad_req in simple_bind cannot be an empty dict')\n                provided_grad_req_names = []\n                provided_grad_req_types = []\n                for k, v in grad_req.items():\n                    provided_grad_req_names.append(k)\n                    provided_grad_req_types.append(v)\n                provided_grad_req_names = c_str_array(provided_grad_req_names)\n                provided_req_type_list_len = len(provided_grad_req_types)\n            provided_grad_req_types = c_str_array(provided_grad_req_types)\n\n        num_ctx_map_keys = mx_uint(0)\n        ctx_map_keys = ctypes.POINTER(ctypes.c_char_p)()\n        ctx_map_dev_types = ctypes.POINTER(ctypes.c_int)()\n        ctx_map_dev_ids = ctypes.POINTER(ctypes.c_int)()\n        if group2ctx is not None:\n            ctx_map_keys = []\n            ctx_map_dev_types = []\n            ctx_map_dev_ids = []\n            for key, val in group2ctx.items():\n                ctx_map_keys.append(key)\n                ctx_map_dev_types.append(val.device_typeid)\n                ctx_map_dev_ids.append(val.device_id)\n            num_ctx_map_keys = mx_uint(len(ctx_map_keys))\n            ctx_map_keys = c_str_array(ctx_map_keys)\n            ctx_map_dev_types = c_array(ctypes.c_int, array('i', ctx_map_dev_types))\n            ctx_map_dev_ids = c_array(ctypes.c_int, array('i', ctx_map_dev_ids))\n\n        # prepare param names\n        shared_arg_name_list = []\n        if shared_arg_names is not None:\n            if not isinstance(shared_arg_names, list):\n                raise ValueError('shared_arg_names in simple_bind must be a list or None')\n            shared_arg_name_list = shared_arg_names\n\n        # prepare shared_buffer\n        if shared_buffer is None:\n            shared_buffer_len = ctypes.c_int(-1)\n            shared_buffer_names = ctypes.POINTER(ctypes.c_char_p)()\n            shared_buffer_handles = ctypes.POINTER(NDArrayHandle)()\n        else:\n            if not isinstance(shared_buffer, dict):\n                raise ValueError('shared_buffer in simple_bind must be dict or None')\n            buffer_names = shared_buffer.keys()\n            buffer_arrays = shared_buffer.values()\n            for v in buffer_arrays:\n                assert(v.stype == 'default'), \\\n                    \"shared_buffer is expected to only contain NDArrays with default storage\"\n            shared_buffer_names = c_str_array(buffer_names)\n            shared_buffer_len = ctypes.c_int(len(buffer_arrays))\n            shared_buffer_handles = c_handle_array(buffer_arrays)\n        updated_shared_buffer_names = ctypes.POINTER(ctypes.c_char_p)()\n        updated_shared_buffer_handles = ctypes.POINTER(NDArrayHandle)()\n\n        # prepare shared_exec_handle\n        shared_exec_handle = shared_exec.handle if shared_exec is not None else ExecutorHandle()\n\n        # prepare current executor handle\n        exe_handle = ExecutorHandle()\n\n        # prepare current executor's in_args, arg_grads, and aux_states\n        num_in_args = ctypes.c_uint()\n        in_arg_handles = ctypes.POINTER(NDArrayHandle)()\n        arg_grad_handles = ctypes.POINTER(NDArrayHandle)()\n        num_aux_states = ctypes.c_uint()\n        aux_state_handles = ctypes.POINTER(NDArrayHandle)()\n\n        try:\n            check_call(_LIB.MXExecutorSimpleBindEx(self.handle,\n                                                   ctypes.c_int(ctx.device_typeid),\n                                                   ctypes.c_int(ctx.device_id),\n                                                   num_ctx_map_keys,\n                                                   ctx_map_keys,\n                                                   ctx_map_dev_types,\n                                                   ctx_map_dev_ids,\n                                                   mx_uint(provided_req_type_list_len),\n                                                   provided_grad_req_names,\n                                                   provided_grad_req_types,\n                                                   mx_uint(len(provided_arg_shape_names)),\n                                                   c_str_array(provided_arg_shape_names),\n                                                   c_array_buf(mx_int,\n                                                               array('I', provided_arg_shape_data)),\n                                                   c_array_buf(mx_uint,\n                                                               array('i', provided_arg_shape_idx)),\n                                                   num_provided_arg_types,\n                                                   provided_arg_type_names,\n                                                   provided_arg_type_data,\n                                                   num_provided_arg_stypes,\n                                                   provided_arg_stype_names,\n                                                   provided_arg_stype_data,\n                                                   mx_uint(len(shared_arg_name_list)),\n                                                   c_str_array(shared_arg_name_list),\n                                                   ctypes.byref(shared_buffer_len),\n                                                   shared_buffer_names,\n                                                   shared_buffer_handles,\n                                                   ctypes.byref(updated_shared_buffer_names),\n                                                   ctypes.byref(updated_shared_buffer_handles),\n                                                   ctypes.byref(num_in_args),\n                                                   ctypes.byref(in_arg_handles),\n                                                   ctypes.byref(arg_grad_handles),\n                                                   ctypes.byref(num_aux_states),\n                                                   ctypes.byref(aux_state_handles),\n                                                   shared_exec_handle,\n                                                   ctypes.byref(exe_handle)))\n        except MXNetError as e:\n            error_msg = \"simple_bind error. Arguments:\\n\"\n            for k, v in kwargs.items():\n                error_msg += \"%s: %s\\n\" % (k, v)\n            error_msg += \"%s\" % e\n            raise RuntimeError(error_msg)\n\n        # update shared_buffer\n        if shared_buffer is not None:\n            for i in range(shared_buffer_len.value):\n                k = py_str(updated_shared_buffer_names[i])\n                v = NDArray(NDArrayHandle(updated_shared_buffer_handles[i]))\n                shared_buffer[k] = v\n\n        # create in_args, arg_grads, and aux_states for the current executor\n        arg_arrays = [_ndarray_cls(NDArrayHandle(in_arg_handles[i]))\n                      for i in range(num_in_args.value)]\n        grad_arrays = [_ndarray_cls(NDArrayHandle(arg_grad_handles[i]))\n                       if arg_grad_handles[i] is not None\n                       else None for i in range(num_in_args.value)]\n        aux_arrays = [_ndarray_cls(NDArrayHandle(aux_state_handles[i]))\n                      for i in range(num_aux_states.value)]\n\n        executor = Executor(exe_handle, self, ctx, grad_req, group2ctx)\n        executor.arg_arrays = arg_arrays\n        executor.grad_arrays = grad_arrays\n        executor.aux_arrays = aux_arrays\n        return executor"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef bind(self, ctx, args, args_grad=None, grad_req='write',\n             aux_states=None, group2ctx=None, shared_exec=None):\n        \"\"\"Binds the current symbol to an executor and returns it.\n\n        We first declare the computation and then bind to the data to run.\n        This function returns an executor which provides method `forward()` method for evaluation\n        and a `outputs()` method to get all the results.\n\n        Example\n        -------\n        >>> a = mx.sym.Variable('a')\n        >>> b = mx.sym.Variable('b')\n        >>> c = a + b\n        <Symbol _plus1>\n        >>> ex = c.bind(ctx=mx.cpu(), args={'a' : mx.nd.ones([2,3]), 'b' : mx.nd.ones([2,3])})\n        >>> ex.forward()\n        [<NDArray 2x3 @cpu(0)>]\n        >>> ex.outputs[0].asnumpy()\n        [[ 2.  2.  2.]\n        [ 2.  2.  2.]]\n\n        Parameters\n        ----------\n        ctx : Context\n            The device context the generated executor to run on.\n\n        args : list of NDArray or dict of str to NDArray\n            Input arguments to the symbol.\n\n            - If the input type is a list of `NDArray`, the order should be same as the order\n              of `list_arguments()`.\n            - If the input type is a dict of str to `NDArray`, then it maps the name of arguments\n              to the corresponding `NDArray`.\n            - In either case, all the arguments must be provided.\n\n        args_grad : list of NDArray or dict of str to `NDArray`, optional\n            When specified, `args_grad` provides NDArrays to hold\n            the result of gradient value in backward.\n\n            - If the input type is a list of `NDArray`, the order should be same as the order\n              of `list_arguments()`.\n            - If the input type is a dict of str to `NDArray`, then it maps the name of arguments\n              to the corresponding NDArray.\n            - When the type is a dict of str to `NDArray`, one only need to provide the dict\n              for required argument gradient.\n              Only the specified argument gradient will be calculated.\n\n        grad_req : {'write', 'add', 'null'}, or list of str or dict of str to str, optional\n            To specify how we should update the gradient to the `args_grad`.\n\n            - 'write' means everytime gradient is write to specified `args_grad` `NDArray`.\n            - 'add' means everytime gradient is add to the specified NDArray.\n            - 'null' means no action is taken, the gradient may not be calculated.\n\n        aux_states : list of `NDArray`, or dict of str to `NDArray`, optional\n            Input auxiliary states to the symbol, only needed when the output of\n            `list_auxiliary_states()` is not empty.\n\n            - If the input type is a list of `NDArray`, the order should be same as the order\n              of `list_auxiliary_states()`.\n            - If the input type is a dict of str to `NDArray`, then it maps the name of\n              `auxiliary_states` to the corresponding `NDArray`,\n            - In either case, all the auxiliary states need to be provided.\n\n        group2ctx : Dict of string to mx.Context\n            The dict mapping the `ctx_group` attribute to the context assignment.\n\n        shared_exec : mx.executor.Executor\n            Executor to share memory with. This is intended for runtime reshaping, variable length\n            sequences, etc. The returned executor shares state with `shared_exec`, and should not be\n            used in parallel with it.\n\n        Returns\n        -------\n        executor : Executor\n            The generated executor\n\n        Notes\n        -----\n        Auxiliary states are the special states of symbols that do not correspond\n        to an argument, and do not have gradient but are still useful\n        for the specific operations. Common examples of auxiliary states include\n        the `moving_mean` and `moving_variance` states in `BatchNorm`.\n        Most operators do not have auxiliary states and in those cases,\n        this parameter can be safely ignored.\n\n        One can give up gradient by using a dict in `args_grad` and only specify\n        gradient they interested in.\n        \"\"\"\n        # pylint: disable=too-many-locals, too-many-branches\n        if not isinstance(ctx, Context):\n            raise TypeError(\"Context type error\")\n\n        listed_arguments = self.list_arguments()\n        args_handle, args = self._get_ndarray_inputs('args', args, listed_arguments, False)\n        # setup args gradient\n        if args_grad is None:\n            args_grad_handle = c_array(NDArrayHandle, [None] * len(args))\n        else:\n            args_grad_handle, args_grad = self._get_ndarray_inputs(\n                'args_grad', args_grad, listed_arguments, True)\n\n        if aux_states is None:\n            aux_states = []\n        aux_args_handle, aux_states = self._get_ndarray_inputs(\n            'aux_states', aux_states, self.list_auxiliary_states(), False)\n\n        # setup requirements\n        if isinstance(grad_req, string_types):\n            if grad_req not in _GRAD_REQ_MAP:\n                raise ValueError('grad_req must be in %s' % str(_GRAD_REQ_MAP))\n            reqs_array = c_array_buf(mx_uint,\n                                     array('I', [_GRAD_REQ_MAP[grad_req]] * len(listed_arguments)))\n        elif isinstance(grad_req, list):\n            reqs_array = c_array_buf(mx_uint,\n                                     array('I', [_GRAD_REQ_MAP[item] for item in grad_req]))\n        elif isinstance(grad_req, dict):\n            req_array = []\n            for name in listed_arguments:\n                if name in grad_req:\n                    req_array.append(_GRAD_REQ_MAP[grad_req[name]])\n                else:\n                    req_array.append(0)\n            reqs_array = c_array_buf(mx_uint, array('I', req_array))\n\n        ctx_map_keys = []\n        ctx_map_dev_types = []\n        ctx_map_dev_ids = []\n\n        if group2ctx:\n            for key, val in group2ctx.items():\n                ctx_map_keys.append(key)\n                ctx_map_dev_types.append(val.device_typeid)\n                ctx_map_dev_ids.append(val.device_id)\n\n        handle = ExecutorHandle()\n        shared_handle = shared_exec.handle if shared_exec is not None else ExecutorHandle()\n        check_call(_LIB.MXExecutorBindEX(self.handle,\n                                         ctypes.c_int(ctx.device_typeid),\n                                         ctypes.c_int(ctx.device_id),\n                                         mx_uint(len(ctx_map_keys)),\n                                         c_str_array(ctx_map_keys),\n                                         c_array_buf(ctypes.c_int, array('i', ctx_map_dev_types)),\n                                         c_array_buf(ctypes.c_int, array('i', ctx_map_dev_ids)),\n                                         mx_uint(len(args)),\n                                         args_handle,\n                                         args_grad_handle,\n                                         reqs_array,\n                                         mx_uint(len(aux_states)),\n                                         aux_args_handle,\n                                         shared_handle,\n                                         ctypes.byref(handle)))\n        executor = Executor(handle, self, ctx, grad_req, group2ctx)\n        executor.arg_arrays = args\n        executor.grad_arrays = args_grad\n        executor.aux_arrays = aux_states\n        return executor", "response": "Binds the current symbol to an executor and returns it."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the gradient of the current symbol.", "response": "def gradient(self, wrt):\n        \"\"\"Gets the autodiff of current symbol.\n\n        This function can only be used if current symbol is a loss function.\n\n        .. note:: This function is currently not implemented.\n\n        Parameters\n        ----------\n        wrt : Array of String\n            keyword arguments of the symbol that the gradients are taken.\n\n        Returns\n        -------\n        grad : Symbol\n            A gradient Symbol with returns to be the corresponding gradients.\n        \"\"\"\n        handle = SymbolHandle()\n        c_wrt = c_str_array(wrt)\n        check_call(_LIB.MXSymbolGrad(self.handle,\n                                     mx_uint(len(wrt)),\n                                     c_wrt,\n                                     ctypes.byref(handle)))\n        return Symbol(handle)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef eval(self, ctx=None, **kwargs):\n        if ctx is None:\n            ctx = current_context()\n        return self.bind(ctx, kwargs).forward()", "response": "Evaluates a symbol given arguments."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a new Symbol for the specified backend.", "response": "def get_backend_symbol(self, backend):\n        \"\"\"Return symbol for target backend.\n\n        Parameters\n        ----------\n        backend : str\n            The backend names.\n\n        Returns\n        -------\n        out : Symbol\n            The created Symbol for target backend.\n        \"\"\"\n        out = SymbolHandle()\n        check_call(_LIB.MXGenBackendSubgraph(self.handle, c_str(backend), ctypes.byref(out)))\n        return Symbol(out)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef hybrid_forward(self, F, x):\n        f = self._factor\n                                             # (N, C*f, W)\n        x = F.reshape(x, (0, -4, -1, f, 0))  # (N, C, f, W)\n        x = F.transpose(x, (0, 1, 3, 2))     # (N, C, W, f)\n        x = F.reshape(x, (0, 0, -3))         # (N, C, W*f)\n        return x", "response": "Perform pixel - shuffling on the input."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef hybrid_forward(self, F, x):\n        f1, f2 = self._factors\n                                                      # (N, f1*f2*C, H, W)\n        x = F.reshape(x, (0, -4, -1, f1 * f2, 0, 0))  # (N, C, f1*f2, H, W)\n        x = F.reshape(x, (0, 0, -4, f1, f2, 0, 0))    # (N, C, f1, f2, H, W)\n        x = F.transpose(x, (0, 1, 4, 2, 5, 3))        # (N, C, H, f1, W, f2)\n        x = F.reshape(x, (0, 0, -3, -3))              # (N, C, H*f1, W*f2)\n        return x", "response": "Perform pixel - shuffling on the input."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nperforming pixel - shuffling on the input.", "response": "def hybrid_forward(self, F, x):\n        \"\"\"Perform pixel-shuffling on the input.\"\"\"\n        # `transpose` doesn't support 8D, need other implementation\n        f1, f2, f3 = self._factors\n                                                              # (N, C*f1*f2*f3, D, H, W)\n        x = F.reshape(x, (0, -4, -1, f1 * f2 * f3, 0, 0, 0))  # (N, C, f1*f2*f3, D, H, W)\n        x = F.swapaxes(x, 2, 3)                               # (N, C, D, f1*f2*f3, H, W)\n        x = F.reshape(x, (0, 0, 0, -4, f1, f2*f3, 0, 0))      # (N, C, D, f1, f2*f3, H, W)\n        x = F.reshape(x, (0, 0, -3, 0, 0, 0))                 # (N, C, D*f1, f2*f3, H, W)\n        x = F.swapaxes(x, 3, 4)                               # (N, C, D*f1, H, f2*f3, W)\n        x = F.reshape(x, (0, 0, 0, 0, -4, f2, f3, 0))         # (N, C, D*f1, H, f2, f3, W)\n        x = F.reshape(x, (0, 0, 0, -3, 0, 0))                 # (N, C, D*f1, H*f2, f3, W)\n        x = F.swapaxes(x, 4, 5)                               # (N, C, D*f1, H*f2, W, f3)\n        x = F.reshape(x, (0, 0, 0, 0, -3))                    # (N, C, D*f1, H*f2, W*f3)\n        return x"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef retry(target_exception, tries=4, delay_s=1, backoff=2):\n    import time\n    from functools import wraps\n\n    def decorated_retry(f):\n        @wraps(f)\n        def f_retry(*args, **kwargs):\n            mtries, mdelay = tries, delay_s\n            while mtries > 1:\n                try:\n                    return f(*args, **kwargs)\n                except target_exception as e:\n                    logging.warning(\"Exception: %s, Retrying in %d seconds...\", str(e), mdelay)\n                    time.sleep(mdelay)\n                    mtries -= 1\n                    mdelay *= backoff\n            return f(*args, **kwargs)\n\n        return f_retry  # true decorator\n\n    return decorated_retry", "response": "Decorator to retry a function in a random order."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_model(model_name, epoch_num, data_shapes, label_shapes, label_names, gpus=''):\n    sym, arg_params, aux_params = mx.model.load_checkpoint(model_name, epoch_num)\n\n    mod = create_module(sym, data_shapes, label_shapes, label_names, gpus)\n\n    mod.set_params(\n        arg_params=arg_params,\n        aux_params=aux_params,\n        allow_missing=True\n    )\n\n    return mod", "response": "Loads a model from a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_module(sym, data_shapes, label_shapes, label_names, gpus=''):\n    if gpus == '':\n        devices = mx.cpu()\n    else:\n        devices = [mx.gpu(int(i)) for i in gpus.split(',')]\n\n    data_names = [data_shape[0] for data_shape in data_shapes]\n\n    mod = mx.mod.Module(\n        symbol=sym,\n        data_names=data_names,\n        context=devices,\n        label_names=label_names\n    )\n    mod.bind(\n        for_training=False,\n        data_shapes=data_shapes,\n        label_shapes=label_shapes\n    )\n    return mod", "response": "Creates a MXNet module."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nevaluate the network given validation record file.", "response": "def evaluate_net(net, path_imgrec, num_classes, num_batch, mean_pixels, data_shape,\n                 model_prefix, epoch, ctx=mx.cpu(), batch_size=32,\n                 path_imglist=\"\", nms_thresh=0.45, force_nms=False,\n                 ovp_thresh=0.5, use_difficult=False, class_names=None,\n                 voc07_metric=False):\n    \"\"\"\n    evalute network given validation record file\n\n    Parameters:\n    ----------\n    net : str or None\n        Network name or use None to load from json without modifying\n    path_imgrec : str\n        path to the record validation file\n    path_imglist : str\n        path to the list file to replace labels in record file, optional\n    num_classes : int\n        number of classes, not including background\n    mean_pixels : tuple\n        (mean_r, mean_g, mean_b)\n    data_shape : tuple or int\n        (3, height, width) or height/width\n    model_prefix : str\n        model prefix of saved checkpoint\n    epoch : int\n        load model epoch\n    ctx : mx.ctx\n        mx.gpu() or mx.cpu()\n    batch_size : int\n        validation batch size\n    nms_thresh : float\n        non-maximum suppression threshold\n    force_nms : boolean\n        whether suppress different class objects\n    ovp_thresh : float\n        AP overlap threshold for true/false postives\n    use_difficult : boolean\n        whether to use difficult objects in evaluation if applicable\n    class_names : comma separated str\n        class names in string, must correspond to num_classes if set\n    voc07_metric : boolean\n        whether to use 11-point evluation as in VOC07 competition\n    \"\"\"\n    # set up logger\n    logging.basicConfig()\n    logger = logging.getLogger()\n    logger.setLevel(logging.INFO)\n\n    # args\n    if isinstance(data_shape, int):\n        data_shape = (3, data_shape, data_shape)\n    assert len(data_shape) == 3 and data_shape[0] == 3\n    model_prefix += '_' + str(data_shape[1])\n\n    # iterator\n    eval_iter = DetRecordIter(path_imgrec, batch_size, data_shape, mean_pixels=mean_pixels,\n                              path_imglist=path_imglist, **cfg.valid)\n    # model params\n    load_net, args, auxs = mx.model.load_checkpoint(model_prefix, epoch)\n    # network\n    if net is None:\n        net = load_net\n    else:\n        net = get_symbol(net, data_shape[1], num_classes=num_classes,\n            nms_thresh=nms_thresh, force_suppress=force_nms)\n    if not 'label' in net.list_arguments():\n        label = mx.sym.Variable(name='label')\n        net = mx.sym.Group([net, label])\n\n    # init module\n    mod = mx.mod.Module(net, label_names=('label',), logger=logger, context=ctx,\n        fixed_param_names=net.list_arguments())\n    mod.bind(data_shapes=eval_iter.provide_data, label_shapes=eval_iter.provide_label)\n    mod.set_params(args, auxs, allow_missing=False, force_init=True)\n\n    # run evaluation\n    if voc07_metric:\n        metric = VOC07MApMetric(ovp_thresh, use_difficult, class_names)\n    else:\n        metric = MApMetric(ovp_thresh, use_difficult, class_names)\n\n    num = num_batch * batch_size\n    data = [mx.random.uniform(-1.0, 1.0, shape=shape, ctx=ctx) for _, shape in mod.data_shapes]\n    batch = mx.io.DataBatch(data, [])  # empty label\n\n    dry_run = 5                 # use 5 iterations to warm up\n    for i in range(dry_run):\n        mod.forward(batch, is_train=False)\n        for output in mod.get_outputs():\n            output.wait_to_read()\n\n    tic = time.time()\n    results = mod.score(eval_iter, metric, num_batch=num_batch)\n    speed = num / (time.time() - tic)\n    if logger is not None:\n        logger.info('Finished inference with %d images' % num)\n        logger.info('Finished with %f images per second', speed)\n\n    for k, v in results:\n        print(\"{}: {}\".format(k, v))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninitializing the parameters and auxiliary states. By default this function does nothing.", "response": "def init_params(self, initializer=Uniform(0.01), arg_params=None, aux_params=None,\n                    allow_missing=False, force_init=False, allow_extra=False):\n        \"\"\"Initializes the parameters and auxiliary states. By default this function\n        does nothing. Subclass should override this method if contains parameters.\n\n        Parameters\n        ----------\n        initializer : Initializer\n            Called to initialize parameters if needed.\n        arg_params : dict\n            If not ``None``, should be a dictionary of existing `arg_params`. Initialization\n            will be copied from that.\n        aux_params : dict\n            If not ``None``, should be a dictionary of existing `aux_params`. Initialization\n            will be copied from that.\n        allow_missing : bool\n            If ``True``, params could contain missing values, and the initializer will be\n            called to fill those missing params.\n        force_init : bool\n            If ``True``, will force re-initialize even if already initialized.\n        allow_extra : boolean, optional\n            Whether allow extra parameters that are not needed by symbol.\n            If this is True, no error will be thrown when arg_params or aux_params\n            contain extra parameters that is not needed by the executor.\n        \"\"\"\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_metric(self, eval_metric, labels, pre_sliced=False):\n        if self._label_shapes is None:\n            # since we do not need labels, we are probably not a module with a loss\n            # function or predictions, so just ignore this call\n            return\n\n        if pre_sliced:\n            raise RuntimeError(\"PythonModule does not support presliced labels\")\n\n        # by default we expect our outputs are some scores that could be evaluated\n        eval_metric.update(labels, self.get_outputs())", "response": "Evaluates and accumulates evaluation metric on outputs of the last forward computation."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef bind(self, data_shapes, label_shapes=None, for_training=True,\n             inputs_need_grad=False, force_rebind=False, shared_module=None,\n             grad_req='write'):\n        \"\"\"Binds the symbols to construct executors. This is necessary before one\n        can perform computation with the module.\n\n        Parameters\n        ----------\n        data_shapes : list of (str, tuple)\n            Typically is ``data_iter.provide_data``.\n        label_shapes : list of (str, tuple)\n            Typically is ``data_iter.provide_label``.\n        for_training : bool\n            Default is ``True``. Whether the executors should be bind for training.\n        inputs_need_grad : bool\n            Default is ``False``. Whether the gradients to the input data need to be computed.\n            Typically this is not needed. But this might be needed when implementing composition\n            of modules.\n        force_rebind : bool\n            Default is ``False``. This function does nothing if the executors are already\n            bound. But with this ``True``, the executors will be forced to rebind.\n        shared_module : Module\n            Default is ``None``. This is used in bucketing. When not ``None``, the shared module\n            essentially corresponds to a different bucket -- a module with different symbol\n            but with the same sets of parameters (e.g. unrolled RNNs with different lengths).\n        grad_req : str, list of str, dict of str to str\n            Requirement for gradient accumulation. Can be 'write', 'add', or 'null'\n            (default to 'write').\n            Can be specified globally (str) or for each argument (list, dict).\n        \"\"\"\n        if self.binded and not force_rebind:\n            self.logger.warning('Already bound, ignoring bind()')\n            return\n\n        assert grad_req == 'write', \"Python module only support write gradient\"\n        self.for_training = for_training\n        self.inputs_need_grad = inputs_need_grad\n\n        assert len(data_shapes) == len(self._data_names)\n        assert [x[0] for x in data_shapes] == self._data_names\n        self._data_shapes = data_shapes\n\n        self._label_shapes = label_shapes\n        if label_shapes is not None:\n            assert self._label_names is not None\n            assert len(self._label_names) == len(label_shapes)\n            assert [x[0] for x in label_shapes] == self._label_names\n\n        self._output_shapes = self._compute_output_shapes()", "response": "Binds the executors to construct executors."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nforward computation. Here we do nothing but to keep a reference to the scores and the labels so that we can do backward computation.", "response": "def forward(self, data_batch, is_train=None):\n        \"\"\"Forward computation. Here we do nothing but to keep a reference to\n        the scores and the labels so that we can do backward computation.\n\n        Parameters\n        ----------\n        data_batch : DataBatch\n            Could be anything with similar API implemented.\n        is_train : bool\n            Default is ``None``, which means `is_train` takes the value of ``self.for_training``.\n        \"\"\"\n        self._scores = data_batch.data[0]\n\n        if is_train is None:\n            is_train = self.for_training\n\n        if is_train:\n            self._labels = data_batch.label[0]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _backward_impl(self):\n        if self._grad_func is not None:\n            grad = self._grad_func(self._scores, self._labels)\n            if not isinstance(grad, nd.NDArray):\n                grad = nd.array(grad)\n            self._scores_grad = grad\n        else:\n            raise NotImplementedError()", "response": "Actual implementation of the backward computation."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nencodes sentences and build a mapping from string tokens to integer indices.", "response": "def encode_sentences(sentences, vocab=None, invalid_label=-1, invalid_key='\\n',\n                     start_label=0, unknown_token=None):\n    \"\"\"Encode sentences and (optionally) build a mapping\n    from string tokens to integer indices. Unknown keys\n    will be added to vocabulary.\n\n    Parameters\n    ----------\n    sentences : list of list of str\n        A list of sentences to encode. Each sentence\n        should be a list of string tokens.\n    vocab : None or dict of str -> int\n        Optional input Vocabulary\n    invalid_label : int, default -1\n        Index for invalid token, like <end-of-sentence>\n    invalid_key : str, default '\\\\n'\n        Key for invalid token. Use '\\\\n' for end\n        of sentence by default.\n    start_label : int\n        lowest index.\n    unknown_token: str\n        Symbol to represent unknown token.\n        If not specified, unknown token will be skipped.\n\n    Returns\n    -------\n    result : list of list of int\n        encoded sentences\n    vocab : dict of str -> int\n        result vocabulary\n    \"\"\"\n    idx = start_label\n    if vocab is None:\n        vocab = {invalid_key: invalid_label}\n        new_vocab = True\n    else:\n        new_vocab = False\n    res = []\n    for sent in sentences:\n        coded = []\n        for word in sent:\n            if word not in vocab:\n                assert (new_vocab or unknown_token), \"Unknown token %s\"%word\n                if idx == invalid_label:\n                    idx += 1\n                if unknown_token:\n                    word = unknown_token\n                vocab[word] = idx\n                idx += 1\n            coded.append(vocab[word])\n        res.append(coded)\n\n    return res, vocab"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nresets the iterator to the beginning of the data.", "response": "def reset(self):\n        \"\"\"Resets the iterator to the beginning of the data.\"\"\"\n        self.curr_idx = 0\n        random.shuffle(self.idx)\n        for buck in self.data:\n            np.random.shuffle(buck)\n\n        self.nddata = []\n        self.ndlabel = []\n        for buck in self.data:\n            label = np.empty_like(buck)\n            label[:, :-1] = buck[:, 1:]\n            label[:, -1] = self.invalid_label\n            self.nddata.append(ndarray.array(buck, dtype=self.dtype))\n            self.ndlabel.append(ndarray.array(label, dtype=self.dtype))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the next batch of data.", "response": "def next(self):\n        \"\"\"Returns the next batch of data.\"\"\"\n        if self.curr_idx == len(self.idx):\n            raise StopIteration\n        i, j = self.idx[self.curr_idx]\n        self.curr_idx += 1\n\n        if self.major_axis == 1:\n            data = self.nddata[i][j:j+self.batch_size].T\n            label = self.ndlabel[i][j:j+self.batch_size].T\n        else:\n            data = self.nddata[i][j:j+self.batch_size]\n            label = self.ndlabel[i][j:j+self.batch_size]\n\n        return DataBatch([data], [label], pad=0,\n                         bucket_key=self.buckets[i],\n                         provide_data=[DataDesc(\n                             name=self.data_name, shape=data.shape,\n                             layout=self.layout)],\n                         provide_label=[DataDesc(\n                             name=self.label_name, shape=label.shape,\n                             layout=self.layout)])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getInstance(self):\n        try:\n            return self._instance\n        except AttributeError:\n            self._instance = self._decorated()\n            return self._instance", "response": "Returns the singleton instance of the class."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--batch_size', type=int, default=64)\n    parser.add_argument('--image_path', type=str, default='./data/datasets/')\n    parser.add_argument('--align_path', type=str, default='./data/align/')\n    parser.add_argument('--num_gpus', type=int, default=1)\n    parser.add_argument('--num_workers', type=int, default=0)\n    parser.add_argument('--data_type', type=str, default='valid')\n    parser.add_argument('--model_path', type=str, default=None)\n    config = parser.parse_args()\n    trainer = Train(config)\n    trainer.build_model(path=config.model_path)\n    trainer.load_dataloader()\n\n    if config.data_type == 'train':\n        data_loader = trainer.train_dataloader\n    elif config.data_type == 'valid':\n        data_loader = trainer.valid_dataloader\n\n    trainer.infer_batch(data_loader)", "response": "This function is called by the command line interface to run lipnet training code using argument info"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the variable given a name if one exists or create a new one if missing.", "response": "def get(self, name, **kwargs):\n        \"\"\"Get the variable given a name if one exists or create a new one if missing.\n\n        Parameters\n        ----------\n        name : str\n            name of the variable\n        **kwargs :\n            more arguments that's passed to symbol.Variable\n        \"\"\"\n        name = self._prefix + name\n        if name not in self._params:\n            self._params[name] = symbol.Variable(name, **kwargs)\n        return self._params[name]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reset(self):\n        self._init_counter = -1\n        self._counter = -1\n        if hasattr(self, '_cells'):\n            for cell in self._cells:\n                cell.reset()", "response": "Reset before re - using the cell for another graph."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of states for the first RNN step.", "response": "def begin_state(self, func=symbol.zeros, **kwargs):\n        \"\"\"Initial state for this cell.\n\n        Parameters\n        ----------\n        func : callable, default symbol.zeros\n            Function for creating initial state. Can be symbol.zeros,\n            symbol.uniform, symbol.Variable etc.\n            Use symbol.Variable if you want to directly\n            feed input as states.\n        **kwargs :\n            more keyword arguments passed to func. For example\n            mean, std, dtype, etc.\n\n        Returns\n        -------\n        states : nested list of Symbol\n            Starting states for the first RNN step.\n        \"\"\"\n        assert not self._modified, \\\n            \"After applying modifier cells (e.g. DropoutCell) the base \" \\\n            \"cell cannot be called directly. Call the modifier cell instead.\"\n        states = []\n        for info in self.state_info:\n            self._init_counter += 1\n            if info is None:\n                state = func(name='%sbegin_state_%d'%(self._prefix, self._init_counter),\n                             **kwargs)\n            else:\n                kwargs.update(info)\n                state = func(name='%sbegin_state_%d'%(self._prefix, self._init_counter),\n                             **kwargs)\n            states.append(state)\n        return states"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef unpack_weights(self, args):\n        args = args.copy()\n        if not self._gate_names:\n            return args\n        h = self._num_hidden\n        for group_name in ['i2h', 'h2h']:\n            weight = args.pop('%s%s_weight'%(self._prefix, group_name))\n            bias = args.pop('%s%s_bias' % (self._prefix, group_name))\n            for j, gate in enumerate(self._gate_names):\n                wname = '%s%s%s_weight' % (self._prefix, group_name, gate)\n                args[wname] = weight[j*h:(j+1)*h].copy()\n                bname = '%s%s%s_bias' % (self._prefix, group_name, gate)\n                args[bname] = bias[j*h:(j+1)*h].copy()\n        return args", "response": "Unpack fused weight matrices into separate\n        weight matrices."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pack_weights(self, args):\n        args = args.copy()\n        if not self._gate_names:\n            return args\n        for group_name in ['i2h', 'h2h']:\n            weight = []\n            bias = []\n            for gate in self._gate_names:\n                wname = '%s%s%s_weight'%(self._prefix, group_name, gate)\n                weight.append(args.pop(wname))\n                bname = '%s%s%s_bias'%(self._prefix, group_name, gate)\n                bias.append(args.pop(bname))\n            args['%s%s_weight'%(self._prefix, group_name)] = ndarray.concatenate(weight)\n            args['%s%s_bias'%(self._prefix, group_name)] = ndarray.concatenate(bias)\n        return args", "response": "Pack separate weight matrices into a single packed\n            weight."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef unroll(self, length, inputs, begin_state=None, layout='NTC', merge_outputs=None):\n        self.reset()\n\n        inputs, _ = _normalize_sequence(length, inputs, layout, False)\n        if begin_state is None:\n            begin_state = self.begin_state()\n\n        states = begin_state\n        outputs = []\n        for i in range(length):\n            output, states = self(inputs[i], states)\n            outputs.append(output)\n\n        outputs, _ = _normalize_sequence(length, outputs, layout, merge_outputs)\n\n        return outputs, states", "response": "Unrolls an RNN cell across time steps."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_activation(self, inputs, activation, **kwargs):\n        if isinstance(activation, string_types):\n            return symbol.Activation(inputs, act_type=activation, **kwargs)\n        else:\n            return activation(inputs, **kwargs)", "response": "Get activation function. Convert if is string"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nslicing fused rnn weights", "response": "def _slice_weights(self, arr, li, lh):\n        \"\"\"slice fused rnn weights\"\"\"\n        args = {}\n        gate_names = self._gate_names\n        directions = self._directions\n\n        b = len(directions)\n        p = 0\n        for layer in range(self._num_layers):\n            for direction in directions:\n                for gate in gate_names:\n                    name = '%s%s%d_i2h%s_weight'%(self._prefix, direction, layer, gate)\n                    if layer > 0:\n                        size = b*lh*lh\n                        args[name] = arr[p:p+size].reshape((lh, b*lh))\n                    else:\n                        size = li*lh\n                        args[name] = arr[p:p+size].reshape((lh, li))\n                    p += size\n                for gate in gate_names:\n                    name = '%s%s%d_h2h%s_weight'%(self._prefix, direction, layer, gate)\n                    size = lh**2\n                    args[name] = arr[p:p+size].reshape((lh, lh))\n                    p += size\n\n        for layer in range(self._num_layers):\n            for direction in directions:\n                for gate in gate_names:\n                    name = '%s%s%d_i2h%s_bias'%(self._prefix, direction, layer, gate)\n                    args[name] = arr[p:p+lh]\n                    p += lh\n                for gate in gate_names:\n                    name = '%s%s%d_h2h%s_bias'%(self._prefix, direction, layer, gate)\n                    args[name] = arr[p:p+lh]\n                    p += lh\n\n        assert p == arr.size, \"Invalid parameters size for FusedRNNCell\"\n        return args"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef unfuse(self):\n        stack = SequentialRNNCell()\n        get_cell = {'rnn_relu': lambda cell_prefix: RNNCell(self._num_hidden,\n                                                            activation='relu',\n                                                            prefix=cell_prefix),\n                    'rnn_tanh': lambda cell_prefix: RNNCell(self._num_hidden,\n                                                            activation='tanh',\n                                                            prefix=cell_prefix),\n                    'lstm': lambda cell_prefix: LSTMCell(self._num_hidden,\n                                                         prefix=cell_prefix),\n                    'gru': lambda cell_prefix: GRUCell(self._num_hidden,\n                                                       prefix=cell_prefix)}[self._mode]\n        for i in range(self._num_layers):\n            if self._bidirectional:\n                stack.add(BidirectionalCell(\n                    get_cell('%sl%d_'%(self._prefix, i)),\n                    get_cell('%sr%d_'%(self._prefix, i)),\n                    output_prefix='%sbi_l%d_'%(self._prefix, i)))\n            else:\n                stack.add(get_cell('%sl%d_'%(self._prefix, i)))\n\n            if self._dropout > 0 and i != self._num_layers - 1:\n                stack.add(DropoutCell(self._dropout, prefix='%s_dropout%d_'%(self._prefix, i)))\n\n        return stack", "response": "Unfuses the fused RNN in to a stack of rnn cells."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add(self, cell):\n        self._cells.append(cell)\n        if self._override_cell_params:\n            assert cell._own_params, \\\n                \"Either specify params for SequentialRNNCell \" \\\n                \"or child cells, not both.\"\n            cell.params._params.update(self.params._params)\n        self.params._params.update(cell.params._params)", "response": "Append a new cell into the stack."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread an image from file path or URL optionally resizing to given image dimensions and subtracting mean.", "response": "def read_image(img_path, image_dims=None, mean=None):\n    \"\"\"\n    Reads an image from file path or URL, optionally resizing to given image dimensions and\n    subtracting mean.\n    :param img_path: path to file, or url to download\n    :param image_dims: image dimensions to resize to, or None\n    :param mean: mean file to subtract, or None\n    :return: loaded image, in RGB format\n    \"\"\"\n\n    import urllib\n\n    filename = img_path.split(\"/\")[-1]\n    if img_path.startswith('http'):\n        urllib.urlretrieve(img_path, filename)\n        img = cv2.imread(filename)\n    else:\n        img = cv2.imread(img_path)\n\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    if image_dims is not None:\n        img = cv2.resize(img, image_dims)  # resize to image_dims to fit model\n    img = np.rollaxis(img, 2) # change to (c, h, w) order\n    img = img[np.newaxis, :]  # extend to (n, c, h, w)\n    if mean is not None:\n        mean = np.array(mean)\n        if mean.shape == (3,):\n            mean = mean[np.newaxis, :, np.newaxis, np.newaxis]  # extend to (n, c, 1, 1)\n        img = img.astype(np.float32) - mean # subtract mean\n\n    return img"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _ch_dev(arg_params, aux_params, ctx):\n    new_args = dict()\n    new_auxs = dict()\n    for k, v in arg_params.items():\n        new_args[k] = v.as_in_context(ctx)\n    for k, v in aux_params.items():\n        new_auxs[k] = v.as_in_context(ctx)\n    return new_args, new_auxs", "response": "Changes the device of given mxnet arguments and auxiliary parameters on given mxnet arguments\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a caffe model to mxnet model and compare it with the current model.", "response": "def convert_and_compare_caffe_to_mxnet(image_url, gpu, caffe_prototxt_path, caffe_model_path,\n                                       caffe_mean, mean_diff_allowed, max_diff_allowed):\n    \"\"\"\n    Run the layer comparison on a caffe model, given its prototxt, weights and mean.\n    The comparison is done by inferring on a given image using both caffe and mxnet model\n    :param image_url: image file or url to run inference on\n    :param gpu: gpu to use, -1 for cpu\n    :param caffe_prototxt_path: path to caffe prototxt\n    :param caffe_model_path: path to caffe weights\n    :param caffe_mean: path to caffe mean file\n    \"\"\"\n\n    import caffe\n    from caffe_proto_utils import read_network_dag, process_network_proto, read_caffe_mean\n    from convert_model import convert_model\n\n    if isinstance(caffe_mean, str):\n        caffe_mean = read_caffe_mean(caffe_mean)\n    elif caffe_mean is None:\n        pass\n    elif len(caffe_mean) == 3:\n        # swap channels from Caffe BGR to RGB\n        caffe_mean = caffe_mean[::-1]\n\n    # get caffe root location, this is needed to run the upgrade network utility, so we only need\n    # to support parsing of latest caffe\n    caffe_root = os.path.dirname(os.path.dirname(caffe.__path__[0]))\n    caffe_prototxt_path = process_network_proto(caffe_root, caffe_prototxt_path)\n\n    _, layer_name_to_record, top_to_layers = read_network_dag(caffe_prototxt_path)\n\n    caffe.set_mode_cpu()\n    caffe_net = caffe.Net(caffe_prototxt_path, caffe_model_path, caffe.TEST)\n\n    image_dims = tuple(caffe_net.blobs['data'].shape)[2:4]\n\n    logging.info('getting image %s', image_url)\n    img_rgb = read_image(image_url, image_dims, caffe_mean)\n    img_bgr = img_rgb[:, ::-1, :, :]\n\n    caffe_net.blobs['data'].reshape(*img_bgr.shape)\n    caffe_net.blobs['data'].data[...] = img_bgr\n    _ = caffe_net.forward()\n\n    # read sym and add all outputs\n    sym, arg_params, aux_params, _ = convert_model(caffe_prototxt_path, caffe_model_path)\n    sym = sym.get_internals()\n\n    # now mxnet\n    if gpu < 0:\n        ctx = mx.cpu(0)\n    else:\n        ctx = mx.gpu(gpu)\n\n    arg_params, aux_params = _ch_dev(arg_params, aux_params, ctx)\n    arg_params[\"data\"] = mx.nd.array(img_rgb, ctx)\n    arg_params[\"prob_label\"] = mx.nd.empty((1,), ctx)\n    exe = sym.bind(ctx, arg_params, args_grad=None, grad_req=\"null\", aux_states=aux_params)\n    exe.forward(is_train=False)\n\n    compare_layers_from_nets(caffe_net, arg_params, aux_params, exe, layer_name_to_record,\n                             top_to_layers, mean_diff_allowed, max_diff_allowed)\n\n    return"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompare layers by layer of a caffe network with mxnet model", "response": "def compare_layers_from_nets(caffe_net, arg_params, aux_params, exe, layer_name_to_record,\n                             top_to_layers, mean_diff_allowed, max_diff_allowed):\n    \"\"\"\n    Compare layer by layer of a caffe network with mxnet network\n    :param caffe_net: loaded caffe network\n    :param arg_params: arguments\n    :param aux_params: auxiliary parameters\n    :param exe: mxnet model\n    :param layer_name_to_record: map between caffe layer and information record\n    :param top_to_layers: map between caffe blob name to layers which outputs it (including inplace)\n    :param mean_diff_allowed: mean difference allowed between caffe blob and mxnet blob\n    :param max_diff_allowed: max difference allowed between caffe blob and mxnet blob\n    \"\"\"\n\n    import re\n\n    log_format = '  {0:<40}  {1:<40}  {2:<8}  {3:>10}  {4:>10}  {5:<1}'\n\n    compare_layers_from_nets.is_first_convolution = True\n\n    def _compare_blob(caf_blob, mx_blob, caf_name, mx_name, blob_type, note):\n        diff = np.abs(mx_blob - caf_blob)\n        diff_mean = diff.mean()\n        diff_max = diff.max()\n        logging.info(log_format.format(caf_name, mx_name, blob_type, '%4.5f' % diff_mean,\n                                       '%4.5f' % diff_max, note))\n        assert diff_mean < mean_diff_allowed\n        assert diff_max < max_diff_allowed\n\n    def _process_layer_parameters(layer):\n\n        logging.debug('processing layer %s of type %s', layer.name, layer.type)\n\n        normalized_layer_name = re.sub('[-/]', '_', layer.name)\n\n        # handle weight and bias of convolution and fully-connected layers\n        if layer.name in caffe_net.params and layer.type in ['Convolution', 'InnerProduct',\n                                                             'Deconvolution']:\n\n            has_bias = len(caffe_net.params[layer.name]) > 1\n\n            mx_name_weight = '{}_weight'.format(normalized_layer_name)\n            mx_beta = arg_params[mx_name_weight].asnumpy()\n\n            # first convolution should change from BGR to RGB\n            if layer.type == 'Convolution' and compare_layers_from_nets.is_first_convolution:\n                compare_layers_from_nets.is_first_convolution = False\n\n                # if RGB or RGBA\n                if mx_beta.shape[1] == 3 or mx_beta.shape[1] == 4:\n                    # Swapping BGR of caffe into RGB in mxnet\n                    mx_beta[:, [0, 2], :, :] = mx_beta[:, [2, 0], :, :]\n\n            caf_beta = caffe_net.params[layer.name][0].data\n            _compare_blob(caf_beta, mx_beta, layer.name, mx_name_weight, 'weight', '')\n\n            if has_bias:\n                mx_name_bias = '{}_bias'.format(normalized_layer_name)\n                mx_gamma = arg_params[mx_name_bias].asnumpy()\n                caf_gamma = caffe_net.params[layer.name][1].data\n                _compare_blob(caf_gamma, mx_gamma, layer.name, mx_name_bias, 'bias', '')\n\n        elif layer.name in caffe_net.params and layer.type == 'Scale':\n\n            if 'scale' in normalized_layer_name:\n                bn_name = normalized_layer_name.replace('scale', 'bn')\n            elif 'sc' in normalized_layer_name:\n                bn_name = normalized_layer_name.replace('sc', 'bn')\n            else:\n                assert False, 'Unknown name convention for bn/scale'\n\n            beta_name = '{}_beta'.format(bn_name)\n            gamma_name = '{}_gamma'.format(bn_name)\n\n            mx_beta = arg_params[beta_name].asnumpy()\n            caf_beta = caffe_net.params[layer.name][1].data\n            _compare_blob(caf_beta, mx_beta, layer.name, beta_name, 'mov_mean', '')\n\n            mx_gamma = arg_params[gamma_name].asnumpy()\n            caf_gamma = caffe_net.params[layer.name][0].data\n            _compare_blob(caf_gamma, mx_gamma, layer.name, gamma_name, 'mov_var', '')\n\n        elif layer.name in caffe_net.params and layer.type == 'BatchNorm':\n\n            mean_name = '{}_moving_mean'.format(normalized_layer_name)\n            var_name = '{}_moving_var'.format(normalized_layer_name)\n\n            caf_rescale_factor = caffe_net.params[layer.name][2].data\n\n            mx_mean = aux_params[mean_name].asnumpy()\n            caf_mean = caffe_net.params[layer.name][0].data / caf_rescale_factor\n            _compare_blob(caf_mean, mx_mean, layer.name, mean_name, 'mean', '')\n\n            mx_var = aux_params[var_name].asnumpy()\n            caf_var = caffe_net.params[layer.name][1].data / caf_rescale_factor\n            _compare_blob(caf_var, mx_var, layer.name, var_name, 'var',\n                          'expect 1e-04 change due to cudnn eps')\n\n        elif layer.type in ['Input', 'Pooling', 'ReLU', 'Eltwise', 'Softmax', 'LRN', 'Concat',\n                            'Dropout', 'Crop']:\n            # no parameters to check for these layers\n            pass\n\n        else:\n            warnings.warn('No handling for layer %s of type %s, should we ignore it?', layer.name,\n                          layer.type)\n\n        return\n\n    def _process_layer_output(caffe_blob_name):\n\n        logging.debug('processing blob %s', caffe_blob_name)\n\n        # skip blobs not originating from actual layers, e.g. artificial split layers added by caffe\n        if caffe_blob_name not in top_to_layers:\n            return\n\n        caf_blob = caffe_net.blobs[caffe_blob_name].data\n\n        # data should change from BGR to RGB\n        if caffe_blob_name == 'data':\n\n            # if RGB or RGBA\n            if caf_blob.shape[1] == 3 or caf_blob.shape[1] == 4:\n                # Swapping BGR of caffe into RGB in mxnet\n                caf_blob[:, [0, 2], :, :] = caf_blob[:, [2, 0], :, :]\n            mx_name = 'data'\n\n        else:\n            # get last layer name which outputs this blob name\n            last_layer_name = top_to_layers[caffe_blob_name][-1]\n            normalized_last_layer_name = re.sub('[-/]', '_', last_layer_name)\n            mx_name = '{}_output'.format(normalized_last_layer_name)\n            if 'scale' in mx_name:\n                mx_name = mx_name.replace('scale', 'bn')\n            elif 'sc' in mx_name:\n                mx_name = mx_name.replace('sc', 'bn')\n\n        if mx_name not in exe.output_dict:\n            logging.error('mxnet blob %s is missing, time to extend the compare tool..', mx_name)\n            return\n\n        mx_blob = exe.output_dict[mx_name].asnumpy()\n        _compare_blob(caf_blob, mx_blob, caffe_blob_name, mx_name, 'output', '')\n\n        return\n\n    # check layer parameters\n    logging.info('\\n***** Network Parameters '.ljust(140, '*'))\n    logging.info(log_format.format('CAFFE', 'MXNET', 'Type', 'Mean(diff)', 'Max(diff)', 'Note'))\n    first_layer_name = layer_name_to_record.keys()[0]\n    _bfs(layer_name_to_record[first_layer_name], _process_layer_parameters)\n\n    # check layer output\n    logging.info('\\n***** Network Outputs '.ljust(140, '*'))\n    logging.info(log_format.format('CAFFE', 'MXNET', 'Type', 'Mean(diff)', 'Max(diff)', 'Note'))\n    for caffe_blob_name in caffe_net.blobs.keys():\n        _process_layer_output(caffe_blob_name)\n\n    return"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting an executor to Stochastic Gradient Langevin Dynamics and or Bayesian Dark Knowledge", "response": "def get_executor(sym, ctx, data_inputs, initializer=None):\n    \"\"\"Get executor to Stochastic Gradient Langevin Dynamics and/or Bayesian Dark Knowledge\"\"\"\n    data_shapes = {k: v.shape for k, v in data_inputs.items()}\n    arg_names = sym.list_arguments()\n    aux_names = sym.list_auxiliary_states()\n    param_names = list(set(arg_names) - set(data_inputs.keys()))\n    arg_shapes, output_shapes, aux_shapes = sym.infer_shape(**data_shapes)\n    arg_name_shape = {k: s for k, s in zip(arg_names, arg_shapes)}\n    params = {n: nd.empty(arg_name_shape[n], ctx=ctx) for n in param_names}\n    params_grad = {n: nd.empty(arg_name_shape[n], ctx=ctx) for n in param_names}\n    aux_states = {k: nd.empty(s, ctx=ctx) for k, s in zip(aux_names, aux_shapes)}\n    exe = sym.bind(ctx=ctx, args=dict(params, **data_inputs),\n                   args_grad=params_grad,\n                   aux_states=aux_states)\n    if initializer is not None:\n        for k, v in params.items():\n            initializer(k, v)\n    return exe, params, params_grad, aux_states"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef copy_param(exe, new_param=None):\n    if new_param is None:\n        new_param = {k: nd.empty(v.shape, ctx=mx.cpu()) for k, v in exe.arg_dict.items()}\n    for k, v in new_param.items():\n        exe.arg_dict[k].copyto(v)\n    return new_param", "response": "Create copy of parameters"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses command line arguments", "response": "def parse_args():\n    \"\"\"Parse command line arguments\"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"font_path\", help=\"Path to ttf font file or directory containing ttf files\")\n    parser.add_argument(\"--loss\", help=\"'ctc' or 'warpctc' loss [Default 'ctc']\", default='ctc')\n    parser.add_argument(\"--cpu\",\n                        help=\"Number of CPUs for training [Default 8]. Ignored if --gpu is specified.\",\n                        type=int, default=8)\n    parser.add_argument(\"--gpu\", help=\"Number of GPUs for training [Default 0]\", type=int)\n    parser.add_argument(\"--num_proc\", help=\"Number CAPTCHA generating processes [Default 4]\", type=int, default=4)\n    parser.add_argument(\"--prefix\", help=\"Checkpoint prefix [Default 'ocr']\", default='ocr')\n    return parser.parse_args()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef main():\n    args = parse_args()\n    if not any(args.loss == s for s in ['ctc', 'warpctc']):\n        raise ValueError(\"Invalid loss '{}' (must be 'ctc' or 'warpctc')\".format(args.loss))\n\n    hp = Hyperparams()\n\n    # Start a multiprocessor captcha image generator\n    mp_captcha = MPDigitCaptcha(\n        font_paths=get_fonts(args.font_path), h=hp.seq_length, w=30,\n        num_digit_min=3, num_digit_max=4, num_processes=args.num_proc, max_queue_size=hp.batch_size * 2)\n    try:\n        # Must call start() before any call to mxnet module (https://github.com/apache/incubator-mxnet/issues/9213)\n        mp_captcha.start()\n\n        if args.gpu:\n            contexts = [mx.context.gpu(i) for i in range(args.gpu)]\n        else:\n            contexts = [mx.context.cpu(i) for i in range(args.cpu)]\n\n        init_states = lstm.init_states(hp.batch_size, hp.num_lstm_layer, hp.num_hidden)\n\n        data_train = OCRIter(\n            hp.train_epoch_size // hp.batch_size, hp.batch_size, init_states, captcha=mp_captcha, name='train')\n        data_val = OCRIter(\n            hp.eval_epoch_size // hp.batch_size, hp.batch_size, init_states, captcha=mp_captcha, name='val')\n\n        symbol = lstm.lstm_unroll(\n            num_lstm_layer=hp.num_lstm_layer,\n            seq_len=hp.seq_length,\n            num_hidden=hp.num_hidden,\n            num_label=hp.num_label,\n            loss_type=args.loss)\n\n        head = '%(asctime)-15s %(message)s'\n        logging.basicConfig(level=logging.DEBUG, format=head)\n\n        module = mx.mod.Module(\n            symbol,\n            data_names=['data', 'l0_init_c', 'l0_init_h', 'l1_init_c', 'l1_init_h'],\n            label_names=['label'],\n            context=contexts)\n\n        metrics = CtcMetrics(hp.seq_length)\n        module.fit(train_data=data_train,\n                   eval_data=data_val,\n                   # use metrics.accuracy or metrics.accuracy_lcs\n                   eval_metric=mx.metric.np(metrics.accuracy, allow_extra_outputs=True),\n                   optimizer='sgd',\n                   optimizer_params={'learning_rate': hp.learning_rate,\n                                     'momentum': hp.momentum,\n                                     'wd': 0.00001,\n                                     },\n                   initializer=mx.init.Xavier(factor_type=\"in\", magnitude=2.34),\n                   num_epoch=hp.num_epoch,\n                   batch_end_callback=mx.callback.Speedometer(hp.batch_size, 50),\n                   epoch_end_callback=mx.callback.do_checkpoint(args.prefix),\n                   )\n    except KeyboardInterrupt:\n        print(\"W: interrupt received, stopping...\")\n    finally:\n        # Reset multiprocessing captcha generator to stop processes\n        mp_captcha.reset()", "response": "Main entry point of the module."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef optimize(args):\n    if args.cuda:\n        ctx = mx.gpu(0)\n    else:\n        ctx = mx.cpu(0)\n    # load the content and style target\n    content_image = utils.tensor_load_rgbimage(args.content_image,ctx, size=args.content_size, keep_asp=True)\n    content_image = utils.subtract_imagenet_mean_preprocess_batch(content_image)\n    style_image = utils.tensor_load_rgbimage(args.style_image, ctx, size=args.style_size)\n    style_image = utils.subtract_imagenet_mean_preprocess_batch(style_image)\n    # load the pre-trained vgg-16 and extract features\n    vgg = net.Vgg16()\n    utils.init_vgg_params(vgg, 'models', ctx=ctx)\n    # content feature\n    f_xc_c = vgg(content_image)[1]\n    # style feature\n    features_style = vgg(style_image)\n    gram_style = [net.gram_matrix(y) for y in features_style]\n    # output\n    output = Parameter('output', shape=content_image.shape)\n    output.initialize(ctx=ctx)\n    output.set_data(content_image)\n    # optimizer\n    trainer = gluon.Trainer([output], 'adam',\n                            {'learning_rate': args.lr})\n    mse_loss = gluon.loss.L2Loss()\n\n    # optimizing the images\n    for e in range(args.iters):\n        utils.imagenet_clamp_batch(output.data(), 0, 255)\n        # fix BN for pre-trained vgg\n        with autograd.record():\n            features_y = vgg(output.data())\n            content_loss = 2 * args.content_weight * mse_loss(features_y[1], f_xc_c)\n            style_loss = 0.\n            for m in range(len(features_y)):\n                gram_y = net.gram_matrix(features_y[m])\n                gram_s = gram_style[m]\n                style_loss = style_loss + 2 * args.style_weight * mse_loss(gram_y, gram_s)\n            total_loss = content_loss + style_loss\n            total_loss.backward()\n\n        trainer.step(1)\n        if (e + 1) % args.log_interval == 0:\n            print('loss:{:.2f}'.format(total_loss.asnumpy()[0]))\n\n    # save the image\n    output = utils.add_imagenet_mean_batch(output.data())\n    utils.tensor_save_bgrimage(output[0], args.output_image, args.cuda)", "response": "Optimize the images using the image - style transfer using Convolutional Neural Networks"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_mnist_sym(output_op=None, num_hidden=400):\n    net = mx.symbol.Variable('data')\n    net = mx.symbol.FullyConnected(data=net, name='mnist_fc1', num_hidden=num_hidden)\n    net = mx.symbol.Activation(data=net, name='mnist_relu1', act_type=\"relu\")\n    net = mx.symbol.FullyConnected(data=net, name='mnist_fc2', num_hidden=num_hidden)\n    net = mx.symbol.Activation(data=net, name='mnist_relu2', act_type=\"relu\")\n    net = mx.symbol.FullyConnected(data=net, name='mnist_fc3', num_hidden=10)\n    if output_op is None:\n        net = mx.symbol.SoftmaxOutput(data=net, name='softmax')\n    else:\n        net = output_op(data=net, name='softmax')\n    return net", "response": "Get symbol of mnist"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget synthetic gradient value", "response": "def synthetic_grad(X, theta, sigma1, sigma2, sigmax, rescale_grad=1.0, grad=None):\n    \"\"\"Get synthetic gradient value\"\"\"\n    if grad is None:\n        grad = nd.empty(theta.shape, theta.context)\n    theta1 = theta.asnumpy()[0]\n    theta2 = theta.asnumpy()[1]\n    v1 = sigma1 ** 2\n    v2 = sigma2 ** 2\n    vx = sigmax ** 2\n    denominator = numpy.exp(-(X - theta1) ** 2 / (2 * vx)) + numpy.exp(\n        -(X - theta1 - theta2) ** 2 / (2 * vx))\n    grad_npy = numpy.zeros(theta.shape)\n    grad_npy[0] = -rescale_grad * ((numpy.exp(-(X - theta1) ** 2 / (2 * vx)) * (X - theta1) / vx\n                                    + numpy.exp(-(X - theta1 - theta2) ** 2 / (2 * vx)) *\n                                    (X - theta1 - theta2) / vx) / denominator).sum() + theta1 / v1\n    grad_npy[1] = -rescale_grad * ((numpy.exp(-(X - theta1 - theta2) ** 2 / (2 * vx)) *\n                                    (X - theta1 - theta2) / vx) / denominator).sum() + theta2 / v2\n    grad[:] = grad_npy\n    return grad"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun DistilledSGLD on mnist dataset", "response": "def run_mnist_DistilledSGLD(num_training=50000, gpu_id=None):\n    \"\"\"Run DistilledSGLD on mnist dataset\"\"\"\n    X, Y, X_test, Y_test = load_mnist(num_training)\n    minibatch_size = 100\n    if num_training >= 10000:\n        num_hidden = 800\n        total_iter_num = 1000000\n        teacher_learning_rate = 1E-6\n        student_learning_rate = 0.0001\n        teacher_prior = 1\n        student_prior = 0.1\n        perturb_deviation = 0.1\n    else:\n        num_hidden = 400\n        total_iter_num = 20000\n        teacher_learning_rate = 4E-5\n        student_learning_rate = 0.0001\n        teacher_prior = 1\n        student_prior = 0.1\n        perturb_deviation = 0.001\n    teacher_net = get_mnist_sym(num_hidden=num_hidden)\n    logsoftmax = LogSoftmax()\n    student_net = get_mnist_sym(output_op=logsoftmax, num_hidden=num_hidden)\n    data_shape = (minibatch_size,) + X.shape[1::]\n    teacher_data_inputs = {'data': nd.zeros(data_shape, ctx=dev(gpu_id)),\n                           'softmax_label': nd.zeros((minibatch_size,), ctx=dev(gpu_id))}\n    student_data_inputs = {'data': nd.zeros(data_shape, ctx=dev(gpu_id)),\n                           'softmax_label': nd.zeros((minibatch_size, 10), ctx=dev(gpu_id))}\n    teacher_initializer = BiasXavier(factor_type=\"in\", magnitude=1)\n    student_initializer = BiasXavier(factor_type=\"in\", magnitude=1)\n    student_exe, student_params, _ = \\\n        DistilledSGLD(teacher_sym=teacher_net, student_sym=student_net,\n                      teacher_data_inputs=teacher_data_inputs,\n                      student_data_inputs=student_data_inputs,\n                      X=X, Y=Y, X_test=X_test, Y_test=Y_test, total_iter_num=total_iter_num,\n                      student_initializer=student_initializer,\n                      teacher_initializer=teacher_initializer,\n                      student_optimizing_algorithm=\"adam\",\n                      teacher_learning_rate=teacher_learning_rate,\n                      student_learning_rate=student_learning_rate,\n                      teacher_prior_precision=teacher_prior, student_prior_precision=student_prior,\n                      perturb_deviation=perturb_deviation, minibatch_size=100, dev=dev(gpu_id))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns SGLD on toy dataset", "response": "def run_toy_SGLD(gpu_id=None):\n    \"\"\"Run SGLD on toy dataset\"\"\"\n    X, Y, X_test, Y_test = load_toy()\n    minibatch_size = 1\n    teacher_noise_precision = 1.0 / 9.0\n    net = get_toy_sym(True, teacher_noise_precision)\n    data_shape = (minibatch_size,) + X.shape[1::]\n    data_inputs = {'data': nd.zeros(data_shape, ctx=dev(gpu_id)),\n                   'teacher_output_label': nd.zeros((minibatch_size, 1), ctx=dev(gpu_id))}\n    initializer = mx.init.Uniform(0.07)\n    exe, params, _ = SGLD(sym=net,\n                          data_inputs=data_inputs,\n                          X=X,\n                          Y=Y,\n                          X_test=X_test,\n                          Y_test=Y_test,\n                          total_iter_num=50000,\n                          initializer=initializer,\n                          learning_rate=1E-4,\n                          # lr_scheduler=mx.lr_scheduler.FactorScheduler(100000, 0.5),\n                          prior_precision=0.1,\n                          burn_in_iter_num=1000,\n                          thin_interval=10,\n                          task='regression',\n                          minibatch_size=minibatch_size,\n                          dev=dev(gpu_id))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run_toy_DistilledSGLD(gpu_id):\n    X, Y, X_test, Y_test = load_toy()\n    minibatch_size = 1\n    teacher_noise_precision = 1.0\n    teacher_net = get_toy_sym(True, teacher_noise_precision)\n    student_net = get_toy_sym(False)\n    data_shape = (minibatch_size,) + X.shape[1::]\n    teacher_data_inputs = {'data': nd.zeros(data_shape, ctx=dev(gpu_id)),\n                           'teacher_output_label': nd.zeros((minibatch_size, 1), ctx=dev(gpu_id))}\n    student_data_inputs = {'data': nd.zeros(data_shape, ctx=dev(gpu_id))}\n\n    teacher_initializer = mx.init.Uniform(0.07)\n    student_initializer = mx.init.Uniform(0.07)\n    student_grad_f = lambda student_outputs, teacher_pred: \\\n        regression_student_grad(student_outputs, teacher_pred, teacher_noise_precision)\n    student_exe, student_params, _ = \\\n        DistilledSGLD(teacher_sym=teacher_net, student_sym=student_net,\n                      teacher_data_inputs=teacher_data_inputs,\n                      student_data_inputs=student_data_inputs,\n                      X=X, Y=Y, X_test=X_test, Y_test=Y_test, total_iter_num=80000,\n                      teacher_initializer=teacher_initializer,\n                      student_initializer=student_initializer,\n                      teacher_learning_rate=1E-4, student_learning_rate=0.01,\n                      # teacher_lr_scheduler=mx.lr_scheduler.FactorScheduler(100000, 0.5),\n                      student_lr_scheduler=mx.lr_scheduler.FactorScheduler(8000, 0.8),\n                      student_grad_f=student_grad_f,\n                      teacher_prior_precision=0.1, student_prior_precision=0.001,\n                      perturb_deviation=0.1, minibatch_size=minibatch_size, task='regression',\n                      dev=dev(gpu_id))", "response": "Run DistilledSGLD on toy dataset"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning HMC on toy dataset", "response": "def run_toy_HMC(gpu_id=None):\n    \"\"\"Run HMC on toy dataset\"\"\"\n    X, Y, X_test, Y_test = load_toy()\n    minibatch_size = Y.shape[0]\n    noise_precision = 1 / 9.0\n    net = get_toy_sym(True, noise_precision)\n    data_shape = (minibatch_size,) + X.shape[1::]\n    data_inputs = {'data': nd.zeros(data_shape, ctx=dev(gpu_id)),\n                   'teacher_output_label': nd.zeros((minibatch_size, 1), ctx=dev(gpu_id))}\n    initializer = mx.init.Uniform(0.07)\n    sample_pool = HMC(net, data_inputs=data_inputs, X=X, Y=Y, X_test=X_test, Y_test=Y_test,\n                      sample_num=300000, initializer=initializer, prior_precision=1.0,\n                      learning_rate=1E-3, L=10, dev=dev(gpu_id))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_pascal(image_set, year, devkit_path, shuffle=False):\n    image_set = [y.strip() for y in image_set.split(',')]\n    assert image_set, \"No image_set specified\"\n    year = [y.strip() for y in year.split(',')]\n    assert year, \"No year specified\"\n\n    # make sure (# sets == # years)\n    if len(image_set) > 1 and len(year) == 1:\n        year = year * len(image_set)\n    if len(image_set) == 1 and len(year) > 1:\n        image_set = image_set * len(year)\n    assert len(image_set) == len(year), \"Number of sets and year mismatch\"\n\n    imdbs = []\n    for s, y in zip(image_set, year):\n        imdbs.append(PascalVoc(s, y, devkit_path, shuffle, is_train=True))\n    if len(imdbs) > 1:\n        return ConcatDB(imdbs, shuffle)\n    else:\n        return imdbs[0]", "response": "wrapper function for loading pascal voc dataset"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads ms coco dataset", "response": "def load_coco(image_set, dirname, shuffle=False):\n    \"\"\"\n    wrapper function for loading ms coco dataset\n\n    Parameters:\n    ----------\n    image_set : str\n        train2014, val2014, valminusminival2014, minival2014\n    dirname: str\n        root dir for coco\n    shuffle: boolean\n        initial shuffle\n    \"\"\"\n    anno_files = ['instances_' + y.strip() + '.json' for y in image_set.split(',')]\n    assert anno_files, \"No image set specified\"\n    imdbs = []\n    for af in anno_files:\n        af_path = os.path.join(dirname, 'annotations', af)\n        imdbs.append(Coco(af_path, dirname, shuffle=shuffle))\n    if len(imdbs) > 1:\n        return ConcatDB(imdbs, shuffle)\n    else:\n        return imdbs[0]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nresetting the iterator to the beginning of the data.", "response": "def reset(self):\n        \"\"\"Resets the iterator to the beginning of the data.\"\"\"\n        self.curr_idx = 0\n        #shuffle data in each bucket\n        random.shuffle(self.idx)\n        for i, buck in enumerate(self.sentences):\n            self.indices[i], self.sentences[i], self.characters[i], self.label[i] = shuffle(self.indices[i],\n                                                                                            self.sentences[i],\n                                                                                            self.characters[i],\n                                                                                            self.label[i])\n\n        self.ndindex = []\n        self.ndsent = []\n        self.ndchar = []\n        self.ndlabel = []\n\n        #for each bucket of data\n        for i, buck in enumerate(self.sentences):\n            #append the lists with an array\n            self.ndindex.append(ndarray.array(self.indices[i], dtype=self.dtype))\n            self.ndsent.append(ndarray.array(self.sentences[i], dtype=self.dtype))\n            self.ndchar.append(ndarray.array(self.characters[i], dtype=self.dtype))\n            self.ndlabel.append(ndarray.array(self.label[i], dtype=self.dtype))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the next batch of data.", "response": "def next(self):\n        \"\"\"Returns the next batch of data.\"\"\"\n        if self.curr_idx == len(self.idx):\n            raise StopIteration\n        #i = batches index, j = starting record\n        i, j = self.idx[self.curr_idx] \n        self.curr_idx += 1\n\n        indices = self.ndindex[i][j:j + self.batch_size]\n        sentences = self.ndsent[i][j:j + self.batch_size]\n        characters = self.ndchar[i][j:j + self.batch_size]\n        label = self.ndlabel[i][j:j + self.batch_size]\n\n        return DataBatch([sentences, characters], [label], pad=0, index = indices, bucket_key=self.buckets[i],\n                         provide_data=[DataDesc(name=self.data_names[0], shape=sentences.shape, layout=self.layout),\n                                       DataDesc(name=self.data_names[1], shape=characters.shape, layout=self.layout)],\n                         provide_label=[DataDesc(name=self.label_name, shape=label.shape, layout=self.layout)])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef convert_reshape(net, node, module, builder):\n    input_name, output_name = _get_input_output_name(net, node)\n    name = node['name']\n    target_shape = node['shape']\n\n    if any(item <= 0 for item in target_shape):\n        raise NotImplementedError('Special dimensional values less than or equal to 0 are not supported yet.'\n                                  'Feel free to file an issue here: https://github.com/dmlc/mxnet/issues.')\n\n    if 'reverse' in node and node['reverse'] == 'True':\n        raise NotImplementedError('\"reverse\" parameter is not supported by yet.'\n                                  'Feel free to file an issue here: https://github.com/dmlc/mxnet/issues.')\n\n    mode = 0 # CHANNEL_FIRST\n    builder.add_reshape(name, input_name, output_name, target_shape, mode)", "response": "Converts a reshape layer from mxnet to coreml."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef convert_transpose(net, node, module, builder):\n    input_name, output_name = _get_input_output_name(net, node)\n    name = node['name']\n    param = _get_attrs(node)\n\n    axes = literal_eval(param['axes'])\n    builder.add_permute(name, axes, input_name, output_name)", "response": "Convert a transpose layer from mxnet to coreml."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a flatten layer from mxnet to coreml.", "response": "def convert_flatten(net, node, module, builder):\n    \"\"\"Convert a flatten layer from mxnet to coreml.\n\n    Parameters\n    ----------\n    network: net\n        A mxnet network object.\n\n    layer: node\n        Node to convert.\n\n    module: module\n        An module for MXNet\n\n    builder: NeuralNetworkBuilder\n        A neural network builder object.\n    \"\"\"\n    input_name, output_name = _get_input_output_name(net, node)\n    name = node['name']\n    mode = 0 # CHANNEL_FIRST\n    builder.add_flatten(name, mode, input_name, output_name)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a softmax layer from mxnet to coreml.", "response": "def convert_softmax(net, node, module, builder):\n    \"\"\"Convert a softmax layer from mxnet to coreml.\n\n    Parameters\n    ----------\n    network: net\n        A mxnet network object.\n\n    layer: node\n        Node to convert.\n\n    module: module\n        An module for MXNet\n\n    builder: NeuralNetworkBuilder\n        A neural network builder object.\n    \"\"\"\n    input_name, output_name = _get_input_output_name(net, node)\n    name = node['name']\n    builder.add_softmax(name=name,\n                        input_name=input_name,\n                        output_name=output_name)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert an activation layer from mxnet to coreml.", "response": "def convert_activation(net, node, module, builder):\n    \"\"\"Convert an activation layer from mxnet to coreml.\n\n    Parameters\n    ----------\n    network: net\n        A mxnet network object.\n\n    layer: node\n        Node to convert.\n\n    module: module\n        An module for MXNet\n\n    builder: NeuralNetworkBuilder\n        A neural network builder object.\n    \"\"\"\n    input_name, output_name = _get_input_output_name(net, node)\n    name = node['name']\n    mx_non_linearity = _get_attrs(node)['act_type']\n    #TODO add SCALED_TANH, SOFTPLUS, SOFTSIGN, SIGMOID_HARD, LEAKYRELU, PRELU, ELU, PARAMETRICSOFTPLUS, THRESHOLDEDRELU, LINEAR\n    if mx_non_linearity == 'relu':\n        non_linearity = 'RELU'\n    elif mx_non_linearity == 'tanh':\n        non_linearity = 'TANH'\n    elif mx_non_linearity == 'sigmoid':\n        non_linearity = 'SIGMOID'\n    else:\n        raise TypeError('Unknown activation type %s' % mx_non_linearity)\n    builder.add_activation(name = name,\n                           non_linearity = non_linearity,\n                           input_name = input_name,\n                           output_name = output_name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a leakyrelu layer from mxnet to coreml.", "response": "def convert_leakyrelu(net, node, module, builder):\n    \"\"\"Convert a leakyrelu layer from mxnet to coreml.\n\n    Parameters\n    ----------\n    network: net\n        A mxnet network object.\n\n    layer: node\n        Node to convert.\n\n    module: module\n        An module for MXNet\n\n    builder: NeuralNetworkBuilder\n        A neural network builder object.\n    \"\"\"\n\n    input_name, output_name = _get_input_output_name(net, node)\n    name = node['name']\n    inputs = node['inputs']\n    args, _ = module.get_params()\n    mx_non_linearity = _get_attrs(node)['act_type']\n    if mx_non_linearity == 'elu':\n        non_linearity = 'ELU'\n        slope = _get_attrs(node)['slope'] if 'slope' in _get_attrs(node) else 0.25\n        params = slope\n    elif mx_non_linearity == 'leaky':\n        non_linearity = 'LEAKYRELU'\n        slope = _get_attrs(node)['slope'] if 'slope' in _get_attrs(node) else 0.25\n        params = [slope]\n    elif mx_non_linearity == 'prelu':\n        non_linearity = 'PRELU'\n        params = args[_get_node_name(net, inputs[1][0])].asnumpy()\n    else:\n        raise TypeError('Unknown activation type %s' % mx_non_linearity)\n    builder.add_activation(name = name,\n                           non_linearity = non_linearity,\n                           input_name = input_name,\n                           output_name = output_name,\n                           params = params)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef convert_elementwise_add(net, node, module, builder):\n\n    input_names, output_name = _get_input_output_name(net, node, [0, 1])\n    name = node['name']\n\n    builder.add_elementwise(name, input_names, output_name, 'ADD')", "response": "Convert an elementwise add layer from mxnet to coreml."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef convert_convolution(net, node, module, builder):\n    input_name, output_name = _get_input_output_name(net, node)\n    name = node['name']\n    param = _get_attrs(node)\n    inputs = node['inputs']\n    args, _ = module.get_params()\n\n    if 'no_bias' in param.keys():\n        has_bias = not literal_eval(param['no_bias'])\n    else:\n        has_bias = True\n\n    if 'pad' in param.keys() and literal_eval(param['pad']) != (0, 0):\n        pad = literal_eval(param['pad'])\n        builder.add_padding(\n            name=name+\"_pad\",\n            left=pad[1],\n            right=pad[1],\n            top=pad[0],\n            bottom=pad[0],\n            value=0,\n            input_name=input_name,\n            output_name=name+\"_pad_output\")\n        input_name = name+\"_pad_output\"\n\n    border_mode = \"valid\"\n\n    n_filters = int(param['num_filter'])\n    n_groups = int(param['num_group']) if 'num_group' in param else 1\n\n    W = args[_get_node_name(net, inputs[1][0])].asnumpy()\n    if has_bias:\n        Wb = args[_get_node_name(net, inputs[2][0])].asnumpy()\n    else:\n        Wb = None\n\n    channels = W.shape[1]\n\n    stride_height = 1\n    stride_width = 1\n    if 'stride' in param.keys():\n        stride_height, stride_width = literal_eval(param['stride'])\n\n    kernel_height, kernel_width = literal_eval(param['kernel'])\n\n    W = W.transpose((2, 3, 1, 0))\n    builder.add_convolution(\n        name=name,\n        kernel_channels=channels,\n        output_channels=n_filters,\n        height=kernel_height,\n        width=kernel_width,\n        stride_height=stride_height,\n        stride_width=stride_width,\n        border_mode=border_mode,\n        groups=n_groups,\n        W=W,\n        b=Wb,\n        has_bias=has_bias,\n        is_deconv=False,\n        output_shape=None,\n        input_name=input_name,\n        output_name=output_name)", "response": "Convert a convolution layer from mxnet to coreml."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a pooling layer from mxnet to coreml.", "response": "def convert_pooling(net, node, module, builder):\n    \"\"\"Convert a pooling layer from mxnet to coreml.\n\n    Parameters\n    ----------\n    network: net\n        A mxnet network object.\n\n    layer: node\n        Node to convert.\n\n    module: module\n        An module for MXNet\n\n    builder: NeuralNetworkBuilder\n        A neural network builder object.\n    \"\"\"\n    input_name, output_name = _get_input_output_name(net, node)\n    name = node['name']\n    param = _get_attrs(node)\n\n    layer_type_mx = param['pool_type']\n    if layer_type_mx == 'max':\n        layer_type = 'MAX'\n    elif layer_type_mx == 'avg':\n        layer_type = 'AVERAGE'\n    else:\n        raise TypeError(\"Pooling type %s not supported\" % layer_type_mx)\n\n    # Add padding if there is any\n    if 'pad' in param.keys() and literal_eval(param['pad']) != (0, 0):\n        pad = literal_eval(param['pad'])\n        builder.add_padding(\n            name=name+\"_pad\",\n            left=pad[1],\n            right=pad[1],\n            top=pad[0],\n            bottom=pad[0],\n            value=0,\n            input_name=input_name,\n            output_name=name+\"_pad_output\")\n        input_name = name+\"_pad_output\"\n\n    stride_height = 1\n    stride_width = 1\n    if 'stride' in param.keys():\n        stride_height, stride_width = literal_eval(param['stride'])\n\n    kernel_width, kernel_height = literal_eval(param['kernel'])\n\n    type_map = {'valid': 'VALID', 'full': 'INCLUDE_LAST_PIXEL'}\n    padding_type = param['pooling_convention'] if 'pooling_convention' in param else 'valid'\n    if padding_type not in type_map:\n        raise KeyError(\"%s type is not supported in this converter. It is a Github issue.\")\n    padding_type = type_map[padding_type]\n\n    if 'global_pool' in param.keys():\n        is_global = literal_eval(param['global_pool'])\n    else:\n        is_global = False\n\n    # For reasons why we are not using the standard builder but having our own implementation,\n    # see the function documentation.\n    _add_pooling.add_pooling_with_padding_types(\n        builder=builder,\n        name=name,\n        height=kernel_height,\n        width=kernel_width,\n        stride_height=stride_height,\n        stride_width=stride_width,\n        layer_type=layer_type,\n        padding_type=padding_type,\n        exclude_pad_area=False,\n        is_global=is_global,\n        input_name=input_name,\n        output_name=output_name\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a batchnorm layer from mxnet to coreml.", "response": "def convert_batchnorm(net, node, module, builder):\n    \"\"\"Convert a batchnorm layer from mxnet to coreml.\n\n    Parameters\n    ----------\n    network: net\n        A mxnet network object.\n\n    layer: node\n        Node to convert.\n\n    module: module\n        An module for MXNet\n\n    builder: NeuralNetworkBuilder\n        A neural network builder object.\n    \"\"\"\n    input_name, output_name = _get_input_output_name(net, node)\n    name = node['name']\n    inputs = node['inputs']\n\n\n    eps = 1e-3  # Default value of eps for MXNet.\n    use_global_stats = False  # Default value of use_global_stats for MXNet.\n    fix_gamma = True  # Default value of fix_gamma for MXNet.\n    attrs = _get_attrs(node)\n    if 'eps' in attrs:\n        eps = literal_eval(attrs['eps'])\n    if 'fix_gamma' in attrs:\n        fix_gamma = literal_eval(attrs['fix_gamma'])\n\n    args, aux = module.get_params()\n    gamma = args[_get_node_name(net, inputs[1][0])].asnumpy()\n    beta = args[_get_node_name(net, inputs[2][0])].asnumpy()\n    mean = aux[_get_node_name(net, inputs[3][0])].asnumpy()\n    variance = aux[_get_node_name(net, inputs[4][0])].asnumpy()\n    nb_channels = gamma.shape[0]\n    if fix_gamma:\n        gamma.fill(1.)\n    builder.add_batchnorm(\n        name=name,\n        channels=nb_channels,\n        gamma=gamma,\n        beta=beta,\n        mean=mean,\n        variance=variance,\n        input_name=input_name,\n        output_name=output_name,\n        epsilon=eps)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a concat layer from mxnet to coreml.", "response": "def convert_concat(net, node, module, builder):\n    \"\"\"Convert concat layer from mxnet to coreml.\n\n    Parameters\n    ----------\n    network: net\n        A mxnet network object.\n\n    layer: node\n        Node to convert.\n\n    module: module\n        An module for MXNet\n\n    builder: NeuralNetworkBuilder\n        A neural network builder object.\n    \"\"\"\n    # Get input and output names\n    input_names, output_name = _get_input_output_name(net, node, 'all')\n    name = node['name']\n    mode = 'CONCAT'\n    builder.add_elementwise(name = name, input_names = input_names,\n            output_name = output_name, mode = mode)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dmlc_opts(opts):\n    args = ['--num-workers', str(opts.num_workers),\n            '--num-servers', str(opts.num_servers),\n            '--cluster', opts.launcher,\n            '--host-file', opts.hostfile,\n            '--sync-dst-dir', opts.sync_dst_dir]\n\n    # convert to dictionary\n    dopts = vars(opts)\n    for key in ['env_server', 'env_worker', 'env']:\n        for v in dopts[key]:\n            args.append('--' + key.replace(\"_\",\"-\"))\n            args.append(v)\n    args += opts.command\n    try:\n        from dmlc_tracker import opts\n    except ImportError:\n        print(\"Can't load dmlc_tracker package.  Perhaps you need to run\")\n        print(\"    git submodule update --init --recursive\")\n        raise\n    dmlc_opts = opts.get_opts(args)\n    return dmlc_opts", "response": "convert from mxnet s opts to dmlc s opts\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _unfuse(self):\n        assert not self._projection_size, \"_unfuse does not support projection layer yet!\"\n        assert not self._lstm_state_clip_min and not self._lstm_state_clip_max, \\\n                \"_unfuse does not support state clipping yet!\"\n        get_cell = {'rnn_relu': lambda **kwargs: rnn_cell.RNNCell(self._hidden_size,\n                                                                  activation='relu',\n                                                                  **kwargs),\n                    'rnn_tanh': lambda **kwargs: rnn_cell.RNNCell(self._hidden_size,\n                                                                  activation='tanh',\n                                                                  **kwargs),\n                    'lstm': lambda **kwargs: rnn_cell.LSTMCell(self._hidden_size,\n                                                               **kwargs),\n                    'gru': lambda **kwargs: rnn_cell.GRUCell(self._hidden_size,\n                                                             **kwargs)}[self._mode]\n\n        stack = rnn_cell.HybridSequentialRNNCell(prefix=self.prefix, params=self.params)\n        with stack.name_scope():\n            ni = self._input_size\n            for i in range(self._num_layers):\n                kwargs = {'input_size': ni,\n                          'i2h_weight_initializer': self._i2h_weight_initializer,\n                          'h2h_weight_initializer': self._h2h_weight_initializer,\n                          'i2h_bias_initializer': self._i2h_bias_initializer,\n                          'h2h_bias_initializer': self._h2h_bias_initializer}\n                if self._dir == 2:\n                    stack.add(rnn_cell.BidirectionalCell(\n                        get_cell(prefix='l%d_'%i, **kwargs),\n                        get_cell(prefix='r%d_'%i, **kwargs)))\n                else:\n                    stack.add(get_cell(prefix='l%d_'%i, **kwargs))\n\n                if self._dropout > 0 and i != self._num_layers - 1:\n                    stack.add(rnn_cell.DropoutCell(self._dropout))\n\n                ni = self._hidden_size * self._dir\n\n        return stack", "response": "Unfuses the fused RNN in to a stack of rnn cells."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef begin_state(self, batch_size=0, func=ndarray.zeros, **kwargs):\n        states = []\n        for i, info in enumerate(self.state_info(batch_size)):\n            if info is not None:\n                info.update(kwargs)\n            else:\n                info = kwargs\n            states.append(func(name='%sh0_%d'%(self.prefix, i), **info))\n        return states", "response": "Begin a state for this cell."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nforward using CUDNN CPU kenrel", "response": "def _forward_kernel(self, F, inputs, states, **kwargs):\n        \"\"\" forward using CUDNN or CPU kenrel\"\"\"\n        if self._layout == 'NTC':\n            inputs = F.swapaxes(inputs, dim1=0, dim2=1)\n        if self._projection_size is None:\n            params = (kwargs['{}{}_{}_{}'.format(d, l, g, t)].reshape(-1)\n                      for t in ['weight', 'bias']\n                      for l in range(self._num_layers)\n                      for d in ['l', 'r'][:self._dir]\n                      for g in ['i2h', 'h2h'])\n        else:\n            params = (kwargs['{}{}_{}_{}'.format(d, l, g, t)].reshape(-1)\n                      for t in ['weight', 'bias']\n                      for l in range(self._num_layers)\n                      for d in ['l', 'r'][:self._dir]\n                      for g in ['i2h', 'h2h', 'h2r']\n                      if g != 'h2r' or t != 'bias')\n\n        params = F._internal._rnn_param_concat(*params, dim=0)\n\n        rnn = F.RNN(inputs, params, *states, state_size=self._hidden_size,\n                    projection_size=self._projection_size,\n                    num_layers=self._num_layers, bidirectional=self._dir == 2,\n                    p=self._dropout, state_outputs=True, mode=self._mode,\n                    lstm_state_clip_min=self._lstm_state_clip_min,\n                    lstm_state_clip_max=self._lstm_state_clip_max,\n                    lstm_state_clip_nan=self._lstm_state_clip_nan)\n\n        if self._mode == 'lstm':\n            outputs, states = rnn[0], [rnn[1], rnn[2]]\n        else:\n            outputs, states = rnn[0], [rnn[1]]\n\n        if self._layout == 'NTC':\n            outputs = F.swapaxes(outputs, dim1=0, dim2=1)\n\n        return outputs, states"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwait for SSH connection to appear in a network service.", "response": "def wait_ssh_open(server, port, keep_waiting=None, timeout=None):\n    \"\"\" Wait for network service to appear\n        @param server: host to connect to (str)\n        @param port: port (int)\n        @param timeout: in seconds, if None or 0 wait forever\n        @return: True of False, if timeout is None may return only True or\n                 throw unhandled network exception\n    \"\"\"\n    import socket\n    import errno\n    import time\n    log = logging.getLogger('wait_ssh_open')\n    sleep_s = 1\n    if timeout:\n        from time import time as now\n        # time module is needed to calc timeout shared between two exceptions\n        end = now() + timeout\n\n    while True:\n        log.debug(\"Sleeping for %s second(s)\", sleep_s)\n        time.sleep(sleep_s)\n        s = socket.socket()\n        try:\n            if keep_waiting and not keep_waiting():\n                log.debug(\"keep_waiting() is set and evaluates to False\")\n                return False\n\n            if timeout:\n                next_timeout = end - now()\n                if next_timeout < 0:\n                    log.debug(\"connect time out\")\n                    return False\n                else:\n                    log.debug(\"connect timeout %d s\", next_timeout)\n                    s.settimeout(next_timeout)\n\n            log.debug(\"connect %s:%d\", server, port)\n            s.connect((server, port))\n            ret = s.recv(1024).decode()\n            if ret and ret.startswith('SSH'):\n                s.close()\n                log.info(\"wait_ssh_open: port %s:%s is open and ssh is ready\", server, port)\n                return True\n            else:\n                log.debug(\"Didn't get the SSH banner\")\n                s.close()\n\n        except ConnectionError as err:\n            log.debug(\"ConnectionError %s\", err)\n            if sleep_s == 0:\n                sleep_s = 1\n            else:\n                sleep_s *= 2\n\n        except socket.gaierror as err:\n            log.debug(\"gaierror %s\",err)\n            return False\n\n        except socket.timeout as err:\n            # this exception occurs only if timeout is set\n            if timeout:\n                return False\n\n        except TimeoutError as err:\n            # catch timeout exception from underlying network library\n            # this one is different from socket.timeout\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef wait_port_open(server, port, timeout=None):\n    import socket\n    import errno\n    import time\n    sleep_s = 0\n    if timeout:\n        from time import time as now\n        # time module is needed to calc timeout shared between two exceptions\n        end = now() + timeout\n\n    while True:\n        logging.debug(\"Sleeping for %s second(s)\", sleep_s)\n        time.sleep(sleep_s)\n        s = socket.socket()\n        try:\n            if timeout:\n                next_timeout = end - now()\n                if next_timeout < 0:\n                    return False\n                else:\n                    s.settimeout(next_timeout)\n\n            logging.info(\"connect %s %d\", server, port)\n            s.connect((server, port))\n\n        except ConnectionError as err:\n            logging.debug(\"ConnectionError %s\", err)\n            if sleep_s == 0:\n                sleep_s = 1\n\n        except socket.gaierror as err:\n            logging.debug(\"gaierror %s\",err)\n            return False\n\n        except socket.timeout as err:\n            # this exception occurs only if timeout is set\n            if timeout:\n                return False\n\n        except TimeoutError as err:\n            # catch timeout exception from underlying network library\n            # this one is different from socket.timeout\n            raise\n\n        else:\n            s.close()\n            logging.info(\"wait_port_open: port %s:%s is open\", server, port)\n            return True", "response": "Wait for a network service to appear\nAttributeNames."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprinting a summary of the log entries in a single line.", "response": "def print_summary(symbol, shape=None, line_length=120, positions=[.44, .64, .74, 1.]):\n    \"\"\"Convert symbol for detail information.\n\n    Parameters\n    ----------\n    symbol: Symbol\n        Symbol to be visualized.\n    shape: dict\n        A dict of shapes, str->shape (tuple), given input shapes.\n    line_length: int\n        Rotal length of printed lines\n    positions: list\n        Relative or absolute positions of log elements in each line.\n\n    Returns\n    ------\n    None\n\n    Notes\n    -----\n    If ``mxnet`` is imported, the visualization module can be used in its short-form.\n    For example, if we ``import mxnet`` as follows::\n\n        import mxnet\n\n    this method in visualization module can be used in its short-form as::\n\n        mxnet.viz.print_summary(...)\n\n    \"\"\"\n    if not isinstance(symbol, Symbol):\n        raise TypeError(\"symbol must be Symbol\")\n    show_shape = False\n    if shape is not None:\n        show_shape = True\n        interals = symbol.get_internals()\n        _, out_shapes, _ = interals.infer_shape(**shape)\n        if out_shapes is None:\n            raise ValueError(\"Input shape is incomplete\")\n        shape_dict = dict(zip(interals.list_outputs(), out_shapes))\n    conf = json.loads(symbol.tojson())\n    nodes = conf[\"nodes\"]\n    heads = set(conf[\"heads\"][0])\n    if positions[-1] <= 1:\n        positions = [int(line_length * p) for p in positions]\n    # header names for the different log elements\n    to_display = ['Layer (type)', 'Output Shape', 'Param #', 'Previous Layer']\n    def print_row(fields, positions):\n        \"\"\"Print format row.\n\n        Parameters\n        ----------\n        fields: list\n            Information field.\n        positions: list\n            Field length ratio.\n        Returns\n        ------\n        None\n        \"\"\"\n        line = ''\n        for i, field in enumerate(fields):\n            line += str(field)\n            line = line[:positions[i]]\n            line += ' ' * (positions[i] - len(line))\n        print(line)\n    print('_' * line_length)\n    print_row(to_display, positions)\n    print('=' * line_length)\n    def print_layer_summary(node, out_shape):\n        \"\"\"print layer information\n\n        Parameters\n        ----------\n        node: dict\n            Node information.\n        out_shape: dict\n            Node shape information.\n        Returns\n        ------\n            Node total parameters.\n        \"\"\"\n        op = node[\"op\"]\n        pre_node = []\n        pre_filter = 0\n        if op != \"null\":\n            inputs = node[\"inputs\"]\n            for item in inputs:\n                input_node = nodes[item[0]]\n                input_name = input_node[\"name\"]\n                if input_node[\"op\"] != \"null\" or item[0] in heads:\n                    # add precede\n                    pre_node.append(input_name)\n                    if show_shape:\n                        if input_node[\"op\"] != \"null\":\n                            key = input_name + \"_output\"\n                        else:\n                            key = input_name\n                        if key in shape_dict:\n                            shape = shape_dict[key][1:]\n                            pre_filter = pre_filter + int(shape[0])\n        cur_param = 0\n        if op == 'Convolution':\n            if \"no_bias\" in node[\"attrs\"] and node[\"attrs\"][\"no_bias\"] == 'True':\n                num_group = int(node['attrs'].get('num_group', '1'))\n                cur_param = pre_filter * int(node[\"attrs\"][\"num_filter\"]) \\\n                   // num_group\n                for k in _str2tuple(node[\"attrs\"][\"kernel\"]):\n                    cur_param *= int(k)\n            else:\n                num_group = int(node['attrs'].get('num_group', '1'))\n                cur_param = pre_filter * int(node[\"attrs\"][\"num_filter\"]) \\\n                   // num_group\n                for k in _str2tuple(node[\"attrs\"][\"kernel\"]):\n                    cur_param *= int(k)\n                cur_param += int(node[\"attrs\"][\"num_filter\"])\n        elif op == 'FullyConnected':\n            if \"no_bias\" in node[\"attrs\"] and node[\"attrs\"][\"no_bias\"] == 'True':\n                cur_param = pre_filter * int(node[\"attrs\"][\"num_hidden\"])\n            else:\n                cur_param = (pre_filter+1) * int(node[\"attrs\"][\"num_hidden\"])\n        elif op == 'BatchNorm':\n            key = node[\"name\"] + \"_output\"\n            if show_shape:\n                num_filter = shape_dict[key][1]\n                cur_param = int(num_filter) * 2\n        elif op == 'Embedding':\n            cur_param = int(node[\"attrs\"]['input_dim']) * int(node[\"attrs\"]['output_dim'])\n        if not pre_node:\n            first_connection = ''\n        else:\n            first_connection = pre_node[0]\n        fields = [node['name'] + '(' + op + ')',\n                  \"x\".join([str(x) for x in out_shape]),\n                  cur_param,\n                  first_connection]\n        print_row(fields, positions)\n        if len(pre_node) > 1:\n            for i in range(1, len(pre_node)):\n                fields = ['', '', '', pre_node[i]]\n                print_row(fields, positions)\n        return cur_param\n    total_params = 0\n    for i, node in enumerate(nodes):\n        out_shape = []\n        op = node[\"op\"]\n        if op == \"null\" and i > 0:\n            continue\n        if op != \"null\" or i in heads:\n            if show_shape:\n                if op != \"null\":\n                    key = node[\"name\"] + \"_output\"\n                else:\n                    key = node[\"name\"]\n                if key in shape_dict:\n                    out_shape = shape_dict[key][1:]\n        total_params += print_layer_summary(nodes[i], out_shape)\n        if i == len(nodes) - 1:\n            print('=' * line_length)\n        else:\n            print('_' * line_length)\n    print(\"Total params: {params}\".format(params=total_params))\n    print('_' * line_length)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef plot_network(symbol, title=\"plot\", save_format='pdf', shape=None, dtype=None, node_attrs={},\n                 hide_weights=True):\n    \"\"\"Creates a visualization (Graphviz digraph object) of the given computation graph.\n    Graphviz must be installed for this function to work.\n\n    Parameters\n    ----------\n    title: str, optional\n        Title of the generated visualization.\n    symbol: Symbol\n        A symbol from the computation graph. The generated digraph will visualize the part\n        of the computation graph required to compute `symbol`.\n    shape: dict, optional\n        Specifies the shape of the input tensors. If specified, the visualization will include\n        the shape of the tensors between the nodes. `shape` is a dictionary mapping\n        input symbol names (str) to the corresponding tensor shape (tuple).\n    dtype: dict, optional\n        Specifies the type of the input tensors. If specified, the visualization will include\n        the type of the tensors between the nodes. `dtype` is a dictionary mapping\n        input symbol names (str) to the corresponding tensor type (e.g. `numpy.float32`).\n    node_attrs: dict, optional\n        Specifies the attributes for nodes in the generated visualization. `node_attrs` is\n        a dictionary of Graphviz attribute names and values. For example::\n\n            node_attrs={\"shape\":\"oval\",\"fixedsize\":\"false\"}\n\n        will use oval shape for nodes and allow variable sized nodes in the visualization.\n    hide_weights: bool, optional\n        If True (default), then inputs with names of form *_weight* (corresponding to weight\n        tensors) or *_bias* (corresponding to bias vectors) will be hidden for a cleaner\n        visualization.\n\n    Returns\n    -------\n    dot: Digraph\n        A Graphviz digraph object visualizing the computation graph to compute `symbol`.\n\n    Example\n    -------\n    >>> net = mx.sym.Variable('data')\n    >>> net = mx.sym.FullyConnected(data=net, name='fc1', num_hidden=128)\n    >>> net = mx.sym.Activation(data=net, name='relu1', act_type=\"relu\")\n    >>> net = mx.sym.FullyConnected(data=net, name='fc2', num_hidden=10)\n    >>> net = mx.sym.SoftmaxOutput(data=net, name='out')\n    >>> digraph = mx.viz.plot_network(net, shape={'data':(100,200)},\n    ... node_attrs={\"fixedsize\":\"false\"})\n    >>> digraph.view()\n\n    Notes\n    -----\n    If ``mxnet`` is imported, the visualization module can be used in its short-form.\n    For example, if we ``import mxnet`` as follows::\n\n        import mxnet\n\n    this method in visualization module can be used in its short-form as::\n\n        mxnet.viz.plot_network(...)\n\n    \"\"\"\n    # todo add shape support\n    try:\n        from graphviz import Digraph\n    except:\n        raise ImportError(\"Draw network requires graphviz library\")\n    if not isinstance(symbol, Symbol):\n        raise TypeError(\"symbol must be a Symbol\")\n    internals = symbol.get_internals()\n    draw_shape = shape is not None\n    if draw_shape:\n        _, out_shapes, _ = internals.infer_shape(**shape)\n        if out_shapes is None:\n            raise ValueError(\"Input shape is incomplete\")\n        shape_dict = dict(zip(internals.list_outputs(), out_shapes))\n    draw_type = dtype is not None\n    if draw_type:\n        _, out_types, _ = internals.infer_type(**dtype)\n        if out_types is None:\n            raise ValueError(\"Input type is incomplete\")\n        type_dict = dict(zip(internals.list_outputs(), out_types))\n    conf = json.loads(symbol.tojson())\n    nodes = conf[\"nodes\"]\n    # check if multiple nodes have the same name\n    if len(nodes) != len(set([node[\"name\"] for node in nodes])):\n        seen_nodes = set()\n        # find all repeated names\n        repeated = set(node['name'] for node in nodes if node['name'] in seen_nodes\n                       or seen_nodes.add(node['name']))\n        warning_message = \"There are multiple variables with the same name in your graph, \" \\\n                          \"this may result in cyclic graph. Repeated names: \" + ','.join(repeated)\n        warnings.warn(warning_message, RuntimeWarning)\n    # default attributes of node\n    node_attr = {\"shape\": \"box\", \"fixedsize\": \"true\",\n                 \"width\": \"1.3\", \"height\": \"0.8034\", \"style\": \"filled\"}\n    # merge the dict provided by user and the default one\n    node_attr.update(node_attrs)\n    dot = Digraph(name=title, format=save_format)\n    # color map\n    cm = (\"#8dd3c7\", \"#fb8072\", \"#ffffb3\", \"#bebada\", \"#80b1d3\",\n          \"#fdb462\", \"#b3de69\", \"#fccde5\")\n\n    def looks_like_weight(name):\n        \"\"\"Internal helper to figure out if node should be hidden with `hide_weights`.\n        \"\"\"\n        weight_like = ('_weight', '_bias', '_beta', '_gamma',\n                       '_moving_var', '_moving_mean', '_running_var', '_running_mean')\n        return name.endswith(weight_like)\n\n    # make nodes\n    hidden_nodes = set()\n    for node in nodes:\n        op = node[\"op\"]\n        name = node[\"name\"]\n        # input data\n        attr = copy.deepcopy(node_attr)\n        label = name\n\n        if op == \"null\":\n            if looks_like_weight(node[\"name\"]):\n                if hide_weights:\n                    hidden_nodes.add(node[\"name\"])\n                # else we don't render a node, but\n                # don't add it to the hidden_nodes set\n                # so it gets rendered as an empty oval\n                continue\n            attr[\"shape\"] = \"oval\" # inputs get their own shape\n            label = node[\"name\"]\n            attr[\"fillcolor\"] = cm[0]\n        elif op == \"Convolution\":\n            label = \"Convolution\\n{kernel}/{stride}, {filter}\".format(\n                kernel=\"x\".join(_str2tuple(node[\"attrs\"][\"kernel\"])),\n                stride=\"x\".join(_str2tuple(node[\"attrs\"][\"stride\"]))\n                if \"stride\" in node[\"attrs\"] else \"1\",\n                filter=node[\"attrs\"][\"num_filter\"]\n            )\n            attr[\"fillcolor\"] = cm[1]\n        elif op == \"FullyConnected\":\n            label = \"FullyConnected\\n{hidden}\".format(hidden=node[\"attrs\"][\"num_hidden\"])\n            attr[\"fillcolor\"] = cm[1]\n        elif op == \"BatchNorm\":\n            attr[\"fillcolor\"] = cm[3]\n        elif op == 'Activation':\n            act_type = node[\"attrs\"][\"act_type\"]\n            label = 'Activation\\n{activation}'.format(activation=act_type)\n            attr[\"fillcolor\"] = cm[2]\n        elif op == 'LeakyReLU':\n            attrs = node.get(\"attrs\")\n            act_type = attrs.get(\"act_type\", \"Leaky\") if attrs else \"Leaky\"\n            label = 'LeakyReLU\\n{activation}'.format(activation=act_type)\n            attr[\"fillcolor\"] = cm[2]\n        elif op == \"Pooling\":\n            label = \"Pooling\\n{pooltype}, {kernel}/{stride}\".format(pooltype=node[\"attrs\"][\"pool_type\"],\n                                                                    kernel=\"x\".join(_str2tuple(node[\"attrs\"][\"kernel\"]))\n                                                                    if \"kernel\" in node[\"attrs\"] else \"[]\",\n                                                                    stride=\"x\".join(_str2tuple(node[\"attrs\"][\"stride\"]))\n                                                                    if \"stride\" in node[\"attrs\"] else \"1\")\n            attr[\"fillcolor\"] = cm[4]\n        elif op in (\"Concat\", \"Flatten\", \"Reshape\"):\n            attr[\"fillcolor\"] = cm[5]\n        elif op == \"Softmax\":\n            attr[\"fillcolor\"] = cm[6]\n        else:\n            attr[\"fillcolor\"] = cm[7]\n            if op == \"Custom\":\n                label = node[\"attrs\"][\"op_type\"]\n\n        dot.node(name=name, label=label, **attr)\n\n    # add edges\n    for node in nodes:          # pylint: disable=too-many-nested-blocks\n        op = node[\"op\"]\n        name = node[\"name\"]\n        if op == \"null\":\n            continue\n        else:\n            inputs = node[\"inputs\"]\n            for item in inputs:\n                input_node = nodes[item[0]]\n                input_name = input_node[\"name\"]\n                if input_name not in hidden_nodes:\n                    attr = {\"dir\": \"back\", 'arrowtail':'open', 'label': ''}\n                    # add shapes\n                    if draw_shape:\n                        if input_node[\"op\"] != \"null\":\n                            key = input_name + \"_output\"\n                            if \"attrs\" in input_node:\n                                params = input_node[\"attrs\"]\n                                if \"num_outputs\" in params:\n                                    key += str(int(params[\"num_outputs\"]) - 1)\n                            shape = shape_dict[key][1:]\n                            label = \"x\".join([str(x) for x in shape])\n                            attr[\"label\"] = label\n                        else:\n                            key = input_name\n                            shape = shape_dict[key][1:]\n                            label = \"x\".join([str(x) for x in shape])\n                            attr[\"label\"] = label\n                    if draw_type:\n                        if input_node[\"op\"] != \"null\":\n                            key = input_name + \"_output\"\n                            if \"attrs\" in input_node:\n                                params = input_node[\"attrs\"]\n                                if \"num_outputs\" in params:\n                                    key += str(int(params[\"num_outputs\"]) - 1)\n                            dtype = type_dict[key]\n                            attr[\"label\"] += '(' + dtype.__name__ + ')'\n                        else:\n                            key = input_name\n                            dtype = type_dict[key]\n                            attr[\"label\"] += '(' + dtype.__name__ + ')'\n                    dot.edge(tail_name=name, head_name=input_name, **attr)\n\n    return dot", "response": "Creates a visualization of the given symbol in the computation graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef evaluate_accuracy(data_iterator, network):\n    acc = mx.metric.Accuracy()\n\n    # Iterate through data and label\n    for i, (data, label) in enumerate(data_iterator):\n\n        # Get the data and label into the GPU\n        data = data.as_in_context(ctx[0])\n        label = label.as_in_context(ctx[0])\n\n        # Get network's output which is a probability distribution\n        # Apply argmax on the probability distribution to get network's classification.\n        output = network(data)\n        predictions = nd.argmax(output, axis=1)\n\n        # Give network's prediction and the correct label to update the metric\n        acc.update(preds=predictions, labels=label)\n\n    # Return the accuracy\n    return acc.get()[1]", "response": "Evaluate the accuracy of ResNet\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntrains a batch of data and label for a single GPU.", "response": "def train_batch(batch_list, context, network, gluon_trainer):\n    \"\"\" Training with multiple GPUs\n\n    Parameters\n    ----------\n    batch_list: List\n      list of dataset\n    context: List\n      a list of all GPUs to be used for training\n    network:\n      ResNet\n    gluon_trainer:\n      rain module of gluon\n    \"\"\"\n    # Split and load data into multiple GPUs\n    data = batch_list[0]\n    data = gluon.utils.split_and_load(data, context)\n\n    # Split and load label into multiple GPUs\n    label = batch_list[1]\n    label = gluon.utils.split_and_load(label, context)\n\n    # Run the forward and backward pass\n    forward_backward(network, data, label)\n\n    # Update the parameters\n    this_batch_size = batch_list[0].shape[0]\n    gluon_trainer.step(this_batch_size)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting an optimized version of the given executor s underlying symbol graph and return its optimized version.", "response": "def get_optimized_symbol(executor):\n    \"\"\"\n    Take an executor's underlying symbol graph and return its generated optimized version.\n\n    Parameters\n    ----------\n    executor :\n        An executor for which you want to see an optimized symbol. Getting an optimized symbol\n        is useful to compare and verify the work TensorRT has done against a legacy behaviour.\n\n    Returns\n    -------\n    symbol : nnvm::Symbol\n        The nnvm symbol optimized.\n    \"\"\"\n    handle = SymbolHandle()\n    try:\n        check_call(_LIB.MXExecutorGetOptimizedSymbol(executor.handle, ctypes.byref(handle)))\n        result = sym.Symbol(handle=handle)\n        return result\n    except MXNetError:\n        logging.error('Error while trying to fetch TRT optimized symbol for graph. Please ensure '\n                      'build was compiled with MXNET_USE_TENSORRT enabled.')\n        raise"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbinding a symbol to an optimized TensorRT executor.", "response": "def tensorrt_bind(symbol, ctx, all_params, type_dict=None, stype_dict=None, group2ctx=None,\n                  **kwargs):\n    \"\"\"Bind current symbol to get an optimized trt executor.\n\n    Parameters\n    ----------\n    symbol : Symbol\n        The symbol you wish to bind, and optimize with TensorRT.\n\n    ctx : Context\n        The device context the generated executor to run on.\n\n    all_params : Dict of str->ndarray\n        A dictionary of mappings from parameter names to parameter NDArrays.\n\n    type_dict  : Dict of str->numpy.dtype\n        Input type dictionary, name->dtype\n\n    stype_dict  : Dict of str->str\n        Input storage type dictionary, name->storage_type\n\n    group2ctx : Dict of string to mx.Context\n        The dict mapping the `ctx_group` attribute to the context assignment.\n\n    kwargs : Dict of str->shape\n        Input shape dictionary, name->shape\n\n    Returns\n    -------\n    executor : mxnet.Executor\n        An optimized TensorRT executor.\n    \"\"\"\n    kwargs['shared_buffer'] = all_params\n    return symbol.simple_bind(ctx, type_dict=type_dict, stype_dict=stype_dict,\n                              group2ctx=group2ctx, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a symbol for the densenet.", "response": "def get_symbol(num_classes, num_layers=11, batch_norm=False, dtype='float32', **kwargs):\n    \"\"\"\n    Parameters\n    ----------\n    num_classes : int, default 1000\n        Number of classification classes.\n    num_layers : int\n        Number of layers for the variant of densenet. Options are 11, 13, 16, 19.\n    batch_norm : bool, default False\n        Use batch normalization.\n    dtype: str, float32 or float16\n        Data precision.\n    \"\"\"\n    vgg_spec = {11: ([1, 1, 2, 2, 2], [64, 128, 256, 512, 512]),\n                13: ([2, 2, 2, 2, 2], [64, 128, 256, 512, 512]),\n                16: ([2, 2, 3, 3, 3], [64, 128, 256, 512, 512]),\n                19: ([2, 2, 4, 4, 4], [64, 128, 256, 512, 512])}\n    if num_layers not in vgg_spec:\n        raise ValueError(\"Invalide num_layers {}. Possible choices are 11,13,16,19.\".format(num_layers))\n    layers, filters = vgg_spec[num_layers]\n    data = mx.sym.Variable(name=\"data\")\n    if dtype == 'float16':\n        data = mx.sym.Cast(data=data, dtype=np.float16)\n    feature = get_feature(data, layers, filters, batch_norm)\n    classifier = get_classifier(feature, num_classes)\n    if dtype == 'float16':\n        classifier = mx.sym.Cast(data=classifier, dtype=np.float32)\n    symbol = mx.sym.SoftmaxOutput(data=classifier, name='softmax')\n    return symbol"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a DataBatch of the same shape as the input image", "response": "def create_batch(self, frame):\n        \"\"\"\n        :param frame: an (w,h,channels) numpy array (image)\n        :return: DataBatch of (1,channels,data_shape,data_shape)\n        \"\"\"\n        frame_resize = mx.nd.array(cv2.resize(frame, (self.data_shape[0], self.data_shape[1])))\n        #frame_resize = mx.img.imresize(frame, self.data_shape[0], self.data_shape[1], cv2.INTER_LINEAR)\n        # Change dimensions from (w,h,channels) to (channels, w, h)\n        frame_t = mx.nd.transpose(frame_resize, axes=(2,0,1))\n        frame_norm = frame_t - self.mean_pixels_nd\n        # Add dimension for batch, results in (1,channels,w,h)\n        batch_frame = [mx.nd.expand_dims(frame_norm, axis=0)]\n        batch_shape = [DataDesc('data', batch_frame[0].shape)]\n        batch = DataBatch(data=batch_frame, provide_data=batch_shape)\n        return batch"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndetects all images in iterator and return a list of detections.", "response": "def detect_iter(self, det_iter, show_timer=False):\n        \"\"\"\n        detect all images in iterator\n\n        Parameters:\n        ----------\n        det_iter : DetIter\n            iterator for all testing images\n        show_timer : Boolean\n            whether to print out detection exec time\n\n        Returns:\n        ----------\n        list of detection results\n        \"\"\"\n        num_images = det_iter._size\n        if not isinstance(det_iter, mx.io.PrefetchingIter):\n            det_iter = mx.io.PrefetchingIter(det_iter)\n        start = timer()\n        detections = self.mod.predict(det_iter).asnumpy()\n        time_elapsed = timer() - start\n        if show_timer:\n            logging.info(\"Detection time for {} images: {:.4f} sec\".format(\n                num_images, time_elapsed))\n        result = Detector.filter_positive_detections(detections)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef detect_batch(self, batch):\n        self.mod.forward(batch, is_train=False)\n        detections = self.mod.get_outputs()[0]\n        positive_detections = Detector.filter_positive_detections(detections)\n        return positive_detections", "response": "Return detections for a batch."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwrapping for detecting multiple images in a single image store", "response": "def im_detect(self, im_list, root_dir=None, extension=None, show_timer=False):\n        \"\"\"\n        wrapper for detecting multiple images\n\n        Parameters:\n        ----------\n        im_list : list of str\n            image path or list of image paths\n        root_dir : str\n            directory of input images, optional if image path already\n            has full directory information\n        extension : str\n            image extension, eg. \".jpg\", optional\n\n        Returns:\n        ----------\n        list of detection results in format [det0, det1...], det is in\n        format np.array([id, score, xmin, ymin, xmax, ymax]...)\n        \"\"\"\n        test_db = TestDB(im_list, root_dir=root_dir, extension=extension)\n        test_iter = DetIter(test_db, 1, self.data_shape, self.mean_pixels,\n                            is_train=False)\n        return self.detect_iter(test_iter, show_timer)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef visualize_detection(self, img, dets, classes=[], thresh=0.6):\n        import matplotlib.pyplot as plt\n        import random\n        plt.imshow(img)\n        height = img.shape[0]\n        width = img.shape[1]\n        colors = dict()\n        for det in dets:\n            (klass, score, x0, y0, x1, y1) = det\n            if score < thresh:\n                continue\n            cls_id = int(klass)\n            if cls_id not in colors:\n                colors[cls_id] = (random.random(), random.random(), random.random())\n            xmin = int(x0 * width)\n            ymin = int(y0 * height)\n            xmax = int(x1 * width)\n            ymax = int(y1 * height)\n            rect = plt.Rectangle((xmin, ymin), xmax - xmin,\n                                 ymax - ymin, fill=False,\n                                 edgecolor=colors[cls_id],\n                                 linewidth=3.5)\n            plt.gca().add_patch(rect)\n            class_name = str(cls_id)\n            if classes and len(classes) > cls_id:\n                class_name = classes[cls_id]\n            plt.gca().text(xmin, ymin - 2,\n                            '{:s} {:.3f}'.format(class_name, score),\n                            bbox=dict(facecolor=colors[cls_id], alpha=0.5),\n                                    fontsize=12, color='white')\n        plt.show()", "response": "Visualize detections in one image."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef filter_positive_detections(detections):\n        class_idx = 0\n        assert(isinstance(detections, mx.nd.NDArray) or isinstance(detections, np.ndarray))\n        detections_per_image = []\n        # for each image\n        for i in range(detections.shape[0]):\n            result = []\n            det = detections[i, :, :]\n            for obj in det:\n                if obj[class_idx] >= 0:\n                    result.append(obj)\n            detections_per_image.append(result)\n        logging.info(\"%d positive detections\", len(result))\n        return detections_per_image", "response": "Filter detections that are positive."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef detect_and_visualize(self, im_list, root_dir=None, extension=None,\n                             classes=[], thresh=0.6, show_timer=False):\n        \"\"\"\n        wrapper for im_detect and visualize_detection\n\n        Parameters:\n        ----------\n        im_list : list of str or str\n            image path or list of image paths\n        root_dir : str or None\n            directory of input images, optional if image path already\n            has full directory information\n        extension : str or None\n            image extension, eg. \".jpg\", optional\n\n        Returns:\n        ----------\n\n        \"\"\"\n        dets = self.im_detect(im_list, root_dir, extension, show_timer=show_timer)\n        if not isinstance(im_list, list):\n            im_list = [im_list]\n        assert len(dets) == len(im_list)\n        for k, det in enumerate(dets):\n            img = cv2.imread(im_list[k])\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            self.visualize_detection(img, det, classes, thresh)", "response": "Wrapper for im_detect and visualize_detection"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun the caffe upgrade tool on the prototxt to create a new prototxt in the latest format", "response": "def process_network_proto(caffe_root, deploy_proto):\n    \"\"\"\n    Runs the caffe upgrade tool on the prototxt to create a prototxt in the latest format.\n    This enable us to work just with latest structures, instead of supporting all the variants\n\n    :param caffe_root: link to caffe root folder, where the upgrade tool is located\n    :param deploy_proto: name of the original prototxt file\n    :return: name of new processed prototxt file\n    \"\"\"\n    processed_deploy_proto = deploy_proto + \".processed\"\n\n    from shutil import copyfile\n    copyfile(deploy_proto, processed_deploy_proto)\n\n    # run upgrade tool on new file name (same output file)\n    import os\n    upgrade_tool_command_line = caffe_root + '/build/tools/upgrade_net_proto_text.bin ' \\\n                                + processed_deploy_proto + ' ' + processed_deploy_proto\n    os.system(upgrade_tool_command_line)\n\n    return processed_deploy_proto"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_network_dag(processed_deploy_prototxt):\n\n    from caffe.proto import caffe_pb2\n    from google.protobuf import text_format # pylint: disable=relative-import\n    from collections import OrderedDict\n\n    # load prototxt file\n    network_def = caffe_pb2.NetParameter()\n    with open(processed_deploy_prototxt, 'r') as proto_file:\n        text_format.Merge(str(proto_file.read()), network_def)\n\n    # map layer name to layer record\n    layer_name_to_record = OrderedDict()\n    for layer_def in network_def.layer:\n        if (len(layer_def.include) == 0) or \\\n           (caffe_pb2.TEST in [item.phase for item in layer_def.include]):\n\n            layer_name_to_record[layer_def.name] = LayerRecord(layer_def)\n\n    top_to_layers = dict()\n    for layer in network_def.layer:\n        # no specific phase, or TEST phase is specifically asked for\n        if (len(layer.include) == 0) or (caffe_pb2.TEST in [item.phase for item in layer.include]):\n            for top in layer.top:\n                if top not in top_to_layers:\n                    top_to_layers[top] = list()\n                top_to_layers[top].append(layer.name)\n\n    # find parents and children of all layers\n    for child_layer_name in layer_name_to_record.keys():  # pylint: disable=too-many-nested-blocks\n        child_layer_def = layer_name_to_record[child_layer_name]\n        for bottom in child_layer_def.bottoms:\n            if bottom in top_to_layers:\n                for parent_layer_name in top_to_layers[bottom]:\n                    if parent_layer_name in layer_name_to_record:\n                        parent_layer_def = layer_name_to_record[parent_layer_name]\n                        if parent_layer_def not in child_layer_def.parents:\n                            child_layer_def.parents.append(parent_layer_def)\n                        if child_layer_def not in parent_layer_def.children:\n                            parent_layer_def.children.append(child_layer_def)\n\n    # update filter, strid, pad for maxout \"structures\"\n    for layer_name in layer_name_to_record.keys():\n        layer_def = layer_name_to_record[layer_name]\n        if layer_def.type == 'Eltwise' and \\\n           len(layer_def.parents) == 1 and \\\n           layer_def.parents[0].type == 'Slice' and \\\n           len(layer_def.parents[0].parents) == 1 and \\\n           layer_def.parents[0].parents[0].type in ['Convolution', 'InnerProduct']:\n            layer_def.filter = layer_def.parents[0].parents[0].filter\n            layer_def.stride = layer_def.parents[0].parents[0].stride\n            layer_def.pad = layer_def.parents[0].parents[0].pad\n\n    return network_def, layer_name_to_record, top_to_layers", "response": "Reads from the caffe prototxt and returns the network structure and the top_to_layers dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread caffe formatted mean file and returns array of RGB image", "response": "def read_caffe_mean(caffe_mean_file):\n    \"\"\"\n    Reads caffe formatted mean file\n    :param caffe_mean_file: path to caffe mean file, presumably with 'binaryproto' suffix\n    :return: mean image, converted from BGR to RGB format\n    \"\"\"\n\n    import caffe_parser\n    import numpy as np\n    mean_blob = caffe_parser.caffe_pb2.BlobProto()\n    with open(caffe_mean_file, 'rb') as f:\n        mean_blob.ParseFromString(f.read())\n\n    img_mean_np = np.array(mean_blob.data)\n    img_mean_np = img_mean_np.reshape(mean_blob.channels, mean_blob.height, mean_blob.width)\n\n    # swap channels from Caffe BGR to RGB\n    img_mean_np[[0, 2], :, :] = img_mean_np[[2, 0], :, :]\n\n    return img_mean_np"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncross entropy loss with a mask", "response": "def cross_entropy_loss(inputs, labels, rescale_loss=1):\n    \"\"\" cross entropy loss with a mask \"\"\"\n    criterion = mx.gluon.loss.SoftmaxCrossEntropyLoss(weight=rescale_loss)\n    loss = criterion(inputs, labels)\n    mask = S.var('mask')\n    loss = loss * S.reshape(mask, shape=(-1,))\n    return S.make_loss(loss.mean())"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwording embedding + LSTM Projected", "response": "def rnn(bptt, vocab_size, num_embed, nhid, num_layers, dropout, num_proj, batch_size):\n    \"\"\" word embedding + LSTM Projected \"\"\"\n    state_names = []\n    data = S.var('data')\n    weight = S.var(\"encoder_weight\", stype='row_sparse')\n    embed = S.sparse.Embedding(data=data, weight=weight, input_dim=vocab_size,\n                               output_dim=num_embed, name='embed', sparse_grad=True)\n    states = []\n    outputs = S.Dropout(embed, p=dropout)\n    for i in range(num_layers):\n        prefix = 'lstmp%d_' % i\n        init_h = S.var(prefix + 'init_h', shape=(batch_size, num_proj), init=mx.init.Zero())\n        init_c = S.var(prefix + 'init_c', shape=(batch_size, nhid), init=mx.init.Zero())\n        state_names += [prefix + 'init_h', prefix + 'init_c']\n        lstmp = mx.gluon.contrib.rnn.LSTMPCell(nhid, num_proj, prefix=prefix)\n        outputs, next_states = lstmp.unroll(bptt, outputs, begin_state=[init_h, init_c], \\\n                                            layout='NTC', merge_outputs=True)\n        outputs = S.Dropout(outputs, p=dropout)\n        states += [S.stop_gradient(s) for s in next_states]\n    outputs = S.reshape(outputs, shape=(-1, num_proj))\n\n    trainable_lstm_args = []\n    for arg in outputs.list_arguments():\n        if 'lstmp' in arg and 'init' not in arg:\n            trainable_lstm_args.append(arg)\n    return outputs, states, trainable_lstm_args, state_names"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sampled_softmax(num_classes, num_samples, in_dim, inputs, weight, bias,\n                    sampled_values, remove_accidental_hits=True):\n        \"\"\" Sampled softmax via importance sampling.\n            This under-estimates the full softmax and is only used for training.\n        \"\"\"\n        # inputs = (n, in_dim)\n        sample, prob_sample, prob_target = sampled_values\n\n        # (num_samples, )\n        sample = S.var('sample', shape=(num_samples,), dtype='float32')\n        # (n, )\n        label = S.var('label')\n        label = S.reshape(label, shape=(-1,), name=\"label_reshape\")\n        # (num_samples+n, )\n        sample_label = S.concat(sample, label, dim=0)\n        # lookup weights and biases\n        # (num_samples+n, dim)\n        sample_target_w = S.sparse.Embedding(data=sample_label, weight=weight,\n                                             input_dim=num_classes, output_dim=in_dim,\n                                             sparse_grad=True)\n        # (num_samples+n, 1)\n        sample_target_b = S.sparse.Embedding(data=sample_label, weight=bias,\n                                             input_dim=num_classes, output_dim=1,\n                                             sparse_grad=True)\n        # (num_samples, dim)\n        sample_w = S.slice(sample_target_w, begin=(0, 0), end=(num_samples, None))\n        target_w = S.slice(sample_target_w, begin=(num_samples, 0), end=(None, None))\n        sample_b = S.slice(sample_target_b, begin=(0, 0), end=(num_samples, None))\n        target_b = S.slice(sample_target_b, begin=(num_samples, 0), end=(None, None))\n\n        # target\n        # (n, 1)\n        true_pred = S.sum(target_w * inputs, axis=1, keepdims=True) + target_b\n        # samples\n        # (n, num_samples)\n        sample_b = S.reshape(sample_b, (-1,))\n        sample_pred = S.FullyConnected(inputs, weight=sample_w, bias=sample_b,\n                                       num_hidden=num_samples)\n\n        # remove accidental hits\n        if remove_accidental_hits:\n            label_v = S.reshape(label, (-1, 1))\n            sample_v = S.reshape(sample, (1, -1))\n            neg = S.broadcast_equal(label_v, sample_v) * -1e37\n            sample_pred = sample_pred + neg\n\n        prob_sample = S.reshape(prob_sample, shape=(1, num_samples))\n        p_target = true_pred - S.log(prob_target)\n        p_sample = S.broadcast_sub(sample_pred, S.log(prob_sample))\n\n        # return logits and new_labels\n        # (n, 1+num_samples)\n        logits = S.concat(p_target, p_sample, dim=1)\n        new_targets = S.zeros_like(label)\n        return logits, new_targets", "response": "Sampled softmax via importance sampling."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generate_samples(label, num_splits, sampler):\n    def listify(x):\n        return x if isinstance(x, list) else [x]\n    label_splits = listify(label.split(num_splits, axis=0))\n    prob_samples = []\n    prob_targets = []\n    samples = []\n    for label_split in label_splits:\n        label_split_2d = label_split.reshape((-1,1))\n        sampled_value = sampler.draw(label_split_2d)\n        sampled_classes, exp_cnt_true, exp_cnt_sampled = sampled_value\n        samples.append(sampled_classes.astype(np.float32))\n        prob_targets.append(exp_cnt_true.astype(np.float32).reshape((-1,1)))\n        prob_samples.append(exp_cnt_sampled.astype(np.float32))\n    return samples, prob_samples, prob_targets", "response": "Split labels into num_splits and generate candidates based on log - uniform distribution."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a pre - defined model by name.", "response": "def get_model(name, **kwargs):\n    \"\"\"Returns a pre-defined model by name\n\n    Parameters\n    ----------\n    name : str\n        Name of the model.\n    pretrained : bool\n        Whether to load the pretrained weights for model.\n    classes : int\n        Number of classes for the output layer.\n    ctx : Context, default CPU\n        The context in which to load the pretrained weights.\n    root : str, default '$MXNET_HOME/models'\n        Location for keeping the model parameters.\n\n    Returns\n    -------\n    HybridBlock\n        The model.\n    \"\"\"\n    models = {'resnet18_v1': resnet18_v1,\n              'resnet34_v1': resnet34_v1,\n              'resnet50_v1': resnet50_v1,\n              'resnet101_v1': resnet101_v1,\n              'resnet152_v1': resnet152_v1,\n              'resnet18_v2': resnet18_v2,\n              'resnet34_v2': resnet34_v2,\n              'resnet50_v2': resnet50_v2,\n              'resnet101_v2': resnet101_v2,\n              'resnet152_v2': resnet152_v2,\n              'vgg11': vgg11,\n              'vgg13': vgg13,\n              'vgg16': vgg16,\n              'vgg19': vgg19,\n              'vgg11_bn': vgg11_bn,\n              'vgg13_bn': vgg13_bn,\n              'vgg16_bn': vgg16_bn,\n              'vgg19_bn': vgg19_bn,\n              'alexnet': alexnet,\n              'densenet121': densenet121,\n              'densenet161': densenet161,\n              'densenet169': densenet169,\n              'densenet201': densenet201,\n              'squeezenet1.0': squeezenet1_0,\n              'squeezenet1.1': squeezenet1_1,\n              'inceptionv3': inception_v3,\n              'mobilenet1.0': mobilenet1_0,\n              'mobilenet0.75': mobilenet0_75,\n              'mobilenet0.5': mobilenet0_5,\n              'mobilenet0.25': mobilenet0_25,\n              'mobilenetv2_1.0': mobilenet_v2_1_0,\n              'mobilenetv2_0.75': mobilenet_v2_0_75,\n              'mobilenetv2_0.5': mobilenet_v2_0_5,\n              'mobilenetv2_0.25': mobilenet_v2_0_25\n             }\n    name = name.lower()\n    if name not in models:\n        raise ValueError(\n            'Model %s is not supported. Available options are\\n\\t%s' % (\n                name, '\\n\\t'.join(sorted(models.keys()))))\n    return models[name](**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a new handle with specified storage type shape dtype and context.", "response": "def _new_alloc_handle(stype, shape, ctx, delay_alloc, dtype, aux_types, aux_shapes=None):\n    \"\"\"Return a new handle with specified storage type, shape, dtype and context.\n\n    Empty handle is only used to hold results\n\n    Returns\n    -------\n    handle\n        A new empty ndarray handle\n    \"\"\"\n    hdl = NDArrayHandle()\n    for aux_t in aux_types:\n        if np.dtype(aux_t) != np.dtype(\"int64\"):\n            raise NotImplementedError(\"only int64 is supported for aux types\")\n    aux_type_ids = [int(_DTYPE_NP_TO_MX[np.dtype(aux_t).type]) for aux_t in aux_types]\n    aux_shapes = [(0,) for aux_t in aux_types] if aux_shapes is None else aux_shapes\n    aux_shape_lens = [len(aux_shape) for aux_shape in aux_shapes]\n    aux_shapes = py_sum(aux_shapes, ())\n    num_aux = mx_uint(len(aux_types))\n    check_call(_LIB.MXNDArrayCreateSparseEx(\n        ctypes.c_int(int(_STORAGE_TYPE_STR_TO_ID[stype])),\n        c_array_buf(mx_uint, native_array('I', shape)),\n        mx_uint(len(shape)),\n        ctypes.c_int(ctx.device_typeid),\n        ctypes.c_int(ctx.device_id),\n        ctypes.c_int(int(delay_alloc)),\n        ctypes.c_int(int(_DTYPE_NP_TO_MX[np.dtype(dtype).type])),\n        num_aux,\n        c_array_buf(ctypes.c_int, native_array('i', aux_type_ids)),\n        c_array_buf(mx_uint, native_array('I', aux_shape_lens)),\n        c_array_buf(mx_uint, native_array('I', aux_shapes)),\n        ctypes.byref(hdl)))\n    return hdl"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _prepare_src_array(source_array, dtype):\n    if not isinstance(source_array, NDArray) and not isinstance(source_array, np.ndarray):\n        try:\n            source_array = np.array(source_array, dtype=dtype)\n        except:\n            raise TypeError('values must be array like object')\n    return source_array", "response": "Prepare source_array so that it can be used to construct NDArray."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprepare the value of dtype if dtype is None.", "response": "def _prepare_default_dtype(src_array, dtype):\n    \"\"\"Prepare the value of dtype if `dtype` is None. If `src_array` is an NDArray, numpy.ndarray\n    or scipy.sparse.csr.csr_matrix, return src_array.dtype. float32 is returned otherwise.\"\"\"\n    if dtype is None:\n        if isinstance(src_array, (NDArray, np.ndarray)):\n            dtype = src_array.dtype\n        elif spsp and isinstance(src_array, spsp.csr.csr_matrix):\n            dtype = src_array.dtype\n        else:\n            dtype = mx_real_t\n    return dtype"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _check_shape(s1, s2):\n    if s1 and s2 and s1 != s2:\n        raise ValueError(\"Shape mismatch detected. \" + str(s1) + \" v.s. \" + str(s2))", "response": "check if shape is correct"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef csr_matrix(arg1, shape=None, ctx=None, dtype=None):\n    # construct a csr matrix from (M, N) or (data, indices, indptr)\n    if isinstance(arg1, tuple):\n        arg_len = len(arg1)\n        if arg_len == 2:\n            # construct a sparse csr matrix from\n            # scipy coo matrix if input format is coo\n            if isinstance(arg1[1], tuple) and len(arg1[1]) == 2:\n                data, (row, col) = arg1\n                if isinstance(data, NDArray):\n                    data = data.asnumpy()\n                if isinstance(row, NDArray):\n                    row = row.asnumpy()\n                if isinstance(col, NDArray):\n                    col = col.asnumpy()\n                coo = spsp.coo_matrix((data, (row, col)), shape=shape)\n                _check_shape(coo.shape, shape)\n                csr = coo.tocsr()\n                return array(csr, ctx=ctx, dtype=dtype)\n            else:\n                # empty matrix with shape\n                _check_shape(arg1, shape)\n                return empty('csr', arg1, ctx=ctx, dtype=dtype)\n        elif arg_len == 3:\n            # data, indices, indptr\n            return _csr_matrix_from_definition(arg1[0], arg1[1], arg1[2], shape=shape,\n                                               ctx=ctx, dtype=dtype)\n        else:\n            raise ValueError(\"Unexpected length of input tuple: \" + str(arg_len))\n    else:\n        # construct a csr matrix from a sparse / dense one\n        if isinstance(arg1, CSRNDArray) or (spsp and isinstance(arg1, spsp.csr.csr_matrix)):\n            # construct a csr matrix from scipy or CSRNDArray\n            _check_shape(arg1.shape, shape)\n            return array(arg1, ctx=ctx, dtype=dtype)\n        elif isinstance(arg1, RowSparseNDArray):\n            raise ValueError(\"Unexpected input type: RowSparseNDArray\")\n        else:\n            # construct a csr matrix from a dense one\n            # prepare default ctx and dtype since mx.nd.array doesn't use default values\n            # based on source_array\n            dtype = _prepare_default_dtype(arg1, dtype)\n            # create dns array with provided dtype. ctx is not passed since copy across\n            # ctx requires dtype to be the same\n            dns = _array(arg1, dtype=dtype)\n            if ctx is not None and dns.context != ctx:\n                dns = dns.as_in_context(ctx)\n            _check_shape(dns.shape, shape)\n            return dns.tostype('csr')", "response": "Constructs a CSRNDArray from a sparse 2D array with compressed sparse row format."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a CSRNDArray based on data indices and indptr.", "response": "def _csr_matrix_from_definition(data, indices, indptr, shape=None, ctx=None,\n                                dtype=None, indices_type=None, indptr_type=None):\n    \"\"\"Create a `CSRNDArray` based on data, indices and indptr\"\"\"\n    # pylint: disable= no-member, protected-access\n    storage_type = 'csr'\n    # context\n    ctx = current_context() if ctx is None else ctx\n    # types\n    dtype = _prepare_default_dtype(data, dtype)\n    indptr_type = _STORAGE_AUX_TYPES[storage_type][0] if indptr_type is None else indptr_type\n    indices_type = _STORAGE_AUX_TYPES[storage_type][1] if indices_type is None else indices_type\n    # prepare src array and types\n    data = _prepare_src_array(data, dtype)\n    indptr = _prepare_src_array(indptr, indptr_type)\n    indices = _prepare_src_array(indices, indices_type)\n\n    # TODO(junwu): Convert data, indptr, and indices to mxnet NDArrays\n    # if they are not for now. In the future, we should provide a c-api\n    # to accept np.ndarray types to copy from to result.data and aux_data\n    if not isinstance(data, NDArray):\n        data = _array(data, ctx, dtype)\n    if not isinstance(indptr, NDArray):\n        indptr = _array(indptr, ctx, indptr_type)\n    if not isinstance(indices, NDArray):\n        indices = _array(indices, ctx, indices_type)\n    if shape is None:\n        if indices.shape[0] == 0:\n            raise ValueError('invalid shape')\n        shape = (len(indptr) - 1, op.max(indices).asscalar() + 1)\n    # verify shapes\n    aux_shapes = [indptr.shape, indices.shape]\n    if data.ndim != 1 or indptr.ndim != 1 or indices.ndim != 1 or \\\n        indptr.shape[0] == 0 or len(shape) != 2:\n        raise ValueError('invalid shape')\n    result = CSRNDArray(_new_alloc_handle(storage_type, shape, ctx, False, dtype,\n                                          [indptr_type, indices_type], aux_shapes))\n    check_call(_LIB.MXNDArraySyncCopyFromNDArray(result.handle, data.handle, ctypes.c_int(-1)))\n    check_call(_LIB.MXNDArraySyncCopyFromNDArray(result.handle, indptr.handle, ctypes.c_int(0)))\n    check_call(_LIB.MXNDArraySyncCopyFromNDArray(result.handle, indices.handle, ctypes.c_int(1)))\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef row_sparse_array(arg1, shape=None, ctx=None, dtype=None):\n    # construct a row sparse array from (D0, D1 ..) or (data, indices)\n    if isinstance(arg1, tuple):\n        arg_len = len(arg1)\n        if arg_len < 2:\n            raise ValueError(\"Unexpected length of input tuple: \" + str(arg_len))\n        elif arg_len > 2:\n            # empty ndarray with shape\n            _check_shape(arg1, shape)\n            return empty('row_sparse', arg1, ctx=ctx, dtype=dtype)\n        else:\n            # len(arg1) = 2, is either shape or (data, indices)\n            if isinstance(arg1[0], integer_types) and isinstance(arg1[1], integer_types):\n                # empty ndarray with shape\n                _check_shape(arg1, shape)\n                return empty('row_sparse', arg1, ctx=ctx, dtype=dtype)\n            else:\n                # data, indices, indptr\n                return _row_sparse_ndarray_from_definition(arg1[0], arg1[1], shape=shape,\n                                                           ctx=ctx, dtype=dtype)\n    else:\n        # construct a row sparse ndarray from a dense / sparse array\n        if isinstance(arg1, RowSparseNDArray):\n            # construct a row sparse ndarray from RowSparseNDArray\n            _check_shape(arg1.shape, shape)\n            return array(arg1, ctx=ctx, dtype=dtype)\n        elif isinstance(arg1, CSRNDArray):\n            raise ValueError(\"Unexpected input type: CSRNDArray\")\n        else:\n            # construct a csr matrix from a dense one\n            # prepare default dtype since mx.nd.array doesn't use default values\n            # based on source_array\n            dtype = _prepare_default_dtype(arg1, dtype)\n            # create dns array with provided dtype. ctx is not passed since copy across\n            # ctx requires dtype to be the same\n            dns = _array(arg1, dtype=dtype)\n            if ctx is not None and dns.context != ctx:\n                dns = dns.as_in_context(ctx)\n            _check_shape(dns.shape, shape)\n            return dns.tostype('row_sparse')", "response": "Creates a multidimensional RowSparseNDArray with a set of slices at given indices."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a RowSparseNDArray based on data and indices.", "response": "def _row_sparse_ndarray_from_definition(data, indices, shape=None, ctx=None,\n                                        dtype=None, indices_type=None):\n    \"\"\"Create a `RowSparseNDArray` based on data and indices\"\"\"\n    storage_type = 'row_sparse'\n    # context\n    ctx = current_context() if ctx is None else ctx\n    # types\n    dtype = _prepare_default_dtype(data, dtype)\n    indices_type = _STORAGE_AUX_TYPES[storage_type][0] if indices_type is None else indices_type\n    # prepare src array and types\n    data = _prepare_src_array(data, dtype)\n    indices = _prepare_src_array(indices, indices_type)\n\n    # TODO(junwu): Convert data, indptr, and indices to mxnet NDArrays\n    # if they are not for now. In the future, we should provide a c-api\n    # to accept np.ndarray types to copy from to result.data and aux_data\n    if not isinstance(data, NDArray):\n        data = _array(data, ctx, dtype)\n    if not isinstance(indices, NDArray):\n        indices = _array(indices, ctx, indices_type)\n    if shape is None:\n        num_indices = indices.shape[0]\n        if num_indices == 0:\n            raise ValueError('invalid shape')\n        dim0 = indices[num_indices - 1].asscalar() + 1\n        shape = (dim0, ) + data.shape[1:]\n    # verify shapes\n    if data.ndim != len(shape) or indices.ndim != 1 or np.prod(shape[1:]) == 0:\n        raise ValueError(\"invalid shape\")\n    result = RowSparseNDArray(_new_alloc_handle(storage_type, shape, ctx, False, dtype,\n                                                [indices_type], [indices.shape]))\n    check_call(_LIB.MXNDArraySyncCopyFromNDArray(result.handle, data.handle, ctypes.c_int(-1)))\n    check_call(_LIB.MXNDArraySyncCopyFromNDArray(result.handle, indices.handle, ctypes.c_int(0)))\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add(lhs, rhs):\n    # pylint: disable= no-member, protected-access\n    if isinstance(lhs, NDArray) and isinstance(rhs, NDArray) and lhs.shape == rhs.shape:\n        return _ufunc_helper(\n            lhs,\n            rhs,\n            op.elemwise_add,\n            operator.add,\n            _internal._plus_scalar,\n            None)\n\n    return _ufunc_helper(\n        lhs,\n        rhs,\n        op.broadcast_add,\n        operator.add,\n        _internal._plus_scalar,\n        None)", "response": "Adds two arrays with broadcasting."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsubtracting the elements of two arrays with broadcasting.", "response": "def subtract(lhs, rhs):\n    \"\"\"Returns element-wise difference of the input arrays with broadcasting.\n\n    Equivalent to ``lhs - rhs``, ``mx.nd.broadcast_sub(lhs, rhs)`` and\n    ``mx.nd.broadcast_minus(lhs, rhs)`` when shapes of lhs and rhs do not\n    match. If lhs.shape == rhs.shape, this is equivalent to\n    ``mx.nd.elemwise_sub(lhs, rhs)``\n\n    .. note::\n\n        If the corresponding dimensions of two arrays have the same size or one of them has size 1,\n        then the arrays are broadcastable to a common shape.\n\n    Parameters\n    ----------\n    lhs : scalar or mxnet.ndarray.sparse.array\n        First array to be subtracted.\n    rhs : scalar or mxnet.ndarray.sparse.array\n         Second array to be subtracted.\n        If ``lhs.shape != rhs.shape``, they must be\n        broadcastable to a common shape.__spec__\n\n    Returns\n    -------\n    NDArray\n        The element-wise difference of the input arrays.\n\n    Examples\n    --------\n    >>> a = mx.nd.ones((2,3)).tostype('csr')\n    >>> b = mx.nd.ones((2,3)).tostype('csr')\n    >>> a.asnumpy()\n    array([[ 1.,  1.,  1.],\n           [ 1.,  1.,  1.]], dtype=float32)\n    >>> b.asnumpy()\n    array([[ 1.,  1.,  1.],\n           [ 1.,  1.,  1.]], dtype=float32)\n    >>> (a-b).asnumpy()\n    array([[ 0.,  0.,  0.],\n           [ 0.,  0.,  0.]], dtype=float32)\n    >>> c = mx.nd.ones((2,3)).tostype('row_sparse')\n    >>> d = mx.nd.ones((2,3)).tostype('row_sparse')\n    >>> c.asnumpy()\n    array([[ 1.,  1.,  1.],\n           [ 1.,  1.,  1.]], dtype=float32)\n    >>> d.asnumpy()\n    array([[ 1.,  1.,  1.],\n           [ 1.,  1.,  1.]], dtype=float32)\n    >>> (c-d).asnumpy()\n    array([[ 0.,  0.,  0.],\n           [ 0.,  0.,  0.]], dtype=float32)\n    \"\"\"\n    # pylint: disable= no-member, protected-access\n    if isinstance(lhs, NDArray) and isinstance(rhs, NDArray) and lhs.shape == rhs.shape:\n        return _ufunc_helper(\n            lhs,\n            rhs,\n            op.elemwise_sub,\n            operator.sub,\n            _internal._minus_scalar,\n            None)\n\n    return _ufunc_helper(\n        lhs,\n        rhs,\n        op.broadcast_sub,\n        operator.sub,\n        _internal._minus_scalar,\n        None)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef multiply(lhs, rhs):\n    # pylint: disable= no-member, protected-access\n    if isinstance(lhs, NDArray) and isinstance(rhs, NDArray) and lhs.shape == rhs.shape:\n        return _ufunc_helper(\n            lhs,\n            rhs,\n            op.elemwise_mul,\n            operator.mul,\n            _internal._mul_scalar,\n            None)\n\n    return _ufunc_helper(\n        lhs,\n        rhs,\n        op.broadcast_mul,\n        operator.mul,\n        _internal._mul_scalar,\n        None)", "response": "Returns an NDArray that is element - wise multiplication of the input arrays with broadcasting."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef divide(lhs, rhs):\n    # pylint: disable= no-member, protected-access\n    if isinstance(lhs, NDArray) and isinstance(rhs, NDArray) and lhs.shape == rhs.shape:\n        return _ufunc_helper(\n            lhs,\n            rhs,\n            op.elemwise_div,\n            operator.truediv,\n            _internal._div_scalar,\n            None)\n\n    return _ufunc_helper(\n        lhs,\n        rhs,\n        op.broadcast_div,\n        operator.truediv,\n        _internal._div_scalar,\n        None)", "response": "Returns an NDArray that is element - wise divided by the input arrays with broadcasting."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a new array of given shape and type filled with zeros.", "response": "def zeros(stype, shape, ctx=None, dtype=None, **kwargs):\n    \"\"\"Return a new array of given shape and type, filled with zeros.\n\n    Parameters\n    ----------\n    stype: string\n        The storage type of the empty array, such as 'row_sparse', 'csr', etc\n    shape : int or tuple of int\n        The shape of the empty array\n    ctx : Context, optional\n        An optional device context (default is the current default context)\n    dtype : str or numpy.dtype, optional\n        An optional value type (default is `float32`)\n\n    Returns\n    -------\n    RowSparseNDArray or CSRNDArray\n        A created array\n    Examples\n    --------\n    >>> mx.nd.sparse.zeros('csr', (1,2))\n    <CSRNDArray 1x2 @cpu(0)>\n    >>> mx.nd.sparse.zeros('row_sparse', (1,2), ctx=mx.cpu(), dtype='float16').asnumpy()\n    array([[ 0.,  0.]], dtype=float16)\n    \"\"\"\n    # pylint: disable= no-member, protected-access\n    if stype == 'default':\n        return _zeros_ndarray(shape, ctx=ctx, dtype=dtype, **kwargs)\n    if ctx is None:\n        ctx = current_context()\n    dtype = mx_real_t if dtype is None else dtype\n    if stype in ('row_sparse', 'csr'):\n        aux_types = _STORAGE_AUX_TYPES[stype]\n    else:\n        raise ValueError(\"unknown storage type\" + stype)\n    out = _ndarray_cls(_new_alloc_handle(stype, shape, ctx, True, dtype, aux_types))\n    return _internal._zeros(shape=shape, ctx=ctx, dtype=dtype, out=out, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a new array of given shape and type with initial entries initialized.", "response": "def empty(stype, shape, ctx=None, dtype=None):\n    \"\"\"Returns a new array of given shape and type, without initializing entries.\n\n    Parameters\n    ----------\n    stype: string\n        The storage type of the empty array, such as 'row_sparse', 'csr', etc\n    shape : int or tuple of int\n        The shape of the empty array.\n    ctx : Context, optional\n        An optional device context (default is the current default context).\n    dtype : str or numpy.dtype, optional\n        An optional value type (default is `float32`).\n\n    Returns\n    -------\n    CSRNDArray or RowSparseNDArray\n        A created array.\n    \"\"\"\n    if isinstance(shape, int):\n        shape = (shape, )\n    if ctx is None:\n        ctx = current_context()\n    if dtype is None:\n        dtype = mx_real_t\n    assert(stype is not None)\n    if stype in ('csr', 'row_sparse'):\n        return zeros(stype, shape, ctx=ctx, dtype=dtype)\n    else:\n        raise Exception(\"unknown stype : \" + str(stype))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a sparse array from any object exposing the array interface.", "response": "def array(source_array, ctx=None, dtype=None):\n    \"\"\"Creates a sparse array from any object exposing the array interface.\n\n    Parameters\n    ----------\n    source_array : RowSparseNDArray, CSRNDArray or scipy.sparse.csr.csr_matrix\n        The source sparse array\n    ctx : Context, optional\n        The default context is ``source_array.context`` if ``source_array`` is an NDArray. \\\n        The current default context otherwise.\n    dtype : str or numpy.dtype, optional\n        The data type of the output array. The default dtype is ``source_array.dtype``\n        if `source_array` is an `NDArray`, `numpy.ndarray` or `scipy.sparse.csr.csr_matrix`, \\\n        `float32` otherwise.\n\n    Returns\n    -------\n    RowSparseNDArray or CSRNDArray\n        An array with the same contents as the `source_array`.\n\n    Examples\n    --------\n    >>> import scipy.sparse as spsp\n    >>> csr = spsp.csr_matrix((2, 100))\n    >>> mx.nd.sparse.array(csr)\n    <CSRNDArray 2x100 @cpu(0)>\n    >>> mx.nd.sparse.array(mx.nd.sparse.zeros('csr', (3, 2)))\n    <CSRNDArray 3x2 @cpu(0)>\n    >>> mx.nd.sparse.array(mx.nd.sparse.zeros('row_sparse', (3, 2)))\n    <RowSparseNDArray 3x2 @cpu(0)>\n    \"\"\"\n    ctx = current_context() if ctx is None else ctx\n    if isinstance(source_array, NDArray):\n        assert(source_array.stype != 'default'), \\\n               \"Please use `tostype` to create RowSparseNDArray or CSRNDArray from an NDArray\"\n        # prepare dtype and ctx based on source_array, if not provided\n        dtype = _prepare_default_dtype(source_array, dtype)\n        # if both dtype and ctx are different from source_array, we cannot copy directly\n        if source_array.dtype != dtype and source_array.context != ctx:\n            arr = empty(source_array.stype, source_array.shape, dtype=dtype)\n            arr[:] = source_array\n            arr = arr.as_in_context(ctx)\n        else:\n            arr = empty(source_array.stype, source_array.shape, dtype=dtype, ctx=ctx)\n            arr[:] = source_array\n        return arr\n    elif spsp and isinstance(source_array, spsp.csr.csr_matrix):\n        # TODO(haibin) implement `_sync_copy_from` with scipy csr object to reduce a copy\n        # preprocess scipy csr to canonical form\n        csr = source_array.sorted_indices()\n        csr.sum_duplicates()\n        dtype = _prepare_default_dtype(source_array, dtype)\n        return csr_matrix((csr.data, csr.indices, csr.indptr), shape=csr.shape, \\\n                          dtype=dtype, ctx=ctx)\n    elif isinstance(source_array, (np.ndarray, np.generic)):\n        raise ValueError(\"Please use mx.nd.array to create an NDArray with source_array of type \",\n                         type(source_array))\n    else:\n        raise ValueError(\"Unexpected source_array type: \", type(source_array))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _aux_type(self, i):\n        aux_type = ctypes.c_int()\n        check_call(_LIB.MXNDArrayGetAuxType(self.handle, i, ctypes.byref(aux_type)))\n        return _DTYPE_MX_TO_NP[aux_type.value]", "response": "Data - type of the array s ith aux data. Returns ------- numpy. dtype\n            This BaseSparseNDArray s aux data type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a copy of the array after casting to a specified type.", "response": "def astype(self, dtype, copy=True):\n        \"\"\"Return a copy of the array after casting to a specified type.\n\n        Parameters\n        ----------\n        dtype : numpy.dtype or str\n            The type of the returned array.\n        copy : bool\n            Default `True`. By default, astype always returns a newly\n            allocated ndarray on the same context. If this is set to\n            `False`, and the dtype requested is the same as the ndarray's\n            dtype, the ndarray is returned instead of a copy.\n\n        Examples\n        --------\n        >>> x = mx.nd.sparse.zeros('row_sparse', (2,3), dtype='float32')\n        >>> y = x.astype('int32')\n        >>> y.dtype\n        <type 'numpy.int32'>\n        \"\"\"\n        if not copy and np.dtype(dtype) == self.dtype:\n            return self\n\n        res = zeros(shape=self.shape, ctx=self.context,\n                    dtype=dtype, stype=self.stype)\n        self.copyto(res)\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck whether the NDArray format is valid.", "response": "def check_format(self, full_check=True):\n        \"\"\"Check whether the NDArray format is valid.\n\n        Parameters\n        ----------\n        full_check : bool, optional\n            If `True`, rigorous check, O(N) operations. Otherwise\n            basic check, O(1) operations (default True).\n        \"\"\"\n        check_call(_LIB.MXNDArraySyncCheckFormat(self.handle, ctypes.c_bool(full_check)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _aux_data(self, i):\n        self.wait_to_read()\n        hdl = NDArrayHandle()\n        check_call(_LIB.MXNDArrayGetAuxNDArray(self.handle, i, ctypes.byref(hdl)))\n        return NDArray(hdl)", "response": "Get a deep copy NDArray of the i - th aux data array associated with the\n        BaseSparseNDArray."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef asscipy(self):\n        data = self.data.asnumpy()\n        indices = self.indices.asnumpy()\n        indptr = self.indptr.asnumpy()\n        if not spsp:\n            raise ImportError(\"scipy is not available. \\\n                               Please check if the scipy python bindings are installed.\")\n        return spsp.csr_matrix((data, indices, indptr), shape=self.shape, dtype=self.dtype)", "response": "Returns a scipy. sparse. csr. csr_matrix object with value copied from this array\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a copy of the array with chosen storage type. Returns ------- NDArray or RowSparseNDArray A copy of the array with chosen storage type.", "response": "def tostype(self, stype):\n        \"\"\"Return a copy of the array with chosen storage type.\n\n        Returns\n        -------\n        NDArray or RowSparseNDArray\n            A copy of the array with the chosen storage stype\n        \"\"\"\n        # pylint: disable= no-member, protected-access\n        if stype == 'csr':\n            raise ValueError(\"cast_storage from row_sparse to csr is not supported\")\n        return op.cast_storage(self, stype=stype)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef copyto(self, other):\n        if isinstance(other, Context):\n            return super(RowSparseNDArray, self).copyto(other)\n        elif isinstance(other, NDArray):\n            stype = other.stype\n            if stype in ('default', 'row_sparse'):\n                return super(RowSparseNDArray, self).copyto(other)\n            else:\n                raise TypeError('copyto does not support destination NDArray stype ' + str(stype))\n        else:\n            raise TypeError('copyto does not support type ' + str(type(other)))", "response": "Copies the value of this array to another array."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef export_model(sym, params, input_shape, input_type=np.float32,\n                 onnx_file_path='model.onnx', verbose=False):\n    \"\"\"Exports the MXNet model file, passed as a parameter, into ONNX model.\n    Accepts both symbol,parameter objects as well as json and params filepaths as input.\n    Operator support and coverage -\n    https://cwiki.apache.org/confluence/display/MXNET/MXNet-ONNX+Integration\n\n    Parameters\n    ----------\n    sym : str or symbol object\n        Path to the json file or Symbol object\n    params : str or symbol object\n        Path to the params file or params dictionary. (Including both arg_params and aux_params)\n    input_shape : List of tuple\n        Input shape of the model e.g [(1,3,224,224)]\n    input_type : data type\n        Input data type e.g. np.float32\n    onnx_file_path : str\n        Path where to save the generated onnx file\n    verbose : Boolean\n        If true will print logs of the model conversion\n\n    Returns\n    -------\n    onnx_file_path : str\n        Onnx file path\n\n    Notes\n    -----\n    This method is available when you ``import mxnet.contrib.onnx``\n\n    \"\"\"\n\n    try:\n        from onnx import helper, mapping\n    except ImportError:\n        raise ImportError(\"Onnx and protobuf need to be installed. \"\n                          + \"Instructions to install - https://github.com/onnx/onnx\")\n\n    converter = MXNetGraph()\n\n    data_format = np.dtype(input_type)\n    # if input parameters are strings(file paths), load files and create symbol parameter objects\n    if isinstance(sym, string_types) and isinstance(params, string_types):\n        logging.info(\"Converting json and weight file to sym and params\")\n        sym_obj, params_obj = load_module(sym, params)\n        onnx_graph = converter.create_onnx_graph_proto(sym_obj, params_obj, input_shape,\n                                                       mapping.NP_TYPE_TO_TENSOR_TYPE[data_format],\n                                                       verbose=verbose)\n    elif isinstance(sym, symbol.Symbol) and isinstance(params, dict):\n        onnx_graph = converter.create_onnx_graph_proto(sym, params, input_shape,\n                                                       mapping.NP_TYPE_TO_TENSOR_TYPE[data_format],\n                                                       verbose=verbose)\n    else:\n        raise ValueError(\"Input sym and params should either be files or objects\")\n\n    # Create the model (ModelProto)\n    onnx_model = helper.make_model(onnx_graph)\n\n    # Save model on disk\n    with open(onnx_file_path, \"wb\") as file_handle:\n        serialized = onnx_model.SerializeToString()\n        file_handle.write(serialized)\n        logging.info(\"Input shape of the model %s \", input_shape)\n        logging.info(\"Exported ONNX file %s saved to disk\", onnx_file_path)\n\n    return onnx_file_path", "response": "Exports the MXNet model file passed as a parameter into ONNX model file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bench_dot(lhs_row_dim, lhs_col_dim, rhs_col_dim, density,\n              rhs_density, dot_func, trans_lhs, lhs_stype,\n              rhs_stype, only_storage, distribution=\"uniform\"):\n    \"\"\" Benchmarking both storage and dot\n    \"\"\"\n    lhs_nd = rand_ndarray((lhs_row_dim, lhs_col_dim), lhs_stype, density, distribution=distribution)\n    if not only_storage:\n        rhs_nd = rand_ndarray((lhs_col_dim, rhs_col_dim), rhs_stype,\n                              density=rhs_density, distribution=distribution)\n        out = dot_func(lhs_nd, rhs_nd, trans_lhs)\n    mx.nd.waitall()", "response": "Benchmarks both storage and dot trees"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts caffe mean into ndarray", "response": "def convert_mean(binaryproto_fname, output=None):\n    \"\"\"Convert caffe mean\n\n    Parameters\n    ----------\n    binaryproto_fname : str\n        Filename of the mean\n    output : str, optional\n        Save the mean into mxnet's format\n\n    Returns\n    -------\n    NDArray\n        Mean in ndarray\n    \"\"\"\n    mean_blob = caffe_parser.caffe_pb2.BlobProto()\n    with open(binaryproto_fname, 'rb') as f:\n        mean_blob.ParseFromString(f.read())\n\n    img_mean_np = np.array(mean_blob.data)\n    img_mean_np = img_mean_np.reshape(\n        mean_blob.channels, mean_blob.height, mean_blob.width\n    )\n    # swap channels from Caffe BGR to RGB\n    img_mean_np[[0, 2], :, :] = img_mean_np[[2, 0], :, :]\n    nd = mx.nd.array(img_mean_np)\n    if output is not None:\n        mx.nd.save(output, {\"mean_image\": nd})\n    return nd"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_densenet(num_layers, pretrained=False, ctx=cpu(),\n                 root=os.path.join(base.data_dir(), 'models'), **kwargs):\n    r\"\"\"Densenet-BC model from the\n    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_ paper.\n\n    Parameters\n    ----------\n    num_layers : int\n        Number of layers for the variant of densenet. Options are 121, 161, 169, 201.\n    pretrained : bool, default False\n        Whether to load the pretrained weights for model.\n    ctx : Context, default CPU\n        The context in which to load the pretrained weights.\n    root : str, default $MXNET_HOME/models\n        Location for keeping the model parameters.\n    \"\"\"\n    num_init_features, growth_rate, block_config = densenet_spec[num_layers]\n    net = DenseNet(num_init_features, growth_rate, block_config, **kwargs)\n    if pretrained:\n        from ..model_store import get_model_file\n        net.load_parameters(get_model_file('densenet%d'%(num_layers), root=root), ctx=ctx)\n    return net", "response": "r Returns a DenseNet object for the given num_layers."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload the MXNet model file and returns MXNet symbol and params.", "response": "def load_module(sym_filepath, params_filepath):\n    \"\"\"Loads the MXNet model file and\n    returns MXNet symbol and params (weights).\n\n    Parameters\n    ----------\n    json_path : str\n        Path to the json file\n    params_path : str\n        Path to the params file\n\n    Returns\n    -------\n    sym : MXNet symbol\n        Model symbol object\n\n    params : params object\n        Model weights including both arg and aux params.\n    \"\"\"\n    if not (os.path.isfile(sym_filepath) and os.path.isfile(params_filepath)):\n        raise ValueError(\"Symbol and params files provided are invalid\")\n    else:\n        try:\n            # reads symbol.json file from given path and\n            # retrieves model prefix and number of epochs\n            model_name = sym_filepath.rsplit('.', 1)[0].rsplit('-', 1)[0]\n            params_file_list = params_filepath.rsplit('.', 1)[0].rsplit('-', 1)\n            # Setting num_epochs to 0 if not present in filename\n            num_epochs = 0 if len(params_file_list) == 1 else int(params_file_list[1])\n        except IndexError:\n            logging.info(\"Model and params name should be in format: \"\n                         \"prefix-symbol.json, prefix-epoch.params\")\n            raise\n\n        sym, arg_params, aux_params = mx.model.load_checkpoint(model_name, num_epochs)\n\n        # Merging arg and aux parameters\n        params = {}\n        params.update(arg_params)\n        params.update(aux_params)\n\n        return sym, params"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef import_module(module_name):\n    import sys, os\n    import importlib\n    sys.path.append(os.path.dirname(__file__))\n    return importlib.import_module(module_name)", "response": "Helper function to import module"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbuilds a base network symbol for training SSD", "response": "def get_symbol_train(network, num_classes, from_layers, num_filters, strides, pads,\n                     sizes, ratios, normalizations=-1, steps=[], min_filter=128,\n                     nms_thresh=0.5, force_suppress=False, nms_topk=400, **kwargs):\n    \"\"\"Build network symbol for training SSD\n\n    Parameters\n    ----------\n    network : str\n        base network symbol name\n    num_classes : int\n        number of object classes not including background\n    from_layers : list of str\n        feature extraction layers, use '' for add extra layers\n        For example:\n        from_layers = ['relu4_3', 'fc7', '', '', '', '']\n        which means extract feature from relu4_3 and fc7, adding 4 extra layers\n        on top of fc7\n    num_filters : list of int\n        number of filters for extra layers, you can use -1 for extracted features,\n        however, if normalization and scale is applied, the number of filter for\n        that layer must be provided.\n        For example:\n        num_filters = [512, -1, 512, 256, 256, 256]\n    strides : list of int\n        strides for the 3x3 convolution appended, -1 can be used for extracted\n        feature layers\n    pads : list of int\n        paddings for the 3x3 convolution, -1 can be used for extracted layers\n    sizes : list or list of list\n        [min_size, max_size] for all layers or [[], [], []...] for specific layers\n    ratios : list or list of list\n        [ratio1, ratio2...] for all layers or [[], [], ...] for specific layers\n    normalizations : int or list of int\n        use normalizations value for all layers or [...] for specific layers,\n        -1 indicate no normalizations and scales\n    steps : list\n        specify steps for each MultiBoxPrior layer, leave empty, it will calculate\n        according to layer dimensions\n    min_filter : int\n        minimum number of filters used in 1x1 convolution\n    nms_thresh : float\n        non-maximum suppression threshold\n    force_suppress : boolean\n        whether suppress different class objects\n    nms_topk : int\n        apply NMS to top K detections\n\n    Returns\n    -------\n    mx.Symbol\n\n    \"\"\"\n    label = mx.sym.Variable('label')\n    body = import_module(network).get_symbol(num_classes, **kwargs)\n    layers = multi_layer_feature(body, from_layers, num_filters, strides, pads,\n        min_filter=min_filter)\n\n    loc_preds, cls_preds, anchor_boxes = multibox_layer(layers, \\\n        num_classes, sizes=sizes, ratios=ratios, normalization=normalizations, \\\n        num_channels=num_filters, clip=False, interm_layer=0, steps=steps)\n\n    tmp = mx.symbol.contrib.MultiBoxTarget(\n        *[anchor_boxes, label, cls_preds], overlap_threshold=.5, \\\n        ignore_label=-1, negative_mining_ratio=3, minimum_negative_samples=0, \\\n        negative_mining_thresh=.5, variances=(0.1, 0.1, 0.2, 0.2),\n        name=\"multibox_target\")\n    loc_target = tmp[0]\n    loc_target_mask = tmp[1]\n    cls_target = tmp[2]\n\n    cls_prob = mx.symbol.SoftmaxOutput(data=cls_preds, label=cls_target, \\\n        ignore_label=-1, use_ignore=True, grad_scale=1., multi_output=True, \\\n        normalization='valid', name=\"cls_prob\")\n    loc_loss_ = mx.symbol.smooth_l1(name=\"loc_loss_\", \\\n        data=loc_target_mask * (loc_preds - loc_target), scalar=1.0)\n    loc_loss = mx.symbol.MakeLoss(loc_loss_, grad_scale=1., \\\n        normalization='valid', name=\"loc_loss\")\n\n    # monitoring training status\n    cls_label = mx.symbol.MakeLoss(data=cls_target, grad_scale=0, name=\"cls_label\")\n    det = mx.symbol.contrib.MultiBoxDetection(*[cls_prob, loc_preds, anchor_boxes], \\\n        name=\"detection\", nms_threshold=nms_thresh, force_suppress=force_suppress,\n        variances=(0.1, 0.1, 0.2, 0.2), nms_topk=nms_topk)\n    det = mx.symbol.MakeLoss(data=det, grad_scale=0, name=\"det_out\")\n\n    # group output\n    out = mx.symbol.Group([cls_prob, loc_loss, cls_label, det])\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_symbol(network, num_classes, from_layers, num_filters, sizes, ratios,\n               strides, pads, normalizations=-1, steps=[], min_filter=128,\n               nms_thresh=0.5, force_suppress=False, nms_topk=400, **kwargs):\n    \"\"\"Build network for testing SSD\n\n    Parameters\n    ----------\n    network : str\n        base network symbol name\n    num_classes : int\n        number of object classes not including background\n    from_layers : list of str\n        feature extraction layers, use '' for add extra layers\n        For example:\n        from_layers = ['relu4_3', 'fc7', '', '', '', '']\n        which means extract feature from relu4_3 and fc7, adding 4 extra layers\n        on top of fc7\n    num_filters : list of int\n        number of filters for extra layers, you can use -1 for extracted features,\n        however, if normalization and scale is applied, the number of filter for\n        that layer must be provided.\n        For example:\n        num_filters = [512, -1, 512, 256, 256, 256]\n    strides : list of int\n        strides for the 3x3 convolution appended, -1 can be used for extracted\n        feature layers\n    pads : list of int\n        paddings for the 3x3 convolution, -1 can be used for extracted layers\n    sizes : list or list of list\n        [min_size, max_size] for all layers or [[], [], []...] for specific layers\n    ratios : list or list of list\n        [ratio1, ratio2...] for all layers or [[], [], ...] for specific layers\n    normalizations : int or list of int\n        use normalizations value for all layers or [...] for specific layers,\n        -1 indicate no normalizations and scales\n    steps : list\n        specify steps for each MultiBoxPrior layer, leave empty, it will calculate\n        according to layer dimensions\n    min_filter : int\n        minimum number of filters used in 1x1 convolution\n    nms_thresh : float\n        non-maximum suppression threshold\n    force_suppress : boolean\n        whether suppress different class objects\n    nms_topk : int\n        apply NMS to top K detections\n\n    Returns\n    -------\n    mx.Symbol\n\n    \"\"\"\n    body = import_module(network).get_symbol(num_classes, **kwargs)\n    layers = multi_layer_feature(body, from_layers, num_filters, strides, pads,\n        min_filter=min_filter)\n\n    loc_preds, cls_preds, anchor_boxes = multibox_layer(layers, \\\n        num_classes, sizes=sizes, ratios=ratios, normalization=normalizations, \\\n        num_channels=num_filters, clip=False, interm_layer=0, steps=steps)\n\n    cls_prob = mx.symbol.softmax(data=cls_preds, axis=1, name='cls_prob')\n    out = mx.symbol.contrib.MultiBoxDetection(*[cls_prob, loc_preds, anchor_boxes], \\\n        name=\"detection\", nms_threshold=nms_thresh, force_suppress=force_suppress,\n        variances=(0.1, 0.1, 0.2, 0.2), nms_topk=nms_topk)\n    return out", "response": "Build a base network symbol for testing SSD"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_conv_out_grad(net, image, class_id=None, conv_layer_name=None):\n    return _get_grad(net, image, class_id, conv_layer_name, image_grad=False)", "response": "Get the output and gradients of a convolutional layer."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the gradients of the image.", "response": "def get_image_grad(net, image, class_id=None):\n    \"\"\"Get the gradients of the image.\n\n    Parameters:\n    ----------\n    net: Block\n        Network to use for visualization.\n    image: NDArray\n        Preprocessed image to use for visualization.\n    class_id: int\n        Category ID this image belongs to. If not provided,\n        network's prediction will be used.\"\"\"\n    return _get_grad(net, image, class_id, image_grad=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting gradients of image obtained using get_image_grad into image.", "response": "def grad_to_image(gradient):\n    \"\"\"Convert gradients of image obtained using `get_image_grad`\n    into image. This shows parts of the image that is most strongly activating\n    the output neurons.\"\"\"\n    gradient = gradient - gradient.min()\n    gradient /= gradient.max()\n    gradient = np.uint8(gradient * 255).transpose(1, 2, 0)\n    gradient = gradient[..., ::-1]\n    return gradient"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing CAM. Refer section 3 of https://arxiv. org / abs / 1610. 02391 for details", "response": "def get_cam(imggrad, conv_out):\n    \"\"\"Compute CAM. Refer section 3 of https://arxiv.org/abs/1610.02391 for details\"\"\"\n    weights = np.mean(imggrad, axis=(1, 2))\n    cam = np.ones(conv_out.shape[1:], dtype=np.float32)\n    for i, w in enumerate(weights):\n        cam += w * conv_out[i, :, :]\n    cam = cv2.resize(cam, (imggrad.shape[1], imggrad.shape[2]))\n    cam = np.maximum(cam, 0)\n    cam = (cam - np.min(cam)) / (np.max(cam) - np.min(cam)) \n    cam = np.uint8(cam * 255)\n    return cam"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_img_heatmap(orig_img, activation_map):\n    heatmap = cv2.applyColorMap(activation_map, cv2.COLORMAP_COOL)\n    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n    img_heatmap = np.float32(heatmap) + np.float32(orig_img)\n    img_heatmap = img_heatmap / np.max(img_heatmap)\n    img_heatmap *= 255\n    return img_heatmap.astype(int)", "response": "Draw a heatmap on top of the original image using intensities from activation_map"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert gradients to grayscale. This gives a saliency map.", "response": "def to_grayscale(cv2im):\n    \"\"\"Convert gradients to grayscale. This gives a saliency map.\"\"\"\n    # How strongly does each position activate the output\n    grayscale_im = np.sum(np.abs(cv2im), axis=0)\n\n    # Normalize between min and 99th percentile\n    im_max = np.percentile(grayscale_im, 99)\n    im_min = np.min(grayscale_im)\n    grayscale_im = np.clip((grayscale_im - im_min) / (im_max - im_min), 0, 1)\n\n    grayscale_im = np.expand_dims(grayscale_im, axis=0)\n    return grayscale_im"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create(metric, *args, **kwargs):\n    if callable(metric):\n        return CustomMetric(metric, *args, **kwargs)\n    elif isinstance(metric, list):\n        composite_metric = CompositeEvalMetric()\n        for child_metric in metric:\n            composite_metric.add(create(child_metric, *args, **kwargs))\n        return composite_metric\n\n    return _create(metric, *args, **kwargs)", "response": "Creates an evaluation metric from a metric name or a list of EvalMetric instances."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef np(numpy_feval, name=None, allow_extra_outputs=False):\n    def feval(label, pred):\n        \"\"\"Internal eval function.\"\"\"\n        return numpy_feval(label, pred)\n    feval.__name__ = numpy_feval.__name__\n    return CustomMetric(feval, name, allow_extra_outputs)", "response": "Creates a custom evaluation metric that receives the inputs of numpy arrays and returns the corresponding custom metric as a floating point number."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_config(self):\n        config = self._kwargs.copy()\n        config.update({\n            'metric': self.__class__.__name__,\n            'name': self.name,\n            'output_names': self.output_names,\n            'label_names': self.label_names})\n        return config", "response": "Get the configuration of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates the internal evaluation with named label and pred.", "response": "def update_dict(self, label, pred):\n        \"\"\"Update the internal evaluation with named label and pred\n\n        Parameters\n        ----------\n        labels : OrderedDict of str -> NDArray\n            name to array mapping for labels.\n\n        preds : OrderedDict of str -> NDArray\n            name to array mapping of predicted outputs.\n        \"\"\"\n        if self.output_names is not None:\n            pred = [pred[name] for name in self.output_names]\n        else:\n            pred = list(pred.values())\n\n        if self.label_names is not None:\n            label = [label[name] for name in self.label_names]\n        else:\n            label = list(label.values())\n\n        self.update(label, pred)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef reset(self):\n        self.num_inst = 0\n        self.sum_metric = 0.0\n        self.global_num_inst = 0\n        self.global_sum_metric = 0.0", "response": "Resets the internal evaluation result to initial state."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get(self):\n        if self.num_inst == 0:\n            return (self.name, float('nan'))\n        else:\n            return (self.name, self.sum_metric / self.num_inst)", "response": "Gets the current evaluation result."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the current global evaluation result.", "response": "def get_global(self):\n        \"\"\"Gets the current global evaluation result.\n\n        Returns\n        -------\n        names : list of str\n           Name of the metrics.\n        values : list of float\n           Value of the evaluations.\n        \"\"\"\n        if self._has_global_stats:\n            if self.global_num_inst == 0:\n                return (self.name, float('nan'))\n            else:\n                return (self.name, self.global_sum_metric / self.global_num_inst)\n        else:\n            return self.get()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of tuples of name and value pairs.", "response": "def get_name_value(self):\n        \"\"\"Returns zipped name and value pairs.\n\n        Returns\n        -------\n        list of tuples\n            A (name, value) tuple list.\n        \"\"\"\n        name, value = self.get()\n        if not isinstance(name, list):\n            name = [name]\n        if not isinstance(value, list):\n            value = [value]\n        return list(zip(name, value))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_global_name_value(self):\n        if self._has_global_stats:\n            name, value = self.get_global()\n            if not isinstance(name, list):\n                name = [name]\n            if not isinstance(value, list):\n                value = [value]\n            return list(zip(name, value))\n        else:\n            return self.get_name_value()", "response": "Returns zipped name and value pairs for global results."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_binary_stats(self, label, pred):\n        pred = pred.asnumpy()\n        label = label.asnumpy().astype('int32')\n        pred_label = numpy.argmax(pred, axis=1)\n\n        check_label_shapes(label, pred)\n        if len(numpy.unique(label)) > 2:\n            raise ValueError(\"%s currently only supports binary classification.\"\n                             % self.__class__.__name__)\n        pred_true = (pred_label == 1)\n        pred_false = 1 - pred_true\n        label_true = (label == 1)\n        label_false = 1 - label_true\n\n        true_pos = (pred_true * label_true).sum()\n        false_pos = (pred_true * label_false).sum()\n        false_neg = (pred_false * label_true).sum()\n        true_neg = (pred_false * label_false).sum()\n        self.true_positives += true_pos\n        self.global_true_positives += true_pos\n        self.false_positives += false_pos\n        self.global_false_positives += false_pos\n        self.false_negatives += false_neg\n        self.global_false_negatives += false_neg\n        self.true_negatives += true_neg\n        self.global_true_negatives += true_neg", "response": "Update the internal statistics for a single label and pred."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef matthewscc(self, use_global=False):\n        if use_global:\n            if not self.global_total_examples:\n                return 0.\n\n            true_pos = float(self.global_true_positives)\n            false_pos = float(self.global_false_positives)\n            false_neg = float(self.global_false_negatives)\n            true_neg = float(self.global_true_negatives)\n        else:\n            if not self.total_examples:\n                return 0.\n\n            true_pos = float(self.true_positives)\n            false_pos = float(self.false_positives)\n            false_neg = float(self.false_negatives)\n            true_neg = float(self.true_negatives)\n\n        terms = [(true_pos + false_pos),\n                 (true_pos + false_neg),\n                 (true_neg + false_pos),\n                 (true_neg + false_neg)]\n        denom = 1.\n        for t in filter(lambda t: t != 0., terms):\n            denom *= t\n        return ((true_pos * true_neg) - (false_pos * false_neg)) / math.sqrt(denom)", "response": "Calculate the Matthew s Correlation Coefficent ArcGIS."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a new dataset with each sample transformed by the the transformer function fn.", "response": "def transform(self, fn, lazy=True):\n        \"\"\"Returns a new dataset with each sample transformed by the\n        transformer function `fn`.\n\n        Parameters\n        ----------\n        fn : callable\n            A transformer function that takes a sample as input and\n            returns the transformed sample.\n        lazy : bool, default True\n            If False, transforms all samples at once. Otherwise,\n            transforms each sample on demand. Note that if `fn`\n            is stochastic, you must set lazy to True or you will\n            get the same result on all epochs.\n\n        Returns\n        -------\n        Dataset\n            The transformed dataset.\n        \"\"\"\n        trans = _LazyTransformDataset(self, fn)\n        if lazy:\n            return trans\n        return SimpleDataset([i for i in trans])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef transform_first(self, fn, lazy=True):\n        return self.transform(_TransformFirstClosure(fn), lazy)", "response": "Returns a new dataset with the first element of each sample\n            transformed by the transformer function fn."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef forward_ocr(self, img_):\n        img_ = cv2.resize(img_, (80, 30))\n        img_ = img_.transpose(1, 0)\n        print(img_.shape)\n        img_ = img_.reshape((1, 80, 30))\n        print(img_.shape)\n        # img_ = img_.reshape((80 * 30))\n        img_ = np.multiply(img_, 1 / 255.0)\n        self.predictor.forward(data=img_, **self.init_state_dict)\n        prob = self.predictor.get_output(0)\n        label_list = []\n        for p in prob:\n            print(np.argsort(p))\n            max_index = np.argsort(p)[::-1][0]\n            label_list.append(max_index)\n        return self.__get_string(label_list)", "response": "Forward the image through the LSTM network model and return the label list of the entries in the label list."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read_prototxt(fname):\n    proto = caffe_pb2.NetParameter()\n    with open(fname, 'r') as f:\n        text_format.Merge(str(f.read()), proto)\n    return proto", "response": "Read a caffe_pb2. NetParameter object that defined in a prototxt file"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_layers(proto):\n    if len(proto.layer):\n        return proto.layer\n    elif len(proto.layers):\n        return proto.layers\n    else:\n        raise ValueError('Invalid proto file.')", "response": "Returns layers in a caffe_pb2. NetParameter object"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read_caffemodel(prototxt_fname, caffemodel_fname):\n    if use_caffe:\n        caffe.set_mode_cpu()\n        net = caffe.Net(prototxt_fname, caffemodel_fname, caffe.TEST)\n        layer_names = net._layer_names\n        layers = net.layers\n        return (layers, layer_names)\n    else:\n        proto = caffe_pb2.NetParameter()\n        with open(caffemodel_fname, 'rb') as f:\n            proto.ParseFromString(f.read())\n        return (get_layers(proto), None)", "response": "Return a caffe_pb2. NetParameter object that defined in a binary caffemodel file"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef layer_iter(layers, layer_names):\n    if use_caffe:\n        for layer_idx, layer in enumerate(layers):\n            layer_name = re.sub('[-/]', '_', layer_names[layer_idx])\n            layer_type = layer.type\n            layer_blobs = layer.blobs\n            yield (layer_name, layer_type, layer_blobs)\n    else:\n        for layer in layers:\n            layer_name = re.sub('[-/]', '_', layer.name)\n            layer_type = layer.type\n            layer_blobs = layer.blobs\n            yield (layer_name, layer_type, layer_blobs)", "response": "Iterate over all layers in a list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting up the configuration of profiler.", "response": "def set_config(**kwargs):\n    \"\"\"Set up the configure of profiler (only accepts keyword arguments).\n\n    Parameters\n    ----------\n    filename : string,\n        output file for profile data\n    profile_all : boolean,\n        all profile types enabled\n    profile_symbolic : boolean,\n        whether to profile symbolic operators\n    profile_imperative : boolean,\n        whether to profile imperative operators\n    profile_memory : boolean,\n        whether to profile memory usage\n    profile_api : boolean,\n        whether to profile the C API\n    contiguous_dump : boolean,\n        whether to periodically dump profiling data to file\n    dump_period : float,\n        seconds between profile data dumps\n    aggregate_stats : boolean,\n        whether to maintain aggregate stats in memory for console\n        dump.  Has some negative performance impact.\n    profile_process : string\n        whether to profile kvstore `server` or `worker`.\n        server can only be profiled when kvstore is of type dist.\n        if this is not passed, defaults to `worker`\n    \"\"\"\n    kk = kwargs.keys()\n    vv = kwargs.values()\n    check_call(_LIB.MXSetProcessProfilerConfig(len(kwargs),\n                                               c_str_array([key for key in kk]),\n                                               c_str_array([str(val) for val in vv]),\n                                               profiler_kvstore_handle))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef profiler_set_config(mode='symbolic', filename='profile.json'):\n    warnings.warn('profiler.profiler_set_config() is deprecated. '\n                  'Please use profiler.set_config() instead')\n    keys = c_str_array([key for key in [\"profile_\" + mode, \"filename\"]])\n    values = c_str_array([str(val) for val in [True, filename]])\n    assert len(keys) == len(values)\n    check_call(_LIB.MXSetProcessProfilerConfig(len(keys), keys, values, profiler_kvstore_handle))", "response": "Set up the configure of profiler (Deprecated).\n\n    Parameters\n    ----------\n    mode : string, optional\n        Indicates whether to enable the profiler, can\n        be 'symbolic', or 'all'. Defaults to `symbolic`.\n    filename : string, optional\n        The name of output trace file. Defaults to 'profile.json'."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_state(state='stop', profile_process='worker'):\n    state2int = {'stop': 0, 'run': 1}\n    profile_process2int = {'worker': 0, 'server': 1}\n    check_call(_LIB.MXSetProcessProfilerState(ctypes.c_int(state2int[state]),\n                                              profile_process2int[profile_process],\n                                              profiler_kvstore_handle))", "response": "Sets the profiler state to run or stop."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dump(finished=True, profile_process='worker'):\n    fin = 1 if finished is True else 0\n    profile_process2int = {'worker': 0, 'server': 1}\n    check_call(_LIB.MXDumpProcessProfile(fin,\n                                         profile_process2int[profile_process],\n                                         profiler_kvstore_handle))", "response": "Dump the current profile and stop profiler."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a printable string of aggregate profile stats.", "response": "def dumps(reset=False):\n    \"\"\"Return a printable string of aggregate profile stats.\n\n    Parameters\n    ----------\n    reset: boolean\n        Indicates whether to clean aggeregate statistical data collected up to this point\n    \"\"\"\n    debug_str = ctypes.c_char_p()\n    do_reset = 1 if reset is True else 0\n    check_call(_LIB.MXAggregateProfileStatsPrint(ctypes.byref(debug_str), int(do_reset)))\n    return py_str(debug_str.value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pause(profile_process='worker'):\n    profile_process2int = {'worker': 0, 'server': 1}\n    check_call(_LIB.MXProcessProfilePause(int(1),\n                                          profile_process2int[profile_process],\n                                          profiler_kvstore_handle))", "response": "Pause profiling.\n\n    Parameters\n    ----------\n    profile_process : string\n        whether to profile kvstore `server` or `worker`.\n        server can only be profiled when kvstore is of type dist.\n        if this is not passed, defaults to `worker`"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nresumes paused profiling. Parameters ---------- profile_process : string whether to profile kvstore `server` or `worker`. server can only be profiled when kvstore is of type dist. if this is not passed, defaults to `worker`", "response": "def resume(profile_process='worker'):\n    \"\"\"\n    Resume paused profiling.\n\n    Parameters\n    ----------\n    profile_process : string\n        whether to profile kvstore `server` or `worker`.\n        server can only be profiled when kvstore is of type dist.\n        if this is not passed, defaults to `worker`\n    \"\"\"\n    profile_process2int = {'worker': 0, 'server': 1}\n    check_call(_LIB.MXProcessProfilePause(int(0),\n                                          profile_process2int[profile_process],\n                                          profiler_kvstore_handle))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_value(self, value):\n        check_call(_LIB.MXProfileSetCounter(self.handle, int(value)))", "response": "Set the value of the counter."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nincrementing the value of the internal counter by delta.", "response": "def increment(self, delta=1):\n        \"\"\"Increment counter value.\n\n        Parameters\n        ----------\n        value_change : int\n            Amount by which to add to the counter\n        \"\"\"\n        check_call(_LIB.MXProfileAdjustCounter(self.handle, int(delta)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef decrement(self, delta=1):\n        check_call(_LIB.MXProfileAdjustCounter(self.handle, -int(delta)))", "response": "Decrement the value of the internal counter by delta."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset up the profiler state to record operator.", "response": "def mark(self, scope='process'):\n        \"\"\"Set up the profiler state to record operator.\n\n        Parameters\n        ----------\n        scope : string, optional\n            Indicates what scope the marker should refer to.\n            Can be 'global', 'process', thread', task', and 'marker'\n            Default is `process`.\n        \"\"\"\n        check_call(_LIB.MXProfileSetMarker(self.domain.handle, c_str(self.name), c_str(scope)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget CUDA kernel from compiled module.", "response": "def get_kernel(self, name, signature):\n        r\"\"\"Get CUDA kernel from compiled module.\n\n        Parameters\n        ----------\n        name : str\n            String name of the kernel.\n        signature : str\n            Function signature for the kernel. For example, if a kernel is\n            declared as::\n\n                extern \"C\" __global__ void axpy(const float *x, double *y, int alpha)\n\n            Then its signature should be::\n\n                const float *x, double *y, int alpha\n\n            or::\n\n                const float *, double *, int\n\n            Note that `*` in signature marks an argument as array and\n            `const` marks an argument as constant (input) array.\n\n        Returns\n        -------\n        CudaKernel\n            CUDA kernels that can be launched on GPUs.\n        \"\"\"\n        hdl = CudaKernelHandle()\n        is_ndarray = []\n        is_const = []\n        dtypes = []\n        pattern = re.compile(r\"\"\"^\\s*(const)?\\s*([\\w_]+)\\s*(\\*)?\\s*([\\w_]+)?\\s*$\"\"\")\n        args = re.sub(r\"\\s+\", \" \", signature).split(\",\")\n        for arg in args:\n            match = pattern.match(arg)\n            if not match or match.groups()[1] == 'const':\n                raise ValueError(\n                    'Invalid function prototype \"%s\". Must be in the '\n                    'form of \"(const) type (*) (name)\"'%arg)\n            is_const.append(bool(match.groups()[0]))\n            dtype = match.groups()[1]\n            is_ndarray.append(bool(match.groups()[2]))\n            if dtype not in _DTYPE_CPP_TO_NP:\n                raise TypeError(\n                    \"Unsupported kernel argument type %s. Supported types are: %s.\"%(\n                        arg, ','.join(_DTYPE_CPP_TO_NP.keys())))\n            dtypes.append(_DTYPE_NP_TO_MX[_DTYPE_CPP_TO_NP[dtype]])\n\n        check_call(_LIB.MXRtcCudaKernelCreate(\n            self.handle,\n            c_str(name),\n            len(dtypes),\n            c_array_buf(ctypes.c_int, array('i', is_ndarray)),\n            c_array_buf(ctypes.c_int, array('i', is_const)),\n            c_array_buf(ctypes.c_int, array('i', dtypes)),\n            ctypes.byref(hdl)))\n\n        return CudaKernel(hdl, name, is_ndarray, dtypes)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef launch(self, args, ctx, grid_dims, block_dims, shared_mem=0):\n        assert ctx.device_type == 'gpu', \"Cuda kernel can only be launched on GPU\"\n        assert len(grid_dims) == 3, \"grid_dims must be a tuple of 3 integers\"\n        assert len(block_dims) == 3, \"grid_dims must be a tuple of 3 integers\"\n        assert len(args) == len(self._dtypes), \\\n            \"CudaKernel(%s) expects %d arguments but got %d\"%(\n                self._name, len(self._dtypes), len(args))\n        void_args = []\n        ref_holder = []\n        for i, (arg, is_nd, dtype) in enumerate(zip(args, self._is_ndarray, self._dtypes)):\n            if is_nd:\n                assert isinstance(arg, NDArray), \\\n                    \"The %d-th argument is expected to be a NDArray but got %s\"%(\n                        i, type(arg))\n                void_args.append(arg.handle)\n            else:\n                assert isinstance(arg, numeric_types), \\\n                    \"The %d-th argument is expected to be a number, but got %s\"%(\n                        i, type(arg))\n                ref_holder.append(np.array(arg, dtype=dtype))\n                void_args.append(ref_holder[-1].ctypes.data_as(ctypes.c_void_p))\n\n        check_call(_LIB.MXRtcCudaKernelCall(\n            self.handle,\n            ctx.device_id,\n            c_array(ctypes.c_void_p, void_args),\n            mx_uint(grid_dims[0]), mx_uint(grid_dims[1]), mx_uint(grid_dims[2]),\n            mx_uint(block_dims[0]), mx_uint(block_dims[1]), mx_uint(block_dims[2]),\n            mx_uint(shared_mem)))", "response": "Launch cuda kernel.\n\n        Parameters\n        ----------\n        args : tuple of NDArray or numbers\n            List of arguments for kernel. NDArrays are expected for pointer\n            types (e.g. `float*`, `double*`) while numbers are expected for\n            non-pointer types (e.g. `int`, `float`).\n        ctx : Context\n            The context to launch kernel on. Must be GPU context.\n        grid_dims : tuple of 3 integers\n            Grid dimensions for CUDA kernel.\n        block_dims : tuple of 3 integers\n            Block dimensions for CUDA kernel.\n        shared_mem : integer, optional\n            Size of dynamically allocated shared memory. Defaults to 0."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef reset(self):\n        if getattr(self, 'num', None) is None:\n            self.num_inst = 0\n            self.sum_metric = 0.0\n        else:\n            self.num_inst = [0] * self.num\n            self.sum_metric = [0.0] * self.num\n        self.records = dict()\n        self.counts = dict()", "response": "Clear the internal statistics to initial state."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates internal buffer with new information.", "response": "def update(self, labels, preds):\n        \"\"\"\n        Update internal records. This function now only update internal buffer,\n        sum_metric and num_inst are updated in _update() function instead when\n        get() is called to return results.\n\n        Params:\n        ----------\n        labels: mx.nd.array (n * 6) or (n * 5), difficult column is optional\n            2-d array of ground-truths, n objects(id-xmin-ymin-xmax-ymax-[difficult])\n        preds: mx.nd.array (m * 6)\n            2-d array of detections, m objects(id-score-xmin-ymin-xmax-ymax)\n        \"\"\"\n        def iou(x, ys):\n            \"\"\"\n            Calculate intersection-over-union overlap\n            Params:\n            ----------\n            x : numpy.array\n                single box [xmin, ymin ,xmax, ymax]\n            ys : numpy.array\n                multiple box [[xmin, ymin, xmax, ymax], [...], ]\n            Returns:\n            -----------\n            numpy.array\n                [iou1, iou2, ...], size == ys.shape[0]\n            \"\"\"\n            ixmin = np.maximum(ys[:, 0], x[0])\n            iymin = np.maximum(ys[:, 1], x[1])\n            ixmax = np.minimum(ys[:, 2], x[2])\n            iymax = np.minimum(ys[:, 3], x[3])\n            iw = np.maximum(ixmax - ixmin, 0.)\n            ih = np.maximum(iymax - iymin, 0.)\n            inters = iw * ih\n            uni = (x[2] - x[0]) * (x[3] - x[1]) + (ys[:, 2] - ys[:, 0]) * \\\n                (ys[:, 3] - ys[:, 1]) - inters\n            ious = inters / uni\n            ious[uni < 1e-12] = 0  # in case bad boxes\n            return ious\n\n        # independant execution for each image\n        for i in range(labels[0].shape[0]):\n            # get as numpy arrays\n            label = labels[0][i].asnumpy()\n            if np.sum(label[:, 0] >= 0) < 1:\n                continue\n            pred = preds[self.pred_idx][i].asnumpy()\n            # calculate for each class\n            while (pred.shape[0] > 0):\n                cid = int(pred[0, 0])\n                indices = np.where(pred[:, 0].astype(int) == cid)[0]\n                if cid < 0:\n                    pred = np.delete(pred, indices, axis=0)\n                    continue\n                dets = pred[indices]\n                pred = np.delete(pred, indices, axis=0)\n                # sort by score, desceding\n                dets = dets[dets[:,1].argsort()[::-1]]\n                records = np.hstack((dets[:, 1][:, np.newaxis], np.zeros((dets.shape[0], 1))))\n                # ground-truths\n                label_indices = np.where(label[:, 0].astype(int) == cid)[0]\n                gts = label[label_indices, :]\n                label = np.delete(label, label_indices, axis=0)\n                if gts.size > 0:\n                    found = [False] * gts.shape[0]\n                    for j in range(dets.shape[0]):\n                        # compute overlaps\n                        ious = iou(dets[j, 2:], gts[:, 1:5])\n                        ovargmax = np.argmax(ious)\n                        ovmax = ious[ovargmax]\n                        if ovmax > self.ovp_thresh:\n                            if (not self.use_difficult and\n                                gts.shape[1] >= 6 and\n                                gts[ovargmax, 5] > 0):\n                                pass\n                            else:\n                                if not found[ovargmax]:\n                                    records[j, -1] = 1  # tp\n                                    found[ovargmax] = True\n                                else:\n                                    # duplicate\n                                    records[j, -1] = 2  # fp\n                        else:\n                            records[j, -1] = 2 # fp\n                else:\n                    # no gt, mark all fp\n                    records[:, -1] = 2\n\n                # ground truth count\n                if (not self.use_difficult and gts.shape[1] >= 6):\n                    gt_count = np.sum(gts[:, 5] < 1)\n                else:\n                    gt_count = gts.shape[0]\n\n                # now we push records to buffer\n                # first column: score, second column: tp/fp\n                # 0: not set(matched to difficult or something), 1: tp, 2: fp\n                records = records[np.where(records[:, -1] > 0)[0], :]\n                if records.size > 0:\n                    self._insert(cid, records, gt_count)\n\n            # add missing class if not present in prediction\n            while (label.shape[0] > 0):\n                cid = int(label[0, 0])\n                label_indices = np.where(label[:, 0].astype(int) == cid)[0]\n                label = np.delete(label, label_indices, axis=0)\n                if cid < 0:\n                    continue\n                gt_count = label_indices.size\n                self._insert(cid, np.array([[0, 0]]), gt_count)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate num_inst and sum_metric", "response": "def _update(self):\n        \"\"\" update num_inst and sum_metric \"\"\"\n        aps = []\n        for k, v in self.records.items():\n            recall, prec = self._recall_prec(v, self.counts[k])\n            ap = self._average_precision(recall, prec)\n            aps.append(ap)\n            if self.num is not None and k < (self.num - 1):\n                self.sum_metric[k] = ap\n                self.num_inst[k] = 1\n        if self.num is None:\n            self.num_inst = 1\n            self.sum_metric = np.mean(aps)\n        else:\n            self.num_inst[-1] = 1\n            self.sum_metric[-1] = np.mean(aps)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _recall_prec(self, record, count):\n        record = np.delete(record, np.where(record[:, 1].astype(int) == 0)[0], axis=0)\n        sorted_records = record[record[:,0].argsort()[::-1]]\n        tp = np.cumsum(sorted_records[:, 1].astype(int) == 1)\n        fp = np.cumsum(sorted_records[:, 1].astype(int) == 2)\n        if count <= 0:\n            recall = tp * 0.0\n        else:\n            recall = tp / float(count)\n        prec = tp.astype(float) / (tp + fp)\n        return recall, prec", "response": "get recall and precision from internal records"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the average precision of a set of recall values and returns the average precision integration of the set of recall values.", "response": "def _average_precision(self, rec, prec):\n        \"\"\"\n        calculate average precision\n\n        Params:\n        ----------\n        rec : numpy.array\n            cumulated recall\n        prec : numpy.array\n            cumulated precision\n        Returns:\n        ----------\n        ap as float\n        \"\"\"\n        # append sentinel values at both ends\n        mrec = np.concatenate(([0.], rec, [1.]))\n        mpre = np.concatenate(([0.], prec, [0.]))\n\n        # compute precision integration ladder\n        for i in range(mpre.size - 1, 0, -1):\n            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n        # look for recall value changes\n        i = np.where(mrec[1:] != mrec[:-1])[0]\n\n        # sum (\\delta recall) * prec\n        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n        return ap"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninserting records according to key", "response": "def _insert(self, key, records, count):\n        \"\"\" Insert records according to key \"\"\"\n        if key not in self.records:\n            assert key not in self.counts\n            self.records[key] = records\n            self.counts[key] = count\n        else:\n            self.records[key] = np.vstack((self.records[key], records))\n            assert key in self.counts\n            self.counts[key] += count"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the average precision of the cumulated recall and precision.", "response": "def _average_precision(self, rec, prec):\n        \"\"\"\n        calculate average precision, override the default one,\n        special 11-point metric\n\n        Params:\n        ----------\n        rec : numpy.array\n            cumulated recall\n        prec : numpy.array\n            cumulated precision\n        Returns:\n        ----------\n        ap as float\n        \"\"\"\n        ap = 0.\n        for t in np.arange(0., 1.1, 0.1):\n            if np.sum(rec >= t) == 0:\n                p = 0\n            else:\n                p = np.max(prec[rec >= t])\n            ap += p / 11.\n        return ap"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_fine_tune_model(symbol, arg_params, num_classes, layer_name, dtype='float32'):\n    all_layers = symbol.get_internals()\n    net = all_layers[layer_name+'_output']\n    net = mx.symbol.FullyConnected(data=net, num_hidden=num_classes, name='fc')\n    if dtype == 'float16':\n        net = mx.sym.Cast(data=net, dtype=np.float32)\n    net = mx.symbol.SoftmaxOutput(data=net, name='softmax')\n    new_args = dict({k:arg_params[k] for k in arg_params if 'fc' not in k})\n    return (net, new_args)", "response": "Returns the model for the fine - tuning dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _list_images(self, root):\n        self.labels = []\n        self.items = []\n\n        valid_unseen_sub_idx = [1, 2, 20, 22]\n        skip_sub_idx = [21]\n\n        if self._mode == 'train':\n            sub_idx = ['s' + str(i) for i in range(1, 35) \\\n                             if i not in valid_unseen_sub_idx + skip_sub_idx]\n        elif self._mode == 'valid':\n            sub_idx = ['s' + str(i) for i in valid_unseen_sub_idx]\n\n        folder_path = []\n        for i in sub_idx:\n            folder_path.extend(glob.glob(os.path.join(root, i, \"*\")))\n\n        for folder in folder_path:\n            filename = glob.glob(os.path.join(folder, \"*\"))\n            if len(filename) != self._seq_len:\n                continue\n            filename.sort()\n            label = os.path.split(folder)[-1]\n            self.items.append((filename, label))", "response": "Generate list of images for lip images"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_verbosity(self, verbose=False, print_func=None):\n        self._verbose = verbose\n        if print_func is None:\n            def asum_stat(x):\n                \"\"\"returns |x|/size(x), async execution.\"\"\"\n                return str((ndarray.norm(x)/sqrt(x.size)).asscalar())\n            print_func = asum_stat\n        self._print_func = print_func\n        return self", "response": "Switch on or off verbose mode."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _legacy_init(self, name, arr):\n        warnings.warn(\n            \"\\033[91mCalling initializer with init(str, NDArray) has been deprecated.\" \\\n            \"please use init(mx.init.InitDesc(...), NDArray) instead.\\033[0m\",\n            DeprecationWarning, stacklevel=3)\n        if not isinstance(name, string_types):\n            raise TypeError('name must be string')\n        if not isinstance(arr, NDArray):\n            raise TypeError('arr must be NDArray')\n        if name.startswith('upsampling'):\n            self._init_bilinear(name, arr)\n        elif name.startswith('stn_loc') and name.endswith('weight'):\n            self._init_zero(name, arr)\n        elif name.startswith('stn_loc') and name.endswith('bias'):\n            self._init_loc_bias(name, arr)\n        elif name.endswith('bias'):\n            self._init_bias(name, arr)\n        elif name.endswith('gamma'):\n            self._init_gamma(name, arr)\n        elif name.endswith('beta'):\n            self._init_beta(name, arr)\n        elif name.endswith('weight'):\n            self._init_weight(name, arr)\n        elif name.endswith(\"moving_mean\"):\n            self._init_zero(name, arr)\n        elif name.endswith(\"moving_var\"):\n            self._init_one(name, arr)\n        elif name.endswith(\"moving_inv_var\"):\n            self._init_zero(name, arr)\n        elif name.endswith(\"moving_avg\"):\n            self._init_zero(name, arr)\n        elif name.endswith('min'):\n            self._init_zero(name, arr)\n        elif name.endswith('max'):\n            self._init_one(name, arr)\n        else:\n            self._init_default(name, arr)", "response": "Legacy initialization method.\n\n        Parameters\n        ----------\n        name : str\n            Name of corresponding NDArray.\n\n        arr : NDArray\n            NDArray to be initialized."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsaves imglist to disk", "response": "def save_imglist(self, fname=None, root=None, shuffle=False):\n        \"\"\"\n        save imglist to disk\n\n        Parameters:\n        ----------\n        fname : str\n            saved filename\n        \"\"\"\n        def progress_bar(count, total, suffix=''):\n            import sys\n            bar_len = 24\n            filled_len = int(round(bar_len * count / float(total)))\n\n            percents = round(100.0 * count / float(total), 1)\n            bar = '=' * filled_len + '-' * (bar_len - filled_len)\n            sys.stdout.write('[%s] %s%s ...%s\\r' % (bar, percents, '%', suffix))\n            sys.stdout.flush()\n\n        str_list = []\n        for index in range(self.num_images):\n            progress_bar(index, self.num_images)\n            label = self.label_from_index(index)\n            if label.size < 1:\n                continue\n            path = self.image_path_from_index(index)\n            if root:\n                path = osp.relpath(path, root)\n            str_list.append('\\t'.join([str(index), str(2), str(label.shape[1])] \\\n              + [\"{0:.4f}\".format(x) for x in label.ravel()] + [path,]) + '\\n')\n        if str_list:\n            if shuffle:\n                import random\n                random.shuffle(str_list)\n            if not fname:\n                fname = self.name + '.lst'\n            with open(fname, 'w') as f:\n                for line in str_list:\n                    f.write(line)\n        else:\n            raise RuntimeError(\"No image in imdb\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads class names from text file.", "response": "def _load_class_names(self, filename, dirname):\n        \"\"\"\n        load class names from text file\n\n        Parameters:\n        ----------\n        filename: str\n            file stores class names\n        dirname: str\n            file directory\n        \"\"\"\n        full_path = osp.join(dirname, filename)\n        classes = []\n        with open(full_path, 'r') as f:\n            classes = [l.strip() for l in f.readlines()]\n        return classes"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_data(label, image):\n    base_url = 'http://yann.lecun.com/exdb/mnist/'\n    with gzip.open(download_file(base_url+label, os.path.join('data',label))) as flbl:\n        magic, num = struct.unpack(\">II\", flbl.read(8))\n        label = np.fromstring(flbl.read(), dtype=np.int8)\n    with gzip.open(download_file(base_url+image, os.path.join('data',image)), 'rb') as fimg:\n        magic, num, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n        image = np.fromstring(fimg.read(), dtype=np.uint8).reshape(len(label), rows, cols)\n    return (label, image)", "response": "download and read data into numpy\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate data iterator with NDArrayIter", "response": "def get_mnist_iter(args, kv):\n    \"\"\"\n    create data iterator with NDArrayIter\n    \"\"\"\n    (train_lbl, train_img) = read_data(\n            'train-labels-idx1-ubyte.gz', 'train-images-idx3-ubyte.gz')\n    (val_lbl, val_img) = read_data(\n            't10k-labels-idx1-ubyte.gz', 't10k-images-idx3-ubyte.gz')\n    train = mx.io.NDArrayIter(\n        to4d(train_img), train_lbl, args.batch_size, shuffle=True)\n    val = mx.io.NDArrayIter(\n        to4d(val_img), val_lbl, args.batch_size)\n    return (train, val)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_file_extension_assertion(extension):\n    def file_extension_assertion(file_path):\n        base, ext = os.path.splitext(file_path)\n        if ext.lower() != extension:\n            raise argparse.ArgumentTypeError('File must have ' + extension + ' extension')\n        return file_path\n    return file_extension_assertion", "response": "Function factory for file extension argparse assertion\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_palette(num_colors=256):\n    pallete = [0]*(num_colors*3)\n    for j in range(0, num_colors):\n        lab = j\n        pallete[j*3+0] = 0\n        pallete[j*3+1] = 0\n        pallete[j*3+2] = 0\n        i = 0\n        while (lab > 0):\n            pallete[j*3+0] |= (((lab >> 0) & 1) << (7-i))\n            pallete[j*3+1] |= (((lab >> 1) & 1) << (7-i))\n            pallete[j*3+2] |= (((lab >> 2) & 1) << (7-i))\n            i = i + 1\n            lab >>= 3\n    return pallete", "response": "generates the colormap for visualizing the segmentation mask\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_data(img_path):\n    mean = np.array([123.68, 116.779, 103.939])  # (R,G,B)\n    img = Image.open(img_path)\n    img = np.array(img, dtype=np.float32)\n    reshaped_mean = mean.reshape(1, 1, 3)\n    img = img - reshaped_mean\n    img = np.swapaxes(img, 0, 2)\n    img = np.swapaxes(img, 1, 2)\n    img = np.expand_dims(img, axis=0)\n    return img", "response": "get the data for the supplied image in the order of the image in order"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef main():\n    # Initialization variables - update to change your model and execution context\n    model_prefix = \"FCN8s_VGG16\"\n    epoch = 19\n\n    # By default, MXNet will run on the CPU. Change to ctx = mx.gpu() to run on GPU.\n    ctx = mx.cpu()\n\n    fcnxs, fcnxs_args, fcnxs_auxs = mx.model.load_checkpoint(model_prefix, epoch)\n    fcnxs_args[\"data\"] = mx.nd.array(get_data(args.input), ctx)\n    data_shape = fcnxs_args[\"data\"].shape\n    label_shape = (1, data_shape[2]*data_shape[3])\n    fcnxs_args[\"softmax_label\"] = mx.nd.empty(label_shape, ctx)\n    exector = fcnxs.bind(ctx, fcnxs_args, args_grad=None, grad_req=\"null\", aux_states=fcnxs_args)\n    exector.forward(is_train=False)\n    output = exector.outputs[0]\n    out_img = np.uint8(np.squeeze(output.asnumpy().argmax(axis=1)))\n    out_img = Image.fromarray(out_img)\n    out_img.putpalette(get_palette())\n    out_img.save(args.output)", "response": "Main function for the\n    module."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks input imdbs make sure they have same classes", "response": "def _check_classes(self):\n        \"\"\"\n        check input imdbs, make sure they have same classes\n        \"\"\"\n        try:\n            self.classes = self.imdbs[0].classes\n            self.num_classes = len(self.classes)\n        except AttributeError:\n            # fine, if no classes is provided\n            pass\n\n        if self.num_classes > 0:\n            for db in self.imdbs:\n                assert self.classes == db.classes, \"Multiple imdb must have same classes\""}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _load_image_set_index(self, shuffle):\n        self.num_images = 0\n        for db in self.imdbs:\n            self.num_images += db.num_images\n        indices = list(range(self.num_images))\n        if shuffle:\n            random.shuffle(indices)\n        return indices", "response": "Load image set indices."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngiving an index find out sub - db and sub - index.", "response": "def _locate_index(self, index):\n        \"\"\"\n        given index, find out sub-db and sub-index\n\n        Parameters\n        ----------\n        index : int\n            index of a specific image\n\n        Returns\n        ----------\n        a tuple (sub-db, sub-index)\n        \"\"\"\n        assert index >= 0 and index < self.num_images, \"index out of range\"\n        pos = self.image_set_index[index]\n        for k, v in enumerate(self.imdbs):\n            if pos >= v.num_images:\n                pos -= v.num_images\n            else:\n                return (k, pos)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive an index find out full path of this image", "response": "def image_path_from_index(self, index):\n        \"\"\"\n        given image index, find out full path\n\n        Parameters\n        ----------\n        index: int\n            index of a specific image\n\n        Returns\n        ----------\n        full path of this image\n        \"\"\"\n        assert self.image_set_index is not None, \"Dataset not initialized\"\n        pos = self.image_set_index[index]\n        n_db, n_index = self._locate_index(index)\n        return self.imdbs[n_db].image_path_from_index(n_index)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef do_checkpoint(prefix, period=1):\n    period = int(max(1, period))\n    def _callback(iter_no, sym, arg, aux):\n        \"\"\"The checkpoint function.\"\"\"\n        if (iter_no + 1) % period == 0:\n            save_checkpoint(prefix, iter_no + 1, sym, arg, aux)\n    return _callback", "response": "A callback that saves a model checkpoint every few epochs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef log_train_metric(period, auto_reset=False):\n    def _callback(param):\n        \"\"\"The checkpoint function.\"\"\"\n        if param.nbatch % period == 0 and param.eval_metric is not None:\n            name_value = param.eval_metric.get_name_value()\n            for name, value in name_value:\n                logging.info('Iter[%d] Batch[%d] Train-%s=%f',\n                             param.epoch, param.nbatch, name, value)\n            if auto_reset:\n                param.eval_metric.reset_local()\n    return _callback", "response": "Callback to log the training evaluation result every period."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef install(self, exe):\n        exe.set_monitor_callback(self.stat_helper, self.monitor_all)\n        self.exes.append(exe)", "response": "Install callback to executor."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef tic(self):\n        if self.step % self.interval == 0:\n            for exe in self.exes:\n                for array in exe.arg_arrays:\n                    array.wait_to_read()\n                for array in exe.aux_arrays:\n                    array.wait_to_read()\n            self.queue = []\n            self.activated = True\n        self.step += 1", "response": "Start collecting stats for current batch."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef toc(self):\n        if not self.activated:\n            return []\n        for exe in self.exes:\n            for array in exe.arg_arrays:\n                array.wait_to_read()\n            for array in exe.aux_arrays:\n                array.wait_to_read()\n        for exe in self.exes:\n            for name, array in zip(exe._symbol.list_arguments(), exe.arg_arrays):\n                if self.re_prog.match(name):\n                    self.queue.append((self.step, name, self.stat_func(array)))\n            for name, array in zip(exe._symbol.list_auxiliary_states(), exe.aux_arrays):\n                if self.re_prog.match(name):\n                    self.queue.append((self.step, name, self.stat_func(array)))\n        self.activated = False\n        res = []\n        if self.sort:\n            self.queue.sort(key=lambda x: x[1])\n        for n, k, v_list in self.queue:\n            if isinstance(v_list, NDArray):\n                v_list = [v_list]\n            assert isinstance(v_list, list)\n            s = ''\n            for v in v_list:\n                assert isinstance(v, NDArray)\n                if v.shape == (1,):\n                    s += str(v.asscalar()) + '\\t'\n                else:\n                    s += str(v.asnumpy()) + '\\t'\n            res.append((n, k, s))\n        self.queue = []\n        return res", "response": "End collecting for current batch and return results."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef toc_print(self):\n        res = self.toc()\n        for n, k, v in res:\n            logging.info('Batch: {:7d} {:30s} {:s}'.format(n, k, v))", "response": "End collecting and print results."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_data_iter_plan(self):\n        \"make a random data iteration plan\"\n        # truncate each bucket into multiple of batch-size\n        bucket_n_batches = []\n        for i in range(len(self.data)):\n            bucket_n_batches.append(np.floor((self.data[i]) / self.batch_size))\n            self.data[i] = self.data[i][:int(bucket_n_batches[i]*self.batch_size)]\n\n        bucket_plan = np.hstack([np.zeros(n, int)+i for i, n in enumerate(bucket_n_batches)])\n        np.random.shuffle(bucket_plan)\n\n        bucket_idx_all = [np.random.permutation(len(x)) for x in self.data]\n\n        self.bucket_plan = bucket_plan\n        self.bucket_idx_all = bucket_idx_all\n        self.bucket_curr_idx = [0 for x in self.data]\n\n        self.data_buffer = []\n        self.label_buffer = []\n        for i_bucket in range(len(self.data)):\n            if not self.model_parallel:\n                data = np.zeros((self.batch_size, self.buckets[i_bucket]))\n                label = np.zeros((self.batch_size, self.buckets[i_bucket]))\n                self.data_buffer.append(data)\n                self.label_buffer.append(label)\n            else:\n                data = np.zeros((self.buckets[i_bucket], self.batch_size))\n                self.data_buffer.append(data)\n\n        if self.model_parallel:\n            # Transpose data if model parallel\n            for i in range(len(self.data)):\n                bucket_data = self.data[i]\n                self.data[i] = np.transpose(bucket_data)", "response": "make a random data iteration plan"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexpand the pending files in the current stage.", "response": "def expand(x, pending, stage):\n    \"\"\"\n    Expand the pending files in the current stage.\n\n    Parameters\n    ----------\n    x: str\n         The file to expand.\n    pending : str\n         The list of pending files to expand.\n    stage: str\n         The current stage for file expansion, used for matching the prefix of files.\n    \"\"\"\n    if x in history and x not in ['mshadow/mshadow/expr_scalar-inl.h']: # MULTIPLE includes\n        return\n\n    if x in pending:\n        #print('loop found: {} in {}'.format(x, pending))\n        return\n\n    whtspace = '  ' * expand.treeDepth\n    expand.fileCount += 1\n    comment = u\"//=====[{:3d}] STAGE:{:>4} {}EXPANDING: {} =====\\n\\n\".format(expand.fileCount, stage, whtspace, x)\n    out.write(comment.encode('ascii'))\n    print(comment)\n\n    with open(x, 'rb') as x_h:\n        for line in x_h.readlines():\n            uline = line.decode('utf-8')\n            if '#define DMLC_LOG_STACK_TRACE 1' in uline.strip():\n                # Do not enable stacktrace logging\n                continue\n            if uline.find('#include') < 0:\n                out.write(line)\n                continue\n            if uline.strip().find('#include') > 0:\n                print(uline)\n                continue\n            m = re1.search(uline)\n            if not m:\n                m = re2.search(uline)\n            if m:\n                path = m.groups()[0]\n            else:\n                m = re3.search(uline)\n                if m:\n                    path = 'execinfo.h'\n                else:\n                    print(uline + ' not found')\n                    continue\n            h = path.strip('./') if \"../3rdparty/\" not in path else path\n            if h.endswith('complex.h') and x.endswith('openblas_config.h'):\n                source = ''\n            elif h.startswith('ps/'):\n                source = '../3rdparty/ps-lite/include/' + h\n            else:\n                source = find_source(h, x, stage)\n            if not source:\n                if (h not in blacklist and\n                    h not in sysheaders and\n                    'mkl' not in h and\n                    'nnpack' not in h and\n                    'tensorrt' not in h and\n                    not h.endswith('.cuh')): sysheaders.append(h)\n            else:\n                expand.treeDepth += 1\n                expand(source, pending + [x], stage)\n                expand.treeDepth -= 1\n\n    out.write(u\"//===== EXPANDED  : {} =====\\n\\n\".format(x).encode('ascii'))\n    history.add(x)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating an instance of token embedding.", "response": "def create(embedding_name, **kwargs):\n    \"\"\"Creates an instance of token embedding.\n\n\n    Creates a token embedding instance by loading embedding vectors from an externally hosted\n    pre-trained token embedding file, such as those of GloVe and FastText. To get all the valid\n    `embedding_name` and `pretrained_file_name`, use\n    `mxnet.contrib.text.embedding.get_pretrained_file_names()`.\n\n\n    Parameters\n    ----------\n    embedding_name : str\n        The token embedding name (case-insensitive).\n\n\n    Returns\n    -------\n    An instance of `mxnet.contrib.text.glossary._TokenEmbedding`:\n        A token embedding instance that loads embedding vectors from an externally hosted\n        pre-trained token embedding file.\n    \"\"\"\n\n    create_text_embedding = registry.get_create_func(_TokenEmbedding, 'token embedding')\n    return create_text_embedding(embedding_name, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets all the pre - trained token embedding file names.", "response": "def get_pretrained_file_names(embedding_name=None):\n    \"\"\"Get valid token embedding names and their pre-trained file names.\n\n\n    To load token embedding vectors from an externally hosted pre-trained token embedding file,\n    such as those of GloVe and FastText, one should use\n    `mxnet.contrib.text.embedding.create(embedding_name, pretrained_file_name)`.\n    This method returns all the valid names of `pretrained_file_name` for the specified\n    `embedding_name`. If `embedding_name` is set to None, this method returns all the valid\n    names of `embedding_name` with their associated `pretrained_file_name`.\n\n\n    Parameters\n    ----------\n    embedding_name : str or None, default None\n        The pre-trained token embedding name.\n\n\n    Returns\n    -------\n    dict or list:\n        A list of all the valid pre-trained token embedding file names (`pretrained_file_name`)\n        for the specified token embedding name (`embedding_name`). If the text embeding name is\n        set to None, returns a dict mapping each valid token embedding name to a list of valid\n        pre-trained files (`pretrained_file_name`). They can be plugged into\n        `mxnet.contrib.text.embedding.create(embedding_name,\n        pretrained_file_name)`.\n    \"\"\"\n\n    text_embedding_reg = registry.get_registry(_TokenEmbedding)\n\n    if embedding_name is not None:\n        if embedding_name not in text_embedding_reg:\n            raise KeyError('Cannot find `embedding_name` %s. Use '\n                           '`get_pretrained_file_names('\n                           'embedding_name=None).keys()` to get all the valid embedding '\n                           'names.' % embedding_name)\n        return list(text_embedding_reg[embedding_name].pretrained_file_name_sha1.keys())\n    else:\n        return {embedding_name: list(embedding_cls.pretrained_file_name_sha1.keys())\n                for embedding_name, embedding_cls in registry.get_registry(_TokenEmbedding).items()}"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading embedding vectors from the pre - trained token embedding file.", "response": "def _load_embedding(self, pretrained_file_path, elem_delim, init_unknown_vec, encoding='utf8'):\n        \"\"\"Load embedding vectors from the pre-trained token embedding file.\n\n\n        For every unknown token, if its representation `self.unknown_token` is encountered in the\n        pre-trained token embedding file, index 0 of `self.idx_to_vec` maps to the pre-trained token\n        embedding vector loaded from the file; otherwise, index 0 of `self.idx_to_vec` maps to the\n        text embedding vector initialized by `init_unknown_vec`.\n\n        If a token is encountered multiple times in the pre-trained text embedding file, only the\n        first-encountered token embedding vector will be loaded and the rest will be skipped.\n        \"\"\"\n\n        pretrained_file_path = os.path.expanduser(pretrained_file_path)\n\n        if not os.path.isfile(pretrained_file_path):\n            raise ValueError('`pretrained_file_path` must be a valid path to '\n                             'the pre-trained token embedding file.')\n\n        logging.info('Loading pre-trained token embedding vectors from %s', pretrained_file_path)\n        vec_len = None\n        all_elems = []\n        tokens = set()\n        loaded_unknown_vec = None\n        line_num = 0\n        with io.open(pretrained_file_path, 'r', encoding=encoding) as f:\n            for line in f:\n                line_num += 1\n                elems = line.rstrip().split(elem_delim)\n\n                assert len(elems) > 1, 'At line %d of the pre-trained text embedding file: the ' \\\n                                       'data format of the pre-trained token embedding file %s ' \\\n                                       'is unexpected.' % (line_num, pretrained_file_path)\n\n                token, elems = elems[0], [float(i) for i in elems[1:]]\n\n                if token == self.unknown_token and loaded_unknown_vec is None:\n                    loaded_unknown_vec = elems\n                    tokens.add(self.unknown_token)\n                elif token in tokens:\n                    warnings.warn('At line %d of the pre-trained token embedding file: the '\n                                  'embedding vector for token %s has been loaded and a duplicate '\n                                  'embedding for the  same token is seen and skipped.' %\n                                  (line_num, token))\n                elif len(elems) == 1:\n                    warnings.warn('At line %d of the pre-trained text embedding file: token %s '\n                                  'with 1-dimensional vector %s is likely a header and is '\n                                  'skipped.' % (line_num, token, elems))\n                else:\n                    if vec_len is None:\n                        vec_len = len(elems)\n                        # Reserve a vector slot for the unknown token at the very beggining because\n                        # the unknown index is 0.\n                        all_elems.extend([0] * vec_len)\n                    else:\n                        assert len(elems) == vec_len, \\\n                            'At line %d of the pre-trained token embedding file: the dimension ' \\\n                            'of token %s is %d but the dimension of previous tokens is %d. ' \\\n                            'Dimensions of all the tokens must be the same.' \\\n                            % (line_num, token, len(elems), vec_len)\n                    all_elems.extend(elems)\n                    self._idx_to_token.append(token)\n                    self._token_to_idx[token] = len(self._idx_to_token) - 1\n                    tokens.add(token)\n\n        self._vec_len = vec_len\n        self._idx_to_vec = nd.array(all_elems).reshape((-1, self.vec_len))\n\n        if loaded_unknown_vec is None:\n            self._idx_to_vec[C.UNKNOWN_IDX] = init_unknown_vec(shape=self.vec_len)\n        else:\n            self._idx_to_vec[C.UNKNOWN_IDX] = nd.array(loaded_unknown_vec)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the mapping between token indices and token embedding vectors.", "response": "def _set_idx_to_vec_by_embeddings(self, token_embeddings, vocab_len, vocab_idx_to_token):\n        \"\"\"Sets the mapping between token indices and token embedding vectors.\n\n\n        Parameters\n        ----------\n        token_embeddings : instance or list `mxnet.contrib.text.embedding._TokenEmbedding`\n            One or multiple pre-trained token embeddings to load. If it is a list of multiple\n            embeddings, these embedding vectors will be concatenated for each token.\n        vocab_len : int\n            Length of vocabulary whose tokens are indexed in the token embedding.\n        vocab_idx_to_token: list of str\n            A list of indexed tokens in the vocabulary. These tokens are indexed in the token\n            embedding.\n        \"\"\"\n\n        new_vec_len = sum(embed.vec_len for embed in token_embeddings)\n        new_idx_to_vec = nd.zeros(shape=(vocab_len, new_vec_len))\n\n        col_start = 0\n        # Concatenate all the embedding vectors in token_embeddings.\n        for embed in token_embeddings:\n            col_end = col_start + embed.vec_len\n            # Cancatenate vectors of the unknown token.\n            new_idx_to_vec[0, col_start:col_end] = embed.idx_to_vec[0]\n            new_idx_to_vec[1:, col_start:col_end] = embed.get_vecs_by_tokens(vocab_idx_to_token[1:])\n            col_start = col_end\n\n        self._vec_len = new_vec_len\n        self._idx_to_vec = new_idx_to_vec"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_vecs_by_tokens(self, tokens, lower_case_backup=False):\n\n        to_reduce = False\n        if not isinstance(tokens, list):\n            tokens = [tokens]\n            to_reduce = True\n\n        if not lower_case_backup:\n            indices = [self.token_to_idx.get(token, C.UNKNOWN_IDX) for token in tokens]\n        else:\n            indices = [self.token_to_idx[token] if token in self.token_to_idx\n                       else self.token_to_idx.get(token.lower(), C.UNKNOWN_IDX)\n                       for token in tokens]\n\n        vecs = nd.Embedding(nd.array(indices), self.idx_to_vec, self.idx_to_vec.shape[0],\n                            self.idx_to_vec.shape[1])\n\n        return vecs[0] if to_reduce else vecs", "response": "Returns an embedding vector of the tokens."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_token_vectors(self, tokens, new_vectors):\n\n        assert self.idx_to_vec is not None, 'The property `idx_to_vec` has not been properly set.'\n\n        if not isinstance(tokens, list) or len(tokens) == 1:\n            assert isinstance(new_vectors, nd.NDArray) and len(new_vectors.shape) in [1, 2], \\\n                '`new_vectors` must be a 1-D or 2-D NDArray if `tokens` is a singleton.'\n            if not isinstance(tokens, list):\n                tokens = [tokens]\n            if len(new_vectors.shape) == 1:\n                new_vectors = new_vectors.expand_dims(0)\n\n        else:\n            assert isinstance(new_vectors, nd.NDArray) and len(new_vectors.shape) == 2, \\\n                '`new_vectors` must be a 2-D NDArray if `tokens` is a list of multiple strings.'\n        assert new_vectors.shape == (len(tokens), self.vec_len), \\\n            'The length of new_vectors must be equal to the number of tokens and the width of' \\\n            'new_vectors must be equal to the dimension of embeddings of the glossary.'\n\n        indices = []\n        for token in tokens:\n            if token in self.token_to_idx:\n                indices.append(self.token_to_idx[token])\n            else:\n                raise ValueError('Token %s is unknown. To update the embedding vector for an '\n                                 'unknown token, please specify it explicitly as the '\n                                 '`unknown_token` %s in `tokens`. This is to avoid unintended '\n                                 'updates.' % (token, self.idx_to_token[C.UNKNOWN_IDX]))\n\n        self._idx_to_vec[nd.array(indices)] = new_vectors", "response": "Updates the embedding vectors for the given list of tokens."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _check_pretrained_file_names(cls, pretrained_file_name):\n\n        embedding_name = cls.__name__.lower()\n        if pretrained_file_name not in cls.pretrained_file_name_sha1:\n            raise KeyError('Cannot find pretrained file %s for token embedding %s. Valid '\n                           'pretrained files for embedding %s: %s' %\n                           (pretrained_file_name, embedding_name, embedding_name,\n                            ', '.join(cls.pretrained_file_name_sha1.keys())))", "response": "Checks if a pre - trained token embedding file name is valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef calc_grad(exe, exe_grads, params, X, Y, label_name=None, outgrad_f=None):\n    exe.copy_params_from(params)\n    exe.arg_dict['data'][:] = X\n    if outgrad_f is None:\n        exe.arg_dict[label_name][:] = Y\n        exe.forward(is_train=True)\n        exe.backward()\n    else:\n        exe.forward(is_train=True)\n        exe.backward(outgrad_f(exe.outpus, Y))\n    for k, v in exe_grads.items():\n        v.wait_to_read()", "response": "Calculate gradient of the experiment."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef step_HMC(exe, exe_params, exe_grads, label_key, noise_precision, prior_precision, L=10, eps=1E-6):\n    init_params = {k: v.copyto(v.context) for k, v in exe_params.items()}\n    end_params = {k: v.copyto(v.context) for k, v in exe_params.items()}\n    init_momentums = {k: mx.random.normal(0, 1, v.shape) for k, v in init_params.items()}\n    end_momentums = {k: v.copyto(v.context) for k, v in init_momentums.items()}\n    init_potential = calc_potential(exe, init_params, label_key, noise_precision, prior_precision)\n\n    # 0. Calculate Initial Energy and Kinetic\n    init_kinetic = sum([nd.sum(nd.square(momentum)) / 2.0\n                        for momentum in init_momentums.values()]).asscalar()\n    # 1. Make a half step for momentum at the beginning\n    exe.copy_params_from(end_params)\n    exe.forward(is_train=True)\n    exe.backward()\n    for k, v in exe_grads.items():\n        v.wait_to_read()\n    for k, momentum in end_momentums.items():\n        momentum[:] = momentum - (eps / 2) * exe_grads[k]\n    # 2. Alternate full steps for position and momentum\n    for i in range(L):\n        # 2.1 Full step for position\n        for k, param in exe_params.items():\n            param[:] = param + eps * end_momentums[k]\n        # 2.2 Full step for the momentum, except at the end of trajectory we perform a half step\n        exe.forward(is_train=True)\n        exe.backward()\n        for v in exe_grads.values():\n            v.wait_to_read()\n        if i != L - 1:\n            for k, momentum in end_momentums.items():\n                momentum[:] = momentum - eps * exe_grads[k]\n        else:\n            for k, momentum in end_momentums.items():\n                # We should reverse the sign of the momentum at the end\n                momentum[:] = -(momentum - eps / 2.0 * exe_grads[k])\n    copy_param(exe, end_params)\n    # 3. Calculate acceptance ratio and accept/reject the move\n    end_potential = calc_potential(exe, end_params, label_key, noise_precision, prior_precision)\n    end_kinetic = sum([nd.sum(nd.square(momentum)) / 2.0\n                       for momentum in end_momentums.values()]).asscalar()\n    # print init_potential, init_kinetic, end_potential, end_kinetic\n    r = numpy.random.rand(1)\n    if r < numpy.exp(-(end_potential + end_kinetic) + (init_potential + init_kinetic)):\n        exe.copy_params_from(end_params)\n        return end_params, 1\n    else:\n        exe.copy_params_from(init_params)\n        return init_params, 0", "response": "This function generates the implementation of step HMC."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate the implementation of HMC", "response": "def HMC(sym, data_inputs, X, Y, X_test, Y_test, sample_num,\n        initializer=None, noise_precision=1 / 9.0, prior_precision=0.1,\n        learning_rate=1E-6, L=10, dev=mx.gpu()):\n    \"\"\"Generate the implementation of HMC\"\"\"\n    label_key = list(set(data_inputs.keys()) - set(['data']))[0]\n    exe, exe_params, exe_grads, _ = get_executor(sym, dev, data_inputs, initializer)\n    exe.arg_dict['data'][:] = X\n    exe.arg_dict[label_key][:] = Y\n    sample_pool = []\n    accept_num = 0\n    start = time.time()\n    for i in range(sample_num):\n        sample_params, is_accept = step_HMC(exe, exe_params, exe_grads, label_key, noise_precision,\n                                            prior_precision, L, learning_rate)\n        accept_num += is_accept\n\n        if (i + 1) % 10 == 0:\n            sample_pool.append(sample_params)\n            if (i + 1) % 100000 == 0:\n                end = time.time()\n                print(\"Current Iter Num: %d\" % (i + 1), \"Time Spent: %f\" % (end - start), \"MSE:\",\n                      sample_test_regression(exe, X=X_test, Y=Y_test, sample_pool=sample_pool,\n                                             minibatch_size=Y.shape[0],\n                                             save_path='regression_HMC.txt'))\n                start = time.time()\n        exe.copy_params_from(sample_params)\n    print('accept ratio', accept_num / float(sample_num))\n    return sample_pool"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating the implementation of SGD", "response": "def SGD(sym, data_inputs, X, Y, X_test, Y_test, total_iter_num,\n        lr=None,\n        lr_scheduler=None, prior_precision=1,\n        out_grad_f=None,\n        initializer=None,\n        minibatch_size=100, dev=mx.gpu()):\n    \"\"\"Generate the implementation of SGD\"\"\"\n    if out_grad_f is None:\n        label_key = list(set(data_inputs.keys()) - set(['data']))[0]\n    exe, params, params_grad, _ = get_executor(sym, dev, data_inputs, initializer)\n    optimizer = mx.optimizer.create('sgd', learning_rate=lr,\n                                    rescale_grad=X.shape[0] / minibatch_size,\n                                    lr_scheduler=lr_scheduler,\n                                    wd=prior_precision)\n    updater = mx.optimizer.get_updater(optimizer)\n    start = time.time()\n    for i in range(total_iter_num):\n        indices = numpy.random.randint(X.shape[0], size=minibatch_size)\n        X_batch = X[indices]\n        Y_batch = Y[indices]\n        exe.arg_dict['data'][:] = X_batch\n        if out_grad_f is None:\n            exe.arg_dict[label_key][:] = Y_batch\n            exe.forward(is_train=True)\n            exe.backward()\n        else:\n            exe.forward(is_train=True)\n            exe.backward(out_grad_f(exe.outputs, nd.array(Y_batch, ctx=dev)))\n        for k in params:\n            updater(k, params_grad[k], params[k])\n        if (i + 1) % 500 == 0:\n            end = time.time()\n            print(\"Current Iter Num: %d\" % (i + 1), \"Time Spent: %f\" % (end - start))\n            sample_test_acc(exe, X=X_test, Y=Y_test, label_num=10, minibatch_size=100)\n            start = time.time()\n    return exe, params, params_grad"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef SGLD(sym, X, Y, X_test, Y_test, total_iter_num,\n         data_inputs=None,\n         learning_rate=None,\n         lr_scheduler=None, prior_precision=1,\n         out_grad_f=None,\n         initializer=None,\n         minibatch_size=100, thin_interval=100, burn_in_iter_num=1000, task='classification',\n         dev=mx.gpu()):\n    \"\"\"Generate the implementation of SGLD\"\"\"\n    if out_grad_f is None:\n        label_key = list(set(data_inputs.keys()) - set(['data']))[0]\n    exe, params, params_grad, _ = get_executor(sym, dev, data_inputs, initializer)\n    optimizer = mx.optimizer.create('sgld', learning_rate=learning_rate,\n                                    rescale_grad=X.shape[0] / minibatch_size,\n                                    lr_scheduler=lr_scheduler,\n                                    wd=prior_precision)\n    updater = mx.optimizer.get_updater(optimizer)\n    sample_pool = []\n    start = time.time()\n    for i in range(total_iter_num):\n        indices = numpy.random.randint(X.shape[0], size=minibatch_size)\n        X_batch = X[indices]\n        Y_batch = Y[indices]\n        exe.arg_dict['data'][:] = X_batch\n        if out_grad_f is None:\n            exe.arg_dict[label_key][:] = Y_batch\n            exe.forward(is_train=True)\n            exe.backward()\n        else:\n            exe.forward(is_train=True)\n            exe.backward(out_grad_f(exe.outputs, nd.array(Y_batch, ctx=dev)))\n        for k in params:\n            updater(k, params_grad[k], params[k])\n        if i < burn_in_iter_num:\n            continue\n        else:\n            if (i - burn_in_iter_num) % thin_interval == 0:\n                if optimizer.lr_scheduler is not None:\n                    lr = optimizer.lr_scheduler(optimizer.num_update)\n                else:\n                    lr = learning_rate\n                sample_pool.append([lr, copy_param(exe)])\n        if (i + 1) % 100000 == 0:\n            end = time.time()\n            if task == 'classification':\n                print(\"Current Iter Num: %d\" % (i + 1), \"Time Spent: %f\" % (end - start))\n                test_correct, test_total, test_acc = \\\n                    sample_test_acc(exe, sample_pool=sample_pool, X=X_test, Y=Y_test, label_num=10,\n                                    minibatch_size=minibatch_size)\n                print(\"Test %d/%d=%f\" % (test_correct, test_total, test_acc))\n            else:\n                print(\"Current Iter Num: %d\" % (i + 1), \"Time Spent: %f\" % (end - start), \"MSE:\",\n                      sample_test_regression(exe=exe, sample_pool=sample_pool,\n                                             X=X_test,\n                                             Y=Y_test, minibatch_size=minibatch_size,\n                                             save_path='regression_SGLD.txt'))\n            start = time.time()\n    return exe, sample_pool", "response": "Generate the implementation of SGLD"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef DistilledSGLD(teacher_sym, student_sym,\n                  teacher_data_inputs, student_data_inputs,\n                  X, Y, X_test, Y_test, total_iter_num,\n                  teacher_learning_rate, student_learning_rate,\n                  teacher_lr_scheduler=None, student_lr_scheduler=None,\n                  student_optimizing_algorithm='sgd',\n                  teacher_grad_f=None, student_grad_f=None,\n                  teacher_prior_precision=1, student_prior_precision=0.001,\n                  perturb_deviation=0.001,\n                  student_initializer=None,\n                  teacher_initializer=None,\n                  minibatch_size=100,\n                  task='classification',\n                  dev=mx.gpu()):\n    \"\"\"Generate the implementation of DistilledSGLD\"\"\"\n    teacher_exe, teacher_params, teacher_params_grad, _ = \\\n        get_executor(teacher_sym, dev, teacher_data_inputs, teacher_initializer)\n    student_exe, student_params, student_params_grad, _ = \\\n        get_executor(student_sym, dev, student_data_inputs, student_initializer)\n    if teacher_grad_f is None:\n        teacher_label_key = list(set(teacher_data_inputs.keys()) - set(['data']))[0]\n    if student_grad_f is None:\n        student_label_key = list(set(student_data_inputs.keys()) - set(['data']))[0]\n    teacher_optimizer = mx.optimizer.create('sgld',\n                                            learning_rate=teacher_learning_rate,\n                                            rescale_grad=X.shape[0] / float(minibatch_size),\n                                            lr_scheduler=teacher_lr_scheduler,\n                                            wd=teacher_prior_precision)\n    student_optimizer = mx.optimizer.create(student_optimizing_algorithm,\n                                            learning_rate=student_learning_rate,\n                                            rescale_grad=1.0 / float(minibatch_size),\n                                            lr_scheduler=student_lr_scheduler,\n                                            wd=student_prior_precision)\n    teacher_updater = mx.optimizer.get_updater(teacher_optimizer)\n    student_updater = mx.optimizer.get_updater(student_optimizer)\n    start = time.time()\n    for i in range(total_iter_num):\n        # 1.1 Draw random minibatch\n        indices = numpy.random.randint(X.shape[0], size=minibatch_size)\n        X_batch = X[indices]\n        Y_batch = Y[indices]\n\n        # 1.2 Update teacher\n        teacher_exe.arg_dict['data'][:] = X_batch\n        if teacher_grad_f is None:\n            teacher_exe.arg_dict[teacher_label_key][:] = Y_batch\n            teacher_exe.forward(is_train=True)\n            teacher_exe.backward()\n        else:\n            teacher_exe.forward(is_train=True)\n            teacher_exe.backward(\n                teacher_grad_f(teacher_exe.outputs, nd.array(Y_batch, ctx=dev)))\n\n        for k in teacher_params:\n            teacher_updater(k, teacher_params_grad[k], teacher_params[k])\n\n        # 2.1 Draw random minibatch and do random perturbation\n        if task == 'classification':\n            indices = numpy.random.randint(X.shape[0], size=minibatch_size)\n            X_student_batch = X[indices] + numpy.random.normal(0,\n                                                               perturb_deviation,\n                                                               X_batch.shape).astype('float32')\n        else:\n            X_student_batch = mx.random.uniform(-6, 6, X_batch.shape, mx.cpu())\n\n        # 2.2 Get teacher predictions\n        teacher_exe.arg_dict['data'][:] = X_student_batch\n        teacher_exe.forward(is_train=False)\n        teacher_pred = teacher_exe.outputs[0]\n        teacher_pred.wait_to_read()\n\n        # 2.3 Update student\n        student_exe.arg_dict['data'][:] = X_student_batch\n        if student_grad_f is None:\n            student_exe.arg_dict[student_label_key][:] = teacher_pred\n            student_exe.forward(is_train=True)\n            student_exe.backward()\n        else:\n            student_exe.forward(is_train=True)\n            student_exe.backward(student_grad_f(student_exe.outputs, teacher_pred))\n        for k in student_params:\n            student_updater(k, student_params_grad[k], student_params[k])\n\n        if (i + 1) % 2000 == 0:\n            end = time.time()\n            if task == 'classification':\n                print(\"Current Iter Num: %d\" % (i + 1), \"Time Spent: %f\" % (end - start))\n                test_correct, test_total, test_acc = \\\n                    sample_test_acc(student_exe, X=X_test, Y=Y_test, label_num=10,\n                                    minibatch_size=minibatch_size)\n                train_correct, train_total, train_acc = \\\n                    sample_test_acc(student_exe, X=X, Y=Y, label_num=10,\n                                    minibatch_size=minibatch_size)\n                teacher_test_correct, teacher_test_total, teacher_test_acc = \\\n                    sample_test_acc(teacher_exe, X=X_test, Y=Y_test, label_num=10,\n                                    minibatch_size=minibatch_size)\n                teacher_train_correct, teacher_train_total, teacher_train_acc = \\\n                    sample_test_acc(teacher_exe, X=X, Y=Y, label_num=10,\n                                    minibatch_size=minibatch_size)\n                print(\"Student: Test ACC %d/%d=%f, Train ACC %d/%d=%f\" % (test_correct, test_total,\n                                                                          test_acc, train_correct,\n                                                                          train_total, train_acc))\n                print(\"Teacher: Test ACC %d/%d=%f, Train ACC %d/%d=%f\" \\\n                      % (teacher_test_correct, teacher_test_total, teacher_test_acc,\n                         teacher_train_correct, teacher_train_total, teacher_train_acc))\n            else:\n                print(\"Current Iter Num: %d\" % (i + 1), \"Time Spent: %f\" % (end - start), \"MSE:\",\n                      sample_test_regression(exe=student_exe, X=X_test, Y=Y_test,\n                                             minibatch_size=minibatch_size,\n                                             save_path='regression_DSGLD.txt'))\n            start = time.time()\n\n    return student_exe, student_params, student_params_grad", "response": "Generate the implementation of DistilledSGLD."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_platforms(path: str = get_dockerfiles_path()) -> List[str]:\n    dockerfiles = glob.glob(os.path.join(path, \"Dockerfile.*\"))\n    dockerfiles = list(filter(lambda x: x[-1] != '~', dockerfiles))\n    files = list(map(lambda x: re.sub(r\"Dockerfile.(.*)\", r\"\\1\", x), dockerfiles))\n    platforms = list(map(lambda x: os.path.split(x)[1], sorted(files)))\n    return platforms", "response": "Get a list of architectures given our dockerfiles"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_docker_tag(platform: str, registry: str) -> str:\n    platform = platform if any(x in platform for x in ['build.', 'publish.']) else 'build.{}'.format(platform)\n    if not registry:\n        registry = \"mxnet_local\"\n    return \"{0}/{1}\".format(registry, platform)", "response": "Returns the docker tag to be used for the container"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build_docker(platform: str, docker_binary: str, registry: str, num_retries: int, no_cache: bool) -> str:\n    tag = get_docker_tag(platform=platform, registry=registry)\n    logging.info(\"Building docker container tagged '%s' with %s\", tag, docker_binary)\n    #\n    # We add a user with the same group as the executing non-root user so files created in the\n    # container match permissions of the local user. Same for the group.\n    #\n    # These variables are used in the docker files to create user and group with these ids.\n    # see: docker/install/ubuntu_adduser.sh\n    #\n    # cache-from is needed so we use the cached images tagged from the remote via\n    # docker pull see: docker_cache.load_docker_cache\n    #\n    # This also prevents using local layers for caching: https://github.com/moby/moby/issues/33002\n    # So to use local caching, we should omit the cache-from by using --no-dockerhub-cache argument to this\n    # script.\n    #\n    # This doesn't work with multi head docker files.\n    #\n    cmd = [docker_binary, \"build\",\n           \"-f\", get_dockerfile(platform),\n           \"--build-arg\", \"USER_ID={}\".format(os.getuid()),\n           \"--build-arg\", \"GROUP_ID={}\".format(os.getgid())]\n    if no_cache:\n        cmd.append(\"--no-cache\")\n    elif registry:\n        cmd.extend([\"--cache-from\", tag])\n    cmd.extend([\"-t\", tag, get_dockerfiles_path()])\n\n    @retry(subprocess.CalledProcessError, tries=num_retries)\n    def run_cmd():\n        logging.info(\"Running command: '%s'\", ' '.join(cmd))\n        check_call(cmd)\n\n    run_cmd()\n    # Get image id by reading the tag. It's guaranteed (except race condition) that the tag exists. Otherwise, the\n    # check_call would have failed\n    image_id = _get_local_image_id(docker_binary=docker_binary, docker_tag=tag)\n    if not image_id:\n        raise FileNotFoundError('Unable to find docker image id matching with {}'.format(tag))\n    return image_id", "response": "Build a container for the given platform and docker binary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the image id of the local docker layer with the passed docker_tag", "response": "def _get_local_image_id(docker_binary, docker_tag):\n    \"\"\"\n    Get the image id of the local docker layer with the passed tag\n    :param docker_tag: docker tag\n    :return: Image id as string or None if tag does not exist\n    \"\"\"\n    cmd = [docker_binary, \"images\", \"-q\", docker_tag]\n    image_id_b = check_output(cmd)\n    image_id = image_id_b.decode('utf-8').strip()\n    if not image_id:\n        raise RuntimeError('Unable to find docker image id matching with tag {}'.format(docker_tag))\n    return image_id"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef default_ccache_dir() -> str:\n    # Share ccache across containers\n    if 'CCACHE_DIR' in os.environ:\n        ccache_dir = os.path.realpath(os.environ['CCACHE_DIR'])\n        try:\n            os.makedirs(ccache_dir, exist_ok=True)\n            return ccache_dir\n        except PermissionError:\n            logging.info('Unable to make dirs at %s, falling back to local temp dir', ccache_dir)\n    # In osx tmpdir is not mountable by default\n    import platform\n    if platform.system() == 'Darwin':\n        ccache_dir = \"/tmp/_mxnet_ccache\"\n        os.makedirs(ccache_dir, exist_ok=True)\n        return ccache_dir\n    return os.path.join(os.path.expanduser(\"~\"), \".ccache\")", "response": "Returns the default ccache directory for the current platform"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef container_run(platform: str,\n                  nvidia_runtime: bool,\n                  docker_registry: str,\n                  shared_memory_size: str,\n                  local_ccache_dir: str,\n                  command: List[str],\n                  cleanup: Cleanup,\n                  environment: Dict[str, str],\n                  dry_run: bool = False) -> int:\n    \"\"\"Run command in a container\"\"\"\n    container_wait_s = 600\n    #\n    # Environment setup\n    #\n    environment.update({\n        'CCACHE_MAXSIZE': '500G',\n        'CCACHE_TEMPDIR': '/tmp/ccache',  # temp dir should be local and not shared\n        'CCACHE_DIR': '/work/ccache',  # this path is inside the container as /work/ccache is\n                                       # mounted\n        'CCACHE_LOGFILE': '/tmp/ccache.log',  # a container-scoped log, useful for ccache\n                                              # verification.\n    })\n    # These variables are passed to the container to the process tree killer can find runaway\n    # process inside the container\n    # https://wiki.jenkins.io/display/JENKINS/ProcessTreeKiller\n    # https://github.com/jenkinsci/jenkins/blob/578d6bacb33a5e99f149de504c80275796f0b231/core/src/main/java/hudson/model/Run.java#L2393\n    #\n    jenkins_env_vars = ['BUILD_NUMBER', 'BUILD_ID', 'BUILD_TAG']\n    environment.update({k: os.environ[k] for k in jenkins_env_vars if k in os.environ})\n    environment.update({k: os.environ[k] for k in ['CCACHE_MAXSIZE'] if k in os.environ})\n\n    tag = get_docker_tag(platform=platform, registry=docker_registry)\n    mx_root = get_mxnet_root()\n    local_build_folder = buildir()\n    # We need to create it first, otherwise it will be created by the docker daemon with root only permissions\n    os.makedirs(local_build_folder, exist_ok=True)\n    os.makedirs(local_ccache_dir, exist_ok=True)\n    logging.info(\"Using ccache directory: %s\", local_ccache_dir)\n    docker_client = docker.from_env()\n    # Equivalent command\n    docker_cmd_list = [\n        get_docker_binary(nvidia_runtime),\n        'run',\n        \"--cap-add\",\n        \"SYS_PTRACE\", # Required by ASAN\n        '--rm',\n        '--shm-size={}'.format(shared_memory_size),\n        # mount mxnet root\n        '-v', \"{}:/work/mxnet\".format(mx_root),\n        # mount mxnet/build for storing build\n        '-v', \"{}:/work/build\".format(local_build_folder),\n        '-v', \"{}:/work/ccache\".format(local_ccache_dir),\n        '-u', '{}:{}'.format(os.getuid(), os.getgid()),\n        '-e', 'CCACHE_MAXSIZE={}'.format(environment['CCACHE_MAXSIZE']),\n        # temp dir should be local and not shared\n        '-e', 'CCACHE_TEMPDIR={}'.format(environment['CCACHE_TEMPDIR']),\n        # this path is inside the container as /work/ccache is mounted\n        '-e', \"CCACHE_DIR={}\".format(environment['CCACHE_DIR']),\n        # a container-scoped log, useful for ccache verification.\n        '-e', \"CCACHE_LOGFILE={}\".format(environment['CCACHE_LOGFILE']),\n        '-ti',\n        tag]\n    docker_cmd_list.extend(command)\n    docker_cmd = ' \\\\\\n\\t'.join(docker_cmd_list)\n    logging.info(\"Running %s in container %s\", command, tag)\n    logging.info(\"Executing the equivalent of:\\n%s\\n\", docker_cmd)\n    # return code of the command inside docker\n    ret = 0\n    if not dry_run:\n        #############################\n        #\n        signal.pthread_sigmask(signal.SIG_BLOCK, {signal.SIGINT, signal.SIGTERM})\n        # noinspection PyShadowingNames\n        runtime = None\n        if nvidia_runtime:\n            # noinspection PyShadowingNames\n            # runc is default (docker info | grep -i runtime)\n            runtime = 'nvidia'\n        container = docker_client.containers.run(\n            tag,\n            runtime=runtime,\n            detach=True,\n            command=command,\n            shm_size=shared_memory_size,\n            user='{}:{}'.format(os.getuid(), os.getgid()),\n            cap_add='SYS_PTRACE',\n            volumes={\n                mx_root:\n                    {'bind': '/work/mxnet', 'mode': 'rw'},\n                local_build_folder:\n                    {'bind': '/work/build', 'mode': 'rw'},\n                local_ccache_dir:\n                    {'bind': '/work/ccache', 'mode': 'rw'},\n            },\n            environment=environment)\n        try:\n            logging.info(\"Started container: %s\", trim_container_id(container.id))\n            # Race condition:\n            # If the previous call is interrupted then it's possible that the container is not cleaned up\n            # We avoid by masking the signals temporarily\n            cleanup.add_container(container)\n            signal.pthread_sigmask(signal.SIG_UNBLOCK, {signal.SIGINT, signal.SIGTERM})\n            #\n            #############################\n\n            stream = container.logs(stream=True, stdout=True, stderr=True)\n            sys.stdout.flush()\n            for chunk in stream:\n                sys.stdout.buffer.write(chunk)\n                sys.stdout.buffer.flush()\n            sys.stdout.flush()\n            stream.close()\n            try:\n                logging.info(\"Waiting for status of container %s for %d s.\",\n                            trim_container_id(container.id),\n                            container_wait_s)\n                wait_result = container.wait(timeout=container_wait_s)\n                logging.info(\"Container exit status: %s\", wait_result)\n                ret = wait_result.get('StatusCode', 200)\n                if ret != 0:\n                    logging.error(\"Container exited with an error \ud83d\ude1e\")\n                else:\n                    logging.info(\"Container exited with success \ud83d\udc4d\")\n            except Exception as e:\n                logging.exception(e)\n                ret = 150\n\n            # Stop\n            try:\n                logging.info(\"Stopping container: %s\", trim_container_id(container.id))\n                container.stop()\n            except Exception as e:\n                logging.exception(e)\n                ret = 151\n\n            # Remove\n            try:\n                logging.info(\"Removing container: %s\", trim_container_id(container.id))\n                container.remove()\n            except Exception as e:\n                logging.exception(e)\n                ret = 152\n            cleanup.remove_container(container)\n            containers = docker_client.containers.list()\n            if containers:\n                logging.info(\"Other running containers: %s\", [trim_container_id(x.id) for x in containers])\n        except docker.errors.NotFound as e:\n            logging.info(\"Container was stopped before cleanup started: %s\", e)\n    return ret", "response": "Run a command in a container"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloads the given tag from the given docker registry.", "response": "def load_docker_cache(tag, docker_registry) -> None:\n    \"\"\"Imports tagged container from the given docker registry\"\"\"\n    if docker_registry:\n        # noinspection PyBroadException\n        try:\n            import docker_cache\n            logging.info('Docker cache download is enabled from registry %s', docker_registry)\n            docker_cache.load_docker_cache(registry=docker_registry, docker_tag=tag)\n        except Exception:\n            logging.exception('Unable to retrieve Docker cache. Continue without...')\n    else:\n        logging.info('Distributed docker cache disabled')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _load_general(data, targets, major_axis):\n    for d_src, d_targets, axis in zip(data, targets, major_axis): # pylint: disable=too-many-nested-blocks\n        if isinstance(d_targets, nd.NDArray):\n            d_src.copyto(d_targets)\n        elif isinstance(d_src, (list, tuple)):\n            for src, dst in zip(d_src, d_targets):\n                src.copyto(dst)\n        else:\n            for slice_idx, d_dst in d_targets:\n                if axis >= 0:\n                    # copy slice\n                    shape = d_src.shape\n                    do_crop = (slice_idx.start != 0 or shape[axis] != slice_idx.stop)\n                    # pylint: disable=no-member,protected-access\n                    if do_crop:\n                        if axis == 0:\n                            d_src[slice_idx.start:slice_idx.stop].copyto(d_dst)\n                        else:\n                            if d_src.context == d_dst.context:\n                                nd.slice_axis(d_src, axis=axis, begin=slice_idx.start,\n                                              end=slice_idx.stop, out=d_dst)\n                            else:\n                                # on different device, crop and then do cross device copy\n                                d_dst_copy = nd.slice_axis(d_src, axis=axis, begin=slice_idx.start,\n                                                           end=slice_idx.stop)\n                                d_dst_copy.copyto(d_dst)\n                    else:\n                        d_src.copyto(d_dst)\n                    # pylint: enable=no-member,protected-access\n                else:\n                    d_src.copyto(d_dst)", "response": "Load a list of arrays into a list of arrays specified by slices."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _load_data(batch, targets, major_axis):\n    if isinstance(batch, list):\n        new_batch = []\n        for i in range(len(targets)):\n            new_batch.append([b.data[i] for b in batch])\n        new_targets = [[dst for _, dst in d_target] for d_target in targets]\n        _load_general(new_batch, new_targets, major_axis)\n    else:\n        _load_general(batch.data, targets, major_axis)", "response": "Load data into sliced arrays."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmerges outputs that lives on multiple context into one so that they look like living on one context.", "response": "def _merge_multi_context(outputs, major_axis):\n    \"\"\"Merge outputs that lives on multiple context into one, so that they look\n    like living on one context.\n    \"\"\"\n    rets = []\n    for tensors, axis in zip(outputs, major_axis):\n        if axis >= 0:\n            # pylint: disable=no-member,protected-access\n            if len(tensors) == 1:\n                rets.append(tensors[0])\n            else:\n                # Concatenate if necessary\n                rets.append(nd.concat(*[tensor.as_in_context(tensors[0].context)\n                                        for tensor in tensors],\n                                      dim=axis))\n            # pylint: enable=no-member,protected-access\n        else:\n            # negative axis means the there is no batch_size axis, and all the\n            # results should be the same on each device. We simply take the\n            # first one, without checking they are actually the same\n            rets.append(tensors[0])\n    return rets"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprepare the group2contexts list of the group2contexts.", "response": "def _prepare_group2ctxs(group2ctxs, ctx_len):\n    \"\"\"Prepare the group2contexts, will duplicate the context\n    if some ctx_group map to only one context.\n    \"\"\"\n    if group2ctxs is None:\n        return [None] * ctx_len\n    elif isinstance(group2ctxs, list):\n        assert(len(group2ctxs) == ctx_len), \"length of group2ctxs\\\n            should be %d\" % ctx_len\n        return group2ctxs\n    elif isinstance(group2ctxs, dict):\n        ret = [{} for i in range(ctx_len)]\n        for k, v in group2ctxs.items():\n            ctxs = None\n            if isinstance(v, ctx.Context):\n                ctxs = [v] * ctx_len\n            else:\n                if len(v) == 1:\n                    ctxs = v * ctx_len\n                else:\n                    assert(len(v) == ctx_len), \"length of group2ctxs[%s]\\\n                        should be %d or 1\" % (k, ctx_len)\n                    ctxs = v\n            for i in range(ctx_len):\n                ret[i][k] = ctxs[i]\n        return ret\n    else:\n        assert(False), \"group2ctxs should be list of dict of str to context,\\\n            or dict of str to context or list of context\"\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef decide_slices(self, data_shapes):\n        assert len(data_shapes) > 0\n        major_axis = [DataDesc.get_batch_axis(x.layout) for x in data_shapes]\n\n        for (name, shape), axis in zip(data_shapes, major_axis):\n            if axis == -1:\n                continue\n\n            batch_size = shape[axis]\n            if self.batch_size is not None:\n                assert batch_size == self.batch_size, (\"all data must have the same batch size: \"\n                                                       + (\"batch_size = %d, but \" % self.batch_size)\n                                                       + (\"%s has shape %s\" % (name, shape)))\n            else:\n                self.batch_size = batch_size\n                self.slices = _split_input_slice(self.batch_size, self.workload)\n\n        return major_axis", "response": "Decide the slices for each context according to the workload."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncollects internal arrays from executors.", "response": "def _collect_arrays(self):\n        \"\"\"Collect internal arrays from executors.\"\"\"\n        # convenient data structures\n        self.data_arrays = [[(self.slices[i], e.arg_dict[name]) for i, e in enumerate(self.execs)]\n                            for name, _ in self.data_shapes]\n\n        self.state_arrays = [[e.arg_dict[name] for e in self.execs]\n                             for name in self.state_names]\n\n        if self.label_shapes is not None:\n            self.label_arrays = [[(self.slices[i], e.arg_dict[name])\n                                  for i, e in enumerate(self.execs)]\n                                 for name, _ in self.label_shapes]\n        else:\n            self.label_arrays = None\n\n        self.param_arrays = [[exec_.arg_arrays[i] for exec_ in self.execs]\n                             for i, name in enumerate(self.arg_names)\n                             if name in self.param_names]\n        if self.for_training:\n            self.grad_arrays = [[exec_.grad_arrays[i] for exec_ in self.execs]\n                                for i, name in enumerate(self.arg_names)\n                                if name in self.param_names]\n        else:\n            self.grad_arrays = None\n\n        data_names = [x[0] for x in self.data_shapes]\n        if self.inputs_need_grad:\n            self.input_grad_arrays = [[exec_.grad_arrays[self.arg_names.index(name)]\n                                       for exec_ in self.execs]\n                                      for name in data_names if name in self.arg_names]\n        else:\n            self.input_grad_arrays = None\n\n        self.aux_arrays = [[exec_.aux_arrays[i] for exec_ in self.execs]\n                           for i in range(len(self.aux_names))]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbinding executors on their respective devices.", "response": "def bind_exec(self, data_shapes, label_shapes, shared_group=None, reshape=False):\n        \"\"\"Bind executors on their respective devices.\n\n        Parameters\n        ----------\n        data_shapes : list\n        label_shapes : list\n        shared_group : DataParallelExecutorGroup\n        reshape : bool\n        \"\"\"\n        assert reshape or not self.execs\n        self.batch_size = None\n\n        # calculate workload and bind executors\n        self.data_layouts = self.decide_slices(data_shapes)\n        if label_shapes is not None:\n            # call it to make sure labels has the same batch size as data\n            self.label_layouts = self.decide_slices(label_shapes)\n\n        for i in range(len(self.contexts)):\n            data_shapes_i = self._sliced_shape(data_shapes, i, self.data_layouts)\n            if label_shapes is not None:\n                label_shapes_i = self._sliced_shape(label_shapes, i, self.label_layouts)\n            else:\n                label_shapes_i = []\n\n            if reshape:\n                self.execs[i] = self._default_execs[i].reshape(\n                    allow_up_sizing=True, **dict(data_shapes_i + label_shapes_i))\n            else:\n                self.execs.append(self._bind_ith_exec(i, data_shapes_i, label_shapes_i,\n                                                      shared_group))\n\n        self.data_shapes = data_shapes\n        self.label_shapes = label_shapes\n        self.data_names = [i.name for i in self.data_shapes]\n        if label_shapes is not None:\n            self.label_names = [i.name for i in self.label_shapes]\n        self._collect_arrays()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef reshape(self, data_shapes, label_shapes):\n        if data_shapes == self.data_shapes and label_shapes == self.label_shapes:\n            return\n        if self._default_execs is None:\n            self._default_execs = [i for i in self.execs]\n        self.bind_exec(data_shapes, label_shapes, reshape=True)", "response": "Reshape executors.\n\n        Parameters\n        ----------\n        data_shapes : list\n        label_shapes : list"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nassign parameters to all the executors.", "response": "def set_params(self, arg_params, aux_params, allow_extra=False):\n        \"\"\"Assign, i.e. copy parameters to all the executors.\n\n        Parameters\n        ----------\n        arg_params : dict\n            A dictionary of name to `NDArray` parameter mapping.\n        aux_params : dict\n            A dictionary of name to `NDArray` auxiliary variable mapping.\n        allow_extra : boolean, optional\n            Whether allow extra parameters that are not needed by symbol.\n            If this is True, no error will be thrown when arg_params or aux_params\n            contain extra parameters that is not needed by the executor.\n        \"\"\"\n        for exec_ in self.execs:\n            exec_.copy_params_from(arg_params, aux_params, allow_extra_params=allow_extra)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_params(self, arg_params, aux_params):\n        for name, block in zip(self.param_names, self.param_arrays):\n            weight = sum(w.copyto(ctx.cpu()) for w in block) / len(block)\n            weight.astype(arg_params[name].dtype).copyto(arg_params[name])\n        for name, block in zip(self.aux_names, self.aux_arrays):\n            weight = sum(w.copyto(ctx.cpu()) for w in block) / len(block)\n            weight.astype(aux_params[name].dtype).copyto(aux_params[name])", "response": "Copy data from each executor to arg_params and aux_params."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsplit data_batch according to workload and run forward on each device.", "response": "def forward(self, data_batch, is_train=None):\n        \"\"\"Split `data_batch` according to workload and run forward on each devices.\n\n        Parameters\n        ----------\n        data_batch : DataBatch\n            Or could be any object implementing similar interface.\n        is_train : bool\n            The hint for the backend, indicating whether we are during training phase.\n            Default is `None`, then the value `self.for_training` will be used.\n        Returns\n        -------\n\n        \"\"\"\n        _load_data(data_batch, self.data_arrays, self.data_layouts)\n        if is_train is None:\n            is_train = self.for_training\n\n        if isinstance(data_batch, list):\n            if self.label_arrays is not None and data_batch is not None and data_batch[0].label:\n                _load_label(data_batch, self.label_arrays, self.label_layouts)\n        else:\n            if self.label_arrays is not None and data_batch.label:\n                _load_label(data_batch, self.label_arrays, self.label_layouts)\n\n        for exec_ in self.execs:\n            exec_.forward(is_train=is_train)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the shapes of the outputs.", "response": "def get_output_shapes(self):\n        \"\"\"Get the shapes of the outputs.\"\"\"\n        outputs = self.execs[0].outputs\n        shapes = [out.shape for out in outputs]\n\n        concat_shapes = []\n        for key, the_shape, axis in zip(self.symbol.list_outputs(), shapes, self.output_layouts):\n            the_shape = list(the_shape)\n            if axis >= 0:\n                the_shape[axis] = self.batch_size\n            concat_shapes.append((key, tuple(the_shape)))\n        return concat_shapes"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_outputs(self, merge_multi_context=True, begin=0, end=None):\n        if end is None:\n            end = self.num_outputs\n        outputs = [[exec_.outputs[i] for exec_ in self.execs]\n                   for i in range(begin, end)]\n        if merge_multi_context:\n            outputs = _merge_multi_context(outputs, self.output_layouts)\n        return outputs", "response": "Get outputs of the previous forward computation."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets value for states. Only one of states & value can be specified.", "response": "def set_states(self, states=None, value=None):\n        \"\"\"Set value for states. Only one of states & value can be specified.\n\n        Parameters\n        ----------\n        states : list of list of NDArrays\n            source states arrays formatted like [[state1_dev1, state1_dev2],\n            [state2_dev1, state2_dev2]].\n        value : number\n            a single scalar value for all state arrays.\n        \"\"\"\n        if states is not None:\n            assert value is None, \"Only one of states & value can be specified.\"\n            _load_general(states, self.state_arrays, (0,)*len(states))\n        else:\n            assert value is not None, \"At least one of states & value must be specified.\"\n            assert states is None, \"Only one of states & value can be specified.\"\n            for d_dst in self.state_arrays:\n                for dst in d_dst:\n                    dst[:] = value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the gradients with respect to the inputs of the module.", "response": "def get_input_grads(self, merge_multi_context=True):\n        \"\"\"Get the gradients with respect to the inputs of the module.\n\n        Parameters\n        ----------\n        merge_multi_context : bool\n            Defaults to ``True``. In the case when data-parallelism is used, the outputs\n            will be collected from multiple devices. A `True` value indicate that we\n            should merge the collected results so that they look like from a single\n            executor.\n\n        Returns\n        -------\n        If `merge_multi_context` is ``True``, it is like ``[grad1, grad2]``. Otherwise, it\n        is like ``[[grad1_dev1, grad1_dev2], [grad2_dev1, grad2_dev2]]``. All the output\n        elements are `NDArray`.\n        \"\"\"\n        assert self.inputs_need_grad\n        if merge_multi_context:\n            return _merge_multi_context(self.input_grad_arrays, self.data_layouts)\n        return self.input_grad_arrays"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef backward(self, out_grads=None):\n        assert self.for_training, 're-bind with for_training=True to run backward'\n        if out_grads is None:\n            out_grads = []\n\n        for i, (exec_, islice) in enumerate(zip(self.execs, self.slices)):\n            out_grads_slice = []\n            for grad, axis in zip(out_grads, self.output_layouts):\n                if axis >= 0:\n                    # pylint: disable=no-member\n                    og_my_slice = nd.slice_axis(grad, axis=axis, begin=islice.start,\n                                                end=islice.stop)\n                    out_grads_slice.append(og_my_slice.as_in_context(self.contexts[i]))\n                    # pylint: enable=no-member\n                else:\n                    out_grads_slice.append(grad.copyto(self.contexts[i]))\n            exec_.backward(out_grads=out_grads_slice)", "response": "Runs backward on all devices."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_metric(self, eval_metric, labels, pre_sliced):\n        for current_exec, (texec, islice) in enumerate(zip(self.execs, self.slices)):\n            if not pre_sliced:\n                labels_slice = []\n                for label, axis in zip(labels, self.label_layouts):\n                    if axis == 0:\n                        # slicing NDArray along axis 0 can avoid copying\n                        labels_slice.append(label[islice])\n                    elif axis > 0:\n                        # pylint: disable=no-member\n                        label_my_slice = nd.slice_axis(label, axis=axis, begin=islice.start,\n                                                       end=islice.stop).as_in_context(label.context)\n                        # pylint: enable=no-member\n                        labels_slice.append(label_my_slice)\n                    else:\n                        labels_slice.append(label)\n            else:\n                labels_slice = labels[current_exec]\n\n            labels_ = OrderedDict(zip(self.label_names, labels_slice))\n            preds = OrderedDict(zip(self.output_names, texec.outputs))\n            eval_metric.update_dict(labels_, preds)", "response": "Accumulate the performance according to eval_metric on all devices."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _bind_ith_exec(self, i, data_shapes, label_shapes, shared_group):\n        shared_exec = None if shared_group is None else shared_group.execs[i]\n        context = self.contexts[i]\n        shared_data_arrays = self.shared_data_arrays[i]\n\n        input_shapes = dict(data_shapes)\n        if label_shapes is not None:\n            input_shapes.update(dict(label_shapes))\n\n        input_types = {x.name: x.dtype for x in data_shapes}\n        if label_shapes is not None:\n            input_types.update({x.name: x.dtype for x in label_shapes})\n\n        group2ctx = self.group2ctxs[i]\n\n        executor = self.symbol.simple_bind(ctx=context, grad_req=self.grad_req,\n                                           type_dict=input_types, shared_arg_names=self.param_names,\n                                           shared_exec=shared_exec, group2ctx=group2ctx,\n                                           shared_buffer=shared_data_arrays, **input_shapes)\n        self._total_exec_bytes += int(executor.debug_str().split('\\n')[-3].split()[1])\n        return executor", "response": "Internal utility function to bind the i - th executor. This function utilizes simple_bind python interface."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _sliced_shape(self, shapes, i, major_axis):\n        sliced_shapes = []\n        for desc, axis in zip(shapes, major_axis):\n            shape = list(desc.shape)\n            if axis >= 0:\n                shape[axis] = self.slices[i].stop - self.slices[i].start\n            sliced_shapes.append(DataDesc(desc.name, tuple(shape), desc.dtype, desc.layout))\n        return sliced_shapes", "response": "Get the sliced shapes for the i - th executor."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses classes and class_names if applicable", "response": "def parse_class_names(args):\n    \"\"\" parse # classes and class_names if applicable \"\"\"\n    num_class = args.num_class\n    if len(args.class_names) > 0:\n        if os.path.isfile(args.class_names):\n            # try to open it to read class names\n            with open(args.class_names, 'r') as f:\n                class_names = [l.strip() for l in f.readlines()]\n        else:\n            class_names = [c.strip() for c in args.class_names.split(',')]\n        assert len(class_names) == num_class, str(len(class_names))\n        for name in class_names:\n            assert len(name) > 0\n    else:\n        class_names = None\n    return class_names"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _has_instance(data, dtype):\n    for item in data:\n        _, arr = item\n        if isinstance(arr, dtype):\n            return True\n    return False", "response": "Return True if data has instance of dtype."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _getdata_by_idx(data, idx):\n    shuffle_data = []\n\n    for k, v in data:\n        if (isinstance(v, h5py.Dataset) if h5py else False):\n            shuffle_data.append((k, v))\n        elif isinstance(v, CSRNDArray):\n            shuffle_data.append((k, sparse_array(v.asscipy()[idx], v.context)))\n        else:\n            shuffle_data.append((k, array(v.asnumpy()[idx], v.context)))\n\n    return shuffle_data", "response": "Shuffle the data by index."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_mobilenet(multiplier, pretrained=False, ctx=cpu(),\n                  root=os.path.join(base.data_dir(), 'models'), **kwargs):\n    r\"\"\"MobileNet model from the\n    `\"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications\"\n    <https://arxiv.org/abs/1704.04861>`_ paper.\n\n    Parameters\n    ----------\n    multiplier : float\n        The width multiplier for controling the model size. Only multipliers that are no\n        less than 0.25 are supported. The actual number of channels is equal to the original\n        channel size multiplied by this multiplier.\n    pretrained : bool, default False\n        Whether to load the pretrained weights for model.\n    ctx : Context, default CPU\n        The context in which to load the pretrained weights.\n    root : str, default $MXNET_HOME/models\n        Location for keeping the model parameters.\n    \"\"\"\n    net = MobileNet(multiplier, **kwargs)\n\n    if pretrained:\n        from ..model_store import get_model_file\n        version_suffix = '{0:.2f}'.format(multiplier)\n        if version_suffix in ('1.00', '0.50'):\n            version_suffix = version_suffix[:-1]\n        net.load_parameters(\n            get_model_file('mobilenet%s' % version_suffix, root=root), ctx=ctx)\n    return net", "response": "r Creates a MobileNet model from the Mobilenet model file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the canonical name for a symbol.", "response": "def get(self, name, hint):\n        \"\"\"Get the canonical name for a symbol.\n\n        This is the default implementation.\n        If the user specifies a name,\n        the user-specified name will be used.\n\n        When user does not specify a name, we automatically generate a\n        name based on the hint string.\n\n        Parameters\n        ----------\n        name : str or None\n            The name specified by the user.\n\n        hint : str\n            A hint string, which can be used to generate name.\n\n        Returns\n        -------\n        full_name : str\n            A canonical name for the symbol.\n        \"\"\"\n        if name:\n            return name\n        if hint not in self._counter:\n            self._counter[hint] = 0\n        name = '%s%d' % (hint, self._counter[hint])\n        self._counter[hint] += 1\n        return name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndraw samples from log uniform distribution and returns sampled candidates expected count for true classes and sampled classes.", "response": "def draw(self, true_classes):\n        \"\"\"Draw samples from log uniform distribution and returns sampled candidates,\n        expected count for true classes and sampled classes.\"\"\"\n        range_max = self.range_max\n        num_sampled = self.num_sampled\n        ctx = true_classes.context\n        log_range = math.log(range_max + 1)\n        num_tries = 0\n        true_classes = true_classes.reshape((-1,))\n        sampled_classes, num_tries = self.sampler.sample_unique(num_sampled)\n\n        true_cls = true_classes.as_in_context(ctx).astype('float64')\n        prob_true = ((true_cls + 2.0) / (true_cls + 1.0)).log() / log_range\n        count_true = self._prob_helper(num_tries, num_sampled, prob_true)\n\n        sampled_classes = ndarray.array(sampled_classes, ctx=ctx, dtype='int64')\n        sampled_cls_fp64 = sampled_classes.astype('float64')\n        prob_sampled = ((sampled_cls_fp64 + 2.0) / (sampled_cls_fp64 + 1.0)).log() / log_range\n        count_sampled = self._prob_helper(num_tries, num_sampled, prob_sampled)\n        return [sampled_classes, count_true, count_sampled]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_inception_score(images, splits=10):\n    assert (images.shape[1] == 3)\n\n    # load inception model\n    if inception_model is None:\n        _init_inception()\n\n    # resize images to adapt inception model(inceptionV3)\n    if images.shape[2] != 299:\n        images = resize(images, 299, 299)\n\n    preds = []\n    bs = 4\n    n_batches = int(math.ceil(float(images.shape[0])/float(bs)))\n\n    # to get the predictions/picture of inception model\n    for i in range(n_batches):\n        sys.stdout.write(\".\")\n        sys.stdout.flush()\n        inps = images[(i * bs):min((i + 1) * bs, len(images))]\n        # inps size. bs x 3 x 299 x 299\n        pred = nd.softmax(inception_model(inps))\n        # pred size. bs x 1000\n        preds.append(pred.asnumpy())\n\n    # list to array\n    preds = np.concatenate(preds, 0)\n    scores = []\n\n    # to calculate the inception_score each split.\n    for i in range(splits):\n        # extract per split image pred\n        part = preds[(i * preds.shape[0] // splits):((i + 1) * preds.shape[0] // splits), :]\n        kl = part * (np.log(part) - np.log(np.expand_dims(np.mean(part, 0), 0)))\n        kl = np.mean(np.sum(kl, 1))\n        scores.append(np.exp(kl))\n\n    return np.mean(scores), np.std(scores)", "response": "Inception_score function.\n        The images will be divided into 'splits' parts, and calculate each inception_score separately,\n        then return the mean and std of inception_scores of these parts.\n    :param images: Images(num x c x w x h) that needs to calculate inception_score.\n    :param splits:\n    :return: mean and std of inception_score"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsave checkpoint for model using RNN cells.", "response": "def save_rnn_checkpoint(cells, prefix, epoch, symbol, arg_params, aux_params):\n    \"\"\"Save checkpoint for model using RNN cells.\n    Unpacks weight before saving.\n\n    Parameters\n    ----------\n    cells : mxnet.rnn.RNNCell or list of RNNCells\n        The RNN cells used by this symbol.\n    prefix : str\n        Prefix of model name.\n    epoch : int\n        The epoch number of the model.\n    symbol : Symbol\n        The input symbol\n    arg_params : dict of str to NDArray\n        Model parameter, dict of name to NDArray of net's weights.\n    aux_params : dict of str to NDArray\n        Model parameter, dict of name to NDArray of net's auxiliary states.\n\n    Notes\n    -----\n    - ``prefix-symbol.json`` will be saved for symbol.\n    - ``prefix-epoch.params`` will be saved for parameters.\n    \"\"\"\n    if isinstance(cells, BaseRNNCell):\n        cells = [cells]\n    for cell in cells:\n        arg_params = cell.unpack_weights(arg_params)\n    save_checkpoint(prefix, epoch, symbol, arg_params, aux_params)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading model checkpoint from file.", "response": "def load_rnn_checkpoint(cells, prefix, epoch):\n    \"\"\"Load model checkpoint from file.\n    Pack weights after loading.\n\n    Parameters\n    ----------\n    cells : mxnet.rnn.RNNCell or list of RNNCells\n        The RNN cells used by this symbol.\n    prefix : str\n        Prefix of model name.\n    epoch : int\n        Epoch number of model we would like to load.\n\n    Returns\n    -------\n    symbol : Symbol\n        The symbol configuration of computation network.\n    arg_params : dict of str to NDArray\n        Model parameter, dict of name to NDArray of net's weights.\n    aux_params : dict of str to NDArray\n        Model parameter, dict of name to NDArray of net's auxiliary states.\n\n    Notes\n    -----\n    - symbol will be loaded from ``prefix-symbol.json``.\n    - parameters will be loaded from ``prefix-epoch.params``.\n    \"\"\"\n    sym, arg, aux = load_checkpoint(prefix, epoch)\n    if isinstance(cells, BaseRNNCell):\n        cells = [cells]\n    for cell in cells:\n        arg = cell.pack_weights(arg)\n\n    return sym, arg, aux"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmake a callback to checkpoint Module to prefix every epoch. unpacks weights used by cells before saving. Parameters ---------- cells : mxnet.rnn.RNNCell or list of RNNCells The RNN cells used by this symbol. prefix : str The file prefix to checkpoint to period : int How many epochs to wait before checkpointing. Default is 1. Returns ------- callback : function The callback function that can be passed as iter_end_callback to fit.", "response": "def do_rnn_checkpoint(cells, prefix, period=1):\n    \"\"\"Make a callback to checkpoint Module to prefix every epoch.\n    unpacks weights used by cells before saving.\n\n    Parameters\n    ----------\n    cells : mxnet.rnn.RNNCell or list of RNNCells\n        The RNN cells used by this symbol.\n    prefix : str\n        The file prefix to checkpoint to\n    period : int\n        How many epochs to wait before checkpointing. Default is 1.\n\n    Returns\n    -------\n    callback : function\n        The callback function that can be passed as iter_end_callback to fit.\n    \"\"\"\n    period = int(max(1, period))\n    # pylint: disable=unused-argument\n    def _callback(iter_no, sym=None, arg=None, aux=None):\n        \"\"\"The checkpoint function.\"\"\"\n        if (iter_no + 1) % period == 0:\n            save_rnn_checkpoint(cells, prefix, iter_no+1, sym, arg, aux)\n    return _callback"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nactivate or deactivates HybridBlock s recursively. Has no effect on non - hybrid children.", "response": "def hybridize(self, active=True, **kwargs):\n        \"\"\"Activates or deactivates `HybridBlock` s recursively. Has no effect on\n        non-hybrid children.\n\n        Parameters\n        ----------\n        active : bool, default True\n            Whether to turn hybrid on or off.\n        **kwargs : string\n            Additional flags for hybridized operator.\n        \"\"\"\n        if self._children and all(isinstance(c, HybridBlock) for c in self._children.values()):\n            warnings.warn(\n                \"All children of this Sequential layer '%s' are HybridBlocks. Consider \"\n                \"using HybridSequential for the best performance.\"%self.prefix, stacklevel=2)\n        super(Sequential, self).hybridize(active, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread image specified by path into numpy. ndarray", "response": "def read_img(path):\n    \"\"\" Reads image specified by path into numpy.ndarray\"\"\"\n    img = cv2.resize(cv2.imread(path, 0), (80, 30)).astype(np.float32) / 255\n    img = np.expand_dims(img.transpose(1, 0), 0)\n    return img"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef lstm_init_states(batch_size):\n    hp = Hyperparams()\n    init_shapes = lstm.init_states(batch_size=batch_size, num_lstm_layer=hp.num_lstm_layer, num_hidden=hp.num_hidden)\n    init_names = [s[0] for s in init_shapes]\n    init_arrays = [mx.nd.zeros(x[1]) for x in init_shapes]\n    return init_names, init_arrays", "response": "Returns a tuple of names and zero arrays for LSTM init states"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_module(prefix, epoch, data_names, data_shapes):\n    sym, arg_params, aux_params = mx.model.load_checkpoint(prefix, epoch)\n\n    # We don't need CTC loss for prediction, just a simple softmax will suffice.\n    # We get the output of the layer just before the loss layer ('pred_fc') and add softmax on top\n    pred_fc = sym.get_internals()['pred_fc_output']\n    sym = mx.sym.softmax(data=pred_fc)\n\n    mod = mx.mod.Module(symbol=sym, context=mx.cpu(), data_names=data_names, label_names=None)\n    mod.bind(for_training=False, data_shapes=data_shapes)\n    mod.set_params(arg_params, aux_params, allow_missing=False)\n    return mod", "response": "Loads the model from checkpoint specified by prefix and epoch binds it\n    to an executor and sets its parameters and returns a mx. mod. Module object"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"path\", help=\"Path to the CAPTCHA image file\")\n    parser.add_argument(\"--prefix\", help=\"Checkpoint prefix [Default 'ocr']\", default='ocr')\n    parser.add_argument(\"--epoch\", help=\"Checkpoint epoch [Default 100]\", type=int, default=100)\n    args = parser.parse_args()\n\n    init_state_names, init_state_arrays = lstm_init_states(batch_size=1)\n    img = read_img(args.path)\n\n    sample = SimpleBatch(\n        data_names=['data'] + init_state_names,\n        data=[mx.nd.array(img)] + init_state_arrays)\n\n    mod = load_module(args.prefix, args.epoch, sample.data_names, sample.provide_data)\n\n    mod.forward(sample)\n    prob = mod.get_outputs()[0].asnumpy()\n\n    prediction = CtcMetrics.ctc_label(np.argmax(prob, axis=-1).tolist())\n    # Predictions are 1 to 10 for digits 0 to 9 respectively (prediction 0 means no-digit)\n    prediction = [p - 1 for p in prediction]\n    print(\"Digits:\", prediction)", "response": "Main entry point for the ctc_label command"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bbox_flip(bbox, width, flip_x=False):\n    if flip_x:\n        xmax = width - bbox[:, 0]\n        xmin = width - bbox[:, 2]\n        bbox[:, 0] = xmin\n        bbox[:, 2] = xmax\n    return bbox", "response": "flips the bbox of a node in the tree tree"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndetermine overlaps between boxes and query_boxes", "response": "def bbox_overlaps(boxes, query_boxes):\n    \"\"\"\n    determine overlaps between boxes and query_boxes\n    :param boxes: n * 4 bounding boxes\n    :param query_boxes: k * 4 bounding boxes\n    :return: overlaps: n * k overlaps\n    \"\"\"\n    n_ = boxes.shape[0]\n    k_ = query_boxes.shape[0]\n    overlaps = np.zeros((n_, k_), dtype=np.float)\n    for k in range(k_):\n        query_box_area = (query_boxes[k, 2] - query_boxes[k, 0] + 1) * (query_boxes[k, 3] - query_boxes[k, 1] + 1)\n        for n in range(n_):\n            iw = min(boxes[n, 2], query_boxes[k, 2]) - max(boxes[n, 0], query_boxes[k, 0]) + 1\n            if iw > 0:\n                ih = min(boxes[n, 3], query_boxes[k, 3]) - max(boxes[n, 1], query_boxes[k, 1]) + 1\n                if ih > 0:\n                    box_area = (boxes[n, 2] - boxes[n, 0] + 1) * (boxes[n, 3] - boxes[n, 1] + 1)\n                    all_area = float(box_area + query_box_area - iw * ih)\n                    overlaps[n, k] = iw * ih / all_area\n    return overlaps"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef clip_boxes(boxes, im_shape):\n    # x1 >= 0\n    boxes[:, 0::4] = np.maximum(np.minimum(boxes[:, 0::4], im_shape[1] - 1), 0)\n    # y1 >= 0\n    boxes[:, 1::4] = np.maximum(np.minimum(boxes[:, 1::4], im_shape[0] - 1), 0)\n    # x2 < im_shape[1]\n    boxes[:, 2::4] = np.maximum(np.minimum(boxes[:, 2::4], im_shape[1] - 1), 0)\n    # y2 < im_shape[0]\n    boxes[:, 3::4] = np.maximum(np.minimum(boxes[:, 3::4], im_shape[0] - 1), 0)\n    return boxes", "response": "Clip boxes to image boundaries."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes bounding box regression targets from ex_rois to gt_rois", "response": "def bbox_transform(ex_rois, gt_rois, box_stds):\n    \"\"\"\n    compute bounding box regression targets from ex_rois to gt_rois\n    :param ex_rois: [N, 4]\n    :param gt_rois: [N, 4]\n    :return: [N, 4]\n    \"\"\"\n    assert ex_rois.shape[0] == gt_rois.shape[0], 'inconsistent rois number'\n\n    ex_widths = ex_rois[:, 2] - ex_rois[:, 0] + 1.0\n    ex_heights = ex_rois[:, 3] - ex_rois[:, 1] + 1.0\n    ex_ctr_x = ex_rois[:, 0] + 0.5 * (ex_widths - 1.0)\n    ex_ctr_y = ex_rois[:, 1] + 0.5 * (ex_heights - 1.0)\n\n    gt_widths = gt_rois[:, 2] - gt_rois[:, 0] + 1.0\n    gt_heights = gt_rois[:, 3] - gt_rois[:, 1] + 1.0\n    gt_ctr_x = gt_rois[:, 0] + 0.5 * (gt_widths - 1.0)\n    gt_ctr_y = gt_rois[:, 1] + 0.5 * (gt_heights - 1.0)\n\n    targets_dx = (gt_ctr_x - ex_ctr_x) / (ex_widths + 1e-14) / box_stds[0]\n    targets_dy = (gt_ctr_y - ex_ctr_y) / (ex_heights + 1e-14) / box_stds[1]\n    targets_dw = np.log(gt_widths / ex_widths) / box_stds[2]\n    targets_dh = np.log(gt_heights / ex_heights) / box_stds[3]\n\n    targets = np.vstack((targets_dx, targets_dy, targets_dw, targets_dh)).transpose()\n    return targets"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bbox_pred(boxes, box_deltas, box_stds):\n    if boxes.shape[0] == 0:\n        return np.zeros((0, box_deltas.shape[1]))\n\n    widths = boxes[:, 2] - boxes[:, 0] + 1.0\n    heights = boxes[:, 3] - boxes[:, 1] + 1.0\n    ctr_x = boxes[:, 0] + 0.5 * (widths - 1.0)\n    ctr_y = boxes[:, 1] + 0.5 * (heights - 1.0)\n\n    dx = box_deltas[:, 0::4] * box_stds[0]\n    dy = box_deltas[:, 1::4] * box_stds[1]\n    dw = box_deltas[:, 2::4] * box_stds[2]\n    dh = box_deltas[:, 3::4] * box_stds[3]\n\n    pred_ctr_x = dx * widths[:, np.newaxis] + ctr_x[:, np.newaxis]\n    pred_ctr_y = dy * heights[:, np.newaxis] + ctr_y[:, np.newaxis]\n    pred_w = np.exp(dw) * widths[:, np.newaxis]\n    pred_h = np.exp(dh) * heights[:, np.newaxis]\n\n    pred_boxes = np.zeros(box_deltas.shape)\n    # x1\n    pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * (pred_w - 1.0)\n    # y1\n    pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * (pred_h - 1.0)\n    # x2\n    pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * (pred_w - 1.0)\n    # y2\n    pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * (pred_h - 1.0)\n\n    return pred_boxes", "response": "Transform the set of class - agnostic boxes into class - specific boxes by applying the predicted offsets."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef nms(dets, thresh):\n    x1 = dets[:, 0]\n    y1 = dets[:, 1]\n    x2 = dets[:, 2]\n    y2 = dets[:, 3]\n    scores = dets[:, 4]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n\n        inds = np.where(ovr <= thresh)[0]\n        order = order[inds + 1]\n\n    return keep", "response": "greedily select boxes with high confidence and overlap with current maximum < = thresh"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef im_detect(rois, scores, bbox_deltas, im_info,\n              bbox_stds, nms_thresh, conf_thresh):\n    \"\"\"rois (nroi, 4), scores (nrois, nclasses), bbox_deltas (nrois, 4 * nclasses), im_info (3)\"\"\"\n    rois = rois.asnumpy()\n    scores = scores.asnumpy()\n    bbox_deltas = bbox_deltas.asnumpy()\n\n    im_info = im_info.asnumpy()\n    height, width, scale = im_info\n\n    # post processing\n    pred_boxes = bbox_pred(rois, bbox_deltas, bbox_stds)\n    pred_boxes = clip_boxes(pred_boxes, (height, width))\n\n    # we used scaled image & roi to train, so it is necessary to transform them back\n    pred_boxes = pred_boxes / scale\n\n    # convert to per class detection results\n    det = []\n    for j in range(1, scores.shape[-1]):\n        indexes = np.where(scores[:, j] > conf_thresh)[0]\n        cls_scores = scores[indexes, j, np.newaxis]\n        cls_boxes = pred_boxes[indexes, j * 4:(j + 1) * 4]\n        cls_dets = np.hstack((cls_boxes, cls_scores))\n        keep = nms(cls_dets, thresh=nms_thresh)\n\n        cls_id = np.ones_like(cls_scores) * j\n        det.append(np.hstack((cls_id, cls_scores, cls_boxes))[keep, :])\n\n    # assemble all classes\n    det = np.concatenate(det, axis=0)\n    return det", "response": "detects the ROI and returns the ROI scores bbox_deltas im_info nclasses - > nms_thresh conf - > conf_thresh"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a python string to C string.", "response": "def c_str(string):\n    \"\"\"\"Convert a python string to C string.\"\"\"\n    if not isinstance(string, str):\n        string = string.decode('ascii')\n    return ctypes.c_char_p(string.encode('utf-8'))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _load_lib():\n    lib_path = _find_lib_path()\n    lib = ctypes.cdll.LoadLibrary(lib_path[0])\n    # DMatrix functions\n    lib.MXGetLastError.restype = ctypes.c_char_p\n    return lib", "response": "Load libary by searching possible path."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_ndarray_file(nd_bytes):\n    handle = NDListHandle()\n    olen = mx_uint()\n    nd_bytes = bytearray(nd_bytes)\n    ptr = (ctypes.c_char * len(nd_bytes)).from_buffer(nd_bytes)\n    _check_call(_LIB.MXNDListCreate(\n        ptr, len(nd_bytes),\n        ctypes.byref(handle), ctypes.byref(olen)))\n    keys = []\n    arrs = []\n\n    for i in range(olen.value):\n        key = ctypes.c_char_p()\n        cptr = mx_float_p()\n        pdata = ctypes.POINTER(mx_uint)()\n        ndim = mx_uint()\n        _check_call(_LIB.MXNDListGet(\n            handle, mx_uint(i), ctypes.byref(key),\n            ctypes.byref(cptr), ctypes.byref(pdata), ctypes.byref(ndim)))\n        shape = tuple(pdata[:ndim.value])\n        dbuffer = (mx_float * np.prod(shape)).from_address(ctypes.addressof(cptr.contents))\n        ret = np.frombuffer(dbuffer, dtype=np.float32).reshape(shape)\n        ret = np.array(ret, dtype=np.float32)\n        keys.append(py_str(key.value))\n        arrs.append(ret)\n    _check_call(_LIB.MXNDListFree(handle))\n\n    if len(keys) == 0 or len(keys[0]) == 0:\n        return arrs\n    else:\n        return {keys[i] : arrs[i] for i in range(len(keys))}", "response": "Load ndarray file and return as list of numpy array."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef forward(self, **kwargs):\n        for k, v in kwargs.items():\n            if not isinstance(v, np.ndarray):\n                raise ValueError(\"Expect numpy ndarray as input\")\n            v = np.asarray(v, dtype=np.float32, order='C')\n            _check_call(_LIB.MXPredSetInput(\n                self.handle, c_str(k),\n                v.ctypes.data_as(mx_float_p),\n                mx_uint(v.size)))\n        _check_call(_LIB.MXPredForward(self.handle))", "response": "Perform forward to get the output."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nchange the input shape of the predictor.", "response": "def reshape(self, input_shapes):\n        \"\"\"Change the input shape of the predictor.\n\n        Parameters\n        ----------\n        input_shapes : dict of str to tuple\n            The new shape of input data.\n\n        Examples\n        --------\n        >>> predictor.reshape({'data':data_shape_tuple})\n        \"\"\"\n        indptr = [0]\n        sdata = []\n        keys = []\n        for k, v  in input_shapes.items():\n            if not isinstance(v, tuple):\n                raise ValueError(\"Expect input_shapes to be dict str->tuple\")\n            keys.append(c_str(k))\n            sdata.extend(v)\n            indptr.append(len(sdata))\n\n        new_handle = PredictorHandle()\n        _check_call(_LIB.MXPredReshape(\n            mx_uint(len(indptr) - 1),\n            c_array(ctypes.c_char_p, keys),\n            c_array(mx_uint, indptr),\n            c_array(mx_uint, sdata),\n            self.handle,\n            ctypes.byref(new_handle)))\n        _check_call(_LIB.MXPredFree(self.handle))\n        self.handle = new_handle"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_output(self, index):\n        pdata = ctypes.POINTER(mx_uint)()\n        ndim = mx_uint()\n        _check_call(_LIB.MXPredGetOutputShape(\n            self.handle, index,\n            ctypes.byref(pdata),\n            ctypes.byref(ndim)))\n        shape = tuple(pdata[:ndim.value])\n        data = np.empty(shape, dtype=np.float32)\n        _check_call(_LIB.MXPredGetOutput(\n            self.handle, mx_uint(index),\n            data.ctypes.data_as(mx_float_p),\n            mx_uint(data.size)))\n        return data", "response": "Get the index - th output."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbegin an episode of a game instance.", "response": "def begin_episode(self, max_episode_step=DEFAULT_MAX_EPISODE_STEP):\n        \"\"\"\n            Begin an episode of a game instance. We can play the game for a maximum of\n            `max_episode_step` and after that, we are forced to restart\n        \"\"\"\n        if self.episode_step > self.max_episode_step or self.ale.game_over():\n            self.start()\n        else:\n            for i in range(self.screen_buffer_length):\n                self.ale.act(0)\n                self.ale.getScreenGrayscale(self.screen_buffer[i % self.screen_buffer_length, :, :])\n        self.max_episode_step = max_episode_step\n        self.start_lives = self.ale.lives()\n        self.episode_reward = 0\n        self.episode_step = 0"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef reset(self):\n        self._init_counter = -1\n        self._counter = -1\n        for cell in self._children.values():\n            cell.reset()", "response": "Reset before re - using the cell for another graph."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of states for the first RNN step of the base cell.", "response": "def begin_state(self, batch_size=0, func=ndarray.zeros, **kwargs):\n        \"\"\"Initial state for this cell.\n\n        Parameters\n        ----------\n        func : callable, default symbol.zeros\n            Function for creating initial state.\n\n            For Symbol API, func can be `symbol.zeros`, `symbol.uniform`,\n            `symbol.var etc`. Use `symbol.var` if you want to directly\n            feed input as states.\n\n            For NDArray API, func can be `ndarray.zeros`, `ndarray.ones`, etc.\n        batch_size: int, default 0\n            Only required for NDArray API. Size of the batch ('N' in layout)\n            dimension of input.\n\n        **kwargs :\n            Additional keyword arguments passed to func. For example\n            `mean`, `std`, `dtype`, etc.\n\n        Returns\n        -------\n        states : nested list of Symbol\n            Starting states for the first RNN step.\n        \"\"\"\n        assert not self._modified, \\\n            \"After applying modifier cells (e.g. ZoneoutCell) the base \" \\\n            \"cell cannot be called directly. Call the modifier cell instead.\"\n        states = []\n        for info in self.state_info(batch_size):\n            self._init_counter += 1\n            if info is not None:\n                info.update(kwargs)\n            else:\n                info = kwargs\n            state = func(name='%sbegin_state_%d'%(self._prefix, self._init_counter),\n                         **info)\n            states.append(state)\n        return states"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef unroll(self, length, inputs, begin_state=None, layout='NTC', merge_outputs=None,\n               valid_length=None):\n        \"\"\"Unrolls an RNN cell across time steps.\n\n        Parameters\n        ----------\n        length : int\n            Number of steps to unroll.\n        inputs : Symbol, list of Symbol, or None\n            If `inputs` is a single Symbol (usually the output\n            of Embedding symbol), it should have shape\n            (batch_size, length, ...) if `layout` is 'NTC',\n            or (length, batch_size, ...) if `layout` is 'TNC'.\n\n            If `inputs` is a list of symbols (usually output of\n            previous unroll), they should all have shape\n            (batch_size, ...).\n        begin_state : nested list of Symbol, optional\n            Input states created by `begin_state()`\n            or output state of another cell.\n            Created from `begin_state()` if `None`.\n        layout : str, optional\n            `layout` of input symbol. Only used if inputs\n            is a single Symbol.\n        merge_outputs : bool, optional\n            If `False`, returns outputs as a list of Symbols.\n            If `True`, concatenates output across time steps\n            and returns a single symbol with shape\n            (batch_size, length, ...) if layout is 'NTC',\n            or (length, batch_size, ...) if layout is 'TNC'.\n            If `None`, output whatever is faster.\n        valid_length : Symbol, NDArray or None\n            `valid_length` specifies the length of the sequences in the batch without padding.\n            This option is especially useful for building sequence-to-sequence models where\n            the input and output sequences would potentially be padded.\n            If `valid_length` is None, all sequences are assumed to have the same length.\n            If `valid_length` is a Symbol or NDArray, it should have shape (batch_size,).\n            The ith element will be the length of the ith sequence in the batch.\n            The last valid state will be return and the padded outputs will be masked with 0.\n            Note that `valid_length` must be smaller or equal to `length`.\n\n        Returns\n        -------\n        outputs : list of Symbol or Symbol\n            Symbol (if `merge_outputs` is True) or list of Symbols\n            (if `merge_outputs` is False) corresponding to the output from\n            the RNN from this unrolling.\n\n        states : list of Symbol\n            The new state of this RNN after this unrolling.\n            The type of this symbol is same as the output of `begin_state()`.\n        \"\"\"\n        # pylint: disable=too-many-locals\n        self.reset()\n\n        inputs, axis, F, batch_size = _format_sequence(length, inputs, layout, False)\n        begin_state = _get_begin_state(self, F, begin_state, inputs, batch_size)\n\n        states = begin_state\n        outputs = []\n        all_states = []\n        for i in range(length):\n            output, states = self(inputs[i], states)\n            outputs.append(output)\n            if valid_length is not None:\n                all_states.append(states)\n        if valid_length is not None:\n            states = [F.SequenceLast(F.stack(*ele_list, axis=0),\n                                     sequence_length=valid_length,\n                                     use_sequence_length=True,\n                                     axis=0)\n                      for ele_list in zip(*all_states)]\n            outputs = _mask_sequence_variable_length(F, outputs, length, valid_length, axis, True)\n        outputs, _, _, _ = _format_sequence(length, outputs, layout, merge_outputs)\n\n        return outputs, states", "response": "Unrolls an RNN cell across time steps."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_activation(self, F, inputs, activation, **kwargs):\n        func = {'tanh': F.tanh,\n                'relu': F.relu,\n                'sigmoid': F.sigmoid,\n                'softsign': F.softsign}.get(activation)\n        if func:\n            return func(inputs, **kwargs)\n        elif isinstance(activation, string_types):\n            return F.Activation(inputs, act_type=activation, **kwargs)\n        elif isinstance(activation, LeakyReLU):\n            return F.LeakyReLU(inputs, act_type='leaky', slope=activation._alpha, **kwargs)\n        return activation(inputs, **kwargs)", "response": "Get activation function. Convert if is string"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef forward(self, inputs, states):\n        # pylint: disable= arguments-differ\n        self._counter += 1\n        return super(RecurrentCell, self).forward(inputs, states)", "response": "Unrolls the recurrent cell for one time step."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _check_input_names(symbol, names, typename, throw):\n    args = symbol.list_arguments()\n    for name in names:\n        if name in args:\n            continue\n        candidates = [arg for arg in args if\n                      not arg.endswith('_weight') and\n                      not arg.endswith('_bias') and\n                      not arg.endswith('_gamma') and\n                      not arg.endswith('_beta')]\n        msg = \"\\033[91mYou created Module with Module(..., %s_names=%s) but \" \\\n              \"input with name '%s' is not found in symbol.list_arguments(). \" \\\n              \"Did you mean one of:\\n\\t%s\\033[0m\"%(\n                  typename, str(names), name, '\\n\\t'.join(candidates))\n        if throw:\n            raise ValueError(msg)\n        else:\n            warnings.warn(msg)", "response": "Check that all input names are in the given symbol s arguments."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck that input names and shapes match input data descriptors.", "response": "def _check_names_match(data_names, data_shapes, name, throw):\n    \"\"\"Check that input names matches input data descriptors.\"\"\"\n    actual = [x[0] for x in data_shapes]\n    if sorted(data_names) != sorted(actual):\n        msg = \"Data provided by %s_shapes don't match names specified by %s_names (%s vs. %s)\"%(\n            name, name, str(data_shapes), str(data_names))\n        if throw:\n            raise ValueError(msg)\n        else:\n            warnings.warn(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse data_attrs into DataDesc format and check that names match", "response": "def _parse_data_desc(data_names, label_names, data_shapes, label_shapes):\n    \"\"\"parse data_attrs into DataDesc format and check that names match\"\"\"\n    data_shapes = [x if isinstance(x, DataDesc) else DataDesc(*x) for x in data_shapes]\n    _check_names_match(data_names, data_shapes, 'data', True)\n    if label_shapes is not None:\n        label_shapes = [x if isinstance(x, DataDesc) else DataDesc(*x) for x in label_shapes]\n        _check_names_match(label_names, label_shapes, 'label', False)\n    else:\n        _check_names_match(label_names, [], 'label', False)\n    return data_shapes, label_shapes"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns prediction on eval_data and evaluates the performance according to eval_metric.", "response": "def score(self, eval_data, eval_metric, num_batch=None, batch_end_callback=None,\n              score_end_callback=None,\n              reset=True, epoch=0, sparse_row_id_fn=None):\n        \"\"\"Runs prediction on ``eval_data`` and evaluates the performance according to\n        the given ``eval_metric``.\n\n        Checkout `Module Tutorial <http://mxnet.io/tutorials/basic/module.html>`_ to see\n        a end-to-end use-case.\n\n        Parameters\n        ----------\n        eval_data : DataIter\n            Evaluation data to run prediction on.\n        eval_metric : EvalMetric or list of EvalMetrics\n            Evaluation metric to use.\n        num_batch : int\n            Number of batches to run. Defaults to ``None``, indicating run until the `DataIter`\n            finishes.\n        batch_end_callback : function\n            Could also be a list of functions.\n        reset : bool\n            Defaults to ``True``. Indicates whether we should reset `eval_data` before starting\n            evaluating.\n        epoch : int\n            Defaults to 0. For compatibility, this will be passed to callbacks (if any).\n            During training, this will correspond to the training epoch number.\n        sparse_row_id_fn : A callback function\n            The function  takes `data_batch` as an input and returns a dict of\n            str -> NDArray. The resulting dict is used for pulling row_sparse\n            parameters from the kvstore, where the str key is the name of the param,\n            and the value is the row id of the param to pull.\n\n        Examples\n        --------\n        >>> # An example of using score for prediction.\n        >>> # Evaluate accuracy on val_dataiter\n        >>> metric = mx.metric.Accuracy()\n        >>> mod.score(val_dataiter, metric)\n        >>> mod.score(val_dataiter, ['mse', 'acc'])\n        \"\"\"\n        assert self.binded and self.params_initialized\n\n        if reset:\n            eval_data.reset()\n\n        if not isinstance(eval_metric, metric.EvalMetric):\n            eval_metric = metric.create(eval_metric)\n\n        eval_metric.reset()\n        actual_num_batch = 0\n\n        for nbatch, eval_batch in enumerate(eval_data):\n            if num_batch is not None and nbatch == num_batch:\n                break\n            self.prepare(eval_batch, sparse_row_id_fn=sparse_row_id_fn)\n            self.forward(eval_batch, is_train=False)\n            if isinstance(eval_batch, list):\n                self.update_metric(eval_metric, [eb.label for eb in eval_batch], pre_sliced=True)\n            else:\n                self.update_metric(eval_metric, eval_batch.label)\n\n            if batch_end_callback is not None:\n                batch_end_params = BatchEndParam(epoch=epoch,\n                                                 nbatch=nbatch,\n                                                 eval_metric=eval_metric,\n                                                 locals=locals())\n                for callback in _as_list(batch_end_callback):\n                    callback(batch_end_params)\n            actual_num_batch += 1\n\n        if score_end_callback:\n            params = BatchEndParam(epoch=epoch,\n                                   nbatch=actual_num_batch,\n                                   eval_metric=eval_metric,\n                                   locals=locals())\n            for callback in _as_list(score_end_callback):\n                callback(params)\n\n        return eval_metric.get_name_value()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef iter_predict(self, eval_data, num_batch=None, reset=True, sparse_row_id_fn=None):\n        assert self.binded and self.params_initialized\n\n        if reset:\n            eval_data.reset()\n\n        for nbatch, eval_batch in enumerate(eval_data):\n            if num_batch is not None and nbatch == num_batch:\n                break\n            self.prepare(eval_batch, sparse_row_id_fn=sparse_row_id_fn)\n            self.forward(eval_batch, is_train=False)\n            pad = eval_batch.pad\n            outputs = [out[0:out.shape[0]-pad] for out in self.get_outputs()]\n\n            yield (outputs, nbatch, eval_batch)", "response": "Yields the outputs of each predicted entry in the module."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrunning prediction on the given data iterator and returns the outputs.", "response": "def predict(self, eval_data, num_batch=None, merge_batches=True, reset=True,\n                always_output_list=False, sparse_row_id_fn=None):\n        \"\"\"Runs prediction and collects the outputs.\n\n        When `merge_batches` is ``True`` (by default), the return value will be a list\n        ``[out1, out2, out3]``, where each element is formed by concatenating the outputs for\n        all the mini-batches. When `always_output_list` is ``False`` (as by default),\n        then in the case of a single output, `out1` is returned instead of ``[out1]``.\n\n        When `merge_batches` is ``False``, the return value will be a nested list like\n        ``[[out1_batch1, out2_batch1], [out1_batch2], ...]``. This mode is useful because\n        in some cases (e.g. bucketing), the module does not necessarily produce the same\n        number of outputs.\n\n        The objects in the results have type `NDArray`. If you need to work with a numpy array,\n        just call ``.asnumpy()`` on each `NDArray`.\n\n        Parameters\n        ----------\n        eval_data : DataIter or NDArray or numpy array\n            Evaluation data to run prediction on.\n        num_batch : int\n            Defaults to ``None``, indicates running all the batches in the data iterator.\n        merge_batches : bool\n            Defaults to ``True``, see above for return values.\n        reset : bool\n            Defaults to ``True``, indicates whether we should reset the data iter before\n            doing prediction.\n        always_output_list : bool\n            Defaults to ``False``, see above for return values.\n        sparse_row_id_fn : A callback function\n            The function  takes `data_batch` as an input and returns a dict of\n            str -> NDArray. The resulting dict is used for pulling row_sparse\n            parameters from the kvstore, where the str key is the name of the param,\n            and the value is the row id of the param to pull.\n\n        Returns\n        -------\n        list of NDArray or list of list of NDArray\n            Prediction results.\n\n        Examples\n        --------\n        >>> # An example of using `predict` for prediction.\n        >>> # Predict on the first 10 batches of val_dataiter\n        >>> mod.predict(eval_data=val_dataiter, num_batch=10)\n        \"\"\"\n        assert self.binded and self.params_initialized\n\n        if isinstance(eval_data, (ndarray.NDArray, np.ndarray)):\n            if isinstance(eval_data, np.ndarray):\n                eval_data = ndarray.array(eval_data)\n            self.forward(DataBatch([eval_data]))\n            return self.get_outputs()[0]\n\n        if not isinstance(eval_data, DataIter):\n            raise ValueError('eval_data must be of type NDArray or DataIter')\n\n        if reset:\n            eval_data.reset()\n\n        output_list = []\n\n        for nbatch, eval_batch in enumerate(eval_data):\n            if num_batch is not None and nbatch == num_batch:\n                break\n            self.prepare(eval_batch, sparse_row_id_fn=sparse_row_id_fn)\n            self.forward(eval_batch, is_train=False)\n            pad = eval_batch.pad\n            outputs = [out[0:out.shape[0]-pad].copy() for out in self.get_outputs()]\n\n            output_list.append(outputs)\n\n        if len(output_list) == 0:\n            return output_list\n\n        if merge_batches:\n            num_outputs = len(output_list[0])\n            for out in output_list:\n                assert len(out) == num_outputs, \\\n                       'Cannot merge batches, as num of outputs is not the same ' + \\\n                       'in mini-batches. Maybe bucketing is used?'\n            output_list2 = [ndarray.concatenate([out[i] for out in output_list])\n                            for i in range(num_outputs)]\n\n            if num_outputs == 1 and not always_output_list:\n                return output_list2[0]\n            return output_list2\n\n        return output_list"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nassign parameter and aux state values.", "response": "def set_params(self, arg_params, aux_params, allow_missing=False, force_init=True,\n                   allow_extra=False):\n        \"\"\"Assigns parameter and aux state values.\n\n        Parameters\n        ----------\n        arg_params : dict\n            Dictionary of name to value (`NDArray`) mapping.\n        aux_params : dict\n            Dictionary of name to value (`NDArray`) mapping.\n        allow_missing : bool\n            If ``True``, params could contain missing values, and the initializer will be\n            called to fill those missing params.\n        force_init : bool\n            If ``True``, will force re-initialize even if already initialized.\n        allow_extra : boolean, optional\n            Whether allow extra parameters that are not needed by symbol.\n            If this is True, no error will be thrown when arg_params or aux_params\n            contain extra parameters that is not needed by the executor.\n\n        Examples\n        --------\n        >>> # An example of setting module parameters.\n        >>> sym, arg_params, aux_params = mx.model.load_checkpoint(model_prefix, n_epoch_load)\n        >>> mod.set_params(arg_params=arg_params, aux_params=aux_params)\n        \"\"\"\n        self.init_params(initializer=None, arg_params=arg_params, aux_params=aux_params,\n                         allow_missing=allow_missing, force_init=force_init,\n                         allow_extra=allow_extra)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsaving model parameters to file.", "response": "def save_params(self, fname):\n        \"\"\"Saves model parameters to file.\n\n        Parameters\n        ----------\n        fname : str\n            Path to output param file.\n\n        Examples\n        --------\n        >>> # An example of saving module parameters.\n        >>> mod.save_params('myfile')\n        \"\"\"\n        arg_params, aux_params = self.get_params()\n        save_dict = {('arg:%s' % k) : v.as_in_context(cpu()) for k, v in arg_params.items()}\n        save_dict.update({('aux:%s' % k) : v.as_in_context(cpu()) for k, v in aux_params.items()})\n        ndarray.save(fname, save_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_params(self, fname):\n        save_dict = ndarray.load(fname)\n        arg_params = {}\n        aux_params = {}\n        for k, value in save_dict.items():\n            arg_type, name = k.split(':', 1)\n            if arg_type == 'arg':\n                arg_params[name] = value\n            elif arg_type == 'aux':\n                aux_params[name] = value\n            else:\n                raise ValueError(\"Invalid param file \" + fname)\n        self.set_params(arg_params, aux_params)", "response": "Loads model parameters from file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bind(self, data_shapes, label_shapes=None, for_training=True,\n             inputs_need_grad=False, force_rebind=False, shared_module=None,\n             grad_req='write'):\n        \"\"\"Binds the symbols to construct executors. This is necessary before one\n        can perform computation with the module.\n\n        Parameters\n        ----------\n        data_shapes : list of (str, tuple) or DataDesc objects\n            Typically is ``data_iter.provide_data``. Can also be a list of\n            (data name, data shape).\n        label_shapes : list of (str, tuple) or DataDesc objects\n            Typically is ``data_iter.provide_label``. Can also be a list of\n            (label name, label shape).\n        for_training : bool\n            Default is ``True``. Whether the executors should be bind for training.\n        inputs_need_grad : bool\n            Default is ``False``. Whether the gradients to the input data need to be computed.\n            Typically this is not needed. But this might be needed when implementing composition\n            of modules.\n        force_rebind : bool\n            Default is ``False``. This function does nothing if the executors are already\n            bound. But with this ``True``, the executors will be forced to rebind.\n        shared_module : Module\n            Default is ``None``. This is used in bucketing. When not ``None``, the shared module\n            essentially corresponds to a different bucket -- a module with different symbol\n            but with the same sets of parameters (e.g. unrolled RNNs with different lengths).\n        grad_req : str, list of str, dict of str to str\n            Requirement for gradient accumulation. Can be 'write', 'add', or 'null'\n            (default to 'write').\n            Can be specified globally (str) or for each argument (list, dict).\n\n        Examples\n        --------\n        >>> # An example of binding symbols.\n        >>> mod.bind(data_shapes=[('data', (1, 10, 10))])\n        >>> # Assume train_iter is already created.\n        >>> mod.bind(data_shapes=train_iter.provide_data, label_shapes=train_iter.provide_label)\n        \"\"\"\n        raise NotImplementedError()", "response": "Binds the executors to construct executors."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_lib_path():\n    lib_from_env = os.environ.get('MXNET_LIBRARY_PATH')\n    if lib_from_env:\n        if os.path.isfile(lib_from_env):\n            if not os.path.isabs(lib_from_env):\n                logging.warning(\"MXNET_LIBRARY_PATH should be an absolute path, instead of: %s\",\n                                lib_from_env)\n            else:\n                if os.name == 'nt':\n                    os.environ['PATH'] = os.environ['PATH'] + ';' + os.path.dirname(lib_from_env)\n                return [lib_from_env]\n        else:\n            logging.warning(\"MXNET_LIBRARY_PATH '%s' doesn't exist\", lib_from_env)\n\n    curr_path = os.path.dirname(os.path.abspath(os.path.expanduser(__file__)))\n    api_path = os.path.join(curr_path, '../../lib/')\n    cmake_build_path = os.path.join(curr_path, '../../build/')\n    dll_path = [curr_path, api_path, cmake_build_path]\n    if os.name == 'nt':\n        dll_path.append(os.path.join(curr_path, '../../build'))\n        vs_configuration = 'Release'\n        if platform.architecture()[0] == '64bit':\n            dll_path.append(os.path.join(curr_path, '../../build', vs_configuration))\n            dll_path.append(os.path.join(curr_path, '../../windows/x64', vs_configuration))\n        else:\n            dll_path.append(os.path.join(curr_path, '../../build', vs_configuration))\n            dll_path.append(os.path.join(curr_path, '../../windows', vs_configuration))\n    elif os.name == \"posix\" and os.environ.get('LD_LIBRARY_PATH', None):\n        dll_path[0:0] = [p.strip() for p in os.environ['LD_LIBRARY_PATH'].split(\":\")]\n    if os.name == 'nt':\n        os.environ['PATH'] = os.path.dirname(__file__) + ';' + os.environ['PATH']\n        dll_path = [os.path.join(p, 'libmxnet.dll') for p in dll_path]\n    elif platform.system() == 'Darwin':\n        dll_path = [os.path.join(p, 'libmxnet.dylib') for p in dll_path] + \\\n                   [os.path.join(p, 'libmxnet.so') for p in dll_path]\n    else:\n        dll_path.append('../../../')\n        dll_path = [os.path.join(p, 'libmxnet.so') for p in dll_path]\n    lib_path = [p for p in dll_path if os.path.exists(p) and os.path.isfile(p)]\n    if len(lib_path) == 0:\n        raise RuntimeError('Cannot find the MXNet library.\\n' +\n                           'List of candidates:\\n' + str('\\n'.join(dll_path)))\n    if os.name == 'nt':\n        os.environ['PATH'] = os.environ['PATH'] + ';' + os.path.dirname(lib_path[0])\n    return lib_path", "response": "Find MXNet dynamic library files."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind MXNet included header files.", "response": "def find_include_path():\n    \"\"\"Find MXNet included header files.\n\n    Returns\n    -------\n    incl_path : string\n        Path to the header files.\n    \"\"\"\n    incl_from_env = os.environ.get('MXNET_INCLUDE_PATH')\n    if incl_from_env:\n        if os.path.isdir(incl_from_env):\n            if not os.path.isabs(incl_from_env):\n                logging.warning(\"MXNET_INCLUDE_PATH should be an absolute path, instead of: %s\",\n                                incl_from_env)\n            else:\n                return incl_from_env\n        else:\n            logging.warning(\"MXNET_INCLUDE_PATH '%s' doesn't exist\", incl_from_env)\n\n    curr_path = os.path.dirname(os.path.abspath(os.path.expanduser(__file__)))\n    # include path in pip package\n    pip_incl_path = os.path.join(curr_path, 'include/')\n    if os.path.isdir(pip_incl_path):\n        return pip_incl_path\n    else:\n        # include path if build from source\n        src_incl_path = os.path.join(curr_path, '../../include/')\n        if os.path.isdir(src_incl_path):\n            return src_incl_path\n        else:\n            raise RuntimeError('Cannot find the MXNet include path in either ' + pip_incl_path +\n                               ' or ' + src_incl_path + '\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating a greyscale image representing number string", "response": "def image(self, captcha_str):\n        \"\"\"Generate a greyscale captcha image representing number string\n\n        Parameters\n        ----------\n        captcha_str: str\n            string a characters for captcha image\n\n        Returns\n        -------\n        numpy.ndarray\n            Generated greyscale image in np.ndarray float type with values normalized to [0, 1]\n        \"\"\"\n        img = self.captcha.generate(captcha_str)\n        img = np.fromstring(img.getvalue(), dtype='uint8')\n        img = cv2.imdecode(img, cv2.IMREAD_GRAYSCALE)\n        img = cv2.resize(img, (self.h, self.w))\n        img = img.transpose(1, 0)\n        img = np.multiply(img, 1 / 255.0)\n        return img"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a character string of digits. Number of digits are between num_digit_min and num_digit_max Returns ------- str A character string of digits.", "response": "def get_rand(num_digit_min, num_digit_max):\n        \"\"\"Generates a character string of digits. Number of digits are\n        between self.num_digit_min and self.num_digit_max\n        Returns\n        -------\n        str\n        \"\"\"\n        buf = \"\"\n        max_len = random.randint(num_digit_min, num_digit_max)\n        for i in range(max_len):\n            buf += str(random.randint(0, 9))\n        return buf"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _gen_sample(self):\n        num_str = self.get_rand(self.num_digit_min, self.num_digit_max)\n        return self.captcha.image(num_str), num_str", "response": "Generate a random captcha image sample\n        Returns ------- numpy. ndarray str"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nregister a new optimizer. This function is meant to be used by the optimizer module.", "response": "def register(klass):\n        \"\"\"Registers a new optimizer.\n\n        Once an optimizer is registered, we can create an instance of this\n        optimizer with `create_optimizer` later.\n\n        Examples\n        --------\n\n        >>> @mx.optimizer.Optimizer.register\n        ... class MyOptimizer(mx.optimizer.Optimizer):\n        ...     pass\n        >>> optim = mx.optimizer.Optimizer.create_optimizer('MyOptimizer')\n        >>> print(type(optim))\n        <class '__main__.MyOptimizer'>\n        \"\"\"\n        assert(isinstance(klass, type))\n        name = klass.__name__.lower()\n        if name in Optimizer.opt_registry:\n            warnings.warn('WARNING: New optimizer %s.%s is overriding '\n                          'existing optimizer %s.%s' %\n                          (klass.__module__, klass.__name__,\n                           Optimizer.opt_registry[name].__module__,\n                           Optimizer.opt_registry[name].__name__))\n        Optimizer.opt_registry[name] = klass\n        return klass"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate auxiliary state for a given weight including FP32 high precision copy if original weight is FP16.", "response": "def create_state_multi_precision(self, index, weight):\n        \"\"\"Creates auxiliary state for a given weight, including FP32 high\n        precision copy if original weight is FP16.\n\n        This method is provided to perform automatic mixed precision training\n        for optimizers that do not support it themselves.\n\n        Parameters\n        ----------\n        index : int\n            An unique index to identify the weight.\n        weight : NDArray\n            The weight.\n\n        Returns\n        -------\n        state : any obj\n            The state associated with the weight.\n        \"\"\"\n        weight_master_copy = None\n        if self.multi_precision and weight.dtype == numpy.float16:\n            weight_master_copy = weight.astype(numpy.float32)\n            return (weight_master_copy,) + (self.create_state(index, weight_master_copy),)\n        if weight.dtype == numpy.float16 and not self.multi_precision:\n            warnings.warn(\"Accumulating with float16 in optimizer can lead to \"\n                          \"poor accuracy or slow convergence. \"\n                          \"Consider using multi_precision=True option of the \"\n                          \"optimizer\")\n        return self.create_state(index, weight)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating the given parameter using the corresponding gradient and state.", "response": "def update_multi_precision(self, index, weight, grad, state):\n        \"\"\"Updates the given parameter using the corresponding gradient and state.\n        Mixed precision version.\n\n        Parameters\n        ----------\n        index : int\n            The unique index of the parameter into the individual learning\n            rates and weight decays. Learning rates and weight decay\n            may be set via `set_lr_mult()` and `set_wd_mult()`, respectively.\n        weight : NDArray\n            The parameter to be updated.\n        grad : NDArray\n            The gradient of the objective with respect to this parameter.\n        state : any obj\n            The state returned by `create_state()`.\n        \"\"\"\n        if self.multi_precision and weight.dtype == numpy.float16:\n            # Wrapper for mixed precision\n            weight_master_copy = state[0]\n            original_state = state[1]\n            grad32 = grad.astype(numpy.float32)\n            self.update(index, weight_master_copy, grad32, original_state)\n            cast(weight_master_copy, dtype=weight.dtype, out=weight)\n        else:\n            self.update(index, weight, grad, state)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_lr_mult(self, args_lr_mult):\n        self.lr_mult = {}\n        if self.sym_info:\n            attr, arg_names = self.sym_info\n            for name in arg_names:\n                if name in attr and '__lr_mult__' in attr[name]:\n                    self.lr_mult[name] = float(attr[name]['__lr_mult__'])\n        self.lr_mult.update(args_lr_mult)", "response": "Sets the learning rate multiplier for each parameter in the current object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the weight decay multipler for each parameter in the all - key - entries of the class.", "response": "def set_wd_mult(self, args_wd_mult):\n        \"\"\"Sets an individual weight decay multiplier for each parameter.\n\n        By default, if `param_idx2name` was provided in the\n        constructor, the weight decay multipler is set as 0 for all\n        parameters whose name don't end with ``_weight`` or\n        ``_gamma``.\n\n        .. note:: The default weight decay multiplier for a `Variable`\n            can be set with its `wd_mult` argument in the constructor.\n\n        Parameters\n        ----------\n        args_wd_mult : dict of string/int to float\n            For each of its key-value entries, the weight decay multipler for the\n            parameter specified in the key will be set as the given value.\n\n            You can specify the parameter with either its name or its index.\n            If you use the name, you should pass `sym` in the constructor,\n            and the name you specified in the key of `args_lr_mult` should match\n            the name of the parameter in `sym`. If you use the index, it should\n            correspond to the index of the parameter used in the `update` method.\n\n            Specifying a parameter by its index is only supported for backward\n            compatibility, and we recommend to use the name instead.\n        \"\"\"\n        self.wd_mult = {}\n        for n in self.idx2name.values():\n            if not (n.endswith('_weight') or n.endswith('_gamma')):\n                self.wd_mult[n] = 0.0\n        if self.sym_info:\n            attr, arg_names = self.sym_info\n            for name in arg_names:\n                if name in attr and '__wd_mult__' in attr[name]:\n                    self.wd_mult[name] = float(attr[name]['__wd_mult__'])\n        self.wd_mult.update(args_wd_mult)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _set_current_context(self, device_id):\n        if device_id not in self._all_index_update_counts:\n            self._all_index_update_counts[device_id] = {}\n        self._index_update_count = self._all_index_update_counts[device_id]", "response": "Sets the number of the currently handled device."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _update_count(self, index):\n        if not isinstance(index, (list, tuple)):\n            index = [index]\n        for idx in index:\n            if idx not in self._index_update_count:\n                self._index_update_count[idx] = self.begin_num_update\n            self._index_update_count[idx] += 1\n            self.num_update = max(self._index_update_count[idx], self.num_update)", "response": "Updates num_update.\n\n        Parameters\n        ----------\n        index : int or list of int\n            The index to be updated."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the learning rates given the indices of the weights.", "response": "def _get_lrs(self, indices):\n        \"\"\"Gets the learning rates given the indices of the weights.\n\n        Parameters\n        ----------\n        indices : list of int\n            Indices corresponding to weights.\n\n        Returns\n        -------\n        lrs : list of float\n            Learning rates for those indices.\n        \"\"\"\n        if self.lr_scheduler is not None:\n            lr = self.lr_scheduler(self.num_update)\n        else:\n            lr = self.lr\n\n        lrs = [lr for _ in indices]\n        for i, index in enumerate(indices):\n            if index in self.param_dict:\n                lrs[i] *= self.param_dict[index].lr_mult\n            elif index in self.lr_mult:\n                lrs[i] *= self.lr_mult[index]\n            elif index in self.idx2name:\n                lrs[i] *= self.lr_mult.get(self.idx2name[index], 1.0)\n        return lrs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_wds(self, indices):\n        wds = [self.wd for _ in indices]\n        for i, index in enumerate(indices):\n            if index in self.param_dict:\n                wds[i] *= self.param_dict[index].wd_mult\n            elif index in self.wd_mult:\n                wds[i] *= self.wd_mult[index]\n            elif index in self.idx2name:\n                wds[i] *= self.wd_mult.get(self.idx2name[index], 1.0)\n        return wds", "response": "Gets weight decays for the given indices."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets updater states. Parameters ---------- dump_optimizer : bool, default False Whether to also save the optimizer itself. This would also save optimizer information such as learning rate and weight decay schedules.", "response": "def get_states(self, dump_optimizer=False):\n        \"\"\"Gets updater states.\n\n        Parameters\n        ----------\n        dump_optimizer : bool, default False\n            Whether to also save the optimizer itself. This would also save optimizer\n            information such as learning rate and weight decay schedules.\n        \"\"\"\n        return pickle.dumps((self.states, self.optimizer) if dump_optimizer else self.states)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads from frames Read from frames", "response": "def from_frames(self, path):\n        \"\"\"\n        Read from frames\n        \"\"\"\n        frames_path = sorted([os.path.join(path, x) for x in os.listdir(path)])\n        frames = [ndimage.imread(frame_path) for frame_path in frames_path]\n        self.handle_type(frames)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprocesses the video types.", "response": "def handle_type(self, frames):\n        \"\"\"\n        Config video types\n        \"\"\"\n        if self.vtype == 'mouth':\n            self.process_frames_mouth(frames)\n        elif self.vtype == 'face':\n            self.process_frames_face(frames)\n        else:\n            raise Exception('Video type not found')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef process_frames_face(self, frames):\n        detector = dlib.get_frontal_face_detector()\n        predictor = dlib.shape_predictor(self.face_predictor_path)\n        mouth_frames = self.get_frames_mouth(detector, predictor, frames)\n        self.face = np.array(frames)\n        self.mouth = np.array(mouth_frames)\n        if mouth_frames[0] is not None:\n            self.set_data(mouth_frames)", "response": "Preprocess from frames using face detector and predictor."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_frames_mouth(self, detector, predictor, frames):\n        mouth_width = 100\n        mouth_height = 50\n        horizontal_pad = 0.19\n        normalize_ratio = None\n        mouth_frames = []\n        for frame in frames:\n            dets = detector(frame, 1)\n            shape = None\n            for det in dets:\n                shape = predictor(frame, det)\n                i = -1\n            if shape is None: # Detector doesn't detect face, just return None\n                return [None]\n            mouth_points = []\n            for part in shape.parts():\n                i += 1\n                if i < 48: # Only take mouth region\n                    continue\n                mouth_points.append((part.x, part.y))\n            np_mouth_points = np.array(mouth_points)\n\n            mouth_centroid = np.mean(np_mouth_points[:, -2:], axis=0)\n\n            if normalize_ratio is None:\n                mouth_left = np.min(np_mouth_points[:, :-1]) * (1.0 - horizontal_pad)\n                mouth_right = np.max(np_mouth_points[:, :-1]) * (1.0 + horizontal_pad)\n\n                normalize_ratio = mouth_width / float(mouth_right - mouth_left)\n\n            new_img_shape = (int(frame.shape[0] * normalize_ratio),\n                             int(frame.shape[1] * normalize_ratio))\n            resized_img = imresize(frame, new_img_shape)\n\n            mouth_centroid_norm = mouth_centroid * normalize_ratio\n\n            mouth_l = int(mouth_centroid_norm[0] - mouth_width / 2)\n            mouth_r = int(mouth_centroid_norm[0] + mouth_width / 2)\n            mouth_t = int(mouth_centroid_norm[1] - mouth_height / 2)\n            mouth_b = int(mouth_centroid_norm[1] + mouth_height / 2)\n\n            mouth_crop_image = resized_img[mouth_t:mouth_b, mouth_l:mouth_r]\n\n            mouth_frames.append(mouth_crop_image)\n        return mouth_frames", "response": "Get frames using mouth crop"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting video frames from skvideo. io. vreader", "response": "def get_video_frames(self, path):\n        \"\"\"\n        Get video frames\n        \"\"\"\n        videogen = skvideo.io.vreader(path)\n        frames = np.array([frame for frame in videogen])\n        return frames"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_data(self, frames):\n        data_frames = []\n        for frame in frames:\n            #frame H x W x C\n            frame = frame.swapaxes(0, 1) # swap width and height to form format W x H x C\n            if len(frame.shape) < 3:\n                frame = np.array([frame]).swapaxes(0, 2).swapaxes(0, 1) # Add grayscale channel\n            data_frames.append(frame)\n        frames_n = len(data_frames)\n        data_frames = np.array(data_frames) # T x W x H x C\n        data_frames = np.rollaxis(data_frames, 3) # C x T x W x H\n        data_frames = data_frames.swapaxes(2, 3) # C x T x H x W  = NCDHW\n\n        self.data = data_frames\n        self.length = frames_n", "response": "Prepare the input of model\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreset the iterator to the beginning of the data.", "response": "def reset(self):\n        \"\"\"Resets the iterator to the beginning of the data.\"\"\"\n        self.curr_idx = 0\n        random.shuffle(self.idx)\n        for buck in self.data:\n            np.random.shuffle(buck)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the next batch of data.", "response": "def next(self):\n        \"\"\"Returns the next batch of data.\"\"\"\n        if self.curr_idx == len(self.idx):\n            raise StopIteration\n        i, j = self.idx[self.curr_idx]\n        self.curr_idx += 1\n\n        audio_paths = []\n        texts = []\n        for duration, audio_path, text in self.data[i][j:j+self.batch_size]:\n            audio_paths.append(audio_path)\n            texts.append(text)\n\n        if self.is_first_epoch:\n            data_set = self.datagen.prepare_minibatch(audio_paths, texts, overwrite=True,\n                                                      is_bi_graphemes=self.is_bi_graphemes,\n                                                      seq_length=self.buckets[i],\n                                                      save_feature_as_csvfile=self.save_feature_as_csvfile)\n        else:\n            data_set = self.datagen.prepare_minibatch(audio_paths, texts, overwrite=False,\n                                                      is_bi_graphemes=self.is_bi_graphemes,\n                                                      seq_length=self.buckets[i],\n                                                      save_feature_as_csvfile=self.save_feature_as_csvfile)\n\n        data_all = [mx.nd.array(data_set['x'])] + self.init_state_arrays\n        label_all = [mx.nd.array(data_set['y'])]\n\n        self.label = label_all\n        provide_data = [('data', (self.batch_size, self.buckets[i], self.width * self.height))] + self.init_states\n\n        return mx.io.DataBatch(data_all, label_all, pad=0,\n                               bucket_key=self.buckets[i],\n                               provide_data=provide_data,\n                               provide_label=self.provide_label)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef subtract_imagenet_mean_preprocess_batch(batch):\n    batch = F.swapaxes(batch,0, 1)\n    (r, g, b) = F.split(batch, num_outputs=3, axis=0)\n    r = r - 123.680\n    g = g - 116.779\n    b = b - 103.939\n    batch = F.concat(b, g, r, dim=0)\n    batch = F.swapaxes(batch,0, 1)\n    return batch", "response": "Subtract ImageNet mean pixel - wise from a BGR image."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nclamps a batch according to the imagenet equation.", "response": "def imagenet_clamp_batch(batch, low, high):\n    \"\"\" Not necessary in practice \"\"\"\n    F.clip(batch[:,0,:,:],low-123.680, high-123.680)\n    F.clip(batch[:,1,:,:],low-116.779, high-116.779)\n    F.clip(batch[:,2,:,:],low-103.939, high-103.939)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a linear regression network for performing SVRG optimization.", "response": "def create_network(batch_size, update_freq):\n    \"\"\"Create a linear regression network for performing SVRG optimization.\n    :return: an instance of mx.io.NDArrayIter\n    :return: an instance of mx.mod.svrgmodule for performing SVRG optimization\n    \"\"\"\n    head = '%(asctime)-15s %(message)s'\n    logging.basicConfig(level=logging.INFO, format=head)\n    data = np.random.randint(1, 5, [1000, 2])\n\n    #Test_Train data split\n    n_train = int(data.shape[0] * 0.8)\n    weights = np.array([1.0, 2.0])\n    label = data.dot(weights)\n\n    di = mx.io.NDArrayIter(data[:n_train, :], label[:n_train], batch_size=batch_size, shuffle=True, label_name='lin_reg_label')\n    val_iter = mx.io.NDArrayIter(data[n_train:, :], label[n_train:], batch_size=batch_size)\n\n    X = mx.sym.Variable('data')\n    Y = mx.symbol.Variable('lin_reg_label')\n    fully_connected_layer = mx.sym.FullyConnected(data=X, name='fc1', num_hidden=1)\n    lro = mx.sym.LinearRegressionOutput(data=fully_connected_layer, label=Y, name=\"lro\")\n\n    mod = SVRGModule(\n        symbol=lro,\n        data_names=['data'],\n        label_names=['lin_reg_label'], update_freq=update_freq, logger=logging)\n\n    return di, val_iter, mod"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef evaluate_accuracy(data_iterator, net):\n    acc = mx.metric.Accuracy()\n    for data, label in data_iterator:\n        output = net(data)\n        predictions = nd.argmax(output, axis=1)\n        predictions = predictions.reshape((-1, 1))\n        acc.update(preds=predictions, labels=label)\n    return acc.get()[1]", "response": "Function to evaluate accuracy of any data iterator passed to it as an argument"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef train(train_dir=None, train_csv=None, epochs=30, batch_size=32):\n\n    if not train_dir or not os.path.exists(train_dir) or not train_csv:\n        warnings.warn(\"No train directory could be found \")\n        return\n    # Make a dataset from the local folder containing Audio data\n    print(\"\\nMaking an Audio Dataset...\\n\")\n    tick = time.time()\n    aud_dataset = AudioFolderDataset(train_dir, train_csv=train_csv, file_format='.wav', skip_header=True)\n    tock = time.time()\n\n    print(\"Loading the dataset took \", (tock-tick), \" seconds.\")\n    print(\"\\n=======================================\\n\")\n    print(\"Number of output classes = \", len(aud_dataset.synsets))\n    print(\"\\nThe labels are : \\n\")\n    print(aud_dataset.synsets)\n    # Get the model to train\n    net = model.get_net(len(aud_dataset.synsets))\n    print(\"\\nNeural Network = \\n\")\n    print(net)\n    print(\"\\nModel - Neural Network Generated!\\n\")\n    print(\"=======================================\\n\")\n\n    #Define the loss - Softmax CE Loss\n    softmax_loss = gluon.loss.SoftmaxCELoss(from_logits=False, sparse_label=True)\n    print(\"Loss function initialized!\\n\")\n    print(\"=======================================\\n\")\n\n    #Define the trainer with the optimizer\n    trainer = gluon.Trainer(net.collect_params(), 'adadelta')\n    print(\"Optimizer - Trainer function initialized!\\n\")\n    print(\"=======================================\\n\")\n    print(\"Loading the dataset to the Gluon's OOTB Dataloader...\")\n\n    #Getting the data loader out of the AudioDataset and passing the transform\n    from transforms import MFCC\n    aud_transform = MFCC()\n    tick = time.time()\n\n    audio_train_loader = gluon.data.DataLoader(aud_dataset.transform_first(aud_transform), batch_size=32, shuffle=True)\n    tock = time.time()\n    print(\"Time taken to load data and apply transform here is \", (tock-tick), \" seconds.\")\n    print(\"=======================================\\n\")\n\n\n    print(\"Starting the training....\\n\")\n    # Training loop\n    tick = time.time()\n    batch_size = batch_size\n    num_examples = len(aud_dataset)\n\n    for epoch in range(epochs):\n        cumulative_loss = 0\n        for data, label in audio_train_loader:\n            with autograd.record():\n                output = net(data)\n                loss = softmax_loss(output, label)\n            loss.backward()\n\n            trainer.step(batch_size)\n            cumulative_loss += mx.nd.sum(loss).asscalar()\n\n        if epoch%5 == 0:\n            train_accuracy = evaluate_accuracy(audio_train_loader, net)\n            print(\"Epoch {}. Loss: {} Train accuracy : {} \".format(epoch, cumulative_loss/num_examples, train_accuracy))\n            print(\"\\n------------------------------\\n\")\n\n    train_accuracy = evaluate_accuracy(audio_train_loader, net)\n    tock = time.time()\n    print(\"\\nFinal training accuracy: \", train_accuracy)\n\n    print(\"Training the sound classification for \", epochs, \" epochs, MLP model took \", (tock-tick), \" seconds\")\n    print(\"====================== END ======================\\n\")\n\n    print(\"Trying to save the model parameters here...\")\n    net.save_parameters(\"./net.params\")\n    print(\"Saved the model parameters in current directory.\")", "response": "Function responsible for running the training of the model."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_bulk_size(size):\n    prev = ctypes.c_int()\n    check_call(_LIB.MXEngineSetBulkSize(\n        ctypes.c_int(size), ctypes.byref(prev)))\n    return prev.value", "response": "Sets the size limit on bulk execution."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef applyLM(parentBeam, childBeam, classes, lm):\n    if lm and not childBeam.lmApplied:\n        c1 = classes[parentBeam.labeling[-1] if parentBeam.labeling else classes.index(' ')] # first char\n        c2 = classes[childBeam.labeling[-1]] # second char\n        lmFactor = 0.01 # influence of language model\n        bigramProb = lm.getCharBigram(c1, c2) ** lmFactor # probability of seeing first and second char next to each other\n        childBeam.prText = parentBeam.prText * bigramProb # probability of char sequence\n        childBeam.lmApplied = True", "response": "calculate LM score of child beam by taking score from parent beam and bigram probability of last two chars"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding beam if it does not exist", "response": "def addBeam(beamState, labeling):\n    \"\"\"\n    add beam if it does not yet exist\n    \"\"\"\n    if labeling not in beamState.entries:\n        beamState.entries[labeling] = BeamEntry()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbeam search as described by the paper of Hwang et al. and the paper of Graves et al.", "response": "def ctcBeamSearch(mat, classes, lm, k, beamWidth):\n    \"\"\"\n    beam search as described by the paper of Hwang et al. and the paper of Graves et al.\n    \"\"\"\n\n    blankIdx = len(classes)\n    maxT, maxC = mat.shape\n\n    # initialise beam state\n    last = BeamState()\n    labeling = ()\n    last.entries[labeling] = BeamEntry()\n    last.entries[labeling].prBlank = 1\n    last.entries[labeling].prTotal = 1\n\n    # go over all time-steps\n    for t in range(maxT):\n        curr = BeamState()\n\n        # get beam-labelings of best beams\n        bestLabelings = last.sort()[0:beamWidth]\n\n\t    # go over best beams\n        for labeling in bestLabelings:\n\n\t        # probability of paths ending with a non-blank\n            prNonBlank = 0\n\t        # in case of non-empty beam\n            if labeling:\n\t\t       # probability of paths with repeated last char at the end\n                try: \n                    prNonBlank = last.entries[labeling].prNonBlank * mat[t, labeling[-1]]\n                except FloatingPointError:\n                    prNonBlank = 0\n\n\t    # probability of paths ending with a blank\n            prBlank = (last.entries[labeling].prTotal) * mat[t, blankIdx]\n\n\t    # add beam at current time-step if needed\n            addBeam(curr, labeling)\n\n            # fill in data\n            curr.entries[labeling].labeling = labeling\n            curr.entries[labeling].prNonBlank += prNonBlank\n            curr.entries[labeling].prBlank += prBlank\n            curr.entries[labeling].prTotal += prBlank + prNonBlank\n            curr.entries[labeling].prText = last.entries[labeling].prText # beam-labeling not changed, therefore also LM score unchanged from\n            curr.entries[labeling].lmApplied = True # LM already applied at previous time-step for this beam-labeling\n\n            # extend current beam-labeling\n            for c in range(maxC - 1):\n                # add new char to current beam-labeling\n                newLabeling = labeling + (c,)\n\n                # if new labeling contains duplicate char at the end, only consider paths ending with a blank\n                if labeling and labeling[-1] == c:\n                    prNonBlank = mat[t, c] * last.entries[labeling].prBlank\n                else:\n                    prNonBlank = mat[t, c] * last.entries[labeling].prTotal\n\n\t\t        # add beam at current time-step if needed\n                addBeam(curr, newLabeling)\n\t\t\t\t\n\t\t        # fill in data\n                curr.entries[newLabeling].labeling = newLabeling\n                curr.entries[newLabeling].prNonBlank += prNonBlank\n                curr.entries[newLabeling].prTotal += prNonBlank\n\t\t\t\t\n\t\t        # apply LM\n                applyLM(curr.entries[labeling], curr.entries[newLabeling], classes, lm)\n\n        # set new beam state\n        last = curr\n\n    # normalise LM scores according to beam-labeling-length\n    last.norm()\n\n    # sort by probability\n    bestLabelings = last.sort()[:k] # get most probable labeling\n\n    output = []\n    for bestLabeling in bestLabelings:\n        # map labels to chars\n        res = ''\n        for l in bestLabeling:\n            res += classes[l]\n        output.append(res)\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef norm(self):\n        for (k, _) in self.entries.items():\n            labelingLen = len(self.entries[k].labeling)\n            self.entries[k].prText = self.entries[k].prText ** (1.0 / (labelingLen if labelingLen else 1.0))", "response": "Normalize the LM score."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning beam - labelings sorted by probability", "response": "def sort(self):\n        \"\"\"\n        return beam-labelings, sorted by probability\n        \"\"\"\n        beams = [v for (_, v) in self.entries.items()]\n        sortedBeams = sorted(beams, reverse=True, key=lambda x: x.prTotal*x.prText)\n        return [x.labeling for x in sortedBeams]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the localisation network in lenet - stn", "response": "def get_loc(data, attr={'lr_mult':'0.01'}):\n    \"\"\"\n    the localisation network in lenet-stn, it will increase acc about more than 1%,\n    when num-epoch >=15\n    \"\"\"\n    loc = mx.symbol.Convolution(data=data, num_filter=30, kernel=(5, 5), stride=(2,2))\n    loc = mx.symbol.Activation(data = loc, act_type='relu')\n    loc = mx.symbol.Pooling(data=loc, kernel=(2, 2), stride=(2, 2), pool_type='max')\n    loc = mx.symbol.Convolution(data=loc, num_filter=60, kernel=(3, 3), stride=(1,1), pad=(1, 1))\n    loc = mx.symbol.Activation(data = loc, act_type='relu')\n    loc = mx.symbol.Pooling(data=loc, global_pool=True, kernel=(2, 2), pool_type='avg')\n    loc = mx.symbol.Flatten(data=loc)\n    loc = mx.symbol.FullyConnected(data=loc, num_hidden=6, name=\"stn_loc\", attr=attr)\n    return loc"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a detector for a given test network", "response": "def get_detector(net, prefix, epoch, data_shape, mean_pixels, ctx, num_class,\n                 nms_thresh=0.5, force_nms=True, nms_topk=400):\n    \"\"\"\n    wrapper for initialize a detector\n\n    Parameters:\n    ----------\n    net : str\n        test network name\n    prefix : str\n        load model prefix\n    epoch : int\n        load model epoch\n    data_shape : int\n        resize image shape\n    mean_pixels : tuple (float, float, float)\n        mean pixel values (R, G, B)\n    ctx : mx.ctx\n        running context, mx.cpu() or mx.gpu(?)\n    num_class : int\n        number of classes\n    nms_thresh : float\n        non-maximum suppression threshold\n    force_nms : bool\n        force suppress different categories\n    \"\"\"\n    if net is not None:\n        if isinstance(data_shape, tuple):\n            data_shape = data_shape[0]\n        net = get_symbol(net, data_shape, num_classes=num_class, nms_thresh=nms_thresh,\n            force_nms=force_nms, nms_topk=nms_topk)\n    detector = Detector(net, prefix, epoch, data_shape, mean_pixels, ctx=ctx)\n    return detector"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_class_names(class_names):\n    if len(class_names) > 0:\n        if os.path.isfile(class_names):\n            # try to open it to read class names\n            with open(class_names, 'r') as f:\n                class_names = [l.strip() for l in f.readlines()]\n        else:\n            class_names = [c.strip() for c in class_names.split(',')]\n        for name in class_names:\n            assert len(name) > 0\n    else:\n        raise RuntimeError(\"No valid class_name provided...\")\n    return class_names", "response": "parse class_names if applicable"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_data_shape(data_shape_str):\n    ds = data_shape_str.strip().split(',')\n    if len(ds) == 1:\n        data_shape = (int(ds[0]), int(ds[0]))\n    elif len(ds) == 2:\n        data_shape = (int(ds[0]), int(ds[1]))\n    else:\n        raise ValueError(\"Unexpected data_shape: %s\", data_shape_str)\n    return data_shape", "response": "Parse string to tuple or int"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef encode_label(label_data):\n    systole = label_data[:, 1]\n    diastole = label_data[:, 2]\n    systole_encode = np.array([\n            (x < np.arange(600)) for x in systole\n        ], dtype=np.uint8)\n    diastole_encode = np.array([\n            (x < np.arange(600)) for x in diastole\n        ], dtype=np.uint8)\n    return systole_encode, diastole_encode", "response": "Run encoding to encode the label into the CDF target."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload an annotation file and return a ROIDB entry.", "response": "def _load_annotation(self, _coco, coco_ind_to_class_ind, index):\n        \"\"\"\n        coco ann: [u'segmentation', u'area', u'iscrowd', u'image_id', u'bbox', u'category_id', u'id']\n        iscrowd:\n            crowd instances are handled by marking their overlaps with all categories to -1\n            and later excluded in training\n        bbox:\n            [x1, y1, w, h]\n        :param index: coco image id\n        :return: roidb entry\n        \"\"\"\n        im_ann = _coco.loadImgs(index)[0]\n        filename = self._image_file_tmpl.format(im_ann['file_name'])\n        width = im_ann['width']\n        height = im_ann['height']\n\n        annIds = _coco.getAnnIds(imgIds=index, iscrowd=None)\n        objs = _coco.loadAnns(annIds)\n\n        # sanitize bboxes\n        valid_objs = []\n        for obj in objs:\n            x, y, w, h = obj['bbox']\n            x1 = np.max((0, x))\n            y1 = np.max((0, y))\n            x2 = np.min((width - 1, x1 + np.max((0, w - 1))))\n            y2 = np.min((height - 1, y1 + np.max((0, h - 1))))\n            if obj['area'] > 0 and x2 >= x1 and y2 >= y1:\n                obj['clean_bbox'] = [x1, y1, x2, y2]\n                valid_objs.append(obj)\n        objs = valid_objs\n        num_objs = len(objs)\n\n        boxes = np.zeros((num_objs, 4), dtype=np.uint16)\n        gt_classes = np.zeros((num_objs,), dtype=np.int32)\n        for ix, obj in enumerate(objs):\n            cls = coco_ind_to_class_ind[obj['category_id']]\n            boxes[ix, :] = obj['clean_bbox']\n            gt_classes[ix] = cls\n\n        roi_rec = {'index': index,\n                   'image': filename,\n                   'height': height,\n                   'width': width,\n                   'boxes': boxes,\n                   'gt_classes': gt_classes,\n                   'flipped': False}\n        return roi_rec"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite the results to the result file", "response": "def _write_coco_results(self, _coco, detections):\n        \"\"\" example results\n        [{\"image_id\": 42,\n          \"category_id\": 18,\n          \"bbox\": [258.15,41.29,348.26,243.78],\n          \"score\": 0.236}, ...]\n        \"\"\"\n        cats = [cat['name'] for cat in _coco.loadCats(_coco.getCatIds())]\n        class_to_coco_ind = dict(zip(cats, _coco.getCatIds()))\n        results = []\n        for cls_ind, cls in enumerate(self.classes):\n            if cls == '__background__':\n                continue\n            logger.info('collecting %s results (%d/%d)' % (cls, cls_ind, self.num_classes - 1))\n            coco_cat_id = class_to_coco_ind[cls]\n            results.extend(self._coco_results_one_category(detections[cls_ind], coco_cat_id))\n        logger.info('writing results json to %s' % self._result_file)\n        with open(self._result_file, 'w') as f:\n            json.dump(results, f, sort_keys=True, indent=4)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rand_zipfian(true_classes, num_sampled, range_max, ctx=None):\n    if ctx is None:\n        ctx = current_context()\n    log_range = math.log(range_max + 1)\n    rand = uniform(0, log_range, shape=(num_sampled,), dtype='float64', ctx=ctx)\n    # make sure sampled_classes are in the range of [0, range_max)\n    sampled_classes = (rand.exp() - 1).astype('int64') % range_max\n\n    true_cls = true_classes.as_in_context(ctx).astype('float64')\n    expected_count_true = ((true_cls + 2.0) / (true_cls + 1.0)).log() / log_range * num_sampled\n    # cast sampled classes to fp64 to avoid interget division\n    sampled_cls_fp64 = sampled_classes.astype('float64')\n    expected_prob_sampled = ((sampled_cls_fp64 + 2.0) / (sampled_cls_fp64 + 1.0)).log() / log_range\n    expected_count_sampled = expected_prob_sampled * num_sampled\n    return sampled_classes, expected_count_true, expected_count_sampled", "response": "Draw random samples from an approximately log - uniform or Zipfian distribution."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef foreach(body, data, init_states):\n\n    def check_input(inputs, in_type, msg):\n        is_NDArray_or_list = True\n        if isinstance(inputs, list):\n            for i in inputs:\n                if not isinstance(i, in_type):\n                    is_NDArray_or_list = False\n                    break\n        else:\n            is_NDArray_or_list = isinstance(inputs, in_type)\n        assert is_NDArray_or_list, msg\n\n    flatten, _ = _flatten(data, \"foreach input\")\n    check_input(flatten, ndarray.NDArray,\n                \"data should be an NDArray or a nested list of NDArrays\")\n    flatten, _ = _flatten(init_states, \"foreach states\")\n    check_input(flatten, ndarray.NDArray,\n                \"init_states should be an NDArray or a nested list of NDArrays\")\n\n    not_data_list = isinstance(data, ndarray.NDArray)\n    num_iters = data.shape[0] if not_data_list else data[0].shape[0]\n    states = init_states\n    outputs = []\n    for i in range(num_iters):\n        if not_data_list:\n            eles = data[i]\n        else:\n            eles = [d[i] for d in data]\n        outs, states = body(eles, states)\n        outs, out_fmt = _flatten(outs, \"foreach output\")\n        outputs.append(outs)\n    outputs = zip(*outputs)\n    tmp_outputs = []\n    for out in outputs:\n        tmp_outputs.append(ndarray.op.stack(*out))\n    outputs = tmp_outputs\n    outputs, _ = _regroup(outputs, out_fmt)\n\n    return (outputs, states)", "response": "This function runs a Python function that runs a user - defined computation over NDArrays on dimension 0 and returns a tuple of two elements out and states."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef while_loop(cond, func, loop_vars, max_iterations=None):\n    def _to_python_scalar(inputs, type_, name):\n        \"\"\"Converts \"inputs\", possibly typed mxnet NDArray, a numpy ndarray, other python types,\n        to the given type\n        \"\"\"\n        if isinstance(inputs, ndarray.NDArray):\n            inputs = inputs.asscalar()\n        try:\n            inputs = type_(inputs)\n        except:\n            raise ValueError(\"Cannot convert %s to python %s\" % (name, type_.__name__))\n        return inputs\n\n    def _func_wrapper(loop_vars):\n        \"\"\"This wrapper unifies\n             \"func: loop_vars -> new_loop_vars\"\n         and \"func: loop_vars -> (step_output, new_loop_vars)\"\n        into \"func: loop_vars -> (None or tuple of step_outputs, tuple of new_loop_vars)\n        \"\"\"\n        step_output, new_loop_vars = func(*loop_vars)\n        if step_output is None:\n            step_output = []\n        if new_loop_vars is None:\n            new_loop_vars = []\n        if isinstance(step_output, tuple):\n            step_output = list(step_output)\n        if isinstance(new_loop_vars, tuple):\n            new_loop_vars = list(new_loop_vars)\n        new_loop_vars = _as_list(new_loop_vars)\n        if len(loop_vars) != len(new_loop_vars):\n            raise ValueError(\"The length of loop_vars should be consistent during the loop\")\n        return step_output, new_loop_vars\n\n    if max_iterations is None:\n        raise ValueError(\"max_iterations should be specified\")\n    max_iterations = _to_python_scalar(max_iterations, int, \"max_iteration\")\n    # It should be work as fine if loop_vars are empty I guess,\n    # but it is semantically unnecessary to include this case.\n    if len(loop_vars) == 0:\n        raise ValueError(\"loop_vars should contain at least one element\")\n\n    steps = 0\n    outputs = []\n    # there might not be an iteration.\n    out_fmt = None\n    not_loop_var_list = isinstance(loop_vars, ndarray.NDArray)\n    loop_vars = _as_list(loop_vars)\n    while steps < max_iterations and \\\n            _to_python_scalar(cond(*loop_vars), bool, \"Return value of cond\"): # loop condition\n        step_output, loop_vars = _func_wrapper(loop_vars)\n        step_output, out_fmt = _flatten(step_output, \"while output\")\n        outputs.append(step_output)\n        steps += 1\n        if len(outputs) != steps or len(step_output) != len(outputs[0]):\n            raise ValueError(\"Number of elements in step_output should be the same in each step\")\n    stacked_outputs = []\n    for i_th, items in enumerate(zip(*outputs), 1):\n        # `mx.ndarray.pad` only support 4-D or 5-D inputs for now\n        # so we could not use it.\n        items = [x.expand_dims(0) for x in items]\n        if steps != max_iterations and items:\n            pad_shape = [max_iterations - steps] + list(items[0].shape[1: ])\n            pad = ndarray.empty(\n                shape=pad_shape,\n                ctx=items[0].context,\n                dtype=items[0].dtype,\n            )\n            items = list(items) + [pad]\n        try:\n            stacked_outputs.append(ndarray.op.concat(*items, dim=0))\n        except ValueError:\n            raise ValueError(\"\\n\".join(\n                [\"Shapes of %d-th elements in step_outputs are inconsistent, which are:\" % i_th] +\n                [\"  Step %d, shape is %s\" % (i, str(x.shape)) for i, x in enumerate(items)]\n            ))\n    if out_fmt is not None:\n        stacked_outputs, _ = _regroup(stacked_outputs, out_fmt)\n    if not_loop_var_list:\n        loop_vars = loop_vars[0]\n    return stacked_outputs, loop_vars", "response": "This operator simulates a while loop with user - defined computation and loop condition."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cond(pred, then_func, else_func):\n    def _to_python_scalar(inputs, type_, name):\n        \"\"\"Converts \"inputs\", possibly typed mxnet NDArray, a numpy ndarray, other python types,\n        to the given type\n        \"\"\"\n        if hasattr(inputs, \"asscalar\"):\n            inputs = inputs.asscalar()\n        try:\n            inputs = type_(inputs)\n        except:\n            raise ValueError(\"Cannot convert %s to python %s\" % (name, type_.__name__))\n        return inputs\n\n    branch = _to_python_scalar(pred, bool, \"pred\")\n    if branch:\n        return then_func()\n    else:\n        return else_func()", "response": "This operator simulates an if - like branch which chooses to do one of the two customized computations according to the specified condition and then_func and else_func."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef isfinite(data):\n    is_data_not_nan = data == data\n    is_data_not_infinite = data.abs() != np.inf\n    return ndarray.logical_and(is_data_not_infinite, is_data_not_nan)", "response": "Performs an element - wise element - wise check to determine if the NDArray contains an infinite element\n    or not."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef vanilla_lstm(num_hidden, indata, prev_state, param, seqidx, layeridx, is_batchnorm=False, gamma=None, beta=None, name=None):\n    i2h = mx.sym.FullyConnected(data=indata,\n                                weight=param.i2h_weight,\n                                bias=param.i2h_bias,\n                                num_hidden=num_hidden * 4,\n                                name=\"t%d_l%d_i2h\" % (seqidx, layeridx))\n    if is_batchnorm:\n        if name is not None:\n            i2h = batchnorm(net=i2h, gamma=gamma, beta=beta, name=\"%s_batchnorm\" % name)\n        else:\n            i2h = batchnorm(net=i2h, gamma=gamma, beta=beta)\n    h2h = mx.sym.FullyConnected(data=prev_state.h,\n                                weight=param.h2h_weight,\n                                bias=param.h2h_bias,\n                                num_hidden=num_hidden * 4,\n                                name=\"t%d_l%d_h2h\" % (seqidx, layeridx))\n    gates = i2h + h2h\n    slice_gates = mx.sym.SliceChannel(gates, num_outputs=4,\n                                      name=\"t%d_l%d_slice\" % (seqidx, layeridx))\n    in_gate = mx.sym.Activation(slice_gates[0], act_type=\"sigmoid\")\n    in_transform = mx.sym.Activation(slice_gates[1], act_type=\"tanh\")\n    forget_gate = mx.sym.Activation(slice_gates[2], act_type=\"sigmoid\")\n    out_gate = mx.sym.Activation(slice_gates[3], act_type=\"sigmoid\")\n    next_c = (forget_gate * prev_state.c) + (in_gate * in_transform)\n    next_h = out_gate * mx.sym.Activation(next_c, act_type=\"tanh\")\n    return LSTMState(c=next_c, h=next_h)", "response": "Returns a vanilla LSTM cell symbol."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_image(roi_rec, short, max_size, mean, std):\n    im = imdecode(roi_rec['image'])\n    if roi_rec[\"flipped\"]:\n        im = im[:, ::-1, :]\n    im, im_scale = resize(im, short, max_size)\n    height, width = im.shape[:2]\n    im_info = np.array([height, width, im_scale], dtype=np.float32)\n    im_tensor = transform(im, mean, std)\n\n    # gt boxes: (x1, y1, x2, y2, cls)\n    if roi_rec['gt_classes'].size > 0:\n        gt_inds = np.where(roi_rec['gt_classes'] != 0)[0]\n        gt_boxes = np.empty((len(gt_inds), 5), dtype=np.float32)\n        gt_boxes[:, 0:4] = roi_rec['boxes'][gt_inds, :]\n        gt_boxes[:, 4] = roi_rec['gt_classes'][gt_inds]\n        # scale gt_boxes\n        gt_boxes[:, 0:4] *= im_scale\n    else:\n        gt_boxes = np.empty((0, 5), dtype=np.float32)\n\n    return im_tensor, im_info, gt_boxes", "response": "get_image is used to get image from ROI record"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn BGR image read by opencv", "response": "def imdecode(image_path):\n    \"\"\"Return BGR image read by opencv\"\"\"\n    import os\n    assert os.path.exists(image_path), image_path + ' not found'\n    im = cv2.imread(image_path)\n    return im"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef resize(im, short, max_size):\n    im_shape = im.shape\n    im_size_min = np.min(im_shape[0:2])\n    im_size_max = np.max(im_shape[0:2])\n    im_scale = float(short) / float(im_size_min)\n    # prevent bigger axis from being more than max_size:\n    if np.round(im_scale * im_size_max) > max_size:\n        im_scale = float(max_size) / float(im_size_max)\n    im = cv2.resize(im, None, None, fx=im_scale, fy=im_scale, interpolation=cv2.INTER_LINEAR)\n    return im, im_scale", "response": "resize image to target size and return scale\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef transform(im, mean, std):\n    im_tensor = np.zeros((3, im.shape[0], im.shape[1]))\n    for i in range(3):\n        im_tensor[i, :, :] = (im[:, :, 2 - i] - mean[i]) / std[i]\n    return im_tensor", "response": "transform into mxnet tensor"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef transform_inverse(im_tensor, mean, std):\n    assert im_tensor.shape[0] == 3\n    im = im_tensor.transpose((1, 2, 0))\n    im = im * std + mean\n    im = im.astype(np.uint8)\n    return im", "response": "transform from mxnet im_tensor to ordinary RGB image\n    im_tensor is limited to one image\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_distance_matrix(x):\n    square = nd.sum(x ** 2.0, axis=1, keepdims=True)\n    distance_square = square + square.transpose() - (2.0 * nd.dot(x, x.transpose()))\n    return nd.sqrt(distance_square)", "response": "Get distance matrix given a matrix. Used in testing."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef evaluate_emb(emb, labels):\n    d_mat = get_distance_matrix(emb)\n    d_mat = d_mat.asnumpy()\n    labels = labels.asnumpy()\n\n    names = []\n    accs = []\n    for k in [1, 2, 4, 8, 16]:\n        names.append('Recall@%d' % k)\n        correct, cnt = 0.0, 0.0\n        for i in range(emb.shape[0]):\n            d_mat[i, i] = 1e10\n            nns = argpartition(d_mat[i], k)[:k]\n            if any(labels[i] == labels[nn] for nn in nns):\n                correct += 1\n            cnt += 1\n        accs.append(correct/cnt)\n    return names, accs", "response": "Evaluate embeddings based on Recall@k."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting learning rate based on schedule.", "response": "def get_lr(lr, epoch, steps, factor):\n    \"\"\"Get learning rate based on schedule.\"\"\"\n    for s in steps:\n        if epoch >= s:\n            lr *= factor\n    return lr"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef train(epochs, ctx):\n    if isinstance(ctx, mx.Context):\n        ctx = [ctx]\n    net.initialize(mx.init.Xavier(magnitude=2), ctx=ctx)\n\n    opt_options = {'learning_rate': opt.lr, 'wd': opt.wd}\n    if opt.optimizer == 'sgd':\n        opt_options['momentum'] = 0.9\n    if opt.optimizer == 'adam':\n        opt_options['epsilon'] = 1e-7\n    trainer = gluon.Trainer(net.collect_params(), opt.optimizer,\n                            opt_options,\n                            kvstore=opt.kvstore)\n    if opt.lr_beta > 0.0:\n        # Jointly train class-specific beta.\n        # See \"sampling matters in deep embedding learning\" paper for details.\n        beta.initialize(mx.init.Constant(opt.beta), ctx=ctx)\n        trainer_beta = gluon.Trainer([beta], 'sgd',\n                                     {'learning_rate': opt.lr_beta, 'momentum': 0.9},\n                                     kvstore=opt.kvstore)\n\n    loss = MarginLoss(margin=opt.margin, nu=opt.nu)\n\n    best_val = 0.0\n    for epoch in range(epochs):\n        tic = time.time()\n        prev_loss, cumulative_loss = 0.0, 0.0\n\n        # Learning rate schedule.\n        trainer.set_learning_rate(get_lr(opt.lr, epoch, steps, opt.factor))\n        logging.info('Epoch %d learning rate=%f', epoch, trainer.learning_rate)\n        if opt.lr_beta > 0.0:\n            trainer_beta.set_learning_rate(get_lr(opt.lr_beta, epoch, steps, opt.factor))\n            logging.info('Epoch %d beta learning rate=%f', epoch, trainer_beta.learning_rate)\n\n        # Inner training loop.\n        for i in range(200):\n            batch = train_data.next()\n            data = gluon.utils.split_and_load(batch.data[0], ctx_list=ctx, batch_axis=0)\n            label = gluon.utils.split_and_load(batch.label[0], ctx_list=ctx, batch_axis=0)\n\n            Ls = []\n            with ag.record():\n                for x, y in zip(data, label):\n                    a_indices, anchors, positives, negatives, _ = net(x)\n\n                    if opt.lr_beta > 0.0:\n                        L = loss(anchors, positives, negatives, beta, y[a_indices])\n                    else:\n                        L = loss(anchors, positives, negatives, opt.beta, None)\n\n                    # Store the loss and do backward after we have done forward\n                    # on all GPUs for better speed on multiple GPUs.\n                    Ls.append(L)\n                    cumulative_loss += nd.mean(L).asscalar()\n\n                for L in Ls:\n                    L.backward()\n\n            # Update.\n            trainer.step(batch.data[0].shape[0])\n            if opt.lr_beta > 0.0:\n                trainer_beta.step(batch.data[0].shape[0])\n\n            if (i+1) % opt.log_interval == 0:\n                logging.info('[Epoch %d, Iter %d] training loss=%f' % (\n                    epoch, i+1, cumulative_loss - prev_loss))\n                prev_loss = cumulative_loss\n\n        logging.info('[Epoch %d] training loss=%f'%(epoch, cumulative_loss))\n        logging.info('[Epoch %d] time cost: %f'%(epoch, time.time()-tic))\n\n        names, val_accs = test(ctx)\n        for name, val_acc in zip(names, val_accs):\n            logging.info('[Epoch %d] validation: %s=%f'%(epoch, name, val_acc))\n\n        if val_accs[0] > best_val:\n            best_val = val_accs[0]\n            logging.info('Saving %s.' % opt.save_model_prefix)\n            net.save_parameters('%s.params' % opt.save_model_prefix)\n    return best_val", "response": "Train the deep embedding model."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _lstm_unroll_base(num_lstm_layer, seq_len, num_hidden):\n    param_cells = []\n    last_states = []\n    for i in range(num_lstm_layer):\n        param_cells.append(LSTMParam(i2h_weight=mx.sym.Variable(\"l%d_i2h_weight\" % i),\n                                     i2h_bias=mx.sym.Variable(\"l%d_i2h_bias\" % i),\n                                     h2h_weight=mx.sym.Variable(\"l%d_h2h_weight\" % i),\n                                     h2h_bias=mx.sym.Variable(\"l%d_h2h_bias\" % i)))\n        state = LSTMState(c=mx.sym.Variable(\"l%d_init_c\" % i),\n                          h=mx.sym.Variable(\"l%d_init_h\" % i))\n        last_states.append(state)\n    assert len(last_states) == num_lstm_layer\n\n    # embedding layer\n    data = mx.sym.Variable('data')\n    wordvec = mx.sym.SliceChannel(data=data, num_outputs=seq_len, squeeze_axis=1)\n\n    hidden_all = []\n    for seqidx in range(seq_len):\n        hidden = wordvec[seqidx]\n        for i in range(num_lstm_layer):\n            next_state = _lstm(\n                num_hidden=num_hidden,\n                indata=hidden,\n                prev_state=last_states[i],\n                param=param_cells[i],\n                seqidx=seqidx,\n                layeridx=i)\n            hidden = next_state.h\n            last_states[i] = next_state\n        hidden_all.append(hidden)\n\n    hidden_concat = mx.sym.Concat(*hidden_all, dim=0)\n    pred_fc = mx.sym.FullyConnected(data=hidden_concat, num_hidden=11, name=\"pred_fc\")\n    return pred_fc", "response": "Returns symbol for LSTM model up to loss and softmax"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _add_warp_ctc_loss(pred, seq_len, num_label, label):\n    label = mx.sym.Reshape(data=label, shape=(-1,))\n    label = mx.sym.Cast(data=label, dtype='int32')\n    return mx.sym.WarpCTC(data=pred, label=label, label_length=num_label, input_length=seq_len)", "response": "Adds a WarpCTC loss on top of pred symbol and returns the resulting symbol"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding Symbol. WapCTC on top of pred symbol and returns the resulting symbol", "response": "def _add_mxnet_ctc_loss(pred, seq_len, label):\n    \"\"\" Adds Symbol.WapCTC on top of pred symbol and returns the resulting symbol \"\"\"\n    pred_ctc = mx.sym.Reshape(data=pred, shape=(-4, seq_len, -1, 0))\n\n    loss = mx.sym.contrib.ctc_loss(data=pred_ctc, label=label)\n    ctc_loss = mx.sym.MakeLoss(loss)\n\n    softmax_class = mx.symbol.SoftmaxActivation(data=pred)\n    softmax_loss = mx.sym.MakeLoss(softmax_class)\n    softmax_loss = mx.sym.BlockGrad(softmax_loss)\n    return mx.sym.Group([softmax_loss, ctc_loss])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _add_ctc_loss(pred, seq_len, num_label, loss_type):\n    label = mx.sym.Variable('label')\n    if loss_type == 'warpctc':\n        print(\"Using WarpCTC Loss\")\n        sm = _add_warp_ctc_loss(pred, seq_len, num_label, label)\n    else:\n        print(\"Using MXNet CTC Loss\")\n        assert loss_type == 'ctc'\n        sm = _add_mxnet_ctc_loss(pred, seq_len, label)\n    return sm", "response": "Adds CTC loss on top of pred symbol and returns the resulting symbol"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates an unrolled LSTM symbol for training or inference.", "response": "def lstm_unroll(num_lstm_layer, seq_len, num_hidden, num_label, loss_type=None):\n    \"\"\"\n    Creates an unrolled LSTM symbol for inference if loss_type is not specified, and for training\n    if loss_type is specified. loss_type must be one of 'ctc' or 'warpctc'\n\n    Parameters\n    ----------\n    num_lstm_layer: int\n    seq_len: int\n    num_hidden: int\n    num_label: int\n    loss_type: str\n        'ctc' or 'warpctc'\n\n    Returns\n    -------\n    mxnet.symbol.symbol.Symbol\n    \"\"\"\n    # Create the base (shared between training and inference) and add loss to the end\n    pred = _lstm_unroll_base(num_lstm_layer, seq_len, num_hidden)\n\n    if loss_type:\n        # Training mode, add loss\n        return _add_ctc_loss(pred, seq_len, num_label, loss_type)\n    else:\n        # Inference mode, add softmax\n        return mx.sym.softmax(data=pred, name='softmax')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef init_states(batch_size, num_lstm_layer, num_hidden):\n    init_c = [('l%d_init_c' % l, (batch_size, num_hidden)) for l in range(num_lstm_layer)]\n    init_h = [('l%d_init_h' % l, (batch_size, num_hidden)) for l in range(num_lstm_layer)]\n    return init_c + init_h", "response": "Returns name and shape of init states of LSTM network"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _imperative_invoke(handle, ndargs, keys, vals, out):\n    if out is not None:\n        original_output = out\n        if isinstance(out, NDArrayBase):\n            out = (out,)\n        num_output = ctypes.c_int(len(out))\n        output_vars = c_handle_array(out)\n        output_vars = ctypes.cast(output_vars, ctypes.POINTER(NDArrayHandle))\n    else:\n        original_output = None\n        output_vars = ctypes.POINTER(NDArrayHandle)()\n        num_output = ctypes.c_int(0)\n\n    # return output stypes to avoid the c_api call for checking\n    # a handle's stype in _ndarray_cls\n    out_stypes = ctypes.POINTER(ctypes.c_int)()\n\n    check_call(_LIB.MXImperativeInvokeEx(\n        ctypes.c_void_p(handle),\n        ctypes.c_int(len(ndargs)),\n        c_handle_array(ndargs),\n        ctypes.byref(num_output),\n        ctypes.byref(output_vars),\n        ctypes.c_int(len(keys)),\n        c_str_array(keys),\n        c_str_array([str(s) for s in vals]),\n        ctypes.byref(out_stypes)))\n\n    if original_output is not None:\n        return original_output\n    if num_output.value == 1:\n        return _ndarray_cls(ctypes.cast(output_vars[0], NDArrayHandle),\n                            stype=out_stypes[0])\n    else:\n        return [_ndarray_cls(ctypes.cast(output_vars[i], NDArrayHandle),\n                             stype=out_stypes[i])\n                for i in range(num_output.value)]", "response": "ctypes implementation of imperative invoke wrapper."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_is_training(is_train):\n    prev = ctypes.c_int()\n    check_call(_LIB.MXAutogradSetIsTraining(\n        ctypes.c_int(is_train), ctypes.byref(prev)))\n    check_call(_LIB.MXAutogradSetIsRecording(\n        ctypes.c_int(is_train), ctypes.byref(prev)))\n    return bool(prev.value)", "response": "Sets the status of the\n    to training or not training."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef backward(outputs, out_grads=None, retain_graph=False):\n    assert isinstance(outputs, (list, tuple)), \\\n        \"outputs must be a list or tuple of NDArrays\"\n\n    if out_grads is None:\n        check_call(_LIB.MXAutogradBackward(\n            len(outputs),\n            c_handle_array(outputs),\n            ctypes.c_void_p(0),\n            ctypes.c_int(retain_graph)))\n        return\n\n    ograd_handles = []\n    for arr in out_grads:\n        if arr is not None:\n            ograd_handles.append(arr.handle)\n        else:\n            ograd_handles.append(NDArrayHandle(0))\n    assert len(ograd_handles) == len(outputs), \\\n        \"outputs and out_grads must have the same length\"\n\n    check_call(_LIB.MXAutogradBackward(\n        len(outputs),\n        c_handle_array(outputs),\n        c_array(NDArrayHandle, ograd_handles),\n        ctypes.c_int(retain_graph)))", "response": "Compute the gradients of outputs w. r. t variables."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef grad_and_loss(func, argnum=None):\n    @functools.wraps(func)\n    def wrapped(*args):\n        \"\"\"Wrapped function.\"\"\"\n        variables = args\n        if argnum is not None:\n            argnum_ = argnum if isinstance(argnum, list) else [argnum]\n            variables = [args[i] for i in argnum_]\n        for x in variables:\n            assert isinstance(x, NDArray), \"type of autograd input should NDArray.\"\n        grads = [zeros_like(x) for x in variables]\n        mark_variables(variables, grads)\n        with train_section():\n            outputs = func(*args)\n        compute_gradient([outputs] if isinstance(outputs, NDArray) else outputs)\n        return grads, outputs\n    return wrapped", "response": "Returns a function that computes both the gradient of arguments and loss value."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a function that computes the gradient of arguments.", "response": "def grad(func, argnum=None):\n    \"\"\"Return function that computes gradient of arguments.\n\n    Parameters\n    ----------\n    func: a python function\n        The forward (loss) function.\n    argnum: an int or a list of int\n        The index of argument to calculate gradient for.\n\n    Returns\n    -------\n    grad_func: a python function\n        A function that would compute the gradient of arguments.\n\n    Examples\n    --------\n    >>> # autograd supports dynamic graph which is changed\n    >>> # every instance\n    >>> def func(x):\n    >>>     r = random.randint(0, 1)\n    >>>     if r % 2:\n    >>>         return x**2\n    >>>     else:\n    >>>         return x/3\n    >>> # use `grad(func)` to get the gradient function\n    >>> for x in range(10):\n    >>>     grad_func = grad(func)\n    >>>     inputs = nd.array([[1, 2, 3], [4, 5, 6]])\n    >>>     grad_vals = grad_func(inputs)\n    \"\"\"\n    grad_with_loss_func = grad_and_loss(func, argnum)\n    @functools.wraps(grad_with_loss_func)\n    def wrapped(*args):\n        return grad_with_loss_func(*args)[0]\n    return wrapped"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsplitting an NDArray into num_slice slices along batch_axis.", "response": "def split_data(data, num_slice, batch_axis=0, even_split=True):\n    \"\"\"Splits an NDArray into `num_slice` slices along `batch_axis`.\n    Usually used for data parallelism where each slices is sent\n    to one device (i.e. GPU).\n\n    Parameters\n    ----------\n    data : NDArray\n        A batch of data.\n    num_slice : int\n        Number of desired slices.\n    batch_axis : int, default 0\n        The axis along which to slice.\n    even_split : bool, default True\n        Whether to force all slices to have the same number of elements.\n        If `True`, an error will be raised when `num_slice` does not evenly\n        divide `data.shape[batch_axis]`.\n\n    Returns\n    -------\n    list of NDArray\n        Return value is a list even if `num_slice` is 1.\n    \"\"\"\n    size = data.shape[batch_axis]\n    if even_split and size % num_slice != 0:\n        raise ValueError(\n            \"data with shape %s cannot be evenly split into %d slices along axis %d. \" \\\n            \"Use a batch size that's multiple of %d or set even_split=False to allow \" \\\n            \"uneven partitioning of data.\"%(\n                str(data.shape), num_slice, batch_axis, num_slice))\n\n    step = size // num_slice\n\n    # If size < num_slice, make fewer slices\n    if not even_split and size < num_slice:\n        step = 1\n        num_slice = size\n\n    if batch_axis == 0:\n        slices = [data[i*step:(i+1)*step] if i < num_slice - 1 else data[i*step:size]\n                  for i in range(num_slice)]\n    elif even_split:\n        slices = ndarray.split(data, num_outputs=num_slice, axis=batch_axis)\n    else:\n        slices = [ndarray.slice_axis(data, batch_axis, i*step, (i+1)*step)\n                  if i < num_slice - 1 else\n                  ndarray.slice_axis(data, batch_axis, i*step, size)\n                  for i in range(num_slice)]\n    return slices"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsplit an NDArray into len ( ctx_list ) along batch_axis and loads each slice to one context in ctx_list.", "response": "def split_and_load(data, ctx_list, batch_axis=0, even_split=True):\n    \"\"\"Splits an NDArray into `len(ctx_list)` slices along `batch_axis` and loads\n    each slice to one context in `ctx_list`.\n\n    Parameters\n    ----------\n    data : NDArray\n        A batch of data.\n    ctx_list : list of Context\n        A list of Contexts.\n    batch_axis : int, default 0\n        The axis along which to slice.\n    even_split : bool, default True\n        Whether to force all slices to have the same number of elements.\n\n    Returns\n    -------\n    list of NDArray\n        Each corresponds to a context in `ctx_list`.\n    \"\"\"\n    if not isinstance(data, ndarray.NDArray):\n        data = ndarray.array(data, ctx=ctx_list[0])\n    if len(ctx_list) == 1:\n        return [data.as_in_context(ctx_list[0])]\n\n    slices = split_data(data, len(ctx_list), batch_axis, even_split)\n    return [i.as_in_context(ctx) for i, ctx in zip(slices, ctx_list)]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clip_global_norm(arrays, max_norm, check_isfinite=True):\n    def _norm(array):\n        if array.stype == 'default':\n            x = array.reshape((-1,))\n            return ndarray.dot(x, x)\n        return array.norm().square()\n    assert len(arrays) > 0\n    ctx = arrays[0].context\n    total_norm = ndarray.add_n(*[_norm(arr).as_in_context(ctx) for arr in arrays])\n    total_norm = ndarray.sqrt(total_norm)\n    if check_isfinite:\n        if not np.isfinite(total_norm.asscalar()):\n            warnings.warn(\n                UserWarning('nan or inf is detected. '\n                            'Clipping results will be undefined.'), stacklevel=2)\n    scale = max_norm / (total_norm + 1e-8)\n    scale = ndarray.min(ndarray.concat(scale, ndarray.ones(1, ctx=ctx), dim=0))\n    for arr in arrays:\n        arr *= scale.as_in_context(arr.context)\n    if check_isfinite:\n        return total_norm.asscalar()\n    else:\n        return total_norm", "response": "Rescales NDArrays so that the sum of their 2 - norm is smaller than max_norm."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nindenting string s_ by numSpaces", "response": "def _indent(s_, numSpaces):\n    \"\"\"Indent string\n    \"\"\"\n    s = s_.split('\\n')\n    if len(s) == 1:\n        return s_\n    first = s.pop(0)\n    s = [first] + [(numSpaces * ' ') + line for line in s]\n    s = '\\n'.join(s)\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_sha1(filename, sha1_hash):\n    sha1 = hashlib.sha1()\n    with open(filename, 'rb') as f:\n        while True:\n            data = f.read(1048576)\n            if not data:\n                break\n            sha1.update(data)\n\n    return sha1.hexdigest() == sha1_hash", "response": "Checks whether the sha1 hash of the file content matches the expected hash."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef download(url, path=None, overwrite=False, sha1_hash=None, retries=5, verify_ssl=True):\n    if path is None:\n        fname = url.split('/')[-1]\n        # Empty filenames are invalid\n        assert fname, 'Can\\'t construct file-name from this URL. ' \\\n            'Please set the `path` option manually.'\n    else:\n        path = os.path.expanduser(path)\n        if os.path.isdir(path):\n            fname = os.path.join(path, url.split('/')[-1])\n        else:\n            fname = path\n    assert retries >= 0, \"Number of retries should be at least 0, currently it's {}\".format(\n        retries)\n\n    if not verify_ssl:\n        warnings.warn(\n            'Unverified HTTPS request is being made (verify_ssl=False). '\n            'Adding certificate verification is strongly advised.')\n\n    if overwrite or not os.path.exists(fname) or (sha1_hash and not check_sha1(fname, sha1_hash)):\n        dirname = os.path.dirname(os.path.abspath(os.path.expanduser(fname)))\n        if not os.path.exists(dirname):\n            os.makedirs(dirname)\n        while retries + 1 > 0:\n            # Disable pyling too broad Exception\n            # pylint: disable=W0703\n            try:\n                print('Downloading {} from {}...'.format(fname, url))\n                r = requests.get(url, stream=True, verify=verify_ssl)\n                if r.status_code != 200:\n                    raise RuntimeError('Failed downloading url {}'.format(url))\n                # create uuid for temporary files\n                random_uuid = str(uuid.uuid4())\n                with open('{}.{}'.format(fname, random_uuid), 'wb') as f:\n                    for chunk in r.iter_content(chunk_size=1024):\n                        if chunk: # filter out keep-alive new chunks\n                            f.write(chunk)\n                # if the target file exists(created by other processes)\n                # and have the same hash with target file\n                # delete the temporary file\n                if not os.path.exists(fname) or (sha1_hash and not check_sha1(fname, sha1_hash)):\n                    # atmoic operation in the same file system\n                    _replace_atomic('{}.{}'.format(fname, random_uuid), fname)\n                else:\n                    try:\n                        os.remove('{}.{}'.format(fname, random_uuid))\n                    except OSError:\n                        pass\n                    finally:\n                        warnings.warn(\n                            'File {} exists in file system so the downloaded file is deleted'.format(fname))\n                if sha1_hash and not check_sha1(fname, sha1_hash):\n                    raise UserWarning(\n                        'File {} is downloaded but the content hash does not match.'\n                        ' The repo may be outdated or download may be incomplete. '\n                        'If the \"repo_url\" is overridden, consider switching to '\n                        'the default repo.'.format(fname))\n                break\n            except Exception as e:\n                retries -= 1\n                if retries <= 0:\n                    raise e\n                else:\n                    print('download failed due to {}, retrying, {} attempt{} left'\n                          .format(repr(e), retries, 's' if retries > 1 else ''))\n\n    return fname", "response": "Download an URL and store it in path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_repo_url():\n    default_repo = 'https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/'\n    repo_url = os.environ.get('MXNET_GLUON_REPO', default_repo)\n    if repo_url[-1] != '/':\n        repo_url = repo_url+'/'\n    return repo_url", "response": "Return the base URL for Gluon dataset and model repository."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_repo_file_url(namespace, filename):\n    return '{base_url}{namespace}/{filename}'.format(base_url=_get_repo_url(),\n                                                     namespace=namespace,\n                                                     filename=filename)", "response": "Returns the URL for hosted file in Gluon repository."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprints at most limit elements of list.", "response": "def _brief_print_list(lst, limit=7):\n    \"\"\"Print at most `limit` elements of list.\"\"\"\n    lst = list(lst)\n    if len(lst) > limit:\n        return _brief_print_list(lst[:limit//2], limit) + ', ..., ' + \\\n            _brief_print_list(lst[-limit//2:], limit)\n    return ', '.join([\"'%s'\"%str(i) for i in lst])"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a symbol function by handle and function name.", "response": "def _make_symbol_function(handle, name, func_name):\n    \"\"\"Create a symbol function by handle and function name.\"\"\"\n    code, doc_str = _generate_symbol_function_code(handle, name, func_name)\n\n    local = {}\n    exec(code, None, local)  # pylint: disable=exec-used\n    symbol_function = local[func_name]\n    symbol_function.__name__ = func_name\n    symbol_function.__doc__ = doc_str\n    symbol_function.__module__ = 'mxnet.symbol'\n    return symbol_function"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef batch_row_ids(data_batch):\n    item = data_batch.data[0]\n    user = data_batch.data[1]\n    return {'user_weight': user.astype(np.int64),\n            'item_weight': item.astype(np.int64)}", "response": "Generate row ids based on the current mini - batch"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef all_row_ids(data_batch):\n    all_users = mx.nd.arange(0, MOVIELENS['max_user'], dtype='int64')\n    all_movies = mx.nd.arange(0, MOVIELENS['max_movie'], dtype='int64')\n    return {'user_weight': all_users, 'item_weight': all_movies}", "response": "Generate row ids for all rows"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef convert_model(prototxt_fname, caffemodel_fname, output_prefix=None):\n    sym, input_dim = convert_symbol(prototxt_fname)\n    arg_shapes, _, aux_shapes = sym.infer_shape(data=tuple(input_dim))\n    arg_names = sym.list_arguments()\n    aux_names = sym.list_auxiliary_states()\n    arg_shape_dic = dict(zip(arg_names, arg_shapes))\n    aux_shape_dic = dict(zip(aux_names, aux_shapes))\n    arg_params = {}\n    aux_params = {}\n    first_conv = True\n\n    layers, names = caffe_parser.read_caffemodel(prototxt_fname, caffemodel_fname)\n    layer_iter = caffe_parser.layer_iter(layers, names)\n    layers_proto = caffe_parser.get_layers(caffe_parser.read_prototxt(prototxt_fname))\n\n    for layer_name, layer_type, layer_blobs in layer_iter:\n        if layer_type == 'Convolution' or layer_type == 'InnerProduct'  \\\n           or layer_type == 4 or layer_type == 14 or layer_type == 'PReLU' \\\n           or layer_type == 'Deconvolution' or layer_type == 39  or layer_type == 'Normalize':\n            if layer_type == 'PReLU':\n                assert (len(layer_blobs) == 1)\n                wmat = layer_blobs[0].data\n                weight_name = layer_name + '_gamma'\n                arg_params[weight_name] = mx.nd.zeros(wmat.shape)\n                arg_params[weight_name][:] = wmat\n                continue\n            if layer_type == 'Normalize':\n                assert (len(layer_blobs) == 1)\n                weight_name = layer_name + '_scale'\n                wmat = layer_blobs[0].data\n                arg_params[weight_name] = mx.nd.zeros((1, len(wmat), 1, 1))\n                arg_params[weight_name][:] = np.array(list(wmat)).reshape((1, len(wmat), 1, 1))\n                continue\n            wmat_dim = []\n            if getattr(layer_blobs[0].shape, 'dim', None) is not None:\n                if len(layer_blobs[0].shape.dim) > 0:\n                    wmat_dim = layer_blobs[0].shape.dim\n                else:\n                    wmat_dim = [layer_blobs[0].num, layer_blobs[0].channels,\n                                layer_blobs[0].height, layer_blobs[0].width]\n            else:\n                wmat_dim = list(layer_blobs[0].shape)\n            wmat = np.array(layer_blobs[0].data).reshape(wmat_dim)\n\n            channels = wmat_dim[1]\n            if channels == 3 or channels == 4:  # RGB or RGBA\n                if first_conv:\n                    # Swapping BGR of caffe into RGB in mxnet\n                    wmat[:, [0, 2], :, :] = wmat[:, [2, 0], :, :]\n\n            assert(wmat.flags['C_CONTIGUOUS'] is True)\n            sys.stdout.write('converting layer {0}, wmat shape = {1}'.format(\n                layer_name, wmat.shape))\n            if len(layer_blobs) == 2:\n                bias = np.array(layer_blobs[1].data)\n                bias = bias.reshape((bias.shape[0], 1))\n                assert(bias.flags['C_CONTIGUOUS'] is True)\n                bias_name = layer_name + \"_bias\"\n\n                if bias_name not in arg_shape_dic:\n                    print(bias_name + ' not found in arg_shape_dic.')\n                    continue\n                bias = bias.reshape(arg_shape_dic[bias_name])\n                arg_params[bias_name] = mx.nd.zeros(bias.shape)\n                arg_params[bias_name][:] = bias\n                sys.stdout.write(', bias shape = {}'.format(bias.shape))\n\n            sys.stdout.write('\\n')\n            sys.stdout.flush()\n            wmat = wmat.reshape((wmat.shape[0], -1))\n            weight_name = layer_name + \"_weight\"\n\n            if weight_name not in arg_shape_dic:\n                print(weight_name + ' not found in arg_shape_dic.')\n                continue\n            wmat = wmat.reshape(arg_shape_dic[weight_name])\n            arg_params[weight_name] = mx.nd.zeros(wmat.shape)\n            arg_params[weight_name][:] = wmat\n\n\n            if first_conv and (layer_type == 'Convolution' or layer_type == 4):\n                first_conv = False\n\n        elif layer_type == 'Scale':\n            if 'scale' in layer_name:\n                bn_name = layer_name.replace('scale', 'bn')\n            elif 'sc' in layer_name:\n                bn_name = layer_name.replace('sc', 'bn')\n            else:\n                assert False, 'Unknown name convention for bn/scale'\n\n            gamma = np.array(layer_blobs[0].data)\n            beta = np.array(layer_blobs[1].data)\n            # beta = np.expand_dims(beta, 1)\n            beta_name = '{}_beta'.format(bn_name)\n            gamma_name = '{}_gamma'.format(bn_name)\n\n            beta = beta.reshape(arg_shape_dic[beta_name])\n            gamma = gamma.reshape(arg_shape_dic[gamma_name])\n            arg_params[beta_name] = mx.nd.zeros(beta.shape)\n            arg_params[gamma_name] = mx.nd.zeros(gamma.shape)\n            arg_params[beta_name][:] = beta\n            arg_params[gamma_name][:] = gamma\n\n            assert gamma.flags['C_CONTIGUOUS'] is True\n            assert beta.flags['C_CONTIGUOUS'] is True\n            print('converting scale layer, beta shape = {}, gamma shape = {}'.format(\n                beta.shape, gamma.shape))\n        elif layer_type == 'BatchNorm':\n            bn_name = layer_name\n            mean = np.array(layer_blobs[0].data)\n            var = np.array(layer_blobs[1].data)\n            rescale_factor = layer_blobs[2].data[0]\n            if rescale_factor != 0:\n                rescale_factor = 1 / rescale_factor\n            mean_name = '{}_moving_mean'.format(bn_name)\n            var_name = '{}_moving_var'.format(bn_name)\n            mean = mean.reshape(aux_shape_dic[mean_name])\n            var = var.reshape(aux_shape_dic[var_name])\n            aux_params[mean_name] = mx.nd.zeros(mean.shape)\n            aux_params[var_name] = mx.nd.zeros(var.shape)\n            # Get the original epsilon\n            for idx, layer in enumerate(layers_proto):\n                if layer.name == bn_name:\n                    bn_index = idx\n            eps_caffe = layers_proto[bn_index].batch_norm_param.eps\n            # Compensate for the epsilon shift performed in convert_symbol\n            eps_symbol = float(sym.attr_dict()[bn_name + '_moving_mean']['eps'])\n            eps_correction = eps_caffe - eps_symbol\n            # Fill parameters\n            aux_params[mean_name][:] = mean * rescale_factor\n            aux_params[var_name][:] = var * rescale_factor + eps_correction\n            assert var.flags['C_CONTIGUOUS'] is True\n            assert mean.flags['C_CONTIGUOUS'] is True\n            print('converting batchnorm layer, mean shape = {}, var shape = {}'.format(\n                mean.shape, var.shape))\n\n            fix_gamma = layers_proto[bn_index+1].type != 'Scale'\n            if fix_gamma:\n                gamma_name = '{}_gamma'.format(bn_name)\n                gamma = np.array(np.ones(arg_shape_dic[gamma_name]))\n                beta_name = '{}_beta'.format(bn_name)\n                beta = np.array(np.zeros(arg_shape_dic[beta_name]))\n                arg_params[beta_name] = mx.nd.zeros(beta.shape)\n                arg_params[gamma_name] = mx.nd.zeros(gamma.shape)\n                arg_params[beta_name][:] = beta\n                arg_params[gamma_name][:] = gamma\n                assert gamma.flags['C_CONTIGUOUS'] is True\n                assert beta.flags['C_CONTIGUOUS'] is True\n\n        else:\n            print('\\tskipping layer {} of type {}'.format(layer_name, layer_type))\n            assert len(layer_blobs) == 0\n\n    if output_prefix is not None:\n        model = mx.mod.Module(symbol=sym, label_names=None)\n        model.bind(data_shapes=[('data', tuple(input_dim))])\n        model.init_params(arg_params=arg_params, aux_params=aux_params)\n        model.save_checkpoint(output_prefix, 0)\n\n    return sym, arg_params, aux_params, input_dim", "response": "Convert a binary caffe model into a single MXNet structure."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _parse_proto(prototxt_fname):\n    proto = caffe_parser.read_prototxt(prototxt_fname)\n\n    # process data layer\n    input_name, input_dim, layers = _get_input(proto)\n    # only support single input, so always use `data` as the input data\n    mapping = {input_name: 'data'}\n    need_flatten = {input_name: False}\n    symbol_string = \"import mxnet as mx\\ndata = mx.symbol.Variable(name='data')\\n\"\n\n    flatten_count = 0\n    output_name = \"\"\n    prev_name = None\n    _output_name = {}\n\n    # convert reset layers one by one\n    for i, layer in enumerate(layers):\n        type_string = ''\n        param_string = ''\n        skip_layer = False\n        name = re.sub('[-/]', '_', layer.name)\n        for k in range(len(layer.bottom)):\n            if layer.bottom[k] in _output_name:\n                _output_name[layer.bottom[k]]['count'] = _output_name[layer.bottom[k]]['count']+1\n            else:\n                _output_name[layer.bottom[k]] = {'count':0}\n        for k in range(len(layer.top)):\n            if layer.top[k] in _output_name:\n                _output_name[layer.top[k]]['count'] = _output_name[layer.top[k]]['count']+1\n            else:\n                _output_name[layer.top[k]] = {'count':0, 'name':name}\n        if layer.type == 'Convolution' or layer.type == 4:\n            type_string = 'mx.symbol.Convolution'\n            param_string = _convert_conv_param(layer.convolution_param)\n            need_flatten[name] = True\n        if layer.type == 'Deconvolution' or layer.type == 39:\n            type_string = 'mx.symbol.Deconvolution'\n            param_string = _convert_conv_param(layer.convolution_param)\n            need_flatten[name] = True\n        if layer.type == 'Pooling' or layer.type == 17:\n            type_string = 'mx.symbol.Pooling'\n            param_string = _convert_pooling_param(layer.pooling_param)\n            need_flatten[name] = True\n        if layer.type == 'ReLU' or layer.type == 18:\n            type_string = 'mx.symbol.Activation'\n            param_string = \"act_type='relu'\"\n            param = layer.relu_param\n            if hasattr(param, 'negative_slope'):\n                if param.negative_slope > 0:\n                    type_string = 'mx.symbol.LeakyReLU'\n                    param_string = \"act_type='leaky', slope=%f\" % param.negative_slope\n            need_flatten[name] = need_flatten[mapping[layer.bottom[0]]]\n        if layer.type == 'TanH' or layer.type == 23:\n            type_string = 'mx.symbol.Activation'\n            param_string = \"act_type='tanh'\"\n            need_flatten[name] = need_flatten[mapping[layer.bottom[0]]]\n        if layer.type == 'Sigmoid' or layer.type == 19:\n            type_string = 'mx.symbol.Activation'\n            param_string = \"act_type='sigmoid'\"\n            need_flatten[name] = need_flatten[mapping[layer.bottom[0]]]\n        if layer.type == 'LRN' or layer.type == 15:\n            type_string = 'mx.symbol.LRN'\n            param = layer.lrn_param\n            param_string = \"alpha=%f, beta=%f, knorm=%f, nsize=%d\" % (\n                param.alpha, param.beta, param.k, param.local_size)\n            need_flatten[name] = True\n        if layer.type == 'InnerProduct' or layer.type == 14:\n            type_string = 'mx.symbol.FullyConnected'\n            param = layer.inner_product_param\n            param_string = \"num_hidden=%d, no_bias=%s\" % (\n                param.num_output, not param.bias_term)\n            need_flatten[name] = False\n        if layer.type == 'Dropout' or layer.type == 6:\n            type_string = 'mx.symbol.Dropout'\n            param = layer.dropout_param\n            param_string = \"p=%f\" % param.dropout_ratio\n            need_flatten[name] = need_flatten[mapping[layer.bottom[0]]]\n        if layer.type == 'Softmax' or layer.type == 20:\n            type_string = 'mx.symbol.SoftmaxOutput'\n        if layer.type == 'Flatten' or layer.type == 8:\n            type_string = 'mx.symbol.Flatten'\n            need_flatten[name] = False\n        if layer.type == 'Split' or layer.type == 22:\n            type_string = 'split'  # will process later\n        if layer.type == 'Concat' or layer.type == 3:\n            type_string = 'mx.symbol.Concat'\n            need_flatten[name] = True\n        if layer.type == 'Crop':\n            type_string = 'mx.symbol.Crop'\n            need_flatten[name] = True\n            param_string = 'center_crop=True'\n        if layer.type == 'BatchNorm':\n            type_string = 'mx.symbol.BatchNorm'\n            param = layer.batch_norm_param\n            # CuDNN requires eps to be greater than 1e-05\n            # We compensate for this change in convert_model\n            epsilon = param.eps\n            if (epsilon <= 1e-05):\n                epsilon = 1e-04\n            # if next layer is scale, don't fix gamma\n            fix_gamma = layers[i+1].type != 'Scale'\n            param_string = 'use_global_stats=%s, fix_gamma=%s, eps=%f' % (\n                param.use_global_stats, fix_gamma, epsilon)\n            need_flatten[name] = need_flatten[mapping[layer.bottom[0]]]\n        if layer.type == 'Scale':\n            assert layers[i-1].type == 'BatchNorm'\n            need_flatten[name] = need_flatten[mapping[layer.bottom[0]]]\n            skip_layer = True\n            prev_name = re.sub('[-/]', '_', layers[i-1].name)\n        if layer.type == 'PReLU':\n            type_string = 'mx.symbol.LeakyReLU'\n            param = layer.prelu_param\n            param_string = \"act_type='prelu', slope=%f\" % param.filler.value\n            need_flatten[name] = need_flatten[mapping[layer.bottom[0]]]\n        if layer.type == 'Eltwise':\n            type_string = 'mx.symbol.broadcast_add'\n            param = layer.eltwise_param\n            param_string = \"\"\n            need_flatten[name] = False\n        if layer.type == 'Reshape':\n            type_string = 'mx.symbol.Reshape'\n            need_flatten[name] = False\n            param = layer.reshape_param\n            param_string = \"shape=(%s)\" % (','.join(param.shape.dim),)\n        if layer.type == 'AbsVal':\n            type_string = 'mx.symbol.abs'\n            need_flatten[name] = need_flatten[mapping[layer.bottom[0]]]\n\n        if skip_layer:\n            assert len(layer.bottom) == 1\n            symbol_string += \"%s = %s\\n\" % (name, prev_name)\n        elif type_string == '':\n            raise ValueError('Unknown layer %s!' % layer.type)\n        elif type_string != 'split':\n            bottom = layer.bottom\n            if param_string != \"\":\n                param_string = \", \" + param_string\n            if len(bottom) == 1:\n                if need_flatten[mapping[bottom[0]]] and type_string == 'mx.symbol.FullyConnected':\n                    flatten_name = \"flatten_%d\" % flatten_count\n                    symbol_string += \"%s=mx.symbol.Flatten(name='%s', data=%s)\\n\" % (\n                        flatten_name, flatten_name, mapping[bottom[0]])\n                    flatten_count += 1\n                    need_flatten[flatten_name] = False\n                    bottom[0] = flatten_name\n                    mapping[bottom[0]] = bottom[0]\n                symbol_string += \"%s = %s(name='%s', data=%s %s)\\n\" % (\n                    name, type_string, name, mapping[bottom[0]], param_string)\n            else:\n                if layer.type == 'Eltwise' and param.operation == 1 and len(param.coeff) > 0:\n                    symbol_string += \"%s = \" % name\n                    symbol_string += \" + \".join([\"%s * %s\" % (\n                        mapping[bottom[i]], param.coeff[i]) for i in range(len(param.coeff))])\n                    symbol_string += \"\\n\"\n                else:\n                    symbol_string += \"%s = %s(name='%s', *[%s] %s)\\n\" % (\n                        name, type_string, name, ','.join(\n                            [mapping[x] for x in bottom]), param_string)\n        for j in range(len(layer.top)):\n            mapping[layer.top[j]] = name\n        output_name = name\n    output_name = []\n    for i in _output_name:\n        if 'name' in _output_name[i] and _output_name[i]['count'] == 0:\n            output_name.append(_output_name[i]['name'])\n\n    return symbol_string, output_name, input_dim", "response": "Parse Caffe prototxt into symbol string"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef convert_symbol(prototxt_fname):\n    sym, output_name, input_dim = _parse_proto(prototxt_fname)\n    exec(sym)                   # pylint: disable=exec-used\n    _locals = locals()\n    ret = []\n    for i in  output_name:\n        exec(\"ret = \" + i, globals(), _locals)  # pylint: disable=exec-used\n        ret.append(_locals['ret'])\n    ret = mx.sym.Group(ret)\n    return ret, input_dim", "response": "Convert caffe model definition into Symbol\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_with_uniform(uf, arg_shapes, dim=None, npuf=None, rmin=-10, type_list=[np.float32]):\n    if isinstance(arg_shapes, int):\n        assert dim\n        shape = tuple(np.random.randint(1, int(1000**(1.0/dim)), size=dim))\n        arg_shapes = [shape] * arg_shapes\n    for dtype in type_list:\n        ndarray_arg = []\n        numpy_arg = []\n        for s in arg_shapes:\n            npy = np.random.uniform(rmin, 10, s).astype(dtype)\n            narr = mx.nd.array(npy, dtype=dtype)\n            ndarray_arg.append(narr)\n            numpy_arg.append(npy)\n        out1 = uf(*ndarray_arg)\n        if npuf is None:\n            out2 = uf(*numpy_arg).astype(dtype)\n        else:\n            out2 = npuf(*numpy_arg).astype(dtype)\n\n        assert out1.shape == out2.shape\n        if isinstance(out1, mx.nd.NDArray):\n            out1 = out1.asnumpy()\n        if dtype == np.float16:\n            assert reldiff(out1, out2) < 2e-3\n        else:\n            assert reldiff(out1, out2) < 1e-6", "response": "check function consistency with uniform random numbers"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nremoves images without usable rois", "response": "def filter_roidb(self):\n        \"\"\"Remove images without usable rois\"\"\"\n        num_roidb = len(self._roidb)\n        self._roidb = [roi_rec for roi_rec in self._roidb if len(roi_rec['gt_classes'])]\n        num_after = len(self._roidb)\n        logger.info('filter roidb: {} -> {}'.format(num_roidb, num_after))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef append_flipped_images(self):\n        logger.info('%s append flipped images to roidb' % self._name)\n        roidb_flipped = []\n        for roi_rec in self._roidb:\n            boxes = roi_rec['boxes'].copy()\n            oldx1 = boxes[:, 0].copy()\n            oldx2 = boxes[:, 2].copy()\n            boxes[:, 0] = roi_rec['width'] - oldx2 - 1\n            boxes[:, 2] = roi_rec['width'] - oldx1 - 1\n            assert (boxes[:, 2] >= boxes[:, 0]).all()\n            roi_rec_flipped = roi_rec.copy()\n            roi_rec_flipped['boxes'] = boxes\n            roi_rec_flipped['flipped'] = True\n            roidb_flipped.append(roi_rec_flipped)\n        self._roidb.extend(roidb_flipped)", "response": "Only flip boxes coordinates images will be flipped when loading into network"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef image_path_from_index(self, index):\n        assert self.image_set_index is not None, \"Dataset not initialized\"\n        name = self.image_set_index[index]\n        image_file = os.path.join(self.image_dir, 'images', name)\n        assert os.path.isfile(image_file), 'Path does not exist: {}'.format(image_file)\n        return image_file", "response": "find out full path of the image file given the index"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _load_all(self, anno_file, shuffle):\n        image_set_index = []\n        labels = []\n        coco = COCO(anno_file)\n        img_ids = coco.getImgIds()\n        # deal with class names\n        cats = [cat['name'] for cat in coco.loadCats(coco.getCatIds())]\n        class_to_coco_ind = dict(zip(cats, coco.getCatIds()))\n        class_to_ind = dict(zip(self.classes, range(len(self.classes))))\n        coco_ind_to_class_ind = dict([(class_to_coco_ind[cls], class_to_ind[cls])\n                                     for cls in self.classes[0:]])\n        for img_id in img_ids:\n            # filename\n            image_info = coco.loadImgs(img_id)[0]\n            filename = image_info[\"file_name\"]\n            subdir = filename.split('_')[1]\n            height = image_info[\"height\"]\n            width = image_info[\"width\"]\n            # label\n            anno_ids = coco.getAnnIds(imgIds=img_id)\n            annos = coco.loadAnns(anno_ids)\n            label = []\n            for anno in annos:\n                cat_id = coco_ind_to_class_ind[anno['category_id']]\n                bbox = anno[\"bbox\"]\n                assert len(bbox) == 4\n                xmin = float(bbox[0]) / width\n                ymin = float(bbox[1]) / height\n                xmax = xmin + float(bbox[2]) / width\n                ymax = ymin + float(bbox[3]) / height\n                label.append([cat_id, xmin, ymin, xmax, ymax, 0])\n            if label:\n                labels.append(np.array(label))\n                image_set_index.append(os.path.join(subdir, filename))\n\n        if shuffle:\n            import random\n            indices = list(range(len(image_set_index)))\n            random.shuffle(indices)\n            image_set_index = [image_set_index[i] for i in indices]\n            labels = [labels[i] for i in indices]\n        # store the results\n        self.image_set_index = image_set_index\n        self.labels = labels", "response": "Initialize all entries given annotation json file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninitializing the parameters and auxiliary states.", "response": "def init_params(self, initializer=mx.init.Uniform(0.01), **kwargs):\n        \"\"\"Initializes the parameters and auxiliary states.\n        \"\"\"\n        self._module.init_params(initializer=initializer, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef forward(self, data_batch, is_train=None, carry_state=True):\n        # propagate states from the previous iteration\n        if carry_state:\n            if isinstance(self._next_states, (int, float)):\n                self._module.set_states(value=self._next_states)\n            else:\n                self._module.set_states(states=self._next_states)\n        self._module.forward(data_batch, is_train=is_train)\n        outputs = self._module.get_outputs(merge_multi_context=False)\n        self._next_states = outputs[:-1]", "response": "Forward computation. States from previous forward computation are carried\n        to the current iteration if `carry_state` is set to `True`."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update(self, max_norm=None):\n        if max_norm is not None:\n            self._clip_by_global_norm(max_norm)\n        self._module.update()", "response": "Updates the parameters according to the installed optimizer and the gradients computed\n            in the previous forward - backward batch."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _clip_by_global_norm(self, max_norm):\n        assert self._module.binded and self._module.params_initialized \\\n               and self._module.optimizer_initialized\n        grad_array = []\n        for grad in self._module._exec_group.grad_arrays:\n            grad_array += grad\n        return mx.gluon.utils.clip_global_norm(grad_array, max_norm)", "response": "Clips gradient norm by max_norm."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef visual(title, X, name):\n    assert len(X.shape) == 4\n    X = X.transpose((0, 2, 3, 1))\n    X = np.clip((X - np.min(X))*(255.0/(np.max(X) - np.min(X))), 0, 255).astype(np.uint8)\n    n = np.ceil(np.sqrt(X.shape[0]))\n    buff = np.zeros((int(n*X.shape[1]), int(n*X.shape[2]), int(X.shape[3])), dtype=np.uint8)\n    for i, img in enumerate(X):\n        fill_buf(buff, i, img, X.shape[1:3])\n    buff = buff[:, :, ::-1]\n    plt.imshow(buff)\n    plt.title(title)\n    plt.savefig(name)", "response": "Image visualization and preservation\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef transformer(data, label):\n    # resize to 64x64\n    data = mx.image.imresize(data, 64, 64)\n    # transpose from (64, 64, 3) to (3, 64, 64)\n    data = mx.nd.transpose(data, (2, 0, 1))\n    # normalize to [-1, 1]\n    data = data.astype(np.float32)/128 - 1\n    # if image is greyscale, repeat 3 times to get RGB image.\n    if data.shape[0] == 1:\n        data = mx.nd.tile(data, (3, 1, 1))\n    return data, label", "response": "Get the translation of images"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_dataset(dataset_name):\n    # mnist\n    if dataset == \"mnist\":\n        train_data = gluon.data.DataLoader(\n            gluon.data.vision.MNIST('./data', train=True, transform=transformer),\n            batch_size, shuffle=True, last_batch='discard')\n\n        val_data = gluon.data.DataLoader(\n            gluon.data.vision.MNIST('./data', train=False, transform=transformer),\n            batch_size, shuffle=False)\n    # cifar10\n    elif dataset == \"cifar10\":\n        train_data = gluon.data.DataLoader(\n            gluon.data.vision.CIFAR10('./data', train=True, transform=transformer),\n            batch_size, shuffle=True, last_batch='discard')\n\n        val_data = gluon.data.DataLoader(\n            gluon.data.vision.CIFAR10('./data', train=False, transform=transformer),\n            batch_size, shuffle=False)\n\n    return train_data, val_data", "response": "Load the dataset and split it to train and valid data\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_netG():\n    # build the generator\n    netG = nn.Sequential()\n    with netG.name_scope():\n        # input is Z, going into a convolution\n        netG.add(nn.Conv2DTranspose(ngf * 8, 4, 1, 0, use_bias=False))\n        netG.add(nn.BatchNorm())\n        netG.add(nn.Activation('relu'))\n        # state size. (ngf*8) x 4 x 4\n        netG.add(nn.Conv2DTranspose(ngf * 4, 4, 2, 1, use_bias=False))\n        netG.add(nn.BatchNorm())\n        netG.add(nn.Activation('relu'))\n        # state size. (ngf*4) x 8 x 8\n        netG.add(nn.Conv2DTranspose(ngf * 2, 4, 2, 1, use_bias=False))\n        netG.add(nn.BatchNorm())\n        netG.add(nn.Activation('relu'))\n        # state size. (ngf*2) x 16 x 16\n        netG.add(nn.Conv2DTranspose(ngf, 4, 2, 1, use_bias=False))\n        netG.add(nn.BatchNorm())\n        netG.add(nn.Activation('relu'))\n        # state size. (ngf) x 32 x 32\n        netG.add(nn.Conv2DTranspose(nc, 4, 2, 1, use_bias=False))\n        netG.add(nn.Activation('tanh'))\n        # state size. (nc) x 64 x 64\n\n    return netG", "response": "Get the net G for the current version of the current version of the current version of the current version of the current version of the current version of the current version of the version."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the netD for the current version of the current version of the current version of the current version of the current version of the current version of the current version of the current version of the current version of the version.", "response": "def get_netD():\n    \"\"\"Get the netD\"\"\"\n    # build the discriminator\n    netD = nn.Sequential()\n    with netD.name_scope():\n        # input is (nc) x 64 x 64\n        netD.add(nn.Conv2D(ndf, 4, 2, 1, use_bias=False))\n        netD.add(nn.LeakyReLU(0.2))\n        # state size. (ndf) x 32 x 32\n        netD.add(nn.Conv2D(ndf * 2, 4, 2, 1, use_bias=False))\n        netD.add(nn.BatchNorm())\n        netD.add(nn.LeakyReLU(0.2))\n        # state size. (ndf*2) x 16 x 16\n        netD.add(nn.Conv2D(ndf * 4, 4, 2, 1, use_bias=False))\n        netD.add(nn.BatchNorm())\n        netD.add(nn.LeakyReLU(0.2))\n        # state size. (ndf*4) x 8 x 8\n        netD.add(nn.Conv2D(ndf * 8, 4, 2, 1, use_bias=False))\n        netD.add(nn.BatchNorm())\n        netD.add(nn.LeakyReLU(0.2))\n        # state size. (ndf*8) x 4 x 4\n        netD.add(nn.Conv2D(2, 4, 1, 0, use_bias=False))\n        # state size. 2 x 1 x 1\n\n    return netD"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_configurations(netG, netD):\n    # loss\n    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n\n    # initialize the generator and the discriminator\n    netG.initialize(mx.init.Normal(0.02), ctx=ctx)\n    netD.initialize(mx.init.Normal(0.02), ctx=ctx)\n\n    # trainer for the generator and the discriminator\n    trainerG = gluon.Trainer(netG.collect_params(), 'adam', {'learning_rate': opt.lr, 'beta1': opt.beta1})\n    trainerD = gluon.Trainer(netD.collect_params(), 'adam', {'learning_rate': opt.lr, 'beta1': opt.beta1})\n\n    return loss, trainerG, trainerD", "response": "Get configurations for the net"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getLogger(name=None, filename=None, filemode=None, level=WARNING):\n    warnings.warn(\"getLogger is deprecated, Use get_logger instead.\",\n                  DeprecationWarning, stacklevel=2)\n    return get_logger(name, filename, filemode, level)", "response": "Gets a customized logger."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a customized logger object.", "response": "def get_logger(name=None, filename=None, filemode=None, level=WARNING):\n    \"\"\"Gets a customized logger.\n\n    Parameters\n    ----------\n    name: str, optional\n        Name of the logger.\n    filename: str, optional\n        The filename to which the logger's output will be sent.\n    filemode: str, optional\n        The file mode to open the file (corresponding to `filename`),\n        default is 'a' if `filename` is not ``None``.\n    level: int, optional\n        The `logging` level for the logger.\n        See: https://docs.python.org/2/library/logging.html#logging-levels\n\n    Returns\n    -------\n    Logger\n        A customized `Logger` object.\n\n    Example\n    -------\n    ## get_logger call with default parameters.\n    >>> from mxnet.log import get_logger\n    >>> logger = get_logger(\"Test\")\n    >>> logger.warn(\"Hello World\")\n    W0505 00:29:47 3525 <stdin>:<module>:1] Hello World\n\n    ## get_logger call with WARNING level.\n    >>> import logging\n    >>> logger = get_logger(\"Test2\", level=logging.WARNING)\n    >>> logger.warn(\"Hello World\")\n    W0505 00:30:50 3525 <stdin>:<module>:1] Hello World\n    >>> logger.debug(\"Hello World\") # This doesn't return anything as the level is logging.WARNING.\n\n    ## get_logger call with DEBUG level.\n    >>> logger = get_logger(\"Test3\", level=logging.DEBUG)\n    >>> logger.debug(\"Hello World\") # Logs the debug output as the level is logging.DEBUG.\n    D0505 00:31:30 3525 <stdin>:<module>:1] Hello World\n    \"\"\"\n    logger = logging.getLogger(name)\n    if name is not None and not getattr(logger, '_init_done', None):\n        logger._init_done = True\n        if filename:\n            mode = filemode if filemode else 'a'\n            hdlr = logging.FileHandler(filename, mode)\n        else:\n            hdlr = logging.StreamHandler() # pylint: disable=redefined-variable-type\n            # the `_Formatter` contain some escape character to\n            # represent color, which is not suitable for FileHandler,\n            # (TODO) maybe we can add another Formatter for FileHandler.\n            hdlr.setFormatter(_Formatter())\n        logger.addHandler(hdlr)\n        logger.setLevel(level)\n    return logger"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _random_helper(random, sampler, params, shape, dtype, kwargs):\n    if isinstance(params[0], Symbol):\n        for i in params[1:]:\n            assert isinstance(i, Symbol), \\\n                \"Distribution parameters must all have the same type, but got \" \\\n                \"both %s and %s.\"%(type(params[0]), type(i))\n        return sampler(*params, shape=shape, dtype=dtype, **kwargs)\n    elif isinstance(params[0], numeric_types):\n        for i in params[1:]:\n            assert isinstance(i, numeric_types), \\\n                \"Distribution parameters must all have the same type, but got \" \\\n                \"both %s and %s.\"%(type(params[0]), type(i))\n        return random(*params, shape=shape, dtype=dtype, **kwargs)\n\n    raise ValueError(\"Distribution parameters must be either Symbol or numbers, \"\n                     \"but got %s.\"%type(params[0]))", "response": "Helper function for random generators."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef poisson(lam=1, shape=_Null, dtype=_Null, **kwargs):\n    return _random_helper(_internal._random_poisson, _internal._sample_poisson,\n                          [lam], shape, dtype, kwargs)", "response": "Draw random samples from a Poisson distribution."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndraws random samples from a generalized negative binomial distribution.", "response": "def generalized_negative_binomial(mu=1, alpha=1, shape=_Null, dtype=_Null, **kwargs):\n    \"\"\"Draw random samples from a generalized negative binomial distribution.\n\n    Samples are distributed according to a generalized negative binomial\n    distribution parametrized by *mu* (mean) and *alpha* (dispersion).\n    *alpha* is defined as *1/k* where *k* is the failure limit of the\n    number of unsuccessful experiments (generalized to real numbers).\n    Samples will always be returned as a floating point data type.\n\n    Parameters\n    ----------\n    mu : float or Symbol, optional\n        Mean of the negative binomial distribution.\n    alpha : float or Symbol, optional\n        Alpha (dispersion) parameter of the negative binomial distribution.\n    shape : int or tuple of ints, optional\n        The number of samples to draw. If shape is, e.g., `(m, n)` and `mu` and\n        `alpha` are scalars, output shape will be `(m, n)`. If `mu` and `alpha`\n        are Symbols with shape, e.g., `(x, y)`, then output will have shape\n        `(x, y, m, n)`, where `m*n` samples are drawn for each `[mu, alpha)` pair.\n    dtype : {'float16', 'float32', 'float64'}, optional\n        Data type of output samples. Default is 'float32'\n\n    Returns\n    -------\n    Symbol\n        If input `shape` has dimensions, e.g., `(m, n)`, and `mu` and\n        `alpha` are scalars, returned Symbol will resolve to shape `(m, n)`. If `mu`\n        and `alpha` are Symbols with shape, e.g., `(x, y)`, returned Symbol will resolve\n        to shape `(x, y, m, n)`, where `m*n` samples are drawn for each `[mu, alpha)` pair.\n    \"\"\"\n    return _random_helper(_internal._random_generalized_negative_binomial,\n                          _internal._sample_generalized_negative_binomial,\n                          [mu, alpha], shape, dtype, kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a single - shot multi - box detection network with VGG 16 layers replaced by conv layers", "response": "def get_symbol(num_classes=20, nms_thresh=0.5, force_suppress=False,\n               nms_topk=400, **kwargs):\n    \"\"\"\n    Single-shot multi-box detection with VGG 16 layers ConvNet\n    This is a modified version, with fc6/fc7 layers replaced by conv layers\n    And the network is slightly smaller than original VGG 16 network\n    This is the detection network\n\n    Parameters:\n    ----------\n    num_classes: int\n        number of object classes not including background\n    nms_thresh : float\n        threshold of overlap for non-maximum suppression\n    force_suppress : boolean\n        whether suppress different class objects\n    nms_topk : int\n        apply NMS to top K detections\n\n    Returns:\n    ----------\n    mx.Symbol\n    \"\"\"\n    net = get_symbol_train(num_classes)\n    cls_preds = net.get_internals()[\"multibox_cls_pred_output\"]\n    loc_preds = net.get_internals()[\"multibox_loc_pred_output\"]\n    anchor_boxes = net.get_internals()[\"multibox_anchors_output\"]\n\n    cls_prob = mx.symbol.softmax(data=cls_preds, axis=1, name='cls_prob')\n    out = mx.symbol.contrib.MultiBoxDetection(*[cls_prob, loc_preds, anchor_boxes], \\\n        name=\"detection\", nms_threshold=nms_thresh, force_suppress=force_suppress,\n        variances=(0.1, 0.1, 0.2, 0.2), nms_topk=nms_topk)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load(prefix, epoch, load_optimizer_states=False, **kwargs):\n        sym, args, auxs = load_checkpoint(prefix, epoch)\n        mod = Module(symbol=sym, **kwargs)\n        mod._arg_params = args\n        mod._aux_params = auxs\n        mod.params_initialized = True\n        if load_optimizer_states:\n            mod._preload_opt_states = '%s-%04d.states'%(prefix, epoch)\n        return mod", "response": "Creates a new model from a previously saved checkpoint."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save_checkpoint(self, prefix, epoch, save_optimizer_states=False):\n        self._symbol.save('%s-symbol.json'%prefix)\n        param_name = '%s-%04d.params' % (prefix, epoch)\n        self.save_params(param_name)\n        logging.info('Saved checkpoint to \\\"%s\\\"', param_name)\n        if save_optimizer_states:\n            state_name = '%s-%04d.states' % (prefix, epoch)\n            self.save_optimizer_states(state_name)\n            logging.info('Saved optimizer state to \\\"%s\\\"', state_name)", "response": "Saves current checkpoint to checkpoint."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _reset_bind(self):\n        self.binded = False\n        self._exec_group = None\n        self._data_shapes = None\n        self._label_shapes = None", "response": "Internal function to reset binded state."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the current parameters.", "response": "def get_params(self):\n        \"\"\"Gets current parameters.\n\n        Returns\n        -------\n        `(arg_params, aux_params)`\n            A pair of dictionaries each mapping parameter names to NDArray values.\n        \"\"\"\n        assert self.binded and self.params_initialized\n\n        if self._params_dirty:\n            self._sync_params_from_devices()\n        return (self._arg_params, self._aux_params)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef init_params(self, initializer=Uniform(0.01), arg_params=None, aux_params=None,\n                    allow_missing=False, force_init=False, allow_extra=False):\n        \"\"\"Initializes the parameters and auxiliary states.\n\n        Parameters\n        ----------\n        initializer : Initializer\n            Called to initialize parameters if needed.\n        arg_params : dict\n            If not ``None``, should be a dictionary of existing arg_params. Initialization\n            will be copied from that.\n        aux_params : dict\n            If not ``None``, should be a dictionary of existing aux_params. Initialization\n            will be copied from that.\n        allow_missing : bool\n            If ``True``, params could contain missing values, and the initializer will be\n            called to fill those missing params.\n        force_init : bool\n            If ``True``, will force re-initialize even if already initialized.\n        allow_extra : boolean, optional\n            Whether allow extra parameters that are not needed by symbol.\n            If this is True, no error will be thrown when arg_params or aux_params\n            contain extra parameters that is not needed by the executor.\n        \"\"\"\n        if self.params_initialized and not force_init:\n            warnings.warn(\"Parameters already initialized and force_init=False. \"\n                          \"init_params call ignored.\", stacklevel=2)\n            return\n        assert self.binded, 'call bind before initializing the parameters'\n\n        def _impl(name, arr, cache):\n            \"\"\"Internal helper for parameter initialization\"\"\"\n            if cache is not None:\n                if name in cache:\n                    cache_arr = cache[name]\n\n                    # just in case the cached array is just the target itself\n                    if cache_arr is not arr:\n                        cache_arr.copyto(arr)\n                else:\n                    if not allow_missing:\n                        raise RuntimeError(\"%s is not presented\" % name)\n                    if initializer is not None:\n                        initializer(name, arr)\n            else:\n                initializer(name, arr)\n\n        attrs = self._symbol.attr_dict()\n        for name, arr in sorted(self._arg_params.items()):\n            desc = InitDesc(name, attrs.get(name, None))\n            _impl(desc, arr, arg_params)\n\n        for name, arr in sorted(self._aux_params.items()):\n            desc = InitDesc(name, attrs.get(name, None))\n            _impl(desc, arr, aux_params)\n\n        self.params_initialized = True\n        self._params_dirty = False\n\n        # copy the initialized parameters to devices\n        self._exec_group.set_params(self._arg_params, self._aux_params,\n                                    allow_extra=allow_extra)", "response": "Initializes the parameters and auxiliary states of the current state."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_params(self, arg_params, aux_params, allow_missing=False, force_init=True,\n                   allow_extra=False):\n        \"\"\"Assigns parameter and aux state values.\n\n        Parameters\n        ----------\n        arg_params : dict\n            Dictionary of name to `NDArray`.\n        aux_params : dict\n            Dictionary of name to `NDArray`.\n        allow_missing : bool\n            If ``True``, params could contain missing values, and the initializer will be\n            called to fill those missing params.\n        force_init : bool\n            If ``True``, will force re-initialize even if already initialized.\n        allow_extra : boolean, optional\n            Whether allow extra parameters that are not needed by symbol.\n            If this is True, no error will be thrown when arg_params or aux_params\n            contain extra parameters that is not needed by the executor.\n        Examples\n        --------\n        >>> # An example of setting module parameters.\n        >>> sym, arg_params, aux_params = mx.model.load_checkpoint(model_prefix, n_epoch_load)\n        >>> mod.set_params(arg_params=arg_params, aux_params=aux_params)\n        \"\"\"\n        if not allow_missing:\n            self.init_params(initializer=None, arg_params=arg_params, aux_params=aux_params,\n                             allow_missing=allow_missing, force_init=force_init,\n                             allow_extra=allow_extra)\n            return\n\n        if self.params_initialized and not force_init:\n            warnings.warn(\"Parameters already initialized and force_init=False. \"\n                          \"set_params call ignored.\", stacklevel=2)\n            return\n\n        self._exec_group.set_params(arg_params, aux_params, allow_extra=allow_extra)\n\n        # because we didn't update self._arg_params, they are dirty now.\n        self._params_dirty = True\n        self.params_initialized = True", "response": "Assigns parameter and aux state values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bind(self, data_shapes, label_shapes=None, for_training=True,\n             inputs_need_grad=False, force_rebind=False, shared_module=None,\n             grad_req='write'):\n        \"\"\"Binds the symbols to construct executors. This is necessary before one\n        can perform computation with the module.\n\n        Parameters\n        ----------\n        data_shapes : list of (str, tuple)\n            Typically is ``data_iter.provide_data``.\n        label_shapes : list of (str, tuple)\n            Typically is ``data_iter.provide_label``.\n        for_training : bool\n            Default is ``True``. Whether the executors should be bound for training.\n        inputs_need_grad : bool\n            Default is ``False``. Whether the gradients to the input data need to be computed.\n            Typically this is not needed. But this might be needed when implementing composition\n            of modules.\n        force_rebind : bool\n            Default is ``False``. This function does nothing if the executors are already\n            bound. But with this ``True``, the executors will be forced to rebind.\n        shared_module : Module\n            Default is ``None``. This is used in bucketing. When not ``None``, the shared module\n            essentially corresponds to a different bucket -- a module with different symbol\n            but with the same sets of parameters (e.g. unrolled RNNs with different lengths).\n        \"\"\"\n        # force rebinding is typically used when one want to switch from\n        # training to prediction phase.\n        if force_rebind:\n            self._reset_bind()\n\n        if self.binded:\n            self.logger.warning('Already bound, ignoring bind()')\n            return\n\n        self.for_training = for_training\n        self.inputs_need_grad = inputs_need_grad\n        self._grad_req = grad_req\n\n        if not for_training:\n            assert not inputs_need_grad\n        else:\n            pass\n            # this is not True, as some module might not contains a loss function\n            # that consumes the labels\n            # assert label_shapes is not None\n\n        self._data_shapes, self._label_shapes = _parse_data_desc(\n            self.data_names, self.label_names, data_shapes, label_shapes)\n\n        if shared_module is not None:\n            assert isinstance(shared_module, Module) and \\\n                    shared_module.binded and shared_module.params_initialized\n            shared_group = shared_module._exec_group\n            assert len(shared_group.execs) >= len(self._context)\n        else:\n            shared_group = None\n\n        self._exec_group = DataParallelExecutorGroup(self._symbol, self._context,\n                                                     self._work_load_list, self._data_shapes,\n                                                     self._label_shapes, self._param_names,\n                                                     for_training, inputs_need_grad,\n                                                     shared_group, logger=self.logger,\n                                                     fixed_param_names=self._fixed_param_names,\n                                                     grad_req=grad_req, group2ctxs=self._group2ctxs,\n                                                     state_names=self._state_names)\n        self._total_exec_bytes = self._exec_group._total_exec_bytes\n        if shared_module is not None:\n            self.params_initialized = True\n            self._arg_params = shared_module._arg_params\n            self._aux_params = shared_module._aux_params\n        elif self.params_initialized:\n            # if the parameters are already initialized, we are re-binding\n            # so automatically copy the already initialized params\n            self._exec_group.set_params(self._arg_params, self._aux_params)\n        else:\n            assert self._arg_params is None and self._aux_params is None\n            param_arrays = [\n                zeros(shape=x[0].shape, dtype=x[0].dtype, stype=x[0].stype)\n                for x in self._exec_group.param_arrays\n            ]\n            self._arg_params = {name:arr for name, arr in zip(self._param_names, param_arrays)}\n\n            aux_arrays = [\n                zeros(x[0].shape, dtype=x[0].dtype)\n                for x in self._exec_group.aux_arrays\n            ]\n            self._aux_params = {name:arr for name, arr in zip(self._aux_names, aux_arrays)}\n\n        if shared_module is not None and shared_module.optimizer_initialized:\n            self.borrow_optimizer(shared_module)\n\n        self.binded = True", "response": "Binds the executors to construct executors."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef reshape(self, data_shapes, label_shapes=None):\n        assert self.binded\n        self._data_shapes, self._label_shapes = _parse_data_desc(\n            self.data_names, self.label_names, data_shapes, label_shapes)\n\n        self._exec_group.reshape(self._data_shapes, self._label_shapes)", "response": "Reshapes the module for new input shapes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef init_optimizer(self, kvstore='local', optimizer='sgd',\n                       optimizer_params=(('learning_rate', 0.01),), force_init=False):\n        \"\"\"Installs and initializes optimizers.\n\n        Parameters\n        ----------\n        kvstore : str or KVStore\n            Default `'local'`.\n        optimizer : str or Optimizer\n            Default `'sgd'`\n        optimizer_params : dict\n            Default `(('learning_rate', 0.01),)`. The default value is not a dictionary,\n            just to avoid pylint warning of dangerous default values.\n        force_init : bool\n            Default ``False``, indicating whether we should force re-initializing the\n            optimizer in the case an optimizer is already installed.\n        \"\"\"\n        assert self.binded and self.params_initialized\n\n        if self.optimizer_initialized and not force_init:\n            self.logger.warning('optimizer already initialized, ignoring...')\n            return\n\n        if self._params_dirty:\n            self._sync_params_from_devices()\n\n        (kvstore, update_on_kvstore) = \\\n                _create_kvstore(kvstore, len(self._context), self._arg_params)\n\n        batch_size = self._exec_group.batch_size\n        if kvstore and 'dist' in kvstore.type and '_sync' in kvstore.type:\n            batch_size *= kvstore.num_workers\n        rescale_grad = 1.0/batch_size\n\n        idx2name = {}\n        if update_on_kvstore:\n            idx2name.update(enumerate(self._exec_group.param_names))\n        else:\n            for k in range(len(self._context)):\n                idx2name.update({i*len(self._context)+k: n\n                                 for i, n in enumerate(self._exec_group.param_names)})\n        if isinstance(optimizer, str):\n            optimizer_params = dict(optimizer_params)\n            if 'rescale_grad' not in optimizer_params:\n                optimizer_params['rescale_grad'] = rescale_grad\n            optimizer = opt.create(optimizer,\n                                   sym=self.symbol, param_idx2name=idx2name,\n                                   **optimizer_params)\n        else:\n            assert isinstance(optimizer, opt.Optimizer)\n            if optimizer.rescale_grad != rescale_grad:\n                #pylint: disable=no-member\n                warnings.warn(\n                    \"Optimizer created manually outside Module but rescale_grad \" +\n                    \"is not normalized to 1.0/batch_size/num_workers (%s vs. %s). \"%(\n                        optimizer.rescale_grad, rescale_grad) +\n                    \"Is this intended?\", stacklevel=2)\n            if not optimizer.idx2name:\n                optimizer.idx2name = idx2name.copy()\n\n        self._optimizer = optimizer\n        self._kvstore = kvstore\n        self._update_on_kvstore = update_on_kvstore\n        self._updater = None\n\n        if kvstore:\n            if self._compression_params:\n                kvstore.set_gradient_compression(self._compression_params)\n            if update_on_kvstore:\n                kvstore.set_optimizer(self._optimizer)\n            # copy initialized local parameters to kvstore\n            _initialize_kvstore(kvstore=kvstore,\n                                param_arrays=self._exec_group.param_arrays,\n                                arg_params=self._arg_params,\n                                param_names=self._param_names,\n                                update_on_kvstore=update_on_kvstore)\n\n        if not update_on_kvstore:\n            self._updater = opt.get_updater(optimizer)\n\n        self.optimizer_initialized = True\n\n        if self._preload_opt_states is not None:\n            self.load_optimizer_states(self._preload_opt_states)\n            self._preload_opt_states = None", "response": "Installs and initializes the optimizers."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef borrow_optimizer(self, shared_module):\n        assert shared_module.optimizer_initialized\n        self._optimizer = shared_module._optimizer\n        self._kvstore = shared_module._kvstore\n        self._update_on_kvstore = shared_module._update_on_kvstore\n        self._updater = shared_module._updater\n        self.optimizer_initialized = True", "response": "Borrows an optimizer from a shared module."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nforwarding computation. It supports data batches with different shapes such as different shape of data and different image sizes.", "response": "def forward(self, data_batch, is_train=None):\n        \"\"\"Forward computation. It supports data batches with different shapes, such as\n        different batch sizes or different image sizes.\n        If reshaping of data batch relates to modification of symbol or module, such as\n        changing image layout ordering or switching from training to predicting, module\n        rebinding is required.\n\n        See Also\n        ----------\n        :meth:`BaseModule.forward`.\n\n        Parameters\n        ----------\n        data_batch : DataBatch\n            Could be anything with similar API implemented.\n        is_train : bool\n            Default is ``None``, which means ``is_train`` takes the value of ``self.for_training``.\n        \"\"\"\n        assert self.binded and self.params_initialized\n\n        curr_data_shapes = tuple(i.shape for i in self._data_shapes)\n        if isinstance(data_batch, list):\n            assert data_batch is not None, \"Encountered empty data batch\"\n            new_data_shapes = []\n            for i in range(len(data_batch[0].data)):\n                shape = data_batch[0].data[i].shape\n                for db in data_batch:\n                    assert shape == db.data[i].shape, \\\n                        \"All data batches in a list need to have the same shape\"\n                new_batch_size = len(data_batch) * shape[0]\n                new_data_shapes.append((new_batch_size,) + shape[1:])\n            new_data_shapes = tuple(new_data_shapes)\n        else:\n            new_data_shapes = tuple(i.shape for i in data_batch.data)\n\n        if curr_data_shapes != new_data_shapes:\n            if hasattr(data_batch, \"provide_data\") and data_batch.provide_data:\n                new_dshape = data_batch.provide_data\n            else:\n                new_dshape = [DataDesc(i.name, shape, i.dtype, i.layout) \\\n                              for i, shape in zip(self._data_shapes, new_data_shapes)]\n\n            if hasattr(data_batch, \"provide_label\") and data_batch.provide_label:\n                new_lshape = data_batch.provide_label\n            elif hasattr(data_batch, \"label\") and data_batch.label:\n                new_lshape = [DataDesc(i.name, j.shape, i.dtype, i.layout) \\\n                              for i, j in zip(self._label_shapes, data_batch.label)]\n            else:\n                new_lshape = None\n\n            self.reshape(new_dshape, new_lshape)\n\n        self._exec_group.forward(data_batch, is_train)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef backward(self, out_grads=None):\n        assert self.binded and self.params_initialized\n        self._exec_group.backward(out_grads=out_grads)", "response": "Backward computation.\n\n        See Also\n        ----------\n        :meth:`BaseModule.backward`.\n\n        Parameters\n        ----------\n        out_grads : NDArray or list of NDArray, optional\n            Gradient on the outputs to be propagated back.\n            This parameter is only needed when bind is called\n            on outputs that are not a loss function."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update(self):\n        assert self.binded and self.params_initialized and self.optimizer_initialized\n\n        self._params_dirty = True\n        if self._update_on_kvstore:\n            _update_params_on_kvstore(self._exec_group.param_arrays,\n                                      self._exec_group.grad_arrays,\n                                      self._kvstore, self._exec_group.param_names)\n        else:\n            _update_params(self._exec_group.param_arrays,\n                           self._exec_group.grad_arrays,\n                           updater=self._updater,\n                           num_device=len(self._context),\n                           kvstore=self._kvstore,\n                           param_names=self._exec_group.param_names)", "response": "Updates the parameters according to the current optimizer and gradients."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_outputs(self, merge_multi_context=True):\n        assert self.binded and self.params_initialized\n        return self._exec_group.get_outputs(merge_multi_context=merge_multi_context)", "response": "Gets outputs of the previous forward computation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the gradients with respect to the inputs of the module.", "response": "def get_input_grads(self, merge_multi_context=True):\n        \"\"\"Gets the gradients with respect to the inputs of the module.\n\n        If ``merge_multi_context`` is ``True``, it is like ``[grad1, grad2]``. Otherwise, it\n        is like ``[[grad1_dev1, grad1_dev2], [grad2_dev1, grad2_dev2]]``. All the output\n        elements are `NDArray`.\n\n        Parameters\n        ----------\n        merge_multi_context : bool\n            Default is ``True``. In the case when data-parallelism is used, the outputs\n            will be collected from multiple devices. A ``True`` value indicate that we\n            should merge the collected results so that they look like from a single\n            executor.\n\n        Returns\n        -------\n        list of NDArray or list of list of NDArray\n              Input gradients\n        \"\"\"\n        assert self.binded and self.params_initialized and self.inputs_need_grad\n        return self._exec_group.get_input_grads(merge_multi_context=merge_multi_context)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets states from all devices.", "response": "def get_states(self, merge_multi_context=True):\n        \"\"\"Gets states from all devices.\n\n        If `merge_multi_context` is ``True``, it is like ``[out1, out2]``. Otherwise, it\n        is like ``[[out1_dev1, out1_dev2], [out2_dev1, out2_dev2]]``. All the output\n        elements are `NDArray`.\n\n        Parameters\n        ----------\n        merge_multi_context : bool\n            Default is ``True``. In the case when data-parallelism is used, the states\n            will be collected from multiple devices. A ``True`` value indicate that we\n            should merge the collected results so that they look like from a single\n            executor.\n\n        Returns\n        -------\n        list of NDArray or list of list of NDArray\n            States\n        \"\"\"\n        assert self.binded and self.params_initialized\n        return self._exec_group.get_states(merge_multi_context=merge_multi_context)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_metric(self, eval_metric, labels, pre_sliced=False):\n        self._exec_group.update_metric(eval_metric, labels, pre_sliced)", "response": "Evaluates and accumulates evaluation metric on outputs of the last forward computation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsynchronize parameters from devices to CPU.", "response": "def _sync_params_from_devices(self):\n        \"\"\"Synchronizes parameters from devices to CPU. This function should be called after\n        calling `update` that updates the parameters on the devices, before one can read the\n        latest parameters from ``self._arg_params`` and ``self._aux_params``.\n\n        For row_sparse parameters on devices, ther are pulled from KVStore with all row ids.\n\n        \"\"\"\n        self._exec_group.get_params(self._arg_params, self._aux_params)\n        if self._kvstore and self._update_on_kvstore:\n            for param_name, param_val in sorted(self._arg_params.items()):\n                if param_val.stype == 'row_sparse':\n                    row_ids = nd.arange(0, param_val.shape[0], dtype='int64')\n                    self._kvstore.row_sparse_pull(param_name, param_val, row_ids=row_ids)\n        self._params_dirty = False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save_optimizer_states(self, fname):\n        assert self.optimizer_initialized\n\n        if self._update_on_kvstore:\n            self._kvstore.save_optimizer_states(fname)\n        else:\n            with open(fname, 'wb') as fout:\n                fout.write(self._updater.get_states())", "response": "Saves optimizer state to a file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_optimizer_states(self, fname):\n        assert self.optimizer_initialized\n\n        if self._update_on_kvstore:\n            self._kvstore.load_optimizer_states(fname)\n        else:\n            self._updater.set_states(open(fname, 'rb').read())", "response": "Loads optimizer ( updater ) state from a file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npreparing the module for processing a data batch.", "response": "def prepare(self, data_batch, sparse_row_id_fn=None):\n        '''Prepares the module for processing a data batch.\n\n        Usually involves switching bucket and reshaping.\n        For modules that contain `row_sparse` parameters in KVStore,\n        it prepares the `row_sparse` parameters based on the sparse_row_id_fn.\n\n        When KVStore is used to update parameters for multi-device or multi-machine training,\n        a copy of the parameters are stored in KVStore. Note that for `row_sparse` parameters,\n        the `update()` updates the copy of parameters in KVStore, but doesn't broadcast\n        the updated parameters to all devices / machines. The `prepare` function is used to\n        broadcast `row_sparse` parameters with the next batch of data.\n\n        Parameters\n        ----------\n        data_batch : DataBatch\n            The current batch of data for forward computation.\n\n        sparse_row_id_fn : A callback function\n            The function  takes `data_batch` as an input and returns a dict of\n            str -> NDArray. The resulting dict is used for pulling row_sparse\n            parameters from the kvstore, where the str key is the name of the param,\n            and the value is the row id of the param to pull.\n        '''\n        assert self.binded\n        if sparse_row_id_fn is not None:\n            if not self._kvstore or not self._update_on_kvstore:\n                warnings.warn(UserWarning(\"Parameters are not updated in the KVStore. \"\n                                          \"No need to call sparse_row_id_fn.\"))\n            else:\n                row_ids = sparse_row_id_fn(data_batch)\n                assert(isinstance(row_ids, dict)), \"Expected dict output from sparse_row_id_fn\"\n                for param_name, row_id in row_ids.items():\n                    param_idx = self._exec_group.param_names.index(param_name)\n                    param_val = self._exec_group.param_arrays[param_idx]\n                    assert(isinstance(param_val, (tuple, list)))\n                    if param_val[0].stype != 'row_sparse':\n                        warnings.warn(UserWarning(\"%s.stype is not 'row_sparse'. No need to \"\n                                                  \"perform row_sparse_pull.\" % param_name))\n                    else:\n                        self._kvstore.row_sparse_pull(param_name, param_val, row_ids=row_id,\n                                                      priority=-param_idx)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _random_helper(random, sampler, params, shape, dtype, ctx, out, kwargs):\n    if isinstance(params[0], NDArray):\n        for i in params[1:]:\n            assert isinstance(i, NDArray), \\\n                \"Distribution parameters must all have the same type, but got \" \\\n                \"both %s and %s.\"%(type(params[0]), type(i))\n        return sampler(*params, shape=shape, dtype=dtype, out=out, **kwargs)\n    elif isinstance(params[0], numeric_types):\n        if ctx is None:\n            ctx = current_context()\n        if shape is _Null and out is None:\n            shape = 1\n        for i in params[1:]:\n            assert isinstance(i, numeric_types), \\\n                \"Distribution parameters must all have the same type, but got \" \\\n                \"both %s and %s.\"%(type(params[0]), type(i))\n        return random(*params, shape=shape, dtype=dtype, ctx=ctx, out=out, **kwargs)\n\n    raise ValueError(\"Distribution parameters must be either NDArray or numbers, \"\n                     \"but got %s.\"%type(params[0]))", "response": "Helper function for random generators."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndraws random samples from a uniform distribution.", "response": "def uniform(low=0, high=1, shape=_Null, dtype=_Null, ctx=None, out=None, **kwargs):\n    \"\"\"Draw random samples from a uniform distribution.\n\n    Samples are uniformly distributed over the half-open interval *[low, high)*\n    (includes *low*, but excludes *high*).\n\n    Parameters\n    ----------\n    low : float or NDArray, optional\n        Lower boundary of the output interval. All values generated will be\n        greater than or equal to low. The default value is 0.\n    high : float or NDArray, optional\n        Upper boundary of the output interval. All values generated will be\n        less than high. The default value is 1.0.\n    shape : int or tuple of ints, optional\n        The number of samples to draw. If shape is, e.g., `(m, n)` and `low` and\n        `high` are scalars, output shape will be `(m, n)`. If `low` and `high`\n        are NDArrays with shape, e.g., `(x, y)`, then output will have shape\n        `(x, y, m, n)`, where `m*n` samples are drawn for each `[low, high)` pair.\n    dtype : {'float16', 'float32', 'float64'}, optional\n        Data type of output samples. Default is 'float32'\n    ctx : Context, optional\n        Device context of output. Default is current context. Overridden by\n        `low.context` when `low` is an NDArray.\n    out : NDArray, optional\n        Store output to an existing NDArray.\n\n    Returns\n    -------\n    NDArray\n        An NDArray of type `dtype`. If input `shape` has shape, e.g.,\n        `(m, n)` and `low` and `high` are scalars, output shape will be `(m, n)`.\n        If `low` and `high` are NDArrays with shape, e.g., `(x, y)`, then the\n        return NDArray will have shape `(x, y, m, n)`, where `m*n` uniformly distributed\n        samples are drawn for each `[low, high)` pair.\n\n    Examples\n    --------\n    >>> mx.nd.random.uniform(0, 1)\n    [ 0.54881352]\n    <NDArray 1 @cpu(0)\n    >>> mx.nd.random.uniform(0, 1, ctx=mx.gpu(0))\n    [ 0.92514056]\n    <NDArray 1 @gpu(0)>\n    >>> mx.nd.random.uniform(-1, 1, shape=(2,))\n    [ 0.71589124  0.08976638]\n    <NDArray 2 @cpu(0)>\n    >>> low = mx.nd.array([1,2,3])\n    >>> high = mx.nd.array([2,3,4])\n    >>> mx.nd.random.uniform(low, high, shape=2)\n    [[ 1.78653979  1.93707538]\n     [ 2.01311183  2.37081361]\n     [ 3.30491424  3.69977832]]\n    <NDArray 3x2 @cpu(0)>\n    \"\"\"\n    return _random_helper(_internal._random_uniform, _internal._sample_uniform,\n                          [low, high], shape, dtype, ctx, out, kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef normal(loc=0, scale=1, shape=_Null, dtype=_Null, ctx=None, out=None, **kwargs):\n    return _random_helper(_internal._random_normal, _internal._sample_normal,\n                          [loc, scale], shape, dtype, ctx, out, kwargs)", "response": "Draw random samples from a normal distribution."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef randn(*shape, **kwargs):\n    loc = kwargs.pop('loc', 0)\n    scale = kwargs.pop('scale', 1)\n    dtype = kwargs.pop('dtype', _Null)\n    ctx = kwargs.pop('ctx', None)\n    out = kwargs.pop('out', None)\n    assert isinstance(loc, (int, float))\n    assert isinstance(scale, (int, float))\n    return _random_helper(_internal._random_normal, _internal._sample_normal,\n                          [loc, scale], shape, dtype, ctx, out, kwargs)", "response": "Draw random samples from a normal distribution parametrized\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef exponential(scale=1, shape=_Null, dtype=_Null, ctx=None, out=None, **kwargs):\n    return _random_helper(_internal._random_exponential, _internal._sample_exponential,\n                          [1.0/scale], shape, dtype, ctx, out, kwargs)", "response": "r Draws samples from an exponential distribution."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndraw random samples from a gamma distribution.", "response": "def gamma(alpha=1, beta=1, shape=_Null, dtype=_Null, ctx=None, out=None, **kwargs):\n    \"\"\"Draw random samples from a gamma distribution.\n\n    Samples are distributed according to a gamma distribution parametrized\n    by *alpha* (shape) and *beta* (scale).\n\n    Parameters\n    ----------\n    alpha : float or NDArray, optional\n        The shape of the gamma distribution. Should be greater than zero.\n    beta : float or NDArray, optional\n        The scale of the gamma distribution. Should be greater than zero.\n        Default is equal to 1.\n    shape : int or tuple of ints, optional\n        The number of samples to draw. If shape is, e.g., `(m, n)` and `alpha` and\n        `beta` are scalars, output shape will be `(m, n)`. If `alpha` and `beta`\n        are NDArrays with shape, e.g., `(x, y)`, then output will have shape\n        `(x, y, m, n)`, where `m*n` samples are drawn for each `[alpha, beta)` pair.\n    dtype : {'float16', 'float32', 'float64'}, optional\n        Data type of output samples. Default is 'float32'\n    ctx : Context, optional\n        Device context of output. Default is current context. Overridden by\n        `alpha.context` when `alpha` is an NDArray.\n    out : NDArray, optional\n        Store output to an existing NDArray.\n\n    Returns\n    -------\n    NDArray\n        If input `shape` has shape, e.g., `(m, n)` and `alpha` and `beta` are scalars, output\n        shape will be `(m, n)`. If `alpha` and `beta` are NDArrays with shape, e.g.,\n        `(x, y)`, then output will have shape `(x, y, m, n)`, where `m*n` samples are\n        drawn for each `[alpha, beta)` pair.\n\n    Examples\n    --------\n    >>> mx.nd.random.gamma(1, 1)\n    [ 1.93308783]\n    <NDArray 1 @cpu(0)>\n    >>> mx.nd.random.gamma(1, 1, shape=(2,))\n    [ 0.48216391  2.09890771]\n    <NDArray 2 @cpu(0)>\n    >>> alpha = mx.nd.array([1,2,3])\n    >>> beta = mx.nd.array([2,3,4])\n    >>> mx.nd.random.gamma(alpha, beta, shape=2)\n    [[  3.24343276   0.94137681]\n     [  3.52734375   0.45568955]\n     [ 14.26264095  14.0170126 ]]\n    <NDArray 3x2 @cpu(0)>\n    \"\"\"\n    return _random_helper(_internal._random_gamma, _internal._sample_gamma,\n                          [alpha, beta], shape, dtype, ctx, out, kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndraw random samples from a negative binomial distribution.", "response": "def negative_binomial(k=1, p=1, shape=_Null, dtype=_Null, ctx=None,\n                      out=None, **kwargs):\n    \"\"\"Draw random samples from a negative binomial distribution.\n\n    Samples are distributed according to a negative binomial distribution\n    parametrized by *k* (limit of unsuccessful experiments) and *p* (failure\n    probability in each experiment). Samples will always be returned as a\n    floating point data type.\n\n    Parameters\n    ----------\n    k : float or NDArray, optional\n        Limit of unsuccessful experiments, > 0.\n    p : float or NDArray, optional\n        Failure probability in each experiment, >= 0 and <=1.\n    shape : int or tuple of ints, optional\n        The number of samples to draw. If shape is, e.g., `(m, n)` and `k` and\n        `p` are scalars, output shape will be `(m, n)`. If `k` and `p`\n        are NDArrays with shape, e.g., `(x, y)`, then output will have shape\n        `(x, y, m, n)`, where `m*n` samples are drawn for each `[k, p)` pair.\n    dtype : {'float16', 'float32', 'float64'}, optional\n        Data type of output samples. Default is 'float32'\n    ctx : Context, optional\n        Device context of output. Default is current context. Overridden by\n        `k.context` when `k` is an NDArray.\n    out : NDArray, optional\n        Store output to an existing NDArray.\n\n    Returns\n    -------\n    NDArray\n        If input `shape` has shape, e.g., `(m, n)` and `k` and `p` are scalars, output shape\n        will be `(m, n)`. If `k` and `p` are NDArrays with shape, e.g., `(x, y)`, then\n        output will have shape `(x, y, m, n)`, where `m*n` samples are drawn for each `[k, p)` pair.\n\n    Examples\n    --------\n    >>> mx.nd.random.negative_binomial(10, 0.5)\n    [ 4.]\n    <NDArray 1 @cpu(0)>\n    >>> mx.nd.random.negative_binomial(10, 0.5, shape=(2,))\n    [ 3.  4.]\n    <NDArray 2 @cpu(0)>\n    >>> k = mx.nd.array([1,2,3])\n    >>> p = mx.nd.array([0.2,0.4,0.6])\n    >>> mx.nd.random.negative_binomial(k, p, shape=2)\n    [[ 3.  2.]\n     [ 4.  4.]\n     [ 0.  5.]]\n    <NDArray 3x2 @cpu(0)>\n    \"\"\"\n    return _random_helper(_internal._random_negative_binomial,\n                          _internal._sample_negative_binomial,\n                          [k, p], shape, dtype, ctx, out, kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndraws random samples from a discrete uniform distribution.", "response": "def randint(low, high, shape=_Null, dtype=_Null, ctx=None, out=None, **kwargs):\n    \"\"\"Draw random samples from a discrete uniform distribution.\n\n    Samples are uniformly distributed over the half-open interval *[low, high)*\n    (includes *low*, but excludes *high*).\n\n    Parameters\n    ----------\n    low : int, required\n        Lower boundary of the output interval. All values generated will be\n        greater than or equal to low.\n    high : int, required\n        Upper boundary of the output interval. All values generated will be\n        less than high.\n    shape : int or tuple of ints, optional\n        The number of samples to draw. If shape is, e.g., `(m, n)` and `low` and\n        `high` are scalars, output shape will be `(m, n)`.\n    dtype : {'int32', 'int64'}, optional\n        Data type of output samples. Default is 'int32'\n    ctx : Context, optional\n        Device context of output. Default is current context. Overridden by\n        `low.context` when `low` is an NDArray.\n    out : NDArray, optional\n        Store output to an existing NDArray.\n\n    Returns\n    -------\n    NDArray\n        An NDArray of type `dtype`. If input `shape` has shape, e.g.,\n        `(m, n)`, the returned NDArray will shape will be `(m, n)`. Contents\n        of the returned NDArray will be samples from the interval `[low, high)`.\n\n    Examples\n    --------\n    >>> mx.nd.random.randint(5, 100)\n    [ 90]\n    <NDArray 1 @cpu(0)\n    >>> mx.nd.random.randint(-10, 2, ctx=mx.gpu(0))\n    [ -8]\n    <NDArray 1 @gpu(0)>\n    >>> mx.nd.random.randint(-10, 10, shape=(2,))\n    [ -5  4]\n    <NDArray 2 @cpu(0)>\n    \"\"\"\n    return _random_helper(_internal._random_randint, None,\n                          [low, high], shape, dtype, ctx, out, kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef preprocess_uci_adult(data_name):\n    csv_columns = [\n        \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\",\n        \"marital_status\", \"occupation\", \"relationship\", \"race\", \"gender\",\n        \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\",\n        \"income_bracket\"\n    ]\n\n    vocabulary_dict = {\n        \"gender\": [\n            \"Female\", \"Male\"\n        ],\n        \"education\": [\n            \"Bachelors\", \"HS-grad\", \"11th\", \"Masters\", \"9th\",\n            \"Some-college\", \"Assoc-acdm\", \"Assoc-voc\", \"7th-8th\",\n            \"Doctorate\", \"Prof-school\", \"5th-6th\", \"10th\", \"1st-4th\",\n            \"Preschool\", \"12th\"\n        ],\n        \"marital_status\": [\n            \"Married-civ-spouse\", \"Divorced\", \"Married-spouse-absent\",\n            \"Never-married\", \"Separated\", \"Married-AF-spouse\", \"Widowed\"\n        ],\n        \"relationship\": [\n            \"Husband\", \"Not-in-family\", \"Wife\", \"Own-child\", \"Unmarried\",\n            \"Other-relative\"\n        ],\n        \"workclass\": [\n            \"Self-emp-not-inc\", \"Private\", \"State-gov\", \"Federal-gov\",\n            \"Local-gov\", \"?\", \"Self-emp-inc\", \"Without-pay\", \"Never-worked\"\n        ]\n    }\n    # wide columns\n    crossed_columns = [\n        [\"education\", \"occupation\"],\n        [\"native_country\", \"occupation\"],\n        [\"age_buckets\", \"education\", \"occupation\"],\n    ]\n    age_boundaries = [18, 25, 30, 35, 40, 45, 50, 55, 60, 65]\n    # deep columns\n    indicator_columns = ['workclass', 'education', 'gender', 'relationship']\n    \n    embedding_columns = ['native_country', 'occupation']\n\n    continuous_columns = ['age', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week']\n    # income_bracket column is the label\n    labels = [\"<\", \">\"]\n\n    hash_bucket_size = 1000\n    \n    csr_ncols = len(crossed_columns) * hash_bucket_size\n    dns_ncols = len(continuous_columns) + len(embedding_columns)\n    for col in indicator_columns:\n        dns_ncols += len(vocabulary_dict[col])\n\n    label_list = []\n    csr_list = []\n    dns_list = []\n\n    with open(data_name) as f:\n        for row in DictReader(f, fieldnames=csv_columns):\n            label_list.append(labels.index(row['income_bracket'].strip()[0]))\n\n            for i, cols in enumerate(crossed_columns):\n                if cols[0] == \"age_buckets\":\n                    age_bucket = np.digitize(float(row[\"age\"]), age_boundaries)\n                    s = '_'.join([row[col].strip() for col in cols[1:]])\n                    s += '_' + str(age_bucket)\n                    csr_list.append((i * hash_bucket_size + hash(s) % hash_bucket_size, 1.0))\n                else:\n                    s = '_'.join([row[col].strip() for col in cols])\n                    csr_list.append((i * hash_bucket_size + hash(s) % hash_bucket_size, 1.0))\n            \n            dns_row = [0] * dns_ncols\n            dns_dim = 0\n            for col in embedding_columns:\n                dns_row[dns_dim] = hash(row[col].strip()) % hash_bucket_size\n                dns_dim += 1\n\n            for col in indicator_columns:\n                dns_row[dns_dim + vocabulary_dict[col].index(row[col].strip())] = 1.0\n                dns_dim += len(vocabulary_dict[col])\n\n            for col in continuous_columns:\n                dns_row[dns_dim] = float(row[col].strip())\n                dns_dim += 1\n\n            dns_list.append(dns_row)\n\n    data_list = [item[1] for item in csr_list]\n    indices_list = [item[0] for item in csr_list]\n    indptr_list = range(0, len(indices_list) + 1, len(crossed_columns))\n    # convert to ndarrays\n    csr = mx.nd.sparse.csr_matrix((data_list, indices_list, indptr_list),\n                                  shape=(len(label_list), hash_bucket_size * len(crossed_columns)))\n    dns = np.array(dns_list)\n    label = np.array(label_list)\n    return csr, dns, label", "response": "Preprocess the UCI adult data."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _init_params(self):\n        assert self._kv_initialized, \"Cannot initialize parameters in KVStore \" \\\n                                     \"when KVStore is not initialized.\"\n        params_to_init = []\n        if self._kvstore:\n            for param in self._params_to_init:\n                if param._deferred_init:\n                    params_to_init.append(param)\n                else:\n                    param_arrays = param._check_and_get(param._data, list)\n                    idx = self._param2idx[param.name]\n                    self._kvstore.init(idx, param_arrays[0])\n                    if param._stype == 'default':\n                        self._kvstore.pull(idx, param_arrays, priority=-idx)\n\n        self._params_to_init = params_to_init", "response": "Initialize parameters in the KVStore."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets a new learning rate of the optimizer.", "response": "def set_learning_rate(self, lr):\n        \"\"\"Sets a new learning rate of the optimizer.\n\n        Parameters\n        ----------\n        lr : float\n            The new learning rate of the optimizer.\n        \"\"\"\n        if not isinstance(self._optimizer, opt.Optimizer):\n            raise UserWarning(\"Optimizer has to be defined before its learning \"\n                              \"rate is mutated.\")\n        else:\n            self._optimizer.set_learning_rate(lr)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _row_sparse_pull(self, parameter, out, row_id, full_idx=False):\n        # initialize kv and params if not already\n        if not self._kv_initialized:\n            self._init_kvstore()\n        if self._params_to_init:\n            self._init_params()\n        idx = self._param2idx[parameter.name]\n        if full_idx and 'dist' not in self._kvstore.type:\n            assert row_id.size == out.shape[0]\n            self._kvstore.pull(idx, out=out, priority=-idx, ignore_sparse=False)\n        else:\n            self._kvstore.row_sparse_pull(idx, out=out, row_ids=row_id, priority=-idx)", "response": "Internal method to invoke pull operations on KVStore."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmake one step of parameter update.", "response": "def step(self, batch_size, ignore_stale_grad=False):\n        \"\"\"Makes one step of parameter update. Should be called after\n        `autograd.backward()` and outside of `record()` scope.\n\n        For normal parameter updates, `step()` should be used, which internally calls\n        `allreduce_grads()` and then `update()`. However, if you need to get the reduced\n        gradients to perform certain transformation, such as in gradient clipping, then\n        you may want to manually call `allreduce_grads()` and `update()` separately.\n\n        Parameters\n        ----------\n        batch_size : int\n            Batch size of data processed. Gradient will be normalized by `1/batch_size`.\n            Set this to 1 if you normalized loss manually with `loss = mean(loss)`.\n        ignore_stale_grad : bool, optional, default=False\n            If true, ignores Parameters with stale gradient (gradient that has not\n            been updated by `backward` after last step) and skip update.\n        \"\"\"\n        rescale_grad = self._scale / batch_size\n        self._check_and_rescale_grad(rescale_grad)\n\n        if not self._kv_initialized:\n            self._init_kvstore()\n        if self._params_to_init:\n            self._init_params()\n\n        self._allreduce_grads()\n        self._update(ignore_stale_grad)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef allreduce_grads(self):\n        if not self._kv_initialized:\n            self._init_kvstore()\n        if self._params_to_init:\n            self._init_params()\n        assert not (self._kvstore and self._update_on_kvstore), \\\n                'allreduce_grads() when parameters are updated on kvstore ' \\\n                'is not supported. Try setting `update_on_kvstore` ' \\\n                'to False when creating trainer.'\n\n        self._allreduce_grads()", "response": "For each parameter reduce the gradients from different contexts."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update(self, batch_size, ignore_stale_grad=False):\n        if not self._kv_initialized:\n            self._init_kvstore()\n        if self._params_to_init:\n            self._init_params()\n        assert not (self._kvstore and self._update_on_kvstore), \\\n                'update() when parameters are updated on kvstore ' \\\n                'is not supported. Try setting `update_on_kvstore` ' \\\n                'to False when creating trainer.'\n\n        self._check_and_rescale_grad(self._scale / batch_size)\n        self._update(ignore_stale_grad)", "response": "Makes one step of parameter update."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef save_states(self, fname):\n        assert self._optimizer is not None\n\n        if not self._kv_initialized:\n            self._init_kvstore()\n        if self._params_to_init:\n            self._init_params()\n\n        if self._update_on_kvstore:\n            assert not self._params_to_init, \"Cannot save trainer states when some \" \\\n                                             \"parameters are not yet initialized in kvstore.\"\n            self._kvstore.save_optimizer_states(fname, dump_optimizer=True)\n        else:\n            with open(fname, 'wb') as fout:\n                fout.write(self._updaters[0].get_states(dump_optimizer=True))", "response": "Saves trainer states to a file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading trainer states from a file.", "response": "def load_states(self, fname):\n        \"\"\"Loads trainer states (e.g. optimizer, momentum) from a file.\n\n        Parameters\n        ----------\n        fname : str\n            Path to input states file.\n\n        Note\n        ----\n        `optimizer.param_dict`, which contains Parameter information (such as\n        `lr_mult` and `wd_mult`) will not be loaded from the file, but rather set\n        based on current Trainer's parameters.\n        \"\"\"\n        if not self._kv_initialized:\n            self._init_kvstore()\n        if self._params_to_init:\n            self._init_params()\n\n        if self._update_on_kvstore:\n            self._kvstore.load_optimizer_states(fname)\n            self._optimizer = self._kvstore._updater.optimizer\n        else:\n            with open(fname, 'rb') as f:\n                states = f.read()\n            for updater in self._updaters:\n                updater.set_states(states)\n                updater.optimizer = self._updaters[0].optimizer\n            self._optimizer = self._updaters[0].optimizer\n        param_dict = {i: param for i, param in enumerate(self._params)}\n        self._optimizer.param_dict = param_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef estimate_density(DATA_PATH, feature_size):\n    if not os.path.exists(DATA_PATH):\n        raise Exception(\"Data is not there!\")\n    density = []\n    P = 0.01\n    for _ in range(10):\n        num_non_zero = 0\n        num_sample = 0\n        with open(DATA_PATH) as f:\n            for line in f:\n                if (random.random() < P):\n                    num_non_zero += len(line.split(\" \")) - 1\n                    num_sample += 1\n        density.append(num_non_zero * 1.0 / (feature_size * num_sample))\n    return sum(density) / len(density)", "response": "estimate the density of the sparse dataset"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexecutes the command line command.", "response": "def exec_cmd(cmd, role, taskid, pass_env):\n    \"\"\"Execute the command line command.\"\"\"\n    if cmd[0].find('/') == -1 and os.path.exists(cmd[0]) and os.name != 'nt':\n        cmd[0] = './' + cmd[0]\n    cmd = ' '.join(cmd)\n    env = os.environ.copy()\n    for k, v in pass_env.items():\n        env[k] = str(v)\n\n    env['DMLC_TASK_ID'] = str(taskid)\n    env['DMLC_ROLE'] = role\n    env['DMLC_JOB_CLUSTER'] = 'local'\n\n    ntrial = 0\n    while True:\n        if os.name == 'nt':\n            env['DMLC_NUM_ATTEMPT'] = str(ntrial)\n            ret = subprocess.call(cmd, shell=True, env=env)\n            if ret != 0:\n                ntrial += 1\n                continue\n        else:\n            bash = cmd\n            ret = subprocess.call(bash, shell=True, executable='bash', env=env)\n        if ret == 0:\n            logging.debug('Thread %d exit with 0', taskid)\n            return\n        else:\n            if os.name == 'nt':\n                sys.exit(-1)\n            else:\n                raise RuntimeError('Get nonzero return code=%d' % ret)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef submit(args):\n    gpus = args.gpus.strip().split(',')\n    \"\"\"Submit function of local jobs.\"\"\"\n    def mthread_submit(nworker, nserver, envs):\n        \"\"\"\n        customized submit script, that submit nslave jobs, each must contain args as parameter\n        note this can be a lambda function containing additional parameters in input\n\n        Parameters\n        ----------\n        nworker: number of slave process to start up\n        nserver: number of server nodes to start up\n        envs: enviroment variables to be added to the starting programs\n        \"\"\"\n        procs = {}\n        for i, gpu in enumerate(gpus):\n            for j in range(args.num_threads):\n                procs[i] = Thread(target=exec_cmd, args=(args.command + ['--gpus=%s'%gpu], 'worker', i*args.num_threads+j, envs))\n                procs[i].setDaemon(True)\n                procs[i].start()\n        for i in range(len(gpus)*args.num_threads, len(gpus)*args.num_threads + nserver):\n            procs[i] = Thread(target=exec_cmd, args=(args.command, 'server', i, envs))\n            procs[i].setDaemon(True)\n            procs[i].start()\n\n    # call submit, with nslave, the commands to run each job and submit function\n    tracker.submit(args.num_threads*len(gpus), args.num_servers, fun_submit=mthread_submit,\n                   pscmd=(' '.join(args.command)))", "response": "Submit function of local jobs."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ctc_label(p):\n        ret = []\n        p1 = [0] + p\n        for i, _ in enumerate(p):\n            c1 = p1[i]\n            c2 = p1[i+1]\n            if c2 in (0, c1):\n                continue\n            ret.append(c2)\n        return ret", "response": "Iterates through p identifying non - zero and non - repeating values and returns them in a list"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove trailing zeros in the list of integers and returns a new list of integers", "response": "def _remove_blank(l):\n        \"\"\" Removes trailing zeros in the list of integers and returns a new list of integers\"\"\"\n        ret = []\n        for i, _ in enumerate(l):\n            if l[i] == 0:\n                break\n            ret.append(l[i])\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _lcs(p, l):\n        # Dynamic Programming Finding LCS\n        if len(p) == 0:\n            return 0\n        P = np.array(list(p)).reshape((1, len(p)))\n        L = np.array(list(l)).reshape((len(l), 1))\n        M = np.ndarray(shape=(len(P), len(L)), dtype=np.int32)\n        for i in range(M.shape[0]):\n            for j in range(M.shape[1]):\n                up = 0 if i == 0 else M[i-1, j]\n                left = 0 if j == 0 else M[i, j-1]\n\n                if i == 0 or j == 0:\n                    M[i, j] = max(up, left, M[i, j])\n                else:\n                    M[i, j] = M[i, j] + M[i - 1, j - 1]\n        return M.max()", "response": "Calculates the Longest Common Subsequence between p and l and returns its length"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef accuracy(self, label, pred):\n        hit = 0.\n        total = 0.\n        batch_size = label.shape[0]\n        for i in range(batch_size):\n            l = self._remove_blank(label[i])\n            p = []\n            for k in range(self.seq_len):\n                p.append(np.argmax(pred[k * batch_size + i]))\n            p = self.ctc_label(p)\n            if len(p) == len(l):\n                match = True\n                for k, _ in enumerate(p):\n                    if p[k] != int(l[k]):\n                        match = False\n                        break\n                if match:\n                    hit += 1.0\n            total += 1.0\n        assert total == batch_size\n        return hit / total", "response": "Simple accuracy measure: number of 100% accurate predictions divided by total number"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_movielens_iter(filename, batch_size):\n    logging.info(\"Preparing data iterators for \" + filename + \" ... \")\n    user = []\n    item = []\n    score = []\n    with open(filename, 'r') as f:\n        num_samples = 0\n        for line in f:\n            tks = line.strip().split('::')\n            if len(tks) != 4:\n                continue\n            num_samples += 1\n            user.append((tks[0]))\n            item.append((tks[1]))\n            score.append((tks[2]))\n    # convert to ndarrays\n    user = mx.nd.array(user, dtype='int32')\n    item = mx.nd.array(item)\n    score = mx.nd.array(score)\n    # prepare data iters\n    data_train = {'user': user, 'item': item}\n    label_train = {'score': score}\n    iter_train = mx.io.NDArrayIter(data=data_train,label=label_train,\n                                   batch_size=batch_size, shuffle=True)\n    return mx.io.PrefetchingIter(iter_train)", "response": "This function is used to get data iterators for the movielens dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef imdecode(str_img, flag=1):\n    hdl = NDArrayHandle()\n    check_call(_LIB.MXCVImdecode(ctypes.c_char_p(str_img),\n                                 mx_uint(len(str_img)),\n                                 flag, ctypes.byref(hdl)))\n    return mx.nd.NDArray(hdl)", "response": "Decode image from str buffer."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndecodes image from str buffer.", "response": "def resize(src, size, interpolation=cv2.INTER_LINEAR):\n    \"\"\"Decode image from str buffer.\n    Wrapper for cv2.imresize that uses mx.nd.NDArray\n\n    Parameters\n    ----------\n    src : NDArray\n        image in (width, height, channels)\n    size : tuple\n        target size in (width, height)\n    interpolation : int\n        same as interpolation for cv2.imresize\n\n    Returns\n    -------\n    img : NDArray\n        resized image\n    \"\"\"\n    hdl = NDArrayHandle()\n    check_call(_LIB.MXCVResize(src.handle, mx_uint(size[0]), mx_uint(size[1]),\n                               interpolation, ctypes.byref(hdl)))\n    return mx.nd.NDArray(hdl)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npadding image border with border type.", "response": "def copyMakeBorder(src, top, bot, left, right, border_type=cv2.BORDER_CONSTANT, value=0):\n    \"\"\"Pad image border\n    Wrapper for cv2.copyMakeBorder that uses mx.nd.NDArray\n\n    Parameters\n    ----------\n    src : NDArray\n        Image in (width, height, channels).\n        Others are the same with cv2.copyMakeBorder\n\n    Returns\n    -------\n    img : NDArray\n        padded image\n    \"\"\"\n    hdl = NDArrayHandle()\n    check_call(_LIB.MXCVcopyMakeBorder(src.handle, ctypes.c_int(top), ctypes.c_int(bot),\n                                       ctypes.c_int(left), ctypes.c_int(right),\n                                       ctypes.c_int(border_type), ctypes.c_double(value),\n                                       ctypes.byref(hdl)))\n    return mx.nd.NDArray(hdl)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fixed_crop(src, x0, y0, w, h, size=None, interpolation=cv2.INTER_CUBIC):\n    out = mx.nd.crop(src, begin=(y0, x0, 0), end=(y0+h, x0+w, int(src.shape[2])))\n    if size is not None and (w, h) != size:\n        out = resize(out, size, interpolation=interpolation)\n    return out", "response": "Crop src at fixed location and resize it to size"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef random_crop(src, size):\n    h, w, _ = src.shape\n    new_w, new_h = scale_down((w, h), size)\n\n    x0 = random.randint(0, w - new_w)\n    y0 = random.randint(0, h - new_h)\n\n    out = fixed_crop(src, x0, y0, new_w, new_h, size)\n    return out, (x0, y0, new_w, new_h)", "response": "Randomly crop src with size. Upsample result with src is smaller than size."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmove iterator position forward", "response": "def next(self):\n        \"\"\"Move iterator position forward\"\"\"\n        batch = mx.nd.zeros((self.batch_size, self.size[1], self.size[0], 3))\n        i = self.cur\n        for i in range(self.cur, min(len(self.list), self.cur+self.batch_size)):\n            str_img = open(self.root+self.list[i]+'.jpg').read()\n            img = imdecode(str_img, 1)\n            img, _ = random_crop(img, self.size)\n            batch[i - self.cur] = img\n        batch = mx.nd.transpose(batch, axes=(0, 3, 1, 2))\n        ret = mx.io.DataBatch(data=[batch],\n                              label=[],\n                              pad=self.batch_size-(i-self.cur),\n                              index=None)\n        self.cur = i\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_label_shapes(labels, preds, shape=0):\n\n    if shape == 0:\n        label_shape, pred_shape = len(labels), len(preds)\n    else:\n        label_shape, pred_shape = labels.shape, preds.shape\n\n    if label_shape != pred_shape:\n        raise ValueError(\"Shape of labels {} does not match shape of \"\n                         \"predictions {}\".format(label_shape, pred_shape))", "response": "Check to see if the two arrays are the same size."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef import_to_gluon(model_file, ctx):\n    graph = GraphProto()\n    try:\n        import onnx\n    except ImportError:\n        raise ImportError(\"Onnx and protobuf need to be installed. Instructions to\"\n                          + \" install - https://github.com/onnx/onnx#installation\")\n    model_proto = onnx.load_model(model_file)\n    net = graph.graph_to_gluon(model_proto.graph, ctx)\n    return net", "response": "Imports the ONNX model file and loads it into Gluon SymbolBlock object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the learning rate for the next N epochs.", "response": "def update_learning_rate(lr, trainer, epoch, ratio, steps):\n    \"\"\"Set the learning rate to the initial value decayed by ratio every N epochs.\"\"\"\n    new_lr = lr * (ratio ** int(np.sum(np.array(steps) < epoch)))\n    trainer.set_learning_rate(new_lr)\n    return trainer"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef seed(seed_state, ctx=\"all\"):\n    if not isinstance(seed_state, integer_types):\n        raise ValueError('seed_state must be int')\n    seed_state = ctypes.c_int(int(seed_state))\n    if ctx == \"all\":\n        check_call(_LIB.MXRandomSeed(seed_state))\n    else:\n        ctx = Context(ctx)\n        check_call(_LIB.MXRandomSeedContext(seed_state, ctx.device_typeid, ctx.device_id))", "response": "Seeds the random number generators in MXNet."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndrawing random samples from a uniform distribtuion.", "response": "def random_uniform(attrs, inputs, proto_obj):\n    \"\"\"Draw random samples from a uniform distribtuion.\"\"\"\n    try:\n        from onnx.mapping import TENSOR_TYPE_TO_NP_TYPE\n    except ImportError:\n        raise ImportError(\"Onnx and protobuf need to be installed. \"\n                          \"Instructions to install - https://github.com/onnx/onnx\")\n    new_attrs = translation_utils._remove_attributes(attrs, ['seed'])\n    new_attrs['dtype'] = TENSOR_TYPE_TO_NP_TYPE[int(new_attrs.get('dtype', 1))]\n    return 'random_uniform', new_attrs, inputs"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndrawing random samples from a Gaussian distribution.", "response": "def random_normal(attrs, inputs, proto_obj):\n    \"\"\"Draw random samples from a Gaussian distribution.\"\"\"\n    try:\n        from onnx.mapping import TENSOR_TYPE_TO_NP_TYPE\n    except ImportError:\n        raise ImportError(\"Onnx and protobuf need to be installed. \"\n                          \"Instructions to install - https://github.com/onnx/onnx\")\n    new_attr = translation_utils._remove_attributes(attrs, ['seed'])\n    new_attr = translation_utils._fix_attribute_names(new_attr, {'mean': 'loc'})\n    new_attr['dtype'] = TENSOR_TYPE_TO_NP_TYPE[int(new_attr.get('dtype', 1))]\n    return 'random_normal', new_attr, inputs"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add(attrs, inputs, proto_obj):\n    new_attr = {}\n\n    if 'broadcast' in attrs and attrs['broadcast'] == 1:\n        broadcast_axis = attrs['axis']\n        op_value = translation_utils._fix_broadcast('broadcast_add', inputs,\n                                                    broadcast_axis, proto_obj)\n        return op_value, new_attr, inputs\n    return 'broadcast_add', new_attr, inputs", "response": "Adds two tensors to the input list"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmeans of all the input tensors.", "response": "def mean(attrs, inputs, proto_obj):\n    \"\"\"Mean of all the input tensors.\"\"\"\n    concat_input = [symbol.expand_dims(op_input, axis=0) for op_input in inputs]\n    concat_sym = symbol.concat(*concat_input, dim=0)\n    mean_sym = symbol.mean(concat_sym, axis=0)\n    return mean_sym, attrs, inputs"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns indices of the maximum values along an axis", "response": "def argmax(attrs, inputs, proto_obj):\n    \"\"\"Returns indices of the maximum values along an axis\"\"\"\n    axis = attrs.get('axis', 0)\n    keepdims = attrs.get('keepdims', 1)\n    argmax_op = symbol.argmax(inputs[0], axis=axis, keepdims=keepdims)\n    # onnx argmax operator always expects int64 as output type\n    cast_attrs = {'dtype': 'int64'}\n    return 'cast', cast_attrs, argmax_op"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns indices of the minimum values along an axis.", "response": "def argmin(attrs, inputs, proto_obj):\n    \"\"\"Returns indices of the minimum values along an axis.\"\"\"\n    axis = attrs.get('axis', 0)\n    keepdims = attrs.get('keepdims', 1)\n    argmin_op = symbol.argmin(inputs[0], axis=axis, keepdims=keepdims)\n    # onnx argmax operator always expects int64 as output type\n    cast_attrs = {'dtype': 'int64'}\n    return 'cast', cast_attrs, argmin_op"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef maximum(attrs, inputs, proto_obj):\n    if len(inputs) > 1:\n        mxnet_op = symbol.maximum(inputs[0], inputs[1])\n        for op_input in inputs[2:]:\n            mxnet_op = symbol.maximum(mxnet_op, op_input)\n    else:\n        mxnet_op = symbol.maximum(inputs[0], inputs[0])\n    return mxnet_op, attrs, inputs", "response": "Elementwise maximum of arrays."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef concat(attrs, inputs, proto_obj):\n    new_attrs = translation_utils._fix_attribute_names(attrs, {'axis': 'dim'})\n    return 'concat', new_attrs, inputs", "response": "Concatenates input arrays along a given axis."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pad(attrs, inputs, proto_obj):\n    new_attrs = translation_utils._fix_attribute_names(attrs, {'pads'  : 'pad_width',\n                                                               'value' : 'constant_value'\n                                                              })\n    new_attrs['pad_width'] = translation_utils._pad_sequence_fix(new_attrs.get('pad_width'))\n    return 'pad', new_attrs, inputs", "response": "Adds padding to input tensor"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef softplus(attrs, inputs, proto_obj):\n    new_attrs = translation_utils._add_extra_attributes(attrs, {'act_type' : 'softrelu'})\n    return 'Activation', new_attrs, inputs", "response": "Applies the sofplus activation function element - wise to the input."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef conv(attrs, inputs, proto_obj):\n    new_attrs = translation_utils._fix_attribute_names(attrs, {'kernel_shape' : 'kernel',\n                                                               'strides' : 'stride',\n                                                               'pads': 'pad',\n                                                               'dilations': 'dilate',\n                                                               'group': 'num_group'})\n    new_attrs = translation_utils._add_extra_attributes(new_attrs, {'num_group' : 1})\n    new_attrs = translation_utils._fix_bias('Convolution', new_attrs, len(inputs))\n\n    new_attrs = translation_utils._fix_channels('Convolution', new_attrs, inputs, proto_obj)\n    kernel = new_attrs['kernel']\n    stride = new_attrs['stride'] if 'stride' in new_attrs else []\n    padding = new_attrs['pad'] if 'pad' in new_attrs else []\n    dilations = new_attrs['dilate'] if 'dilate' in new_attrs else []\n    num_filter = new_attrs['num_filter']\n    num_group = new_attrs['num_group']\n    no_bias = new_attrs['no_bias'] if 'no_bias' in new_attrs else 0\n    bias = None if no_bias is True else inputs[2]\n\n    # Unlike ONNX, MXNet's convolution operator does not support asymmetric padding, so we first\n    # use 'Pad' operator, which supports asymmetric padding. Then use the convolution operator.\n    pad_width = (0, 0, 0, 0) + translation_utils._pad_sequence_fix(padding, kernel_dim=len(kernel))\n    pad_op = symbol.pad(inputs[0], mode='constant', pad_width=pad_width)\n\n    conv_op = symbol.Convolution(pad_op, inputs[1], bias,\n                                 kernel=kernel, stride=stride, dilate=dilations,\n                                 num_filter=num_filter, num_group=num_group, no_bias=no_bias)\n\n    return conv_op, new_attrs, inputs", "response": "Compute N - D convolution on input."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef deconv(attrs, inputs, proto_obj):\n    new_attrs = translation_utils._fix_attribute_names(attrs, {'kernel_shape' : 'kernel',\n                                                               'strides' : 'stride',\n                                                               'pads': 'pad',\n                                                               'dilations': 'dilate',\n                                                               'group': 'num_group'})\n    new_attrs = translation_utils._add_extra_attributes(new_attrs, {'num_group' : 1})\n    new_attrs = translation_utils._fix_bias('Deconvolution', new_attrs, len(inputs))\n\n    new_attrs = translation_utils._fix_channels('Deconvolution', new_attrs, inputs, proto_obj)\n    kernel = new_attrs['kernel']\n    stride = new_attrs['stride'] if 'stride' in new_attrs else []\n    padding = new_attrs['pad'] if 'pad' in new_attrs else []\n    dilations = new_attrs['dilate'] if 'dilate' in new_attrs else []\n    num_filter = new_attrs['num_filter']\n    num_group = new_attrs['num_group']\n    no_bias = new_attrs['no_bias'] if 'no_bias' in new_attrs else False\n    bias = None if no_bias is True else inputs[2]\n\n    # Unlike ONNX, MXNet's deconvolution operator does not support asymmetric padding, so we first\n    # use 'Pad' operator, which supports asymmetric padding. Then use the deconvolution operator.\n    pad_width = (0, 0, 0, 0) + translation_utils._pad_sequence_fix(padding, kernel_dim=len(kernel))\n    pad_op = symbol.pad(inputs[0], mode='constant', pad_width=pad_width)\n\n    deconv_op = symbol.Deconvolution(pad_op, inputs[1], bias,\n                                     kernel=kernel, stride=stride, dilate=dilations,\n                                     num_filter=num_filter, num_group=num_group, no_bias=no_bias)\n\n    return deconv_op, new_attrs, inputs", "response": "Computes the transposed convolution of the input tensor."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fully_connected(attrs, inputs, proto_obj):\n    new_attrs = translation_utils._remove_attributes(attrs, ['axis'])\n\n    new_attrs = translation_utils._fix_bias('FullyConnected', new_attrs, len(inputs))\n\n    new_attrs = translation_utils._fix_channels('FullyConnected', new_attrs, inputs, proto_obj)\n\n    return 'FullyConnected', new_attrs, inputs", "response": "Applies a linear transformation Y = XWT + b."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef global_maxpooling(attrs, inputs, proto_obj):\n    new_attrs = translation_utils._add_extra_attributes(attrs, {'global_pool': True,\n                                                                'kernel': (1, 1),\n                                                                'pool_type': 'max'})\n    return 'Pooling', new_attrs, inputs", "response": "Performs max pooling on the input."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef global_avgpooling(attrs, inputs, proto_obj):\n    new_attrs = translation_utils._add_extra_attributes(attrs, {'global_pool': True,\n                                                                'kernel': (1, 1),\n                                                                'pool_type': 'avg'})\n    return 'Pooling', new_attrs, inputs", "response": "Performs avg pooling on the input."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nperforms global lp pooling on the input.", "response": "def global_lppooling(attrs, inputs, proto_obj):\n    \"\"\"Performs global lp pooling on the input.\"\"\"\n    p_value = attrs.get('p', 2)\n    new_attrs = translation_utils._add_extra_attributes(attrs, {'global_pool': True,\n                                                                'kernel': (1, 1),\n                                                                'pool_type': 'lp',\n                                                                'p_value': p_value})\n    new_attrs = translation_utils._remove_attributes(new_attrs, ['p'])\n    return 'Pooling', new_attrs, inputs"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef linalg_gemm(attrs, inputs, proto_obj):\n    trans_a = 0\n    trans_b = 0\n    alpha = 1\n    beta = 1\n    if 'transA' in attrs:\n        trans_a = attrs['transA']\n    if 'transB' in attrs:\n        trans_b = attrs['transB']\n    if 'alpha' in attrs:\n        alpha = attrs['alpha']\n    if 'beta' in attrs:\n        beta = attrs['beta']\n    flatten_a = symbol.flatten(inputs[0])\n    matmul_op = symbol.linalg_gemm2(A=flatten_a, B=inputs[1],\n                                    transpose_a=trans_a, transpose_b=trans_b,\n                                    alpha=alpha)\n    gemm_op = symbol.broadcast_add(matmul_op, beta*inputs[2])\n    new_attrs = translation_utils._fix_attribute_names(attrs, {'transA': 'transpose_a',\n                                                               'transB': 'transpose_b'})\n    new_attrs = translation_utils._remove_attributes(new_attrs, ['broadcast'])\n    return gemm_op, new_attrs, inputs", "response": "Performs general matrix multiplication and accumulation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncasting input to a given dtype", "response": "def cast(attrs, inputs, proto_obj):\n    \"\"\" Cast input to a given dtype\"\"\"\n    try:\n        from onnx.mapping import TENSOR_TYPE_TO_NP_TYPE\n    except ImportError:\n        raise ImportError(\"Onnx and protobuf need to be installed. \"\n                          + \"Instructions to install - https://github.com/onnx/onnx\")\n    new_attrs = translation_utils._fix_attribute_names(attrs, {'to' : 'dtype'})\n    new_attrs['dtype'] = TENSOR_TYPE_TO_NP_TYPE[int(new_attrs['dtype'])]\n    return 'cast', new_attrs, inputs"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef split(attrs, inputs, proto_obj):\n    split_list = attrs.get('split') if 'split' in attrs else []\n    new_attrs = translation_utils._fix_attribute_names(attrs,\n                                                       {'split' : 'num_outputs'})\n    if 'axis' not in attrs:\n        new_attrs = translation_utils._add_extra_attributes(new_attrs, {'axis': 0})\n\n    if not split_list:\n        num_outputs = len(proto_obj.model_metadata.get('output_tensor_data'))\n    else:\n        if len(set(split_list)) == 1:\n            num_outputs = len(split_list)\n        else:\n            raise NotImplementedError(\"Operator {} in MXNet does not support variable splits.\"\n                                      \"Tracking the issue to support variable split here: \"\n                                      \"https://github.com/apache/incubator-mxnet/issues/11594\"\n                                      .format('split'))\n\n    new_attrs['num_outputs'] = num_outputs\n    return 'split', new_attrs, inputs", "response": "Splits an array along a particular axis into multiple sub - arrays."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _slice(attrs, inputs, proto_obj):\n    new_attrs = translation_utils._fix_attribute_names(attrs,\n                                                       {'axes' : 'axis',\n                                                        'ends' : 'end',\n                                                        'starts' : 'begin'})\n    # onnx slice provides slicing on multiple axis. Adding multiple slice_axis operator\n    # for multiple axes from mxnet\n    begin = new_attrs.get('begin')\n    end = new_attrs.get('end')\n    axes = new_attrs.get('axis', tuple(range(len(begin))))\n    slice_op = symbol.slice_axis(inputs[0], axis=axes[0], begin=begin[0], end=end[0])\n    if len(axes) > 1:\n        for i, axis in enumerate(axes):\n            slice_op = symbol.slice_axis(slice_op, axis=axis, begin=begin[i], end=end[i])\n    return slice_op, new_attrs, inputs", "response": "Returns a slice of the input tensor along multiple axes."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef transpose(attrs, inputs, proto_obj):\n    new_attrs = translation_utils._fix_attribute_names(attrs,\n                                                       {'perm' : 'axes'})\n    return 'transpose', new_attrs, inputs", "response": "Transposes the input array."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove single - dimensional entries from the shape of a tensor.", "response": "def squeeze(attrs, inputs, proto_obj):\n    \"\"\"Remove single-dimensional entries from the shape of a tensor.\"\"\"\n    new_attrs = translation_utils._fix_attribute_names(attrs,\n                                                       {'axes' : 'axis'})\n    return 'squeeze', new_attrs, inputs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unsqueeze(attrs, inputs, cls):\n    # MXNet can only add one axis at a time.\n    mxnet_op = inputs[0]\n    for axis in attrs[\"axes\"]:\n        mxnet_op = symbol.expand_dims(mxnet_op, axis=axis)\n\n    return mxnet_op, attrs, inputs", "response": "Removes a new axis of size 1 into the array shape."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef flatten(attrs, inputs, proto_obj):\n    #Mxnet does not have axis support. By default uses axis=1\n    if 'axis' in attrs and attrs['axis'] != 1:\n        raise RuntimeError(\"Flatten operator only supports axis=1\")\n    new_attrs = translation_utils._remove_attributes(attrs, ['axis'])\n    return 'Flatten', new_attrs, inputs", "response": "Flattens the input array into a 2 - D array by collapsing the higher dimensions."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clip(attrs, inputs, proto_obj):\n    new_attrs = translation_utils._fix_attribute_names(attrs, {'min' : 'a_min',\n                                                               'max' : 'a_max'})\n    if 'a_max' not in new_attrs:\n        new_attrs = translation_utils._add_extra_attributes(new_attrs, {'a_max' : np.inf})\n    if 'a_min' not in new_attrs:\n        new_attrs = translation_utils._add_extra_attributes(new_attrs, {'a_min' : -np.inf})\n    return 'clip', new_attrs, inputs", "response": "Clips the values in an array."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning element - wise result of base element raised to powers from exp element.", "response": "def power(attrs, inputs, proto_obj):\n    \"\"\"Returns element-wise result of base element raised to powers from exp element.\"\"\"\n    new_attrs = translation_utils._fix_attribute_names(attrs, {'exponent':'exp'})\n    if 'broadcast' in attrs:\n        new_attrs = translation_utils._remove_attributes(new_attrs, ['broadcast'])\n        if attrs['broadcast'] == 1:\n            return 'broadcast_power', new_attrs, inputs\n        else:\n            mxnet_op = symbol.pow(inputs[0], inputs[1])\n            return mxnet_op, new_attrs, inputs\n    mxnet_op = symbol.broadcast_power(inputs[0], inputs[1])\n    return mxnet_op, new_attrs, inputs"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reduce_max(attrs, inputs, proto_obj):\n    new_attrs = translation_utils._fix_attribute_names(attrs, {'axes':'axis'})\n    return 'max', new_attrs, inputs", "response": "Reduce the array along a given axis by maximum value."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reduce_mean(attrs, inputs, proto_obj):\n    new_attrs = translation_utils._fix_attribute_names(attrs, {'axes':'axis'})\n    return 'mean', new_attrs, inputs", "response": "Reduce the array along a given axis by mean value."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreduces the array along a given axis by minimum value.", "response": "def reduce_min(attrs, inputs, proto_obj):\n    \"\"\"Reduce the array along a given axis by minimum value\"\"\"\n    new_attrs = translation_utils._fix_attribute_names(attrs, {'axes':'axis'})\n    return 'min', new_attrs, inputs"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reduce_sum(attrs, inputs, proto_obj):\n    new_attrs = translation_utils._fix_attribute_names(attrs, {'axes':'axis'})\n    return 'sum', new_attrs, inputs", "response": "Reduce the array along a given axis by sum value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef reduce_prod(attrs, inputs, proto_obj):\n    new_attrs = translation_utils._fix_attribute_names(attrs, {'axes':'axis'})\n    return 'prod', new_attrs, inputs", "response": "Reduce the array along a given axis by product value."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreduces the array along a given axis by log sum value", "response": "def reduce_log_sum(attrs, inputs, proto_obj):\n    \"\"\"Reduce the array along a given axis by log sum value\"\"\"\n    keep_dims = True if 'keepdims' not in attrs else attrs.get('keepdims')\n    sum_op = symbol.sum(inputs[0], axis=attrs.get('axes'),\n                        keepdims=keep_dims)\n    log_sym = symbol.log(sum_op)\n    return log_sym, attrs, inputs"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reduce_log_sum_exp(attrs, inputs, proto_obj):\n    keep_dims = True if 'keepdims' not in attrs else attrs.get('keepdims')\n    exp_op = symbol.exp(inputs[0])\n    sum_op = symbol.sum(exp_op, axis=attrs.get('axes'),\n                        keepdims=keep_dims)\n    log_sym = symbol.log(sum_op)\n    return log_sym, attrs, inputs", "response": "Reduce the array along a given axis by log sum exp value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreduce the array along a given axis by sum square value", "response": "def reduce_sum_square(attrs, inputs, proto_obj):\n    \"\"\"Reduce the array along a given axis by sum square value\"\"\"\n    square_op = symbol.square(inputs[0])\n    sum_op = symbol.sum(square_op, axis=attrs.get('axes'),\n                        keepdims=attrs.get('keepdims'))\n    return sum_op, attrs, inputs"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreduces input tensor by l1 normalization.", "response": "def reduce_l1(attrs, inputs, proto_obj):\n    \"\"\"Reduce input tensor by l1 normalization.\"\"\"\n    new_attrs = translation_utils._fix_attribute_names(attrs, {'axes':'axis'})\n    new_attrs = translation_utils._add_extra_attributes(new_attrs,\n                                                        {'ord' : 1})\n    return 'norm', new_attrs, inputs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreducing input tensor by l2 normalization.", "response": "def reduce_l2(attrs, inputs, proto_obj):\n    \"\"\"Reduce input tensor by l2 normalization.\"\"\"\n    new_attrs = translation_utils._fix_attribute_names(attrs, {'axes':'axis'})\n    return 'norm', new_attrs, inputs"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef avg_pooling(attrs, inputs, proto_obj):\n    new_attrs = translation_utils._fix_attribute_names(attrs,\n                                                       {'kernel_shape': 'kernel',\n                                                        'strides': 'stride',\n                                                        'pads': 'pad',\n                                                       })\n    new_attrs = translation_utils._add_extra_attributes(new_attrs,\n                                                        {'pooling_convention': 'valid'\n                                                        })\n    new_op = translation_utils._fix_pooling('avg', inputs, new_attrs)\n\n    return new_op, new_attrs, inputs", "response": "Average pooling of a single resource."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrearrange data from depth into blocks of spatial data.", "response": "def depthtospace(attrs, inputs, proto_obj):\n    \"\"\"Rearranges data from depth into blocks of spatial data.\"\"\"\n    new_attrs = translation_utils._fix_attribute_names(attrs, {'blocksize':'block_size'})\n\n    return \"depth_to_space\", new_attrs, inputs"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef spacetodepth(attrs, inputs, proto_obj):\n    new_attrs = translation_utils._fix_attribute_names(attrs, {'blocksize':'block_size'})\n\n    return \"space_to_depth\", new_attrs, inputs", "response": "Rearranges blocks of spatial data into depth."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef hardmax(attrs, inputs, proto_obj):\n    input_tensor_data = proto_obj.model_metadata.get('input_tensor_data')[0]\n    input_shape = input_tensor_data[1]\n\n    axis = int(attrs.get('axis', 1))\n    axis = axis if axis >= 0 else len(input_shape) + axis\n\n    if axis == len(input_shape) - 1:\n        amax = symbol.argmax(inputs[0], axis=-1)\n        one_hot = symbol.one_hot(amax, depth=input_shape[-1])\n        return one_hot, attrs, inputs\n\n    # since reshape doesn't take a tensor for shape,\n    # computing with np.prod. This needs to be changed to\n    # to use mx.sym.prod() when mx.sym.reshape() is fixed.\n    # (https://github.com/apache/incubator-mxnet/issues/10789)\n    new_shape = (int(np.prod(input_shape[:axis])),\n                 int(np.prod(input_shape[axis:])))\n    reshape_op = symbol.reshape(inputs[0], new_shape)\n    amax = symbol.argmax(reshape_op, axis=-1)\n    one_hot = symbol.one_hot(amax, depth=new_shape[-1])\n    hardmax_op = symbol.reshape(one_hot, input_shape)\n    return hardmax_op, attrs, inputs", "response": "Returns batched one - hot vectors."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndownloading the aligns from from_idx to to_idx", "response": "def download_align(from_idx, to_idx, _params):\n    \"\"\"\n    download aligns\n    \"\"\"\n    succ = set()\n    fail = set()\n    for idx in range(from_idx, to_idx):\n        name = 's' + str(idx)\n        if idx == 0:\n            continue\n        script = \"http://spandh.dcs.shef.ac.uk/gridcorpus/{nm}/align/{nm}.tar\".format(nm=name)\n        down_sc = 'cd {align_path} && wget {script} && \\\n                    tar -xvf {nm}.tar'.format(script=script,\n                                              nm=name,\n                                              align_path=_params['align_path'])\n        try:\n            print(down_sc)\n            os.system(down_sc)\n            succ.add(idx)\n        except OSError as error:\n            print(error)\n            fail.add(idx)\n    return (succ, fail)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run_ut_py3_qemu():\n    from vmcontrol import VM\n    with VM() as vm:\n        qemu_provision(vm.ssh_port)\n        logging.info(\"execute tests\")\n        qemu_ssh(vm.ssh_port, \"./runtime_functions.py\", \"run_ut_python3_qemu_internal\")\n        qemu_rsync_to_host(vm.ssh_port, \"*.xml\", \"mxnet\")\n        logging.info(\"copied to host\")\n        logging.info(\"tests finished, vm shutdown.\")\n        vm.shutdown()", "response": "Run unit tests in the emulator and copy the results back to the host through the vmcontrol module"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of subword - units presentation given a word and gram.", "response": "def _get_subword_units(token, gram):\n    \"\"\"Return subword-units presentation, given a word/token.\n    \"\"\"\n    if token == '</s>':  # special token for padding purpose.\n        return [token]\n    t = '#' + token + '#'\n    return [t[i:i + gram] for i in range(0, len(t) - gram + 1)]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fit(args, network, data_loader, eval_metrics=None, batch_end_callback=None):\n    # kvstore\n    kv = mx.kvstore.create(args.kv_store)\n\n    # logging\n    head = '%(asctime)-15s Node[' + str(kv.rank) + '] %(message)s'\n    if 'log_file' in args and args.log_file is not None:\n        log_file = args.log_file\n        log_dir = args.log_dir\n        log_file_full_name = os.path.join(log_dir, log_file)\n        if not os.path.exists(log_dir):\n            os.mkdir(log_dir)\n        logger = logging.getLogger()\n        handler = logging.FileHandler(log_file_full_name)\n        formatter = logging.Formatter(head)\n        handler.setFormatter(formatter)\n        logger.addHandler(handler)\n        logger.setLevel(logging.DEBUG)\n        logger.info('start with arguments %s', args)\n    else:\n        logging.basicConfig(level=logging.DEBUG, format=head)\n        logging.info('start with arguments %s', args)\n\n    # load model\n    model_prefix = args.model_prefix\n    if model_prefix is not None:\n        model_prefix += \"-%d\" % (kv.rank)\n    model_args = {}\n    if args.load_epoch is not None:\n        assert model_prefix is not None\n        tmp = mx.model.FeedForward.load(model_prefix, args.load_epoch)\n        model_args = {'arg_params' : tmp.arg_params,\n                      'aux_params' : tmp.aux_params,\n                      'begin_epoch' : args.load_epoch}\n    # save model\n    save_model_prefix = args.save_model_prefix\n    if save_model_prefix is None:\n        save_model_prefix = model_prefix\n    checkpoint = None if save_model_prefix is None else mx.callback.do_checkpoint(save_model_prefix)\n\n    # data\n    (train, val) = data_loader(args, kv)\n\n    # train\n    devs = mx.cpu() if args.gpus is None else [\n        mx.gpu(int(i)) for i in args.gpus.split(',')]\n\n    epoch_size = args.num_examples / args.batch_size\n\n    if args.kv_store == 'dist_sync':\n        epoch_size /= kv.num_workers\n        model_args['epoch_size'] = epoch_size\n\n    if 'lr_factor' in args and args.lr_factor < 1:\n        model_args['lr_scheduler'] = mx.lr_scheduler.FactorScheduler(\n            step=max(int(epoch_size * args.lr_factor_epoch), 1),\n            factor=args.lr_factor)\n\n    if 'clip_gradient' in args and args.clip_gradient is not None:\n        model_args['clip_gradient'] = args.clip_gradient\n\n    # disable kvstore for single device\n    if 'local' in kv.type and (\n            args.gpus is None or len(args.gpus.split(',')) is 1):\n        kv = None\n\n    mod = mx.mod.Module(network, context=devs)\n\n    if eval_metrics is None:\n        eval_metrics = ['accuracy']\n        # TopKAccuracy only allows top_k > 1\n        for top_k in [5, 10, 20]:\n            eval_metrics.append(mx.metric.create('top_k_accuracy', top_k=top_k))\n\n    if batch_end_callback is not None:\n        if not isinstance(batch_end_callback, list):\n            batch_end_callback = [batch_end_callback]\n    else:\n        batch_end_callback = []\n    batch_end_callback.append(mx.callback.Speedometer(args.batch_size, 50))\n\n    mod.fit(train_data=train, eval_metric=eval_metrics, eval_data=val, optimizer='sgd',\n            optimizer_params={'learning_rate':args.lr, 'momentum': 0.9, 'wd': 0.00001},\n            num_epoch=args.num_epochs, batch_end_callback=batch_end_callback,\n            initializer=mx.init.Xavier(factor_type=\"in\", magnitude=2.34),\n            kvstore=kv, epoch_end_callback=checkpoint, **model_args)", "response": "Train the model using Caffe operator in MXNet"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _new_empty_handle():\n    hdl = NDArrayHandle()\n    check_call(_LIB.MXNDArrayCreateNone(ctypes.byref(hdl)))\n    return hdl", "response": "Returns a new empty handle."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _new_alloc_handle(shape, ctx, delay_alloc, dtype=mx_real_t):\n    hdl = NDArrayHandle()\n    check_call(_LIB.MXNDArrayCreateEx(\n        c_array_buf(mx_uint, native_array('I', shape)),\n        mx_uint(len(shape)),\n        ctypes.c_int(ctx.device_typeid),\n        ctypes.c_int(ctx.device_id),\n        ctypes.c_int(int(delay_alloc)),\n        ctypes.c_int(int(_DTYPE_NP_TO_MX[np.dtype(dtype).type])),\n        ctypes.byref(hdl)))\n    return hdl", "response": "Return a new handle with specified shape and context."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_indexing_dispatch_code(key):\n    if isinstance(key, (NDArray, np.ndarray)):\n        return _NDARRAY_ADVANCED_INDEXING\n    elif isinstance(key, list):\n        # TODO(junwu): Add support for nested lists besides integer list\n        for i in key:\n            if not isinstance(i, integer_types):\n                raise TypeError('Indexing NDArray only supports a list of integers as index'\n                                ' when key is of list type, received element=%s of type=%s'\n                                % (str(i), str(type(i))))\n        return _NDARRAY_ADVANCED_INDEXING\n    elif isinstance(key, (integer_types, py_slice)):\n        return _NDARRAY_BASIC_INDEXING\n    elif isinstance(key, tuple):\n        for idx in key:\n            if isinstance(idx, (NDArray, np.ndarray, list, tuple)):\n                return _NDARRAY_ADVANCED_INDEXING\n            elif not isinstance(idx, (py_slice, integer_types)):\n                raise ValueError(\"NDArray does not support slicing with key %s of type %s.\"\n                                 % (str(idx), str(type(idx))))\n        return _NDARRAY_BASIC_INDEXING\n    else:\n        return _NDARRAY_UNSUPPORTED_INDEXING", "response": "Returns a dispatch code for calling basic or advanced indexing functions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive start stop and step and array length return absolute values of start stop and step for generating index range.", "response": "def _get_index_range(start, stop, length, step=1):\n    \"\"\"Given start, stop, step and array length, return\n    absolute values of start, stop, and step for generating index range.\n    The returned values have been compensated by adding length if they\n    are less than zero for all the cases but slice(None, None, -1).\n    Note that the returned value of stop is not necessarily >= 0, since\n    absolute stop is -1 in the case of slice(None, None, -1).\"\"\"\n    if step == 0:\n        raise ValueError('step size cannot be zero')\n    if length < 0:\n        raise ValueError('array length cannot be less than zero')\n    if step is None:\n        step = 1\n    if start is None:\n        if step > 0:\n            start = 0\n        else:\n            start = length - 1\n    elif start < 0:\n        start += length\n        if start < 0:\n            raise IndexError('Slicing start %d exceeds limit of %d' % (start-length, length))\n    elif start >= length:\n        raise IndexError('Slicing start %d exceeds limit of %d' % (start, length))\n\n    if stop is None:\n        if step > 0:\n            stop = length\n        else:\n            # this supports case such as ::-1\n            # stop = -1 here refers to the element before index 0,\n            # instead of the last element in the array\n            stop = -1\n    elif stop < 0:\n        stop += length\n        if stop < 0:\n            raise IndexError('Slicing stop %d exceeds limit of %d' % (stop-length, length))\n    elif stop > length:\n        raise IndexError('Slicing stop %d exceeds limit of %d' % (stop, length))\n\n    return start, stop, step"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive data and index shapes get the output NDArray shape.", "response": "def _get_oshape_of_gather_nd_op(dshape, ishape):\n    \"\"\"Given data and index shapes, get the output `NDArray` shape.\n    This basically implements the infer shape logic of op gather_nd.\"\"\"\n    assert len(dshape) > 0 and len(ishape) > 0\n    oshape = list(ishape[1:])\n    if ishape[0] < len(dshape):\n        oshape.extend(dshape[ishape[0]:])\n    return tuple(oshape)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngiving start stop and step calculate the number of elements of this slice.", "response": "def _get_dim_size(start, stop, step):\n    \"\"\"Given start, stop, and stop, calculate the number of elements\n    of this slice.\"\"\"\n    assert step != 0\n    if step > 0:\n        assert start < stop\n        dim_size = (stop - start - 1) // step + 1\n    else:\n        assert stop < start\n        dim_size = (start - stop - 1) // (-step) + 1\n    return dim_size"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_broadcast_shape(shape1, shape2):\n    if shape1 == shape2:\n        return shape1\n\n    length1 = len(shape1)\n    length2 = len(shape2)\n    if length1 > length2:\n        shape = list(shape1)\n    else:\n        shape = list(shape2)\n    i = max(length1, length2) - 1\n    for a, b in zip(shape1[::-1], shape2[::-1]):\n        if a != 1 and b != 1 and a != b:\n            raise ValueError('shape1=%s is not broadcastable to shape2=%s' % (shape1, shape2))\n        shape[i] = max(a, b)\n        i -= 1\n    return tuple(shape)", "response": "Given two shapes that are not identical find the shape that both input shapes can broadcast to."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ones(shape, ctx=None, dtype=None, **kwargs):\n    # pylint: disable= unused-argument\n    if ctx is None:\n        ctx = current_context()\n    dtype = mx_real_t if dtype is None else dtype\n    # pylint: disable= no-member, protected-access\n    return _internal._ones(shape=shape, ctx=ctx, dtype=dtype, **kwargs)", "response": "Returns a new array filled with all ones with the given shape and type."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a new array of given shape and type filled with the given value val.", "response": "def full(shape, val, ctx=None, dtype=mx_real_t, out=None):\n    \"\"\"Returns a new array of given shape and type, filled with the given value `val`.\n\n    Parameters\n    --------\n    shape : int or tuple of int\n        The shape of the new array.\n    val : scalar\n        Fill value.\n    ctx : Context, optional\n        Device context (default is the current default context).\n    dtype : `str` or `numpy.dtype`, optional\n        The data type of the returned `NDArray`. The default datatype is `float32`.\n    out : NDArray, optional\n        The output NDArray (default is `None`).\n\n    Returns\n    -------\n    NDArray\n        `NDArray` filled with `val`, with the given shape, ctx, and dtype.\n\n    Examples\n    --------\n    >>> mx.nd.full(1, 2.0).asnumpy()\n    array([ 2.], dtype=float32)\n    >>> mx.nd.full((1, 2), 2.0, mx.gpu(0))\n    <NDArray 1x2 @gpu(0)>\n    >>> mx.nd.full((1, 2), 2.0, dtype='float16').asnumpy()\n    array([[ 2.,  2.]], dtype=float16)\n    \"\"\"\n    out = empty(shape, ctx, dtype) if out is None else out\n    out[:] = val\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef array(source_array, ctx=None, dtype=None):\n    if isinstance(source_array, NDArray):\n        dtype = source_array.dtype if dtype is None else dtype\n    else:\n        dtype = mx_real_t if dtype is None else dtype\n        if not isinstance(source_array, np.ndarray):\n            try:\n                source_array = np.array(source_array, dtype=dtype)\n            except:\n                raise TypeError('source_array must be array like object')\n    arr = empty(source_array.shape, ctx, dtype)\n    arr[:] = source_array\n    return arr", "response": "Creates an array from any object exposing the array interface."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef moveaxis(tensor, source, destination):\n    try:\n        source = np.core.numeric.normalize_axis_tuple(source, tensor.ndim)\n    except IndexError:\n        raise ValueError('Source should verify 0 <= source < tensor.ndim'\n                         'Got %d' % source)\n    try:\n        destination = np.core.numeric.normalize_axis_tuple(destination, tensor.ndim)\n    except IndexError:\n        raise ValueError('Destination should verify 0 <= destination < tensor.ndim (%d).'\n                         % tensor.ndim, 'Got %d' % destination)\n\n    if len(source) != len(destination):\n        raise ValueError('`source` and `destination` arguments must have '\n                         'the same number of elements')\n\n    order = [n for n in range(tensor.ndim) if n not in source]\n\n    for dest, src in sorted(zip(destination, source)):\n        order.insert(dest, src)\n\n    return op.transpose(tensor, order)", "response": "Moves the source axis into the destination position\n    while leaving the other axes in their original order\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef arange(start, stop=None, step=1.0, repeat=1, infer_range=None, ctx=None, dtype=mx_real_t):\n    if infer_range is not None:\n        warnings.warn('`infer_range` argument has been deprecated',\n                      DeprecationWarning)\n    if ctx is None:\n        ctx = current_context()\n    return _internal._arange(start=start, stop=stop, step=step, repeat=repeat,\n                             infer_range=False, dtype=dtype, ctx=str(ctx))", "response": "Returns an NDArrayArray of evenly spaced values within a given interval."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add(lhs, rhs):\n    # pylint: disable= no-member, protected-access\n    return _ufunc_helper(\n        lhs,\n        rhs,\n        op.broadcast_add,\n        operator.add,\n        _internal._plus_scalar,\n        None)", "response": "Adds two arrays with broadcasting."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning element - wise difference of two arrays with broadcasting.", "response": "def subtract(lhs, rhs):\n    \"\"\"Returns element-wise difference of the input arrays with broadcasting.\n\n    Equivalent to ``lhs - rhs``, ``mx.nd.broadcast_sub(lhs, rhs)`` and\n    ``mx.nd.broadcast_minus(lhs, rhs)``.\n\n    .. note::\n\n       If the corresponding dimensions of two arrays have the same size or one of them has size 1,\n       then the arrays are broadcastable to a common shape.\n\n    Parameters\n    ----------\n    lhs : scalar or mxnet.ndarray.array\n        First array to be subtracted.\n    rhs : scalar or mxnet.ndarray.array\n         Second array to be subtracted.\n        If ``lhs.shape != rhs.shape``, they must be\n        broadcastable to a common shape.\n\n    Returns\n    -------\n    NDArray\n        The element-wise difference of the input arrays.\n\n    Examples\n    --------\n    >>> x = mx.nd.ones((2,3))\n    >>> y = mx.nd.arange(2).reshape((2,1))\n    >>> z = mx.nd.arange(2).reshape((1,2))\n    >>> x.asnumpy()\n    array([[ 1.,  1.,  1.],\n           [ 1.,  1.,  1.]], dtype=float32)\n    >>> y.asnumpy()\n    array([[ 0.],\n           [ 1.]], dtype=float32)\n    >>> z.asnumpy()\n    array([[ 0.,  1.]], dtype=float32)\n    >>> (x-2).asnumpy()\n    array([[-1., -1., -1.],\n           [-1., -1., -1.]], dtype=float32)\n    >>> (x-y).asnumpy()\n    array([[ 1.,  1.,  1.],\n           [ 0.,  0.,  0.]], dtype=float32)\n    >>> mx.nd.subtract(x,y).asnumpy()\n    array([[ 1.,  1.,  1.],\n           [ 0.,  0.,  0.]], dtype=float32)\n    >>> (z-y).asnumpy()\n    array([[ 0.,  1.],\n           [-1.,  0.]], dtype=float32)\n    \"\"\"\n    # pylint: disable= no-member, protected-access\n    return _ufunc_helper(\n        lhs,\n        rhs,\n        op.broadcast_sub,\n        operator.sub,\n        _internal._minus_scalar,\n        _internal._rminus_scalar)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn element - wise multiplication of the input arrays with broadcasting.", "response": "def multiply(lhs, rhs):\n    \"\"\"Returns element-wise product of the input arrays with broadcasting.\n\n    Equivalent to ``lhs * rhs`` and ``mx.nd.broadcast_mul(lhs, rhs)``.\n\n    .. note::\n\n       If the corresponding dimensions of two arrays have the same size or one of them has size 1,\n       then the arrays are broadcastable to a common shape.\n\n    Parameters\n    ----------\n    lhs : scalar or mxnet.ndarray.array\n        First array to be multiplied.\n    rhs : scalar or mxnet.ndarray.array\n         Second array to be multiplied.\n        If ``lhs.shape != rhs.shape``, they must be\n        broadcastable to a common shape.\n\n    Returns\n    -------\n    NDArray\n        The element-wise multiplication of the input arrays.\n\n    Examples\n    --------\n    >>> x = mx.nd.ones((2,3))\n    >>> y = mx.nd.arange(2).reshape((2,1))\n    >>> z = mx.nd.arange(2).reshape((1,2))\n    >>> x.asnumpy()\n    array([[ 1.,  1.,  1.],\n           [ 1.,  1.,  1.]], dtype=float32)\n    >>> y.asnumpy()\n    array([[ 0.],\n           [ 1.]], dtype=float32)\n    >>> z.asnumpy()\n    array([[ 0.,  1.]], dtype=float32)\n    >>> (x*2).asnumpy()\n    array([[ 2.,  2.,  2.],\n           [ 2.,  2.,  2.]], dtype=float32)\n    >>> (x*y).asnumpy()\n    array([[ 0.,  0.,  0.],\n           [ 1.,  1.,  1.]], dtype=float32)\n    >>> mx.nd.multiply(x, y).asnumpy()\n    array([[ 0.,  0.,  0.],\n           [ 1.,  1.,  1.]], dtype=float32)\n    >>> (z*y).asnumpy()\n    array([[ 0.,  0.],\n           [ 0.,  1.]], dtype=float32)\n    \"\"\"\n    # pylint: disable= no-member, protected-access\n    return _ufunc_helper(\n        lhs,\n        rhs,\n        op.broadcast_mul,\n        operator.mul,\n        _internal._mul_scalar,\n        None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef divide(lhs, rhs):\n    # pylint: disable= no-member, protected-access\n    return _ufunc_helper(\n        lhs,\n        rhs,\n        op.broadcast_div,\n        operator.truediv,\n        _internal._div_scalar,\n        _internal._rdiv_scalar)", "response": "Returns an NDArray that is the element - wise division of the input arrays with broadcasting."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef modulo(lhs, rhs):\n    # pylint: disable= no-member, protected-access\n    return _ufunc_helper(\n        lhs,\n        rhs,\n        op.broadcast_mod,\n        operator.mod,\n        _internal._mod_scalar,\n        _internal._rmod_scalar)", "response": "Returns an NDArray containing the element - wise modulo of the input arrays."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef power(base, exp):\n    # pylint: disable= no-member, protected-access\n    return _ufunc_helper(\n        base,\n        exp,\n        op.broadcast_power,\n        operator.pow,\n        _internal._power_scalar,\n        _internal._rpower_scalar)", "response": "Returns result of first array elements raised to powers from second array with broadcasting."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef maximum(lhs, rhs):\n    # pylint: disable= no-member, protected-access\n    return _ufunc_helper(\n        lhs,\n        rhs,\n        op.broadcast_maximum,\n        lambda x, y: x if x > y else y,\n        _internal._maximum_scalar,\n        None)", "response": "Returns the element - wise maximum of the input arrays with broadcasting."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef minimum(lhs, rhs):\n    # pylint: disable= no-member, protected-access\n    return _ufunc_helper(\n        lhs,\n        rhs,\n        op.broadcast_minimum,\n        lambda x, y: x if x < y else y,\n        _internal._minimum_scalar,\n        None)", "response": "Returns the element - wise minimum of the input arrays with broadcasting."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef equal(lhs, rhs):\n    # pylint: disable= no-member, protected-access\n    return _ufunc_helper(\n        lhs,\n        rhs,\n        op.broadcast_equal,\n        lambda x, y: 1 if x == y else 0,\n        _internal._equal_scalar,\n        None)", "response": "Returns the result of element - wise equal to comparison with broadcasting."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef not_equal(lhs, rhs):\n    # pylint: disable= no-member, protected-access\n    return _ufunc_helper(\n        lhs,\n        rhs,\n        op.broadcast_not_equal,\n        lambda x, y: 1 if x != y else 0,\n        _internal._not_equal_scalar,\n        None)", "response": "Returns the result of element - wise not equal to ** = operator."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the result of element - wise greater than rhs comparison operation with broadcasting.", "response": "def greater(lhs, rhs):\n    \"\"\"Returns the result of element-wise **greater than** (>) comparison operation\n    with broadcasting.\n\n    For each element in input arrays, return 1(true) if lhs elements are greater than rhs,\n    otherwise return 0(false).\n\n    Equivalent to ``lhs > rhs`` and ``mx.nd.broadcast_greater(lhs, rhs)``.\n\n    .. note::\n\n       If the corresponding dimensions of two arrays have the same size or one of them has size 1,\n       then the arrays are broadcastable to a common shape.\n\n    Parameters\n    ----------\n    lhs : scalar or mxnet.ndarray.array\n        First array to be compared.\n    rhs : scalar or mxnet.ndarray.array\n         Second array to be compared. If ``lhs.shape != rhs.shape``, they must be\n        broadcastable to a common shape.\n\n    Returns\n    -------\n    NDArray\n        Output array of boolean values.\n\n    Examples\n    --------\n    >>> x = mx.nd.ones((2,3))\n    >>> y = mx.nd.arange(2).reshape((2,1))\n    >>> z = mx.nd.arange(2).reshape((1,2))\n    >>> x.asnumpy()\n    array([[ 1.,  1.,  1.],\n           [ 1.,  1.,  1.]], dtype=float32)\n    >>> y.asnumpy()\n    array([[ 0.],\n           [ 1.]], dtype=float32)\n    >>> z.asnumpy()\n    array([[ 0.,  1.]], dtype=float32)\n    >>> (x > 1).asnumpy()\n    array([[ 0.,  0.,  0.],\n           [ 0.,  0.,  0.]], dtype=float32)\n    >>> (x > y).asnumpy()\n    array([[ 1.,  1.,  1.],\n           [ 0.,  0.,  0.]], dtype=float32)\n    >>> mx.nd.greater(x, y).asnumpy()\n    array([[ 1.,  1.,  1.],\n           [ 0.,  0.,  0.]], dtype=float32)\n    >>> (z > y).asnumpy()\n    array([[ 0.,  1.],\n           [ 0.,  0.]], dtype=float32)\n    \"\"\"\n    # pylint: disable= no-member, protected-access\n    return _ufunc_helper(\n        lhs,\n        rhs,\n        op.broadcast_greater,\n        lambda x, y: 1 if x > y else 0,\n        _internal._greater_scalar,\n        _internal._lesser_scalar)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the result of element - wise greater than or equal to ( > = ) comparison operation with broadcasting.", "response": "def greater_equal(lhs, rhs):\n    \"\"\"Returns the result of element-wise **greater than or equal to** (>=) comparison\n    operation with broadcasting.\n\n    For each element in input arrays, return 1(true) if lhs elements are greater than equal to rhs,\n    otherwise return 0(false).\n\n    Equivalent to ``lhs >= rhs`` and ``mx.nd.broadcast_greater_equal(lhs, rhs)``.\n\n    .. note::\n\n       If the corresponding dimensions of two arrays have the same size or one of them has size 1,\n       then the arrays are broadcastable to a common shape.\n\n    Parameters\n    ----------\n    lhs : scalar or mxnet.ndarray.array\n        First array to be compared.\n    rhs : scalar or mxnet.ndarray.array\n         Second array to be compared. If ``lhs.shape != rhs.shape``, they must be\n        broadcastable to a common shape.\n\n    Returns\n    -------\n    NDArray\n        Output array of boolean values.\n\n    Examples\n    --------\n    >>> x = mx.nd.ones((2,3))\n    >>> y = mx.nd.arange(2).reshape((2,1))\n    >>> z = mx.nd.arange(2).reshape((1,2))\n    >>> x.asnumpy()\n    array([[ 1.,  1.,  1.],\n           [ 1.,  1.,  1.]], dtype=float32)\n    >>> y.asnumpy()\n    array([[ 0.],\n           [ 1.]], dtype=float32)\n    >>> z.asnumpy()\n    array([[ 0.,  1.]], dtype=float32)\n    >>> (x >= 1).asnumpy()\n    array([[ 1.,  1.,  1.],\n           [ 1.,  1.,  1.]], dtype=float32)\n    >>> (x >= y).asnumpy()\n    array([[ 1.,  1.,  1.],\n           [ 1.,  1.,  1.]], dtype=float32)\n    >>> mx.nd.greater_equal(x, y).asnumpy()\n    array([[ 1.,  1.,  1.],\n           [ 1.,  1.,  1.]], dtype=float32)\n    >>> (z >= y).asnumpy()\n    array([[ 1.,  1.],\n           [ 0.,  1.]], dtype=float32)\n    \"\"\"\n    # pylint: disable= no-member, protected-access\n    return _ufunc_helper(\n        lhs,\n        rhs,\n        op.broadcast_greater_equal,\n        lambda x, y: 1 if x >= y else 0,\n        _internal._greater_equal_scalar,\n        _internal._lesser_equal_scalar)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef lesser(lhs, rhs):\n    # pylint: disable= no-member, protected-access\n    return _ufunc_helper(\n        lhs,\n        rhs,\n        op.broadcast_lesser,\n        lambda x, y: 1 if x < y else 0,\n        _internal._lesser_scalar,\n        _internal._greater_scalar)", "response": "Returns the result of element - wise lesser than operation on the input arrays."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef lesser_equal(lhs, rhs):\n    # pylint: disable= no-member, protected-access\n    return _ufunc_helper(\n        lhs,\n        rhs,\n        op.broadcast_lesser_equal,\n        lambda x, y: 1 if x <= y else 0,\n        _internal._lesser_equal_scalar,\n        _internal._greater_equal_scalar)", "response": "Returns the result of element - wise lesser than or equal to ** = rhs **."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the result of element - wise logical and comparison operation with broadcasting.", "response": "def logical_and(lhs, rhs):\n    \"\"\"Returns the result of element-wise **logical and** comparison\n    operation with broadcasting.\n\n    For each element in input arrays, return 1(true) if lhs elements and rhs elements\n    are true, otherwise return 0(false).\n\n    Equivalent to ``lhs and rhs`` and ``mx.nd.broadcast_logical_and(lhs, rhs)``.\n\n    .. note::\n\n       If the corresponding dimensions of two arrays have the same size or one of them has size 1,\n       then the arrays are broadcastable to a common shape.\n\n    Parameters\n    ----------\n    lhs : scalar or mxnet.ndarray.array\n        First input of the function.\n    rhs : scalar or mxnet.ndarray.array\n         Second input of the function. If ``lhs.shape != rhs.shape``, they must be\n        broadcastable to a common shape.\n\n    Returns\n    -------\n    NDArray\n        Output array of boolean values.\n\n    Examples\n    --------\n    >>> x = mx.nd.ones((2,3))\n    >>> y = mx.nd.arange(2).reshape((2,1))\n    >>> z = mx.nd.arange(2).reshape((1,2))\n    >>> x.asnumpy()\n    array([[ 1.,  1.,  1.],\n           [ 1.,  1.,  1.]], dtype=float32)\n    >>> y.asnumpy()\n    array([[ 0.],\n           [ 1.]], dtype=float32)\n    >>> z.asnumpy()\n    array([[ 0.,  1.]], dtype=float32)\n    >>> mx.nd.logical_and(x, 1).asnumpy()\n    array([[ 1.,  1.,  1.],\n           [ 1.,  1.,  1.]], dtype=float32)\n    >>> mx.nd.logical_and(x, y).asnumpy()\n    array([[ 0.,  0.,  0.],\n           [ 1.,  1.,  1.]], dtype=float32)\n    >>> mx.nd.logical_and(z, y).asnumpy()\n    array([[ 0.,  0.],\n           [ 0.,  1.]], dtype=float32)\n    \"\"\"\n    # pylint: disable= no-member, protected-access\n    return _ufunc_helper(\n        lhs,\n        rhs,\n        op.broadcast_logical_and,\n        lambda x, y: 1 if x and y else 0,\n        _internal._logical_and_scalar,\n        None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef logical_or(lhs, rhs):\n    # pylint: disable= no-member, protected-access\n    return _ufunc_helper(\n        lhs,\n        rhs,\n        op.broadcast_logical_or,\n        lambda x, y: 1 if x or y else 0,\n        _internal._logical_or_scalar,\n        None)", "response": "Returns the result of element - wise logical or comparison with broadcasting."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef logical_xor(lhs, rhs):\n    # pylint: disable= no-member, protected-access\n    return _ufunc_helper(\n        lhs,\n        rhs,\n        op.broadcast_logical_xor,\n        lambda x, y: 1 if bool(x) ^ bool(y) else 0,\n        _internal._logical_xor_scalar,\n        None)", "response": "Returns the result of element - wise logical xor operation with broadcasting."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndeprecating use concat instead.", "response": "def concatenate(arrays, axis=0, always_copy=True):\n    \"\"\"DEPRECATED, use ``concat`` instead\n\n    Parameters\n    ----------\n    arrays : list of `NDArray`\n        Arrays to be concatenate. They must have identical shape except\n        the first dimension. They also must have the same data type.\n    axis : int\n        The axis along which to concatenate.\n    always_copy : bool\n        Default `True`. When not `True`, if the arrays only contain one\n        `NDArray`, that element will be returned directly, avoid copying.\n\n    Returns\n    -------\n    NDArray\n        An `NDArray` that lives on the same context as `arrays[0].context`.\n    \"\"\"\n    assert isinstance(arrays, list)\n    assert len(arrays) > 0\n    assert isinstance(arrays[0], NDArray)\n\n    if not always_copy and len(arrays) == 1:\n        return arrays[0]\n\n    shape_axis = arrays[0].shape[axis]\n    shape_rest1 = arrays[0].shape[0:axis]\n    shape_rest2 = arrays[0].shape[axis+1:]\n    dtype = arrays[0].dtype\n    for arr in arrays[1:]:\n        shape_axis += arr.shape[axis]\n        assert shape_rest1 == arr.shape[0:axis]\n        assert shape_rest2 == arr.shape[axis+1:]\n        assert dtype == arr.dtype\n    ret_shape = shape_rest1 + (shape_axis,) + shape_rest2\n    ret = empty(ret_shape, ctx=arrays[0].context, dtype=dtype)\n\n    idx = 0\n    begin = [0 for _ in ret_shape]\n    end = list(ret_shape)\n    for arr in arrays:\n        if axis == 0:\n            ret[idx:idx+arr.shape[0]] = arr\n        else:\n            begin[axis] = idx\n            end[axis] = idx+arr.shape[axis]\n            # pylint: disable=no-member,protected-access\n            _internal._crop_assign(ret, arr, out=ret,\n                                   begin=tuple(begin),\n                                   end=tuple(end))\n            # pylint: enable=no-member,protected-access\n        idx += arr.shape[axis]\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndecoding binary image into an NDArray.", "response": "def imdecode(str_img, clip_rect=(0, 0, 0, 0), out=None, index=0, channels=3, mean=None):\n    \"\"\"DEPRECATED, use mx.img instead\n\n    Parameters\n    ----------\n    str_img : str\n        Binary image data\n    clip_rect : iterable of 4 int\n        Clip decoded image to rectangle (x0, y0, x1, y1).\n    out : NDArray\n        Output buffer. Can be 3 dimensional (c, h, w) or 4 dimensional (n, c, h, w).\n    index : int\n        Output decoded image to i-th slice of 4 dimensional buffer.\n    channels : int\n        Number of channels to output. Decode to grey scale when channels = 1.\n    mean : NDArray\n        Subtract mean from decode image before outputing.\n    \"\"\"\n    # pylint: disable= no-member, protected-access, too-many-arguments\n    if mean is None:\n        mean = NDArray(_new_empty_handle())\n    if out is None:\n        return _internal._imdecode(mean, index,\n                                   clip_rect[0],\n                                   clip_rect[1],\n                                   clip_rect[2],\n                                   clip_rect[3],\n                                   channels,\n                                   len(str_img),\n                                   str_img=str_img)\n    else:\n        return _internal._imdecode(mean, index,\n                                   clip_rect[0],\n                                   clip_rect[1],\n                                   clip_rect[2],\n                                   clip_rect[3],\n                                   channels,\n                                   len(str_img),\n                                   str_img=str_img,\n                                   out=out)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a new array filled with all zeros with the given shape and type.", "response": "def zeros(shape, ctx=None, dtype=None, **kwargs):\n    \"\"\"Returns a new array filled with all zeros, with the given shape and type.\n\n    Parameters\n    ----------\n    shape : int or tuple of int\n        The shape of the empty array.\n    ctx : Context, optional\n        An optional device context (default is the current default context).\n    dtype : str or numpy.dtype, optional\n        An optional value type (default is `float32`).\n    out : NDArray, optional\n        The output NDArray (default is `None`).\n\n    Returns\n    -------\n    NDArray\n        A created array\n\n    Examples\n    --------\n    >>> mx.nd.zeros(1).asnumpy()\n    array([ 0.], dtype=float32)\n    >>> mx.nd.zeros((1,2), mx.gpu(0))\n    <NDArray 1x2 @gpu(0)>\n    >>> mx.nd.zeros((1,2), mx.gpu(0), 'float16').asnumpy()\n    array([[ 0.,  0.]], dtype=float16)\n    \"\"\"\n    # pylint: disable= unused-argument\n    if ctx is None:\n        ctx = current_context()\n    dtype = mx_real_t if dtype is None else dtype\n    # pylint: disable= no-member, protected-access\n    return _internal._zeros(shape=shape, ctx=ctx, dtype=dtype, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef eye(N, M=0, k=0, ctx=None, dtype=None, **kwargs):\n    # pylint: disable= unused-argument\n    if ctx is None:\n        ctx = current_context()\n    dtype = mx_real_t if dtype is None else dtype\n    # pylint: disable= no-member, protected-access\n    return _internal._eye(N=N, M=M, k=k, ctx=ctx, dtype=dtype, **kwargs)", "response": "Return a 2 - D array with ones on the diagonal and zeros elsewhere."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns an empty array with given shape and type.", "response": "def empty(shape, ctx=None, dtype=None):\n    \"\"\"Returns a new array of given shape and type, without initializing entries.\n\n    Parameters\n    ----------\n    shape : int or tuple of int\n        The shape of the empty array.\n    ctx : Context, optional\n        An optional device context (default is the current default context).\n    dtype : str or numpy.dtype, optional\n        An optional value type (default is `float32`).\n\n    Returns\n    -------\n    NDArray\n        A created array.\n\n    \"\"\"\n    if isinstance(shape, int):\n        shape = (shape, )\n    if ctx is None:\n        ctx = current_context()\n    if dtype is None:\n        dtype = mx_real_t\n    return NDArray(handle=_new_alloc_handle(shape, ctx, False, dtype))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the histogram over the input data.", "response": "def histogram(a, bins=10, range=None):\n    \"\"\"Compute the histogram of the input data.\n\n    Parameters\n    ----------\n    a : NDArray\n        Input data. The histogram is computed over the flattened array.\n    bins : int or sequence of scalars\n        If bins is an int, it defines the number of equal-width bins in the\n        given range (10, by default). If bins is a sequence, it defines the bin edges,\n        including the rightmost edge, allowing for non-uniform bin widths.\n    range : (float, float), optional\n        The lower and upper range of the bins. If not provided, range is simply (a.min(), a.max()).\n        Values outside the range are ignored. The first element of the range must be less than or\n        equal to the second. range affects the automatic bin computation as well, the range will\n        be equally divided by the number of bins.\n\n    Returns\n    -------\n    NDArray\n        A created array.\n\n    \"\"\"\n\n    # pylint: disable= no-member, protected-access\n    if isinstance(bins, NDArray):\n        return _internal._histogram(data=a, bins=bins)\n    elif isinstance(bins, integer_types):\n        if range is None:\n            warnings.warn(\"range is not specified, using numpy's result \"\n                          \"to ensure consistency with numpy\")\n            res, bin_bounds = np.histogram(a.asnumpy(), bins=bins)\n            return array(res), array(bin_bounds)\n        return _internal._histogram(data=a, bin_cnt=bins, range=range)\n    raise ValueError(\"bins argument should be either an integer or an NDArray\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef split_v2(ary, indices_or_sections, axis=0, squeeze_axis=False):\n    indices = []\n    axis_size = ary.shape[axis]\n    if isinstance(indices_or_sections, int):\n        sections = indices_or_sections\n        if axis_size % sections:\n            raise ValueError('array split does not result in an equal division')\n        section_size = int(axis_size / sections)\n        indices = [i * section_size for i in range(sections)]\n    elif isinstance(indices_or_sections, tuple):\n        indices = [0] + list(indices_or_sections)\n    else:\n        raise ValueError('indices_or_sections must either int or tuple of ints')\n    return _internal._split_v2(ary, indices, axis, squeeze_axis)", "response": "Splits an array into multiple sub - arrays along a given axis."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_dlpack_for_read(data):\n    data.wait_to_read()\n    dlpack = DLPackHandle()\n    check_call(_LIB.MXNDArrayToDLPack(data.handle, ctypes.byref(dlpack)))\n    return ctypes.pythonapi.PyCapsule_New(dlpack, _c_str_dltensor, _c_dlpack_deleter)", "response": "Returns a reference view of NDArray that represents as DLManagedTensor until all previous write operations on the current array are finished."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a reference view of NDArray that represents as DLManagedTensor until all previous read and write operations on the current array are finished.", "response": "def to_dlpack_for_write(data):\n    \"\"\"Returns a reference view of NDArray that represents as DLManagedTensor until\n       all previous read/write operations on the current array are finished.\n\n    Parameters\n    ----------\n    data: NDArray\n        input data.\n\n    Returns\n    -------\n    PyCapsule (the pointer of DLManagedTensor)\n        a reference view of NDArray that represents as DLManagedTensor.\n\n    Examples\n    --------\n    >>> x = mx.nd.ones((2,3))\n    >>> w = mx.nd.to_dlpack_for_write(x)\n    >>> type(w)\n    <class 'PyCapsule'>\n    >>> u = mx.nd.from_dlpack(w)\n    >>> u += 1\n    >>> x\n    [[2. 2. 2.]\n     [2. 2. 2.]]\n    <NDArray 2x3 @cpu(0)>\n    \"\"\"\n    check_call(_LIB.MXNDArrayWaitToWrite(data.handle))\n    dlpack = DLPackHandle()\n    check_call(_LIB.MXNDArrayToDLPack(data.handle, ctypes.byref(dlpack)))\n    return ctypes.pythonapi.PyCapsule_New(dlpack, _c_str_dltensor, _c_dlpack_deleter)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a NDArray backed by a dlpack tensor.", "response": "def from_dlpack(dlpack):\n    \"\"\"Returns a NDArray backed by a dlpack tensor.\n\n    Parameters\n    ----------\n    dlpack: PyCapsule (the pointer of DLManagedTensor)\n        input data\n\n    Returns\n    -------\n    NDArray\n        a NDArray backed by a dlpack tensor\n\n    Examples\n    --------\n    >>> x = mx.nd.ones((2,3))\n    >>> y = mx.nd.to_dlpack_for_read(x)\n    >>> type(y)\n    <class 'PyCapsule'>\n    >>> z = mx.nd.from_dlpack(y)\n    >>> type(z)\n    <class 'mxnet.ndarray.ndarray.NDArray'>\n    >>> z\n    [[ 1.  1.  1.]\n     [ 1.  1.  1.]]\n    <NDArray 2x3 @cpu(0)>\n\n    >>> w = mx.nd.to_dlpack_for_write(x)\n    >>> type(w)\n    <class 'PyCapsule'>\n    >>> u = mx.nd.from_dlpack(w)\n    >>> u += 1\n    >>> x\n    [[2. 2. 2.]\n     [2. 2. 2.]]\n    <NDArray 2x3 @cpu(0)>\n    \"\"\"\n    handle = NDArrayHandle()\n    dlpack = ctypes.py_object(dlpack)\n    assert ctypes.pythonapi.PyCapsule_IsValid(dlpack, _c_str_dltensor), ValueError(\n        'Invalid DLPack Tensor. DLTensor capsules can be consumed only once.')\n    dlpack_handle = ctypes.c_void_p(ctypes.pythonapi.PyCapsule_GetPointer(dlpack, _c_str_dltensor))\n    check_call(_LIB.MXNDArrayFromDLPack(dlpack_handle, ctypes.byref(handle)))\n    # Rename PyCapsule (DLPack)\n    ctypes.pythonapi.PyCapsule_SetName(dlpack, _c_str_used_dltensor)\n    # delete the deleter of the old dlpack\n    ctypes.pythonapi.PyCapsule_SetDestructor(dlpack, None)\n    return NDArray(handle=handle)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns an MXNet s NDArray backed by Numpy s ndarray.", "response": "def from_numpy(ndarray, zero_copy=True):\n    \"\"\"Returns an MXNet's NDArray backed by Numpy's ndarray.\n\n    Parameters\n    ----------\n    ndarray: numpy.ndarray\n        input data\n\n    zero_copy: bool\n        Whether we use DLPack's zero-copy conversion to convert to MXNet's NDArray.\n        This is only available for c-contiguous arrays, i.e. array.flags[C_CONTIGUOUS] == True.\n\n    Returns\n    -------\n    NDArray\n        a NDArray backed by a dlpack tensor\n\n    \"\"\"\n\n    def _make_manager_ctx(obj):\n        pyobj = ctypes.py_object(obj)\n        void_p = ctypes.c_void_p.from_buffer(pyobj)\n        ctypes.pythonapi.Py_IncRef(pyobj)\n        return void_p\n\n    def _make_dl_tensor(array):\n        if str(array.dtype) not in DLDataType.TYPE_MAP:\n            raise ValueError(str(array.dtype) + \" is not supported.\")\n        dl_tensor = DLTensor()\n        dl_tensor.data = array.ctypes.data_as(ctypes.c_void_p)\n        dl_tensor.ctx = DLContext(1, 0)\n        dl_tensor.ndim = array.ndim\n        dl_tensor.dtype = DLDataType.TYPE_MAP[str(array.dtype)]\n        dl_tensor.shape = array.ctypes.shape_as(ctypes.c_int64)\n        dl_tensor.strides = None\n        dl_tensor.byte_offset = 0\n        return dl_tensor\n\n    def _make_dl_managed_tensor(array):\n        c_obj = DLManagedTensor()\n        c_obj.dl_tensor = _make_dl_tensor(array)\n        c_obj.manager_ctx = _make_manager_ctx(array)\n        c_obj.deleter = dl_managed_tensor_deleter\n        return c_obj\n\n    if not zero_copy:\n        return array(ndarray, dtype=ndarray.dtype)\n\n    if not ndarray.flags['C_CONTIGUOUS']:\n        raise ValueError(\"Only c-contiguous arrays are supported for zero-copy\")\n    c_obj = _make_dl_managed_tensor(ndarray)\n    address = ctypes.addressof(c_obj)\n    address = ctypes.cast(address, ctypes.c_void_p)\n    handle = NDArrayHandle()\n    check_call(_LIB.MXNDArrayFromDLPack(address, ctypes.byref(handle)))\n    return NDArray(handle=handle)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn an index array for use in scatter_nd and gather_nd.", "response": "def _get_index_nd(self, key):\n        \"\"\"Returns an index array for use in scatter_nd and gather_nd.\"\"\"\n        def _is_advanced_index(index):\n            \"\"\"The definition of advanced index here includes integers as well, while\n            integers are considered as basic index type when the key contains only\n            slices and integers.\"\"\"\n            return not isinstance(index, py_slice)\n\n        if isinstance(key, (NDArray, np.ndarray, list, integer_types, py_slice)):\n            key = (key,)\n\n        assert isinstance(key, tuple),\\\n            'index=%s must be a NDArray, or np.ndarray, or list, or tuple ' \\\n            ' type to use advanced indexing, received type=%s' % (str(key), str(type(key)))\n\n        assert len(key) > 0, \"Cannot slice with empty indices\"\n        shape = self.shape\n        assert len(shape) >= len(key),\\\n            \"Slicing dimensions exceeds array dimensions, %d vs %d\" % (len(key), len(shape))\n        indices = []\n        dtype = 'int32'  # index data type passed to gather_nd op\n        need_broadcast = (len(key) != 1)\n        advanced_indices = []  # include list, NDArray, np.ndarray, integer\n        basic_indices = []  # include only slices\n        advanced_index_bshape = None  # final advanced index shape\n        for i, idx_i in enumerate(key):\n            is_advanced_index = True\n            if isinstance(idx_i, (np.ndarray, list, tuple)):\n                idx_i = array(idx_i, ctx=self.context, dtype=dtype)\n                advanced_indices.append(i)\n            elif isinstance(idx_i, py_slice):\n                start, stop, step = _get_index_range(idx_i.start, idx_i.stop, shape[i], idx_i.step)\n                idx_i = arange(start, stop, step, ctx=self.context, dtype=dtype)\n                basic_indices.append(i)\n                is_advanced_index = False\n            elif isinstance(idx_i, integer_types):\n                start, stop, step = _get_index_range(idx_i, idx_i+1, shape[i], 1)\n                idx_i = arange(start, stop, step, ctx=self.context, dtype=dtype)\n                advanced_indices.append(i)\n            elif isinstance(idx_i, NDArray):\n                if dtype != idx_i.dtype:\n                    idx_i = idx_i.astype(dtype)\n                advanced_indices.append(i)\n            else:\n                raise IndexError('Indexing NDArray with index=%s of type=%s is not supported'\n                                 % (str(key), str(type(key))))\n            if is_advanced_index:\n                if advanced_index_bshape is None:\n                    advanced_index_bshape = idx_i.shape\n                elif advanced_index_bshape != idx_i.shape:\n                    need_broadcast = True\n                    advanced_index_bshape = _get_broadcast_shape(advanced_index_bshape, idx_i.shape)\n            indices.append(idx_i)\n\n        # Get final index shape for gather_nd. See the following reference\n        # for determining the output array shape.\n        # https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html#combining-advanced-and-basic-indexing  # pylint: disable=line-too-long\n        if len(advanced_indices) == 0:\n            raise ValueError('Advanced index tuple must contain at least one of the following types:'\n                             ' list, tuple, NDArray, np.ndarray, integer, received index=%s' % key)\n        # determine the output array's shape by checking whether advanced_indices are all adjacent\n        # or separated by slices\n        advanced_indices_adjacent = True\n        for i in range(0, len(advanced_indices)-1):\n            if advanced_indices[i] + 1 != advanced_indices[i+1]:\n                advanced_indices_adjacent = False\n                break\n\n        index_bshape_list = []  # index broadcasted shape\n        if advanced_indices_adjacent:\n            for i in range(0, advanced_indices[0]):\n                index_bshape_list.extend(indices[i].shape)\n                if not need_broadcast and indices[i].shape != advanced_index_bshape:\n                    need_broadcast = True\n            index_bshape_list.extend(advanced_index_bshape)\n            for i in range(advanced_indices[-1]+1, len(indices)):\n                if not need_broadcast and indices[i].shape != advanced_index_bshape:\n                    need_broadcast = True\n                index_bshape_list.extend(indices[i].shape)\n        else:\n            index_bshape_list.extend(advanced_index_bshape)\n            for i in basic_indices:\n                index_bshape_list.extend(indices[i].shape)\n                if not need_broadcast and indices[i].shape != advanced_index_bshape:\n                    need_broadcast = True\n        index_bshape = tuple(index_bshape_list)\n\n        # Need to broadcast all ndarrays in indices to the final shape.\n        # For example, suppose an array has shape=(5, 6, 7, 8) and\n        # key=(slice(1, 5), [[1, 2]], slice(2, 5), [1]).\n        # Since key[1] and key[3] are two advanced indices here and they are\n        # separated by basic indices key[0] and key[2], the output shape\n        # is (1, 2, 4, 3), where the first two elements come from the shape\n        # that key[1] and key[3] should broadcast to, which is (1, 2), and\n        # the last two elements come from the shape of two basic indices.\n        # In order to broadcast all basic and advanced indices to the output shape,\n        # we need to reshape them based on their axis. For example, to broadcast key[0],\n        # with shape=(4,), we first need to reshape it into (1, 1, 4, 1), and then\n        # broadcast the reshaped array to (1, 2, 4, 3); to broadcast key[1], we first\n        # reshape it into (1, 2, 1, 1), then broadcast the reshaped array to (1, 2, 4, 3).\n        if need_broadcast:\n            broadcasted_indices = []\n            idx_rshape = [1] * len(index_bshape)\n            if advanced_indices_adjacent:\n                advanced_index_bshape_start = advanced_indices[0]  # start index of advanced_index_bshape in index_shape\n                advanced_index_bshape_stop = advanced_index_bshape_start + len(advanced_index_bshape)\n                for i, idx in enumerate(key):\n                    if _is_advanced_index(idx):\n                        k = advanced_index_bshape_stop\n                        # find the reshaped shape for indices[i]\n                        for dim_size in indices[i].shape[::-1]:\n                            k -= 1\n                            idx_rshape[k] = dim_size\n                    else:\n                        if i < advanced_indices[0]:  # slice is on the left side of advanced indices\n                            idx_rshape[i] = indices[i].shape[0]\n                        elif i > advanced_indices[-1]:  # slice is on the right side of advanced indices\n                            idx_rshape[i-len(key)] = indices[i].shape[0]\n                        else:\n                            raise ValueError('basic index i=%d cannot be between advanced index i=%d and i=%d'\n                                             % (i, advanced_indices[0], advanced_indices[-1]))\n                    # broadcast current index to the final shape\n                    broadcasted_indices.append(indices[i].reshape(tuple(idx_rshape)).broadcast_to(index_bshape))\n                    # reset idx_rshape to ones\n                    for j, _ in enumerate(idx_rshape):\n                        idx_rshape[j] = 1\n            else:\n                basic_index_offset = len(advanced_index_bshape)\n                for i, idx in enumerate(key):\n                    if _is_advanced_index(idx):\n                        k = len(advanced_index_bshape)\n                        for dim_size in indices[i].shape[::-1]:\n                            k -= 1\n                            idx_rshape[k] = dim_size\n                    else:\n                        idx_rshape[basic_index_offset] = indices[i].shape[0]\n                        basic_index_offset += 1\n                    # broadcast current index to the final shape\n                    broadcasted_indices.append(indices[i].reshape(tuple(idx_rshape)).broadcast_to(index_bshape))\n                    # reset idx_rshape to ones\n                    for j, _ in enumerate(idx_rshape):\n                        idx_rshape[j] = 1\n\n            indices = broadcasted_indices\n        return op.stack(*indices)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _prepare_value_nd(self, value, vshape):\n        if isinstance(value, numeric_types):\n            value_nd = full(shape=vshape, val=value, ctx=self.context, dtype=self.dtype)\n        elif isinstance(value, NDArray):\n            value_nd = value.as_in_context(self.context)\n            if value_nd.dtype != self.dtype:\n                value_nd = value_nd.astype(self.dtype)\n        else:\n            try:\n                value_nd = array(value, ctx=self.context, dtype=self.dtype)\n            except:\n                raise TypeError('NDArray does not support assignment with non-array-like'\n                                ' object %s of type %s' % (str(value), str(type(value))))\n        if value_nd.shape != vshape:\n            value_nd = value_nd.broadcast_to(vshape)\n        return value_nd", "response": "Given value and vshape create an NDArray from value with the same\n        context and dtype as the current one and broadcast it to vshape."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _set_nd_basic_indexing(self, key, value):\n        shape = self.shape\n        if isinstance(key, integer_types):\n            if key < 0:\n                key += shape[0]\n            if key < 0 or key >= shape[0]:\n                if key < 0:\n                    key -= shape[0]\n                raise IndexError('index %d is out of bounds for axis 0 with size %d'\n                                 % (key, shape[0]))\n            key = py_slice(key, key+1)  # key must be >= 0 here\n\n        if isinstance(key, py_slice):\n            assign_to_self = key.step is None or key.step == 1\n            assign_to_self &= key.start is None or key.start == 0\n            assign_to_self &= key.stop is None or key.stop == shape[0]\n            if assign_to_self:  # trivial case, assign value to self\n                if isinstance(value, NDArray):\n                    if value.handle is not self.handle:\n                        if value.shape != shape:\n                            value = value.broadcast_to(shape)\n                        value.copyto(self)\n                elif isinstance(value, numeric_types):\n                    _internal._full(shape=shape, ctx=self.context,\n                                    dtype=self.dtype, value=float(value), out=self)\n                elif isinstance(value, (np.ndarray, np.generic)):\n                    if isinstance(value, np.generic) or value.shape != shape:\n                        value = np.broadcast_to(value, shape)\n                    self._sync_copyfrom(value)\n                else:  # value might be a list or a tuple\n                    value_nd = self._prepare_value_nd(value, shape)\n                    value_nd.copyto(self)\n                return\n            else:  # non-trivial case, use _slice_assign or _slice_assign_scalar\n                key = (key,)\n\n        assert isinstance(key, tuple), \"key=%s must be a tuple of slices and integers\" % str(key)\n\n        assert len(key) <= len(shape), \"Indexing dimensions exceed array dimensions, %d vs %d\"\\\n                                       % (len(key), len(shape))\n        begin = []\n        end = []\n        steps = []\n        oshape = []  # output shape of slice using key\n        vshape = []  # value shape of data[key]\n        for i, slice_i in enumerate(key):\n            dim_size = 1\n            if isinstance(slice_i, py_slice):\n                begin.append(slice_i.start)\n                end.append(slice_i.stop)\n                steps.append(slice_i.step)\n                start, stop, step = _get_index_range(slice_i.start, slice_i.stop,\n                                                     shape[i], slice_i.step)\n                dim_size = _get_dim_size(start, stop, step)\n                vshape.append(dim_size)\n            elif isinstance(slice_i, integer_types):\n                begin.append(slice_i)\n                end.append(slice_i+1 if slice_i != -1 else self.shape[i])\n                steps.append(1)\n            else:\n                raise ValueError(\"basic indexing does not support index=%s of type=%s\"\n                                 % (str(slice_i), str(type(slice_i))))\n            oshape.append(dim_size)\n\n        oshape.extend(shape[len(key):])\n        vshape.extend(shape[len(key):])\n        # if key contains all integers, vshape should be (1,)\n        if len(vshape) == 0:\n            vshape.append(1)\n        oshape = tuple(oshape)\n        vshape = tuple(vshape)\n\n        if isinstance(value, numeric_types):\n            _internal._slice_assign_scalar(self, out=self, begin=begin, end=end,\n                                           step=steps, scalar=float(value))\n        else:\n            value_nd = self._prepare_value_nd(value, vshape)\n            if vshape != oshape:\n                value_nd = value_nd.reshape(oshape)\n            _internal._slice_assign(self, value_nd, begin, end, steps, out=self)", "response": "This function is called by the dict s basic indexing methods."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _set_nd_advanced_indexing(self, key, value):\n        indices = self._get_index_nd(key)\n        vshape = _get_oshape_of_gather_nd_op(self.shape, indices.shape)\n        value_nd = self._prepare_value_nd(value, vshape)\n        _internal._scatter_set_nd(lhs=self, rhs=value_nd, indices=indices,\n                                  shape=self.shape, out=self)", "response": "This function is called by __setitem__ when key is an advanced index."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nperform a synchronized copy from the source_array to the current array.", "response": "def _sync_copyfrom(self, source_array):\n        \"\"\"Performs a synchronized copy from the `source_array` to the current array.\n        This is called through ``x[:] = source_array``, where the `source_array`\n        is a `numpy.ndarray` or array-like object.\n        This function blocks until all the pending read/write operations with respect\n        to the current `NDArray` are finished and carry out the copy operation to the\n        current NDArray.\n\n        Parameters\n        ----------\n        source_array : array_like\n            The data source we would like to copy from.\n\n        Example\n        -------\n        >>> a = mx.nd.array([1, 2])\n        >>> a.asnumpy()\n        array([ 1.,  2.], dtype=float32)\n        >>> a[:] = np.array([3, 4])\n        >> a.asnumpy()\n        array([ 3.,  4.], dtype=float32)\n        \"\"\"\n        if not isinstance(source_array, np.ndarray):\n            try:\n                source_array = np.array(source_array, dtype=self.dtype)\n            except:\n                raise TypeError('array must consist of array-like data,' +\n                                'type %s is not supported' % str(type(array)))\n        source_array = np.asarray(source_array, dtype=self.dtype, order='C')\n        if source_array.shape != self.shape:\n            raise ValueError('Shape inconsistent: expected %s vs got %s'%(\n                str(source_array.shape), str(self.shape)))\n        check_call(_LIB.MXNDArraySyncCopyFromCPU(\n            self.handle,\n            source_array.ctypes.data_as(ctypes.c_void_p),\n            ctypes.c_size_t(source_array.size)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _slice(self, start, stop):\n        handle = NDArrayHandle()\n        start, stop, _ = _get_index_range(start, stop, self.shape[0])\n\n        check_call(_LIB.MXNDArraySlice(\n            self.handle, mx_uint(start), mx_uint(stop), ctypes.byref(handle)))\n        return NDArray(handle=handle, writable=self.writable)", "response": "Returns a new NDArray that shares the current one."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a view of the array sliced at idx.", "response": "def _at(self, idx):\n        \"\"\"Returns a view of the array sliced at `idx` in the first dim.\n        This is called through ``x[idx]``.\n\n        Parameters\n        ----------\n        idx : int\n            index for slicing the `NDArray` in the first dim.\n\n        Returns\n        -------\n        NDArray\n            `NDArray` sharing the memory with the current one sliced at `idx` in the first dim.\n\n        Examples\n        --------\n        >>> a = mx.nd.array([[1,2], [3, 4]])\n        >>> a[1].asnumpy()\n        array([ 3.,  4.], dtype=float32)\n        >>> b = mx.nd.array([1, 2, 3, 4])\n        >>> b[0].asnumpy()\n        array([ 1.], dtype=float32)\n        \"\"\"\n        handle = NDArrayHandle()\n        if idx < 0:\n            length = self.shape[0]\n            idx += length\n            if idx < 0:\n                raise IndexError('index %d is out of bounds for axis 0 with size %d'\n                                 % (idx-length, length))\n        check_call(_LIB.MXNDArrayAt(\n            self.handle, mx_uint(idx), ctypes.byref(handle)))\n        return NDArray(handle=handle, writable=self.writable)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a ** view ** of this array with a new shape without altering any data.", "response": "def reshape(self, *shape, **kwargs):\n        \"\"\"Returns a **view** of this array with a new shape without altering any data.\n\n        Parameters\n        ----------\n        shape : tuple of int, or n ints\n            The new shape should not change the array size, namely\n            ``np.prod(new_shape)`` should be equal to ``np.prod(self.shape)``.\n            Some dimensions of the shape can take special values from the set {0, -1, -2, -3, -4}.\n            The significance of each is explained below:\n\n            - ``0``  copy this dimension from the input to the output shape.\n\n              Example::\n\n              - input shape = (2,3,4), shape = (4,0,2), output shape = (4,3,2)\n              - input shape = (2,3,4), shape = (2,0,0), output shape = (2,3,4)\n\n            - ``-1`` infers the dimension of the output shape by using the remainder of the\n              input dimensions keeping the size of the new array same as that of the input array.\n              At most one dimension of shape can be -1.\n\n              Example::\n\n              - input shape = (2,3,4), shape = (6,1,-1), output shape = (6,1,4)\n              - input shape = (2,3,4), shape = (3,-1,8), output shape = (3,1,8)\n              - input shape = (2,3,4), shape=(-1,), output shape = (24,)\n\n            - ``-2`` copy all/remainder of the input dimensions to the output shape.\n\n              Example::\n\n              - input shape = (2,3,4), shape = (-2,), output shape = (2,3,4)\n              - input shape = (2,3,4), shape = (2,-2), output shape = (2,3,4)\n              - input shape = (2,3,4), shape = (-2,1,1), output shape = (2,3,4,1,1)\n\n            - ``-3`` use the product of two consecutive dimensions of the input shape as the\n              output dimension.\n\n              Example::\n\n              - input shape = (2,3,4), shape = (-3,4), output shape = (6,4)\n              - input shape = (2,3,4,5), shape = (-3,-3), output shape = (6,20)\n              - input shape = (2,3,4), shape = (0,-3), output shape = (2,12)\n              - input shape = (2,3,4), shape = (-3,-2), output shape = (6,4)\n\n            - ``-4`` split one dimension of the input into two dimensions passed subsequent to\n              -4 in shape (can contain -1).\n\n              Example::\n\n              - input shape = (2,3,4), shape = (-4,1,2,-2), output shape =(1,2,3,4)\n              - input shape = (2,3,4), shape = (2,-4,-1,3,-2), output shape = (2,1,3,4)\n\n            - If the argument `reverse` is set to 1, then the special values are inferred from right\n              to left.\n\n              Example::\n\n              - without reverse=1, for input shape = (10,5,4), shape = (-1,0), output shape would be \\\n                (40,5).\n              - with reverse=1, output shape will be (50,4).\n\n        reverse : bool, default False\n            If true then the special values are inferred from right to left. Only supported as\n            keyword argument.\n\n\n        Returns\n        -------\n        NDArray\n            An array with desired shape that shares data with this array.\n\n        Examples\n        --------\n        >>> x = mx.nd.arange(0,6).reshape(2,3)\n        >>> x.asnumpy()\n        array([[ 0.,  1.,  2.],\n               [ 3.,  4.,  5.]], dtype=float32)\n        >>> y = x.reshape(3,2)\n        >>> y.asnumpy()\n        array([[ 0.,  1.],\n               [ 2.,  3.],\n               [ 4.,  5.]], dtype=float32)\n        >>> y = x.reshape(3,-1)\n        >>> y.asnumpy()\n        array([[ 0.,  1.],\n               [ 2.,  3.],\n               [ 4.,  5.]], dtype=float32)\n        >>> y = x.reshape(3,2)\n        >>> y.asnumpy()\n        array([[ 0.,  1.],\n               [ 2.,  3.],\n               [ 4.,  5.]], dtype=float32)\n        >>> y = x.reshape(-3)\n        >>> y.asnumpy()\n        array([ 0.  1.  2.  3.  4.  5.], dtype=float32)\n        >>> y[:] = -1\n        >>> x.asnumpy()\n        array([[-1., -1., -1.],\n               [-1., -1., -1.]], dtype=float32)\n        \"\"\"\n        if len(shape) == 1 and isinstance(shape[0], (list, tuple)):\n            shape = shape[0]\n        elif not shape:\n            shape = kwargs.get('shape')\n            assert shape, \"Shape must be provided.\"\n        if not all(k in ['shape', 'reverse'] for k in kwargs):\n            raise TypeError(\n                \"Got unknown keywords in reshape: {}. \" \\\n                \"Accepted keyword arguments are 'shape' and 'reverse'.\".format(\n                    ', '.join([k for k in kwargs if k not in ['shape', 'reverse']])))\n        reverse = kwargs.get('reverse', False)\n        handle = NDArrayHandle()\n\n        # Actual reshape\n        check_call(_LIB.MXNDArrayReshape64(self.handle,\n                                           len(shape),\n                                           c_array(ctypes.c_int64, shape),\n                                           reverse,\n                                           ctypes.byref(handle)))\n        return NDArray(handle=handle, writable=self.writable)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbroadcasting the input array to a new shape.", "response": "def broadcast_to(self, shape):\n        \"\"\"Broadcasts the input array to a new shape.\n\n        Broadcasting is only allowed on axes with size 1. The new shape cannot change\n        the number of dimensions.\n        For example, you could broadcast from shape (2, 1) to (2, 3), but not from\n        shape (2, 3) to (2, 3, 3).\n\n        Parameters\n        ----------\n        shape : tuple of int\n            The shape of the desired array.\n\n        Returns\n        -------\n        NDArray\n            A NDArray with the desired shape that is not sharing data with this\n            array, even if the new shape is the same as ``self.shape``.\n\n        Examples\n        --------\n        >>> x = mx.nd.arange(0,3).reshape((1,3,1))\n        >>> x.asnumpy()\n        array([[[ 0.],\n                [ 1.],\n                [ 2.]]], dtype=float32)\n        >>> y = x.broadcast_to((2,3,3))\n        >>> y.asnumpy()\n        array([[[ 0.,  0.,  0.],\n                [ 1.,  1.,  1.],\n                [ 2.,  2.,  2.]],\n        <BLANKLINE>\n               [[ 0.,  0.,  0.],\n                [ 1.,  1.,  1.],\n                [ 2.,  2.,  2.]]], dtype=float32)\n        \"\"\"\n        cur_shape = self.shape\n        err_str = 'operands could not be broadcast together with remapped shapes' \\\n                  '[original->remapped]: {} and requested shape {}'.format(cur_shape, shape)\n        if len(shape) < len(cur_shape):\n            raise ValueError(err_str)\n        cur_shape = (1,) * (len(shape) - len(cur_shape)) + cur_shape\n        cur_shape_arr = np.array(cur_shape)\n        broadcasting_axes = np.nonzero(cur_shape_arr != np.array(shape))\n        if (cur_shape_arr[broadcasting_axes] != 1).any():\n            raise ValueError(err_str)\n        if cur_shape != self.shape:\n            return op.broadcast_to(self.reshape(cur_shape), shape=shape)\n        else:\n            return op.broadcast_to(self, shape=tuple(shape))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef context(self):\n        dev_typeid = ctypes.c_int()\n        dev_id = ctypes.c_int()\n        check_call(_LIB.MXNDArrayGetContext(\n            self.handle, ctypes.byref(dev_typeid), ctypes.byref(dev_id)))\n        return Context(Context.devtype2str[dev_typeid.value], dev_id.value)", "response": "Return the device context of the array."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _fresh_grad(self):\n        out = ctypes.c_int()\n        check_call(_LIB.MXNDArrayGetGradState(self.handle, ctypes.byref(out)))\n        return out.value", "response": "Whether this array has corresponding gradient array\n        updated by Autograd. backward."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef asnumpy(self):\n        data = np.empty(self.shape, dtype=self.dtype)\n        check_call(_LIB.MXNDArraySyncCopyToCPU(\n            self.handle,\n            data.ctypes.data_as(ctypes.c_void_p),\n            ctypes.c_size_t(data.size)))\n        return data", "response": "Returns a numpy. ndarray with value copied from this array."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef astype(self, dtype, copy=True):\n\n        if not copy and np.dtype(dtype) == self.dtype:\n            return self\n\n        res = empty(self.shape, ctx=self.context, dtype=dtype)\n        self.copyto(res)\n        return res", "response": "Returns a copy of the array after casting to a specified type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncopy the value of this array to another array.", "response": "def copyto(self, other):\n        \"\"\"Copies the value of this array to another array.\n\n        If ``other`` is a ``NDArray`` object, then ``other.shape`` and\n        ``self.shape`` should be the same. This function copies the value from\n        ``self`` to ``other``.\n\n        If ``other`` is a context, a new ``NDArray`` will be first created on\n        the target context, and the value of ``self`` is copied.\n\n        Parameters\n        ----------\n        other : NDArray or Context\n            The destination array or context.\n\n        Returns\n        -------\n        NDArray, CSRNDArray or RowSparseNDArray\n            The copied array. If ``other`` is an ``NDArray``, then the return value\n            and ``other`` will point to the same ``NDArray``.\n\n        Examples\n        --------\n        >>> x = mx.nd.ones((2,3))\n        >>> y = mx.nd.zeros((2,3), mx.gpu(0))\n        >>> z = x.copyto(y)\n        >>> z is y\n        True\n        >>> y.asnumpy()\n        array([[ 1.,  1.,  1.],\n               [ 1.,  1.,  1.]], dtype=float32)\n        >>> y.copyto(mx.gpu(0))\n        <NDArray 2x3 @gpu(0)>\n\n        \"\"\"\n        if isinstance(other, NDArray):\n            if other.handle is self.handle:\n                warnings.warn('You are attempting to copy an array to itself', RuntimeWarning)\n                return False\n            return _internal._copyto(self, out=other)\n        elif isinstance(other, Context):\n            hret = NDArray(_new_alloc_handle(self.shape, other, True, self.dtype))\n            return _internal._copyto(self, out=hret)\n        else:\n            raise TypeError('copyto does not support type ' + str(type(other)))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef as_in_context(self, context):\n        if self.context == context:\n            return self\n        return self.copyto(context)", "response": "Returns an array on the target device with the same value as this array."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef attach_grad(self, grad_req='write', stype=None):\n        from . import zeros as _zeros\n        if stype is not None:\n            grad = _zeros(self.shape, stype=stype)\n        else:\n            grad = op.zeros_like(self)  # pylint: disable=undefined-variable\n        grad_req = _GRAD_REQ_MAP[grad_req]\n        check_call(_LIB.MXAutogradMarkVariables(\n            1, ctypes.pointer(self.handle),\n            ctypes.pointer(mx_uint(grad_req)),\n            ctypes.pointer(grad.handle)))", "response": "Attach a gradient buffer to this NDArray so that backward can compute gradient with respect to it."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef grad(self):\n        from . import _ndarray_cls\n        hdl = NDArrayHandle()\n        check_call(_LIB.MXNDArrayGetGrad(self.handle, ctypes.byref(hdl)))\n        if hdl.value is None:\n            return None\n        return _ndarray_cls(hdl)", "response": "Returns the gradient buffer attached to this NDArray."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef detach(self):\n        from . import _ndarray_cls\n        hdl = NDArrayHandle()\n        check_call(_LIB.MXNDArrayDetach(self.handle, ctypes.byref(hdl)))\n        return _ndarray_cls(hdl)", "response": "Returns a new NDArray detached from the current graph."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the gradients of this NDArray w. r. t variables.", "response": "def backward(self, out_grad=None, retain_graph=False, train_mode=True):\n        \"\"\"Compute the gradients of this NDArray w.r.t variables.\n\n        Parameters\n        ----------\n        out_grad : NDArray, optional\n            Gradient with respect to head.\n        retain_graph : bool, optional\n            Whether to retain the computaion graph for another backward\n            pass on the same graph. By default the computaion history\n            is cleared.\n        train_mode : bool, optional\n            Whether to compute gradient for training or inference.\n        \"\"\"\n        if out_grad is None:\n            ograd_handles = [NDArrayHandle(0)]\n        else:\n            ograd_handles = [out_grad.handle]\n\n        check_call(_LIB.MXAutogradBackwardEx(\n            1, c_handle_array([self]),\n            c_array(NDArrayHandle, ograd_handles),\n            0,\n            ctypes.c_void_p(0),\n            ctypes.c_int(retain_graph),\n            ctypes.c_int(0),\n            ctypes.c_int(train_mode),\n            ctypes.c_void_p(0),\n            ctypes.c_void_p(0)))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbuilding the align array", "response": "def build(self, align_path):\n        \"\"\"\n        Build the align array\n        \"\"\"\n        file = open(align_path, 'r')\n        lines = file.readlines()\n        file.close()\n        # words: list([op, ed, word])\n        words = []\n        for line in lines:\n            _op, _ed, word = line.strip().split(' ')\n            if word not in Align.skip_list:\n                words.append((int(_op), int(_ed), word))\n        self.words = words\n        self.n_words = len(words)\n        self.sentence_str = \" \".join([w[2] for w in self.words])\n        self.sentence_length = len(self.sentence_str)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the sentence of the log entry", "response": "def sentence(self, padding=75):\n        \"\"\"\n        Get sentence\n        \"\"\"\n        vec = word_to_vector(self.sentence_str)\n        vec += [-1] * (padding - self.sentence_length)\n        return np.array(vec, dtype=np.int32)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef word(self, _id, padding=75):\n        word = self.words[_id][2]\n        vec = word_to_vector(word)\n        vec += [-1] * (padding - len(vec))\n        return np.array(vec, dtype=np.int32)", "response": "Get the word vector for a given ID"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef word_frame_pos(self, _id):\n        left = int(self.words[_id][0]/1000)\n        right = max(left+1, int(self.words[_id][1]/1000))\n        return (left, right)", "response": "Get the position of the word frame in the words array"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef prepare_sparse_params(self, param_rowids):\n        '''Prepares the module for processing a data batch by pulling row_sparse\n        parameters from kvstore to all devices based on rowids.\n\n        Parameters\n        ----------\n        param_rowids : dict of str to NDArray of list of NDArrays\n        '''\n        if not self._kvstore:\n            return\n        assert(isinstance(param_rowids, dict))\n        for param_name, rowids in param_rowids.items():\n            if isinstance(rowids, (tuple, list)):\n                rowids_1d = []\n                for r in rowids:\n                    rowids_1d.append(r.reshape((-1,)).astype(np.int64))\n                rowid = mx.nd.concat(*rowids_1d, dim=0)\n            else:\n                rowid = rowids\n            param_idx = self._exec_group.param_names.index(param_name)\n            param_val = self._exec_group.param_arrays[param_idx]\n            self._kvstore.row_sparse_pull(param_name, param_val, row_ids=rowid,\n                                          priority=-param_idx)", "response": "Prepares the module for processing a data batch by pulling row_sparse\n            parameters from kvstore to all devices based on rowids."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsave model parameters to file.", "response": "def save_params(self, fname):\n        \"\"\"Saves model parameters to file.\n        Parameters\n        ----------\n        fname : str\n            Path to output param file.\n        Examples\n        --------\n        >>> # An example of saving module parameters.\n        >>> mod.save_params('myfile')\n        \"\"\"\n        arg_params, aux_params = self.get_params_from_kv(self._arg_params, self._aux_params)\n        save_dict = {('arg:%s' % k) : v.as_in_context(mx.cpu()) for k, v in arg_params.items()}\n        save_dict.update({('aux:%s' % k) : v.as_in_context(mx.cpu()) for k, v in aux_params.items()})\n        mx.nd.save(fname, save_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_params_from_kv(self, arg_params, aux_params):\n        assert(self._kvstore is not None)\n        for name, block in zip(self._exec_group.param_names, self._exec_group.param_arrays):\n            assert(isinstance(block, list))\n            if block[0].stype == 'row_sparse':\n                row_ids = mx.nd.arange(start=0, stop=block[0].shape[0], dtype='int64')\n                self._kvstore.row_sparse_pull(name, arg_params[name], row_ids=row_ids)\n            else:\n                assert(block[0].stype == 'default')\n                self._kvstore.pull(name, out=arg_params[name])\n        if len(aux_params) > 0:\n            raise NotImplementedError()\n        return arg_params, aux_params", "response": "Copy data from kvstore to arg_params and aux_params."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nclips gradient norm by max_norm per context.", "response": "def clip_by_global_norm_per_ctx(self, max_norm=1.0, param_names=None):\n        \"\"\"Clips gradient norm.\n\n        The norm is computed over all gradients together, as if they were\n         concatenated into a single vector. Gradients are modified in-place.\n\n        The method is first used in\n         `[ICML2013] On the difficulty of training recurrent neural networks`\n\n        Note that the gradients are concatenated per context in this implementation.\n\n        Examples\n        --------\n        An example of using clip_grad_norm to clip the gradient before updating the parameters::\n            >>> #Get the gradient via back-propagation\n            >>> net.forward_backward(data_batch=data_batch)\n            >>> norm_val = net.clip_by_global_norm(max_norm=2.0, param_names='w0')\n            >>> net.update()\n        \"\"\"\n        assert self.binded and self.params_initialized and self.optimizer_initialized\n        num_ctx = len(self._exec_group.grad_arrays[0])\n        grad_array_per_ctx = [[] for i in range(num_ctx)]\n        assert(param_names is not None)\n        for param_name in param_names:\n            param_idx = self._exec_group.param_names.index(param_name)\n            grad_val = self._exec_group.grad_arrays[param_idx]\n            assert(len(grad_val) == num_ctx)\n            for i in range(num_ctx):\n                grad_array_per_ctx[i].append(grad_val[i])\n        norm_vals = []\n        for i in range(num_ctx):\n            mx.gluon.utils.clip_global_norm(grad_array_per_ctx[i], max_norm)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rescale_grad(self, scale=None, param_name=None):\n        if scale is None or param_name is None:\n            return\n        param_idx = self._exec_group.param_names.index(param_name)\n        grad_vals = self._exec_group.grad_arrays[param_idx]\n        for grad in grad_vals:\n            grad[:] *= scale", "response": "Rescales the gradient of the parameters of the specified parameters by a certain scale."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds a factorization machine network with proper formulation.", "response": "def factorization_machine_model(factor_size, num_features,\n                                lr_mult_config, wd_mult_config, init_config):\n    \"\"\" builds factorization machine network with proper formulation:\n    y = w_0 \\sum(x_i w_i) + 0.5(\\sum\\sum<v_i,v_j>x_ix_j - \\sum<v_iv_i>x_i^2)\n    \"\"\"\n    x = mx.symbol.Variable(\"data\", stype='csr')\n    # factor, linear and bias terms\n    v = mx.symbol.Variable(\"v\", shape=(num_features, factor_size), stype='row_sparse',\n                           init=init_config['v'], lr_mult=lr_mult_config['v'],\n                           wd_mult=wd_mult_config['v'])\n    w = mx.symbol.Variable('w', shape=(num_features, 1), stype='row_sparse',\n                      init=init_config['w'], lr_mult=lr_mult_config['w'],\n                      wd_mult=wd_mult_config['w'])\n    w0 = mx.symbol.Variable('w0', shape=(1,), init=init_config['w0'],\n                       lr_mult=lr_mult_config['w0'], wd_mult=wd_mult_config['w0'])\n    w1 = mx.symbol.broadcast_add(mx.symbol.dot(x, w), w0)\n\n    # squared terms for subtracting self interactions\n    v_s = mx.symbol._internal._square_sum(data=v, axis=1, keepdims=True)\n    x_s = x.square()\n    bd_sum = mx.sym.dot(x_s, v_s)\n\n    # interactions\n    w2 = mx.symbol.dot(x, v)\n    w2_squared = 0.5 * mx.symbol.square(data=w2)\n\n    # putting everything together\n    w_all = mx.symbol.Concat(w1, w2_squared, dim=1)\n    sum1 = w_all.sum(axis=1, keepdims=True)\n    sum2 = -0.5 * bd_sum\n    model = sum1 + sum2\n\n    y = mx.symbol.Variable(\"softmax_label\")\n    model = mx.symbol.LogisticRegressionOutput(data=model, label=y)\n    return model"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef batchify(data, batch_size):\n    nbatch = data.shape[0] // batch_size\n    data = data[:nbatch * batch_size]\n    data = data.reshape((batch_size, nbatch)).T\n    return data", "response": "Reshape data into batch_size"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef tokenize(self, path):\n        assert os.path.exists(path)\n        # Add words to the dictionary\n        with open(path, 'r') as f:\n            tokens = 0\n            for line in f:\n                words = line.split() + ['<eos>']\n                tokens += len(words)\n                for word in words:\n                    self.dictionary.add_word(word)\n\n        # Tokenize file content\n        with open(path, 'r') as f:\n            ids = np.zeros((tokens,), dtype='int32')\n            token = 0\n            for line in f:\n                words = line.split() + ['<eos>']\n                for word in words:\n                    ids[token] = self.dictionary.word2idx[word]\n                    token += 1\n\n        return mx.nd.array(ids, dtype='int32')", "response": "Tokenizes a text file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _build_doc(func_name,\n               desc,\n               arg_names,\n               arg_types,\n               arg_desc,\n               key_var_num_args=None,\n               ret_type=None):\n    \"\"\"Build docstring for symbolic functions.\"\"\"\n    param_str = _build_param_doc(arg_names, arg_types, arg_desc)\n    if key_var_num_args:\n        desc += '\\nThis function support variable length of positional input.'\n    doc_str = ('%s\\n\\n' +\n               '%s\\n' +\n               'name : string, optional.\\n' +\n               '    Name of the resulting symbol.\\n\\n' +\n               'Returns\\n' +\n               '-------\\n' +\n               'Symbol\\n' +\n               '    The result symbol.')\n    doc_str = doc_str % (desc, param_str)\n    extra_doc = \"\\n\" + '\\n'.join([x.__doc__ for x in type.__subclasses__(SymbolDoc)\n                                  if x.__name__ == '%sDoc' % func_name])\n    doc_str += _re.sub(_re.compile(\"    \"), \"\", extra_doc)\n    doc_str = _re.sub('NDArray-or-Symbol', 'Symbol', doc_str)\n    return doc_str", "response": "Build docstring for symbolic functions."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_output_shape(sym, **input_shapes):\n        _, s_outputs, _ = sym.infer_shape(**input_shapes)\n        return dict(zip(sym.list_outputs(), s_outputs))", "response": "Get user friendly information of the output shapes."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nqueries CUDA for the number of GPUs present.", "response": "def num_gpus():\n    \"\"\"Query CUDA for the number of GPUs present.\n\n    Raises\n    ------\n    Will raise an exception on any CUDA error.\n\n    Returns\n    -------\n    count : int\n        The number of GPUs.\n\n    \"\"\"\n    count = ctypes.c_int()\n    check_call(_LIB.MXGetGPUCount(ctypes.byref(count)))\n    return count.value"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nqueries CUDA for the free and total bytes of GPU global memory.", "response": "def gpu_memory_info(device_id=0):\n    \"\"\"Query CUDA for the free and total bytes of GPU global memory.\n\n    Parameters\n    ----------\n    device_id : int, optional\n        The device id of the GPU device.\n\n    Raises\n    ------\n    Will raise an exception on any CUDA error.\n\n    Returns\n    -------\n    (free, total) : (int, int)\n        The number of GPUs.\n\n    \"\"\"\n    free = ctypes.c_uint64()\n    total = ctypes.c_uint64()\n    dev_id = ctypes.c_int(device_id)\n    check_call(_LIB.MXGetGPUMemoryInformation64(dev_id, ctypes.byref(free), ctypes.byref(total)))\n    return (free.value, total.value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the current context.", "response": "def current_context():\n    \"\"\"Returns the current context.\n\n    By default, `mx.cpu()` is used for all the computations\n    and it can be overridden by using `with mx.Context(x)` statement where\n    x can be cpu(device_id) or gpu(device_id).\n\n    Examples\n    -------\n    >>> mx.current_context()\n    cpu(0)\n    >>> with mx.Context('gpu', 1):  # Context changed in `with` block.\n    ...    mx.current_context()  # Computation done here will be on gpu(1).\n    ...\n    gpu(1)\n    >>> mx.current_context() # Back to default context.\n    cpu(0)\n\n    Returns\n    -------\n    default_ctx : Context\n    \"\"\"\n    if not hasattr(Context._default_ctx, \"value\"):\n        Context._default_ctx.value = Context('cpu', 0)\n    return Context._default_ctx.value"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npopulates the items and synsets - a map of index to label for the data items.", "response": "def _list_audio_files(self, root, skip_rows=0):\n        \"\"\"Populates synsets - a map of index to label for the data items.\n        Populates the data in the dataset, making tuples of (data, label)\n        \"\"\"\n        self.synsets = []\n        self.items = []\n        if not self._train_csv:\n            # The audio files are organized in folder structure with\n            # directory name as label and audios in them\n            self._folder_structure(root)\n        else:\n            # train_csv contains mapping between filename and label\n            self._csv_labelled_dataset(root, skip_rows=skip_rows)\n\n        # Generating the synset.txt file now\n        if not os.path.exists(\"./synset.txt\"):\n            with open(\"./synset.txt\", \"w\") as synsets_file:\n                for item in self.synsets:\n                    synsets_file.write(item+os.linesep)\n            print(\"Synsets is generated as synset.txt\")\n        else:\n            warnings.warn(\"Synset file already exists in the current directory! Not generating synset.txt.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef transform_first(self, fn, lazy=False):\n        return super(AudioFolderDataset, self).transform_first(fn, lazy=lazy)", "response": "Returns a new dataset with the first element of each sample\n            transformed by the transformer function fn."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntry to configure cython and return cython configuration", "response": "def config_cython():\n    \"\"\"Try to configure cython and return cython configuration\"\"\"\n    if not with_cython:\n        return []\n    # pylint: disable=unreachable\n    if os.name == 'nt':\n        print(\"WARNING: Cython is not supported on Windows, will compile without cython module\")\n        return []\n\n    try:\n        from Cython.Build import cythonize\n        # from setuptools.extension import Extension\n        if sys.version_info >= (3, 0):\n            subdir = \"_cy3\"\n        else:\n            subdir = \"_cy2\"\n        ret = []\n        path = \"mxnet/cython\"\n        if os.name == 'nt':\n            library_dirs = ['mxnet', '../build/Release', '../build']\n            libraries = ['libmxnet']\n        else:\n            library_dirs = None\n            libraries = None\n\n        for fn in os.listdir(path):\n            if not fn.endswith(\".pyx\"):\n                continue\n            ret.append(Extension(\n                \"mxnet/%s/.%s\" % (subdir, fn[:-4]),\n                [\"mxnet/cython/%s\" % fn],\n                include_dirs=[\"../include/\", \"../3rdparty/tvm/nnvm/include\"],\n                library_dirs=library_dirs,\n                libraries=libraries,\n                language=\"c++\"))\n        return cythonize(ret)\n    except ImportError:\n        print(\"WARNING: Cython is not installed, will compile without cython module\")\n        return []"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _compose(self, *args, **kwargs):\n        name = kwargs.pop('name', None)\n\n        if name:\n            name = c_str(name)\n        if len(args) != 0 and len(kwargs) != 0:\n            raise TypeError('compose only accept input Symbols \\\n                either as positional or keyword arguments, not both')\n\n        for arg in args:\n            if not isinstance(arg, SymbolBase):\n                raise TypeError('Compose expect `Symbol` as arguments')\n        for val in kwargs.values():\n            if not isinstance(val, SymbolBase):\n                raise TypeError('Compose expect `Symbol` as arguments')\n\n        num_args = len(args) + len(kwargs)\n        if len(kwargs) != 0:\n            keys = c_str_array(kwargs.keys())\n            args = c_handle_array(kwargs.values())\n        else:\n            keys = None\n            args = c_handle_array(kwargs.values())\n        check_call(_LIB.NNSymbolCompose(\n            self.handle, name, num_args, keys, args))", "response": "This method is used to compose a current symbol on inputs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _set_attr(self, **kwargs):\n        keys = c_str_array(kwargs.keys())\n        vals = c_str_array([str(s) for s in kwargs.values()])\n        num_args = mx_uint(len(kwargs))\n        check_call(_LIB.MXSymbolSetAttrs(\n            self.handle, num_args, keys, vals))", "response": "Set the attribute of the symbol."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_config(network, data_shape, **kwargs):\n    if network == 'vgg16_reduced':\n        if data_shape >= 448:\n            from_layers = ['relu4_3', 'relu7', '', '', '', '', '']\n            num_filters = [512, -1, 512, 256, 256, 256, 256]\n            strides = [-1, -1, 2, 2, 2, 2, 1]\n            pads = [-1, -1, 1, 1, 1, 1, 1]\n            sizes = [[.07, .1025], [.15,.2121], [.3, .3674], [.45, .5196], [.6, .6708], \\\n                [.75, .8216], [.9, .9721]]\n            ratios = [[1,2,.5], [1,2,.5,3,1./3], [1,2,.5,3,1./3], [1,2,.5,3,1./3], \\\n                [1,2,.5,3,1./3], [1,2,.5], [1,2,.5]]\n            normalizations = [20, -1, -1, -1, -1, -1, -1]\n            steps = [] if data_shape != 512 else [x / 512.0 for x in\n                [8, 16, 32, 64, 128, 256, 512]]\n        else:\n            from_layers = ['relu4_3', 'relu7', '', '', '', '']\n            num_filters = [512, -1, 512, 256, 256, 256]\n            strides = [-1, -1, 2, 2, 1, 1]\n            pads = [-1, -1, 1, 1, 0, 0]\n            sizes = [[.1, .141], [.2,.272], [.37, .447], [.54, .619], [.71, .79], [.88, .961]]\n            ratios = [[1,2,.5], [1,2,.5,3,1./3], [1,2,.5,3,1./3], [1,2,.5,3,1./3], \\\n                [1,2,.5], [1,2,.5]]\n            normalizations = [20, -1, -1, -1, -1, -1]\n            steps = [] if data_shape != 300 else [x / 300.0 for x in [8, 16, 32, 64, 100, 300]]\n        if not (data_shape == 300 or data_shape == 512):\n            logging.warn('data_shape %d was not tested, use with caucious.' % data_shape)\n        return locals()\n    elif network == 'inceptionv3':\n        from_layers = ['ch_concat_mixed_7_chconcat', 'ch_concat_mixed_10_chconcat', '', '', '', '']\n        num_filters = [-1, -1, 512, 256, 256, 128]\n        strides = [-1, -1, 2, 2, 2, 2]\n        pads = [-1, -1, 1, 1, 1, 1]\n        sizes = [[.1, .141], [.2,.272], [.37, .447], [.54, .619], [.71, .79], [.88, .961]]\n        ratios = [[1,2,.5], [1,2,.5,3,1./3], [1,2,.5,3,1./3], [1,2,.5,3,1./3], \\\n            [1,2,.5], [1,2,.5]]\n        normalizations = -1\n        steps = []\n        return locals()\n    elif network == 'resnet50':\n        num_layers = 50\n        image_shape = '3,224,224'  # resnet require it as shape check\n        network = 'resnet'\n        from_layers = ['_plus12', '_plus15', '', '', '', '']\n        num_filters = [-1, -1, 512, 256, 256, 128]\n        strides = [-1, -1, 2, 2, 2, 2]\n        pads = [-1, -1, 1, 1, 1, 1]\n        sizes = [[.1, .141], [.2,.272], [.37, .447], [.54, .619], [.71, .79], [.88, .961]]\n        ratios = [[1,2,.5], [1,2,.5,3,1./3], [1,2,.5,3,1./3], [1,2,.5,3,1./3], \\\n            [1,2,.5], [1,2,.5]]\n        normalizations = -1\n        steps = []\n        return locals()\n    elif network == 'resnet101':\n        num_layers = 101\n        image_shape = '3,224,224'\n        network = 'resnet'\n        from_layers = ['_plus29', '_plus32', '', '', '', '']\n        num_filters = [-1, -1, 512, 256, 256, 128]\n        strides = [-1, -1, 2, 2, 2, 2]\n        pads = [-1, -1, 1, 1, 1, 1]\n        sizes = [[.1, .141], [.2,.272], [.37, .447], [.54, .619], [.71, .79], [.88, .961]]\n        ratios = [[1,2,.5], [1,2,.5,3,1./3], [1,2,.5,3,1./3], [1,2,.5,3,1./3], \\\n            [1,2,.5], [1,2,.5]]\n        normalizations = -1\n        steps = []\n        return locals()\n    else:\n        msg = 'No configuration found for %s with data_shape %d' % (network, data_shape)\n        raise NotImplementedError(msg)", "response": "Returns a configuration dictionary for the base network."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_symbol_train(network, data_shape, **kwargs):\n    if network.startswith('legacy'):\n        logging.warn('Using legacy model.')\n        return symbol_builder.import_module(network).get_symbol_train(**kwargs)\n    config = get_config(network, data_shape, **kwargs).copy()\n    config.update(kwargs)\n    return symbol_builder.get_symbol_train(**config)", "response": "Wrapper for get symbol for train\n\n    Parameters\n    ----------\n    network : str\n        name for the base network symbol\n    data_shape : int\n        input shape\n    kwargs : dict\n        see symbol_builder.get_symbol_train for more details"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the trainer this parameter is associated with.", "response": "def _set_trainer(self, trainer):\n        \"\"\" Set the trainer this parameter is associated with. \"\"\"\n        # trainer cannot be replaced for sparse params\n        if self._stype != 'default' and self._trainer and trainer and self._trainer is not trainer:\n            raise RuntimeError(\n                \"Failed to set the trainer for Parameter '%s' because it was already set. \" \\\n                \"More than one trainers for a %s Parameter is not supported.\" \\\n                %(self.name, self._stype))\n        self._trainer = trainer"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets row_sparse data from the trainer based on row ids.", "response": "def _get_row_sparse(self, arr_list, ctx, row_id):\n        \"\"\" Get row_sparse data from row_sparse parameters based on row_id. \"\"\"\n        # get row sparse params based on row ids\n        if not isinstance(row_id, ndarray.NDArray):\n            raise TypeError(\"row_id must have NDArray type, but %s is given\"%(type(row_id)))\n        if not self._trainer:\n            raise RuntimeError(\"Cannot get row_sparse data for Parameter '%s' when no \" \\\n                               \"Trainer is created with it.\"%self.name)\n        results = self._check_and_get(arr_list, ctx)\n\n        # fetch row sparse params from the trainer\n        self._trainer._row_sparse_pull(self, results, row_id)\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _load_init(self, data, ctx):\n        if self.shape:\n            for self_dim, data_dim in zip(self.shape, data.shape):\n                assert self_dim in (0, data_dim), \\\n                    \"Failed loading Parameter '%s' from saved params: \" \\\n                    \"shape incompatible expected %s vs saved %s\"%(\n                        self.name, str(self.shape), str(data.shape))\n            self.shape = tuple(i if i != 0 else j for i, j in zip(self.shape, data.shape))\n        if self.dtype:\n            assert np.dtype(self.dtype).type == data.dtype, \\\n                \"Failed loading Parameter '%s' from saved params: \" \\\n                \"dtype incompatible expected %s vs saved %s\"%(\n                    self.name, str(self.dtype), str(data.dtype))\n        if self._stype != data.stype:\n            data = data.tostype(self._stype)\n        if isinstance(ctx, Context):\n            ctx = [ctx]\n        if self._data is None:\n            if self._deferred_init:\n                assert ctx is None or set(ctx) == set(self._deferred_init[1]), \\\n                    \"Failed to load Parameter '%s' on %s because it was \" \\\n                    \"previous initialized on %s.\"%(\n                        self.name, str(ctx), str(self.list_ctx()))\n                ctx = self._deferred_init[1]\n            elif ctx is None:\n                ctx = [cpu()]\n            self._init_impl(data, ctx)\n        else:\n            assert ctx is None or set(ctx) == set(self.list_ctx()), \\\n                \"Failed to load Parameter '%s' on %s because it was \" \\\n                \"previous initialized on %s.\"%(\n                    self.name, str(ctx), str(self.list_ctx()))\n            self.set_data(data)\n        self._deferred_init = ()", "response": "Load the parameters from the data."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _init_impl(self, data, ctx_list):\n        self._ctx_list = list(ctx_list)\n        self._ctx_map = [[], []]\n        for i, ctx in enumerate(self._ctx_list):\n            dev_list = self._ctx_map[ctx.device_typeid&1]\n            while len(dev_list) <= ctx.device_id:\n                dev_list.append(None)\n            dev_list[ctx.device_id] = i\n\n        self._data = [data.copyto(ctx) for ctx in self._ctx_list]\n        self._init_grad()", "response": "Sets data and grad."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _init_grad(self):\n        if self.grad_req == 'null':\n            self._grad = None\n            return\n\n        self._grad = [ndarray.zeros(shape=i.shape, dtype=i.dtype, ctx=i.context,\n                                    stype=self._grad_stype) for i in self._data]\n\n        autograd.mark_variables(self._check_and_get(self._data, list),\n                                self._grad, self.grad_req)", "response": "Initialize the gradients buffers."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreducing data from multiple context to cpu.", "response": "def _reduce(self):\n        \"\"\"Reduce data from multiple context to cpu.\"\"\"\n        ctx = context.cpu()\n        if self._stype == 'default':\n            block = self.list_data()\n            data = ndarray.add_n(*(w.copyto(ctx) for w in block)) / len(block)\n        else:\n            # fetch all rows for 'row_sparse' param\n            all_row_ids = ndarray.arange(0, self.shape[0], dtype='int64', ctx=ctx)\n            data = ndarray.zeros(self.shape, stype='row_sparse', ctx=ctx)\n            self._trainer._row_sparse_pull(self, data, all_row_ids, full_idx=True)\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninitialize the parameter and gradient arrays.", "response": "def initialize(self, init=None, ctx=None, default_init=initializer.Uniform(),\n                   force_reinit=False):\n        \"\"\"Initializes parameter and gradient arrays. Only used for :py:class:`NDArray` API.\n\n        Parameters\n        ----------\n        init : Initializer\n            The initializer to use. Overrides :py:meth:`Parameter.init` and default_init.\n        ctx : Context or list of Context, defaults to :py:meth:`context.current_context()`.\n            Initialize Parameter on given context. If ctx is a list of Context, a\n            copy will be made for each context.\n\n            .. note::\n                Copies are independent arrays. User is responsible for keeping\n                their values consistent when updating.\n                Normally :py:class:`gluon.Trainer` does this for you.\n\n        default_init : Initializer\n            Default initializer is used when both :py:func:`init`\n            and :py:meth:`Parameter.init` are ``None``.\n        force_reinit : bool, default False\n            Whether to force re-initialization if parameter is already initialized.\n\n        Examples\n        --------\n        >>> weight = mx.gluon.Parameter('weight', shape=(2, 2))\n        >>> weight.initialize(ctx=mx.cpu(0))\n        >>> weight.data()\n        [[-0.01068833  0.01729892]\n         [ 0.02042518 -0.01618656]]\n        <NDArray 2x2 @cpu(0)>\n        >>> weight.grad()\n        [[ 0.  0.]\n         [ 0.  0.]]\n        <NDArray 2x2 @cpu(0)>\n        >>> weight.initialize(ctx=[mx.gpu(0), mx.gpu(1)])\n        >>> weight.data(mx.gpu(0))\n        [[-0.00873779 -0.02834515]\n         [ 0.05484822 -0.06206018]]\n        <NDArray 2x2 @gpu(0)>\n        >>> weight.data(mx.gpu(1))\n        [[-0.00873779 -0.02834515]\n         [ 0.05484822 -0.06206018]]\n        <NDArray 2x2 @gpu(1)>\n        \"\"\"\n        if self._data is not None and not force_reinit:\n            warnings.warn(\"Parameter '%s' is already initialized, ignoring. \" \\\n                          \"Set force_reinit=True to re-initialize.\"%self.name,\n                          stacklevel=2)\n            return\n        self._data = self._grad = None\n\n        if ctx is None:\n            ctx = [context.current_context()]\n        if isinstance(ctx, Context):\n            ctx = [ctx]\n        if init is None:\n            init = default_init if self.init is None else self.init\n        if not self.shape or np.prod(self.shape) <= 0:\n            if self._allow_deferred_init:\n                self._deferred_init = (init, ctx, default_init, None)\n                return\n            raise ValueError(\"Cannot initialize Parameter '%s' because it has \" \\\n                             \"invalid shape: %s.\"%(self.name, str(self.shape)))\n\n        self._deferred_init = (init, ctx, default_init, None)\n        self._finish_deferred_init()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_data(self, data):\n        self.shape = data.shape\n\n        if self._data is None:\n            assert self._deferred_init, \\\n                \"Parameter '%s' has not been initialized\"%self.name\n            self._deferred_init = self._deferred_init[:3] + (data,)\n            return\n\n        # if update_on_kvstore, we need to make sure the copy stored in kvstore is in sync\n        if self._trainer and self._trainer._kv_initialized and self._trainer._update_on_kvstore:\n            if self not in self._trainer._params_to_init:\n                self._trainer._reset_kvstore()\n\n        for arr in self._check_and_get(self._data, list):\n            arr[:] = data", "response": "Sets this parameter s value on all contexts."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a copy of the row_sparse parameter on the same context as row_id s.", "response": "def row_sparse_data(self, row_id):\n        \"\"\"Returns a copy of the 'row_sparse' parameter on the same context as row_id's.\n        The copy only retains rows whose ids occur in provided row ids.\n        The parameter must have been initialized on this context before.\n\n        Parameters\n        ----------\n        row_id: NDArray\n            Row ids to retain for the 'row_sparse' parameter.\n\n        Returns\n        -------\n        NDArray on row_id's context\n        \"\"\"\n        if self._stype != 'row_sparse':\n            raise RuntimeError(\"Cannot return a copy of Parameter %s via row_sparse_data() \" \\\n                               \"because its storage type is %s. Please use data() instead.\" \\\n                               %(self.name, self._stype))\n        return self._get_row_sparse(self._data, row_id.context, row_id)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list_row_sparse_data(self, row_id):\n        if self._stype != 'row_sparse':\n            raise RuntimeError(\"Cannot return copies of Parameter '%s' on all contexts via \" \\\n                               \"list_row_sparse_data() because its storage type is %s. Please \" \\\n                               \"use data() instead.\" % (self.name, self._stype))\n        return self._get_row_sparse(self._data, list, row_id)", "response": "Returns a list of NDArrays for the row_sparse parameter on all contexts in the same order as the create_row_sparse_data method."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef data(self, ctx=None):\n        if self._stype != 'default':\n            raise RuntimeError(\"Cannot return a copy of Parameter '%s' on ctx %s via data() \" \\\n                               \"because its storage type is %s. Please use row_sparse_data() \" \\\n                               \"instead.\" % (self.name, str(ctx), self._stype))\n        return self._check_and_get(self._data, ctx)", "response": "Returns a copy of this parameter on one context."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list_data(self):\n        if self._stype != 'default':\n            raise RuntimeError(\"Cannot return copies of Parameter '%s' on all contexts via \" \\\n                               \"list_data() because its storage type is %s. Please use \" \\\n                               \"row_sparse_data() instead.\" % (self.name, self._stype))\n        return self._check_and_get(self._data, list)", "response": "Returns a list of NDArrays of this Parameter."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a gradient buffer for this parameter on one context.", "response": "def grad(self, ctx=None):\n        \"\"\"Returns a gradient buffer for this parameter on one context.\n\n        Parameters\n        ----------\n        ctx : Context\n            Desired context.\n        \"\"\"\n        if self._data is not None and self._grad is None:\n            raise RuntimeError(\n                \"Cannot get gradient array for Parameter '%s' \" \\\n                \"because grad_req='null'\"%(self.name))\n        return self._check_and_get(self._grad, ctx)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn gradient buffers on all contexts in the same order as self. values.", "response": "def list_grad(self):\n        \"\"\"Returns gradient buffers on all contexts, in the same order\n        as :py:meth:`values`.\"\"\"\n        if self._data is not None and self._grad is None:\n            raise RuntimeError(\n                \"Cannot get gradient array for Parameter '%s' \" \\\n                \"because grad_req='null'\"%(self.name))\n        return self._check_and_get(self._grad, list)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of contexts this parameter is initialized on.", "response": "def list_ctx(self):\n        \"\"\"Returns a list of contexts this parameter is initialized on.\"\"\"\n        if self._data is None:\n            if self._deferred_init:\n                return self._deferred_init[1]\n            raise RuntimeError(\"Parameter '%s' has not been initialized\"%self.name)\n        return self._ctx_list"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef zero_grad(self):\n        if self._grad is None:\n            return\n        for i in self._grad:\n            ndarray.zeros_like(i, out=i)", "response": "Sets gradient buffer on all contexts to 0. No action is taken if\n        parameter is uninitialized."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef var(self):\n        if self._var is None:\n            self._var = symbol.var(self.name, shape=self.shape, dtype=self.dtype,\n                                   lr_mult=self.lr_mult, wd_mult=self.wd_mult,\n                                   init=self.init, stype=self._stype)\n        return self._var", "response": "Returns a symbol representing this parameter."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cast(self, dtype):\n        self.dtype = dtype\n        if self._data is None:\n            return\n        with autograd.pause():\n            self._data = [i.astype(dtype) for i in self._data]\n            if self._grad is None:\n                return\n            self._grad = [i.astype(dtype) for i in self._grad]\n            autograd.mark_variables(self._data, self._grad, self.grad_req)", "response": "Cast data and gradient of this Parameter to a new data type."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving a Parameter from the shared dictionary.", "response": "def get(self, name, **kwargs):\n        \"\"\"Retrieves a :py:class:`Parameter` with name ``self.prefix+name``. If not found,\n        :py:func:`get` will first try to retrieve it from \"shared\" dict. If still not\n        found, :py:func:`get` will create a new :py:class:`Parameter` with key-word arguments and\n        insert it to self.\n\n        Parameters\n        ----------\n        name : str\n            Name of the desired Parameter. It will be prepended with this dictionary's\n            prefix.\n        **kwargs : dict\n            The rest of key-word arguments for the created :py:class:`Parameter`.\n\n        Returns\n        -------\n        Parameter\n            The created or retrieved :py:class:`Parameter`.\n        \"\"\"\n        name = self.prefix + name\n        param = self._get_impl(name)\n        if param is None: # pylint: disable=too-many-nested-blocks\n            param = Parameter(name, **kwargs)\n            self._params[name] = param\n        else:\n            for k, v in kwargs.items():\n                if hasattr(param, k) and getattr(param, k) is not None:\n                    existing = getattr(param, k)\n                    if k == 'shape' and len(v) == len(existing):\n                        inferred_shape = []\n                        matched = True\n                        for dim1, dim2 in zip(v, existing):\n                            if dim1 != dim2 and dim1 * dim2 != 0:\n                                matched = False\n                                break\n                            elif dim1 == dim2:\n                                inferred_shape.append(dim1)\n                            elif dim1 == 0:\n                                inferred_shape.append(dim2)\n                            else:\n                                inferred_shape.append(dim1)\n\n                        if matched:\n                            param._shape = tuple(inferred_shape)\n                            continue\n                    elif k == 'dtype' and np.dtype(v) == np.dtype(existing):\n                        continue\n\n                    assert v is None or v == existing, \\\n                        \"Cannot retrieve Parameter '%s' because desired attribute \" \\\n                        \"does not match with stored for attribute '%s': \" \\\n                        \"desired '%s' vs stored '%s'.\"%(\n                            name, k, str(v), str(getattr(param, k)))\n                else:\n                    setattr(param, k, v)\n        return param"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_constant(self, name, value=None):\n        name = self.prefix + name\n        param = self._get_impl(name)\n        if param is None:\n            if value is None:\n                raise KeyError(\"No constant named '{}'. Please specify value \" \\\n                               \"if you want to create a new constant.\".format(\n                                   name))\n            param = Constant(name, value)\n            self._params[name] = param\n        elif value is not None:\n            assert isinstance(param, Constant), \\\n                \"Parameter '{}' already exists but it is not a constant.\".format(\n                    name)\n            if isinstance(value, ndarray.NDArray):\n                value = value.asnumpy()\n            assert param.shape == value.shape and \\\n                (param.value.asnumpy() == value).all(), \\\n                \"Constant '{}' already exists but it's value doesn't match new \" \\\n                \"value\".format(name)\n        return param", "response": "Retrieves a Constant object from the shared dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncopy all Parameters in other to self.", "response": "def update(self, other):\n        \"\"\"Copies all Parameters in ``other`` to self.\"\"\"\n        for k, v in other.items():\n            if k in self._params:\n                assert self._params[k] is v, \\\n                    \"Cannot update self with other because they have different \" \\\n                    \"Parameters with the same name '%s'\"%k\n\n        for k, v in other.items():\n            self._params[k] = v"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninitialize all Parameters managed by this dictionary to be used for Symbol API.", "response": "def initialize(self, init=initializer.Uniform(), ctx=None, verbose=False,\n                   force_reinit=False):\n        \"\"\"Initializes all Parameters managed by this dictionary to be used for :py:class:`NDArray`\n        API. It has no effect when using :py:class:`Symbol` API.\n\n        Parameters\n        ----------\n        init : Initializer\n            Global default Initializer to be used when :py:meth:`Parameter.init` is ``None``.\n            Otherwise, :py:meth:`Parameter.init` takes precedence.\n        ctx : Context or list of Context\n            Keeps a copy of Parameters on one or many context(s).\n        verbose : bool, default False\n            Whether to verbosely print out details on initialization.\n        force_reinit : bool, default False\n            Whether to force re-initialization if parameter is already initialized.\n        \"\"\"\n        if verbose:\n            init.set_verbosity(verbose=verbose)\n        for _, v in self.items():\n            v.initialize(None, ctx, init, force_reinit=force_reinit)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save(self, filename, strip_prefix=''):\n        arg_dict = {}\n        for param in self.values():\n            weight = param._reduce()\n            if not param.name.startswith(strip_prefix):\n                raise ValueError(\n                    \"Prefix '%s' is to be striped before saving, but Parameter's \"\n                    \"name '%s' does not start with '%s'. \"\n                    \"this may be due to your Block shares parameters from other \"\n                    \"Blocks or you forgot to use 'with name_scope()' when creating \"\n                    \"child blocks. For more info on naming, please see \"\n                    \"http://mxnet.incubator.apache.org/tutorials/basic/naming.html\"%(\n                        strip_prefix, param.name, strip_prefix))\n            arg_dict[param.name[len(strip_prefix):]] = weight\n        ndarray.save(filename, arg_dict)", "response": "Save parameters to file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload parameters from file.", "response": "def load(self, filename, ctx=None, allow_missing=False,\n             ignore_extra=False, restore_prefix=''):\n        \"\"\"Load parameters from file.\n\n        Parameters\n        ----------\n        filename : str\n            Path to parameter file.\n        ctx : Context or list of Context\n            Context(s) initialize loaded parameters on.\n        allow_missing : bool, default False\n            Whether to silently skip loading parameters not represents in the file.\n        ignore_extra : bool, default False\n            Whether to silently ignore parameters from the file that are not\n            present in this ParameterDict.\n        restore_prefix : str, default ''\n            prepend prefix to names of stored parameters before loading.\n        \"\"\"\n        if restore_prefix:\n            for name in self.keys():\n                assert name.startswith(restore_prefix), \\\n                    \"restore_prefix is '%s' but Parameters name '%s' does not start \" \\\n                    \"with '%s'\"%(restore_prefix, name, restore_prefix)\n        lprefix = len(restore_prefix)\n        loaded = [(k[4:] if k.startswith('arg:') or k.startswith('aux:') else k, v) \\\n                  for k, v in ndarray.load(filename).items()]\n        arg_dict = {restore_prefix+k: v for k, v in loaded}\n        if not allow_missing:\n            for name in self.keys():\n                assert name in arg_dict, \\\n                    \"Parameter '%s' is missing in file '%s', which contains parameters: %s. \" \\\n                    \"Please make sure source and target networks have the same prefix.\"%(\n                        name[lprefix:], filename, _brief_print_list(arg_dict.keys()))\n        for name in arg_dict:\n            if name not in self._params:\n                assert ignore_extra, \\\n                    \"Parameter '%s' loaded from file '%s' is not present in ParameterDict, \" \\\n                    \"choices are: %s. Set ignore_extra to True to ignore. \" \\\n                    \"Please make sure source and target networks have the same prefix.\"%(\n                        name[lprefix:], filename, _brief_print_list(self._params.keys()))\n                continue\n            self[name]._load_init(arg_dict[name], ctx)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _make_torch_function(handle):\n    # Get the property of function\n    n_used_vars = mx_uint()\n    n_scalars = mx_uint()\n    n_mutate_vars = mx_uint()\n    type_mask = ctypes.c_int()\n    check_call(_LIB.MXFuncDescribe(\n        handle,\n        ctypes.byref(n_used_vars),\n        ctypes.byref(n_scalars),\n        ctypes.byref(n_mutate_vars),\n        ctypes.byref(type_mask)))\n    n_mutate_vars = n_mutate_vars.value\n    n_used_vars = n_used_vars.value\n    n_scalars = n_scalars.value\n    type_mask = type_mask.value\n\n    # Get the information from the function\n    name = ctypes.c_char_p()\n    desc = ctypes.c_char_p()\n    num_args = mx_uint()\n    arg_names = ctypes.POINTER(ctypes.c_char_p)()\n    arg_types = ctypes.POINTER(ctypes.c_char_p)()\n    arg_descs = ctypes.POINTER(ctypes.c_char_p)()\n    ret_type = ctypes.c_char_p()\n\n    check_call(_LIB.MXFuncGetInfo(\n        handle, ctypes.byref(name), ctypes.byref(desc),\n        ctypes.byref(num_args),\n        ctypes.byref(arg_names),\n        ctypes.byref(arg_types),\n        ctypes.byref(arg_descs),\n        ctypes.byref(ret_type)))\n    func_name = py_str(name.value)\n    if not func_name.startswith('_th_'):\n        return None\n    narg = int(num_args.value)\n    param_str = _build_param_doc(\n        [py_str(arg_names[i]) for i in range(narg)],\n        [py_str(arg_types[i]) for i in range(narg)],\n        [py_str(arg_descs[i]) for i in range(narg)])\n\n    if n_mutate_vars > 1:\n        res = ','.join(['res%d '%i for i in range(n_mutate_vars)])\n    else:\n        res = 'res '\n    doc_str = (('Interface for Torch function {name}.\\n' +\n                'Invoke with\\n{res}= mxnet.th.{name}(Parameters)\\nor\\n'+\n                'mxnet.th.{name}({res}, Parameters).\\n\\n' +\n                '{param_str}\\n' +\n                'References: ' +\n                'https://github.com/torch/torch7/blob/master/doc/maths.md\\n').format(\n                    name=func_name[4:], param_str=param_str,\n                    res=res))\n\n    def generic_torch_function(*args, **kwargs):\n        \"\"\"Invoke this function by passing in parameters.\n\n        Parameters\n        ----------\n        *args\n            Positional arguments of inputs (both scalar and `NDArray`).\n\n        Returns\n        -------\n        out : NDArray\n            The result NDArray(tuple) of result of computation.\n        \"\"\"\n        ndargs = []\n        arg_format = ''\n        value = ''\n        for arg in args:\n            if isinstance(arg, NDArray):\n                ndargs.append(arg)\n                arg_format += 'n'\n                value += ','\n            elif isinstance(arg, int):\n                arg_format += 'i'\n                value += str(arg) + ','\n            elif isinstance(arg, str):\n                arg_format += 's'\n                value += str(arg) + ','\n            elif isinstance(arg, float):\n                arg_format += 'f'\n                value += str(arg) + ','\n            elif isinstance(arg, bool):\n                arg_format += 'b'\n                value += str(arg) + ','\n        value = value[:-1]\n        if len(ndargs) == n_used_vars:\n            ndargs = [NDArray(_new_empty_handle()) for _ in range(n_mutate_vars)] + ndargs\n            arg_format = 'n'*n_mutate_vars + arg_format\n            value = ','*n_mutate_vars + value\n        elif len(ndargs) == n_mutate_vars + n_used_vars:\n            pass\n        else:\n            raise AssertionError(('Incorrect number of input NDArrays. ' +\n                                  'Need to be either %d (inputs) or %d ' +\n                                  '(output buffer) + %d (input)') %\n                                 (n_used_vars, n_mutate_vars, n_used_vars))\n\n        kwargs['format'] = arg_format\n        kwargs['args'] = value\n\n        for k in kwargs:\n            kwargs[k] = str(kwargs[k])\n\n        check_call(_LIB.MXFuncInvokeEx(\n            handle,\n            c_handle_array(ndargs[n_mutate_vars:]), # pylint: disable=invalid-slice-index\n            c_array(mx_float, []),\n            c_handle_array(ndargs[:n_mutate_vars]),   # pylint: disable=invalid-slice-index\n            ctypes.c_int(len(kwargs)),\n            c_str_array(kwargs.keys()),\n            c_str_array(kwargs.values())))\n\n        if n_mutate_vars == 1:\n            return ndargs[0]\n        else:\n            return ndargs[:n_mutate_vars] # pylint: disable=invalid-slice-index\n\n    # End of function declaration\n    ret_function = generic_torch_function\n    ret_function.__name__ = func_name[4:]\n    ret_function.__doc__ = doc_str\n    return ret_function", "response": "Create a Torch function from the FunctionHandle."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _init_torch_module():\n    plist = ctypes.POINTER(FunctionHandle)()\n    size = ctypes.c_uint()\n    check_call(_LIB.MXListFunctions(ctypes.byref(size),\n                                    ctypes.byref(plist)))\n\n    module_obj = sys.modules[__name__]\n    for i in range(size.value):\n        hdl = FunctionHandle(plist[i])\n        function = _make_torch_function(hdl)\n        # if function name starts with underscore, register as static method of NDArray\n        if function is not None:\n            setattr(module_obj, function.__name__, function)", "response": "List and add all the torch backed ndarray functions to current module."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\npack a string into MXImageRecord.", "response": "def pack(header, s):\n    \"\"\"Pack a string into MXImageRecord.\n\n    Parameters\n    ----------\n    header : IRHeader\n        Header of the image record.\n        ``header.label`` can be a number or an array. See more detail in ``IRHeader``.\n    s : str\n        Raw image string to be packed.\n\n    Returns\n    -------\n    s : str\n        The packed string.\n\n    Examples\n    --------\n    >>> label = 4 # label can also be a 1-D array, for example: label = [1,2,3]\n    >>> id = 2574\n    >>> header = mx.recordio.IRHeader(0, label, id, 0)\n    >>> with open(path, 'r') as file:\n    ...     s = file.read()\n    >>> packed_s = mx.recordio.pack(header, s)\n    \"\"\"\n    header = IRHeader(*header)\n    if isinstance(header.label, numbers.Number):\n        header = header._replace(flag=0)\n    else:\n        label = np.asarray(header.label, dtype=np.float32)\n        header = header._replace(flag=label.size, label=0)\n        s = label.tostring() + s\n    s = struct.pack(_IR_FORMAT, *header) + s\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nunpack a MXImageRecord to string.", "response": "def unpack(s):\n    \"\"\"Unpack a MXImageRecord to string.\n\n    Parameters\n    ----------\n    s : str\n        String buffer from ``MXRecordIO.read``.\n\n    Returns\n    -------\n    header : IRHeader\n        Header of the image record.\n    s : str\n        Unpacked string.\n\n    Examples\n    --------\n    >>> record = mx.recordio.MXRecordIO('test.rec', 'r')\n    >>> item = record.read()\n    >>> header, s = mx.recordio.unpack(item)\n    >>> header\n    HEADER(flag=0, label=14.0, id=20129312, id2=0)\n    \"\"\"\n    header = IRHeader(*struct.unpack(_IR_FORMAT, s[:_IR_SIZE]))\n    s = s[_IR_SIZE:]\n    if header.flag > 0:\n        header = header._replace(label=np.frombuffer(s, np.float32, header.flag))\n        s = s[header.flag*4:]\n    return header, s"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef unpack_img(s, iscolor=-1):\n    header, s = unpack(s)\n    img = np.frombuffer(s, dtype=np.uint8)\n    assert cv2 is not None\n    img = cv2.imdecode(img, iscolor)\n    return header, img", "response": "Unpack a MXImageRecord to image."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pack_img(header, img, quality=95, img_fmt='.jpg'):\n    assert cv2 is not None\n    jpg_formats = ['.JPG', '.JPEG']\n    png_formats = ['.PNG']\n    encode_params = None\n    if img_fmt.upper() in jpg_formats:\n        encode_params = [cv2.IMWRITE_JPEG_QUALITY, quality]\n    elif img_fmt.upper() in png_formats:\n        encode_params = [cv2.IMWRITE_PNG_COMPRESSION, quality]\n\n    ret, buf = cv2.imencode(img_fmt, img, encode_params)\n    assert ret, 'failed to encode image'\n    return pack(header, buf.tostring())", "response": "Pack an image into a new record."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nopening the record file.", "response": "def open(self):\n        \"\"\"Opens the record file.\"\"\"\n        if self.flag == \"w\":\n            check_call(_LIB.MXRecordIOWriterCreate(self.uri, ctypes.byref(self.handle)))\n            self.writable = True\n        elif self.flag == \"r\":\n            check_call(_LIB.MXRecordIOReaderCreate(self.uri, ctypes.byref(self.handle)))\n            self.writable = False\n        else:\n            raise ValueError(\"Invalid flag %s\"%self.flag)\n        self.pid = current_process().pid\n        self.is_open = True"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck process id to ensure integrity reset if in new process.", "response": "def _check_pid(self, allow_reset=False):\n        \"\"\"Check process id to ensure integrity, reset if in new process.\"\"\"\n        if not self.pid == current_process().pid:\n            if allow_reset:\n                self.reset()\n            else:\n                raise RuntimeError(\"Forbidden operation in multiple processes\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef close(self):\n        if not self.is_open:\n            return\n        if self.writable:\n            check_call(_LIB.MXRecordIOWriterFree(self.handle))\n        else:\n            check_call(_LIB.MXRecordIOReaderFree(self.handle))\n        self.is_open = False\n        self.pid = None", "response": "Closes the record file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write(self, buf):\n        assert self.writable\n        self._check_pid(allow_reset=False)\n        check_call(_LIB.MXRecordIOWriterWriteRecord(self.handle,\n                                                    ctypes.c_char_p(buf),\n                                                    ctypes.c_size_t(len(buf))))", "response": "Inserts a string buffer as a record."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread the record from the recordio file and returns it as a string.", "response": "def read(self):\n        \"\"\"Returns record as a string.\n\n        Examples\n        ---------\n        >>> record = mx.recordio.MXRecordIO('tmp.rec', 'r')\n        >>> for i in range(5):\n        ...    item = record.read()\n        ...    print(item)\n        record_0\n        record_1\n        record_2\n        record_3\n        record_4\n        >>> record.close()\n\n        Returns\n        ----------\n        buf : string\n            Buffer read.\n        \"\"\"\n        assert not self.writable\n        # trying to implicitly read from multiple processes is forbidden,\n        # there's no elegant way to handle unless lock is introduced\n        self._check_pid(allow_reset=False)\n        buf = ctypes.c_char_p()\n        size = ctypes.c_size_t()\n        check_call(_LIB.MXRecordIOReaderReadRecord(self.handle,\n                                                   ctypes.byref(buf),\n                                                   ctypes.byref(size)))\n        if buf:\n            buf = ctypes.cast(buf, ctypes.POINTER(ctypes.c_char*size.value))\n            return buf.contents.raw\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nclose the record file.", "response": "def close(self):\n        \"\"\"Closes the record file.\"\"\"\n        if not self.is_open:\n            return\n        super(MXIndexedRecordIO, self).close()\n        self.fidx.close()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef seek(self, idx):\n        assert not self.writable\n        self._check_pid(allow_reset=True)\n        pos = ctypes.c_size_t(self.idx[idx])\n        check_call(_LIB.MXRecordIOReaderSeek(self.handle, pos))", "response": "Sets the current read pointer position."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef tell(self):\n        assert self.writable\n        pos = ctypes.c_size_t()\n        check_call(_LIB.MXRecordIOWriterTell(self.handle, ctypes.byref(pos)))\n        return pos.value", "response": "Returns the current position of the record."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninserting input record at given index.", "response": "def write_idx(self, idx, buf):\n        \"\"\"Inserts input record at given index.\n\n        Examples\n        ---------\n        >>> for i in range(5):\n        ...     record.write_idx(i, 'record_%d'%i)\n        >>> record.close()\n\n        Parameters\n        ----------\n        idx : int\n            Index of a file.\n        buf :\n            Record to write.\n        \"\"\"\n        key = self.key_type(idx)\n        pos = self.tell()\n        self.write(buf)\n        self.fidx.write('%s\\t%d\\n'%(str(key), pos))\n        self.idx[key] = pos\n        self.keys.append(key)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _add_new_columns(dataframe, metrics):\n    #TODO(leodirac): we don't really need to do this on every update.  Optimize\n    new_columns = set(metrics.keys()) - set(dataframe.columns)\n    for col in new_columns:\n        dataframe[col] = None", "response": "Add new columns to selected pandas dataframe."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates callback arguments for a set of callback objects.", "response": "def args_wrapper(*args):\n    \"\"\"Generates callback arguments for model.fit()\n    for a set of callback objects.\n    Callback objects like PandasLogger(), LiveLearningCurve()\n    get passed in.  This assembles all their callback arguments.\n    \"\"\"\n    out = defaultdict(list)\n    for callback in args:\n        callback_args = callback.callback_args()\n        for k, v in callback_args.items():\n            out[k].append(v)\n    return dict(out)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nappends new metrics to the selected dataframes.", "response": "def append_metrics(self, metrics, df_name):\n        \"\"\"Append new metrics to selected dataframes.\n\n        Parameters\n        ----------\n        metrics : metric.EvalMetric\n            New metrics to be added.\n        df_name : str\n            Name of the dataframe to be modified.\n        \"\"\"\n        dataframe = self._dataframes[df_name]\n        _add_new_columns(dataframe, metrics)\n        dataframe.loc[len(dataframe)] = metrics"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the parameters for the selected dataframe after a completed batch.", "response": "def _process_batch(self, param, dataframe):\n        \"\"\"Update parameters for selected dataframe after a completed batch\n        Parameters\n        ----------\n        dataframe : pandas.DataFrame\n            Selected dataframe needs to be modified.\n        \"\"\"\n        now = time.time()\n        if param.eval_metric is not None:\n            metrics = dict(param.eval_metric.get_name_value())\n            param.eval_metric.reset()\n        else:\n            metrics = {}\n        # #11504\n        try:\n            speed = self.frequent / (now - self.last_time)\n        except ZeroDivisionError:\n            speed = float('inf')\n        metrics['batches_per_sec'] = speed * self.batch_size\n        metrics['records_per_sec'] = speed\n        metrics['elapsed'] = self.elapsed()\n        metrics['minibatch_count'] = param.nbatch\n        metrics['epoch'] = param.epoch\n        self.append_metrics(metrics, dataframe)\n        self.last_time = now"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef epoch_cb(self):\n        metrics = {}\n        metrics['elapsed'] = self.elapsed()\n        now = datetime.datetime.now()\n        metrics['epoch_time'] = now - self.last_epoch_time\n        self.append_metrics(metrics, 'epoch')\n        self.last_epoch_time = now", "response": "Callback function after each epoch. Now it records each epoch time\n        and append it to epoch dataframe."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrendering the plot with bokeh. io and push to notebook.", "response": "def _push_render(self):\n        \"\"\"Render the plot with bokeh.io and push to notebook.\n        \"\"\"\n        bokeh.io.push_notebook(handle=self.handle)\n        self.last_update = time.time()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating the selected dataframe after a completed batch.", "response": "def _process_batch(self, param, df_name):\n        \"\"\"Update selected dataframe after a completed batch\n        Parameters\n        ----------\n        df_name : str\n            Selected dataframe name needs to be modified.\n        \"\"\"\n        if param.eval_metric is not None:\n            metrics = dict(param.eval_metric.get_name_value())\n            param.eval_metric.reset()\n        else:\n            metrics = {}\n        metrics['elapsed'] = datetime.datetime.now() - self.start_time\n        for key, value in metrics.items():\n            if key not in self._data[df_name]:\n                self._data[df_name][key] = []\n            self._data[df_name][key].append(value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef build_vocab(nested_list):\n    # Build vocabulary\n    word_counts = Counter(itertools.chain(*nested_list))\n\n    # Mapping from index to label\n    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n\n    # Mapping from label to index\n    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n    return vocabulary, vocabulary_inv", "response": "Builds the vocabulary for the given list of nested strings."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef build_iters(data_dir, max_records, train_fraction, batch_size, buckets=None):\n    # Read in data as numpy array\n    df = pd.read_pickle(os.path.join(data_dir, \"ner_data.pkl\"))[:max_records]\n\n    # Get feature lists\n    entities=[list(array) for array in df[\"BILOU_tag\"].values]\n    sentences = [list(array) for array in df[\"token\"].values]\n    chars=[[[c for c in word] for word in sentence] for sentence in sentences]\n\n    # Build vocabularies\n    entity_to_index, index_to_entity = build_vocab(entities)\n    word_to_index, index_to_word = build_vocab(sentences)\n    char_to_index, index_to_char = build_vocab([np.array([c for c in word]) for word in index_to_word])\n    save_obj(entity_to_index, os.path.join(args.data_dir, \"tag_to_index\"))\n\n    # Map strings to integer values\n    indexed_entities=[list(map(entity_to_index.get, l)) for l in entities]\n    indexed_tokens=[list(map(word_to_index.get, l)) for l in sentences]\n    indexed_chars=[[list(map(char_to_index.get, word)) for word in sentence] for sentence in chars]\n\n    # Split into training and testing data\n    idx=int(len(indexed_tokens)*train_fraction)\n    X_token_train, X_char_train, Y_train = indexed_tokens[:idx], indexed_chars[:idx], indexed_entities[:idx]\n    X_token_test, X_char_test, Y_test = indexed_tokens[idx:], indexed_chars[idx:], indexed_entities[idx:]\n\n    # build iterators to feed batches to network\n    train_iter = iterators.BucketNerIter(sentences=X_token_train, characters=X_char_train, label=Y_train,\n                                         max_token_chars=5, batch_size=batch_size, buckets=buckets)\n    val_iter = iterators.BucketNerIter(sentences=X_token_test, characters=X_char_test, label=Y_test,\n                                         max_token_chars=train_iter.max_token_chars, batch_size=batch_size, buckets=train_iter.buckets)\n    return train_iter, val_iter, word_to_index, char_to_index, entity_to_index", "response": "Reads a csv of sentences and tag sequences into a pandas dataframe and builds training and test sets\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds the NN symbol depending on the length of the input sequence", "response": "def sym_gen(seq_len):\n    \"\"\"\n    Build NN symbol depending on the length of the input sequence\n    \"\"\"\n    sentence_shape = train_iter.provide_data[0][1]\n    char_sentence_shape = train_iter.provide_data[1][1]\n    entities_shape = train_iter.provide_label[0][1]\n\n    X_sent = mx.symbol.Variable(train_iter.provide_data[0].name)\n    X_char_sent = mx.symbol.Variable(train_iter.provide_data[1].name)\n    Y = mx.sym.Variable(train_iter.provide_label[0].name)\n\n    ###############################\n    # Character embedding component\n    ###############################\n    char_embeddings = mx.sym.Embedding(data=X_char_sent, input_dim=len(char_to_index), output_dim=args.char_embed, name='char_embed')\n    char_embeddings = mx.sym.reshape(data=char_embeddings, shape=(0,1,seq_len,-1,args.char_embed), name='char_embed2')\n\n    char_cnn_outputs = []\n    for i, filter_size in enumerate(args.char_filter_list):\n        # Kernel that slides over entire words resulting in a 1d output\n        convi = mx.sym.Convolution(data=char_embeddings, kernel=(1, filter_size, args.char_embed), stride=(1, 1, 1),\n                                   num_filter=args.char_filters, name=\"char_conv_layer_\" + str(i))\n        acti = mx.sym.Activation(data=convi, act_type='tanh')\n        pooli = mx.sym.Pooling(data=acti, pool_type='max', kernel=(1, char_sentence_shape[2] - filter_size + 1, 1),\n                               stride=(1, 1, 1), name=\"char_pool_layer_\" + str(i))\n        pooli = mx.sym.transpose(mx.sym.Reshape(pooli, shape=(0, 0, 0)), axes=(0, 2, 1), name=\"cchar_conv_layer_\" + str(i))\n        char_cnn_outputs.append(pooli)\n\n    # combine features from all filters & apply dropout\n    cnn_char_features = mx.sym.Concat(*char_cnn_outputs, dim=2, name=\"cnn_char_features\")\n    regularized_cnn_char_features = mx.sym.Dropout(data=cnn_char_features, p=args.dropout, mode='training',\n                                                   name='regularized charCnn features')\n\n    ##################################\n    # Combine char and word embeddings\n    ##################################\n    word_embeddings = mx.sym.Embedding(data=X_sent, input_dim=len(word_to_index), output_dim=args.word_embed, name='word_embed')\n    rnn_features = mx.sym.Concat(*[word_embeddings, regularized_cnn_char_features], dim=2, name='rnn input')\n\n    ##############################\n    # Bidirectional LSTM component\n    ##############################\n\n    # unroll the lstm cell in time, merging outputs\n    bi_cell.reset()\n    output, states = bi_cell.unroll(length=seq_len, inputs=rnn_features, merge_outputs=True)\n\n    # Map to num entity classes\n    rnn_output = mx.sym.Reshape(output, shape=(-1, args.lstm_state_size * 2), name='r_output')\n    fc = mx.sym.FullyConnected(data=rnn_output, num_hidden=len(entity_to_index), name='fc_layer')\n\n    # reshape back to same shape as loss will be\n    reshaped_fc = mx.sym.transpose(mx.sym.reshape(fc, shape=(-1, seq_len, len(entity_to_index))), axes=(0, 2, 1))\n    sm = mx.sym.SoftmaxOutput(data=reshaped_fc, label=Y, ignore_label=-1, use_ignore=True, multi_output=True, name='softmax')\n    return sm, [v.name for v in train_iter.provide_data], [v.name for v in train_iter.provide_label]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndraw random samples from an approximately log - uniform or Zipfian distribution.", "response": "def rand_zipfian(true_classes, num_sampled, range_max):\n    \"\"\"Draw random samples from an approximately log-uniform or Zipfian distribution.\n\n    This operation randomly samples *num_sampled* candidates the range of integers [0, range_max).\n    The elements of sampled_candidates are drawn with replacement from the base distribution.\n\n    The base distribution for this operator is an approximately log-uniform or Zipfian distribution:\n\n    P(class) = (log(class + 2) - log(class + 1)) / log(range_max + 1)\n\n    This sampler is useful when the true classes approximately follow such a distribution.\n    For example, if the classes represent words in a lexicon sorted in decreasing order of \\\n    frequency. If your classes are not ordered by decreasing frequency, do not use this op.\n\n    Additionaly, it also returns the number of times each of the \\\n    true classes and the sampled classes is expected to occur.\n\n    Parameters\n    ----------\n    true_classes : Symbol\n        The target classes in 1-D.\n    num_sampled: int\n        The number of classes to randomly sample.\n    range_max: int\n        The number of possible classes.\n\n    Returns\n    -------\n    samples: Symbol\n        The sampled candidate classes in 1-D `int64` dtype.\n    expected_count_true: Symbol\n        The expected count for true classes in 1-D `float64` dtype.\n    expected_count_sample: Symbol\n        The expected count for sampled candidates in 1-D `float64` dtype.\n\n    Examples\n    --------\n    >>> true_cls = mx.sym.Variable('true_cls')\n    >>> samples, exp_count_true, exp_count_sample = mx.sym.contrib.rand_zipfian(true_cls, 4, 5)\n    >>> samples.eval(true_cls=mx.nd.array([3]))[0].asnumpy()\n    array([1, 3, 3, 3])\n    >>> exp_count_true.eval(true_cls=mx.nd.array([3]))[0].asnumpy()\n    array([0.12453879])\n    >>> exp_count_sample.eval(true_cls=mx.nd.array([3]))[0].asnumpy()\n    array([0.22629439, 0.12453879, 0.12453879, 0.12453879])\n    \"\"\"\n    assert(isinstance(true_classes, Symbol)), \"unexpected type %s\" % type(true_classes)\n    log_range = math.log(range_max + 1)\n    rand = uniform(0, log_range, shape=(num_sampled,), dtype='float64')\n    # make sure sampled_classes are in the range of [0, range_max)\n    sampled_classes = (rand.exp() - 1).astype('int64') % range_max\n\n    true_classes = true_classes.astype('float64')\n    expected_prob_true = ((true_classes + 2.0) / (true_classes + 1.0)).log() / log_range\n    expected_count_true = expected_prob_true * num_sampled\n    # cast sampled classes to fp64 to avoid interget division\n    sampled_cls_fp64 = sampled_classes.astype('float64')\n    expected_prob_sampled = ((sampled_cls_fp64 + 2.0) / (sampled_cls_fp64 + 1.0)).log() / log_range\n    expected_count_sampled = expected_prob_sampled * num_sampled\n    return sampled_classes, expected_count_true, expected_count_sampled"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef while_loop(cond, func, loop_vars, max_iterations=None, name=\"while_loop\"):\n    def _to_python_scalar(inputs, type_, name):\n        \"\"\"Converts \"inputs\", possibly typed mxnet NDArray, a numpy ndarray, other python types,\n        to the given type\n        \"\"\"\n        if hasattr(inputs, \"asscalar\"):\n            inputs = inputs.asscalar()\n        try:\n            inputs = type_(inputs)\n        except:\n            raise ValueError(\"Cannot convert %s to python %s\" % (name, type_.__name__))\n        return inputs\n\n    def _cond_wrapper(loop_vars):\n        result = cond(*loop_vars)\n        if not isinstance(result, Symbol):\n            raise ValueError(\"Return of cond must be a Symbol\")\n        return [], [result], [], []\n\n    def _func_wrapper(loop_vars):\n        \"\"\"This wrapper unifies\n             \"func: loop_vars -> new_loop_vars\"\n         and \"func: loop_vars -> (step_output, new_loop_vars)\"\n        into \"func: loop_vars -> (list of step_outputs, tuple of new_loop_vars)\n        \"\"\"\n        step_output, new_loop_vars = func(*loop_vars)\n        if step_output is None:\n            step_output = []\n        if new_loop_vars is None:\n            new_loop_vars = []\n        if isinstance(step_output, tuple):\n            step_output = list(step_output)\n        if isinstance(new_loop_vars, tuple):\n            new_loop_vars = list(new_loop_vars)\n        step_output, out_fmt = _flatten(step_output, \"while output\")\n        new_loop_vars, var_fmt = _flatten(new_loop_vars, \"while loop_vars\")\n        if len(loop_vars) != len(new_loop_vars):\n            raise ValueError(\"The number of loop_vars should be consistent during the loop\")\n        return step_output, new_loop_vars, out_fmt, var_fmt\n\n    def _create_subgraph(graph_vars, graph_func, subgraph_name):\n        subgraph_name = _get_unique_subgraph_name(subgraph_name)\n        with AttrScope(__subgraph_name__=subgraph_name):\n            # create new variables with the same name,\n            # them feed them to the given func\n            graph_vars, var_fmt = _flatten(graph_vars, \"while loop_vars\")\n            new_graph_vars = [symbol.var(_get_sym_uniq_name(sym)) for sym in graph_vars]\n            new_graph_vars, _ = _regroup(new_graph_vars, var_fmt)\n            outputs, final_state, out_fmt, var_fmt = graph_func(new_graph_vars)\n            # first `num_out_data` elements belong to `outputs`\n            # other elements belong to `final_state`\n            num_out_data = len(outputs)\n            num_outputs = len(outputs) + len(final_state)\n            # nnvm cut-graph does not allow inputs and outputs overlap\n            # so we calculate the name of inputs, and copy outputs once it overlaps with inputs\n            # group all outputs of graph_func\n            all_input_names = symbol.Group(outputs + final_state).list_inputs()\n            in_input = lambda x: x.name in all_input_names\n            in_graph = lambda x: x.list_attr().get(\"__subgraph_name__\", \"\") == subgraph_name\n            make_identity = lambda x: symbol.op.identity(x) if in_input(x) or not in_graph(x) \\\n                                      else x\n            graph = symbol.Group(list(map(make_identity, outputs + final_state)))\n        return graph, num_out_data, num_outputs, out_fmt, var_fmt\n\n    flatten_loop_vars, init_loop_var_fmt = _flatten(loop_vars, \"while loop_vars\")\n    _check_data(flatten_loop_vars, symbol.Symbol,\n                \"loop_vars should be a symbol or a nested list of symbols\")\n\n    def _union_inputs(*graphs):\n        # Given a list of graphs, each whose inputs are either from loop_vars or other variables.\n        # 1) calculate a list `inputs`, the union of their inputs.\n        # 2) for each graph, determine in which indices their inputs reside in `inputs`\n        # 3) for each variable in the input of `graph`, find which index it is\n        inputs = []             # List[Symbol], result of 1)\n        locs = []               # List[Tuple(List[Int], List[Int])], a list of tuples,\n                                # where tuples are results of 2) and 3)\n        input_id_to_loc = {}    # Dict[int, int], given id(sym), input_id_to_loc maps it\n                                # to a `loc`, where inputs[loc] = sym\n        for graph in graphs:\n            # some loop_vars are inputs to `graph`, some are not\n            name_to_loop_vars = {_get_sym_uniq_name(sym): sym for sym in flatten_loop_vars}\n            # other inputs to `graph` created by cut_graph\n            name_to_cut_g_syms = {sym.list_outputs()[0]: sym for sym in _cut_subgraph(graph)}\n            # input_syms: all inputs to the `graph`\n            name_to_input_syms = {sym.name: sym for sym in _get_graph_inputs(graph)}\n            # also we collect the mapping from var's name to var's loc in loop_vars\n            name_to_var_locs = {_get_sym_uniq_name(sym): i for i, sym in enumerate(flatten_loop_vars)}\n            # collect arguments for each subgraph\n            input_locs = []                         # results from the second step\n            var_locs = [-1] * len(flatten_loop_vars)        # results from the third step\n            subg_input_names = graph.list_inputs()\n            assert len(set(subg_input_names)) == len(subg_input_names), \\\n                    \"The inputs of the subgraph don't have unique names: \" + str(subg_input_names)\n            for name in subg_input_names:\n                assert name in name_to_input_syms   # it should obviously hold\n                # name -> sym\n                if name in name_to_loop_vars:\n                    sym = name_to_loop_vars[name]\n                elif name in name_to_cut_g_syms:\n                    sym = name_to_cut_g_syms[name]\n                else:\n                    sym = copy.deepcopy(name_to_input_syms[name])\n                # do 2), and 1) is implicitly done\n                if id(sym) in input_id_to_loc:\n                    loc = input_id_to_loc[id(sym)]\n                else:\n                    loc = len(input_id_to_loc)\n                    inputs.append(sym)\n                    input_id_to_loc[id(sym)] = loc\n                input_locs.append(loc)\n                # do 3)\n                if name in name_to_var_locs:\n                    var_locs[name_to_var_locs[name]] = len(input_locs) - 1\n            locs.append((input_locs, var_locs))\n        return inputs, locs\n    if max_iterations is None:\n        raise ValueError(\"max_iterations should be specified\")\n    max_iterations = _to_python_scalar(max_iterations, int, \"max_iteration\")\n    # It should be work as fine if loop_vars are empty I guess,\n    # but it is semantically unnecessary to include this case.\n    if len(loop_vars) == 0:\n        raise ValueError(\"loop_vars should contain at least one element\")\n    # create graph for `cond'\n    cond_g, num_out_data, num_outputs, _, _ = \\\n        _create_subgraph(loop_vars, _cond_wrapper, name + \"_cond\")\n    assert num_out_data == 0\n    assert num_outputs == 1\n    # create graph for `func`\n    func_g, num_out_data, num_outputs, out_fmt, _ = \\\n        _create_subgraph(loop_vars, _func_wrapper, name + \"_func\")\n    # find symbols used in either cond_g or func_g\n    input_syms, ((cond_input_locs, _), (func_input_locs, func_var_locs)) = \\\n        _union_inputs(cond_g, func_g)\n    for i_th, loc in enumerate(func_var_locs, 1):\n        if loc == -1:\n            raise ValueError(\"The %d-th loop_var doesn't involve into the computation\" % i_th)\n    result = symbol._internal._while_loop(\n        cond_g,\n        func_g,\n        *input_syms,\n        max_iterations=max_iterations,\n        cond_input_locs=cond_input_locs,\n        func_input_locs=func_input_locs,\n        func_var_locs=func_var_locs,\n        num_out_data=num_out_data,\n        num_outputs=num_outputs\n    )\n    outputs = [result[i] for i in range(num_out_data)]\n    outputs, _ = _regroup(outputs, out_fmt)\n    final_loop_vars = [result[i] for i in range(num_out_data, num_outputs)]\n    final_loop_vars, _ = _regroup(final_loop_vars, init_loop_var_fmt)\n    return outputs, final_loop_vars", "response": "This operator simulates a while loop with user - defined computation and loop condition."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cond(pred, then_func, else_func, name=\"cond\"):\n\n    def _create_subgraph(graph_vars, graph_func, subgraph_name):\n        subgraph_name = _get_unique_subgraph_name(subgraph_name)\n        with AttrScope(__subgraph_name__=subgraph_name):\n            # create new variables with the same name,\n            # them feed them to the given func\n            new_graph_vars = [symbol.var(sym.name) for sym in graph_vars]\n            outputs = graph_func(*new_graph_vars)\n            outputs, out_fmt = _flatten(outputs, \"cond outputs\")\n            num_outputs = len(outputs)\n            # nnvm cut-graph does not allow inputs and outputs overlap\n            # so we calculate the name of inputs, and copy outputs once it overlaps with inputs\n            # group all outputs of graph_func\n            all_input_names = symbol.Group(outputs).list_inputs()\n            in_input = lambda x: x.name in all_input_names\n            in_graph = lambda x: x.list_attr().get(\"__subgraph_name__\", \"\") == subgraph_name\n            make_identity = lambda x: symbol.op.identity(x) if in_input(x) or not in_graph(x) \\\n                                      else x\n            graph = symbol.Group(list(map(make_identity, outputs)))\n        return graph, num_outputs, out_fmt\n\n    def _union_inputs(*graphs):\n        # Given a list of graphs, each whose inputs are either from input_vars or other variables.\n        # 1) calculate a list `inputs`, the union of their inputs.\n        # 2) for each graph, determine in which indices their inputs reside in `inputs`\n        # 3) for each variable in the input of `graph`, find which index it is\n        inputs = []             # List[Symbol], result of 1)\n        locs = []               # List[Tuple(List[Int], List[Int])], a list of tuples,\n                                # where tuples are results of 2) and 3)\n        input_id_to_loc = {}    # Dict[int, int], given id(sym), input_id_to_loc maps it\n                                # to a `loc`, where inputs[loc] = sym\n        for graph in graphs:\n            # some input_vars are inputs to `graph`, some are not\n            name_to_input_vars = {sym.name: sym for sym in inputs}\n            # other inputs to `graph` created by cut_graph\n            name_to_cut_g_syms = {sym.list_outputs()[0]: sym for sym in _cut_subgraph(graph)}\n            # input_syms: all inputs to the `graph`\n            name_to_input_syms = {sym.name: sym for sym in _get_graph_inputs(graph)}\n            # collect arguments for each subgraph\n            input_locs = []                         # results from the second step\n            for name in graph.list_inputs():\n                assert name in name_to_input_syms   # it should obviously hold\n                # name -> sym\n                if name in name_to_input_vars:\n                    sym = name_to_input_vars[name]\n                elif name in name_to_cut_g_syms:\n                    sym = name_to_cut_g_syms[name]\n                else:\n                    sym = copy.deepcopy(name_to_input_syms[name])\n                # do 2), and 1) is implicitly done\n                if id(sym) in input_id_to_loc:\n                    loc = input_id_to_loc[id(sym)]\n                else:\n                    loc = len(input_id_to_loc)\n                    inputs.append(sym)\n                    input_id_to_loc[id(sym)] = loc\n                input_locs.append(loc)\n            locs.append(input_locs)\n        return inputs, locs\n    inputs = []\n    # create graph for `cond_func'\n    cond_g, cond_num_outputs, _ = _create_subgraph(inputs, lambda: pred, name + \"_pred\")\n    if cond_num_outputs != 1:\n        raise ValueError(\"pred should always be a single output\")\n    # create graph for `then`\n    then_g, then_num_outputs, then_fmt = _create_subgraph(inputs, then_func, name + \"_then\")\n    # create graph for `else`\n    else_g, else_num_outputs, _ = _create_subgraph(inputs, else_func, name + \"_else\")\n    if then_num_outputs != else_num_outputs:\n        raise ValueError(\"Number of outputs differs between then-branch and else-branch\")\n    # find symbols used in either cond_g or func_g\n    input_syms, (cond_input_locs, then_input_locs, else_input_locs) = \\\n        _union_inputs(cond_g, then_g, else_g)\n    result = symbol._internal._cond(\n        # [cond, then_g, else_g, *input_syms]\n        cond_g,\n        then_g,\n        else_g,\n        *input_syms,\n        cond_input_locs=cond_input_locs,\n        then_input_locs=then_input_locs,\n        else_input_locs=else_input_locs,\n        num_outputs=then_num_outputs\n    )\n    outputs = [result[i] for i in range(then_num_outputs)]\n    outputs, _ = _regroup(outputs, then_fmt)\n    return outputs", "response": "This operator simulates an if - like branch which chooses to do one of the two customized computations according to the specified condition and then_func and else_func."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _index_unknown_and_reserved_tokens(self, unknown_token, reserved_tokens):\n\n        self._unknown_token = unknown_token\n        # Thus, constants.UNKNOWN_IDX must be 0.\n        self._idx_to_token = [unknown_token]\n\n        if reserved_tokens is None:\n            self._reserved_tokens = None\n        else:\n            self._reserved_tokens = reserved_tokens[:]\n            self._idx_to_token.extend(reserved_tokens)\n\n        self._token_to_idx = {token: idx for idx, token in enumerate(self._idx_to_token)}", "response": "Indexes unknown and reserved tokens."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _index_counter_keys(self, counter, unknown_token, reserved_tokens, most_freq_count,\n                            min_freq):\n        \"\"\"Indexes keys of `counter`.\n\n\n        Indexes keys of `counter` according to frequency thresholds such as `most_freq_count` and\n        `min_freq`.\n        \"\"\"\n\n        assert isinstance(counter, collections.Counter), \\\n            '`counter` must be an instance of collections.Counter.'\n\n        unknown_and_reserved_tokens = set(reserved_tokens) if reserved_tokens is not None else set()\n        unknown_and_reserved_tokens.add(unknown_token)\n\n        token_freqs = sorted(counter.items(), key=lambda x: x[0])\n        token_freqs.sort(key=lambda x: x[1], reverse=True)\n\n        token_cap = len(unknown_and_reserved_tokens) + (\n            len(counter) if most_freq_count is None else most_freq_count)\n\n        for token, freq in token_freqs:\n            if freq < min_freq or len(self._idx_to_token) == token_cap:\n                break\n            if token not in unknown_and_reserved_tokens:\n                self._idx_to_token.append(token)\n                self._token_to_idx[token] = len(self._idx_to_token) - 1", "response": "Indexes keys of counter according to frequency thresholds such as most_freq_count and min_freq."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_indices(self, tokens):\n\n        to_reduce = False\n        if not isinstance(tokens, list):\n            tokens = [tokens]\n            to_reduce = True\n\n        indices = [self.token_to_idx[token] if token in self.token_to_idx\n                   else C.UNKNOWN_IDX for token in tokens]\n\n        return indices[0] if to_reduce else indices", "response": "Converts a list of source tokens to indices according to the vocabulary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting token indices to tokens according to the vocabulary.", "response": "def to_tokens(self, indices):\n        \"\"\"Converts token indices to tokens according to the vocabulary.\n\n\n        Parameters\n        ----------\n        indices : int or list of ints\n            A source token index or token indices to be converted.\n\n\n        Returns\n        -------\n        str or list of strs\n            A token or a list of tokens according to the vocabulary.\n        \"\"\"\n\n        to_reduce = False\n        if not isinstance(indices, list):\n            indices = [indices]\n            to_reduce = True\n\n        max_idx = len(self.idx_to_token) - 1\n\n        tokens = []\n        for idx in indices:\n            if not isinstance(idx, int) or idx > max_idx:\n                raise ValueError('Token index %d in the provided `indices` is invalid.' % idx)\n            else:\n                tokens.append(self.idx_to_token[idx])\n\n        return tokens[0] if to_reduce else tokens"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates an io iterator by handle.", "response": "def _make_io_iterator(handle):\n    \"\"\"Create an io iterator by handle.\"\"\"\n    name = ctypes.c_char_p()\n    desc = ctypes.c_char_p()\n    num_args = mx_uint()\n    arg_names = ctypes.POINTER(ctypes.c_char_p)()\n    arg_types = ctypes.POINTER(ctypes.c_char_p)()\n    arg_descs = ctypes.POINTER(ctypes.c_char_p)()\n\n    check_call(_LIB.MXDataIterGetIterInfo( \\\n            handle, ctypes.byref(name), ctypes.byref(desc), \\\n            ctypes.byref(num_args), \\\n            ctypes.byref(arg_names), \\\n            ctypes.byref(arg_types), \\\n            ctypes.byref(arg_descs)))\n    iter_name = py_str(name.value)\n\n    narg = int(num_args.value)\n    param_str = _build_param_doc(\n        [py_str(arg_names[i]) for i in range(narg)],\n        [py_str(arg_types[i]) for i in range(narg)],\n        [py_str(arg_descs[i]) for i in range(narg)])\n\n    doc_str = ('%s\\n\\n' +\n               '%s\\n' +\n               'Returns\\n' +\n               '-------\\n' +\n               'MXDataIter\\n'+\n               '    The result iterator.')\n    doc_str = doc_str % (desc.value, param_str)\n\n    def creator(*args, **kwargs):\n        \"\"\"Create an iterator.\n        The parameters listed below can be passed in as keyword arguments.\n\n        Parameters\n        ----------\n        name : string, required.\n            Name of the resulting data iterator.\n\n        Returns\n        -------\n        dataiter: Dataiter\n            The resulting data iterator.\n        \"\"\"\n        param_keys = []\n        param_vals = []\n\n        for k, val in kwargs.items():\n            param_keys.append(k)\n            param_vals.append(str(val))\n        # create atomic symbol\n        param_keys = c_str_array(param_keys)\n        param_vals = c_str_array(param_vals)\n        iter_handle = DataIterHandle()\n        check_call(_LIB.MXDataIterCreateIter(\n            handle,\n            mx_uint(len(param_keys)),\n            param_keys, param_vals,\n            ctypes.byref(iter_handle)))\n\n        if len(args):\n            raise TypeError('%s can only accept keyword arguments' % iter_name)\n\n        return MXDataIter(iter_handle, **kwargs)\n\n    creator.__name__ = iter_name\n    creator.__doc__ = doc_str\n    return creator"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _init_io_module():\n    plist = ctypes.POINTER(ctypes.c_void_p)()\n    size = ctypes.c_uint()\n    check_call(_LIB.MXListDataIters(ctypes.byref(size), ctypes.byref(plist)))\n    module_obj = sys.modules[__name__]\n    for i in range(size.value):\n        hdl = ctypes.c_void_p(plist[i])\n        dataiter = _make_io_iterator(hdl)\n        setattr(module_obj, dataiter.__name__, dataiter)", "response": "List and add all the data iterators to current module."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_list(shapes, types):\n        if types is not None:\n            type_dict = dict(types)\n            return [DataDesc(x[0], x[1], type_dict[x[0]]) for x in shapes]\n        else:\n            return [DataDesc(x[0], x[1]) for x in shapes]", "response": "Get DataDesc list from attribute lists."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef next(self):\n        if self.iter_next():\n            return DataBatch(data=self.getdata(), label=self.getlabel(), \\\n                    pad=self.getpad(), index=self.getindex())\n        else:\n            raise StopIteration", "response": "Get next data batch from iterator."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef hard_reset(self):\n        if self.shuffle:\n            self._shuffle_data()\n        self.cursor = -self.batch_size\n        self._cache_data = None\n        self._cache_label = None", "response": "Reset the cache to start."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reset(self):\n        if self.shuffle:\n            self._shuffle_data()\n        # the range below indicate the last batch\n        if self.last_batch_handle == 'roll_over' and \\\n            self.num_data - self.batch_size < self.cursor < self.num_data:\n            # (self.cursor - self.num_data) represents the data we have for the last batch\n            self.cursor = self.cursor - self.num_data - self.batch_size\n        else:\n            self.cursor = -self.batch_size", "response": "Resets the iterator to the beginning of the data."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef iter_next(self):\n        self.cursor += self.batch_size\n        return self.cursor < self.num_data", "response": "Increments the coursor by batch_size for next batch\n       . Returns True if the cursor is not exceed the number of data points."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef next(self):\n        if not self.iter_next():\n            raise StopIteration\n        data = self.getdata()\n        label = self.getlabel()\n        # iter should stop when last batch is not complete\n        if data[0].shape[0] != self.batch_size:\n        # in this case, cache it for next epoch\n            self._cache_data = data\n            self._cache_label = label\n            raise StopIteration\n        return DataBatch(data=data, label=label, \\\n            pad=self.getpad(), index=None)", "response": "Returns the next batch of data."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _getdata(self, data_source, start=None, end=None):\n        assert start is not None or end is not None, 'should at least specify start or end'\n        start = start if start is not None else 0\n        if end is None:\n            end = data_source[0][1].shape[0] if data_source else 0\n        s = slice(start, end)\n        return [\n            x[1][s]\n            if isinstance(x[1], (np.ndarray, NDArray)) else\n            # h5py (only supports indices in increasing order)\n            array(x[1][sorted(self.idx[s])][[\n                list(self.idx[s]).index(i)\n                for i in sorted(self.idx[s])\n            ]]) for x in data_source\n        ]", "response": "Load data from underlying arrays."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _concat(self, first_data, second_data):\n        assert len(first_data) == len(\n            second_data), 'data source should contain the same size'\n        if first_data and second_data:\n            return [\n                concat(\n                    first_data[x],\n                    second_data[x],\n                    dim=0\n                ) for x in range(len(first_data))\n            ]\n        elif (not first_data) and (not second_data):\n            return []\n        else:\n            return [\n                first_data[0] if first_data else second_data[0]\n                for x in range(len(first_data))\n            ]", "response": "Helper function to concatenate two NDArrays."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading data from underlying arrays internal use only.", "response": "def _batchify(self, data_source):\n        \"\"\"Load data from underlying arrays, internal use only.\"\"\"\n        assert self.cursor < self.num_data, 'DataIter needs reset.'\n        # first batch of next epoch with 'roll_over'\n        if self.last_batch_handle == 'roll_over' and \\\n            -self.batch_size < self.cursor < 0:\n            assert self._cache_data is not None or self._cache_label is not None, \\\n                'next epoch should have cached data'\n            cache_data = self._cache_data if self._cache_data is not None else self._cache_label\n            second_data = self._getdata(\n                data_source, end=self.cursor + self.batch_size)\n            if self._cache_data is not None:\n                self._cache_data = None\n            else:\n                self._cache_label = None\n            return self._concat(cache_data, second_data)\n        # last batch with 'pad'\n        elif self.last_batch_handle == 'pad' and \\\n            self.cursor + self.batch_size > self.num_data:\n            pad = self.batch_size - self.num_data + self.cursor\n            first_data = self._getdata(data_source, start=self.cursor)\n            second_data = self._getdata(data_source, end=pad)\n            return self._concat(first_data, second_data)\n        # normal case\n        else:\n            if self.cursor + self.batch_size < self.num_data:\n                end_idx = self.cursor + self.batch_size\n            # get incomplete last batch\n            else:\n                end_idx = self.num_data\n            return self._getdata(data_source, self.cursor, end_idx)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getpad(self):\n        if self.last_batch_handle == 'pad' and \\\n           self.cursor + self.batch_size > self.num_data:\n            return self.cursor + self.batch_size - self.num_data\n        # check the first batch\n        elif self.last_batch_handle == 'roll_over' and \\\n            -self.batch_size < self.cursor < 0:\n            return -self.cursor\n        else:\n            return 0", "response": "Get pad value of DataBatch."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngiving a quantized symbol and a dict of params that have not been quantized, generate quantized params. Currently only supports quantizing the arg_params with names of `weight` or `bias`, not aux_params. If `qsym` contains symbols that are excluded from being quantized, their corresponding params will not be quantized, but saved together with quantized params of the symbols that have been quantized. Parameters ---------- qsym : Symbol Quantized symbol from FP32 symbol. params : dict of str->NDArray th_dict: dict of min/max pairs of layers' output", "response": "def _quantize_params(qsym, params, th_dict):\n    \"\"\"Given a quantized symbol and a dict of params that have not been quantized,\n    generate quantized params. Currently only supports quantizing the arg_params\n    with names of `weight` or `bias`, not aux_params. If `qsym` contains symbols\n    that are excluded from being quantized, their corresponding params will\n    not be quantized, but saved together with quantized params of the symbols that\n    have been quantized.\n\n    Parameters\n    ----------\n    qsym : Symbol\n        Quantized symbol from FP32 symbol.\n    params : dict of str->NDArray\n    th_dict: dict of min/max pairs of layers' output\n    \"\"\"\n    inputs_name = qsym.list_arguments()\n    quantized_params = {}\n    for name in inputs_name:\n        if name.endswith(('weight_quantize', 'bias_quantize')):\n            original_name = name[:-len('_quantize')]\n            param = params[original_name]\n            val, vmin, vmax = ndarray.contrib.quantize(data=param,\n                                                       min_range=ndarray.min(param),\n                                                       max_range=ndarray.max(param),\n                                                       out_type='int8')\n            quantized_params[name] = val\n            quantized_params[name+'_min'] = vmin\n            quantized_params[name+'_max'] = vmax\n        elif name in params:\n            quantized_params[name] = params[name]\n        elif name.endswith(('_min')):\n            output = name[: - len('_min')]\n            if output in th_dict:\n                quantized_params[name] = ndarray.array([th_dict[output][0]])\n        elif name.endswith(('_max')):\n            output = name[: - len('_min')]\n            if output in th_dict:\n                quantized_params[name] = ndarray.array([th_dict[output][1]])\n    return quantized_params"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngiving a symbol object representing a neural network of data type FP32, quantize it into a INT8 network. Parameters ---------- sym : Symbol FP32 neural network symbol. excluded_sym_names : list of strings A list of strings representing the names of the symbols that users want to excluding from being quantized. offline_params : list of strs Names of the parameters that users want to quantize offline. It's always recommended to quantize parameters offline so that quantizing parameters during the inference can be avoided. quantized_dtype: str The quantized destination type for input data.", "response": "def _quantize_symbol(sym, excluded_symbols=None, offline_params=None, quantized_dtype='int8'):\n    \"\"\"Given a symbol object representing a neural network of data type FP32,\n    quantize it into a INT8 network.\n\n    Parameters\n    ----------\n    sym : Symbol\n        FP32 neural network symbol.\n    excluded_sym_names : list of strings\n        A list of strings representing the names of the symbols that users want to excluding\n        from being quantized.\n    offline_params : list of strs\n        Names of the parameters that users want to quantize offline. It's always recommended to\n        quantize parameters offline so that quantizing parameters during the inference can be\n        avoided.\n    quantized_dtype: str\n        The quantized destination type for input data.\n    \"\"\"\n    num_excluded_symbols = 0\n    if excluded_symbols is not None:\n        assert isinstance(excluded_symbols, list)\n        num_excluded_symbols = len(excluded_symbols)\n    else:\n        excluded_symbols = []\n\n    num_offline = 0\n    offline = []\n    if offline_params is not None:\n        num_offline = len(offline_params)\n        for k in offline_params:\n            offline.append(c_str(k))\n\n    out = SymbolHandle()\n    check_call(_LIB.MXQuantizeSymbol(sym.handle,\n                                     ctypes.byref(out),\n                                     mx_uint(num_excluded_symbols),\n                                     c_str_array(excluded_symbols),\n                                     mx_uint(num_offline),\n                                     c_array(ctypes.c_char_p, offline),\n                                     c_str(quantized_dtype),\n                                     ctypes.c_bool(True)))\n    return Symbol(out)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalibrates the quantized symbol.", "response": "def _calibrate_quantized_sym(qsym, th_dict):\n    \"\"\"Given a dictionary containing the thresholds for quantizing the layers,\n    set the thresholds into the quantized symbol as the params of requantize operators.\n    \"\"\"\n    if th_dict is None or len(th_dict) == 0:\n        return qsym\n    num_layer_outputs = len(th_dict)\n    layer_output_names = []\n    min_vals = []\n    max_vals = []\n    for k, v in th_dict.items():\n        layer_output_names.append(k)\n        min_vals.append(v[0])\n        max_vals.append(v[1])\n\n    calibrated_sym = SymbolHandle()\n    check_call(_LIB.MXSetCalibTableToQuantizedSymbol(qsym.handle,\n                                                     mx_uint(num_layer_outputs),\n                                                     c_str_array(layer_output_names),\n                                                     c_array(ctypes.c_float, min_vals),\n                                                     c_array(ctypes.c_float, max_vals),\n                                                     ctypes.byref(calibrated_sym)))\n    return Symbol(calibrated_sym)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _collect_layer_output_min_max(mod, data, include_layer=None,\n                                  max_num_examples=None, logger=None):\n    \"\"\"Collect min and max values from layer outputs and save them in\n    a dictionary mapped by layer names.\n    \"\"\"\n    collector = _LayerOutputMinMaxCollector(include_layer=include_layer, logger=logger)\n    num_examples = _collect_layer_statistics(mod, data, collector, max_num_examples, logger)\n    return collector.min_max_dict, num_examples", "response": "Collect min and max values from layer outputs and save them in\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncollects layer outputs and save them in a dictionary mapped by layer names.", "response": "def _collect_layer_outputs(mod, data, include_layer=None, max_num_examples=None, logger=None):\n    \"\"\"Collect layer outputs and save them in a dictionary mapped by layer names.\"\"\"\n    collector = _LayerOutputCollector(include_layer=include_layer, logger=logger)\n    num_examples = _collect_layer_statistics(mod, data, collector, max_num_examples, logger)\n    return collector.nd_dict, num_examples"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _smooth_distribution(p, eps=0.0001):\n    is_zeros = (p == 0).astype(np.float32)\n    is_nonzeros = (p != 0).astype(np.float32)\n    n_zeros = is_zeros.sum()\n    n_nonzeros = p.size - n_zeros\n    if not n_nonzeros:\n        raise ValueError('The discrete probability distribution is malformed. All entries are 0.')\n    eps1 = eps * float(n_zeros) / float(n_nonzeros)\n    assert eps1 < 1.0, 'n_zeros=%d, n_nonzeros=%d, eps1=%f' % (n_zeros, n_nonzeros, eps1)\n    hist = p.astype(np.float32)\n    hist += eps * is_zeros + (-eps1) * is_nonzeros\n    assert (hist <= 0).sum() == 0\n    return hist", "response": "Given a discrete distribution p smooth it by replacing zeros with eps multiplied by a scaling factor and taking the corresponding amount off the non - zero values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving a dataset find the optimal threshold for quantizing it.", "response": "def _get_optimal_threshold(arr, quantized_dtype, num_bins=8001, num_quantized_bins=255):\n    \"\"\"Given a dataset, find the optimal threshold for quantizing it.\n    The reference distribution is `q`, and the candidate distribution is `p`.\n    `q` is a truncated version of the original distribution.\n\n    Ref: http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf\n    \"\"\"\n    if isinstance(arr, NDArray):\n        arr = arr.asnumpy()\n    elif isinstance(arr, list):\n        assert len(arr) != 0\n        for i, nd in enumerate(arr):\n            if isinstance(nd, NDArray):\n                arr[i] = nd.asnumpy()\n            elif not isinstance(nd, np.ndarray):\n                raise TypeError('get_optimal_threshold only supports input type of NDArray,'\n                                ' list of np.ndarrays or NDArrays, and np.ndarray,'\n                                ' while received type=%s' % (str(type(nd))))\n        arr = np.concatenate(arr)\n    elif not isinstance(arr, np.ndarray):\n        raise TypeError('get_optimal_threshold only supports input type of NDArray,'\n                        ' list of NDArrays and np.ndarray,'\n                        ' while received type=%s' % (str(type(arr))))\n    min_val = np.min(arr)\n    max_val = np.max(arr)\n    th = max(abs(min_val), abs(max_val))\n\n    if min_val >= 0 and quantized_dtype in ['auto', 'uint8']:\n        # We need to move negative bins to positive bins to fit uint8 range.\n        num_quantized_bins = num_quantized_bins * 2 + 1\n\n    hist, hist_edges = np.histogram(arr, bins=num_bins, range=(-th, th))\n    zero_bin_idx = num_bins // 2\n    num_half_quantized_bins = num_quantized_bins // 2\n\n    thresholds = np.zeros(num_bins // 2 + 1 - num_quantized_bins // 2)\n    divergence = np.zeros_like(thresholds)\n    quantized_bins = np.zeros(num_quantized_bins, dtype=np.int32)\n    # i means the number of bins on half axis excluding the zero bin.\n    for i in range(num_quantized_bins // 2,\n                   num_bins // 2 + 1):\n        p_bin_idx_start = zero_bin_idx - i\n        p_bin_idx_stop = zero_bin_idx + i + 1\n        thresholds[i - num_half_quantized_bins] = hist_edges[p_bin_idx_stop]\n        sliced_nd_hist = hist[p_bin_idx_start:p_bin_idx_stop]\n\n        # generate reference distribution p\n        p = sliced_nd_hist.copy()\n        assert p.size % 2 == 1\n        assert p.size >= num_quantized_bins\n        # put left outlier count in p[0]\n        left_outlier_count = np.sum(hist[0:p_bin_idx_start])\n        p[0] += left_outlier_count\n        # put right outlier count in p[-1]\n        right_outlier_count = np.sum(hist[p_bin_idx_stop:])\n        p[-1] += right_outlier_count\n        # is_nonzeros[k] indicates whether hist[k] is nonzero\n        is_nonzeros = (p != 0).astype(np.int32)\n\n        # calculate how many bins should be merged to generate quantized distribution q\n        num_merged_bins = sliced_nd_hist.size // num_quantized_bins\n        # merge hist into num_quantized_bins bins\n        for j in range(num_quantized_bins):\n            start = j * num_merged_bins\n            stop = start + num_merged_bins\n            quantized_bins[j] = sliced_nd_hist[start:stop].sum()\n        quantized_bins[-1] += sliced_nd_hist[num_quantized_bins * num_merged_bins:].sum()\n        # expand quantized_bins into p.size bins\n        q = np.zeros(sliced_nd_hist.size, dtype=np.float32)\n        for j in range(num_quantized_bins):\n            start = j * num_merged_bins\n            if j == num_quantized_bins - 1:\n                stop = len(is_nonzeros)\n            else:\n                stop = start + num_merged_bins\n            norm = is_nonzeros[start:stop].sum()\n            if norm != 0:\n                q[start:stop] = float(quantized_bins[j]) / float(norm)\n        q[p == 0] = 0\n        p = _smooth_distribution(p)\n        # There is a chance that q is an invalid probability distribution.\n        try:\n            q = _smooth_distribution(q)\n        except ValueError:\n            divergence[i - num_half_quantized_bins] = float(\"inf\")\n        divergence[i - num_half_quantized_bins] = stats.entropy(p, q)\n\n    min_divergence_idx = np.argmin(divergence)\n    min_divergence = divergence[min_divergence_idx]\n    opt_th = thresholds[min_divergence_idx]\n    return min_val, max_val, min_divergence, opt_th"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives a ndarray dict find the optimal thresholds for quantizing each value of the key.", "response": "def _get_optimal_thresholds(nd_dict, quantized_dtype, num_bins=8001, num_quantized_bins=255, logger=None):\n    \"\"\"Given a ndarray dict, find the optimal threshold for quantizing each value of the key.\"\"\"\n    if stats is None:\n        raise ImportError('scipy.stats is required for running entropy mode of calculating'\n                          ' the optimal thresholds for quantizing FP32 ndarrays into int8.'\n                          ' Please check if the scipy python bindings are installed.')\n    assert isinstance(nd_dict, dict)\n    if logger is not None:\n        logger.info('Calculating optimal thresholds for quantization using KL divergence'\n                    ' with num_bins=%d and num_quantized_bins=%d' % (num_bins, num_quantized_bins))\n    th_dict = {}\n    # copy nd_dict keys since the keys() only returns a view in python3\n    layer_names = list(nd_dict.keys())\n    for name in layer_names:\n        assert name in nd_dict\n        min_val, max_val, min_divergence, opt_th = \\\n            _get_optimal_threshold(nd_dict[name], quantized_dtype, num_bins=num_bins,\n                                   num_quantized_bins=num_quantized_bins)\n        del nd_dict[name]  # release the memory of ndarray\n        if min_val < 0:\n            th_dict[name] = (-opt_th, opt_th)\n        else:\n            th_dict[name] = (0, opt_th)\n        if logger is not None:\n            logger.info('layer=%s, min_val=%f, max_val=%f, min_divergence=%f, optimal_threshold=%f'\n                        % (name, min_val, max_val, min_divergence, opt_th))\n    return th_dict"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading a symbol from a file or a symbol object.", "response": "def _load_sym(sym, logger=logging):\n    \"\"\"Given a str as a path the symbol .json file or a symbol, returns a Symbol object.\"\"\"\n    if isinstance(sym, str):  # sym is a symbol file path\n        cur_path = os.path.dirname(os.path.realpath(__file__))\n        symbol_file_path = os.path.join(cur_path, sym)\n        logger.info('Loading symbol from file %s' % symbol_file_path)\n        return sym_load(symbol_file_path)\n    elif isinstance(sym, Symbol):\n        return sym\n    else:\n        raise ValueError('_load_sym only accepts Symbol or path to the symbol file,'\n                         ' while received type %s' % str(type(sym)))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _load_params(params, logger=logging):\n    if isinstance(params, str):\n        cur_path = os.path.dirname(os.path.realpath(__file__))\n        param_file_path = os.path.join(cur_path, params)\n        logger.info('Loading params from file %s' % param_file_path)\n        save_dict = nd_load(param_file_path)\n        arg_params = {}\n        aux_params = {}\n        for k, v in save_dict.items():\n            tp, name = k.split(':', 1)\n            if tp == 'arg':\n                arg_params[name] = v\n            if tp == 'aux':\n                aux_params[name] = v\n        return arg_params, aux_params\n    elif isinstance(params, (tuple, list)) and len(params) == 2:\n        return params[0], params[1]\n    else:\n        raise ValueError('Unsupported params provided. Must be either a path to the param file or'\n                         ' a pair of dictionaries representing arg_params and aux_params')", "response": "Load the params file and return two dictionaries representing arg_params and aux_params."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef collect(self, name, arr):\n        name = py_str(name)\n        if self.include_layer is not None and not self.include_layer(name):\n            return\n        handle = ctypes.cast(arr, NDArrayHandle)\n        arr = NDArray(handle, writable=False).copyto(cpu())\n        if self.logger is not None:\n            self.logger.info(\"Collecting layer %s output of shape %s\" % (name, arr.shape))\n        if name in self.nd_dict:\n            self.nd_dict[name].append(arr)\n        else:\n            self.nd_dict[name] = [arr]", "response": "Callback function for collecting layer output NDArrays."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef collect(self, name, arr):\n        name = py_str(name)\n        if self.include_layer is not None and not self.include_layer(name):\n            return\n        handle = ctypes.cast(arr, NDArrayHandle)\n        arr = NDArray(handle, writable=False)\n        min_range = ndarray.min(arr).asscalar()\n        max_range = ndarray.max(arr).asscalar()\n        if name in self.min_max_dict:\n            cur_min_max = self.min_max_dict[name]\n            self.min_max_dict[name] = (min(cur_min_max[0], min_range),\n                                       max(cur_min_max[1], max_range))\n        else:\n            self.min_max_dict[name] = (min_range, max_range)\n        if self.logger is not None:\n            self.logger.info(\"Collecting layer %s min_range=%f, max_range=%f\"\n                             % (name, min_range, max_range))", "response": "Callback function for collecting min and max values from an NDArray."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef generator(ngf, nc, no_bias=True, fix_gamma=True, eps=1e-5 + 1e-12, z_dim=100, activation='sigmoid'):\n    '''The genrator is a CNN which takes 100 dimensional embedding as input\n    and reconstructs the input image given to the encoder\n    '''\n    BatchNorm = mx.sym.BatchNorm\n    rand = mx.sym.Variable('rand')\n\n    rand = mx.sym.Reshape(rand, shape=(-1, z_dim, 1, 1))\n\n    g1 = mx.sym.Deconvolution(rand, name='gen1', kernel=(5,5), stride=(2,2),target_shape=(2,2), num_filter=ngf*8, no_bias=no_bias)\n    gbn1 = BatchNorm(g1, name='genbn1', fix_gamma=fix_gamma, eps=eps)\n    gact1 = mx.sym.Activation(gbn1, name=\"genact1\", act_type=\"relu\")\n\n    g2 = mx.sym.Deconvolution(gact1, name='gen2', kernel=(5,5), stride=(2,2),target_shape=(4,4), num_filter=ngf*4, no_bias=no_bias)\n    gbn2 = BatchNorm(g2, name='genbn2', fix_gamma=fix_gamma, eps=eps)\n    gact2 = mx.sym.Activation(gbn2, name='genact2', act_type='relu')\n\n    g3 = mx.sym.Deconvolution(gact2, name='gen3', kernel=(5,5), stride=(2,2), target_shape=(8,8), num_filter=ngf*2, no_bias=no_bias)\n    gbn3 = BatchNorm(g3, name='genbn3', fix_gamma=fix_gamma, eps=eps)\n    gact3 = mx.sym.Activation(gbn3, name='genact3', act_type='relu')\n\n    g4 = mx.sym.Deconvolution(gact3, name='gen4', kernel=(5,5), stride=(2,2), target_shape=(16,16), num_filter=ngf, no_bias=no_bias)\n    gbn4 = BatchNorm(g4, name='genbn4', fix_gamma=fix_gamma, eps=eps)\n    gact4 = mx.sym.Activation(gbn4, name='genact4', act_type='relu')\n\n    g5 = mx.sym.Deconvolution(gact4, name='gen5', kernel=(5,5), stride=(2,2), target_shape=(32,32), num_filter=nc, no_bias=no_bias)\n    gout = mx.sym.Activation(g5, name='genact5', act_type=activation)\n\n    return gout", "response": "The generator is a CNN which takes 100 dimensional embedding as input\n    and reconstructs the input image given to the encoder\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef discriminator2(ndf, no_bias=True, fix_gamma=True, eps=1e-5 + 1e-12):\n    '''Second part of the discriminator which takes a 256x8x8 feature map as input\n    and generates the loss based on whether the input image was a real one or fake one'''\n\n    BatchNorm = mx.sym.BatchNorm\n\n    data = mx.sym.Variable('data')\n\n    label = mx.sym.Variable('label')\n\n    d4 = mx.sym.Convolution(data, name='d4', kernel=(5,5), stride=(2,2), pad=(2,2), num_filter=ndf*8, no_bias=no_bias)\n    dbn4 = BatchNorm(d4, name='dbn4', fix_gamma=fix_gamma, eps=eps)\n    dact4 = mx.sym.LeakyReLU(dbn4, name='dact4', act_type='leaky', slope=0.2)\n\n    h = mx.sym.Flatten(dact4)\n\n    d5 = mx.sym.FullyConnected(h, num_hidden=1, name=\"d5\")\n\n    dloss = mx.sym.LogisticRegressionOutput(data=d5, label=label, name='dloss')\n\n    return dloss", "response": "Second part of the discriminator which takes a 256x8x8 feature map as input\n    and generates the loss based on whether the input image was a real one or fake one"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef GaussianLogDensity(x, mu, log_var, name='GaussianLogDensity', EPSILON = 1e-6):\n    '''GaussianLogDensity loss calculation for layer wise loss\n    '''\n    c = mx.sym.ones_like(log_var)*2.0 * 3.1416\n    c = mx.symbol.log(c)\n    var = mx.sym.exp(log_var)\n    x_mu2 = mx.symbol.square(x - mu)   # [Issue] not sure the dim works or not?\n    x_mu2_over_var = mx.symbol.broadcast_div(x_mu2, var + EPSILON)\n    log_prob = -0.5 * (c + log_var + x_mu2_over_var)\n    log_prob = mx.symbol.sum(log_prob, axis=1, name=name)   # keep_dims=True,\n    return log_prob", "response": "GaussianLogDensity loss calculation for layer wise loss"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the discriminator layer loss", "response": "def DiscriminatorLayerLoss():\n    '''Calculate the discriminator layer loss\n    '''\n\n    data = mx.sym.Variable('data')\n\n    label = mx.sym.Variable('label')\n\n    data = mx.sym.Flatten(data)\n    label = mx.sym.Flatten(label)\n\n    label = mx.sym.BlockGrad(label)\n\n    zeros = mx.sym.zeros_like(data)\n\n    output = -GaussianLogDensity(label, data, zeros)\n\n    dloss = mx.symbol.MakeLoss(mx.symbol.mean(output),name='lloss')\n\n    return dloss"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fill_buf(buf, i, img, shape):\n    '''fill the ith grid of the buffer matrix with the values from the img\n    buf : buffer matrix\n    i : serial of the image in the 2D grid\n    img : image data\n    shape : ( height width depth ) of image'''\n\n    # grid height is a multiple of individual image height\n    m = buf.shape[0]/shape[0]\n\n    sx = (i%m)*shape[1]\n    sy = (i//m)*shape[0]\n    sx = int(sx)\n    sy = int(sy)\n    buf[sy:sy+shape[0], sx:sx+shape[1], :] = img", "response": "fill the ith grid of the buffer matrix with the values from the img\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a grid of images and save it as a final image", "response": "def visual(title, X, activation):\n    '''create a grid of images and save it as a final image\n    title : grid image name\n    X : array of images\n    '''\n    assert len(X.shape) == 4\n\n    X = X.transpose((0, 2, 3, 1))\n    if activation == 'sigmoid':\n        X = np.clip((X)*(255.0), 0, 255).astype(np.uint8)\n    elif activation == 'tanh':\n        X = np.clip((X+1.0)*(255.0/2.0), 0, 255).astype(np.uint8)\n    n = np.ceil(np.sqrt(X.shape[0]))\n    buff = np.zeros((int(n*X.shape[1]), int(n*X.shape[2]), int(X.shape[3])), dtype=np.uint8)\n    for i, img in enumerate(X):\n        fill_buf(buff, i, img, X.shape[1:3])\n    cv2.imwrite('%s.jpg' % (title), buff)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef train(dataset, nef, ndf, ngf, nc, batch_size, Z, lr, beta1, epsilon, ctx, check_point, g_dl_weight, output_path, checkpoint_path, data_path, activation,num_epoch, save_after_every, visualize_after_every, show_after_every):\n    '''adversarial training of the VAE\n    '''\n\n    #encoder\n    z_mu, z_lv, z = encoder(nef, Z, batch_size)\n    symE = mx.sym.Group([z_mu, z_lv, z])\n\n    #generator\n    symG = generator(ngf, nc, no_bias=True, fix_gamma=True, eps=1e-5 + 1e-12, z_dim = Z, activation=activation )\n\n    #discriminator\n    h  = discriminator1(ndf)\n    dloss  = discriminator2(ndf)\n    symD1 = h\n    symD2 = dloss\n\n\n    # ==============data==============\n    X_train, _ = get_data(data_path, activation)\n    train_iter = mx.io.NDArrayIter(X_train, batch_size=batch_size, shuffle=True)\n    rand_iter = RandIter(batch_size, Z)\n    label = mx.nd.zeros((batch_size,), ctx=ctx)\n\n    # =============module E=============\n    modE = mx.mod.Module(symbol=symE, data_names=('data',), label_names=None, context=ctx)\n    modE.bind(data_shapes=train_iter.provide_data)\n    modE.init_params(initializer=mx.init.Normal(0.02))\n    modE.init_optimizer(\n        optimizer='adam',\n        optimizer_params={\n            'learning_rate': lr,\n            'wd': 1e-6,\n            'beta1': beta1,\n            'epsilon': epsilon,\n            'rescale_grad': (1.0/batch_size)\n        })\n    mods = [modE]\n\n    # =============module G=============\n    modG = mx.mod.Module(symbol=symG, data_names=('rand',), label_names=None, context=ctx)\n    modG.bind(data_shapes=rand_iter.provide_data, inputs_need_grad=True)\n    modG.init_params(initializer=mx.init.Normal(0.02))\n    modG.init_optimizer(\n        optimizer='adam',\n        optimizer_params={\n            'learning_rate': lr,\n            'wd': 1e-6,\n            'beta1': beta1,\n            'epsilon': epsilon,\n        })\n    mods.append(modG)\n\n    # =============module D=============\n    modD1 = mx.mod.Module(symD1, label_names=[], context=ctx)\n    modD2 = mx.mod.Module(symD2, label_names=('label',), context=ctx)\n    modD = mx.mod.SequentialModule()\n    modD.add(modD1).add(modD2, take_labels=True, auto_wiring=True)\n    modD.bind(data_shapes=train_iter.provide_data,\n              label_shapes=[('label', (batch_size,))],\n              inputs_need_grad=True)\n    modD.init_params(initializer=mx.init.Normal(0.02))\n    modD.init_optimizer(\n        optimizer='adam',\n        optimizer_params={\n            'learning_rate': lr,\n            'wd': 1e-3,\n            'beta1': beta1,\n            'epsilon': epsilon,\n            'rescale_grad': (1.0/batch_size)\n        })\n    mods.append(modD)\n\n\n    # =============module DL=============\n    symDL = DiscriminatorLayerLoss()\n    modDL = mx.mod.Module(symbol=symDL, data_names=('data',), label_names=('label',), context=ctx)\n    modDL.bind(data_shapes=[('data', (batch_size,nef * 4,4,4))], ################################################################################################################################ fix 512 here\n              label_shapes=[('label', (batch_size,nef * 4,4,4))],\n              inputs_need_grad=True)\n    modDL.init_params(initializer=mx.init.Normal(0.02))\n    modDL.init_optimizer(\n        optimizer='adam',\n        optimizer_params={\n            'learning_rate': lr,\n            'wd': 0.,\n            'beta1': beta1,\n            'epsilon': epsilon,\n            'rescale_grad': (1.0/batch_size)\n        })\n\n    # =============module KL=============\n    symKL = KLDivergenceLoss()\n    modKL = mx.mod.Module(symbol=symKL, data_names=('data',), label_names=None, context=ctx)\n    modKL.bind(data_shapes=[('data', (batch_size*2,Z))],\n               inputs_need_grad=True)\n    modKL.init_params(initializer=mx.init.Normal(0.02))\n    modKL.init_optimizer(\n        optimizer='adam',\n        optimizer_params={\n            'learning_rate': lr,\n            'wd': 0.,\n            'beta1': beta1,\n            'epsilon': epsilon,\n            'rescale_grad': (1.0/batch_size)\n        })\n    mods.append(modKL)\n\n    def norm_stat(d):\n        return mx.nd.norm(d)/np.sqrt(d.size)\n    mon = mx.mon.Monitor(10, norm_stat, pattern=\".*output|d1_backward_data\", sort=True)\n    mon = None\n    if mon is not None:\n        for mod in mods:\n            pass\n\n    def facc(label, pred):\n        '''calculating prediction accuracy\n        '''\n        pred = pred.ravel()\n        label = label.ravel()\n        return ((pred > 0.5) == label).mean()\n\n    def fentropy(label, pred):\n        '''calculating binary cross-entropy loss\n        '''\n        pred = pred.ravel()\n        label = label.ravel()\n        return -(label*np.log(pred+1e-12) + (1.-label)*np.log(1.-pred+1e-12)).mean()\n\n    def kldivergence(label, pred):\n        '''calculating KL divergence loss\n        '''\n        mean, log_var = np.split(pred, 2, axis=0)\n        var = np.exp(log_var)\n        KLLoss = -0.5 * np.sum(1 + log_var - np.power(mean, 2) - var)\n        KLLoss = KLLoss / nElements\n        return KLLoss\n\n    mG = mx.metric.CustomMetric(fentropy)\n    mD = mx.metric.CustomMetric(fentropy)\n    mE = mx.metric.CustomMetric(kldivergence)\n    mACC = mx.metric.CustomMetric(facc)\n\n    print('Training...')\n    stamp =  datetime.now().strftime('%Y_%m_%d-%H_%M')\n\n    # =============train===============\n    for epoch in range(num_epoch):\n        train_iter.reset()\n        for t, batch in enumerate(train_iter):\n\n            rbatch = rand_iter.next()\n\n            if mon is not None:\n                mon.tic()\n\n            modG.forward(rbatch, is_train=True)\n            outG = modG.get_outputs()\n\n            # update discriminator on fake\n            label[:] = 0\n            modD.forward(mx.io.DataBatch(outG, [label]), is_train=True)\n            modD.backward()\n            gradD11 = [[grad.copyto(grad.context) for grad in grads] for grads in modD1._exec_group.grad_arrays]\n            gradD12 = [[grad.copyto(grad.context) for grad in grads] for grads in modD2._exec_group.grad_arrays]\n\n            modD.update_metric(mD, [label])\n            modD.update_metric(mACC, [label])\n\n\n            #update discriminator on decoded\n            modE.forward(batch, is_train=True)\n            mu, lv, z = modE.get_outputs()\n            z = z.reshape((batch_size, Z, 1, 1))\n            sample = mx.io.DataBatch([z], label=None, provide_data = [('rand', (batch_size, Z, 1, 1))])\n            modG.forward(sample, is_train=True)\n            xz = modG.get_outputs()\n            label[:] = 0\n            modD.forward(mx.io.DataBatch(xz, [label]), is_train=True)\n            modD.backward()\n\n            #modD.update()\n            gradD21 = [[grad.copyto(grad.context) for grad in grads] for grads in modD1._exec_group.grad_arrays]\n            gradD22 = [[grad.copyto(grad.context) for grad in grads] for grads in modD2._exec_group.grad_arrays]\n            modD.update_metric(mD, [label])\n            modD.update_metric(mACC, [label])\n\n            # update discriminator on real\n            label[:] = 1\n            batch.label = [label]\n            modD.forward(batch, is_train=True)\n            lx = [out.copyto(out.context) for out in modD1.get_outputs()]\n            modD.backward()\n            for gradsr, gradsf, gradsd in zip(modD1._exec_group.grad_arrays, gradD11, gradD21):\n                for gradr, gradf, gradd in zip(gradsr, gradsf, gradsd):\n                    gradr += 0.5 * (gradf + gradd)\n            for gradsr, gradsf, gradsd in zip(modD2._exec_group.grad_arrays, gradD12, gradD22):\n                for gradr, gradf, gradd in zip(gradsr, gradsf, gradsd):\n                    gradr += 0.5 * (gradf + gradd)\n\n            modD.update()\n            modD.update_metric(mD, [label])\n            modD.update_metric(mACC, [label])\n\n            modG.forward(rbatch, is_train=True)\n            outG = modG.get_outputs()\n            label[:] = 1\n            modD.forward(mx.io.DataBatch(outG, [label]), is_train=True)\n            modD.backward()\n            diffD = modD1.get_input_grads()\n            modG.backward(diffD)\n            gradG1 = [[grad.copyto(grad.context) for grad in grads] for grads in modG._exec_group.grad_arrays]\n            mG.update([label], modD.get_outputs())\n\n            modG.forward(sample, is_train=True)\n            xz = modG.get_outputs()\n            label[:] = 1\n            modD.forward(mx.io.DataBatch(xz, [label]), is_train=True)\n            modD.backward()\n            diffD = modD1.get_input_grads()\n            modG.backward(diffD)\n            gradG2 = [[grad.copyto(grad.context) for grad in grads] for grads in modG._exec_group.grad_arrays]\n            mG.update([label], modD.get_outputs())\n\n            modG.forward(sample, is_train=True)\n            xz = modG.get_outputs()\n            modD1.forward(mx.io.DataBatch(xz, []), is_train=True)\n            outD1 = modD1.get_outputs()\n            modDL.forward(mx.io.DataBatch(outD1, lx), is_train=True)\n            modDL.backward()\n            dlGrad = modDL.get_input_grads()\n            modD1.backward(dlGrad)\n            diffD = modD1.get_input_grads()\n            modG.backward(diffD)\n\n            for grads, gradsG1, gradsG2 in zip(modG._exec_group.grad_arrays, gradG1, gradG2):\n                for grad, gradg1, gradg2 in zip(grads, gradsG1, gradsG2):\n                    grad = g_dl_weight * grad + 0.5 * (gradg1 + gradg2)\n\n            modG.update()\n            mG.update([label], modD.get_outputs())\n\n            modG.forward(rbatch, is_train=True)\n            outG = modG.get_outputs()\n            label[:] = 1\n            modD.forward(mx.io.DataBatch(outG, [label]), is_train=True)\n            modD.backward()\n            diffD = modD1.get_input_grads()\n            modG.backward(diffD)\n            gradG1 = [[grad.copyto(grad.context) for grad in grads] for grads in modG._exec_group.grad_arrays]\n            mG.update([label], modD.get_outputs())\n\n            modG.forward(sample, is_train=True)\n            xz = modG.get_outputs()\n            label[:] = 1\n            modD.forward(mx.io.DataBatch(xz, [label]), is_train=True)\n            modD.backward()\n            diffD = modD1.get_input_grads()\n            modG.backward(diffD)\n            gradG2 = [[grad.copyto(grad.context) for grad in grads] for grads in modG._exec_group.grad_arrays]\n            mG.update([label], modD.get_outputs())\n\n            modG.forward(sample, is_train=True)\n            xz = modG.get_outputs()\n            modD1.forward(mx.io.DataBatch(xz, []), is_train=True)\n            outD1 = modD1.get_outputs()\n            modDL.forward(mx.io.DataBatch(outD1, lx), is_train=True)\n            modDL.backward()\n            dlGrad = modDL.get_input_grads()\n            modD1.backward(dlGrad)\n            diffD = modD1.get_input_grads()\n            modG.backward(diffD)\n\n            for grads, gradsG1, gradsG2 in zip(modG._exec_group.grad_arrays, gradG1, gradG2):\n                for grad, gradg1, gradg2 in zip(grads, gradsG1, gradsG2):\n                    grad = g_dl_weight * grad + 0.5 * (gradg1 + gradg2)\n\n            modG.update()\n            mG.update([label], modD.get_outputs())\n\n            modG.forward(sample, is_train=True)\n            xz = modG.get_outputs()\n\n            #update generator\n            modD1.forward(mx.io.DataBatch(xz, []), is_train=True)\n            outD1 = modD1.get_outputs()\n            modDL.forward(mx.io.DataBatch(outD1, lx), is_train=True)\n            DLloss = modDL.get_outputs()\n            modDL.backward()\n            dlGrad = modDL.get_input_grads()\n            modD1.backward(dlGrad)\n            diffD = modD1.get_input_grads()\n            modG.backward(diffD)\n            #update encoder\n            nElements = batch_size\n            modKL.forward(mx.io.DataBatch([mx.ndarray.concat(mu,lv, dim=0)]), is_train=True)\n            KLloss = modKL.get_outputs()\n            modKL.backward()\n            gradKLLoss = modKL.get_input_grads()\n            diffG = modG.get_input_grads()\n            diffG = diffG[0].reshape((batch_size, Z))\n            modE.backward(mx.ndarray.split(gradKLLoss[0], num_outputs=2, axis=0) + [diffG])\n            modE.update()\n            pred = mx.ndarray.concat(mu,lv, dim=0)\n            mE.update([pred], [pred])\n            if mon is not None:\n                mon.toc_print()\n\n            t += 1\n            if t % show_after_every == 0:\n                print('epoch:', epoch, 'iter:', t, 'metric:', mACC.get(), mG.get(), mD.get(), mE.get(), KLloss[0].asnumpy(), DLloss[0].asnumpy())\n                mACC.reset()\n                mG.reset()\n                mD.reset()\n                mE.reset()\n\n            if epoch % visualize_after_every == 0:\n                visual(output_path +'gout'+str(epoch), outG[0].asnumpy(), activation)\n                visual(output_path + 'data'+str(epoch), batch.data[0].asnumpy(), activation)\n\n        if check_point and epoch % save_after_every == 0:\n            print('Saving...')\n            modG.save_params(checkpoint_path + '/%s_G-%04d.params'%(dataset, epoch))\n            modD.save_params(checkpoint_path + '/%s_D-%04d.params'%(dataset, epoch))\n            modE.save_params(checkpoint_path + '/%s_E-%04d.params'%(dataset, epoch))", "response": "Train the VAE with the given dataset."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating and validates the directory structure.", "response": "def create_and_validate_dir(data_dir):\n    '''Creates/Validates dir\n    '''\n    if data_dir != \"\":\n        if not os.path.exists(data_dir):\n            try:\n                logging.info('create directory %s', data_dir)\n                os.makedirs(data_dir)\n            except OSError as exc:\n                if exc.errno != errno.EEXIST:\n                    raise OSError('failed to create ' + data_dir)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse command line arguments for the function.", "response": "def parse_args():\n    '''Parse args\n    '''\n    parser = argparse.ArgumentParser(description='Train and Test an Adversarial Variatiional Encoder')\n\n    parser.add_argument('--train', help='train the network', action='store_true')\n    parser.add_argument('--test', help='test the network', action='store_true')\n    parser.add_argument('--save_embedding', help='saves the shape embedding of each input image', action='store_true')\n    parser.add_argument('--dataset', help='dataset name', default='caltech', type=str)\n    parser.add_argument('--activation', help='activation i.e. sigmoid or tanh', default='sigmoid', type=str)\n    parser.add_argument('--training_data_path', help='training data path', default='datasets/caltech101/data/images32x32', type=str)\n    parser.add_argument('--testing_data_path', help='testing data path', default='datasets/caltech101/test_data', type=str)\n    parser.add_argument('--pretrained_encoder_path', help='pretrained encoder model path', default='checkpoints32x32_sigmoid/caltech_E-0045.params', type=str)\n    parser.add_argument('--pretrained_generator_path', help='pretrained generator model path', default='checkpoints32x32_sigmoid/caltech_G-0045.params', type=str)\n    parser.add_argument('--output_path', help='output path for the generated images', default='outputs32x32_sigmoid', type=str)\n    parser.add_argument('--embedding_path', help='output path for the generated embeddings', default='outputs32x32_sigmoid', type=str)\n    parser.add_argument('--checkpoint_path', help='checkpoint saving path ', default='checkpoints32x32_sigmoid', type=str)\n    parser.add_argument('--nef', help='encoder filter count in the first layer', default=64, type=int)\n    parser.add_argument('--ndf', help='discriminator filter count in the first layer', default=64, type=int)\n    parser.add_argument('--ngf', help='generator filter count in the second last layer', default=64, type=int)\n    parser.add_argument('--nc', help='generator filter count in the last layer i.e. 1 for grayscale image, 3 for RGB image', default=1, type=int)\n    parser.add_argument('--batch_size', help='batch size, keep it 1 during testing', default=64, type=int)\n    parser.add_argument('--Z', help='embedding size', default=100, type=int)\n    parser.add_argument('--lr', help='learning rate', default=0.0002, type=float)\n    parser.add_argument('--beta1', help='beta1 for adam optimizer', default=0.5, type=float)\n    parser.add_argument('--epsilon', help='epsilon for adam optimizer', default=1e-5, type=float)\n    parser.add_argument('--g_dl_weight', help='discriminator layer loss weight', default=1e-1, type=float)\n    parser.add_argument('--gpu', help='gpu index', default=0, type=int)\n    parser.add_argument('--use_cpu', help='use cpu', action='store_true')\n    parser.add_argument('--num_epoch', help='number of maximum epochs ', default=45, type=int)\n    parser.add_argument('--save_after_every', help='save checkpoint after every this number of epochs ', default=5, type=int)\n    parser.add_argument('--visualize_after_every', help='save output images after every this number of epochs', default=5, type=int)\n    parser.add_argument('--show_after_every', help='show metrics after this number of iterations', default=10, type=int)\n\n    args = parser.parse_args()\n    return args"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting root mse between the logarithms of the prediction and the truth.", "response": "def get_rmse_log(net, X_train, y_train):\n    \"\"\"Gets root mse between the logarithms of the prediction and the truth.\"\"\"\n    num_train = X_train.shape[0]\n    clipped_preds = nd.clip(net(X_train), 1, float('inf'))\n    return np.sqrt(2 * nd.sum(square_loss(\n        nd.log(clipped_preds), nd.log(y_train))).asscalar() / num_train)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_net():\n    net = gluon.nn.Sequential()\n    with net.name_scope():\n        net.add(gluon.nn.Dense(50, activation=\"relu\"))\n        net.add(gluon.nn.Dense(1))\n    net.initialize()\n    return net", "response": "Gets a neural network. Better results are obtained with modifications."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef k_fold_cross_valid(k, epochs, verbose_epoch, X_train, y_train,\n                       learning_rate, weight_decay, batch_size):\n    \"\"\"Conducts k-fold cross validation for the model.\"\"\"\n    assert k > 1\n    fold_size = X_train.shape[0] // k\n\n    train_loss_sum = 0.0\n    test_loss_sum = 0.0\n    for test_idx in range(k):\n        X_val_test = X_train[test_idx * fold_size: (test_idx + 1) *\n                                                   fold_size, :]\n        y_val_test = y_train[test_idx * fold_size: (test_idx + 1) * fold_size]\n        val_train_defined = False\n        for i in range(k):\n            if i != test_idx:\n                X_cur_fold = X_train[i * fold_size: (i + 1) * fold_size, :]\n                y_cur_fold = y_train[i * fold_size: (i + 1) * fold_size]\n                if not val_train_defined:\n                    X_val_train = X_cur_fold\n                    y_val_train = y_cur_fold\n                    val_train_defined = True\n                else:\n                    X_val_train = nd.concat(X_val_train, X_cur_fold, dim=0)\n                    y_val_train = nd.concat(y_val_train, y_cur_fold, dim=0)\n        net = get_net()\n        train_loss = train(net, X_val_train, y_val_train, epochs, verbose_epoch,\n                           learning_rate, weight_decay, batch_size)\n        train_loss_sum += train_loss\n        test_loss = get_rmse_log(net, X_val_test, y_val_test)\n        print(\"Test loss: %f\" % test_loss)\n        test_loss_sum += test_loss\n    return train_loss_sum / k, test_loss_sum / k", "response": "Conducts k - fold cross validation for the model."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef learn(epochs, verbose_epoch, X_train, y_train, test, learning_rate,\n          weight_decay, batch_size):\n    \"\"\"Trains the model and predicts on the test data set.\"\"\"\n    net = get_net()\n    _ = train(net, X_train, y_train, epochs, verbose_epoch, learning_rate,\n                 weight_decay, batch_size)\n    preds = net(X_test).asnumpy()\n    test['SalePrice'] = pd.Series(preds.reshape(1, -1)[0])\n    submission = pd.concat([test['Id'], test['SalePrice']], axis=1)\n    submission.to_csv('submission.csv', index=False)", "response": "Trains the model and predicts on the test data set."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef capsnet(batch_size, n_class, num_routing, recon_loss_weight):\n    # data.shape = [batch_size, 1, 28, 28]\n    data = mx.sym.Variable('data')\n\n    input_shape = (1, 28, 28)\n    # Conv2D layer\n    # net.shape = [batch_size, 256, 20, 20]\n    conv1 = mx.sym.Convolution(data=data,\n                               num_filter=256,\n                               kernel=(9, 9),\n                               layout='NCHW',\n                               name='conv1')\n    conv1 = mx.sym.Activation(data=conv1, act_type='relu', name='conv1_act')\n    # net.shape = [batch_size, 256, 6, 6]\n\n    primarycaps = primary_caps(data=conv1,\n                               dim_vector=8,\n                               n_channels=32,\n                               kernel=(9, 9),\n                               strides=[2, 2],\n                               name='primarycaps')\n    primarycaps.infer_shape(data=(batch_size, 1, 28, 28))\n    # CapsuleLayer\n    kernel_initializer = mx.init.Xavier(rnd_type='uniform', factor_type='avg', magnitude=3)\n    bias_initializer = mx.init.Zero()\n    digitcaps = CapsuleLayer(num_capsule=10,\n                             dim_vector=16,\n                             batch_size=batch_size,\n                             kernel_initializer=kernel_initializer,\n                             bias_initializer=bias_initializer,\n                             num_routing=num_routing)(primarycaps)\n\n    # out_caps : (batch_size, 10)\n    out_caps = mx.sym.sqrt(data=mx.sym.sum(mx.sym.square(digitcaps), 2))\n    out_caps.infer_shape(data=(batch_size, 1, 28, 28))\n\n    y = mx.sym.Variable('softmax_label', shape=(batch_size,))\n    y_onehot = mx.sym.one_hot(y, n_class)\n    y_reshaped = mx.sym.Reshape(data=y_onehot, shape=(batch_size, -4, n_class, -1))\n    y_reshaped.infer_shape(softmax_label=(batch_size,))\n\n    # inputs_masked : (batch_size, 16)\n    inputs_masked = mx.sym.linalg_gemm2(y_reshaped, digitcaps, transpose_a=True)\n    inputs_masked = mx.sym.Reshape(data=inputs_masked, shape=(-3, 0))\n    x_recon = mx.sym.FullyConnected(data=inputs_masked, num_hidden=512, name='x_recon')\n    x_recon = mx.sym.Activation(data=x_recon, act_type='relu', name='x_recon_act')\n    x_recon = mx.sym.FullyConnected(data=x_recon, num_hidden=1024, name='x_recon2')\n    x_recon = mx.sym.Activation(data=x_recon, act_type='relu', name='x_recon_act2')\n    x_recon = mx.sym.FullyConnected(data=x_recon, num_hidden=np.prod(input_shape), name='x_recon3')\n    x_recon = mx.sym.Activation(data=x_recon, act_type='sigmoid', name='x_recon_act3')\n\n    data_flatten = mx.sym.flatten(data=data)\n    squared_error = mx.sym.square(x_recon-data_flatten)\n    recon_error = mx.sym.mean(squared_error)\n    recon_error_stopped = recon_error\n    recon_error_stopped = mx.sym.BlockGrad(recon_error_stopped)\n    loss = mx.symbol.MakeLoss((1-recon_loss_weight)*margin_loss(y_onehot, out_caps)+recon_loss_weight*recon_error)\n\n    out_caps_blocked = out_caps\n    out_caps_blocked = mx.sym.BlockGrad(out_caps_blocked)\n    return mx.sym.Group([out_caps_blocked, loss, recon_error_stopped])", "response": "Create a CapsNet for the given batch size and class."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update(self, labels, preds):\n        batch_sum_metric = 0\n        batch_num_inst = 0\n        for label, pred_outcaps in zip(labels[0], preds[0]):\n            label_np = int(label.asnumpy())\n            pred_label = int(np.argmax(pred_outcaps.asnumpy()))\n            batch_sum_metric += int(label_np == pred_label)\n            batch_num_inst += 1\n        batch_loss = preds[1].asnumpy()\n        recon_loss = preds[2].asnumpy()\n        self.sum_metric += batch_sum_metric\n        self.num_inst += batch_num_inst\n        self.loss += batch_loss\n        self.recon_loss += recon_loss\n        self.batch_sum_metric = batch_sum_metric\n        self.batch_num_inst = batch_num_inst\n        self.batch_loss = batch_loss\n        self.n_batch += 1", "response": "Update the hyper - parameters and loss of CapsNet"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef reset(self):\n        # shuffle data\n        if self.is_train:\n            np.random.shuffle(self.idx)\n            self.data = _shuffle(self.data, self.idx)\n            self.label = _shuffle(self.label, self.idx)\n\n        if self.last_batch_handle == 'roll_over' and self.cursor > self.num_data:\n            self.cursor = -self.batch_size + (self.cursor % self.num_data) % self.batch_size\n        else:\n            self.cursor = -self.batch_size", "response": "Reset the MNIST custom iterator."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate next of iterator", "response": "def next(self):\n        \"\"\"Generate next of iterator\"\"\"\n        if self.iter_next():\n            if self.is_train:\n                data_raw_list = self.getdata()\n                data_shifted = []\n                for data_raw in data_raw_list[0]:\n                    data_shifted.append(random_shift(data_raw.asnumpy(), 0.1, 0.1))\n                return mx.io.DataBatch(data=[mx.nd.array(data_shifted)], label=self.getlabel(),\n                                       pad=self.getpad(), index=None)\n            else:\n                return mx.io.DataBatch(data=self.getdata(), label=self.getlabel(), pad=self.getpad(), index=None)\n\n        else:\n            raise StopIteration"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get(self, attr):\n        if self._attr:\n            ret = self._attr.copy()\n            if attr:\n                ret.update(attr)\n            return ret\n        else:\n            return attr if attr else {}", "response": "Get the attribute dict given the attribute set by the symbol."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _create_sparse_kvstore(kvstore):\n    # always update on kvstore\n    update_on_kvstore = True\n    if isinstance(kvstore, kvs.KVStore):\n        kv = kvstore\n    elif isinstance(kvstore, str):\n        kv = kvs.create(kvstore)\n    else:\n        raise TypeError(\"Cannot create '%s' KVStore with row_sparse parameters. \"\n                        \"The type must be KVStore or str.\" % kvstore)\n    return (kv, update_on_kvstore)", "response": "Create a kvstore assuming some parameters storage types are row_sparse."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _create_kvstore(kvstore, num_device, arg_params):\n    update_on_kvstore = bool(int(os.getenv('MXNET_UPDATE_ON_KVSTORE', \"1\")))\n    if kvstore is None:\n        kv = None\n    elif isinstance(kvstore, kvs.KVStore):\n        kv = kvstore\n    elif isinstance(kvstore, str):\n        # create kvstore using the string type\n        if num_device == 1 and 'dist' not in kvstore:\n            # no need to use kv for single device and single machine\n            kv = None\n        else:\n            kv = kvs.create(kvstore)\n            if kvstore == 'local':\n            # automatically select a proper local\n                max_size = max(np.prod(param.shape) for param in\n                               arg_params.values())\n                if max_size > 1024 * 1024 * 16:\n                    update_on_kvstore = False\n    else:\n        raise TypeError('kvstore must be KVStore, str or None')\n\n    if kv is None:\n        update_on_kvstore = False\n\n    return (kv, update_on_kvstore)", "response": "Create a kvstore if given the kvstore type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _initialize_kvstore(kvstore, param_arrays, arg_params, param_names, update_on_kvstore):\n    for idx, param_on_devs in enumerate(param_arrays):\n        name = param_names[idx]\n        kvstore.init(name, arg_params[name])\n\n        if update_on_kvstore:\n            kvstore.pull(name, param_on_devs, priority=-idx)", "response": "Initialize the kvstore with the given parameters."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nperform update of param_arrays from grad_arrays on NCCL kvstore.", "response": "def _update_params_on_kvstore_nccl(param_arrays, grad_arrays, kvstore, param_names):\n    \"\"\"Perform update of param_arrays from grad_arrays on NCCL kvstore.\"\"\"\n    valid_indices = [index for index, grad_list in\n                     enumerate(grad_arrays) if grad_list[0] is not None]\n    valid_grad_arrays = [grad_arrays[i] for i in valid_indices]\n    valid_param_arrays = [param_arrays[i] for i in valid_indices]\n    valid_param_names = [param_names[i] for i in valid_indices]\n    size = len(valid_grad_arrays)\n    start = 0\n    # Use aggregation by default only with NCCL\n    default_batch = '16'\n    batch = int(os.getenv('MXNET_UPDATE_AGGREGATION_SIZE', default_batch))\n    while start < size:\n        end = start + batch if start + batch < size else size\n        # push gradient, priority is negative index\n        kvstore.push(valid_param_names[start:end], valid_grad_arrays[start:end], priority=-start)\n        # pull back the weights\n        kvstore.pull(valid_param_names[start:end], valid_param_arrays[start:end], priority=-start)\n        start = end"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _update_params_on_kvstore(param_arrays, grad_arrays, kvstore, param_names):\n    for index, pair in enumerate(zip(param_arrays, grad_arrays)):\n        arg_list, grad_list = pair\n        if grad_list[0] is None:\n            continue\n        name = param_names[index]\n        # push gradient, priority is negative index\n        kvstore.push(name, grad_list, priority=-index)\n        # pull back the weights\n        kvstore.pull(name, arg_list, priority=-index)", "response": "Perform update of param_arrays from grad_arrays on kvstore."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nperform update of param_arrays from grad_arrays not on kvstore.", "response": "def _update_params(param_arrays, grad_arrays, updater, num_device,\n                   kvstore=None, param_names=None):\n    \"\"\"Perform update of param_arrays from grad_arrays not on kvstore.\"\"\"\n    updates = [[] for _ in range(num_device)]\n    for i, pair in enumerate(zip(param_arrays, grad_arrays)):\n        arg_list, grad_list = pair\n        if grad_list[0] is None:\n            continue\n        index = i\n        if kvstore:\n            name = param_names[index]\n            # push gradient, priority is negative index\n            kvstore.push(name, grad_list, priority=-index)\n            # pull back the sum gradients, to the same locations.\n            kvstore.pull(name, grad_list, priority=-index)\n        for k, p in enumerate(zip(arg_list, grad_list)):\n            # faked an index here, to make optimizer create diff\n            # state for the same index but on diff devs, TODO(mli)\n            # use a better solution later\n            w, g = p\n            updates[k].append((index*num_device+k, g, w))\n    for dev_updates in updates:\n        # update params if param_arrays and grad_arrays are not empty\n        if dev_updates:\n            i, w, g = zip(*dev_updates)\n            updater(i, w, g)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _multiple_callbacks(callbacks, *args, **kwargs):\n    if isinstance(callbacks, list):\n        for cb in callbacks:\n            cb(*args, **kwargs)\n        return\n    if callbacks:\n        callbacks(*args, **kwargs)", "response": "Sends args and kwargs to any configured callbacks."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _train_multi_device(symbol, ctx, arg_names, param_names, aux_names,\n                        arg_params, aux_params,\n                        begin_epoch, end_epoch, epoch_size, optimizer,\n                        kvstore, update_on_kvstore,\n                        train_data, eval_data=None, eval_metric=None,\n                        epoch_end_callback=None, batch_end_callback=None,\n                        logger=None, work_load_list=None, monitor=None,\n                        eval_end_callback=None,\n                        eval_batch_end_callback=None, sym_gen=None):\n    \"\"\"Internal training function on multiple devices.\n    This function will also work for single device as well.\n\n    Parameters\n    ----------\n    symbol : Symbol\n        The network configuration.\n    ctx : list of Context\n        The training devices.\n    arg_names: list of str\n        Name of all arguments of the network.\n    param_names: list of str\n        Name of all trainable parameters of the network.\n    aux_names: list of str\n        Name of all auxiliary states of the network.\n    arg_params : dict of str to NDArray\n        Model parameter, dict of name to NDArray of net's weights.\n    aux_params : dict of str to NDArray\n        Model parameter, dict of name to NDArray of net's auxiliary states.\n    begin_epoch : int\n        The begining training epoch.\n    end_epoch : int\n        The end training epoch.\n    epoch_size : int, optional\n        Number of batches in a epoch. In default, it is set to\n        ``ceil(num_train_examples / batch_size)``.\n    optimizer : Optimizer\n        The optimization algorithm\n    train_data : DataIter\n        Training data iterator.\n    eval_data : DataIter\n        Validation data iterator.\n    eval_metric : EvalMetric\n        An evaluation function or a list of evaluation functions.\n    epoch_end_callback : callable(epoch, symbol, arg_params, aux_states)\n        A callback that is invoked at end of each epoch.\n        This can be used to checkpoint model each epoch.\n    batch_end_callback : callable(BatchEndParams)\n        A callback that is invoked at end of each batch.\n        This can be used to measure speed, get result from evaluation metric. etc.\n    kvstore : KVStore\n        The KVStore.\n    update_on_kvstore : bool\n        Whether or not perform weight updating on kvstore.\n    logger : logging logger\n        When not specified, default logger will be used.\n    work_load_list : list of float or int, optional\n        The list of work load for different devices,\n        in the same order as ``ctx``.\n    monitor : Monitor, optional\n        Monitor installed to executor,\n        for monitoring outputs, weights, and gradients for debugging.\n    Notes\n    -----\n    - This function will inplace update the NDArrays in `arg_params` and `aux_states`.\n    \"\"\"\n    if logger is None:\n        logger = logging\n    executor_manager = DataParallelExecutorManager(symbol=symbol,\n                                                   sym_gen=sym_gen,\n                                                   ctx=ctx,\n                                                   train_data=train_data,\n                                                   param_names=param_names,\n                                                   arg_names=arg_names,\n                                                   aux_names=aux_names,\n                                                   work_load_list=work_load_list,\n                                                   logger=logger)\n    if monitor:\n        executor_manager.install_monitor(monitor)\n\n    executor_manager.set_params(arg_params, aux_params)\n\n    if not update_on_kvstore:\n        updater = get_updater(optimizer)\n    else:\n        kvstore.set_optimizer(optimizer)\n\n    if kvstore:\n        _initialize_kvstore(kvstore=kvstore,\n                            param_arrays=executor_manager.param_arrays,\n                            arg_params=arg_params,\n                            param_names=executor_manager.param_names,\n                            update_on_kvstore=update_on_kvstore)\n\n    # Now start training\n    train_data.reset()\n    for epoch in range(begin_epoch, end_epoch):\n        # Training phase\n        tic = time.time()\n        eval_metric.reset()\n        nbatch = 0\n        # Iterate over training data.\n        while True:\n            do_reset = True\n            for data_batch in train_data:\n                executor_manager.load_data_batch(data_batch)\n\n                if monitor is not None:\n                    monitor.tic()\n\n                executor_manager.forward(is_train=True)\n                executor_manager.backward()\n\n                if update_on_kvstore:\n                    if 'nccl' in kvstore.type:\n                        _update_params_on_kvstore_nccl(executor_manager.param_arrays,\n                                                       executor_manager.grad_arrays,\n                                                       kvstore, executor_manager.param_names)\n                    else:\n                        _update_params_on_kvstore(executor_manager.param_arrays,\n                                                  executor_manager.grad_arrays,\n                                                  kvstore, executor_manager.param_names)\n                else:\n                    _update_params(executor_manager.param_arrays,\n                                   executor_manager.grad_arrays,\n                                   updater=updater,\n                                   num_device=len(ctx),\n                                   kvstore=kvstore,\n                                   param_names=executor_manager.param_names)\n\n                if monitor is not None:\n                    monitor.toc_print()\n\n                # evaluate at end, so we can lazy copy\n                executor_manager.update_metric(eval_metric, data_batch.label)\n\n                nbatch += 1\n                # batch callback (for print purpose)\n                if batch_end_callback is not None:\n                    batch_end_params = BatchEndParam(epoch=epoch,\n                                                     nbatch=nbatch,\n                                                     eval_metric=eval_metric,\n                                                     locals=locals())\n                    _multiple_callbacks(batch_end_callback, batch_end_params)\n\n                # this epoch is done possibly earlier\n                if epoch_size is not None and nbatch >= epoch_size:\n                    do_reset = False\n                    break\n\n            if do_reset:\n                logger.info('Epoch[%d] Resetting Data Iterator', epoch)\n                train_data.reset()\n\n            # this epoch is done\n            if epoch_size is None or nbatch >= epoch_size:\n                break\n\n        toc = time.time()\n        logger.info('Epoch[%d] Time cost=%.3f', epoch, (toc - tic))\n\n        if epoch_end_callback or epoch + 1 == end_epoch:\n            executor_manager.copy_to(arg_params, aux_params)\n\n        _multiple_callbacks(epoch_end_callback, epoch, symbol, arg_params, aux_params)\n\n        # evaluation\n        if eval_data:\n            eval_metric.reset()\n            eval_data.reset()\n            total_num_batch = 0\n            for i, eval_batch in enumerate(eval_data):\n                executor_manager.load_data_batch(eval_batch)\n                executor_manager.forward(is_train=False)\n                executor_manager.update_metric(eval_metric, eval_batch.label)\n                if eval_batch_end_callback is not None:\n                    batch_end_params = BatchEndParam(epoch=epoch,\n                                                     nbatch=i,\n                                                     eval_metric=eval_metric,\n                                                     locals=locals())\n                    _multiple_callbacks(eval_batch_end_callback, batch_end_params)\n                total_num_batch += 1\n            if eval_end_callback is not None:\n                eval_end_params = BatchEndParam(epoch=epoch,\n                                                nbatch=total_num_batch,\n                                                eval_metric=eval_metric,\n                                                locals=locals())\n                _multiple_callbacks(eval_end_callback, eval_end_params)\n            eval_data.reset()", "response": "Internal training function for multi - device training."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef save_checkpoint(prefix, epoch, symbol, arg_params, aux_params):\n    if symbol is not None:\n        symbol.save('%s-symbol.json' % prefix)\n\n    save_dict = {('arg:%s' % k) : v.as_in_context(cpu()) for k, v in arg_params.items()}\n    save_dict.update({('aux:%s' % k) : v.as_in_context(cpu()) for k, v in aux_params.items()})\n    param_name = '%s-%04d.params' % (prefix, epoch)\n    nd.save(param_name, save_dict)\n    logging.info('Saved checkpoint to \\\"%s\\\"', param_name)", "response": "Save the model data into file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading the model checkpoint from file.", "response": "def load_checkpoint(prefix, epoch):\n    \"\"\"Load model checkpoint from file.\n\n    Parameters\n    ----------\n    prefix : str\n        Prefix of model name.\n    epoch : int\n        Epoch number of model we would like to load.\n\n    Returns\n    -------\n    symbol : Symbol\n        The symbol configuration of computation network.\n    arg_params : dict of str to NDArray\n        Model parameter, dict of name to NDArray of net's weights.\n    aux_params : dict of str to NDArray\n        Model parameter, dict of name to NDArray of net's auxiliary states.\n\n    Notes\n    -----\n    - Symbol will be loaded from ``prefix-symbol.json``.\n    - Parameters will be loaded from ``prefix-epoch.params``.\n    \"\"\"\n    symbol = sym.load('%s-symbol.json' % prefix)\n    save_dict = nd.load('%s-%04d.params' % (prefix, epoch))\n    arg_params = {}\n    aux_params = {}\n    for k, v in save_dict.items():\n        tp, name = k.split(':', 1)\n        if tp == 'arg':\n            arg_params[name] = v\n        if tp == 'aux':\n            aux_params[name] = v\n    return (symbol, arg_params, aux_params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _check_arguments(self):\n        if self.argument_checked:\n            return\n\n        assert(self.symbol is not None)\n        self.argument_checked = True\n\n        # check if symbol contain duplicated names.\n        _check_arguments(self.symbol)\n        # rematch parameters to delete useless ones\n        if self.allow_extra_params:\n            if self.arg_params:\n                arg_names = set(self.symbol.list_arguments())\n                self.arg_params = {k : v for k, v in self.arg_params.items()\n                                   if k in arg_names}\n            if self.aux_params:\n                aux_names = set(self.symbol.list_auxiliary_states())\n                self.aux_params = {k : v for k, v in self.aux_params.items()\n                                   if k in aux_names}", "response": "verify the argument of the default symbol and user provided parameters"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _init_params(self, inputs, overwrite=False):\n        inputs = [x if isinstance(x, DataDesc) else DataDesc(*x) for x in inputs]\n        input_shapes = {item.name: item.shape for item in inputs}\n        arg_shapes, _, aux_shapes = self.symbol.infer_shape(**input_shapes)\n        assert arg_shapes is not None\n        input_dtypes = {item.name: item.dtype for item in inputs}\n        arg_dtypes, _, aux_dtypes = self.symbol.infer_type(**input_dtypes)\n        assert arg_dtypes is not None\n\n        arg_names = self.symbol.list_arguments()\n        input_names = input_shapes.keys()\n        param_names = [key for key in arg_names if key not in input_names]\n        aux_names = self.symbol.list_auxiliary_states()\n\n        param_name_attrs = [x for x in zip(arg_names, arg_shapes, arg_dtypes)\n                            if x[0] in param_names]\n        arg_params = {k : nd.zeros(shape=s, dtype=t)\n                      for k, s, t in param_name_attrs}\n        aux_name_attrs = [x for x in zip(aux_names, aux_shapes, aux_dtypes)\n                          if x[0] in aux_names]\n        aux_params = {k : nd.zeros(shape=s, dtype=t)\n                      for k, s, t in aux_name_attrs}\n\n        for k, v in arg_params.items():\n            if self.arg_params and k in self.arg_params and (not overwrite):\n                arg_params[k][:] = self.arg_params[k][:]\n            else:\n                self.initializer(k, v)\n\n        for k, v in aux_params.items():\n            if self.aux_params and k in self.aux_params and (not overwrite):\n                aux_params[k][:] = self.aux_params[k][:]\n            else:\n                self.initializer(k, v)\n\n        self.arg_params = arg_params\n        self.aux_params = aux_params\n        return (arg_names, list(param_names), aux_names)", "response": "Initialize weight parameters and auxiliary states."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninitializing the predictor module for running prediction.", "response": "def _init_predictor(self, input_shapes, type_dict=None):\n        \"\"\"Initialize the predictor module for running prediction.\"\"\"\n        shapes = {name: self.arg_params[name].shape for name in self.arg_params}\n        shapes.update(dict(input_shapes))\n        if self._pred_exec is not None:\n            arg_shapes, _, _ = self.symbol.infer_shape(**shapes)\n            assert arg_shapes is not None, \"Incomplete input shapes\"\n            pred_shapes = [x.shape for x in self._pred_exec.arg_arrays]\n            if arg_shapes == pred_shapes:\n                return\n        # for now only use the first device\n        pred_exec = self.symbol.simple_bind(\n            self.ctx[0], grad_req='null', type_dict=type_dict, **shapes)\n        pred_exec.copy_params_from(self.arg_params, self.aux_params)\n\n        _check_arguments(self.symbol)\n        self._pred_exec = pred_exec"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninitializes the iterator given input.", "response": "def _init_iter(self, X, y, is_train):\n        \"\"\"Initialize the iterator given input.\"\"\"\n        if isinstance(X, (np.ndarray, nd.NDArray)):\n            if y is None:\n                if is_train:\n                    raise ValueError('y must be specified when X is numpy.ndarray')\n                else:\n                    y = np.zeros(X.shape[0])\n            if not isinstance(y, (np.ndarray, nd.NDArray)):\n                raise TypeError('y must be ndarray when X is numpy.ndarray')\n            if X.shape[0] != y.shape[0]:\n                raise ValueError(\"The numbers of data points and labels not equal\")\n            if y.ndim == 2 and y.shape[1] == 1:\n                y = y.flatten()\n            if y.ndim != 1:\n                raise ValueError(\"Label must be 1D or 2D (with 2nd dimension being 1)\")\n            if is_train:\n                return io.NDArrayIter(X, y, min(X.shape[0], self.numpy_batch_size),\n                                      shuffle=is_train, last_batch_handle='roll_over')\n            else:\n                return io.NDArrayIter(X, y, min(X.shape[0], self.numpy_batch_size), shuffle=False)\n        if not isinstance(X, io.DataIter):\n            raise TypeError('X must be DataIter, NDArray or numpy.ndarray')\n        return X"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninitializes the iterator given eval_data.", "response": "def _init_eval_iter(self, eval_data):\n        \"\"\"Initialize the iterator given eval_data.\"\"\"\n        if eval_data is None:\n            return eval_data\n        if isinstance(eval_data, (tuple, list)) and len(eval_data) == 2:\n            if eval_data[0] is not None:\n                if eval_data[1] is None and isinstance(eval_data[0], io.DataIter):\n                    return eval_data[0]\n                input_data = (np.array(eval_data[0]) if isinstance(eval_data[0], list)\n                              else eval_data[0])\n                input_label = (np.array(eval_data[1]) if isinstance(eval_data[1], list)\n                               else eval_data[1])\n                return self._init_iter(input_data, input_label, is_train=True)\n            else:\n                raise ValueError(\"Eval data is NONE\")\n        if not isinstance(eval_data, io.DataIter):\n            raise TypeError('Eval data must be DataIter, or ' \\\n                            'NDArray/numpy.ndarray/list pair (i.e. tuple/list of length 2)')\n        return eval_data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef predict(self, X, num_batch=None, return_data=False, reset=True):\n        X = self._init_iter(X, None, is_train=False)\n\n        if reset:\n            X.reset()\n        data_shapes = X.provide_data\n        data_names = [x[0] for x in data_shapes]\n        type_dict = dict((key, value.dtype) for (key, value) in self.arg_params.items())\n        for x in X.provide_data:\n            if isinstance(x, DataDesc):\n                type_dict[x.name] = x.dtype\n            else:\n                type_dict[x[0]] = mx_real_t\n\n        self._init_predictor(data_shapes, type_dict)\n        batch_size = X.batch_size\n        data_arrays = [self._pred_exec.arg_dict[name] for name in data_names]\n        output_list = [[] for _ in range(len(self._pred_exec.outputs))]\n        if return_data:\n            data_list = [[] for _ in X.provide_data]\n            label_list = [[] for _ in X.provide_label]\n\n        i = 0\n        for batch in X:\n\n            _load_data(batch, data_arrays)\n            self._pred_exec.forward(is_train=False)\n            padded = batch.pad\n            real_size = batch_size - padded\n\n            for o_list, o_nd in zip(output_list, self._pred_exec.outputs):\n                o_list.append(o_nd[0:real_size].asnumpy())\n\n            if return_data:\n                for j, x in enumerate(batch.data):\n                    data_list[j].append(x[0:real_size].asnumpy())\n                for j, x in enumerate(batch.label):\n                    label_list[j].append(x[0:real_size].asnumpy())\n            i += 1\n            if num_batch is not None and i == num_batch:\n                break\n\n        outputs = [np.concatenate(x) for x in output_list]\n        if len(outputs) == 1:\n            outputs = outputs[0]\n\n        if return_data:\n            data = [np.concatenate(x) for x in data_list]\n            label = [np.concatenate(x) for x in label_list]\n            if len(data) == 1:\n                data = data[0]\n            if len(label) == 1:\n                label = label[0]\n            return outputs, data, label\n        else:\n            return outputs", "response": "Run the prediction on the specified set of data."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef score(self, X, eval_metric='acc', num_batch=None, batch_end_callback=None, reset=True):\n        # setup metric\n        if not isinstance(eval_metric, metric.EvalMetric):\n            eval_metric = metric.create(eval_metric)\n\n        X = self._init_iter(X, None, is_train=False)\n        if reset:\n            X.reset()\n\n        data_shapes = X.provide_data\n        data_names = [x[0] for x in data_shapes]\n        type_dict = dict((key, value.dtype) for (key, value) in self.arg_params.items())\n        for x in X.provide_data:\n            if isinstance(x, DataDesc):\n                type_dict[x.name] = x.dtype\n            else:\n                type_dict[x[0]] = mx_real_t\n\n        self._init_predictor(data_shapes, type_dict)\n        data_arrays = [self._pred_exec.arg_dict[name] for name in data_names]\n\n        for i, batch in enumerate(X):\n            if num_batch is not None and i == num_batch:\n                break\n            _load_data(batch, data_arrays)\n            self._pred_exec.forward(is_train=False)\n            eval_metric.update(batch.label, self._pred_exec.outputs)\n\n            if batch_end_callback is not None:\n                batch_end_params = BatchEndParam(epoch=0,\n                                                 nbatch=i,\n                                                 eval_metric=eval_metric,\n                                                 locals=locals())\n                _multiple_callbacks(batch_end_callback, batch_end_params)\n        return eval_metric.get()[1]", "response": "Run the model given an input and calculate the score."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfit the model to the symbolic graph.", "response": "def fit(self, X, y=None, eval_data=None, eval_metric='acc',\n            epoch_end_callback=None, batch_end_callback=None, kvstore='local', logger=None,\n            work_load_list=None, monitor=None, eval_end_callback=LogValidationMetricsCallback(),\n            eval_batch_end_callback=None):\n        \"\"\"Fit the model.\n\n        Parameters\n        ----------\n        X : DataIter, or numpy.ndarray/NDArray\n            Training data. If `X` is a `DataIter`, the name or (if name not available)\n            the position of its outputs should match the corresponding variable\n            names defined in the symbolic graph.\n        y : numpy.ndarray/NDArray, optional\n            Training set label.\n            If X is ``numpy.ndarray`` or `NDArray`, `y` is required to be set.\n            While y can be 1D or 2D (with 2nd dimension as 1), its first dimension must be\n            the same as `X`, i.e. the number of data points and labels should be equal.\n        eval_data : DataIter or numpy.ndarray/list/NDArray pair\n            If eval_data is numpy.ndarray/list/NDArray pair,\n            it should be ``(valid_data, valid_label)``.\n        eval_metric : metric.EvalMetric or str or callable\n            The evaluation metric. This could be the name of evaluation metric\n            or a custom evaluation function that returns statistics\n            based on a minibatch.\n        epoch_end_callback : callable(epoch, symbol, arg_params, aux_states)\n            A callback that is invoked at end of each epoch.\n            This can be used to checkpoint model each epoch.\n        batch_end_callback: callable(epoch)\n            A callback that is invoked at end of each batch for purposes of printing.\n        kvstore: KVStore or str, optional\n           The KVStore or a string kvstore type: 'local', 'dist_sync', 'dist_async'\n           In default uses 'local', often no need to change for single machiine.\n        logger : logging logger, optional\n            When not specified, default logger will be used.\n        work_load_list : float or int, optional\n            The list of work load for different devices,\n            in the same order as `ctx`.\n\n        Note\n        ----\n        KVStore behavior\n        - 'local', multi-devices on a single machine, will automatically choose best type.\n        - 'dist_sync', multiple machines communicating via BSP.\n        - 'dist_async', multiple machines with asynchronous communication.\n        \"\"\"\n\n        data = self._init_iter(X, y, is_train=True)\n        eval_data = self._init_eval_iter(eval_data)\n\n        if self.sym_gen:\n            self.symbol = self.sym_gen(data.default_bucket_key) # pylint: disable=no-member\n            self._check_arguments()\n        self.kwargs[\"sym\"] = self.symbol\n\n        arg_names, param_names, aux_names = \\\n                self._init_params(data.provide_data+data.provide_label)\n\n        # setup metric\n        if not isinstance(eval_metric, metric.EvalMetric):\n            eval_metric = metric.create(eval_metric)\n\n        # create kvstore\n        (kvstore, update_on_kvstore) = _create_kvstore(\n            kvstore, len(self.ctx), self.arg_params)\n\n        param_idx2name = {}\n        if update_on_kvstore:\n            param_idx2name.update(enumerate(param_names))\n        else:\n            for i, n in enumerate(param_names):\n                for k in range(len(self.ctx)):\n                    param_idx2name[i*len(self.ctx)+k] = n\n        self.kwargs[\"param_idx2name\"] = param_idx2name\n\n        # init optmizer\n        if isinstance(self.optimizer, str):\n            batch_size = data.batch_size\n            if kvstore and 'dist' in kvstore.type and '_async' not in kvstore.type:\n                batch_size *= kvstore.num_workers\n            optimizer = opt.create(self.optimizer,\n                                   rescale_grad=(1.0/batch_size),\n                                   **(self.kwargs))\n        elif isinstance(self.optimizer, opt.Optimizer):\n            if not optimizer.idx2name:\n                optimizer.idx2name = param_idx2name.copy()\n            optimizer = self.optimizer\n\n        # do training\n        _train_multi_device(self.symbol, self.ctx, arg_names, param_names, aux_names,\n                            self.arg_params, self.aux_params,\n                            begin_epoch=self.begin_epoch, end_epoch=self.num_epoch,\n                            epoch_size=self.epoch_size,\n                            optimizer=optimizer,\n                            train_data=data, eval_data=eval_data,\n                            eval_metric=eval_metric,\n                            epoch_end_callback=epoch_end_callback,\n                            batch_end_callback=batch_end_callback,\n                            kvstore=kvstore, update_on_kvstore=update_on_kvstore,\n                            logger=logger, work_load_list=work_load_list, monitor=monitor,\n                            eval_end_callback=eval_end_callback,\n                            eval_batch_end_callback=eval_batch_end_callback,\n                            sym_gen=self.sym_gen)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef save(self, prefix, epoch=None):\n        if epoch is None:\n            epoch = self.num_epoch\n        assert epoch is not None\n        save_checkpoint(prefix, epoch, self.symbol, self.arg_params, self.aux_params)", "response": "Save the current state of the object into file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load(prefix, epoch, ctx=None, **kwargs):\n        symbol, arg_params, aux_params = load_checkpoint(prefix, epoch)\n        return FeedForward(symbol, ctx=ctx,\n                           arg_params=arg_params, aux_params=aux_params,\n                           begin_epoch=epoch,\n                           **kwargs)", "response": "Load a feedforward model checkpoint from file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build_save_containers(platforms, registry, load_cache) -> int:\n    from joblib import Parallel, delayed\n    if len(platforms) == 0:\n        return 0\n\n    platform_results = Parallel(n_jobs=PARALLEL_BUILDS, backend=\"multiprocessing\")(\n        delayed(_build_save_container)(platform, registry, load_cache)\n        for platform in platforms)\n\n    is_error = False\n    for platform_result in platform_results:\n        if platform_result is not None:\n            logging.error('Failed to generate %s', platform_result)\n            is_error = True\n\n    return 1 if is_error else 0", "response": "Build and upload all built dockerimages in parallel"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _build_save_container(platform, registry, load_cache) -> Optional[str]:\n    docker_tag = build_util.get_docker_tag(platform=platform, registry=registry)\n\n    # Preload cache\n    if load_cache:\n        load_docker_cache(registry=registry, docker_tag=docker_tag)\n\n    # Start building\n    logging.debug('Building %s as %s', platform, docker_tag)\n    try:\n        # Increase the number of retries for building the cache.\n        image_id = build_util.build_docker(docker_binary='docker', platform=platform, registry=registry, num_retries=10, no_cache=False)\n        logging.info('Built %s as %s', docker_tag, image_id)\n\n        # Push cache to registry\n        _upload_image(registry=registry, docker_tag=docker_tag, image_id=image_id)\n        return None\n    except Exception:\n        logging.exception('Unexpected exception during build of %s', docker_tag)\n        return platform", "response": "Build image for passed platform and upload the cache to S3 bucket\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nuploading the image to S3 bucket and push it to S3 bucket", "response": "def _upload_image(registry, docker_tag, image_id) -> None:\n    \"\"\"\n    Upload the passed image by id, tag it with docker tag and upload to S3 bucket\n    :param registry: Docker registry name\n    :param docker_tag: Docker tag\n    :param image_id: Image id\n    :return: None\n    \"\"\"\n    # We don't have to retag the image since it is already in the right format\n    logging.info('Uploading %s (%s) to %s', docker_tag, image_id, registry)\n    push_cmd = ['docker', 'push', docker_tag]\n    subprocess.check_call(push_cmd)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _login_dockerhub():\n    dockerhub_credentials = _get_dockerhub_credentials()\n\n    logging.info('Logging in to DockerHub')\n    # We use password-stdin instead of --password to avoid leaking passwords in case of an error.\n    # This method will produce the following output:\n    # > WARNING! Your password will be stored unencrypted in /home/jenkins_slave/.docker/config.json.\n    # > Configure a credential helper to remove this warning. See\n    # > https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n    # Since we consider the restricted slaves a secure environment, that's fine. Also, using this will require\n    # third party applications which would need a review first as well.\n    p = subprocess.run(['docker', 'login', '--username', dockerhub_credentials['username'], '--password-stdin'],\n                       stdout=subprocess.PIPE, input=str.encode(dockerhub_credentials['password']))\n    logging.info(p.stdout)\n    logging.info('Successfully logged in to DockerHub')", "response": "Login to the Docker Hub account"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload the precompiled docker cache from the registry", "response": "def load_docker_cache(registry, docker_tag) -> None:\n    \"\"\"\n    Load the precompiled docker cache from the registry\n    :param registry: Docker registry name\n    :param docker_tag: Docker tag to load\n    :return: None\n    \"\"\"\n    # We don't have to retag the image since it's already in the right format\n    if not registry:\n        return\n    assert docker_tag\n\n    logging.info('Loading Docker cache for %s from %s', docker_tag, registry)\n    pull_cmd = ['docker', 'pull', docker_tag]\n\n    # Don't throw an error if the image does not exist\n    subprocess.run(pull_cmd, timeout=DOCKER_CACHE_TIMEOUT_MINS*60)\n    logging.info('Successfully pulled docker cache')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndelete the local docker cache for the entire docker image chain", "response": "def delete_local_docker_cache(docker_tag):\n    \"\"\"\n    Delete the local docker cache for the entire docker image chain\n    :param docker_tag: Docker tag\n    :return: None\n    \"\"\"\n    history_cmd = ['docker', 'history', '-q', docker_tag]\n\n    try:\n        image_ids_b = subprocess.check_output(history_cmd)\n        image_ids_str = image_ids_b.decode('utf-8').strip()\n        layer_ids = [id.strip() for id in image_ids_str.split('\\n') if id != '<missing>']\n\n        delete_cmd = ['docker', 'image', 'rm', '--force']\n        delete_cmd.extend(layer_ids)\n        subprocess.check_call(delete_cmd)\n    except subprocess.CalledProcessError as error:\n        # Could be caused by the image not being present\n        logging.debug('Error during local cache deletion %s', error)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_chinese_text():\n    if not os.path.isdir(\"data/\"):\n        os.system(\"mkdir data/\")\n    if (not os.path.exists('data/pos.txt')) or \\\n       (not os.path.exists('data/neg')):\n        os.system(\"wget -q https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/example/chinese_text.zip \"\n                  \"-P data/\")\n        os.chdir(\"./data\")\n        os.system(\"unzip -u chinese_text.zip\")\n        os.chdir(\"..\")", "response": "Download the chinese_text dataset and unzip it"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload MR polarity data from files splits the data into words and generates labels.", "response": "def load_data_and_labels():\n    \"\"\"Loads MR polarity data from files, splits the data into words and generates labels.\n    Returns split sentences and labels.\n    \"\"\"\n    # download dataset\n    get_chinese_text()\n\n    # Load data from files\n    positive_examples = list(codecs.open(\"./data/pos.txt\", \"r\", \"utf-8\").readlines())\n    positive_examples = [s.strip() for s in positive_examples]\n    positive_examples = [pe for pe in positive_examples if len(pe) < 100]\n    negative_examples = list(codecs.open(\"./data/neg.txt\", \"r\", \"utf-8\").readlines())\n    negative_examples = [s.strip() for s in negative_examples]\n    negative_examples = [ne for ne in negative_examples if len(ne) < 100]\n    # Split by words\n    x_text = positive_examples + negative_examples\n    # x_text = [clean_str(sent) for sent in x_text]\n    x_text = [list(s) for s in x_text]\n\n    # Generate labels\n    positive_labels = [[0, 1] for _ in positive_examples]\n    negative_labels = [[1, 0] for _ in negative_examples]\n    y = np.concatenate([positive_labels, negative_labels], 0)\n    return [x_text, y]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef reset(self):\n        if getattr(self, 'num', None) is None:\n            self.num_inst = 0\n            self.sum_metric = 0.0\n        else:\n            self.num_inst = [0] * self.num\n            self.sum_metric = [0.0] * self.num", "response": "reset the internal state of the object to 0"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreset the local state of the object", "response": "def reset_local(self):\n        \"\"\"\n        override reset behavior\n        \"\"\"\n        if getattr(self, 'num', None) is None:\n            self.num_inst = 0\n            self.sum_metric = 0.0\n        else:\n            self.num_inst = [0] * self.num\n            self.sum_metric = [0.0] * self.num"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update(self, labels, preds):\n        # get generated multi label from network\n        cls_prob = preds[0].asnumpy()\n        loc_loss = preds[1].asnumpy()\n        cls_label = preds[2].asnumpy()\n        valid_count = np.sum(cls_label >= 0)\n        # overall accuracy & object accuracy\n        label = cls_label.flatten()\n        mask = np.where(label >= 0)[0]\n        indices = np.int64(label[mask])\n        prob = cls_prob.transpose((0, 2, 1)).reshape((-1, cls_prob.shape[1]))\n        prob = prob[mask, indices]\n        self.sum_metric[0] += (-np.log(prob + self.eps)).sum()\n        self.num_inst[0] += valid_count\n        # smoothl1loss\n        self.sum_metric[1] += np.sum(loc_loss)\n        self.num_inst[1] += valid_count", "response": "Implementation of updating metrics\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get(self):\n        if self.num is None:\n            if self.num_inst == 0:\n                return (self.name, float('nan'))\n            else:\n                return (self.name, self.sum_metric / self.num_inst)\n        else:\n            names = ['%s'%(self.name[i]) for i in range(self.num)]\n            values = [x / y if y != 0 else float('nan') \\\n                for x, y in zip(self.sum_metric, self.num_inst)]\n            return (names, values)", "response": "Get the current evaluation result."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dqn_sym_nips(action_num, data=None, name='dqn'):\n    if data is None:\n        net = mx.symbol.Variable('data')\n    else:\n        net = data\n    net = mx.symbol.Convolution(data=net, name='conv1', kernel=(8, 8), stride=(4, 4), num_filter=16)\n    net = mx.symbol.Activation(data=net, name='relu1', act_type=\"relu\")\n    net = mx.symbol.Convolution(data=net, name='conv2', kernel=(4, 4), stride=(2, 2), num_filter=32)\n    net = mx.symbol.Activation(data=net, name='relu2', act_type=\"relu\")\n    net = mx.symbol.Flatten(data=net)\n    net = mx.symbol.FullyConnected(data=net, name='fc3', num_hidden=256)\n    net = mx.symbol.Activation(data=net, name='relu3', act_type=\"relu\")\n    net = mx.symbol.FullyConnected(data=net, name='fc4', num_hidden=action_num)\n    net = mx.symbol.Custom(data=net, name=name, op_type='DQNOutput')\n    return net", "response": "Structure of the Deep Q Network in the NIPS 2013 workshop paper"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _monitor_callback_wrapper(callback):\n    def callback_handle(name, array, _):\n        \"\"\" ctypes function \"\"\"\n        callback(name, array)\n    return callback_handle", "response": "A wrapper for the user - defined callback function."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the dictionary given name and ndarray pairs.", "response": "def _get_dict(names, ndarrays):\n        \"\"\"Get the dictionary given name and ndarray pairs.\"\"\"\n        nset = set()\n        for nm in names:\n            if nm in nset:\n                raise ValueError('Duplicate names detected, %s' % str(names))\n            nset.add(nm)\n        return dict(zip(names, ndarrays))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_outputs(self):\n        out_size = mx_uint()\n        handles = ctypes.POINTER(NDArrayHandle)()\n        check_call(_LIB.MXExecutorOutputs(self.handle,\n                                          ctypes.byref(out_size), ctypes.byref(handles)))\n        num_output = out_size.value\n        outputs = [_ndarray_cls(NDArrayHandle(handles[i])) for i in range(num_output)]\n        return outputs", "response": "List all the output NDArray.\n        Returns -------\n        A list of ndarray bound to the heads of executor.\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the outputs specified by the bound symbol.", "response": "def forward(self, is_train=False, **kwargs):\n        \"\"\"Calculate the outputs specified by the bound symbol.\n\n        Parameters\n        ----------\n        is_train: bool, optional\n            Whether this forward is for evaluation purpose. If True,\n            a backward call is expected to follow.\n\n        **kwargs\n            Additional specification of input arguments.\n\n        Examples\n        --------\n        >>> # doing forward by specifying data\n        >>> texec.forward(is_train=True, data=mydata)\n        >>> # doing forward by not specifying things, but copy to the executor before hand\n        >>> mydata.copyto(texec.arg_dict['data'])\n        >>> texec.forward(is_train=True)\n        >>> # doing forward by specifying data and get outputs\n        >>> outputs = texec.forward(is_train=True, data=mydata)\n        >>> print(outputs[0].asnumpy())\n        \"\"\"\n        if len(kwargs) != 0:\n            arg_dict = self.arg_dict\n            for name, array in kwargs.items():\n                if not isinstance(array, (NDArray, np.ndarray)):\n                    raise ValueError('only accept keyword argument of NDArrays and numpy.ndarray')\n                if name not in arg_dict:\n                    raise TypeError('Unknown argument %s' % name)\n                if arg_dict[name].shape != array.shape:\n                    raise ValueError('Shape not match! Argument %s, need: %s, received: %s'\n                                     %(name, str(arg_dict[name].shape), str(array.shape)))\n                arg_dict[name][:] = array\n\n        check_call(_LIB.MXExecutorForward(\n            self.handle,\n            ctypes.c_int(int(is_train))))\n\n        return self.outputs"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndoes backward pass to get the gradient of arguments. Parameters ---------- out_grads : NDArray or list of NDArray or dict of str to NDArray, optional Gradient on the outputs to be propagated back. This parameter is only needed when bind is called on outputs that are not a loss function. is_train : bool, default True Whether this backward is for training or inference. Note that in rare cases you want to call backward with is_train=False to get gradient during inference. Examples -------- >>> # Example for binding on loss function symbol, which gives the loss value of the model. >>> # Equivalently it gives the head gradient for backward pass. >>> # In this example the built-in SoftmaxOutput is used as loss function. >>> # MakeLoss can be used to define customized loss function symbol. >>> net = mx.sym.Variable('data') >>> net = mx.sym.FullyConnected(net, name='fc', num_hidden=6) >>> net = mx.sym.Activation(net, name='relu', act_type=\"relu\") >>> net = mx.sym.SoftmaxOutput(net, name='softmax') >>> args = {'data': mx.nd.ones((1, 4)), 'fc_weight': mx.nd.ones((6, 4)), >>> 'fc_bias': mx.nd.array((1, 4, 4, 4, 5, 6)), 'softmax_label': mx.nd.ones((1))} >>> args_grad = {'fc_weight': mx.nd.zeros((6, 4)), 'fc_bias': mx.nd.zeros((6))} >>> texec = net.bind(ctx=mx.cpu(), args=args, args_grad=args_grad) >>> out = texec.forward(is_train=True)[0].copy() >>> print out.asnumpy() [[ 0.00378404 0.07600445 0.07600445 0.07600445 0.20660152 0.5616011 ]] >>> texec.backward() >>> print(texec.grad_arrays[1].asnumpy()) [[ 0.00378404 0.00378404 0.00378404 0.00378404] [-0.92399555 -0.92399555 -0.92399555 -0.92399555] [ 0.07600445 0.07600445 0.07600445 0.07600445] [ 0.07600445 0.07600445 0.07600445 0.07600445] [ 0.20660152 0.20660152 0.20660152 0.20660152] [ 0.5616011 0.5616011 0.5616011 0.5616011 ]] >>> >>> # Example for binding on non-loss function symbol. >>> # Here the binding symbol is neither built-in loss function >>> # nor customized loss created by MakeLoss. >>> # As a result the head gradient is not automatically provided. >>> a = mx.sym.Variable('a') >>> b = mx.sym.Variable('b') >>> # c is not a loss function symbol >>> c = 2 * a + b >>> args = {'a': mx.nd.array([1,2]), 'b':mx.nd.array([2,3])} >>> args_grad = {'a': mx.nd.zeros((2)), 'b': mx.nd.zeros((2))} >>> texec = c.bind(ctx=mx.cpu(), args=args, args_grad=args_grad) >>> out = texec.forward(is_train=True)[0].copy() >>> print(out.asnumpy()) [ 4. 7.] >>> # out_grads is the head gradient in backward pass. >>> # Here we define 'c' as loss function. >>> # Then 'out' is passed as head gradient of backward pass. >>> texec.backward(out) >>> print(texec.grad_arrays[0].asnumpy()) [ 8. 14.] >>> print(texec.grad_arrays[1].asnumpy()) [ 4. 7.]", "response": "def backward(self, out_grads=None, is_train=True):\n        \"\"\"Do backward pass to get the gradient of arguments.\n\n        Parameters\n        ----------\n        out_grads : NDArray or list of NDArray or dict of str to NDArray, optional\n            Gradient on the outputs to be propagated back.\n            This parameter is only needed when bind is called\n            on outputs that are not a loss function.\n        is_train : bool, default True\n            Whether this backward is for training or inference. Note that in rare\n            cases you want to call backward with is_train=False to get gradient\n            during inference.\n\n\n        Examples\n        --------\n        >>> # Example for binding on loss function symbol, which gives the loss value of the model.\n        >>> # Equivalently it gives the head gradient for backward pass.\n        >>> # In this example the built-in SoftmaxOutput is used as loss function.\n        >>> # MakeLoss can be used to define customized loss function symbol.\n        >>> net = mx.sym.Variable('data')\n        >>> net = mx.sym.FullyConnected(net, name='fc', num_hidden=6)\n        >>> net = mx.sym.Activation(net, name='relu', act_type=\"relu\")\n        >>> net = mx.sym.SoftmaxOutput(net, name='softmax')\n\n        >>> args =  {'data': mx.nd.ones((1, 4)), 'fc_weight': mx.nd.ones((6, 4)),\n        >>>          'fc_bias': mx.nd.array((1, 4, 4, 4, 5, 6)), 'softmax_label': mx.nd.ones((1))}\n        >>> args_grad = {'fc_weight': mx.nd.zeros((6, 4)), 'fc_bias': mx.nd.zeros((6))}\n        >>> texec = net.bind(ctx=mx.cpu(), args=args, args_grad=args_grad)\n        >>> out = texec.forward(is_train=True)[0].copy()\n        >>> print out.asnumpy()\n        [[ 0.00378404  0.07600445  0.07600445  0.07600445  0.20660152  0.5616011 ]]\n        >>> texec.backward()\n        >>> print(texec.grad_arrays[1].asnumpy())\n        [[ 0.00378404  0.00378404  0.00378404  0.00378404]\n         [-0.92399555 -0.92399555 -0.92399555 -0.92399555]\n         [ 0.07600445  0.07600445  0.07600445  0.07600445]\n         [ 0.07600445  0.07600445  0.07600445  0.07600445]\n         [ 0.20660152  0.20660152  0.20660152  0.20660152]\n         [ 0.5616011   0.5616011   0.5616011   0.5616011 ]]\n        >>>\n        >>> # Example for binding on non-loss function symbol.\n        >>> # Here the binding symbol is neither built-in loss function\n        >>> # nor customized loss created by MakeLoss.\n        >>> # As a result the head gradient is not automatically provided.\n        >>> a = mx.sym.Variable('a')\n        >>> b = mx.sym.Variable('b')\n        >>> # c is not a loss function symbol\n        >>> c = 2 * a + b\n        >>> args = {'a': mx.nd.array([1,2]), 'b':mx.nd.array([2,3])}\n        >>> args_grad = {'a': mx.nd.zeros((2)), 'b': mx.nd.zeros((2))}\n        >>> texec = c.bind(ctx=mx.cpu(), args=args, args_grad=args_grad)\n        >>> out = texec.forward(is_train=True)[0].copy()\n        >>> print(out.asnumpy())\n        [ 4.  7.]\n        >>> # out_grads is the head gradient in backward pass.\n        >>> # Here we define 'c' as loss function.\n        >>> # Then 'out' is passed as head gradient of backward pass.\n        >>> texec.backward(out)\n        >>> print(texec.grad_arrays[0].asnumpy())\n        [ 8.  14.]\n        >>> print(texec.grad_arrays[1].asnumpy())\n        [ 4.  7.]\n        \"\"\"\n        if out_grads is None:\n            out_grads = []\n        elif isinstance(out_grads, NDArray):\n            out_grads = [out_grads]\n        elif isinstance(out_grads, dict):\n            out_grads = [out_grads[k] for k in self._symbol.list_outputs()]\n\n        for obj in out_grads:\n            if not isinstance(obj, NDArray):\n                raise TypeError(\"inputs must be NDArray\")\n        ndarray = c_handle_array(out_grads)\n        check_call(_LIB.MXExecutorBackwardEx(\n            self.handle,\n            mx_uint(len(out_grads)),\n            ndarray,\n            ctypes.c_int(is_train)))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_monitor_callback(self, callback, monitor_all=False):\n        cb_type = ctypes.CFUNCTYPE(None, ctypes.c_char_p, NDArrayHandle, ctypes.c_void_p)\n        self._monitor_callback = cb_type(_monitor_callback_wrapper(callback))\n        check_call(_LIB.MXExecutorSetMonitorCallbackEX(\n            self.handle,\n            self._monitor_callback,\n            None,\n            ctypes.c_int(monitor_all)))", "response": "Install callback for monitor."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef arg_dict(self):\n        if self._arg_dict is None:\n            self._arg_dict = Executor._get_dict(\n                self._symbol.list_arguments(), self.arg_arrays)\n        return self._arg_dict", "response": "Get dictionary representation of arguments."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets dictionary representation of gradient arrays.", "response": "def grad_dict(self):\n        \"\"\"Get dictionary representation of gradient arrays.\n\n        Returns\n        -------\n        grad_dict : dict of str to NDArray\n            The dictionary that maps name of arguments to gradient arrays.\n        \"\"\"\n        if self._grad_dict is None:\n            self._grad_dict = Executor._get_dict(\n                self._symbol.list_arguments(), self.grad_arrays)\n        return self._grad_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets dictionary representation of auxiliary states arrays.", "response": "def aux_dict(self):\n        \"\"\"Get dictionary representation of auxiliary states arrays.\n\n        Returns\n        -------\n        aux_dict : dict of str to NDArray\n            The dictionary that maps name of auxiliary states to NDArrays.\n\n        Raises\n        ------\n        ValueError : if there are duplicated names in the auxiliary states.\n        \"\"\"\n        if self._aux_dict is None:\n            self._aux_dict = Executor._get_dict(\n                self._symbol.list_auxiliary_states(), self.aux_arrays)\n        return self._aux_dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef output_dict(self):\n        if self._output_dict is None:\n            self._output_dict = Executor._get_dict(\n                self._symbol.list_outputs(), self.outputs)\n        return self._output_dict", "response": "Get dictionary representation of output arrays."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef copy_params_from(self, arg_params, aux_params=None, allow_extra_params=False):\n        for name, array in arg_params.items():\n            if name in self.arg_dict:\n                dst = self.arg_dict[name]\n                array.astype(dst.dtype).copyto(dst)\n            elif not allow_extra_params:\n                raise ValueError('Find name \\\"%s\\\" that is not in the arguments' % name)\n\n        if aux_params is None:\n            return\n\n        for name, array in aux_params.items():\n            if name in self.aux_dict:\n                dst = self.aux_dict[name]\n                array.astype(dst.dtype).copyto(dst)\n            elif not allow_extra_params:\n                raise ValueError('Find name %s that is not in the auxiliary states' % name)", "response": "Copy parameters from arg_params aux_params into executor s internal array."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a new executor with the same symbol and shared memory but different input and output shapes.", "response": "def reshape(self, partial_shaping=False, allow_up_sizing=False, **kwargs):\n        \"\"\"Return a new executor with the same symbol and shared memory,\n        but different input/output shapes.\n        For runtime reshaping, variable length sequences, etc.\n        The returned executor shares state with the current one,\n        and cannot be used in parallel with it.\n\n        Parameters\n        ----------\n        partial_shaping : bool\n            Whether to allow changing the shape of unspecified arguments.\n        allow_up_sizing : bool\n            Whether to allow allocating new ndarrays that's larger than the original.\n        kwargs : dict of string to tuple of int\n            New shape for arguments.\n\n        Returns\n        -------\n        exec : Executor\n            A new executor that shares memory with self.\n\n        Examples\n        --------\n        >>> a = mx.sym.Variable('a')\n        >>> b = mx.sym.Variable('b')\n        >>> c = 2 * a + b\n        >>> texec = c.bind(mx.cpu(), {'a': mx.nd.zeros((2, 1)), 'b': mx.nd.ones((2,1))})\n        >>> new_shape = {'a': (4, 2), 'b': (4, 2)}\n        >>> texec.reshape(allow_up_sizing=True, **new_shape)\n        \"\"\"\n        # pylint: disable=too-many-branches\n        provided_arg_shape_data = []  # shape data\n        # argument shape index in sdata,\n        # e.g. [sdata[indptr[0]], sdata[indptr[1]]) is the shape of the first arg\n        provided_arg_shape_idx = [0]\n        provided_arg_shape_names = []  # provided argument names\n        for k, v in kwargs.items():\n            if isinstance(v, tuple):\n                provided_arg_shape_names.append(k)\n                provided_arg_shape_data.extend(v)\n                provided_arg_shape_idx.append(len(provided_arg_shape_data))\n\n        ctx_map_keys = []\n        ctx_map_dev_types = []\n        ctx_map_dev_ids = []\n\n        if self._group2ctx:\n            for key, val in self._group2ctx.items():\n                ctx_map_keys.append(key)\n                ctx_map_dev_types.append(val.device_typeid)\n                ctx_map_dev_ids.append(val.device_id)\n\n        handle = ExecutorHandle()\n        shared_handle = self.handle\n\n        num_in_args = ctypes.c_uint()\n        in_arg_handles = ctypes.POINTER(NDArrayHandle)()\n        arg_grad_handles = ctypes.POINTER(NDArrayHandle)()\n        num_aux_states = ctypes.c_uint()\n        aux_state_handles = ctypes.POINTER(NDArrayHandle)()\n\n        check_call(_LIB.MXExecutorReshapeEx(ctypes.c_int(int(partial_shaping)),\n                                            ctypes.c_int(int(allow_up_sizing)),\n                                            ctypes.c_int(self._ctx.device_typeid),\n                                            ctypes.c_int(self._ctx.device_id),\n                                            mx_uint(len(ctx_map_keys)),\n                                            c_str_array(ctx_map_keys),\n                                            c_array_buf(ctypes.c_int,\n                                                        py_array('i', ctx_map_dev_types)),\n                                            c_array_buf(ctypes.c_int,\n                                                        py_array('i', ctx_map_dev_ids)),\n                                            mx_uint(len(provided_arg_shape_names)),\n                                            c_str_array(provided_arg_shape_names),\n                                            c_array_buf(mx_int,\n                                                        py_array('i', provided_arg_shape_data)),\n                                            c_array_buf(mx_uint,\n                                                        py_array('I', provided_arg_shape_idx)),\n                                            ctypes.byref(num_in_args),\n                                            ctypes.byref(in_arg_handles),\n                                            ctypes.byref(arg_grad_handles),\n                                            ctypes.byref(num_aux_states),\n                                            ctypes.byref(aux_state_handles),\n                                            shared_handle,\n                                            ctypes.byref(handle)))\n\n        arg_arrays = [_ndarray_cls(NDArrayHandle(in_arg_handles[i]))\n                      for i in range(num_in_args.value)]\n        grad_arrays = [_ndarray_cls(NDArrayHandle(arg_grad_handles[i]))\n                       if arg_grad_handles[i] is not None\n                       else None for i in range(num_in_args.value)]\n        aux_arrays = [_ndarray_cls(NDArrayHandle(aux_state_handles[i]))\n                      for i in range(num_aux_states.value)]\n\n        executor = Executor(handle, self._symbol, self._ctx, self._grad_req, self._group2ctx)\n        executor.arg_arrays = arg_arrays\n        executor.grad_arrays = grad_arrays\n        executor.aux_arrays = aux_arrays\n        return executor"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a debug string of the executor.", "response": "def debug_str(self):\n        \"\"\"Get a debug string about internal execution plan.\n\n        Returns\n        -------\n        debug_str : string\n            Debug string of the executor.\n\n        Examples\n        --------\n        >>> a = mx.sym.Variable('a')\n        >>> b = mx.sym.sin(a)\n        >>> c = 2 * a + b\n        >>> texec = c.bind(mx.cpu(), {'a': mx.nd.array([1,2]), 'b':mx.nd.array([2,3])})\n        >>> print(texec.debug_str())\n        Symbol Outputs:\n\t            output[0]=_plus0(0)\n        Variable:a\n        --------------------\n        Op:_mul_scalar, Name=_mulscalar0\n        Inputs:\n\t        arg[0]=a(0) version=0\n        Attrs:\n\t        scalar=2\n        --------------------\n        Op:sin, Name=sin0\n        Inputs:\n\t        arg[0]=a(0) version=0\n        --------------------\n        Op:elemwise_add, Name=_plus0\n        Inputs:\n\t        arg[0]=_mulscalar0(0)\n\t        arg[1]=sin0(0)\n        Total 0 MB allocated\n        Total 11 TempSpace resource requested\n        \"\"\"\n        debug_str = ctypes.c_char_p()\n        check_call(_LIB.MXExecutorPrint(\n            self.handle, ctypes.byref(debug_str)))\n        return py_str(debug_str.value)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_voc_rec(filename):\n    import xml.etree.ElementTree as ET\n    tree = ET.parse(filename)\n    objects = []\n    for obj in tree.findall('object'):\n        obj_dict = dict()\n        obj_dict['name'] = obj.find('name').text\n        obj_dict['difficult'] = int(obj.find('difficult').text)\n        bbox = obj.find('bndbox')\n        obj_dict['bbox'] = [int(bbox.find('xmin').text),\n                            int(bbox.find('ymin').text),\n                            int(bbox.find('xmax').text),\n                            int(bbox.find('ymax').text)]\n        objects.append(obj_dict)\n    return objects", "response": "parse pascal voc record into a list of dicts"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef voc_eval(detpath, annopath, imageset_file, classname, cache_dir, ovthresh=0.5, use_07_metric=False):\n    if not os.path.isdir(cache_dir):\n        os.mkdir(cache_dir)\n    cache_file = os.path.join(cache_dir, 'annotations.pkl')\n    with open(imageset_file, 'r') as f:\n        lines = f.readlines()\n    image_filenames = [x.strip() for x in lines]\n\n    # load annotations from cache\n    if not os.path.isfile(cache_file):\n        recs = {}\n        for ind, image_filename in enumerate(image_filenames):\n            recs[image_filename] = parse_voc_rec(annopath.format(image_filename))\n            if ind % 100 == 0:\n                print('reading annotations for {:d}/{:d}'.format(ind + 1, len(image_filenames)))\n        print('saving annotations cache to {:s}'.format(cache_file))\n        with open(cache_file, 'wb') as f:\n            pickle.dump(recs, f)\n    else:\n        with open(cache_file, 'rb') as f:\n            recs = pickle.load(f)\n\n    # extract objects in :param classname:\n    class_recs = {}\n    npos = 0\n    for image_filename in image_filenames:\n        objects = [obj for obj in recs[image_filename] if obj['name'] == classname]\n        bbox = np.array([x['bbox'] for x in objects])\n        difficult = np.array([x['difficult'] for x in objects]).astype(np.bool)\n        det = [False] * len(objects)  # stand for detected\n        npos = npos + sum(~difficult)\n        class_recs[image_filename] = {'bbox': bbox,\n                                      'difficult': difficult,\n                                      'det': det}\n\n    # read detections\n    detfile = detpath.format(classname)\n    with open(detfile, 'r') as f:\n        lines = f.readlines()\n\n    splitlines = [x.strip().split(' ') for x in lines]\n    image_ids = [x[0] for x in splitlines]\n    confidence = np.array([float(x[1]) for x in splitlines])\n    bbox = np.array([[float(z) for z in x[2:]] for x in splitlines])\n\n    # sort by confidence\n    sorted_inds = np.argsort(-confidence)\n    sorted_scores = np.sort(-confidence)\n    bbox = bbox[sorted_inds, :]\n    image_ids = [image_ids[x] for x in sorted_inds]\n\n    # go down detections and mark true positives and false positives\n    nd = len(image_ids)\n    tp = np.zeros(nd)\n    fp = np.zeros(nd)\n    for d in range(nd):\n        r = class_recs[image_ids[d]]\n        bb = bbox[d, :].astype(float)\n        ovmax = -np.inf\n        bbgt = r['bbox'].astype(float)\n\n        if bbgt.size > 0:\n            # compute overlaps\n            # intersection\n            ixmin = np.maximum(bbgt[:, 0], bb[0])\n            iymin = np.maximum(bbgt[:, 1], bb[1])\n            ixmax = np.minimum(bbgt[:, 2], bb[2])\n            iymax = np.minimum(bbgt[:, 3], bb[3])\n            iw = np.maximum(ixmax - ixmin + 1., 0.)\n            ih = np.maximum(iymax - iymin + 1., 0.)\n            inters = iw * ih\n\n            # union\n            uni = ((bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) +\n                   (bbgt[:, 2] - bbgt[:, 0] + 1.) *\n                   (bbgt[:, 3] - bbgt[:, 1] + 1.) - inters)\n\n            overlaps = inters / uni\n            ovmax = np.max(overlaps)\n            jmax = np.argmax(overlaps)\n\n        if ovmax > ovthresh:\n            if not r['difficult'][jmax]:\n                if not r['det'][jmax]:\n                    tp[d] = 1.\n                    r['det'][jmax] = 1\n                else:\n                    fp[d] = 1.\n        else:\n            fp[d] = 1.\n\n    # compute precision recall\n    fp = np.cumsum(fp)\n    tp = np.cumsum(tp)\n    rec = tp / float(npos)\n    # avoid division by zero in case first detection matches a difficult ground ruth\n    prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n    ap = voc_ap(rec, prec, use_07_metric)\n\n    return rec, prec, ap", "response": "evaluates the voc of a given class"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef register(op_name):\n        def wrapper(func):\n            \"\"\"Helper function to map functions\"\"\"\n            try:\n                import onnx as _\n                MXNetGraph.registry_[op_name] = func\n            except ImportError:\n                pass\n            return func\n\n        return wrapper", "response": "Register a function to map to a specific attribute of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts MXNet layer to ONNX layer.", "response": "def convert_layer(node, **kwargs):\n        \"\"\"Convert MXNet layer to ONNX\"\"\"\n        op = str(node[\"op\"])\n        if op not in MXNetGraph.registry_:\n            raise AttributeError(\"No conversion function registered for op type %s yet.\" % op)\n        convert_func = MXNetGraph.registry_[op]\n        return convert_func(node, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef split_params(sym, params):\n        arg_params = {}\n        aux_params = {}\n        for args in sym.list_arguments():\n            if args in params:\n                arg_params.update({args: nd.array(params[args])})\n        for aux in sym.list_auxiliary_states():\n            if aux in params:\n                aux_params.update({aux: nd.array(params[aux])})\n        return arg_params, aux_params", "response": "Helper function to split params dictionary into args and aux params"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_outputs(sym, params, in_shape, in_label):\n        # remove any input listed in params from sym.list_inputs() and bind them to the input shapes provided\n        # by user. Also remove in_label, which is the name of the label symbol that may have been used\n        # as the label for loss during training.\n        inputs = {n: tuple(s) for n, s in zip([n for n in sym.list_inputs() if n not in params and n != in_label],\n                                              in_shape)}\n        # Add params and their shape to list of inputs\n        inputs.update({n: v.shape for n, v in params.items() if n in sym.list_inputs()})\n        # Provide input data as well as input params to infer_shape()\n        _, out_shapes, _ = sym.infer_shape(**inputs)\n\n        out_names = list()\n        for name in sym.list_outputs():\n            if name.endswith('_output'):\n                out_names.append(name[:-len('_output')])\n            else:\n                logging.info(\"output '%s' does not end with '_output'\", name)\n                out_names.append(name)\n\n        assert len(out_shapes) == len(out_names)\n        # bind output shapes with output names\n        graph_outputs = {n: s for n, s in zip(out_names, out_shapes)}\n\n        return graph_outputs", "response": "Infer output shapes and return dictionary of output name to shape\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert weights to numpy", "response": "def convert_weights_to_numpy(weights_dict):\n        \"\"\"Convert weights to numpy\"\"\"\n        return dict([(k.replace(\"arg:\", \"\").replace(\"aux:\", \"\"), v.asnumpy())\n                     for k, v in weights_dict.items()])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_onnx_graph_proto(self, sym, params, in_shape, in_type, verbose=False):\n        try:\n            from onnx import (checker, helper, NodeProto, ValueInfoProto, TensorProto)\n            from onnx.helper import make_tensor_value_info\n        except ImportError:\n            raise ImportError(\"Onnx and protobuf need to be installed. \"\n                              + \"Instructions to install - https://github.com/onnx/onnx\")\n\n        # When MXNet model is saved to json file , MXNet adds a node for label.\n        # The name of this node is, name of the last node + \"_label\" ( i.e if last node\n        # name is \"Softmax\", this node will have a name \"Softmax_label\". Also, the new node\n        # will always be second last node in the json graph.\n        # Deriving the output_label name.\n        output_label = sym.get_internals()[len(sym.get_internals()) - 1].name + \"_label\"\n\n        weights = MXNetGraph.convert_weights_to_numpy(params)\n\n        mx_graph = json.loads(sym.tojson())[\"nodes\"]\n\n        initializer = []\n        all_processed_nodes = []\n        onnx_processed_nodes = []\n        onnx_processed_inputs = []\n        onnx_processed_outputs = []\n        index_lookup = []\n\n        # Determine output shape\n        graph_outputs = MXNetGraph.get_outputs(sym, params, in_shape, output_label)\n\n        graph_input_idx = 0\n        for idx, node in enumerate(mx_graph):\n            op = node[\"op\"]\n            name = node[\"name\"]\n            if verbose:\n                logging.info(\"Converting idx: %d, op: %s, name: %s\", idx, op, name)\n\n            # A node is an input node if its op_name is \"null\" and is not\n            # in params dict\n            if op == \"null\" and name not in params:\n                # Handling graph input\n\n                # Skipping output_label node, as this node is not part of graph\n                # Refer \"output_label\" assignment above for more details.\n                if name == output_label:\n                    continue\n                converted = MXNetGraph.convert_layer(\n                    node,\n                    is_input=True,\n                    mx_graph=mx_graph,\n                    weights=weights,\n                    in_shape=in_shape[graph_input_idx],\n                    in_type=in_type,\n                    proc_nodes=all_processed_nodes,\n                    initializer=initializer,\n                    index_lookup=index_lookup)\n                graph_input_idx += 1\n\n            else:\n                # Handling graph layers\n                converted = MXNetGraph.convert_layer(\n                    node,\n                    is_input=False,\n                    mx_graph=mx_graph,\n                    weights=weights,\n                    in_shape=in_shape,\n                    in_type=in_type,\n                    proc_nodes=all_processed_nodes,\n                    initializer=initializer,\n                    index_lookup=index_lookup,\n                    idx=idx\n                )\n\n            if isinstance(converted, list):\n                # Iterate for all converted nodes\n                for converted_node in converted:\n                    # If converted node is ValueInfoProto, add it in inputs\n                    if isinstance(converted_node, ValueInfoProto):\n                        onnx_processed_inputs.append(converted_node)\n                    # If converted node is NodeProto, add it in processed nodes list\n                    elif isinstance(converted_node, NodeProto):\n                        onnx_processed_nodes.append(converted_node)\n                        # some operators have multiple outputs,\n                        # therefore, check all output node names\n                        node_names = list(converted_node.output)\n                        for nodename in node_names:\n                            if nodename in graph_outputs:\n                                onnx_processed_outputs.append(\n                                    make_tensor_value_info(\n                                        name=nodename,\n                                        elem_type=in_type,\n                                        shape=graph_outputs[nodename]\n                                    )\n                                )\n                                if verbose:\n                                    logging.info(\"Output node is: %s\", nodename)\n                    elif isinstance(converted_node, TensorProto):\n                        raise ValueError(\"Did not expect TensorProto\")\n                    else:\n                        raise ValueError(\"node is of an unrecognized type: %s\" % type(node))\n\n                    all_processed_nodes.append(converted_node)\n\n                if idx > 0:\n                    # Handling extra node added to the graph if the MXNet model was\n                    # saved to json file,\n                    # refer \"output_label\" initialization above for more details.\n                    # if extra node was added then prev_index to the last node is adjusted.\n                    if idx == (len(mx_graph) - 1) and \\\n                            mx_graph[len(mx_graph)-2][\"name\"] == output_label:\n                        prev_index = index_lookup[idx - 2]\n                    else:\n                        prev_index = index_lookup[idx - 1]\n\n                    index_lookup.append(prev_index+len(converted))\n                else:\n                    index_lookup.append(len(converted) - 1)\n            else:\n                logging.info(\"Operator converter function should always return a list\")\n\n        graph = helper.make_graph(\n            onnx_processed_nodes,\n            \"mxnet_converted_model\",\n            onnx_processed_inputs,\n            onnx_processed_outputs\n        )\n\n        graph.initializer.extend(initializer)\n\n        checker.check_graph(graph)\n        return graph", "response": "Convert MXNet graph to ONNX graph proto."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes learning rate and refactor scheduler.", "response": "def get_lr_scheduler(learning_rate, lr_refactor_step, lr_refactor_ratio,\n                     num_example, batch_size, begin_epoch):\n    \"\"\"\n    Compute learning rate and refactor scheduler\n\n    Parameters:\n    ---------\n    learning_rate : float\n        original learning rate\n    lr_refactor_step : comma separated str\n        epochs to change learning rate\n    lr_refactor_ratio : float\n        lr *= ratio at certain steps\n    num_example : int\n        number of training images, used to estimate the iterations given epochs\n    batch_size : int\n        training batch size\n    begin_epoch : int\n        starting epoch\n\n    Returns:\n    ---------\n    (learning_rate, mx.lr_scheduler) as tuple\n    \"\"\"\n    assert lr_refactor_ratio > 0\n    iter_refactor = [int(r) for r in lr_refactor_step.split(',') if r.strip()]\n    if lr_refactor_ratio >= 1:\n        return (learning_rate, None)\n    else:\n        lr = learning_rate\n        epoch_size = num_example // batch_size\n        for s in iter_refactor:\n            if begin_epoch >= s:\n                lr *= lr_refactor_ratio\n        if lr != learning_rate:\n            logging.getLogger().info(\"Adjusted learning rate to {} for epoch {}\".format(lr, begin_epoch))\n        steps = [epoch_size * (x - begin_epoch) for x in iter_refactor if x > begin_epoch]\n        if not steps:\n            return (lr, None)\n        lr_scheduler = mx.lr_scheduler.MultiFactorScheduler(step=steps, factor=lr_refactor_ratio)\n        return (lr, lr_scheduler)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef train_net(net, train_path, num_classes, batch_size,\n              data_shape, mean_pixels, resume, finetune, pretrained, epoch,\n              prefix, ctx, begin_epoch, end_epoch, frequent, learning_rate,\n              momentum, weight_decay, lr_refactor_step, lr_refactor_ratio,\n              freeze_layer_pattern='',\n              num_example=10000, label_pad_width=350,\n              nms_thresh=0.45, force_nms=False, ovp_thresh=0.5,\n              use_difficult=False, class_names=None,\n              voc07_metric=False, nms_topk=400, force_suppress=False,\n              train_list=\"\", val_path=\"\", val_list=\"\", iter_monitor=0,\n              monitor_pattern=\".*\", log_file=None, kv_store=None):\n    \"\"\"\n    Wrapper for training phase.\n\n    Parameters:\n    ----------\n    net : str\n        symbol name for the network structure\n    train_path : str\n        record file path for training\n    num_classes : int\n        number of object classes, not including background\n    batch_size : int\n        training batch-size\n    data_shape : int or tuple\n        width/height as integer or (3, height, width) tuple\n    mean_pixels : tuple of floats\n        mean pixel values for red, green and blue\n    resume : int\n        resume from previous checkpoint if > 0\n    finetune : int\n        fine-tune from previous checkpoint if > 0\n    pretrained : str\n        prefix of pretrained model, including path\n    epoch : int\n        load epoch of either resume/finetune/pretrained model\n    prefix : str\n        prefix for saving checkpoints\n    ctx : [mx.cpu()] or [mx.gpu(x)]\n        list of mxnet contexts\n    begin_epoch : int\n        starting epoch for training, should be 0 if not otherwise specified\n    end_epoch : int\n        end epoch of training\n    frequent : int\n        frequency to print out training status\n    learning_rate : float\n        training learning rate\n    momentum : float\n        trainig momentum\n    weight_decay : float\n        training weight decay param\n    lr_refactor_ratio : float\n        multiplier for reducing learning rate\n    lr_refactor_step : comma separated integers\n        at which epoch to rescale learning rate, e.g. '30, 60, 90'\n    freeze_layer_pattern : str\n        regex pattern for layers need to be fixed\n    num_example : int\n        number of training images\n    label_pad_width : int\n        force padding training and validation labels to sync their label widths\n    nms_thresh : float\n        non-maximum suppression threshold for validation\n    force_nms : boolean\n        suppress overlaped objects from different classes\n    train_list : str\n        list file path for training, this will replace the embeded labels in record\n    val_path : str\n        record file path for validation\n    val_list : str\n        list file path for validation, this will replace the embeded labels in record\n    iter_monitor : int\n        monitor internal stats in networks if > 0, specified by monitor_pattern\n    monitor_pattern : str\n        regex pattern for monitoring network stats\n    log_file : str\n        log to file if enabled\n    \"\"\"\n    # set up logger\n    logging.basicConfig()\n    logger = logging.getLogger()\n    logger.setLevel(logging.INFO)\n    if log_file:\n        fh = logging.FileHandler(log_file)\n        logger.addHandler(fh)\n\n    # check args\n    if isinstance(data_shape, int):\n        data_shape = (3, data_shape, data_shape)\n    assert len(data_shape) == 3 and data_shape[0] == 3\n    prefix += '_' + net + '_' + str(data_shape[1])\n\n    if isinstance(mean_pixels, (int, float)):\n        mean_pixels = [mean_pixels, mean_pixels, mean_pixels]\n    assert len(mean_pixels) == 3, \"must provide all RGB mean values\"\n\n    train_iter = DetRecordIter(train_path, batch_size, data_shape, mean_pixels=mean_pixels,\n        label_pad_width=label_pad_width, path_imglist=train_list, **cfg.train)\n\n    if val_path:\n        val_iter = DetRecordIter(val_path, batch_size, data_shape, mean_pixels=mean_pixels,\n            label_pad_width=label_pad_width, path_imglist=val_list, **cfg.valid)\n    else:\n        val_iter = None\n\n    # load symbol\n    net = get_symbol_train(net, data_shape[1], num_classes=num_classes,\n        nms_thresh=nms_thresh, force_suppress=force_suppress, nms_topk=nms_topk)\n\n    # define layers with fixed weight/bias\n    if freeze_layer_pattern.strip():\n        re_prog = re.compile(freeze_layer_pattern)\n        fixed_param_names = [name for name in net.list_arguments() if re_prog.match(name)]\n    else:\n        fixed_param_names = None\n\n    # load pretrained or resume from previous state\n    ctx_str = '('+ ','.join([str(c) for c in ctx]) + ')'\n    if resume > 0:\n        logger.info(\"Resume training with {} from epoch {}\"\n            .format(ctx_str, resume))\n        _, args, auxs = mx.model.load_checkpoint(prefix, resume)\n        begin_epoch = resume\n    elif finetune > 0:\n        logger.info(\"Start finetuning with {} from epoch {}\"\n            .format(ctx_str, finetune))\n        _, args, auxs = mx.model.load_checkpoint(prefix, finetune)\n        begin_epoch = finetune\n        # the prediction convolution layers name starts with relu, so it's fine\n        fixed_param_names = [name for name in net.list_arguments() \\\n            if name.startswith('conv')]\n    elif pretrained:\n        logger.info(\"Start training with {} from pretrained model {}\"\n            .format(ctx_str, pretrained))\n        _, args, auxs = mx.model.load_checkpoint(pretrained, epoch)\n        args = convert_pretrained(pretrained, args)\n    else:\n        logger.info(\"Experimental: start training from scratch with {}\"\n            .format(ctx_str))\n        args = None\n        auxs = None\n        fixed_param_names = None\n\n    # helper information\n    if fixed_param_names:\n        logger.info(\"Freezed parameters: [\" + ','.join(fixed_param_names) + ']')\n\n    # init training module\n    mod = mx.mod.Module(net, label_names=('label',), logger=logger, context=ctx,\n                        fixed_param_names=fixed_param_names)\n\n    # fit parameters\n    batch_end_callback = mx.callback.Speedometer(train_iter.batch_size, frequent=frequent)\n    epoch_end_callback = mx.callback.do_checkpoint(prefix)\n    learning_rate, lr_scheduler = get_lr_scheduler(learning_rate, lr_refactor_step,\n        lr_refactor_ratio, num_example, batch_size, begin_epoch)\n    optimizer_params={'learning_rate':learning_rate,\n                      'momentum':momentum,\n                      'wd':weight_decay,\n                      'lr_scheduler':lr_scheduler,\n                      'clip_gradient':None,\n                      'rescale_grad': 1.0 / len(ctx) if len(ctx) > 0 else 1.0 }\n    monitor = mx.mon.Monitor(iter_monitor, pattern=monitor_pattern) if iter_monitor > 0 else None\n\n    # run fit net, every n epochs we run evaluation network to get mAP\n    if voc07_metric:\n        valid_metric = VOC07MApMetric(ovp_thresh, use_difficult, class_names, pred_idx=3)\n    else:\n        valid_metric = MApMetric(ovp_thresh, use_difficult, class_names, pred_idx=3)\n\n    # create kvstore when there are gpus\n    kv = mx.kvstore.create(kv_store) if kv_store else None\n\n    mod.fit(train_iter,\n            val_iter,\n            eval_metric=MultiBoxMetric(),\n            validation_metric=valid_metric,\n            batch_end_callback=batch_end_callback,\n            epoch_end_callback=epoch_end_callback,\n            optimizer='sgd',\n            optimizer_params=optimizer_params,\n            begin_epoch=begin_epoch,\n            num_epoch=end_epoch,\n            initializer=mx.init.Xavier(),\n            arg_params=args,\n            aux_params=auxs,\n            allow_missing=True,\n            monitor=monitor,\n            kvstore=kv)", "response": "Train a network structure using the training phase."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef boston(display=False):\n\n    d = sklearn.datasets.load_boston()\n    df = pd.DataFrame(data=d.data, columns=d.feature_names) # pylint: disable=E1101\n    return df, d.target", "response": "Return the boston housing data in a nice package."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef imdb(display=False):\n\n    with open(cache(github_data_url + \"imdb_train.txt\")) as f:\n        data = f.readlines()\n    y = np.ones(25000, dtype=np.bool)\n    y[:12500] = 0\n    return data, y", "response": "Return the clssic IMDB sentiment analysis training data in a nice package."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npredicting total number of non - violent crimes per 100K popuation.", "response": "def communitiesandcrime(display=False):\n    \"\"\" Predict total number of non-violent crimes per 100K popuation.\n\n    This dataset is from the classic UCI Machine Learning repository:\n    https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime+Unnormalized\n    \"\"\"\n\n    raw_data = pd.read_csv(\n        cache(github_data_url + \"CommViolPredUnnormalizedData.txt\"),\n        na_values=\"?\"\n    )\n\n    # find the indices where the total violent crimes are known\n    valid_inds = np.where(np.invert(np.isnan(raw_data.iloc[:,-2])))[0]\n    y = np.array(raw_data.iloc[valid_inds,-2], dtype=np.float)\n\n    # extract the predictive features and remove columns with missing values\n    X = raw_data.iloc[valid_inds,5:-18]\n    valid_cols = np.where(np.isnan(X.values).sum(0) == 0)[0]\n    X = X.iloc[:,valid_cols]\n\n    return X, y"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the diabetes data in a nice package.", "response": "def diabetes(display=False):\n    \"\"\" Return the diabetes data in a nice package. \"\"\"\n\n    d = sklearn.datasets.load_diabetes()\n    df = pd.DataFrame(data=d.data, columns=d.feature_names) # pylint: disable=E1101\n    return df, d.target"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef iris(display=False):\n\n    d = sklearn.datasets.load_iris()\n    df = pd.DataFrame(data=d.data, columns=d.feature_names) # pylint: disable=E1101\n    if display:\n        return df, [d.target_names[v] for v in d.target] # pylint: disable=E1101\n    else:\n        return df, d.target", "response": "Return the classic iris data in a nice package."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the Adult census data in a nice package.", "response": "def adult(display=False):\n    \"\"\" Return the Adult census data in a nice package. \"\"\"\n    dtypes = [\n        (\"Age\", \"float32\"), (\"Workclass\", \"category\"), (\"fnlwgt\", \"float32\"),\n        (\"Education\", \"category\"), (\"Education-Num\", \"float32\"), (\"Marital Status\", \"category\"),\n        (\"Occupation\", \"category\"), (\"Relationship\", \"category\"), (\"Race\", \"category\"),\n        (\"Sex\", \"category\"), (\"Capital Gain\", \"float32\"), (\"Capital Loss\", \"float32\"),\n        (\"Hours per week\", \"float32\"), (\"Country\", \"category\"), (\"Target\", \"category\")\n    ]\n    raw_data = pd.read_csv(\n        cache(github_data_url + \"adult.data\"),\n        names=[d[0] for d in dtypes],\n        na_values=\"?\",\n        dtype=dict(dtypes)\n    )\n    data = raw_data.drop([\"Education\"], axis=1)  # redundant with Education-Num\n    filt_dtypes = list(filter(lambda x: not (x[0] in [\"Target\", \"Education\"]), dtypes))\n    data[\"Target\"] = data[\"Target\"] == \" >50K\"\n    rcode = {\n        \"Not-in-family\": 0,\n        \"Unmarried\": 1,\n        \"Other-relative\": 2,\n        \"Own-child\": 3,\n        \"Husband\": 4,\n        \"Wife\": 5\n    }\n    for k, dtype in filt_dtypes:\n        if dtype == \"category\":\n            if k == \"Relationship\":\n                data[k] = np.array([rcode[v.strip()] for v in data[k]])\n            else:\n                data[k] = data[k].cat.codes\n\n    if display:\n        return raw_data.drop([\"Education\", \"Target\", \"fnlwgt\"], axis=1), data[\"Target\"].values\n    else:\n        return data.drop([\"Target\", \"fnlwgt\"], axis=1), data[\"Target\"].values"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef nhanesi(display=False):\n    X = pd.read_csv(cache(github_data_url + \"NHANESI_subset_X.csv\"))\n    y = pd.read_csv(cache(github_data_url + \"NHANESI_subset_y.csv\"))[\"y\"]\n    if display:\n        X_display = X.copy()\n        X_display[\"Sex\"] = [\"Male\" if v == 1 else \"Female\" for v in X[\"Sex\"]]\n        return X_display, np.array(y)\n    else:\n        return X, np.array(y)", "response": "A nicely packaged version of NHANES I data with surivival times as labels."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cric(display=False):\n    X = pd.read_csv(cache(github_data_url + \"CRIC_time_4yearESRD_X.csv\"))\n    y = np.loadtxt(cache(github_data_url + \"CRIC_time_4yearESRD_y.csv\"))\n    if display:\n        X_display = X.copy()\n        return X_display, y\n    else:\n        return X, y", "response": "A nicely packaged version of CRIC data with progression to ESRD within 4 years as the label."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef corrgroups60(display=False):\n\n    # set a constant seed\n    old_seed = np.random.seed()\n    np.random.seed(0)\n\n    # generate dataset with known correlation\n    N = 1000\n    M = 60\n\n    # set one coefficent from each group of 3 to 1\n    beta = np.zeros(M)\n    beta[0:30:3] = 1\n\n    # build a correlation matrix with groups of 3 tightly correlated features\n    C = np.eye(M)\n    for i in range(0,30,3):\n        C[i,i+1] = C[i+1,i] = 0.99\n        C[i,i+2] = C[i+2,i] = 0.99\n        C[i+1,i+2] = C[i+2,i+1] = 0.99\n    f = lambda X: np.matmul(X, beta)\n\n    # Make sure the sample correlation is a perfect match\n    X_start = np.random.randn(N, M)\n    X_centered = X_start - X_start.mean(0)\n    Sigma = np.matmul(X_centered.T, X_centered) / X_centered.shape[0]\n    W = np.linalg.cholesky(np.linalg.inv(Sigma)).T\n    X_white = np.matmul(X_centered, W.T)\n    assert np.linalg.norm(np.corrcoef(np.matmul(X_centered, W.T).T) - np.eye(M)) < 1e-6 # ensure this decorrelates the data\n\n    # create the final data\n    X_final = np.matmul(X_white, np.linalg.cholesky(C).T)\n    X = X_final\n    y = f(X) + np.random.randn(N) * 1e-2\n\n    # restore the previous numpy random seed\n    np.random.seed(old_seed)\n\n    return pd.DataFrame(X), y", "response": "A simulated dataset with tight correlations among distinct groups of features."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nranking datasets from lightgbm repository.", "response": "def rank():\n    \"\"\" Ranking datasets from lightgbm repository.\n    \"\"\"\n    rank_data_url = 'https://raw.githubusercontent.com/Microsoft/LightGBM/master/examples/lambdarank/'\n    x_train, y_train = sklearn.datasets.load_svmlight_file(cache(rank_data_url + 'rank.train'))\n    x_test, y_test = sklearn.datasets.load_svmlight_file(cache(rank_data_url + 'rank.test'))\n    q_train = np.loadtxt(cache(rank_data_url + 'rank.train.query'))\n    q_test = np.loadtxt(cache(rank_data_url + 'rank.test.query'))\n    return x_train, y_train, x_test, y_test, q_train, q_test"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nkeeps the model for each test sample and re - train it for each test sample.", "response": "def keep_retrain(nkeep, X_train, y_train, X_test, y_test, attr_test, model_generator, metric, trained_model, random_state):\n    \"\"\" The model is retrained for each test sample with the non-important features set to a constant.\n\n    If you want to know how important a set of features is you can ask how the model would be\n    different if only those features had existed. To determine this we can mask the other features\n    across the entire training and test datasets, then retrain the model. If we apply compare the\n    output of this retrained model to the original model we can see the effect produced by only\n    knowning the important features. Since for individualized explanation methods each test sample\n    has a different set of most important features we need to retrain the model for every test sample\n    to get the change in model performance when a specified fraction of the most important features\n    are retained.\n    \"\"\"\n\n    warnings.warn(\"The retrain based measures can incorrectly evaluate models in some cases!\")\n\n    # see if we match the last cached call\n    global _keep_cache\n    args = (X_train, y_train, X_test, y_test, model_generator, metric)\n    cache_match = False\n    if \"args\" in _keep_cache:\n        if all(a is b for a,b in zip(_keep_cache[\"args\"], args)) and np.all(_keep_cache[\"attr_test\"] == attr_test):\n            cache_match = True\n\n    X_train, X_test = to_array(X_train, X_test)\n\n    # how many features to mask\n    assert X_train.shape[1] == X_test.shape[1]\n\n    # this is the model we will retrain many times\n    model_masked = model_generator()\n\n    # keep nkeep top features and re-train the model for each test explanation\n    X_train_tmp = np.zeros(X_train.shape)\n    X_test_tmp = np.zeros(X_test.shape)\n    yp_masked_test = np.zeros(y_test.shape)\n    tie_breaking_noise = const_rand(X_train.shape[1]) * 1e-6\n    last_nkeep = _keep_cache.get(\"nkeep\", None)\n    last_yp_masked_test = _keep_cache.get(\"yp_masked_test\", None)\n    for i in tqdm(range(len(y_test)), \"Retraining for the 'keep' metric\"):\n        if cache_match and last_nkeep[i] == nkeep[i]:\n            yp_masked_test[i] = last_yp_masked_test[i]\n        elif nkeep[i] == attr_test.shape[1]:\n            yp_masked_test[i] = trained_model.predict(X_test[i:i+1])[0]\n        else:\n\n            # mask out the most important features for this test instance\n            X_train_tmp[:] = X_train\n            X_test_tmp[:] = X_test\n            ordering = np.argsort(-attr_test[i,:] + tie_breaking_noise)\n            X_train_tmp[:,ordering[nkeep[i]:]] = X_train[:,ordering[nkeep[i]:]].mean()\n            X_test_tmp[i,ordering[nkeep[i]:]] = X_train[:,ordering[nkeep[i]:]].mean()\n\n            # retrain the model and make a prediction\n            model_masked.fit(X_train_tmp, y_train)\n            yp_masked_test[i] = model_masked.predict(X_test_tmp[i:i+1])[0]\n\n    # save our results so the next call to us can be faster when there is redundancy\n    _keep_cache[\"nkeep\"] = nkeep\n    _keep_cache[\"yp_masked_test\"] = yp_masked_test\n    _keep_cache[\"attr_test\"] = attr_test\n    _keep_cache[\"args\"] = args\n\n    return metric(y_test, yp_masked_test)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nkeep the top features of each test sample with the non - important features set to their mean.", "response": "def keep_mask(nkeep, X_train, y_train, X_test, y_test, attr_test, model_generator, metric, trained_model, random_state):\n    \"\"\" The model is revaluated for each test sample with the non-important features set to their mean.\n    \"\"\"\n\n    X_train, X_test = to_array(X_train, X_test)\n\n    # how many features to mask\n    assert X_train.shape[1] == X_test.shape[1]\n\n    # keep nkeep top features for each test explanation\n    X_test_tmp = X_test.copy()\n    yp_masked_test = np.zeros(y_test.shape)\n    tie_breaking_noise = const_rand(X_train.shape[1], random_state) * 1e-6\n    mean_vals = X_train.mean(0)\n    for i in range(len(y_test)):\n        if nkeep[i] < X_test.shape[1]:\n            ordering = np.argsort(-attr_test[i,:] + tie_breaking_noise)\n            X_test_tmp[i,ordering[nkeep[i]:]] = mean_vals[ordering[nkeep[i]:]]\n\n    yp_masked_test = trained_model.predict(X_test_tmp)\n\n    return metric(y_test, yp_masked_test)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nkeep the imputed value for each test sample.", "response": "def keep_impute(nkeep, X_train, y_train, X_test, y_test, attr_test, model_generator, metric, trained_model, random_state):\n    \"\"\" The model is revaluated for each test sample with the non-important features set to an imputed value.\n\n    Note that the imputation is done using a multivariate normality assumption on the dataset. This depends on\n    being able to estimate the full data covariance matrix (and inverse) accuractly. So X_train.shape[0] should\n    be significantly bigger than X_train.shape[1].\n    \"\"\"\n\n    X_train, X_test = to_array(X_train, X_test)\n\n    # how many features to mask\n    assert X_train.shape[1] == X_test.shape[1]\n\n    # keep nkeep top features for each test explanation\n    C = np.cov(X_train.T)\n    C += np.eye(C.shape[0]) * 1e-6\n    X_test_tmp = X_test.copy()\n    yp_masked_test = np.zeros(y_test.shape)\n    tie_breaking_noise = const_rand(X_train.shape[1], random_state) * 1e-6\n    mean_vals = X_train.mean(0)\n    for i in range(len(y_test)):\n        if nkeep[i] < X_test.shape[1]:\n            ordering = np.argsort(-attr_test[i,:] + tie_breaking_noise)\n            observe_inds = ordering[:nkeep[i]]\n            impute_inds = ordering[nkeep[i]:]\n            \n            # impute missing data assuming it follows a multivariate normal distribution\n            Coo_inv = np.linalg.inv(C[observe_inds,:][:,observe_inds])\n            Cio = C[impute_inds,:][:,observe_inds]\n            impute = mean_vals[impute_inds] + Cio @ Coo_inv @ (X_test[i, observe_inds] - mean_vals[observe_inds])\n            \n            X_test_tmp[i, impute_inds] = impute\n\n    yp_masked_test = trained_model.predict(X_test_tmp)\n\n    return metric(y_test, yp_masked_test)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef local_accuracy(X_train, y_train, X_test, y_test, attr_test, model_generator, metric, trained_model):\n\n    X_train, X_test = to_array(X_train, X_test)\n\n    # how many features to mask\n    assert X_train.shape[1] == X_test.shape[1]\n\n    # keep nkeep top features and re-train the model for each test explanation\n    yp_test = trained_model.predict(X_test)\n\n    return metric(yp_test, strip_list(attr_test).sum(1))", "response": "The how well do the features plus a constant base rate sum up to the model output."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef const_rand(size, seed=23980):\n    old_seed = np.random.seed()\n    np.random.seed(seed)\n    out = np.random.rand(size)\n    np.random.seed(old_seed)\n    return out", "response": "Generate a random array with a fixed seed."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef const_shuffle(arr, seed=23980):\n    old_seed = np.random.seed()\n    np.random.seed(seed)\n    np.random.shuffle(arr)\n    np.random.seed(old_seed)", "response": "Shuffle an array in - place with a fixed seed."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef shap_values(self, X, **kwargs):\n\n        phi = None\n        if self.mimic_model_type == \"xgboost\":\n            if not str(type(X)).endswith(\"xgboost.core.DMatrix'>\"):\n                X = xgboost.DMatrix(X)\n            phi = self.trees.predict(X, pred_contribs=True)\n\n        if phi is not None:\n            if len(phi.shape) == 3:\n                return [phi[:, i, :] for i in range(phi.shape[1])]\n            else:\n                return phi", "response": "Estimate the SHAP values for a set of samples."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef image_plot(shap_values, x, labels=None, show=True, width=20, aspect=0.2, hspace=0.2, labelpad=None):\n\n    multi_output = True\n    if type(shap_values) != list:\n        multi_output = False\n        shap_values = [shap_values]\n\n    # make sure labels\n    if labels is not None:\n        assert labels.shape[0] == shap_values[0].shape[0], \"Labels must have same row count as shap_values arrays!\"\n        if multi_output:\n            assert labels.shape[1] == len(shap_values), \"Labels must have a column for each output in shap_values!\"\n        else:\n            assert len(labels.shape) == 1, \"Labels must be a vector for single output shap_values.\"\n\n    label_kwargs = {} if labelpad is None else {'pad': labelpad}\n\n    # plot our explanations\n    fig_size = np.array([3 * (len(shap_values) + 1), 2.5 * (x.shape[0] + 1)])\n    if fig_size[0] > width:\n        fig_size *= width / fig_size[0]\n    fig, axes = pl.subplots(nrows=x.shape[0], ncols=len(shap_values) + 1, figsize=fig_size)\n    if len(axes.shape) == 1:\n        axes = axes.reshape(1,axes.size)\n    for row in range(x.shape[0]):\n        x_curr = x[row].copy()\n\n        # make sure\n        if len(x_curr.shape) == 3 and x_curr.shape[2] == 1:\n            x_curr = x_curr.reshape(x_curr.shape[:2])\n        if x_curr.max() > 1:\n            x_curr /= 255.\n\n        # get a grayscale version of the image\n        if len(x_curr.shape) == 3 and x_curr.shape[2] == 3:\n            x_curr_gray = (0.2989 * x_curr[:,:,0] + 0.5870 * x_curr[:,:,1] + 0.1140 * x_curr[:,:,2]) # rgb to gray\n        else:\n            x_curr_gray = x_curr\n\n        axes[row,0].imshow(x_curr, cmap=pl.get_cmap('gray'))\n        axes[row,0].axis('off')\n        if len(shap_values[0][row].shape) == 2:\n            abs_vals = np.stack([np.abs(shap_values[i]) for i in range(len(shap_values))], 0).flatten()\n        else:\n            abs_vals = np.stack([np.abs(shap_values[i].sum(-1)) for i in range(len(shap_values))], 0).flatten()\n        max_val = np.nanpercentile(abs_vals, 99.9)\n        for i in range(len(shap_values)):\n            if labels is not None:\n                axes[row,i+1].set_title(labels[row,i], **label_kwargs)\n            sv = shap_values[i][row] if len(shap_values[i][row].shape) == 2 else shap_values[i][row].sum(-1)\n            axes[row,i+1].imshow(x_curr_gray, cmap=pl.get_cmap('gray'), alpha=0.15, extent=(-1, sv.shape[0], sv.shape[1], -1))\n            im = axes[row,i+1].imshow(sv, cmap=colors.red_transparent_blue, vmin=-max_val, vmax=max_val)\n            axes[row,i+1].axis('off')\n    if hspace == 'auto':\n        fig.tight_layout()\n    else:\n        fig.subplots_adjust(hspace=hspace)\n    cb = fig.colorbar(im, ax=np.ravel(axes).tolist(), label=\"SHAP value\", orientation=\"horizontal\", aspect=fig_size[0]/aspect)\n    cb.outline.set_visible(False)\n    if show:\n        pl.show()", "response": "Plot the image inputs in a single image."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the approximate interactions for a given feature at a given index.", "response": "def approximate_interactions(index, shap_values, X, feature_names=None):\n    \"\"\" Order other features by how much interaction they seem to have with the feature at the given index.\n\n    This just bins the SHAP values for a feature along that feature's value. For true Shapley interaction\n    index values for SHAP see the interaction_contribs option implemented in XGBoost.\n    \"\"\"\n\n    # convert from DataFrames if we got any\n    if str(type(X)).endswith(\"'pandas.core.frame.DataFrame'>\"):\n        if feature_names is None:\n            feature_names = X.columns\n        X = X.values\n\n    index = convert_name(index, shap_values, feature_names)\n\n    if X.shape[0] > 10000:\n        a = np.arange(X.shape[0])\n        np.random.shuffle(a)\n        inds = a[:10000]\n    else:\n        inds = np.arange(X.shape[0])\n\n    x = X[inds, index]\n    srt = np.argsort(x)\n    shap_ref = shap_values[inds, index]\n    shap_ref = shap_ref[srt]\n    inc = max(min(int(len(x) / 10.0), 50), 1)\n    interactions = []\n    for i in range(X.shape[1]):\n        val_other = X[inds, i][srt].astype(np.float)\n        v = 0.0\n        if not (i == index or np.sum(np.abs(val_other)) < 1e-8):\n            for j in range(0, len(x), inc):\n                if np.std(val_other[j:j + inc]) > 0 and np.std(shap_ref[j:j + inc]) > 0:\n                    v += abs(np.corrcoef(shap_ref[j:j + inc], val_other[j:j + inc])[0, 1])\n        val_v = v\n\n        val_other = np.isnan(X[inds, i][srt].astype(np.float))\n        v = 0.0\n        if not (i == index or np.sum(np.abs(val_other)) < 1e-8):\n            for j in range(0, len(x), inc):\n                if np.std(val_other[j:j + inc]) > 0 and np.std(shap_ref[j:j + inc]) > 0:\n                    v += abs(np.corrcoef(shap_ref[j:j + inc], val_other[j:j + inc])[0, 1])\n        nan_v = v\n\n        interactions.append(max(val_v, nan_v))\n\n    return np.argsort(-np.abs(interactions))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _human_score_map(human_consensus, methods_attrs):\n\n    v = 1 - min(np.sum(np.abs(methods_attrs - human_consensus)) / (np.abs(human_consensus).sum() + 1), 1.0)\n    return v", "response": "Converts human agreement differences to numerical scores for coloring."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndraw the bars and separators.", "response": "def draw_bars(out_value, features, feature_type, width_separators, width_bar):\n    \"\"\"Draw the bars and separators.\"\"\"\n    rectangle_list = []\n    separator_list = []\n    \n    pre_val = out_value\n    for index, features in zip(range(len(features)), features):\n        if feature_type == 'positive':\n            left_bound = float(features[0])\n            right_bound = pre_val\n            pre_val = left_bound\n            \n            separator_indent = np.abs(width_separators)\n            separator_pos = left_bound\n            colors = ['#FF0D57', '#FFC3D5']\n        else:\n            left_bound = pre_val\n            right_bound = float(features[0])\n            pre_val = right_bound\n            \n            separator_indent = - np.abs(width_separators)\n            separator_pos = right_bound\n            colors = ['#1E88E5', '#D1E6FA']\n        \n        # Create rectangle\n        if index == 0:\n            if feature_type == 'positive':\n                points_rectangle = [[left_bound, 0],\n                                    [right_bound, 0],\n                                    [right_bound, width_bar],\n                                    [left_bound, width_bar],\n                                    [left_bound + separator_indent, (width_bar / 2)]\n                                    ]\n            else:\n                points_rectangle = [[right_bound, 0],\n                                    [left_bound, 0],\n                                    [left_bound, width_bar],\n                                    [right_bound, width_bar],\n                                    [right_bound + separator_indent, (width_bar / 2)]\n                                    ]\n        \n        else:\n            points_rectangle = [[left_bound, 0],\n                                [right_bound, 0],\n                                [right_bound + separator_indent * 0.90, (width_bar / 2)],\n                                [right_bound, width_bar],\n                                [left_bound, width_bar],\n                                [left_bound + separator_indent * 0.90, (width_bar / 2)]]\n\n        line = plt.Polygon(points_rectangle, closed=True, fill=True,\n                           facecolor=colors[0], linewidth=0)\n        rectangle_list += [line]\n\n        # Create seperator\n        points_separator = [[separator_pos, 0],\n                            [separator_pos + separator_indent, (width_bar / 2)],\n                            [separator_pos, width_bar]]\n        \n        line = plt.Polygon(points_separator, closed=None, fill=None,\n                           edgecolor=colors[1], lw=3)\n        separator_list += [line]\n\n    return rectangle_list, separator_list"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef format_data(data):\n    # Format negative features\n    neg_features = np.array([[data['features'][x]['effect'],\n                              data['features'][x]['value'],\n                              data['featureNames'][x]]\n                             for x in data['features'].keys() if data['features'][x]['effect'] < 0])\n    \n    neg_features = np.array(sorted(neg_features, key=lambda x: float(x[0]), reverse=False))\n    \n    # Format postive features\n    pos_features = np.array([[data['features'][x]['effect'],\n                              data['features'][x]['value'],\n                              data['featureNames'][x]]\n                             for x in data['features'].keys() if data['features'][x]['effect'] >= 0])\n    pos_features = np.array(sorted(pos_features, key=lambda x: float(x[0]), reverse=True))\n    \n    # Define link function\n    if data['link'] == 'identity':\n        convert_func = lambda x: x\n    elif data['link'] == 'logit':\n        convert_func = lambda x: 1 / (1 + np.exp(-x))\n    else:\n        assert False, \"ERROR: Unrecognized link function: \" + str(data['link'])\n    \n    # Convert negative feature values to plot values\n    neg_val = data['outValue']\n    for i in neg_features:\n        val = float(i[0])\n        neg_val = neg_val + np.abs(val)\n        i[0] = convert_func(neg_val)\n    if len(neg_features) > 0:\n        total_neg = np.max(neg_features[:, 0].astype(float)) - \\\n                    np.min(neg_features[:, 0].astype(float))\n    else:\n        total_neg = 0\n    \n    # Convert positive feature values to plot values\n    pos_val = data['outValue']\n    for i in pos_features:\n        val = float(i[0])\n        pos_val = pos_val - np.abs(val)\n        i[0] = convert_func(pos_val)\n        \n    if len(pos_features) > 0:\n        total_pos = np.max(pos_features[:, 0].astype(float)) - \\\n                    np.min(pos_features[:, 0].astype(float))\n    else:\n        total_pos = 0\n    \n    # Convert output value and base value\n    data['outValue'] = convert_func(data['outValue'])\n    data['baseValue'] = convert_func(data['baseValue'])\n    \n    return neg_features, total_neg, pos_features, total_pos", "response": "Format data into a single tree node."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef try_run_setup(**kwargs):\n\n    try:\n        run_setup(**kwargs)\n    except Exception as e:\n        print(str(e))\n        if \"xgboost\" in str(e).lower():\n            kwargs[\"test_xgboost\"] = False\n            print(\"Couldn't install XGBoost for testing!\")\n            try_run_setup(**kwargs)\n        elif \"lightgbm\" in str(e).lower():\n            kwargs[\"test_lightgbm\"] = False\n            print(\"Couldn't install LightGBM for testing!\")\n            try_run_setup(**kwargs)\n        elif kwargs[\"with_binary\"]:\n            kwargs[\"with_binary\"] = False\n            print(\"WARNING: The C extension could not be compiled, sklearn tree models not supported.\")\n            try_run_setup(**kwargs)\n        else:\n            print(\"ERROR: Failed to build!\")", "response": "Fail gracefully when various install steps don t work."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd interim values to the given module.", "response": "def add_interim_values(module, input, output):\n    \"\"\"The forward hook used to save interim tensors, detached\n    from the graph. Used to calculate the multipliers\n    \"\"\"\n    try:\n        del module.x\n    except AttributeError:\n        pass\n    try:\n        del module.y\n    except AttributeError:\n        pass\n    module_type = module.__class__.__name__\n    if module_type in op_handler:\n        func_name = op_handler[module_type].__name__\n        # First, check for cases where we don't need to save the x and y tensors\n        if func_name == 'passthrough':\n            pass\n        else:\n            # check only the 0th input varies\n            for i in range(len(input)):\n                if i != 0 and type(output) is tuple:\n                    assert input[i] == output[i], \"Only the 0th input may vary!\"\n            # if a new method is added, it must be added here too. This ensures tensors\n            # are only saved if necessary\n            if func_name in ['maxpool', 'nonlinear_1d']:\n                # only save tensors if necessary\n                if type(input) is tuple:\n                    setattr(module, 'x', torch.nn.Parameter(input[0].detach()))\n                else:\n                    setattr(module, 'x', torch.nn.Parameter(input.detach()))\n                if type(output) is tuple:\n                    setattr(module, 'y', torch.nn.Parameter(output[0].detach()))\n                else:\n                    setattr(module, 'y', torch.nn.Parameter(output.detach()))\n            if module_type in failure_case_modules:\n                input[0].register_hook(deeplift_tensor_grad)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_target_input(module, input, output):\n    try:\n        del module.target_input\n    except AttributeError:\n        pass\n    setattr(module, 'target_input', input)", "response": "A forward hook which saves the input - attached to the model\n    Used to explain the interim outputs of a model\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_handles(self, model, forward_handle, backward_handle):\n        handles_list = []\n        for child in model.children():\n            if 'nn.modules.container' in str(type(child)):\n                handles_list.extend(self.add_handles(child, forward_handle, backward_handle))\n            else:\n                handles_list.append(child.register_forward_hook(forward_handle))\n                handles_list.append(child.register_backward_hook(backward_handle))\n        return handles_list", "response": "Add handles to all non - container layers in the model."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nestimates the Shap values for a set of samples.", "response": "def shap_values(self, X, y=None, tree_limit=None, approximate=False):\n        \"\"\" Estimate the SHAP values for a set of samples.\n\n        Parameters\n        ----------\n        X : numpy.array, pandas.DataFrame or catboost.Pool (for catboost)\n            A matrix of samples (# samples x # features) on which to explain the model's output.\n\n        y : numpy.array\n            An array of label values for each sample. Used when explaining loss functions.\n\n        tree_limit : None (default) or int \n            Limit the number of trees used by the model. By default None means no use the limit of the\n            original model, and -1 means no limit.\n\n        approximate : bool\n            Run fast, but only roughly approximate the Tree SHAP values. This runs a method\n            previously proposed by Saabas which only considers a single feature ordering. Take care\n            since this does not have the consistency guarantees of Shapley values and places too\n            much weight on lower splits in the tree.\n\n        Returns\n        -------\n        For models with a single output this returns a matrix of SHAP values\n        (# samples x # features). Each row sums to the difference between the model output for that\n        sample and the expected value of the model output (which is stored in the expected_value\n        attribute of the explainer when it is constant). For models with vector outputs this returns\n        a list of such matrices, one for each output.\n        \"\"\"\n\n        # see if we have a default tree_limit in place.\n        if tree_limit is None:\n            tree_limit = -1 if self.model.tree_limit is None else self.model.tree_limit\n\n        # shortcut using the C++ version of Tree SHAP in XGBoost, LightGBM, and CatBoost\n        if self.feature_dependence == \"tree_path_dependent\" and self.model.model_type != \"internal\" and self.data is None:\n            phi = None\n            if self.model.model_type == \"xgboost\":\n                assert_import(\"xgboost\")\n                if not str(type(X)).endswith(\"xgboost.core.DMatrix'>\"):\n                    X = xgboost.DMatrix(X)\n                if tree_limit == -1:\n                    tree_limit = 0\n                phi = self.model.original_model.predict(\n                    X, ntree_limit=tree_limit, pred_contribs=True,\n                    approx_contribs=approximate, validate_features=False\n                )\n            \n            elif self.model.model_type == \"lightgbm\":\n                assert not approximate, \"approximate=True is not supported for LightGBM models!\"\n                phi = self.model.original_model.predict(X, num_iteration=tree_limit, pred_contrib=True)\n                if phi.shape[1] != X.shape[1] + 1:\n                    phi = phi.reshape(X.shape[0], phi.shape[1]//(X.shape[1]+1), X.shape[1]+1)\n            \n            elif self.model.model_type == \"catboost\": # thanks to the CatBoost team for implementing this...\n                assert not approximate, \"approximate=True is not supported for CatBoost models!\"\n                assert tree_limit == -1, \"tree_limit is not yet supported for CatBoost models!\"\n                if type(X) != catboost.Pool:\n                    X = catboost.Pool(X)\n                phi = self.model.original_model.get_feature_importance(data=X, fstr_type='ShapValues')\n\n            # note we pull off the last column and keep it as our expected_value\n            if phi is not None:\n                if len(phi.shape) == 3:\n                    self.expected_value = [phi[0, i, -1] for i in range(phi.shape[1])]\n                    return [phi[:, i, :-1] for i in range(phi.shape[1])]\n                else:\n                    self.expected_value = phi[0, -1]\n                    return phi[:, :-1]\n\n        # convert dataframes\n        orig_X = X\n        if str(type(X)).endswith(\"pandas.core.series.Series'>\"):\n            X = X.values\n        elif str(type(X)).endswith(\"pandas.core.frame.DataFrame'>\"):\n            X = X.values\n        flat_output = False\n        if len(X.shape) == 1:\n            flat_output = True\n            X = X.reshape(1, X.shape[0])\n        if X.dtype != self.model.dtype:\n            X = X.astype(self.model.dtype)\n        X_missing = np.isnan(X, dtype=np.bool)\n        assert str(type(X)).endswith(\"'numpy.ndarray'>\"), \"Unknown instance type: \" + str(type(X))\n        assert len(X.shape) == 2, \"Passed input data matrix X must have 1 or 2 dimensions!\"\n\n        if tree_limit < 0 or tree_limit > self.model.values.shape[0]:\n            tree_limit = self.model.values.shape[0]\n        \n        if self.model_output == \"logloss\":\n            assert y is not None, \"Both samples and labels must be provided when explaining the loss (i.e. `explainer.shap_values(X, y)`)!\"\n            assert X.shape[0] == len(y), \"The number of labels (%d) does not match the number of samples to explain (%d)!\" % (len(y), X.shape[0])\n        transform = self.model.get_transform(self.model_output)\n\n        if self.feature_dependence == \"tree_path_dependent\":\n            assert self.model.fully_defined_weighting, \"The background dataset you provided does not cover all the leaves in the model, \" \\\n                                                       \"so TreeExplainer cannot run with the feature_dependence=\\\"tree_path_dependent\\\" option! \" \\\n                                                       \"Try providing a larger background dataset, or using feature_dependence=\\\"independent\\\".\"\n \n        # run the core algorithm using the C extension\n        assert_import(\"cext\")\n        phi = np.zeros((X.shape[0], X.shape[1]+1, self.model.n_outputs))\n        if not approximate:\n            _cext.dense_tree_shap(\n                self.model.children_left, self.model.children_right, self.model.children_default,\n                self.model.features, self.model.thresholds, self.model.values, self.model.node_sample_weight,\n                self.model.max_depth, X, X_missing, y, self.data, self.data_missing, tree_limit,\n                self.model.base_offset, phi, feature_dependence_codes[self.feature_dependence],\n                output_transform_codes[transform], False\n            )\n        else:\n            _cext.dense_tree_saabas(\n                self.model.children_left, self.model.children_right, self.model.children_default,\n                self.model.features, self.model.thresholds, self.model.values,\n                self.model.max_depth, tree_limit, self.model.base_offset, output_transform_codes[transform], \n                X, X_missing, y, phi\n            )\n\n        # note we pull off the last column and keep it as our expected_value\n        if self.model.n_outputs == 1:\n            if self.model_output != \"logloss\":\n                self.expected_value = phi[0, -1, 0]\n            if flat_output:\n                return phi[0, :-1, 0]\n            else:\n                return phi[:, :-1, 0]\n        else:\n            if self.model_output != \"logloss\":\n                self.expected_value = [phi[0, -1, i] for i in range(phi.shape[2])]\n            if flat_output:\n                return [phi[0, :-1, i] for i in range(self.model.n_outputs)]\n            else:\n                return [phi[:, :-1, i] for i in range(self.model.n_outputs)]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nestimating the SHAP interaction values for a set of samples.", "response": "def shap_interaction_values(self, X, y=None, tree_limit=None):\n        \"\"\" Estimate the SHAP interaction values for a set of samples.\n\n        Parameters\n        ----------\n        X : numpy.array, pandas.DataFrame or catboost.Pool (for catboost)\n            A matrix of samples (# samples x # features) on which to explain the model's output.\n\n        y : numpy.array\n            An array of label values for each sample. Used when explaining loss functions (not yet supported).\n\n        tree_limit : None (default) or int \n            Limit the number of trees used by the model. By default None means no use the limit of the\n            original model, and -1 means no limit.\n\n        Returns\n        -------\n        For models with a single output this returns a tensor of SHAP values\n        (# samples x # features x # features). The matrix (# features x # features) for each sample sums\n        to the difference between the model output for that sample and the expected value of the model output\n        (which is stored in the expected_value attribute of the explainer). Each row of this matrix sums to the\n        SHAP value for that feature for that sample. The diagonal entries of the matrix represent the\n        \"main effect\" of that feature on the prediction and the symmetric off-diagonal entries represent the\n        interaction effects between all pairs of features for that sample. For models with vector outputs\n        this returns a list of tensors, one for each output.\n        \"\"\"\n\n        assert self.model_output == \"margin\", \"Only model_output = \\\"margin\\\" is supported for SHAP interaction values right now!\"\n        assert self.feature_dependence == \"tree_path_dependent\", \"Only feature_dependence = \\\"tree_path_dependent\\\" is supported for SHAP interaction values right now!\"\n        transform = \"identity\"\n\n        # see if we have a default tree_limit in place.\n        if tree_limit is None:\n            tree_limit = -1 if self.model.tree_limit is None else self.model.tree_limit\n\n        # shortcut using the C++ version of Tree SHAP in XGBoost\n        if self.model.model_type == \"xgboost\":\n            assert_import(\"xgboost\")\n            if not str(type(X)).endswith(\"xgboost.core.DMatrix'>\"):\n                X = xgboost.DMatrix(X)\n            if tree_limit == -1:\n                tree_limit = 0\n            phi = self.model.original_model.predict(X, ntree_limit=tree_limit, pred_interactions=True)\n\n            # note we pull off the last column and keep it as our expected_value\n            if len(phi.shape) == 4:\n                self.expected_value = [phi[0, i, -1, -1] for i in range(phi.shape[1])]\n                return [phi[:, i, :-1, :-1] for i in range(phi.shape[1])]\n            else:\n                self.expected_value = phi[0, -1, -1]\n                return phi[:, :-1, :-1]\n\n        # convert dataframes\n        if str(type(X)).endswith(\"pandas.core.series.Series'>\"):\n            X = X.values\n        elif str(type(X)).endswith(\"pandas.core.frame.DataFrame'>\"):\n            X = X.values\n        flat_output = False\n        if len(X.shape) == 1:\n            flat_output = True\n            X = X.reshape(1, X.shape[0])\n        if X.dtype != self.model.dtype:\n            X = X.astype(self.model.dtype)\n        X_missing = np.isnan(X, dtype=np.bool)\n        assert str(type(X)).endswith(\"'numpy.ndarray'>\"), \"Unknown instance type: \" + str(type(X))\n        assert len(X.shape) == 2, \"Passed input data matrix X must have 1 or 2 dimensions!\"\n\n        if tree_limit < 0 or tree_limit > self.model.values.shape[0]:\n            tree_limit = self.model.values.shape[0]\n\n        # run the core algorithm using the C extension\n        assert_import(\"cext\")\n        phi = np.zeros((X.shape[0], X.shape[1]+1, X.shape[1]+1, self.model.n_outputs))\n        _cext.dense_tree_shap(\n            self.model.children_left, self.model.children_right, self.model.children_default,\n            self.model.features, self.model.thresholds, self.model.values, self.model.node_sample_weight,\n            self.model.max_depth, X, X_missing, y, self.data, self.data_missing, tree_limit,\n            self.model.base_offset, phi, feature_dependence_codes[self.feature_dependence],\n            output_transform_codes[transform], True\n        )\n\n        # note we pull off the last column and keep it as our expected_value\n        if self.model.n_outputs == 1:\n            self.expected_value = phi[0, -1, -1, 0]\n            if flat_output:\n                return phi[0, :-1, :-1, 0]\n            else:\n                return phi[:, :-1, :-1, 0]\n        else:\n            self.expected_value = [phi[0, -1, -1, i] for i in range(phi.shape[3])]\n            if flat_output:\n                return [phi[0, :-1, :-1, i] for i in range(self.model.n_outputs)]\n            else:\n                return [phi[:, :-1, :-1, i] for i in range(self.model.n_outputs)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_transform(self, model_output):\n        if model_output == \"margin\":\n            transform = \"identity\"\n        elif model_output == \"probability\":\n            if self.tree_output == \"log_odds\":\n                transform = \"logistic\"\n            elif self.tree_output == \"probability\":\n                transform = \"identity\"\n            else:\n                raise Exception(\"model_output = \\\"probability\\\" is not yet supported when model.tree_output = \\\"\" + self.tree_output + \"\\\"!\")\n        elif model_output == \"logloss\":\n\n            if self.objective == \"squared_error\":\n                transform = \"squared_loss\"\n            elif self.objective == \"binary_crossentropy\":\n                transform = \"logistic_nlogloss\"\n            else:\n                raise Exception(\"model_output = \\\"logloss\\\" is not yet supported when model.objective = \\\"\" + self.objective + \"\\\"!\")\n        return transform", "response": "A consistent interface to make predictions from this model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\npredict the current state of the object.", "response": "def predict(self, X, y=None, output=\"margin\", tree_limit=None):\n        \"\"\" A consistent interface to make predictions from this model.\n\n        Parameters\n        ----------\n        tree_limit : None (default) or int \n            Limit the number of trees used by the model. By default None means no use the limit of the\n            original model, and -1 means no limit.\n        \"\"\"\n\n        # see if we have a default tree_limit in place.\n        if tree_limit is None:\n            tree_limit = -1 if self.tree_limit is None else self.tree_limit\n\n        # convert dataframes\n        if str(type(X)).endswith(\"pandas.core.series.Series'>\"):\n            X = X.values\n        elif str(type(X)).endswith(\"pandas.core.frame.DataFrame'>\"):\n            X = X.values\n        flat_output = False\n        if len(X.shape) == 1:\n            flat_output = True\n            X = X.reshape(1, X.shape[0])\n        if X.dtype != self.dtype:\n            X = X.astype(self.dtype)\n        X_missing = np.isnan(X, dtype=np.bool)\n        assert str(type(X)).endswith(\"'numpy.ndarray'>\"), \"Unknown instance type: \" + str(type(X))\n        assert len(X.shape) == 2, \"Passed input data matrix X must have 1 or 2 dimensions!\"\n\n        if tree_limit < 0 or tree_limit > self.values.shape[0]:\n            tree_limit = self.values.shape[0]\n\n        if output == \"logloss\":\n            assert y is not None, \"Both samples and labels must be provided when explaining the loss (i.e. `explainer.shap_values(X, y)`)!\"\n            assert X.shape[0] == len(y), \"The number of labels (%d) does not match the number of samples to explain (%d)!\" % (len(y), X.shape[0])\n        transform = self.get_transform(output)\n        \n        if True or self.model_type == \"internal\":\n            output = np.zeros((X.shape[0], self.n_outputs))\n            assert_import(\"cext\")\n            _cext.dense_tree_predict(\n                self.children_left, self.children_right, self.children_default,\n                self.features, self.thresholds, self.values,\n                self.max_depth, tree_limit, self.base_offset, output_transform_codes[transform], \n                X, X_missing, y, output\n            )\n\n        elif self.model_type == \"xgboost\":\n            assert_import(\"xgboost\")\n            output = self.original_model.predict(X, output_margin=True, tree_limit=tree_limit)\n\n        # drop dimensions we don't need\n        if flat_output:\n            if self.n_outputs == 1:\n                return output.flatten()[0]\n            else:\n                return output.reshape(-1, self.n_outputs)\n        else:\n            if self.n_outputs == 1:\n                return output.flatten()\n            else:\n                return output"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the values for the model applied to X.", "response": "def shap_values(self, X, nsamples=200, ranked_outputs=None, output_rank_order=\"max\", rseed=None):\n        \"\"\" Return the values for the model applied to X.\n\n        Parameters\n        ----------\n        X : list,\n            if framework == 'tensorflow': numpy.array, or pandas.DataFrame\n            if framework == 'pytorch': torch.tensor\n            A tensor (or list of tensors) of samples (where X.shape[0] == # samples) on which to\n            explain the model's output.\n\n        ranked_outputs : None or int\n            If ranked_outputs is None then we explain all the outputs in a multi-output model. If\n            ranked_outputs is a positive integer then we only explain that many of the top model\n            outputs (where \"top\" is determined by output_rank_order). Note that this causes a pair\n            of values to be returned (shap_values, indexes), where phi is a list of numpy arrays for each of\n            the output ranks, and indexes is a matrix that tells for each sample which output indexes\n            were choses as \"top\".\n\n        output_rank_order : \"max\", \"min\", \"max_abs\", or \"custom\"\n            How to order the model outputs when using ranked_outputs, either by maximum, minimum, or\n            maximum absolute value. If \"custom\" Then \"ranked_outputs\" contains a list of output nodes.\n\n        rseed : None or int\n            Seeding the randomness in shap value computation  (background example choice, \n            interpolation between current and background example, smoothing).\n\n        Returns\n        -------\n        For a models with a single output this returns a tensor of SHAP values with the same shape\n        as X. For a model with multiple outputs this returns a list of SHAP value tensors, each of\n        which are the same shape as X. If ranked_outputs is None then this list of tensors matches\n        the number of model outputs. If ranked_outputs is a positive integer a pair is returned\n        (shap_values, indexes), where shap_values is a list of tensors with a length of\n        ranked_outputs, and indexes is a matrix that tells for each sample which output indexes\n        were chosen as \"top\".\n        \"\"\"\n        return self.explainer.shap_values(X, nsamples, ranked_outputs, output_rank_order, rseed)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef force_plot(base_value, shap_values, features=None, feature_names=None, out_names=None, link=\"identity\",\n               plot_cmap=\"RdBu\", matplotlib=False, show=True, figsize=(20,3), ordering_keys=None, ordering_keys_time_format=None,\n               text_rotation=0):\n    \"\"\" Visualize the given SHAP values with an additive force layout.\n    \n    Parameters\n    ----------\n    base_value : float\n        This is the reference value that the feature contributions start from. For SHAP values it should\n        be the value of explainer.expected_value.\n\n    shap_values : numpy.array\n        Matrix of SHAP values (# features) or (# samples x # features). If this is a 1D array then a single\n        force plot will be drawn, if it is a 2D array then a stacked force plot will be drawn.\n\n    features : numpy.array\n        Matrix of feature values (# features) or (# samples x # features). This provides the values of all the\n        features, and should be the same shape as the shap_values argument.\n\n    feature_names : list\n        List of feature names (# features).\n\n    out_names : str\n        The name of the outout of the model (plural to support multi-output plotting in the future).\n    \n    link : \"identity\" or \"logit\"\n        The transformation used when drawing the tick mark labels. Using logit will change log-odds numbers\n        into probabilities. \n\n    matplotlib : bool\n        Whether to use the default Javascript output, or the (less developed) matplotlib output. Using matplotlib\n        can be helpful in scenarios where rendering Javascript/HTML is inconvenient. \n\n    \"\"\"\n\n    # auto unwrap the base_value\n    if type(base_value) == np.ndarray and len(base_value) == 1:\n        base_value = base_value[0]\n\n    if (type(base_value) == np.ndarray or type(base_value) == list):\n        if type(shap_values) != list or len(shap_values) != len(base_value):\n            raise Exception(\"In v0.20 force_plot now requires the base value as the first parameter! \" \\\n                            \"Try shap.force_plot(explainer.expected_value, shap_values) or \" \\\n                            \"for multi-output models try \" \\\n                            \"shap.force_plot(explainer.expected_value[0], shap_values[0]).\")\n\n\n    assert not type(shap_values) == list, \"The shap_values arg looks looks multi output, try shap_values[i].\"\n\n    link = convert_to_link(link)\n\n    if type(shap_values) != np.ndarray:\n        return visualize(shap_values)\n\n    # convert from a DataFrame or other types\n    if str(type(features)) == \"<class 'pandas.core.frame.DataFrame'>\":\n        if feature_names is None:\n            feature_names = list(features.columns)\n        features = features.values\n    elif str(type(features)) == \"<class 'pandas.core.series.Series'>\":\n        if feature_names is None:\n            feature_names = list(features.index)\n        features = features.values\n    elif isinstance(features, list):\n        if feature_names is None:\n            feature_names = features\n        features = None\n    elif features is not None and len(features.shape) == 1 and feature_names is None:\n        feature_names = features\n        features = None\n\n    if len(shap_values.shape) == 1:\n        shap_values = np.reshape(shap_values, (1, len(shap_values)))\n\n    if out_names is None:\n        out_names = [\"output value\"]\n    elif type(out_names) == str:\n        out_names = [out_names]\n\n    if shap_values.shape[0] == 1:\n        if feature_names is None:\n            feature_names = [labels['FEATURE'] % str(i) for i in range(shap_values.shape[1])]\n        if features is None:\n            features = [\"\" for _ in range(len(feature_names))]\n        if type(features) == np.ndarray:\n            features = features.flatten()\n\n        # check that the shape of the shap_values and features match\n        if len(features) != shap_values.shape[1]:\n            msg = \"Length of features is not equal to the length of shap_values!\"\n            if len(features) == shap_values.shape[1] - 1:\n                msg += \" You might be using an old format shap_values array with the base value \" \\\n                       \"as the last column. In this case just pass the array without the last column.\"\n            raise Exception(msg)\n\n        instance = Instance(np.zeros((1, len(feature_names))), features)\n        e = AdditiveExplanation(\n            base_value,\n            np.sum(shap_values[0, :]) + base_value,\n            shap_values[0, :],\n            None,\n            instance,\n            link,\n            Model(None, out_names),\n            DenseData(np.zeros((1, len(feature_names))), list(feature_names))\n        )\n        \n        return visualize(e, plot_cmap, matplotlib, figsize=figsize, show=show, text_rotation=text_rotation)\n        \n    else:\n        if matplotlib:\n            raise Exception(\"matplotlib = True is not yet supported for force plots with multiple samples!\")\n        \n        if shap_values.shape[0] > 3000:\n            warnings.warn(\"shap.force_plot is slow for many thousands of rows, try subsampling your data.\")\n\n        exps = []\n        for i in range(shap_values.shape[0]):\n            if feature_names is None:\n                feature_names = [labels['FEATURE'] % str(i) for i in range(shap_values.shape[1])]\n            if features is None:\n                display_features = [\"\" for i in range(len(feature_names))]\n            else:\n                display_features = features[i, :]\n\n            instance = Instance(np.ones((1, len(feature_names))), display_features)\n            e = AdditiveExplanation(\n                base_value,\n                np.sum(shap_values[i, :]) + base_value,\n                shap_values[i, :],\n                None,\n                instance,\n                link,\n                Model(None, out_names),\n                DenseData(np.ones((1, len(feature_names))), list(feature_names))\n            )\n            exps.append(e)\n        \n        return visualize(\n                    exps, \n                    plot_cmap=plot_cmap, \n                    ordering_keys=ordering_keys, \n                    ordering_keys_time_format=ordering_keys_time_format, \n                    text_rotation=text_rotation\n                )", "response": "Visualize the given SHAP values with a additive force plot."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsave html plots to an output file.", "response": "def save_html(out_file, plot_html):\n    \"\"\" Save html plots to an output file.\n    \"\"\"\n\n    internal_open = False\n    if type(out_file) == str:\n        out_file = open(out_file, \"w\")\n        internal_open = True\n    out_file.write(\"<html><head><script>\\n\")\n\n    # dump the js code\n    bundle_path = os.path.join(os.path.split(__file__)[0], \"resources\", \"bundle.js\")\n    with io.open(bundle_path, encoding=\"utf-8\") as f:\n        bundle_data = f.read()\n    out_file.write(bundle_data)\n    out_file.write(\"</script></head><body>\\n\")\n\n    out_file.write(plot_html.data)\n\n    out_file.write(\"</body></html>\\n\")\n\n    if internal_open:\n        out_file.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef tensors_blocked_by_false(ops):\n    blocked = []\n    def recurse(op):\n        if op.type == \"Switch\":\n            blocked.append(op.outputs[1]) # the true path is blocked since we assume the ops we trace are False\n        else:\n            for out in op.outputs:\n                for c in out.consumers():\n                    recurse(c)\n    for op in ops:\n        recurse(op)\n\n    return blocked", "response": "Follows a set of ops assuming their value is False and finds blocked Switch paths."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef softmax(explainer, op, *grads):\n    in0 = op.inputs[0]\n    in0_max = tf.reduce_max(in0, axis=-1, keepdims=True, name=\"in0_max\")\n    in0_centered = in0 - in0_max\n    evals = tf.exp(in0_centered, name=\"custom_exp\")\n    rsum = tf.reduce_sum(evals, axis=-1, keepdims=True)\n    div = evals / rsum\n    explainer.between_ops.extend([evals.op, rsum.op, div.op, in0_centered.op]) # mark these as in-between the inputs and outputs\n    out = tf.gradients(div, in0_centered, grad_ys=grads[0])[0]\n    del explainer.between_ops[-4:]\n\n    # rescale to account for our shift by in0_max (which we did for numerical stability)\n    xin0,rin0 = tf.split(in0, 2)\n    xin0_centered,rin0_centered = tf.split(in0_centered, 2)\n    delta_in0 = xin0 - rin0\n    dup0 = [2] + [1 for i in delta_in0.shape[1:]]\n    return tf.where(\n        tf.tile(tf.abs(delta_in0), dup0) < 1e-6,\n        out,\n        out * tf.tile((xin0_centered - rin0_centered) / delta_in0, dup0)\n    )", "response": "This function takes a softmax op and returns a tuple of the components that can be used to create a new graph."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _variable_inputs(self, op):\n        if op.name not in self._vinputs:\n            self._vinputs[op.name] = np.array([t.op in self.between_ops or t in self.model_inputs for t in op.inputs])\n        return self._vinputs[op.name]", "response": "Return the list of variable inputs of this operation."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the SHAP value computation graph for a given model output.", "response": "def phi_symbolic(self, i):\n        \"\"\" Get the SHAP value computation graph for a given model output.\n        \"\"\"\n        if self.phi_symbolics[i] is None:\n\n            # replace the gradients for all the non-linear activations\n            # we do this by hacking our way into the registry (TODO: find a public API for this if it exists)\n            reg = tf_ops._gradient_registry._registry\n            for n in op_handlers:\n                if n in reg:\n                    self.orig_grads[n] = reg[n][\"type\"]\n                    if op_handlers[n] is not passthrough:\n                        reg[n][\"type\"] = self.custom_grad\n                elif n in self.used_types:\n                    raise Exception(n + \" was used in the model but is not in the gradient registry!\")\n            # In TensorFlow 1.10 they started pruning out nodes that they think can't be backpropped\n            # unfortunately that includes the index of embedding layers so we disable that check here\n            if hasattr(tf_gradients_impl, \"_IsBackpropagatable\"):\n                orig_IsBackpropagatable = tf_gradients_impl._IsBackpropagatable\n                tf_gradients_impl._IsBackpropagatable = lambda tensor: True\n            \n            # define the computation graph for the attribution values using custom a gradient-like computation\n            try:\n                out = self.model_output[:,i] if self.multi_output else self.model_output\n                self.phi_symbolics[i] = tf.gradients(out, self.model_inputs)\n\n            finally:\n\n                # reinstate the backpropagatable check\n                if hasattr(tf_gradients_impl, \"_IsBackpropagatable\"):\n                    tf_gradients_impl._IsBackpropagatable = orig_IsBackpropagatable\n\n                # restore the original gradient definitions\n                for n in op_handlers:\n                    if n in reg:\n                        reg[n][\"type\"] = self.orig_grads[n]\n        return self.phi_symbolics[i]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun the model while also setting the learning phase flags to False.", "response": "def run(self, out, model_inputs, X):\n        \"\"\" Runs the model while also setting the learning phase flags to False.\n        \"\"\"\n        feed_dict = dict(zip(model_inputs, X))\n        for t in self.learning_phase_flags:\n            feed_dict[t] = False\n        return self.session.run(out, feed_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npassing a gradient op creation request to the correct handler.", "response": "def custom_grad(self, op, *grads):\n        \"\"\" Passes a gradient op creation request to the correct handler.\n        \"\"\"\n        return op_handlers[op.type](self, op, *grads)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun the experiments on the remote machines in parallel.", "response": "def run_remote_experiments(experiments, thread_hosts, rate_limit=10):\n    \"\"\" Use ssh to run the experiments on remote machines in parallel.\n\n    Parameters\n    ----------\n    experiments : iterable\n        Output of shap.benchmark.experiments(...).\n\n    thread_hosts : list of strings\n        Each host has the format \"host_name:path_to_python_binary\" and can appear multiple times\n        in the list (one for each parallel execution you want on that machine).\n\n    rate_limit : int\n        How many ssh connections we make per minute to each host (to avoid throttling issues).\n    \"\"\"\n\n    global ssh_conn_per_min_limit\n    ssh_conn_per_min_limit = rate_limit\n    \n    # first we kill any remaining workers from previous runs\n    # note we don't check_call because pkill kills our ssh call as well\n    thread_hosts = copy.copy(thread_hosts)\n    random.shuffle(thread_hosts)\n    for host in set(thread_hosts):\n        hostname,_ = host.split(\":\")\n        try:\n            subprocess.run([\"ssh\", hostname, \"pkill -f shap.benchmark.run_experiment\"], timeout=15)\n        except subprocess.TimeoutExpired:\n            print(\"Failed to connect to\", hostname, \"after 15 seconds! Exiting.\")\n            return\n    \n    experiments = copy.copy(list(experiments))\n    random.shuffle(experiments) # this way all the hard experiments don't get put on one machine\n    global nexperiments, total_sent, total_done, total_failed, host_records\n    nexperiments = len(experiments)\n    total_sent = 0\n    total_done = 0\n    total_failed = 0\n    host_records = {}\n\n    q = Queue()\n\n    for host in thread_hosts:\n        worker = Thread(target=__thread_worker, args=(q, host))\n        worker.setDaemon(True)\n        worker.start()\n\n    for experiment in experiments:\n        q.put(experiment)\n\n    q.join()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef monitoring_plot(ind, shap_values, features, feature_names=None):\n    \n    if str(type(features)).endswith(\"'pandas.core.frame.DataFrame'>\"):\n        if feature_names is None:\n            feature_names = features.columns\n        features = features.values\n        \n    pl.figure(figsize=(10,3))\n    ys = shap_values[:,ind]\n    xs = np.arange(len(ys))#np.linspace(0, 12*2, len(ys))\n    \n    pvals = []\n    inc = 50\n    for i in range(inc, len(ys)-inc, inc):\n        #stat, pval = scipy.stats.mannwhitneyu(v[:i], v[i:], alternative=\"two-sided\")\n        stat, pval = scipy.stats.ttest_ind(ys[:i], ys[i:])\n        pvals.append(pval)\n    min_pval = np.min(pvals)\n    min_pval_ind = np.argmin(pvals)*inc + inc\n    \n    if min_pval < 0.05 / shap_values.shape[1]:\n        pl.axvline(min_pval_ind, linestyle=\"dashed\", color=\"#666666\", alpha=0.2)\n        \n    pl.scatter(xs, ys, s=10, c=features[:,ind], cmap=colors.red_blue)\n    \n    pl.xlabel(\"Sample index\")\n    pl.ylabel(truncate_text(feature_names[ind], 30) + \"\\nSHAP value\", size=13)\n    pl.gca().xaxis.set_ticks_position('bottom')\n    pl.gca().yaxis.set_ticks_position('left')\n    pl.gca().spines['right'].set_visible(False)\n    pl.gca().spines['top'].set_visible(False)\n    cb = pl.colorbar()\n    cb.outline.set_visible(False)\n    bbox = cb.ax.get_window_extent().transformed(pl.gcf().dpi_scale_trans.inverted())\n    cb.ax.set_aspect((bbox.height - 0.7) * 20)\n    cb.set_label(truncate_text(feature_names[ind], 30), size=13)\n    pl.show()", "response": "Create a SHAP monitoring plot."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef kmeans(X, k, round_values=True):\n\n    group_names = [str(i) for i in range(X.shape[1])]\n    if str(type(X)).endswith(\"'pandas.core.frame.DataFrame'>\"):\n        group_names = X.columns\n        X = X.values\n    kmeans = KMeans(n_clusters=k, random_state=0).fit(X)\n\n    if round_values:\n        for i in range(k):\n            for j in range(X.shape[1]):\n                ind = np.argmin(np.abs(X[:,j] - kmeans.cluster_centers_[i,j]))\n                kmeans.cluster_centers_[i,j] = X[ind,j]\n    return DenseData(kmeans.cluster_centers_, group_names, None, 1.0*np.bincount(kmeans.labels_))", "response": "Summarize a dataset with k mean samples weighted by the number of data points they\n    each represent."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nestimating the SHAP values for a set of samples.", "response": "def shap_values(self, X, **kwargs):\n        \"\"\" Estimate the SHAP values for a set of samples.\n\n        Parameters\n        ----------\n        X : numpy.array or pandas.DataFrame or any scipy.sparse matrix\n            A matrix of samples (# samples x # features) on which to explain the model's output.\n\n        nsamples : \"auto\" or int\n            Number of times to re-evaluate the model when explaining each prediction. More samples\n            lead to lower variance estimates of the SHAP values. The \"auto\" setting uses\n            `nsamples = 2 * X.shape[1] + 2048`.\n\n        l1_reg : \"num_features(int)\", \"auto\" (default for now, but deprecated), \"aic\", \"bic\", or float\n            The l1 regularization to use for feature selection (the estimation procedure is based on\n            a debiased lasso). The auto option currently uses \"aic\" when less that 20% of the possible sample\n            space is enumerated, otherwise it uses no regularization. THE BEHAVIOR OF \"auto\" WILL CHANGE\n            in a future version to be based on num_features instead of AIC.\n            The \"aic\" and \"bic\" options use the AIC and BIC rules for regularization.\n            Using \"num_features(int)\" selects a fix number of top features. Passing a float directly sets the\n            \"alpha\" parameter of the sklearn.linear_model.Lasso model used for feature selection.\n\n        Returns\n        -------\n        For models with a single output this returns a matrix of SHAP values\n        (# samples x # features). Each row sums to the difference between the model output for that\n        sample and the expected value of the model output (which is stored as expected_value\n        attribute of the explainer). For models with vector outputs this returns a list\n        of such matrices, one for each output.\n        \"\"\"\n\n        # convert dataframes\n        if str(type(X)).endswith(\"pandas.core.series.Series'>\"):\n            X = X.values\n        elif str(type(X)).endswith(\"'pandas.core.frame.DataFrame'>\"):\n            if self.keep_index:\n                index_value = X.index.values\n                index_name = X.index.name\n                column_name = list(X.columns)\n            X = X.values\n        \n        x_type = str(type(X))\n        arr_type = \"'numpy.ndarray'>\"\n        # if sparse, convert to lil for performance\n        if sp.sparse.issparse(X) and not sp.sparse.isspmatrix_lil(X):\n            X = X.tolil()\n        assert x_type.endswith(arr_type) or sp.sparse.isspmatrix_lil(X), \"Unknown instance type: \" + x_type\n        assert len(X.shape) == 1 or len(X.shape) == 2, \"Instance must have 1 or 2 dimensions!\"\n\n        # single instance\n        if len(X.shape) == 1:\n            data = X.reshape((1, X.shape[0]))\n            if self.keep_index:\n                data = convert_to_instance_with_index(data, column_name, index_name, index_value)\n            explanation = self.explain(data, **kwargs)\n\n            # vector-output\n            s = explanation.shape\n            if len(s) == 2:\n                outs = [np.zeros(s[0]) for j in range(s[1])]\n                for j in range(s[1]):\n                    outs[j] = explanation[:, j]\n                return outs\n\n            # single-output\n            else:\n                out = np.zeros(s[0])\n                out[:] = explanation\n                return out\n\n        # explain the whole dataset\n        elif len(X.shape) == 2:\n            explanations = []\n            for i in tqdm(range(X.shape[0]), disable=kwargs.get(\"silent\", False)):\n                data = X[i:i + 1, :]\n                if self.keep_index:\n                    data = convert_to_instance_with_index(data, column_name, index_value[i:i + 1], index_name)\n                explanations.append(self.explain(data, **kwargs))\n\n            # vector-output\n            s = explanations[0].shape\n            if len(s) == 2:\n                outs = [np.zeros((X.shape[0], s[0])) for j in range(s[1])]\n                for i in range(X.shape[0]):\n                    for j in range(s[1]):\n                        outs[j][i] = explanations[i][:, j]\n                return outs\n\n            # single-output\n            else:\n                out = np.zeros((X.shape[0], s[0]))\n                for i in range(X.shape[0]):\n                    out[i] = explanations[i]\n                return out"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nplotting the embedding of a single feature in a 2D dataset.", "response": "def embedding_plot(ind, shap_values, feature_names=None, method=\"pca\", alpha=1.0, show=True):\n    \"\"\" Use the SHAP values as an embedding which we project to 2D for visualization.\n\n    Parameters\n    ----------\n    ind : int or string\n        If this is an int it is the index of the feature to use to color the embedding.\n        If this is a string it is either the name of the feature, or it can have the\n        form \"rank(int)\" to specify the feature with that rank (ordered by mean absolute\n        SHAP value over all the samples), or \"sum()\" to mean the sum of all the SHAP values,\n        which is the model's output (minus it's expected value).\n\n    shap_values : numpy.array\n        Matrix of SHAP values (# samples x # features).\n\n    feature_names : None or list\n        The names of the features in the shap_values array.\n\n    method : \"pca\" or numpy.array\n        How to reduce the dimensions of the shap_values to 2D. If \"pca\" then the 2D\n        PCA projection of shap_values is used. If a numpy array then is should be\n        (# samples x 2) and represent the embedding of that values. \n\n    alpha : float\n        The transparency of the data points (between 0 and 1). This can be useful to the\n        show density of the data points when using a large dataset.\n    \"\"\"\n    \n    if feature_names is None:\n        feature_names = [labels['FEATURE'] % str(i) for i in range(shap_values.shape[1])]\n    \n    ind = convert_name(ind, shap_values, feature_names)\n    if ind == \"sum()\":\n        cvals = shap_values.sum(1)\n        fname = \"sum(SHAP values)\"\n    else:\n        cvals = shap_values[:,ind]\n        fname = feature_names[ind]\n    \n    # see if we need to compute the embedding\n    if type(method) == str and method == \"pca\":\n        pca = sklearn.decomposition.PCA(2)\n        embedding_values = pca.fit_transform(shap_values)\n    elif hasattr(method, \"shape\") and method.shape[1] == 2:\n        embedding_values = method\n    else:\n        print(\"Unsupported embedding method:\", method)\n\n    pl.scatter(\n        embedding_values[:,0], embedding_values[:,1], c=cvals,\n        cmap=colors.red_blue, alpha=alpha, linewidth=0\n    )\n    pl.axis(\"off\")\n    #pl.title(feature_names[ind])\n    cb = pl.colorbar()\n    cb.set_label(\"SHAP value for\\n\"+fname, size=13)\n    cb.outline.set_visible(False)\n    \n    \n    pl.gcf().set_size_inches(7.5, 5)\n    bbox = cb.ax.get_window_extent().transformed(pl.gcf().dpi_scale_trans.inverted())\n    cb.ax.set_aspect((bbox.height - 0.7) * 10)\n    cb.set_alpha(1)\n    if show:\n        pl.show()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dependence_plot(ind, shap_values, features, feature_names=None, display_features=None,\n                    interaction_index=\"auto\",\n                    color=\"#1E88E5\", axis_color=\"#333333\", cmap=colors.red_blue,\n                    dot_size=16, x_jitter=0, alpha=1, title=None, xmin=None, xmax=None, show=True):\n    \"\"\" Create a SHAP dependence plot, colored by an interaction feature.\n\n    Plots the value of the feature on the x-axis and the SHAP value of the same feature\n    on the y-axis. This shows how the model depends on the given feature, and is like a\n    richer extenstion of the classical parital dependence plots. Vertical dispersion of the\n    data points represents interaction effects. Grey ticks along the y-axis are data\n    points where the feature's value was NaN.\n\n\n    Parameters\n    ----------\n    ind : int or string\n        If this is an int it is the index of the feature to plot. If this is a string it is\n        either the name of the feature to plot, or it can have the form \"rank(int)\" to specify\n        the feature with that rank (ordered by mean absolute SHAP value over all the samples).\n\n    shap_values : numpy.array\n        Matrix of SHAP values (# samples x # features).\n\n    features : numpy.array or pandas.DataFrame\n        Matrix of feature values (# samples x # features).\n\n    feature_names : list\n        Names of the features (length # features).\n\n    display_features : numpy.array or pandas.DataFrame\n        Matrix of feature values for visual display (such as strings instead of coded values).\n\n    interaction_index : \"auto\", None, int, or string\n        The index of the feature used to color the plot. The name of a feature can also be passed\n        as a string. If \"auto\" then shap.common.approximate_interactions is used to pick what\n        seems to be the strongest interaction (note that to find to true stongest interaction you\n        need to compute the SHAP interaction values).\n        \n    x_jitter : float (0 - 1)\n        Adds random jitter to feature values. May increase plot readability when feature\n        is discrete.\n\n    alpha : float\n        The transparency of the data points (between 0 and 1). This can be useful to the\n        show density of the data points when using a large dataset.\n\n    xmin : float or string\n        Represents the lower bound of the plot's x-axis. It can be a string of the format\n        \"percentile(float)\" to denote that percentile of the feature's value used on the x-axis.\n\n    xmax : float or string\n        Represents the upper bound of the plot's x-axis. It can be a string of the format\n        \"percentile(float)\" to denote that percentile of the feature's value used on the x-axis.\n    \"\"\"\n\n    # convert from DataFrames if we got any\n    if str(type(features)).endswith(\"'pandas.core.frame.DataFrame'>\"):\n        if feature_names is None:\n            feature_names = features.columns\n        features = features.values\n    if str(type(display_features)).endswith(\"'pandas.core.frame.DataFrame'>\"):\n        if feature_names is None:\n            feature_names = display_features.columns\n        display_features = display_features.values\n    elif display_features is None:\n        display_features = features\n\n    if feature_names is None:\n        feature_names = [labels['FEATURE'] % str(i) for i in range(shap_values.shape[1])]\n\n    # allow vectors to be passed\n    if len(shap_values.shape) == 1:\n        shap_values = np.reshape(shap_values, len(shap_values), 1)\n    if len(features.shape) == 1:\n        features = np.reshape(features, len(features), 1)\n\n    ind = convert_name(ind, shap_values, feature_names)\n    \n    # plotting SHAP interaction values\n    if len(shap_values.shape) == 3 and len(ind) == 2:\n        ind1 = convert_name(ind[0], shap_values, feature_names)\n        ind2 = convert_name(ind[1], shap_values, feature_names)\n        if ind1 == ind2:\n            proj_shap_values = shap_values[:, ind2, :]\n        else:\n            proj_shap_values = shap_values[:, ind2, :] * 2  # off-diag values are split in half\n\n        # TODO: remove recursion; generally the functions should be shorter for more maintainable code\n        dependence_plot(\n            ind1, proj_shap_values, features, feature_names=feature_names,\n            interaction_index=ind2, display_features=display_features, show=False,\n            xmin=xmin, xmax=xmax\n        )\n        if ind1 == ind2:\n            pl.ylabel(labels['MAIN_EFFECT'] % feature_names[ind1])\n        else:\n            pl.ylabel(labels['INTERACTION_EFFECT'] % (feature_names[ind1], feature_names[ind2]))\n\n        if show:\n            pl.show()\n        return\n\n    assert shap_values.shape[0] == features.shape[0], \\\n        \"'shap_values' and 'features' values must have the same number of rows!\"\n    assert shap_values.shape[1] == features.shape[1], \\\n        \"'shap_values' must have the same number of columns as 'features'!\"\n\n    # get both the raw and display feature values\n    oinds = np.arange(shap_values.shape[0]) # we randomize the ordering so plotting overlaps are not related to data ordering\n    np.random.shuffle(oinds)\n    xv = features[oinds, ind].astype(np.float64)\n    xd = display_features[oinds, ind]\n    s = shap_values[oinds, ind]\n    if type(xd[0]) == str:\n        name_map = {}\n        for i in range(len(xv)):\n            name_map[xd[i]] = xv[i]\n        xnames = list(name_map.keys())\n\n    # allow a single feature name to be passed alone\n    if type(feature_names) == str:\n        feature_names = [feature_names]\n    name = feature_names[ind]\n\n    # guess what other feature as the stongest interaction with the plotted feature\n    if interaction_index == \"auto\":\n        interaction_index = approximate_interactions(ind, shap_values, features)[0]\n    interaction_index = convert_name(interaction_index, shap_values, feature_names)\n    categorical_interaction = False\n\n    # get both the raw and display color values\n    color_norm = None\n    if interaction_index is not None:\n        cv = features[:, interaction_index]\n        cd = display_features[:, interaction_index]\n        clow = np.nanpercentile(cv.astype(np.float), 5)\n        chigh = np.nanpercentile(cv.astype(np.float), 95)\n        if type(cd[0]) == str:\n            cname_map = {}\n            for i in range(len(cv)):\n                cname_map[cd[i]] = cv[i]\n            cnames = list(cname_map.keys())\n            categorical_interaction = True\n        elif clow % 1 == 0 and chigh % 1 == 0 and chigh - clow < 10:\n            categorical_interaction = True\n\n        # discritize colors for categorical features\n        if categorical_interaction and clow != chigh:\n            clow = np.nanmin(cv.astype(np.float))\n            chigh = np.nanmax(cv.astype(np.float))\n            bounds = np.linspace(clow, chigh, int(chigh - clow + 2))\n            color_norm = matplotlib.colors.BoundaryNorm(bounds, cmap.N-1)\n\n    # optionally add jitter to feature values\n    if x_jitter > 0:\n        if x_jitter > 1: x_jitter = 1\n        xvals = xv.copy()\n        if isinstance(xvals[0], float):\n            xvals = xvals.astype(np.float)\n            xvals = xvals[~np.isnan(xvals)]\n        xvals = np.unique(xvals)\n        if len(xvals) >= 2:\n            smallest_diff = np.min(np.diff(np.sort(xvals)))\n            jitter_amount = x_jitter * smallest_diff\n            xv += (np.random.ranf(size = len(xv))*jitter_amount) - (jitter_amount/2)\n\n    # the actual scatter plot, TODO: adapt the dot_size to the number of data points?\n    xv_nan = np.isnan(xv)\n    xv_notnan = np.invert(xv_nan)\n    if interaction_index is not None:\n\n        # plot the nan values in the interaction feature as grey\n        cvals = features[oinds, interaction_index].astype(np.float64)\n        cvals_imp = cvals.copy()\n        cvals_imp[np.isnan(cvals)] = (clow + chigh) / 2.0\n        cvals[cvals_imp > chigh] = chigh\n        cvals[cvals_imp < clow] = clow\n        p = pl.scatter(\n            xv[xv_notnan], s[xv_notnan], s=dot_size, linewidth=0, c=cvals[xv_notnan],\n            cmap=cmap, alpha=alpha, vmin=clow, vmax=chigh,\n            norm=color_norm, rasterized=len(xv) > 500\n        )\n        p.set_array(cvals[xv_notnan])\n    else:\n        pl.scatter(xv, s, s=dot_size, linewidth=0, color=color,\n                   alpha=alpha, rasterized=len(xv) > 500)\n\n    if interaction_index != ind and interaction_index is not None:\n        # draw the color bar\n        if type(cd[0]) == str:\n            tick_positions = [cname_map[n] for n in cnames]\n            if len(tick_positions) == 2:\n                tick_positions[0] -= 0.25\n                tick_positions[1] += 0.25\n            cb = pl.colorbar(ticks=tick_positions)\n            cb.set_ticklabels(cnames)\n        else:\n            cb = pl.colorbar()\n\n        cb.set_label(feature_names[interaction_index], size=13)\n        cb.ax.tick_params(labelsize=11)\n        if categorical_interaction:\n            cb.ax.tick_params(length=0)\n        cb.set_alpha(1)\n        cb.outline.set_visible(False)\n        bbox = cb.ax.get_window_extent().transformed(pl.gcf().dpi_scale_trans.inverted())\n        cb.ax.set_aspect((bbox.height - 0.7) * 20)\n\n    # handles any setting of xmax and xmin\n    # note that we handle None,float, or \"percentile(float)\" formats\n    if xmin is not None or xmax is not None:\n        if type(xmin) == str and xmin.startswith(\"percentile\"):\n            xmin = np.nanpercentile(xv, float(xmin[11:-1]))\n        if type(xmax) == str and xmax.startswith(\"percentile\"):\n            xmax = np.nanpercentile(xv, float(xmax[11:-1]))\n\n        if xmin is None or xmin == np.nanmin(xv):\n            xmin = np.nanmin(xv) - (xmax - np.nanmin(xv))/20\n        if xmax is None or xmax == np.nanmax(xv):\n            xmax = np.nanmax(xv) + (np.nanmax(xv) - xmin)/20\n        \n        pl.xlim(xmin, xmax)\n\n    # plot any nan feature values as tick marks along the y-axis\n    xlim = pl.xlim()\n    if interaction_index is not None:\n        p = pl.scatter(\n            xlim[0] * np.ones(xv_nan.sum()), s[xv_nan], marker=1,\n            linewidth=2, c=cvals_imp[xv_nan], cmap=cmap, alpha=alpha,\n            vmin=clow, vmax=chigh\n        )\n        p.set_array(cvals[xv_nan])\n    else:\n        pl.scatter(\n            xlim[0] * np.ones(xv_nan.sum()), s[xv_nan], marker=1,\n            linewidth=2, color=color, alpha=alpha\n        )\n    pl.xlim(*xlim)\n\n    # make the plot more readable\n    if interaction_index != ind:\n        pl.gcf().set_size_inches(7.5, 5)\n    else:\n        pl.gcf().set_size_inches(6, 5)\n    pl.xlabel(name, color=axis_color, fontsize=13)\n    pl.ylabel(labels['VALUE_FOR'] % name, color=axis_color, fontsize=13)\n    if title is not None:\n        pl.title(title, color=axis_color, fontsize=13)\n    pl.gca().xaxis.set_ticks_position('bottom')\n    pl.gca().yaxis.set_ticks_position('left')\n    pl.gca().spines['right'].set_visible(False)\n    pl.gca().spines['top'].set_visible(False)\n    pl.gca().tick_params(color=axis_color, labelcolor=axis_color, labelsize=11)\n    for spine in pl.gca().spines.values():\n        spine.set_edgecolor(axis_color)\n    if type(xd[0]) == str:\n        pl.xticks([name_map[n] for n in xnames], xnames, rotation='vertical', fontsize=11)\n    if show:\n        with warnings.catch_warnings(): # ignore expected matplotlib warnings\n            warnings.simplefilter(\"ignore\", RuntimeWarning)\n            pl.show()", "response": "Creates a SHAP dependence plot for a given feature."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef runtime(X, y, model_generator, method_name):\n\n    old_seed = np.random.seed()\n    np.random.seed(3293)\n\n    # average the method scores over several train/test splits\n    method_reps = []\n    for i in range(1):\n        X_train, X_test, y_train, _ = train_test_split(__toarray(X), y, test_size=100, random_state=i)\n\n        # define the model we are going to explain\n        model = model_generator()\n        model.fit(X_train, y_train)\n\n        # evaluate each method\n        start = time.time()\n        explainer = getattr(methods, method_name)(model, X_train)\n        build_time = time.time() - start\n\n        start = time.time()\n        explainer(X_test)\n        explain_time = time.time() - start\n\n        # we always normalize the explain time as though we were explaining 1000 samples\n        # even if to reduce the runtime of the benchmark we do less (like just 100)\n        method_reps.append(build_time + explain_time * 1000.0 / X_test.shape[0])\n    np.random.seed(old_seed)\n\n    return None, np.mean(method_reps)", "response": "Compute the runtime of the given method"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef local_accuracy(X, y, model_generator, method_name):\n\n    def score_map(true, pred):\n        \"\"\" Converts local accuracy from % of standard deviation to numerical scores for coloring.\n        \"\"\"\n\n        v = min(1.0, np.std(pred - true) / (np.std(true) + 1e-8))\n        if v < 1e-6:\n            return 1.0\n        elif v < 0.01:\n            return 0.9\n        elif v < 0.05:\n            return 0.75\n        elif v < 0.1:\n            return 0.6\n        elif v < 0.2:\n            return 0.4\n        elif v < 0.3:\n            return 0.3\n        elif v < 0.5:\n            return 0.2\n        elif v < 0.7:\n            return 0.1\n        else:\n            return 0.0\n    def score_function(X_train, X_test, y_train, y_test, attr_function, trained_model, random_state):\n        return measures.local_accuracy(\n            X_train, y_train, X_test, y_test, attr_function(X_test),\n            model_generator, score_map, trained_model\n        )\n    return None, __score_method(X, y, None, model_generator, score_function, method_name)", "response": "Local Accuracy\n    transform = \"identity\"\n    sort_order = 2"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nkeep Negative (mask) xlabel = \"Max fraction of features kept\" ylabel = \"Negative mean model output\" transform = \"negate\" sort_order = 5", "response": "def keep_negative_mask(X, y, model_generator, method_name, num_fcounts=11):\n    \"\"\" Keep Negative (mask)\n    xlabel = \"Max fraction of features kept\"\n    ylabel = \"Negative mean model output\"\n    transform = \"negate\"\n    sort_order = 5\n    \"\"\"\n    return __run_measure(measures.keep_mask, X, y, model_generator, method_name, -1, num_fcounts, __mean_pred)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef keep_absolute_mask__r2(X, y, model_generator, method_name, num_fcounts=11):\n    return __run_measure(measures.keep_mask, X, y, model_generator, method_name, 0, num_fcounts, sklearn.metrics.r2_score)", "response": "Keep Absolute (mask)\n    xlabel = \"Max fraction of features kept\"\n    ylabel = \"R^2\"\n    transform = \"identity\"\n    sort_order = 6"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves Positive (mask) xlabel = \"Max fraction of features removed\" ylabel = \"Negative mean model output\" transform = \"negate\" sort_order = 7", "response": "def remove_positive_mask(X, y, model_generator, method_name, num_fcounts=11):\n    \"\"\" Remove Positive (mask)\n    xlabel = \"Max fraction of features removed\"\n    ylabel = \"Negative mean model output\"\n    transform = \"negate\"\n    sort_order = 7\n    \"\"\"\n    return __run_measure(measures.remove_mask, X, y, model_generator, method_name, 1, num_fcounts, __mean_pred)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove_absolute_mask__r2(X, y, model_generator, method_name, num_fcounts=11):\n    return __run_measure(measures.remove_mask, X, y, model_generator, method_name, 0, num_fcounts, sklearn.metrics.r2_score)", "response": "Remove Absolute (mask)\n    xlabel = \"Max fraction of features removed\"\n    ylabel = \"1 - R^2\"\n    transform = \"one_minus\"\n    sort_order = 9"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef keep_negative_resample(X, y, model_generator, method_name, num_fcounts=11):\n    return __run_measure(measures.keep_resample, X, y, model_generator, method_name, -1, num_fcounts, __mean_pred)", "response": "Keep Negative (resample)\n    xlabel = \"Max fraction of features kept\"\n    ylabel = \"Negative mean model output\"\n    transform = \"negate\"\n    sort_order = 11"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef keep_absolute_resample__r2(X, y, model_generator, method_name, num_fcounts=11):\n    return __run_measure(measures.keep_resample, X, y, model_generator, method_name, 0, num_fcounts, sklearn.metrics.r2_score)", "response": "Keep Absolute (resample)\n    xlabel = \"Max fraction of features kept\"\n    ylabel = \"R^2\"\n    transform = \"identity\"\n    sort_order = 12"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nkeeping Absolute (resample) xlabel = \"Max fraction of features kept\" ylabel = \"ROC AUC\" transform = \"identity\" sort_order = 12", "response": "def keep_absolute_resample__roc_auc(X, y, model_generator, method_name, num_fcounts=11):\n    \"\"\" Keep Absolute (resample)\n    xlabel = \"Max fraction of features kept\"\n    ylabel = \"ROC AUC\"\n    transform = \"identity\"\n    sort_order = 12\n    \"\"\"\n    return __run_measure(measures.keep_resample, X, y, model_generator, method_name, 0, num_fcounts, sklearn.metrics.roc_auc_score)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove_positive_resample(X, y, model_generator, method_name, num_fcounts=11):\n    return __run_measure(measures.remove_resample, X, y, model_generator, method_name, 1, num_fcounts, __mean_pred)", "response": "Remove Positive (resample)\n    xlabel = \"Max fraction of features removed\"\n    ylabel = \"Negative mean model output\"\n    transform = \"negate\"\n    sort_order = 13"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving Absolute (resample) xlabel = \"Max fraction of features removed\" ylabel = \"1 - R^2\" transform = \"one_minus\" sort_order = 15", "response": "def remove_absolute_resample__r2(X, y, model_generator, method_name, num_fcounts=11):\n    \"\"\" Remove Absolute (resample)\n    xlabel = \"Max fraction of features removed\"\n    ylabel = \"1 - R^2\"\n    transform = \"one_minus\"\n    sort_order = 15\n    \"\"\"\n    return __run_measure(measures.remove_resample, X, y, model_generator, method_name, 0, num_fcounts, sklearn.metrics.r2_score)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving Absolute (resample) xlabel = \"Max fraction of features removed\" ylabel = \"1 - ROC AUC\" transform = \"one_minus\" sort_order = 15", "response": "def remove_absolute_resample__roc_auc(X, y, model_generator, method_name, num_fcounts=11):\n    \"\"\" Remove Absolute (resample)\n    xlabel = \"Max fraction of features removed\"\n    ylabel = \"1 - ROC AUC\"\n    transform = \"one_minus\"\n    sort_order = 15\n    \"\"\"\n    return __run_measure(measures.remove_resample, X, y, model_generator, method_name, 0, num_fcounts, sklearn.metrics.roc_auc_score)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nkeeps Negative (impute) xlabel = \"Max fraction of features kept\" ylabel = \"Negative mean model output\" transform = \"negate\" sort_order = 17", "response": "def keep_negative_impute(X, y, model_generator, method_name, num_fcounts=11):\n    \"\"\" Keep Negative (impute)\n    xlabel = \"Max fraction of features kept\"\n    ylabel = \"Negative mean model output\"\n    transform = \"negate\"\n    sort_order = 17\n    \"\"\"\n    return __run_measure(measures.keep_impute, X, y, model_generator, method_name, -1, num_fcounts, __mean_pred)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef keep_absolute_impute__r2(X, y, model_generator, method_name, num_fcounts=11):\n    return __run_measure(measures.keep_impute, X, y, model_generator, method_name, 0, num_fcounts, sklearn.metrics.r2_score)", "response": "Keep Absolute (impute)\n    xlabel = \"Max fraction of features kept\"\n    ylabel = \"R^2\"\n    transform = \"identity\"\n    sort_order = 18"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef keep_absolute_impute__roc_auc(X, y, model_generator, method_name, num_fcounts=11):\n    return __run_measure(measures.keep_mask, X, y, model_generator, method_name, 0, num_fcounts, sklearn.metrics.roc_auc_score)", "response": "Keep Absolute (impute)\n    xlabel = \"Max fraction of features kept\"\n    ylabel = \"ROC AUC\"\n    transform = \"identity\"\n    sort_order = 19"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove_positive_impute(X, y, model_generator, method_name, num_fcounts=11):\n    return __run_measure(measures.remove_impute, X, y, model_generator, method_name, 1, num_fcounts, __mean_pred)", "response": "Remove Positive (impute)\n    xlabel = \"Max fraction of features removed\"\n    ylabel = \"Negative mean model output\"\n    transform = \"negate\"\n    sort_order = 7"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remove_absolute_impute__r2(X, y, model_generator, method_name, num_fcounts=11):\n    return __run_measure(measures.remove_impute, X, y, model_generator, method_name, 0, num_fcounts, sklearn.metrics.r2_score)", "response": "Remove Absolute (impute)\n    xlabel = \"Max fraction of features removed\"\n    ylabel = \"1 - R^2\"\n    transform = \"one_minus\"\n    sort_order = 9"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove_absolute_impute__roc_auc(X, y, model_generator, method_name, num_fcounts=11):\n    return __run_measure(measures.remove_mask, X, y, model_generator, method_name, 0, num_fcounts, sklearn.metrics.roc_auc_score)", "response": "Remove Absolute (impute)\n    xlabel = \"Max fraction of features removed\"\n    ylabel = \"1 - ROC AUC\"\n    transform = \"one_minus\"\n    sort_order = 9"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nkeeping Negative (retrain) xlabel = \"Max fraction of features kept\" ylabel = \"Negative mean model output\" transform = \"negate\" sort_order = 7", "response": "def keep_negative_retrain(X, y, model_generator, method_name, num_fcounts=11):\n    \"\"\" Keep Negative (retrain)\n    xlabel = \"Max fraction of features kept\"\n    ylabel = \"Negative mean model output\"\n    transform = \"negate\"\n    sort_order = 7\n    \"\"\"\n    return __run_measure(measures.keep_retrain, X, y, model_generator, method_name, -1, num_fcounts, __mean_pred)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef remove_positive_retrain(X, y, model_generator, method_name, num_fcounts=11):\n    return __run_measure(measures.remove_retrain, X, y, model_generator, method_name, 1, num_fcounts, __mean_pred)", "response": "Remove Positive (retrain)\n    xlabel = \"Max fraction of features removed\"\n    ylabel = \"Negative mean model output\"\n    transform = \"negate\"\n    sort_order = 11"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef batch_remove_absolute_retrain__r2(X, y, model_generator, method_name, num_fcounts=11):\n    return __run_batch_abs_metric(measures.batch_remove_retrain, X, y, model_generator, method_name, sklearn.metrics.r2_score, num_fcounts)", "response": "Batch Remove Absolute Retrain"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbatching Keep Absolute Retrain model.", "response": "def batch_keep_absolute_retrain__r2(X, y, model_generator, method_name, num_fcounts=11):\n    \"\"\" Batch Keep Absolute (retrain)\n    xlabel = \"Fraction of features kept\"\n    ylabel = \"R^2\"\n    transform = \"identity\"\n    sort_order = 13\n    \"\"\"\n    return __run_batch_abs_metric(measures.batch_keep_retrain, X, y, model_generator, method_name, sklearn.metrics.r2_score, num_fcounts)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbatches Remove Absolute Retrain", "response": "def batch_remove_absolute_retrain__roc_auc(X, y, model_generator, method_name, num_fcounts=11):\n    \"\"\" Batch Remove Absolute (retrain)\n    xlabel = \"Fraction of features removed\"\n    ylabel = \"1 - ROC AUC\"\n    transform = \"one_minus\"\n    sort_order = 13\n    \"\"\"\n    return __run_batch_abs_metric(measures.batch_remove_retrain, X, y, model_generator, method_name, sklearn.metrics.roc_auc_score, num_fcounts)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef batch_keep_absolute_retrain__roc_auc(X, y, model_generator, method_name, num_fcounts=11):\n    return __run_batch_abs_metric(measures.batch_keep_retrain, X, y, model_generator, method_name, sklearn.metrics.roc_auc_score, num_fcounts)", "response": "Batch Keep Absolute Retrain model."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntests an explanation method.", "response": "def __score_method(X, y, fcounts, model_generator, score_function, method_name, nreps=10, test_size=100, cache_dir=\"/tmp\"):\n    \"\"\" Test an explanation method.\n    \"\"\"\n\n    old_seed = np.random.seed()\n    np.random.seed(3293)\n\n    # average the method scores over several train/test splits\n    method_reps = []\n\n    data_hash = hashlib.sha256(__toarray(X).flatten()).hexdigest() + hashlib.sha256(__toarray(y)).hexdigest()\n    for i in range(nreps):\n        X_train, X_test, y_train, y_test = train_test_split(__toarray(X), y, test_size=test_size, random_state=i)\n\n        # define the model we are going to explain, caching so we onlu build it once\n        model_id = \"model_cache__v\" + \"__\".join([__version__, data_hash, model_generator.__name__])+\".pickle\"\n        cache_file = os.path.join(cache_dir, model_id + \".pickle\")\n        if os.path.isfile(cache_file):\n            with open(cache_file, \"rb\") as f:\n                model = pickle.load(f)\n        else:\n            model = model_generator()\n            model.fit(X_train, y_train)\n            with open(cache_file, \"wb\") as f:\n                pickle.dump(model, f)\n\n        attr_key = \"_\".join([model_generator.__name__, method_name, str(test_size), str(nreps), str(i), data_hash])\n        def score(attr_function):\n            def cached_attr_function(X_inner):\n                if attr_key not in _attribution_cache:\n                    _attribution_cache[attr_key] = attr_function(X_inner)\n                return _attribution_cache[attr_key]\n\n            #cached_attr_function = lambda X: __check_cache(attr_function, X)\n            if fcounts is None:\n                return score_function(X_train, X_test, y_train, y_test, cached_attr_function, model, i)\n            else:\n                scores = []\n                for f in fcounts:\n                    scores.append(score_function(f, X_train, X_test, y_train, y_test, cached_attr_function, model, i))\n                return np.array(scores)\n\n        # evaluate the method (only building the attribution function if we need to)\n        if attr_key not in _attribution_cache:\n            method_reps.append(score(getattr(methods, method_name)(model, X_train)))\n        else:\n            method_reps.append(score(None))\n\n    np.random.seed(old_seed)\n    return np.array(method_reps).mean(0)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef human_and_11(X, y, model_generator, method_name):\n    return _human_and(X, model_generator, method_name, True, True)", "response": "A human - readable version of the and - 11 metric."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef human_or_00(X, y, model_generator, method_name):\n    return _human_or(X, model_generator, method_name, False, False)", "response": "A human - readable version of _human_or that tests whether a feature attribution method agrees with linear effects."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef human_or_11(X, y, model_generator, method_name):\n    return _human_or(X, model_generator, method_name, True, True)", "response": "A human - readable version of the OR method."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef human_xor_00(X, y, model_generator, method_name):\n    return _human_xor(X, model_generator, method_name, False, False)", "response": "A human - readable XOR with the same parameters as the original XOR function."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsums (false/false) This tests how well a feature attribution method agrees with human intuition for a SUM operation. This metric deals specifically with the question of credit allocation for the following function when all three inputs are true: if fever: +2 points if cough: +2 points transform = \"identity\" sort_order = 0", "response": "def human_sum_00(X, y, model_generator, method_name):\n    \"\"\" SUM (false/false)\n\n    This tests how well a feature attribution method agrees with human intuition\n    for a SUM operation. This metric deals\n    specifically with the question of credit allocation for the following function\n    when all three inputs are true:\n    if fever: +2 points\n    if cough: +2 points\n\n    transform = \"identity\"\n    sort_order = 0\n    \"\"\"\n    return _human_sum(X, model_generator, method_name, False, False)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef human_sum_01(X, y, model_generator, method_name):\n    return _human_sum(X, model_generator, method_name, False, True)", "response": "A human - readable sum of the features."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef human_sum_11(X, y, model_generator, method_name):\n    return _human_sum(X, model_generator, method_name, True, True)", "response": "A human - readable sum of the features."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _estimate_transforms(self, nsamples):\n        M = len(self.coef)\n\n        mean_transform = np.zeros((M,M))\n        x_transform = np.zeros((M,M))\n        inds = np.arange(M, dtype=np.int)\n        for _ in tqdm(range(nsamples), \"Estimating transforms\"):\n            np.random.shuffle(inds)\n            cov_inv_SiSi = np.zeros((0,0))\n            cov_Si = np.zeros((M,0))\n            for j in range(M):\n                i = inds[j]\n\n                # use the last Si as the new S\n                cov_S = cov_Si\n                cov_inv_SS = cov_inv_SiSi\n\n                # get the new cov_Si\n                cov_Si = self.cov[:,inds[:j+1]]\n\n                # compute the new cov_inv_SiSi from cov_inv_SS\n                d = cov_Si[i,:-1].T\n                t = np.matmul(cov_inv_SS, d)\n                Z = self.cov[i, i]\n                u = Z - np.matmul(t.T, d)\n                cov_inv_SiSi = np.zeros((j+1, j+1))\n                if j > 0:\n                    cov_inv_SiSi[:-1, :-1] = cov_inv_SS + np.outer(t, t) / u\n                    cov_inv_SiSi[:-1, -1] = cov_inv_SiSi[-1,:-1] = -t / u\n                cov_inv_SiSi[-1, -1] = 1 / u\n\n                # + coef @ (Q(bar(Sui)) - Q(bar(S)))\n                mean_transform[i, i] += self.coef[i]\n\n                # + coef @ R(Sui)\n                coef_R_Si = np.matmul(self.coef[inds[j+1:]], np.matmul(cov_Si, cov_inv_SiSi)[inds[j+1:]])\n                mean_transform[i, inds[:j+1]] += coef_R_Si\n\n                # - coef @ R(S)\n                coef_R_S = np.matmul(self.coef[inds[j:]], np.matmul(cov_S, cov_inv_SS)[inds[j:]])\n                mean_transform[i, inds[:j]] -= coef_R_S\n\n                # - coef @ (Q(Sui) - Q(S))\n                x_transform[i, i] += self.coef[i]\n\n                # + coef @ R(Sui)\n                x_transform[i, inds[:j+1]] += coef_R_Si\n\n                # - coef @ R(S)\n                x_transform[i, inds[:j]] -= coef_R_S\n\n        mean_transform /= nsamples\n        x_transform /= nsamples\n        return mean_transform, x_transform", "response": "Estimate the transforms for the current set of class names."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nestimating the SHAP values for a set of samples.", "response": "def shap_values(self, X):\n        \"\"\" Estimate the SHAP values for a set of samples.\n\n        Parameters\n        ----------\n        X : numpy.array or pandas.DataFrame\n            A matrix of samples (# samples x # features) on which to explain the model's output.\n\n        Returns\n        -------\n        For models with a single output this returns a matrix of SHAP values\n        (# samples x # features). Each row sums to the difference between the model output for that\n        sample and the expected value of the model output (which is stored as expected_value\n        attribute of the explainer).\n        \"\"\"\n\n        # convert dataframes\n        if str(type(X)).endswith(\"pandas.core.series.Series'>\"):\n            X = X.values\n        elif str(type(X)).endswith(\"'pandas.core.frame.DataFrame'>\"):\n            X = X.values\n\n        #assert str(type(X)).endswith(\"'numpy.ndarray'>\"), \"Unknown instance type: \" + str(type(X))\n        assert len(X.shape) == 1 or len(X.shape) == 2, \"Instance must have 1 or 2 dimensions!\"\n\n        if self.feature_dependence == \"correlation\":\n            phi = np.matmul(np.matmul(X[:,self.valid_inds], self.avg_proj.T), self.x_transform.T) - self.mean_transformed\n            phi = np.matmul(phi, self.avg_proj)\n\n            full_phi = np.zeros(((phi.shape[0], self.M)))\n            full_phi[:,self.valid_inds] = phi\n\n            return full_phi\n\n        elif self.feature_dependence == \"independent\":\n            if len(self.coef.shape) == 1:\n                return np.array(X - self.mean) * self.coef\n            else:\n                return [np.array(X - self.mean) * self.coef[i] for i in range(self.coef.shape[0])]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef independentlinear60__ffnn():\n    from keras.models import Sequential\n    from keras.layers import Dense\n\n    model = Sequential()\n    model.add(Dense(32, activation='relu', input_dim=60))\n    model.add(Dense(20, activation='relu'))\n    model.add(Dense(20, activation='relu'))\n    model.add(Dense(1))\n\n    model.compile(optimizer='adam',\n                loss='mean_squared_error',\n                metrics=['mean_squared_error'])\n\n    return KerasWrap(model, 30, flatten_output=True)", "response": "4 - Layer Neural Network with 30 layers"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef human__decision_tree():\n\n    # build data\n    N = 1000000\n    M = 3\n    X = np.zeros((N,M))\n    X.shape\n    y = np.zeros(N)\n    X[0, 0] = 1\n    y[0] = 8\n    X[1, 1] = 1\n    y[1] = 8\n    X[2, 0:2] = 1\n    y[2] = 4\n\n    # fit model\n    xor_model = sklearn.tree.DecisionTreeRegressor(max_depth=2)\n    xor_model.fit(X, y)\n\n    return xor_model", "response": "Return Decision TreeRegressor for human - readable tree."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a summary plot from a set of SHAP values.", "response": "def summary_plot(shap_values, features=None, feature_names=None, max_display=None, plot_type=\"dot\",\n                 color=None, axis_color=\"#333333\", title=None, alpha=1, show=True, sort=True,\n                 color_bar=True, auto_size_plot=True, layered_violin_max_num_bins=20, class_names=None):\n    \"\"\"Create a SHAP summary plot, colored by feature values when they are provided.\n\n    Parameters\n    ----------\n    shap_values : numpy.array\n        Matrix of SHAP values (# samples x # features)\n\n    features : numpy.array or pandas.DataFrame or list\n        Matrix of feature values (# samples x # features) or a feature_names list as shorthand\n\n    feature_names : list\n        Names of the features (length # features)\n\n    max_display : int\n        How many top features to include in the plot (default is 20, or 7 for interaction plots)\n\n    plot_type : \"dot\" (default) or \"violin\"\n        What type of summary plot to produce\n    \"\"\"\n\n    multi_class = False\n    if isinstance(shap_values, list):\n        multi_class = True\n        plot_type = \"bar\" # only type supported for now\n    else:\n        assert len(shap_values.shape) != 1, \"Summary plots need a matrix of shap_values, not a vector.\"\n\n    # default color:\n    if color is None:\n        if plot_type == 'layered_violin':\n            color = \"coolwarm\"\n        elif multi_class:\n            color = lambda i: colors.red_blue_circle(i/len(shap_values))\n        else:\n            color = colors.blue_rgb\n\n    # convert from a DataFrame or other types\n    if str(type(features)) == \"<class 'pandas.core.frame.DataFrame'>\":\n        if feature_names is None:\n            feature_names = features.columns\n        features = features.values\n    elif isinstance(features, list):\n        if feature_names is None:\n            feature_names = features\n        features = None\n    elif (features is not None) and len(features.shape) == 1 and feature_names is None:\n        feature_names = features\n        features = None\n\n    num_features = (shap_values[0].shape[1] if multi_class else shap_values.shape[1])\n\n    if feature_names is None:\n        feature_names = np.array([labels['FEATURE'] % str(i) for i in range(num_features)])\n\n    # plotting SHAP interaction values\n    if not multi_class and len(shap_values.shape) == 3:\n        if max_display is None:\n            max_display = 7\n        else:\n            max_display = min(len(feature_names), max_display)\n\n        sort_inds = np.argsort(-np.abs(shap_values.sum(1)).sum(0))\n\n        # get plotting limits\n        delta = 1.0 / (shap_values.shape[1] ** 2)\n        slow = np.nanpercentile(shap_values, delta)\n        shigh = np.nanpercentile(shap_values, 100 - delta)\n        v = max(abs(slow), abs(shigh))\n        slow = -v\n        shigh = v\n\n        pl.figure(figsize=(1.5 * max_display + 1, 0.8 * max_display + 1))\n        pl.subplot(1, max_display, 1)\n        proj_shap_values = shap_values[:, sort_inds[0], sort_inds]\n        proj_shap_values[:, 1:] *= 2  # because off diag effects are split in half\n        summary_plot(\n            proj_shap_values, features[:, sort_inds] if features is not None else None,\n            feature_names=feature_names[sort_inds],\n            sort=False, show=False, color_bar=False,\n            auto_size_plot=False,\n            max_display=max_display\n        )\n        pl.xlim((slow, shigh))\n        pl.xlabel(\"\")\n        title_length_limit = 11\n        pl.title(shorten_text(feature_names[sort_inds[0]], title_length_limit))\n        for i in range(1, min(len(sort_inds), max_display)):\n            ind = sort_inds[i]\n            pl.subplot(1, max_display, i + 1)\n            proj_shap_values = shap_values[:, ind, sort_inds]\n            proj_shap_values *= 2\n            proj_shap_values[:, i] /= 2  # because only off diag effects are split in half\n            summary_plot(\n                proj_shap_values, features[:, sort_inds] if features is not None else None,\n                sort=False,\n                feature_names=[\"\" for i in range(len(feature_names))],\n                show=False,\n                color_bar=False,\n                auto_size_plot=False,\n                max_display=max_display\n            )\n            pl.xlim((slow, shigh))\n            pl.xlabel(\"\")\n            if i == min(len(sort_inds), max_display) // 2:\n                pl.xlabel(labels['INTERACTION_VALUE'])\n            pl.title(shorten_text(feature_names[ind], title_length_limit))\n        pl.tight_layout(pad=0, w_pad=0, h_pad=0.0)\n        pl.subplots_adjust(hspace=0, wspace=0.1)\n        if show:\n            pl.show()\n        return\n\n    if max_display is None:\n        max_display = 20\n\n    if sort:\n        # order features by the sum of their effect magnitudes\n        if multi_class:\n            feature_order = np.argsort(np.sum(np.mean(np.abs(shap_values), axis=0), axis=0))\n        else:\n            feature_order = np.argsort(np.sum(np.abs(shap_values), axis=0))\n        feature_order = feature_order[-min(max_display, len(feature_order)):]\n    else:\n        feature_order = np.flip(np.arange(min(max_display, num_features)), 0)\n\n    row_height = 0.4\n    if auto_size_plot:\n        pl.gcf().set_size_inches(8, len(feature_order) * row_height + 1.5)\n    pl.axvline(x=0, color=\"#999999\", zorder=-1)\n\n    if plot_type == \"dot\":\n        for pos, i in enumerate(feature_order):\n            pl.axhline(y=pos, color=\"#cccccc\", lw=0.5, dashes=(1, 5), zorder=-1)\n            shaps = shap_values[:, i]\n            values = None if features is None else features[:, i]\n            inds = np.arange(len(shaps))\n            np.random.shuffle(inds)\n            if values is not None:\n                values = values[inds]\n            shaps = shaps[inds]\n            colored_feature = True\n            try:\n                values = np.array(values, dtype=np.float64)  # make sure this can be numeric\n            except:\n                colored_feature = False\n            N = len(shaps)\n            # hspacing = (np.max(shaps) - np.min(shaps)) / 200\n            # curr_bin = []\n            nbins = 100\n            quant = np.round(nbins * (shaps - np.min(shaps)) / (np.max(shaps) - np.min(shaps) + 1e-8))\n            inds = np.argsort(quant + np.random.randn(N) * 1e-6)\n            layer = 0\n            last_bin = -1\n            ys = np.zeros(N)\n            for ind in inds:\n                if quant[ind] != last_bin:\n                    layer = 0\n                ys[ind] = np.ceil(layer / 2) * ((layer % 2) * 2 - 1)\n                layer += 1\n                last_bin = quant[ind]\n            ys *= 0.9 * (row_height / np.max(ys + 1))\n\n            if features is not None and colored_feature:\n                # trim the color range, but prevent the color range from collapsing\n                vmin = np.nanpercentile(values, 5)\n                vmax = np.nanpercentile(values, 95)\n                if vmin == vmax:\n                    vmin = np.nanpercentile(values, 1)\n                    vmax = np.nanpercentile(values, 99)\n                    if vmin == vmax:\n                        vmin = np.min(values)\n                        vmax = np.max(values)\n\n                assert features.shape[0] == len(shaps), \"Feature and SHAP matrices must have the same number of rows!\"\n\n                # plot the nan values in the interaction feature as grey\n                nan_mask = np.isnan(values)\n                pl.scatter(shaps[nan_mask], pos + ys[nan_mask], color=\"#777777\", vmin=vmin,\n                           vmax=vmax, s=16, alpha=alpha, linewidth=0,\n                           zorder=3, rasterized=len(shaps) > 500)\n\n                # plot the non-nan values colored by the trimmed feature value\n                cvals = values[np.invert(nan_mask)].astype(np.float64)\n                cvals_imp = cvals.copy()\n                cvals_imp[np.isnan(cvals)] = (vmin + vmax) / 2.0\n                cvals[cvals_imp > vmax] = vmax\n                cvals[cvals_imp < vmin] = vmin\n                pl.scatter(shaps[np.invert(nan_mask)], pos + ys[np.invert(nan_mask)],\n                           cmap=colors.red_blue, vmin=vmin, vmax=vmax, s=16,\n                           c=cvals, alpha=alpha, linewidth=0,\n                           zorder=3, rasterized=len(shaps) > 500)\n            else:\n\n                pl.scatter(shaps, pos + ys, s=16, alpha=alpha, linewidth=0, zorder=3,\n                           color=color if colored_feature else \"#777777\", rasterized=len(shaps) > 500)\n\n    elif plot_type == \"violin\":\n        for pos, i in enumerate(feature_order):\n            pl.axhline(y=pos, color=\"#cccccc\", lw=0.5, dashes=(1, 5), zorder=-1)\n\n        if features is not None:\n            global_low = np.nanpercentile(shap_values[:, :len(feature_names)].flatten(), 1)\n            global_high = np.nanpercentile(shap_values[:, :len(feature_names)].flatten(), 99)\n            for pos, i in enumerate(feature_order):\n                shaps = shap_values[:, i]\n                shap_min, shap_max = np.min(shaps), np.max(shaps)\n                rng = shap_max - shap_min\n                xs = np.linspace(np.min(shaps) - rng * 0.2, np.max(shaps) + rng * 0.2, 100)\n                if np.std(shaps) < (global_high - global_low) / 100:\n                    ds = gaussian_kde(shaps + np.random.randn(len(shaps)) * (global_high - global_low) / 100)(xs)\n                else:\n                    ds = gaussian_kde(shaps)(xs)\n                ds /= np.max(ds) * 3\n\n                values = features[:, i]\n                window_size = max(10, len(values) // 20)\n                smooth_values = np.zeros(len(xs) - 1)\n                sort_inds = np.argsort(shaps)\n                trailing_pos = 0\n                leading_pos = 0\n                running_sum = 0\n                back_fill = 0\n                for j in range(len(xs) - 1):\n\n                    while leading_pos < len(shaps) and xs[j] >= shaps[sort_inds[leading_pos]]:\n                        running_sum += values[sort_inds[leading_pos]]\n                        leading_pos += 1\n                        if leading_pos - trailing_pos > 20:\n                            running_sum -= values[sort_inds[trailing_pos]]\n                            trailing_pos += 1\n                    if leading_pos - trailing_pos > 0:\n                        smooth_values[j] = running_sum / (leading_pos - trailing_pos)\n                        for k in range(back_fill):\n                            smooth_values[j - k - 1] = smooth_values[j]\n                    else:\n                        back_fill += 1\n\n                vmin = np.nanpercentile(values, 5)\n                vmax = np.nanpercentile(values, 95)\n                if vmin == vmax:\n                    vmin = np.nanpercentile(values, 1)\n                    vmax = np.nanpercentile(values, 99)\n                    if vmin == vmax:\n                        vmin = np.min(values)\n                        vmax = np.max(values)\n                pl.scatter(shaps, np.ones(shap_values.shape[0]) * pos, s=9, cmap=colors.red_blue, vmin=vmin, vmax=vmax,\n                           c=values, alpha=alpha, linewidth=0, zorder=1)\n                # smooth_values -= nxp.nanpercentile(smooth_values, 5)\n                # smooth_values /= np.nanpercentile(smooth_values, 95)\n                smooth_values -= vmin\n                if vmax - vmin > 0:\n                    smooth_values /= vmax - vmin\n                for i in range(len(xs) - 1):\n                    if ds[i] > 0.05 or ds[i + 1] > 0.05:\n                        pl.fill_between([xs[i], xs[i + 1]], [pos + ds[i], pos + ds[i + 1]],\n                                        [pos - ds[i], pos - ds[i + 1]], color=colors.red_blue(smooth_values[i]),\n                                        zorder=2)\n\n        else:\n            parts = pl.violinplot(shap_values[:, feature_order], range(len(feature_order)), points=200, vert=False,\n                                  widths=0.7,\n                                  showmeans=False, showextrema=False, showmedians=False)\n\n            for pc in parts['bodies']:\n                pc.set_facecolor(color)\n                pc.set_edgecolor('none')\n                pc.set_alpha(alpha)\n\n    elif plot_type == \"layered_violin\":  # courtesy of @kodonnell\n        num_x_points = 200\n        bins = np.linspace(0, features.shape[0], layered_violin_max_num_bins + 1).round(0).astype(\n            'int')  # the indices of the feature data corresponding to each bin\n        shap_min, shap_max = np.min(shap_values), np.max(shap_values)\n        x_points = np.linspace(shap_min, shap_max, num_x_points)\n\n        # loop through each feature and plot:\n        for pos, ind in enumerate(feature_order):\n            # decide how to handle: if #unique < layered_violin_max_num_bins then split by unique value, otherwise use bins/percentiles.\n            # to keep simpler code, in the case of uniques, we just adjust the bins to align with the unique counts.\n            feature = features[:, ind]\n            unique, counts = np.unique(feature, return_counts=True)\n            if unique.shape[0] <= layered_violin_max_num_bins:\n                order = np.argsort(unique)\n                thesebins = np.cumsum(counts[order])\n                thesebins = np.insert(thesebins, 0, 0)\n            else:\n                thesebins = bins\n            nbins = thesebins.shape[0] - 1\n            # order the feature data so we can apply percentiling\n            order = np.argsort(feature)\n            # x axis is located at y0 = pos, with pos being there for offset\n            y0 = np.ones(num_x_points) * pos\n            # calculate kdes:\n            ys = np.zeros((nbins, num_x_points))\n            for i in range(nbins):\n                # get shap values in this bin:\n                shaps = shap_values[order[thesebins[i]:thesebins[i + 1]], ind]\n                # if there's only one element, then we can't\n                if shaps.shape[0] == 1:\n                    warnings.warn(\n                        \"not enough data in bin #%d for feature %s, so it'll be ignored. Try increasing the number of records to plot.\"\n                        % (i, feature_names[ind]))\n                    # to ignore it, just set it to the previous y-values (so the area between them will be zero). Not ys is already 0, so there's\n                    # nothing to do if i == 0\n                    if i > 0:\n                        ys[i, :] = ys[i - 1, :]\n                    continue\n                # save kde of them: note that we add a tiny bit of gaussian noise to avoid singular matrix errors\n                ys[i, :] = gaussian_kde(shaps + np.random.normal(loc=0, scale=0.001, size=shaps.shape[0]))(x_points)\n                # scale it up so that the 'size' of each y represents the size of the bin. For continuous data this will\n                # do nothing, but when we've gone with the unqique option, this will matter - e.g. if 99% are male and 1%\n                # female, we want the 1% to appear a lot smaller.\n                size = thesebins[i + 1] - thesebins[i]\n                bin_size_if_even = features.shape[0] / nbins\n                relative_bin_size = size / bin_size_if_even\n                ys[i, :] *= relative_bin_size\n            # now plot 'em. We don't plot the individual strips, as this can leave whitespace between them.\n            # instead, we plot the full kde, then remove outer strip and plot over it, etc., to ensure no\n            # whitespace\n            ys = np.cumsum(ys, axis=0)\n            width = 0.8\n            scale = ys.max() * 2 / width  # 2 is here as we plot both sides of x axis\n            for i in range(nbins - 1, -1, -1):\n                y = ys[i, :] / scale\n                c = pl.get_cmap(color)(i / (\n                        nbins - 1)) if color in pl.cm.datad else color  # if color is a cmap, use it, otherwise use a color\n                pl.fill_between(x_points, pos - y, pos + y, facecolor=c)\n        pl.xlim(shap_min, shap_max)\n\n    elif not multi_class and plot_type == \"bar\":\n        feature_inds = feature_order[:max_display]\n        y_pos = np.arange(len(feature_inds))\n        global_shap_values = np.abs(shap_values).mean(0)\n        pl.barh(y_pos, global_shap_values[feature_inds], 0.7, align='center', color=color)\n        pl.yticks(y_pos, fontsize=13)\n        pl.gca().set_yticklabels([feature_names[i] for i in feature_inds])\n\n    elif multi_class and plot_type == \"bar\":\n        if class_names is None:\n            class_names = [\"Class \"+str(i) for i in range(len(shap_values))]\n        feature_inds = feature_order[:max_display]\n        y_pos = np.arange(len(feature_inds))\n        left_pos = np.zeros(len(feature_inds))\n\n        class_inds = np.argsort([-np.abs(shap_values[i]).mean() for i in range(len(shap_values))])\n        for i,ind in enumerate(class_inds):\n            global_shap_values = np.abs(shap_values[ind]).mean(0)\n            pl.barh(\n                y_pos, global_shap_values[feature_inds], 0.7, left=left_pos, align='center',\n                color=color(i), label=class_names[ind]\n            )\n            left_pos += global_shap_values[feature_inds]\n        pl.yticks(y_pos, fontsize=13)\n        pl.gca().set_yticklabels([feature_names[i] for i in feature_inds])\n        pl.legend(frameon=False, fontsize=12)\n\n    # draw the color bar\n    if color_bar and features is not None and plot_type != \"bar\" and \\\n            (plot_type != \"layered_violin\" or color in pl.cm.datad):\n        import matplotlib.cm as cm\n        m = cm.ScalarMappable(cmap=colors.red_blue if plot_type != \"layered_violin\" else pl.get_cmap(color))\n        m.set_array([0, 1])\n        cb = pl.colorbar(m, ticks=[0, 1], aspect=1000)\n        cb.set_ticklabels([labels['FEATURE_VALUE_LOW'], labels['FEATURE_VALUE_HIGH']])\n        cb.set_label(labels['FEATURE_VALUE'], size=12, labelpad=0)\n        cb.ax.tick_params(labelsize=11, length=0)\n        cb.set_alpha(1)\n        cb.outline.set_visible(False)\n        bbox = cb.ax.get_window_extent().transformed(pl.gcf().dpi_scale_trans.inverted())\n        cb.ax.set_aspect((bbox.height - 0.9) * 20)\n        # cb.draw_all()\n\n    pl.gca().xaxis.set_ticks_position('bottom')\n    pl.gca().yaxis.set_ticks_position('none')\n    pl.gca().spines['right'].set_visible(False)\n    pl.gca().spines['top'].set_visible(False)\n    pl.gca().spines['left'].set_visible(False)\n    pl.gca().tick_params(color=axis_color, labelcolor=axis_color)\n    pl.yticks(range(len(feature_order)), [feature_names[i] for i in feature_order], fontsize=13)\n    if plot_type != \"bar\":\n        pl.gca().tick_params('y', length=20, width=0.5, which='major')\n    pl.gca().tick_params('x', labelsize=11)\n    pl.ylim(-1, len(feature_order))\n    if plot_type == \"bar\":\n        pl.xlabel(labels['GLOBAL_VALUE'], fontsize=13)\n    else:\n        pl.xlabel(labels['VALUE'], fontsize=13)\n    if show:\n        pl.show()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nkerneling SHAP 1000 mean ref.", "response": "def kernel_shap_1000_meanref(model, data):\n    \"\"\" Kernel SHAP 1000 mean ref.\n    color = red_blue_circle(0.5)\n    linestyle = solid\n    \"\"\"\n    return lambda X: KernelExplainer(model.predict, kmeans(data, 1)).shap_values(X, nsamples=1000, l1_reg=0)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sampling_shap_1000(model, data):\n    return lambda X: SamplingExplainer(model.predict, data).shap_values(X, nsamples=1000)", "response": "IME 1000 sample shading"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef tree_shap_independent_200(model, data):\n    data_subsample = sklearn.utils.resample(data, replace=False, n_samples=min(200, data.shape[0]), random_state=0)\n    return TreeExplainer(model, data_subsample, feature_dependence=\"independent\").shap_values", "response": "TreeExplainer with independent features."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmeans absolute tree - experience", "response": "def mean_abs_tree_shap(model, data):\n    \"\"\" mean(|TreeExplainer|)\n    color = red_blue_circle(0.25)\n    linestyle = solid\n    \"\"\"\n    def f(X):\n        v = TreeExplainer(model).shap_values(X)\n        if isinstance(v, list):\n            return [np.tile(np.abs(sv).mean(0), (X.shape[0], 1)) for sv in v]\n        else:\n            return np.tile(np.abs(v).mean(0), (X.shape[0], 1))\n    return f"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef saabas(model, data):\n    return lambda X: TreeExplainer(model).shap_values(X, approximate=True)", "response": "A simple tree expansion function."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef expected_gradients(model, data):\n    if isinstance(model, KerasWrap):\n        model = model.model\n    explainer = GradientExplainer(model, data)\n    def f(X):\n        phi = explainer.shap_values(X)\n        if type(phi) is list and len(phi) == 1:\n            return phi[0]\n        else:\n            return phi\n    \n    return f", "response": "Returns a function that returns the expected gradients for a Keras object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef shap_values(self, X, ranked_outputs=None, output_rank_order='max'):\n        return self.explainer.shap_values(X, ranked_outputs, output_rank_order)", "response": "Return the approximate SHAP values for the model applied to the data given by X."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _agent_import_failed(trace):\n\n    class _AgentImportFailed(Trainer):\n        _name = \"AgentImportFailed\"\n        _default_config = with_common_config({})\n\n        def _setup(self, config):\n            raise ImportError(trace)\n\n    return _AgentImportFailed", "response": "Returns a dummy agent class for if PyTorch etc. is not installed."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run(run_or_experiment,\n        name=None,\n        stop=None,\n        config=None,\n        resources_per_trial=None,\n        num_samples=1,\n        local_dir=None,\n        upload_dir=None,\n        trial_name_creator=None,\n        loggers=None,\n        sync_function=None,\n        checkpoint_freq=0,\n        checkpoint_at_end=False,\n        export_formats=None,\n        max_failures=3,\n        restore=None,\n        search_alg=None,\n        scheduler=None,\n        with_server=False,\n        server_port=TuneServer.DEFAULT_PORT,\n        verbose=2,\n        resume=False,\n        queue_trials=False,\n        reuse_actors=False,\n        trial_executor=None,\n        raise_on_failed_trial=True):\n    \"\"\"Executes training.\n\n    Args:\n        run_or_experiment (function|class|str|Experiment): If\n            function|class|str, this is the algorithm or model to train.\n            This may refer to the name of a built-on algorithm\n            (e.g. RLLib's DQN or PPO), a user-defined trainable\n            function or class, or the string identifier of a\n            trainable function or class registered in the tune registry.\n            If Experiment, then Tune will execute training based on\n            Experiment.spec.\n        name (str): Name of experiment.\n        stop (dict): The stopping criteria. The keys may be any field in\n            the return result of 'train()', whichever is reached first.\n            Defaults to empty dict.\n        config (dict): Algorithm-specific configuration for Tune variant\n            generation (e.g. env, hyperparams). Defaults to empty dict.\n            Custom search algorithms may ignore this.\n        resources_per_trial (dict): Machine resources to allocate per trial,\n            e.g. ``{\"cpu\": 64, \"gpu\": 8}``. Note that GPUs will not be\n            assigned unless you specify them here. Defaults to 1 CPU and 0\n            GPUs in ``Trainable.default_resource_request()``.\n        num_samples (int): Number of times to sample from the\n            hyperparameter space. Defaults to 1. If `grid_search` is\n            provided as an argument, the grid will be repeated\n            `num_samples` of times.\n        local_dir (str): Local dir to save training results to.\n            Defaults to ``~/ray_results``.\n        upload_dir (str): Optional URI to sync training results\n            to (e.g. ``s3://bucket``).\n        trial_name_creator (func): Optional function for generating\n            the trial string representation.\n        loggers (list): List of logger creators to be used with\n            each Trial. If None, defaults to ray.tune.logger.DEFAULT_LOGGERS.\n            See `ray/tune/logger.py`.\n        sync_function (func|str): Function for syncing the local_dir to\n            upload_dir. If string, then it must be a string template for\n            syncer to run. If not provided, the sync command defaults\n            to standard S3 or gsutil sync comamnds.\n        checkpoint_freq (int): How many training iterations between\n            checkpoints. A value of 0 (default) disables checkpointing.\n        checkpoint_at_end (bool): Whether to checkpoint at the end of the\n            experiment regardless of the checkpoint_freq. Default is False.\n        export_formats (list): List of formats that exported at the end of\n            the experiment. Default is None.\n        max_failures (int): Try to recover a trial from its last\n            checkpoint at least this many times. Only applies if\n            checkpointing is enabled. Setting to -1 will lead to infinite\n            recovery retries. Defaults to 3.\n        restore (str): Path to checkpoint. Only makes sense to set if\n            running 1 trial. Defaults to None.\n        search_alg (SearchAlgorithm): Search Algorithm. Defaults to\n            BasicVariantGenerator.\n        scheduler (TrialScheduler): Scheduler for executing\n            the experiment. Choose among FIFO (default), MedianStopping,\n            AsyncHyperBand, and HyperBand.\n        with_server (bool): Starts a background Tune server. Needed for\n            using the Client API.\n        server_port (int): Port number for launching TuneServer.\n        verbose (int): 0, 1, or 2. Verbosity mode. 0 = silent,\n            1 = only status updates, 2 = status and trial results.\n        resume (bool|\"prompt\"): If checkpoint exists, the experiment will\n            resume from there. If resume is \"prompt\", Tune will prompt if\n            checkpoint detected.\n        queue_trials (bool): Whether to queue trials when the cluster does\n            not currently have enough resources to launch one. This should\n            be set to True when running on an autoscaling cluster to enable\n            automatic scale-up.\n        reuse_actors (bool): Whether to reuse actors between different trials\n            when possible. This can drastically speed up experiments that start\n            and stop actors often (e.g., PBT in time-multiplexing mode). This\n            requires trials to have the same resource requirements.\n        trial_executor (TrialExecutor): Manage the execution of trials.\n        raise_on_failed_trial (bool): Raise TuneError if there exists failed\n            trial (of ERROR state) when the experiments complete.\n\n    Returns:\n        List of Trial objects.\n\n    Raises:\n        TuneError if any trials failed and `raise_on_failed_trial` is True.\n\n    Examples:\n        >>> tune.run(mytrainable, scheduler=PopulationBasedTraining())\n\n        >>> tune.run(mytrainable, num_samples=5, reuse_actors=True)\n\n        >>> tune.run(\n                \"PG\",\n                num_samples=5,\n                config={\n                    \"env\": \"CartPole-v0\",\n                    \"lr\": tune.sample_from(lambda _: np.random.rand())\n                }\n            )\n    \"\"\"\n    experiment = run_or_experiment\n    if not isinstance(run_or_experiment, Experiment):\n        experiment = Experiment(\n            name, run_or_experiment, stop, config, resources_per_trial,\n            num_samples, local_dir, upload_dir, trial_name_creator, loggers,\n            sync_function, checkpoint_freq, checkpoint_at_end, export_formats,\n            max_failures, restore)\n    else:\n        logger.debug(\"Ignoring some parameters passed into tune.run.\")\n\n    checkpoint_dir = _find_checkpoint_dir(experiment)\n    should_restore = _prompt_restore(checkpoint_dir, resume)\n\n    runner = None\n    if should_restore:\n        try:\n            runner = TrialRunner.restore(checkpoint_dir, search_alg, scheduler,\n                                         trial_executor)\n        except Exception:\n            logger.exception(\"Runner restore failed. Restarting experiment.\")\n    else:\n        logger.info(\"Starting a new experiment.\")\n\n    if not runner:\n        scheduler = scheduler or FIFOScheduler()\n        search_alg = search_alg or BasicVariantGenerator()\n\n        search_alg.add_configurations([experiment])\n\n        runner = TrialRunner(\n            search_alg,\n            scheduler=scheduler,\n            metadata_checkpoint_dir=checkpoint_dir,\n            launch_web_server=with_server,\n            server_port=server_port,\n            verbose=bool(verbose > 1),\n            queue_trials=queue_trials,\n            reuse_actors=reuse_actors,\n            trial_executor=trial_executor)\n\n    if verbose:\n        print(runner.debug_string(max_debug=99999))\n\n    last_debug = 0\n    while not runner.is_finished():\n        runner.step()\n        if time.time() - last_debug > DEBUG_PRINT_INTERVAL:\n            if verbose:\n                print(runner.debug_string())\n            last_debug = time.time()\n\n    if verbose:\n        print(runner.debug_string(max_debug=99999))\n\n    wait_for_log_sync()\n\n    errored_trials = []\n    for trial in runner.get_trials():\n        if trial.status != Trial.TERMINATED:\n            errored_trials += [trial]\n\n    if errored_trials:\n        if raise_on_failed_trial:\n            raise TuneError(\"Trials did not complete\", errored_trials)\n        else:\n            logger.error(\"Trials did not complete: %s\", errored_trials)\n\n    return runner.get_trials()", "response": "Runs training on the given object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run_experiments(experiments,\n                    search_alg=None,\n                    scheduler=None,\n                    with_server=False,\n                    server_port=TuneServer.DEFAULT_PORT,\n                    verbose=2,\n                    resume=False,\n                    queue_trials=False,\n                    reuse_actors=False,\n                    trial_executor=None,\n                    raise_on_failed_trial=True):\n    \"\"\"Runs and blocks until all trials finish.\n\n    Examples:\n        >>> experiment_spec = Experiment(\"experiment\", my_func)\n        >>> run_experiments(experiments=experiment_spec)\n\n        >>> experiment_spec = {\"experiment\": {\"run\": my_func}}\n        >>> run_experiments(experiments=experiment_spec)\n\n        >>> run_experiments(\n        >>>     experiments=experiment_spec,\n        >>>     scheduler=MedianStoppingRule(...))\n\n        >>> run_experiments(\n        >>>     experiments=experiment_spec,\n        >>>     search_alg=SearchAlgorithm(),\n        >>>     scheduler=MedianStoppingRule(...))\n\n    Returns:\n        List of Trial objects, holding data for each executed trial.\n\n    \"\"\"\n    # This is important to do this here\n    # because it schematize the experiments\n    # and it conducts the implicit registration.\n    experiments = convert_to_experiment_list(experiments)\n\n    trials = []\n    for exp in experiments:\n        trials += run(\n            exp,\n            search_alg=search_alg,\n            scheduler=scheduler,\n            with_server=with_server,\n            server_port=server_port,\n            verbose=verbose,\n            resume=resume,\n            queue_trials=queue_trials,\n            reuse_actors=reuse_actors,\n            trial_executor=trial_executor,\n            raise_on_failed_trial=raise_on_failed_trial)\n    return trials", "response": "Runs and blocks until all trials finish."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nflushes remaining output records in the output queues to plasma.", "response": "def _flush(self, close=False):\n        \"\"\"Flushes remaining output records in the output queues to plasma.\n\n        None is used as special type of record that is propagated from sources\n        to sink to notify that the end of data in a stream.\n\n        Attributes:\n             close (bool): A flag denoting whether the channel should be\n             also marked as 'closed' (True) or not (False) after flushing.\n        \"\"\"\n        for channel in self.forward_channels:\n            if close is True:\n                channel.queue.put_next(None)\n            channel.queue._flush_writes()\n        for channels in self.shuffle_channels:\n            for channel in channels:\n                if close is True:\n                    channel.queue.put_next(None)\n                channel.queue._flush_writes()\n        for channels in self.shuffle_key_channels:\n            for channel in channels:\n                if close is True:\n                    channel.queue.put_next(None)\n                channel.queue._flush_writes()\n        for channels in self.round_robin_channels:\n            for channel in channels:\n                if close is True:\n                    channel.queue.put_next(None)\n                channel.queue._flush_writes()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns an appropriate preprocessor class for the given space.", "response": "def get_preprocessor(space):\n    \"\"\"Returns an appropriate preprocessor class for the given space.\"\"\"\n\n    legacy_patch_shapes(space)\n    obs_shape = space.shape\n\n    if isinstance(space, gym.spaces.Discrete):\n        preprocessor = OneHotPreprocessor\n    elif obs_shape == ATARI_OBS_SHAPE:\n        preprocessor = GenericPixelPreprocessor\n    elif obs_shape == ATARI_RAM_OBS_SHAPE:\n        preprocessor = AtariRamPreprocessor\n    elif isinstance(space, gym.spaces.Tuple):\n        preprocessor = TupleFlatteningPreprocessor\n    elif isinstance(space, gym.spaces.Dict):\n        preprocessor = DictFlatteningPreprocessor\n    else:\n        preprocessor = NoPreprocessor\n\n    return preprocessor"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef legacy_patch_shapes(space):\n\n    if not hasattr(space, \"shape\"):\n        if isinstance(space, gym.spaces.Discrete):\n            space.shape = ()\n        elif isinstance(space, gym.spaces.Tuple):\n            shapes = []\n            for s in space.spaces:\n                shape = legacy_patch_shapes(s)\n                shapes.append(shape)\n            space.shape = tuple(shapes)\n\n    return space.shape", "response": "Assigns shapes to spaces that don t have shapes."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef transform(self, observation):\n        self.check_shape(observation)\n        scaled = observation[25:-25, :, :]\n        if self._dim < 84:\n            scaled = cv2.resize(scaled, (84, 84))\n        # OpenAI: Resize by half, then down to 42x42 (essentially mipmapping).\n        # If we resize directly we lose pixels that, when mapped to 42x42,\n        # aren't close enough to the pixel boundary.\n        scaled = cv2.resize(scaled, (self._dim, self._dim))\n        if self._grayscale:\n            scaled = scaled.mean(2)\n            scaled = scaled.astype(np.float32)\n            # Rescale needed for maintaining 1 channel\n            scaled = np.reshape(scaled, [self._dim, self._dim, 1])\n        if self._zero_mean:\n            scaled = (scaled - 128) / 128\n        else:\n            scaled *= 1.0 / 255.0\n        return scaled", "response": "Downsamples images from 210 160 3 by the configured factor."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a new batch from the internal ring buffer.", "response": "def get(self):\n        \"\"\"Get a new batch from the internal ring buffer.\n\n        Returns:\n           buf: Data item saved from inqueue.\n           released: True if the item is now removed from the ring buffer.\n        \"\"\"\n        if self.ttl[self.idx] <= 0:\n            self.buffers[self.idx] = self.inqueue.get(timeout=300.0)\n            self.ttl[self.idx] = self.cur_max_ttl\n            if self.cur_max_ttl < self.max_ttl:\n                self.cur_max_ttl += 1\n        buf = self.buffers[self.idx]\n        self.ttl[self.idx] -= 1\n        released = self.ttl[self.idx] <= 0\n        if released:\n            self.buffers[self.idx] = None\n        self.idx = (self.idx + 1) % len(self.buffers)\n        return buf, released"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun one logical iteration of training.", "response": "def train(self):\n        \"\"\"Runs one logical iteration of training.\n\n        Subclasses should override ``_train()`` instead to return results.\n        This class automatically fills the following fields in the result:\n\n            `done` (bool): training is terminated. Filled only if not provided.\n\n            `time_this_iter_s` (float): Time in seconds this iteration\n            took to run. This may be overriden in order to override the\n            system-computed time difference.\n\n            `time_total_s` (float): Accumulated time in seconds for this\n            entire experiment.\n\n            `experiment_id` (str): Unique string identifier\n            for this experiment. This id is preserved\n            across checkpoint / restore calls.\n\n            `training_iteration` (int): The index of this\n            training iteration, e.g. call to train().\n\n            `pid` (str): The pid of the training process.\n\n            `date` (str): A formatted date of when the result was processed.\n\n            `timestamp` (str): A UNIX timestamp of when the result\n            was processed.\n\n            `hostname` (str): Hostname of the machine hosting the training\n            process.\n\n            `node_ip` (str): Node ip of the machine hosting the training\n            process.\n\n        Returns:\n            A dict that describes training progress.\n        \"\"\"\n\n        start = time.time()\n        result = self._train()\n        assert isinstance(result, dict), \"_train() needs to return a dict.\"\n\n        # We do not modify internal state nor update this result if duplicate.\n        if RESULT_DUPLICATE in result:\n            return result\n\n        result = result.copy()\n\n        self._iteration += 1\n        self._iterations_since_restore += 1\n\n        if result.get(TIME_THIS_ITER_S) is not None:\n            time_this_iter = result[TIME_THIS_ITER_S]\n        else:\n            time_this_iter = time.time() - start\n        self._time_total += time_this_iter\n        self._time_since_restore += time_this_iter\n\n        result.setdefault(DONE, False)\n\n        # self._timesteps_total should only be tracked if increments provided\n        if result.get(TIMESTEPS_THIS_ITER) is not None:\n            if self._timesteps_total is None:\n                self._timesteps_total = 0\n            self._timesteps_total += result[TIMESTEPS_THIS_ITER]\n            self._timesteps_since_restore += result[TIMESTEPS_THIS_ITER]\n\n        # self._episodes_total should only be tracked if increments provided\n        if result.get(EPISODES_THIS_ITER) is not None:\n            if self._episodes_total is None:\n                self._episodes_total = 0\n            self._episodes_total += result[EPISODES_THIS_ITER]\n\n        # self._timesteps_total should not override user-provided total\n        result.setdefault(TIMESTEPS_TOTAL, self._timesteps_total)\n        result.setdefault(EPISODES_TOTAL, self._episodes_total)\n        result.setdefault(TRAINING_ITERATION, self._iteration)\n\n        # Provides auto-filled neg_mean_loss for avoiding regressions\n        if result.get(\"mean_loss\"):\n            result.setdefault(\"neg_mean_loss\", -result[\"mean_loss\"])\n\n        now = datetime.today()\n        result.update(\n            experiment_id=self._experiment_id,\n            date=now.strftime(\"%Y-%m-%d_%H-%M-%S\"),\n            timestamp=int(time.mktime(now.timetuple())),\n            time_this_iter_s=time_this_iter,\n            time_total_s=self._time_total,\n            pid=os.getpid(),\n            hostname=os.uname()[1],\n            node_ip=self._local_ip,\n            config=self.config,\n            time_since_restore=self._time_since_restore,\n            timesteps_since_restore=self._timesteps_since_restore,\n            iterations_since_restore=self._iterations_since_restore)\n\n        self._log_result(result)\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete_checkpoint(self, checkpoint_dir):\n        if os.path.isfile(checkpoint_dir):\n            shutil.rmtree(os.path.dirname(checkpoint_dir))\n        else:\n            shutil.rmtree(checkpoint_dir)", "response": "Removes subdirectory within checkpoint_folder"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef save(self, checkpoint_dir=None):\n\n        checkpoint_dir = os.path.join(checkpoint_dir or self.logdir,\n                                      \"checkpoint_{}\".format(self._iteration))\n        if not os.path.exists(checkpoint_dir):\n            os.makedirs(checkpoint_dir)\n        checkpoint = self._save(checkpoint_dir)\n        saved_as_dict = False\n        if isinstance(checkpoint, string_types):\n            if (not checkpoint.startswith(checkpoint_dir)\n                    or checkpoint == checkpoint_dir):\n                raise ValueError(\n                    \"The returned checkpoint path must be within the \"\n                    \"given checkpoint dir {}: {}\".format(\n                        checkpoint_dir, checkpoint))\n            if not os.path.exists(checkpoint):\n                raise ValueError(\n                    \"The returned checkpoint path does not exist: {}\".format(\n                        checkpoint))\n            checkpoint_path = checkpoint\n        elif isinstance(checkpoint, dict):\n            saved_as_dict = True\n            checkpoint_path = os.path.join(checkpoint_dir, \"checkpoint\")\n            with open(checkpoint_path, \"wb\") as f:\n                pickle.dump(checkpoint, f)\n        else:\n            raise ValueError(\n                \"`_save` must return a dict or string type: {}\".format(\n                    str(type(checkpoint))))\n        with open(checkpoint_path + \".tune_metadata\", \"wb\") as f:\n            pickle.dump({\n                \"experiment_id\": self._experiment_id,\n                \"iteration\": self._iteration,\n                \"timesteps_total\": self._timesteps_total,\n                \"time_total\": self._time_total,\n                \"episodes_total\": self._episodes_total,\n                \"saved_as_dict\": saved_as_dict\n            }, f)\n        return checkpoint_path", "response": "Saves the current state of the current state to a checkpoint."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsaves the current state to a Python object. It also saves the current model state to disk but does not return the checkpoint path.", "response": "def save_to_object(self):\n        \"\"\"Saves the current model state to a Python object. It also\n        saves to disk but does not return the checkpoint path.\n\n        Returns:\n            Object holding checkpoint data.\n        \"\"\"\n\n        tmpdir = tempfile.mkdtemp(\"save_to_object\", dir=self.logdir)\n        checkpoint_prefix = self.save(tmpdir)\n\n        data = {}\n        base_dir = os.path.dirname(checkpoint_prefix)\n        for path in os.listdir(base_dir):\n            path = os.path.join(base_dir, path)\n            if path.startswith(checkpoint_prefix):\n                with open(path, \"rb\") as f:\n                    data[os.path.basename(path)] = f.read()\n\n        out = io.BytesIO()\n        data_dict = pickle.dumps({\n            \"checkpoint_name\": os.path.basename(checkpoint_prefix),\n            \"data\": data,\n        })\n        if len(data_dict) > 10e6:  # getting pretty large\n            logger.info(\"Checkpoint size is {} bytes\".format(len(data_dict)))\n        out.write(data_dict)\n\n        shutil.rmtree(tmpdir)\n        return out.getvalue()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef restore(self, checkpoint_path):\n\n        with open(checkpoint_path + \".tune_metadata\", \"rb\") as f:\n            metadata = pickle.load(f)\n        self._experiment_id = metadata[\"experiment_id\"]\n        self._iteration = metadata[\"iteration\"]\n        self._timesteps_total = metadata[\"timesteps_total\"]\n        self._time_total = metadata[\"time_total\"]\n        self._episodes_total = metadata[\"episodes_total\"]\n        saved_as_dict = metadata[\"saved_as_dict\"]\n        if saved_as_dict:\n            with open(checkpoint_path, \"rb\") as loaded_state:\n                checkpoint_dict = pickle.load(loaded_state)\n            self._restore(checkpoint_dict)\n        else:\n            self._restore(checkpoint_path)\n        self._time_since_restore = 0.0\n        self._timesteps_since_restore = 0\n        self._iterations_since_restore = 0\n        self._restored = True", "response": "Restores training state from a given model checkpoint."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef restore_from_object(self, obj):\n\n        info = pickle.loads(obj)\n        data = info[\"data\"]\n        tmpdir = tempfile.mkdtemp(\"restore_from_object\", dir=self.logdir)\n        checkpoint_path = os.path.join(tmpdir, info[\"checkpoint_name\"])\n\n        for file_name, file_contents in data.items():\n            with open(os.path.join(tmpdir, file_name), \"wb\") as f:\n                f.write(file_contents)\n\n        self.restore(checkpoint_path)\n        shutil.rmtree(tmpdir)", "response": "Restores training state from a checkpoint object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef export_model(self, export_formats, export_dir=None):\n        export_dir = export_dir or self.logdir\n        return self._export_model(export_formats, export_dir)", "response": "Exports the current state of the object to a local directory."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dump_json(json_info, json_file, overwrite=True):\n    if overwrite:\n        mode = \"w\"\n    else:\n        mode = \"w+\"\n\n    try:\n        with open(json_file, mode) as f:\n            f.write(json.dumps(json_info))\n    except BaseException as e:\n        logging.error(e.message)", "response": "Dump a whole json record into a file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing a whole json record from the given file.", "response": "def parse_json(json_file):\n    \"\"\"Parse a whole json record from the given file.\n\n    Return None if the json file does not exists or exception occurs.\n\n    Args:\n        json_file (str): File path to be parsed.\n\n    Returns:\n        A dict of json info.\n    \"\"\"\n    if not os.path.exists(json_file):\n        return None\n\n    try:\n        with open(json_file, \"r\") as f:\n            info_str = f.readlines()\n            info_str = \"\".join(info_str)\n            json_info = json.loads(info_str)\n            return unicode2str(json_info)\n    except BaseException as e:\n        logging.error(e.message)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_multiple_json(json_file, offset=None):\n    json_info_list = []\n    if not os.path.exists(json_file):\n        return json_info_list\n\n    try:\n        with open(json_file, \"r\") as f:\n            if offset:\n                f.seek(offset)\n            for line in f:\n                if line[-1] != \"\\n\":\n                    # Incomplete line\n                    break\n                json_info = json.loads(line)\n                json_info_list.append(json_info)\n                offset += len(line)\n    except BaseException as e:\n        logging.error(e.message)\n\n    return json_info_list, offset", "response": "Parse multiple json records from the given file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef unicode2str(content):\n    if isinstance(content, dict):\n        result = {}\n        for key in content.keys():\n            result[unicode2str(key)] = unicode2str(content[key])\n        return result\n    elif isinstance(content, list):\n        return [unicode2str(element) for element in content]\n    elif isinstance(content, int) or isinstance(content, float):\n        return content\n    else:\n        return content.encode(\"utf-8\")", "response": "Convert the unicode element of the content to str recursively."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the loss of the network.", "response": "def loss(self, xs, ys):\n        \"\"\"Computes the loss of the network.\"\"\"\n        return float(\n            self.sess.run(\n                self.cross_entropy, feed_dict={\n                    self.x: xs,\n                    self.y_: ys\n                }))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef grad(self, xs, ys):\n        return self.sess.run(\n            self.cross_entropy_grads, feed_dict={\n                self.x: xs,\n                self.y_: ys\n            })", "response": "Computes the gradients of the network."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbuilds the queue and preprocessing operations for the cifar10 dataset.", "response": "def build_data(data_path, size, dataset):\n    \"\"\"Creates the queue and preprocessing operations for the dataset.\n\n    Args:\n        data_path: Filename for cifar10 data.\n        size: The number of images in the dataset.\n        dataset: The dataset we are using.\n\n    Returns:\n        queue: A Tensorflow queue for extracting the images and labels.\n    \"\"\"\n    image_size = 32\n    if dataset == \"cifar10\":\n        label_bytes = 1\n        label_offset = 0\n    elif dataset == \"cifar100\":\n        label_bytes = 1\n        label_offset = 1\n    depth = 3\n    image_bytes = image_size * image_size * depth\n    record_bytes = label_bytes + label_offset + image_bytes\n\n    def load_transform(value):\n        # Convert these examples to dense labels and processed images.\n        record = tf.reshape(tf.decode_raw(value, tf.uint8), [record_bytes])\n        label = tf.cast(tf.slice(record, [label_offset], [label_bytes]),\n                        tf.int32)\n        # Convert from string to [depth * height * width] to\n        # [depth, height, width].\n        depth_major = tf.reshape(\n            tf.slice(record, [label_bytes], [image_bytes]),\n            [depth, image_size, image_size])\n        # Convert from [depth, height, width] to [height, width, depth].\n        image = tf.cast(tf.transpose(depth_major, [1, 2, 0]), tf.float32)\n        return (image, label)\n    # Read examples from files in the filename queue.\n    data_files = tf.gfile.Glob(data_path)\n    data = tf.contrib.data.FixedLengthRecordDataset(data_files,\n                                                    record_bytes=record_bytes)\n    data = data.map(load_transform)\n    data = data.batch(size)\n    iterator = data.make_one_shot_iterator()\n    return iterator.get_next()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef build_input(data, batch_size, dataset, train):\n    image_size = 32\n    depth = 3\n    num_classes = 10 if dataset == \"cifar10\" else 100\n    images, labels = data\n    num_samples = images.shape[0] - images.shape[0] % batch_size\n    dataset = tf.contrib.data.Dataset.from_tensor_slices(\n        (images[:num_samples], labels[:num_samples]))\n\n    def map_train(image, label):\n        image = tf.image.resize_image_with_crop_or_pad(image, image_size + 4,\n                                                       image_size + 4)\n        image = tf.random_crop(image, [image_size, image_size, 3])\n        image = tf.image.random_flip_left_right(image)\n        image = tf.image.per_image_standardization(image)\n        return (image, label)\n\n    def map_test(image, label):\n        image = tf.image.resize_image_with_crop_or_pad(image, image_size,\n                                                       image_size)\n        image = tf.image.per_image_standardization(image)\n        return (image, label)\n\n    dataset = dataset.map(map_train if train else map_test)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.repeat()\n    if train:\n        dataset = dataset.shuffle(buffer_size=16 * batch_size)\n    images, labels = dataset.make_one_shot_iterator().get_next()\n    images = tf.reshape(images, [batch_size, image_size, image_size, depth])\n    labels = tf.reshape(labels, [batch_size, 1])\n    indices = tf.reshape(tf.range(0, batch_size, 1), [batch_size, 1])\n    labels = tf.sparse_to_dense(\n        tf.concat([indices, labels], 1),\n        [batch_size, num_classes], 1.0, 0.0)\n\n    assert len(images.get_shape()) == 4\n    assert images.get_shape()[0] == batch_size\n    assert images.get_shape()[-1] == 3\n    assert len(labels.get_shape()) == 2\n    assert labels.get_shape()[0] == batch_size\n    assert labels.get_shape()[1] == num_classes\n    if not train:\n        tf.summary.image(\"images\", images)\n    return images, labels", "response": "Builds a CIFAR image and labels for a single dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_or_update(cluster_config_file, min_workers, max_workers, no_restart,\n                     restart_only, yes, cluster_name):\n    \"\"\"Create or update a Ray cluster.\"\"\"\n    if restart_only or no_restart:\n        assert restart_only != no_restart, \"Cannot set both 'restart_only' \" \\\n            \"and 'no_restart' at the same time!\"\n    create_or_update_cluster(cluster_config_file, min_workers, max_workers,\n                             no_restart, restart_only, yes, cluster_name)", "response": "Create or update a Ray cluster."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntearing down the Ray cluster.", "response": "def teardown(cluster_config_file, yes, workers_only, cluster_name):\n    \"\"\"Tear down the Ray cluster.\"\"\"\n    teardown_cluster(cluster_config_file, yes, workers_only, cluster_name)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nkilling a random Ray node. For testing purposes only.", "response": "def kill_random_node(cluster_config_file, yes, cluster_name):\n    \"\"\"Kills a random Ray node. For testing purposes only.\"\"\"\n    click.echo(\"Killed node with IP \" +\n               kill_node(cluster_config_file, yes, cluster_name))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef submit(cluster_config_file, docker, screen, tmux, stop, start,\n           cluster_name, port_forward, script, script_args):\n    \"\"\"Uploads and runs a script on the specified cluster.\n\n    The script is automatically synced to the following location:\n\n        os.path.join(\"~\", os.path.basename(script))\n    \"\"\"\n    assert not (screen and tmux), \"Can specify only one of `screen` or `tmux`.\"\n\n    if start:\n        create_or_update_cluster(cluster_config_file, None, None, False, False,\n                                 True, cluster_name)\n\n    target = os.path.join(\"~\", os.path.basename(script))\n    rsync(cluster_config_file, script, target, cluster_name, down=False)\n\n    cmd = \" \".join([\"python\", target] + list(script_args))\n    exec_cluster(cluster_config_file, cmd, docker, screen, tmux, stop, False,\n                 cluster_name, port_forward)", "response": "Uploads and runs a script on the specified cluster."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild a whole graph for the model.", "response": "def build_graph(self):\n        \"\"\"Build a whole graph for the model.\"\"\"\n        self.global_step = tf.Variable(0, trainable=False)\n        self._build_model()\n        if self.mode == \"train\":\n            self._build_train_op()\n        else:\n            # Additional initialization for the test network.\n            self.variables = ray.experimental.tf_utils.TensorFlowVariables(\n                self.cost)\n            self.summaries = tf.summary.merge_all()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _build_model(self):\n\n        with tf.variable_scope(\"init\"):\n            x = self._conv(\"init_conv\", self._images, 3, 3, 16,\n                           self._stride_arr(1))\n\n        strides = [1, 2, 2]\n        activate_before_residual = [True, False, False]\n        if self.hps.use_bottleneck:\n            res_func = self._bottleneck_residual\n            filters = [16, 64, 128, 256]\n        else:\n            res_func = self._residual\n            filters = [16, 16, 32, 64]\n\n        with tf.variable_scope(\"unit_1_0\"):\n            x = res_func(x, filters[0], filters[1], self._stride_arr(\n                strides[0]), activate_before_residual[0])\n        for i in range(1, self.hps.num_residual_units):\n            with tf.variable_scope(\"unit_1_%d\" % i):\n                x = res_func(x, filters[1], filters[1], self._stride_arr(1),\n                             False)\n\n        with tf.variable_scope(\"unit_2_0\"):\n            x = res_func(x, filters[1], filters[2], self._stride_arr(\n                strides[1]), activate_before_residual[1])\n        for i in range(1, self.hps.num_residual_units):\n            with tf.variable_scope(\"unit_2_%d\" % i):\n                x = res_func(x, filters[2], filters[2], self._stride_arr(1),\n                             False)\n\n        with tf.variable_scope(\"unit_3_0\"):\n            x = res_func(x, filters[2], filters[3], self._stride_arr(\n                strides[2]), activate_before_residual[2])\n        for i in range(1, self.hps.num_residual_units):\n            with tf.variable_scope(\"unit_3_%d\" % i):\n                x = res_func(x, filters[3], filters[3], self._stride_arr(1),\n                             False)\n        with tf.variable_scope(\"unit_last\"):\n            x = self._batch_norm(\"final_bn\", x)\n            x = self._relu(x, self.hps.relu_leakiness)\n            x = self._global_avg_pool(x)\n\n        with tf.variable_scope(\"logit\"):\n            logits = self._fully_connected(x, self.hps.num_classes)\n            self.predictions = tf.nn.softmax(logits)\n\n        with tf.variable_scope(\"costs\"):\n            xent = tf.nn.softmax_cross_entropy_with_logits(\n                logits=logits, labels=self.labels)\n            self.cost = tf.reduce_mean(xent, name=\"xent\")\n            self.cost += self._decay()\n\n            if self.mode == \"eval\":\n                tf.summary.scalar(\"cost\", self.cost)", "response": "Builds the core model within the graph."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds training specific ops for the graph.", "response": "def _build_train_op(self):\n        \"\"\"Build training specific ops for the graph.\"\"\"\n        num_gpus = self.hps.num_gpus if self.hps.num_gpus != 0 else 1\n        # The learning rate schedule is dependent on the number of gpus.\n        boundaries = [int(20000 * i / np.sqrt(num_gpus)) for i in range(2, 5)]\n        values = [0.1, 0.01, 0.001, 0.0001]\n        self.lrn_rate = tf.train.piecewise_constant(self.global_step,\n                                                    boundaries, values)\n        tf.summary.scalar(\"learning rate\", self.lrn_rate)\n\n        if self.hps.optimizer == \"sgd\":\n            optimizer = tf.train.GradientDescentOptimizer(self.lrn_rate)\n        elif self.hps.optimizer == \"mom\":\n            optimizer = tf.train.MomentumOptimizer(self.lrn_rate, 0.9)\n\n        apply_op = optimizer.minimize(self.cost, global_step=self.global_step)\n        train_ops = [apply_op] + self._extra_train_ops\n        self.train_op = tf.group(*train_ops)\n        self.variables = ray.experimental.tf_utils.TensorFlowVariables(\n            self.train_op)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _fully_connected(self, x, out_dim):\n        x = tf.reshape(x, [self.hps.batch_size, -1])\n        w = tf.get_variable(\n            \"DW\", [x.get_shape()[1], out_dim],\n            initializer=tf.uniform_unit_scaling_initializer(factor=1.0))\n        b = tf.get_variable(\n            \"biases\", [out_dim], initializer=tf.constant_initializer())\n        return tf.nn.xw_plus_b(x, w, b)", "response": "FullyConnected layer for final output."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _mac(model, obs, h):\n    B, n_agents = obs.size(0), obs.size(1)\n    obs_flat = obs.reshape([B * n_agents, -1])\n    h_flat = [s.reshape([B * n_agents, -1]) for s in h]\n    q_flat, _, _, h_flat = model.forward({\"obs\": obs_flat}, h_flat)\n    return q_flat.reshape(\n        [B, n_agents, -1]), [s.reshape([B, n_agents, -1]) for s in h_flat]", "response": "Forward pass of the multi - agent controller."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nforward pass of the loss.", "response": "def forward(self, rewards, actions, terminated, mask, obs, action_mask):\n        \"\"\"Forward pass of the loss.\n\n        Arguments:\n            rewards: Tensor of shape [B, T-1, n_agents]\n            actions: Tensor of shape [B, T-1, n_agents]\n            terminated: Tensor of shape [B, T-1, n_agents]\n            mask: Tensor of shape [B, T-1, n_agents]\n            obs: Tensor of shape [B, T, n_agents, obs_size]\n            action_mask: Tensor of shape [B, T, n_agents, n_actions]\n        \"\"\"\n\n        B, T = obs.size(0), obs.size(1)\n\n        # Calculate estimated Q-Values\n        mac_out = []\n        h = [s.expand([B, self.n_agents, -1]) for s in self.model.state_init()]\n        for t in range(T):\n            q, h = _mac(self.model, obs[:, t], h)\n            mac_out.append(q)\n        mac_out = th.stack(mac_out, dim=1)  # Concat over time\n\n        # Pick the Q-Values for the actions taken -> [B * n_agents, T-1]\n        chosen_action_qvals = th.gather(\n            mac_out[:, :-1], dim=3, index=actions.unsqueeze(3)).squeeze(3)\n\n        # Calculate the Q-Values necessary for the target\n        target_mac_out = []\n        target_h = [\n            s.expand([B, self.n_agents, -1])\n            for s in self.target_model.state_init()\n        ]\n        for t in range(T):\n            target_q, target_h = _mac(self.target_model, obs[:, t], target_h)\n            target_mac_out.append(target_q)\n\n        # We don't need the first timesteps Q-Value estimate for targets\n        target_mac_out = th.stack(\n            target_mac_out[1:], dim=1)  # Concat across time\n\n        # Mask out unavailable actions\n        target_mac_out[action_mask[:, 1:] == 0] = -9999999\n\n        # Max over target Q-Values\n        if self.double_q:\n            # Get actions that maximise live Q (for double q-learning)\n            mac_out[action_mask == 0] = -9999999\n            cur_max_actions = mac_out[:, 1:].max(dim=3, keepdim=True)[1]\n            target_max_qvals = th.gather(target_mac_out, 3,\n                                         cur_max_actions).squeeze(3)\n        else:\n            target_max_qvals = target_mac_out.max(dim=3)[0]\n\n        # Mix\n        if self.mixer is not None:\n            # TODO(ekl) add support for handling global state? This is just\n            # treating the stacked agent obs as the state.\n            chosen_action_qvals = self.mixer(chosen_action_qvals, obs[:, :-1])\n            target_max_qvals = self.target_mixer(target_max_qvals, obs[:, 1:])\n\n        # Calculate 1-step Q-Learning targets\n        targets = rewards + self.gamma * (1 - terminated) * target_max_qvals\n\n        # Td-error\n        td_error = (chosen_action_qvals - targets.detach())\n\n        mask = mask.expand_as(td_error)\n\n        # 0-out the targets that came from padded data\n        masked_td_error = td_error * mask\n\n        # Normal L2 loss, take mean over actual data\n        loss = (masked_td_error**2).sum() / mask.sum()\n        return loss, mask, masked_td_error, chosen_action_qvals, targets"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nunpacks the action mask and the observation from agent grouping.", "response": "def _unpack_observation(self, obs_batch):\n        \"\"\"Unpacks the action mask / tuple obs from agent grouping.\n\n        Returns:\n            obs (Tensor): flattened obs tensor of shape [B, n_agents, obs_size]\n            mask (Tensor): action mask, if any\n        \"\"\"\n        unpacked = _unpack_obs(\n            np.array(obs_batch),\n            self.observation_space.original_space,\n            tensorlib=np)\n        if self.has_action_mask:\n            obs = np.concatenate(\n                [o[\"obs\"] for o in unpacked],\n                axis=1).reshape([len(obs_batch), self.n_agents, self.obs_size])\n            action_mask = np.concatenate(\n                [o[\"action_mask\"] for o in unpacked], axis=1).reshape(\n                    [len(obs_batch), self.n_agents, self.n_actions])\n        else:\n            obs = np.concatenate(\n                unpacked,\n                axis=1).reshape([len(obs_batch), self.n_agents, self.obs_size])\n            action_mask = np.ones(\n                [len(obs_batch), self.n_agents, self.n_actions])\n        return obs, action_mask"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_actor(name):\n    actor_name = _calculate_key(name)\n    pickled_state = _internal_kv_get(actor_name)\n    if pickled_state is None:\n        raise ValueError(\"The actor with name={} doesn't exist\".format(name))\n    handle = pickle.loads(pickled_state)\n    return handle", "response": "Get a named actor which was previously created."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nregisters an actor under a string key.", "response": "def register_actor(name, actor_handle):\n    \"\"\"Register a named actor under a string key.\n\n   Args:\n       name: The name of the named actor.\n       actor_handle: The actor object to be associated with this name\n   \"\"\"\n    if not isinstance(name, str):\n        raise TypeError(\"The name argument must be a string.\")\n    if not isinstance(actor_handle, ray.actor.ActorHandle):\n        raise TypeError(\"The actor_handle argument must be an ActorHandle \"\n                        \"object.\")\n    actor_name = _calculate_key(name)\n    pickled_state = pickle.dumps(actor_handle)\n\n    # Add the actor to Redis if it does not already exist.\n    already_exists = _internal_kv_put(actor_name, pickled_state)\n    if already_exists:\n        # If the registration fails, then erase the new actor handle that\n        # was added when pickling the actor handle.\n        actor_handle._ray_new_actor_handles.pop()\n        raise ValueError(\n            \"Error: the actor with name={} already exists\".format(name))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_extraneous(config, schema):\n    if not isinstance(config, dict):\n        raise ValueError(\"Config {} is not a dictionary\".format(config))\n    for k in config:\n        if k not in schema:\n            raise ValueError(\"Unexpected config key `{}` not in {}\".format(\n                k, list(schema.keys())))\n        v, kreq = schema[k]\n        if v is None:\n            continue\n        elif isinstance(v, type):\n            if not isinstance(config[k], v):\n                if v is str and isinstance(config[k], string_types):\n                    continue\n                raise ValueError(\n                    \"Config key `{}` has wrong type {}, expected {}\".format(\n                        k,\n                        type(config[k]).__name__, v.__name__))\n        else:\n            check_extraneous(config[k], v)", "response": "Make sure all items of config are in schema"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef validate_config(config, schema=CLUSTER_CONFIG_SCHEMA):\n    if not isinstance(config, dict):\n        raise ValueError(\"Config {} is not a dictionary\".format(config))\n\n    check_required(config, schema)\n    check_extraneous(config, schema)", "response": "Validate a cluster configuration."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the settings according to the keyword arguments.", "response": "def update(self, **kwargs):\n        \"\"\"Update the settings according to the keyword arguments.\n\n        Args:\n            kwargs: The keyword arguments to set corresponding fields.\n        \"\"\"\n        for arg in kwargs:\n            if hasattr(self, arg):\n                setattr(self, arg, kwargs[arg])\n            else:\n                raise ValueError(\"Invalid RayParams parameter in\"\n                                 \" update: %s\" % arg)\n\n        self._check_usage()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_if_absent(self, **kwargs):\n        for arg in kwargs:\n            if hasattr(self, arg):\n                if getattr(self, arg) is None:\n                    setattr(self, arg, kwargs[arg])\n            else:\n                raise ValueError(\"Invalid RayParams parameter in\"\n                                 \" update_if_absent: %s\" % arg)\n\n        self._check_usage()", "response": "Update the settings when the target fields are None."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef method(*args, **kwargs):\n    assert len(args) == 0\n    assert len(kwargs) == 1\n    assert \"num_return_vals\" in kwargs\n    num_return_vals = kwargs[\"num_return_vals\"]\n\n    def annotate_method(method):\n        method.__ray_num_return_vals__ = num_return_vals\n        return method\n\n    return annotate_method", "response": "Annotate an actor method."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_checkpoints_for_actor(actor_id):\n    checkpoint_info = ray.worker.global_state.actor_checkpoint_info(actor_id)\n    if checkpoint_info is None:\n        return []\n    checkpoints = [\n        Checkpoint(checkpoint_id, timestamp) for checkpoint_id, timestamp in\n        zip(checkpoint_info[\"CheckpointIds\"], checkpoint_info[\"Timestamps\"])\n    ]\n    return sorted(\n        checkpoints,\n        key=lambda checkpoint: checkpoint.timestamp,\n        reverse=True,\n    )", "response": "Get the available checkpoints for the given actor ID."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating an actor. handle", "response": "def remote(self, *args, **kwargs):\n        \"\"\"Create an actor.\n\n        Args:\n            args: These arguments are forwarded directly to the actor\n                constructor.\n            kwargs: These arguments are forwarded directly to the actor\n                constructor.\n\n        Returns:\n            A handle to the newly created actor.\n        \"\"\"\n        return self._remote(args=args, kwargs=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates an actor. This method allows more flexibility than the remote method.", "response": "def _remote(self,\n                args=None,\n                kwargs=None,\n                num_cpus=None,\n                num_gpus=None,\n                resources=None):\n        \"\"\"Create an actor.\n\n        This method allows more flexibility than the remote method because\n        resource requirements can be specified and override the defaults in the\n        decorator.\n\n        Args:\n            args: The arguments to forward to the actor constructor.\n            kwargs: The keyword arguments to forward to the actor constructor.\n            num_cpus: The number of CPUs required by the actor creation task.\n            num_gpus: The number of GPUs required by the actor creation task.\n            resources: The custom resources required by the actor creation\n                task.\n\n        Returns:\n            A handle to the newly created actor.\n        \"\"\"\n        if args is None:\n            args = []\n        if kwargs is None:\n            kwargs = {}\n\n        worker = ray.worker.get_global_worker()\n        if worker.mode is None:\n            raise Exception(\"Actors cannot be created before ray.init() \"\n                            \"has been called.\")\n\n        actor_id = ActorID(_random_string())\n        # The actor cursor is a dummy object representing the most recent\n        # actor method invocation. For each subsequent method invocation,\n        # the current cursor should be added as a dependency, and then\n        # updated to reflect the new invocation.\n        actor_cursor = None\n\n        # Set the actor's default resources if not already set. First three\n        # conditions are to check that no resources were specified in the\n        # decorator. Last three conditions are to check that no resources were\n        # specified when _remote() was called.\n        if (self._num_cpus is None and self._num_gpus is None\n                and self._resources is None and num_cpus is None\n                and num_gpus is None and resources is None):\n            # In the default case, actors acquire no resources for\n            # their lifetime, and actor methods will require 1 CPU.\n            cpus_to_use = ray_constants.DEFAULT_ACTOR_CREATION_CPU_SIMPLE\n            actor_method_cpu = ray_constants.DEFAULT_ACTOR_METHOD_CPU_SIMPLE\n        else:\n            # If any resources are specified (here or in decorator), then\n            # all resources are acquired for the actor's lifetime and no\n            # resources are associated with methods.\n            cpus_to_use = (ray_constants.DEFAULT_ACTOR_CREATION_CPU_SPECIFIED\n                           if self._num_cpus is None else self._num_cpus)\n            actor_method_cpu = ray_constants.DEFAULT_ACTOR_METHOD_CPU_SPECIFIED\n\n        # Do not export the actor class or the actor if run in LOCAL_MODE\n        # Instead, instantiate the actor locally and add it to the worker's\n        # dictionary\n        if worker.mode == ray.LOCAL_MODE:\n            worker.actors[actor_id] = self._modified_class(\n                *copy.deepcopy(args), **copy.deepcopy(kwargs))\n        else:\n            # Export the actor.\n            if not self._exported:\n                worker.function_actor_manager.export_actor_class(\n                    self._modified_class, self._actor_method_names)\n                self._exported = True\n\n            resources = ray.utils.resources_from_resource_arguments(\n                cpus_to_use, self._num_gpus, self._resources, num_cpus,\n                num_gpus, resources)\n\n            # If the actor methods require CPU resources, then set the required\n            # placement resources. If actor_placement_resources is empty, then\n            # the required placement resources will be the same as resources.\n            actor_placement_resources = {}\n            assert actor_method_cpu in [0, 1]\n            if actor_method_cpu == 1:\n                actor_placement_resources = resources.copy()\n                actor_placement_resources[\"CPU\"] += 1\n\n            function_name = \"__init__\"\n            function_signature = self._method_signatures[function_name]\n            creation_args = signature.extend_args(function_signature, args,\n                                                  kwargs)\n            function_descriptor = FunctionDescriptor(\n                self._modified_class.__module__, function_name,\n                self._modified_class.__name__)\n            [actor_cursor] = worker.submit_task(\n                function_descriptor,\n                creation_args,\n                actor_creation_id=actor_id,\n                max_actor_reconstructions=self._max_reconstructions,\n                num_return_vals=1,\n                resources=resources,\n                placement_resources=actor_placement_resources)\n            assert isinstance(actor_cursor, ObjectID)\n\n        actor_handle = ActorHandle(\n            actor_id, self._modified_class.__module__, self._class_name,\n            actor_cursor, self._actor_method_names, self._method_signatures,\n            self._actor_method_num_return_vals, actor_cursor, actor_method_cpu,\n            worker.task_driver_id)\n        # We increment the actor counter by 1 to account for the actor creation\n        # task.\n        actor_handle._ray_actor_counter += 1\n\n        return actor_handle"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _actor_method_call(self,\n                           method_name,\n                           args=None,\n                           kwargs=None,\n                           num_return_vals=None):\n        \"\"\"Method execution stub for an actor handle.\n\n        This is the function that executes when\n        `actor.method_name.remote(*args, **kwargs)` is called. Instead of\n        executing locally, the method is packaged as a task and scheduled\n        to the remote actor instance.\n\n        Args:\n            method_name: The name of the actor method to execute.\n            args: A list of arguments for the actor method.\n            kwargs: A dictionary of keyword arguments for the actor method.\n            num_return_vals (int): The number of return values for the method.\n\n        Returns:\n            object_ids: A list of object IDs returned by the remote actor\n                method.\n        \"\"\"\n        worker = ray.worker.get_global_worker()\n\n        worker.check_connected()\n\n        function_signature = self._ray_method_signatures[method_name]\n        if args is None:\n            args = []\n        if kwargs is None:\n            kwargs = {}\n        args = signature.extend_args(function_signature, args, kwargs)\n\n        # Execute functions locally if Ray is run in LOCAL_MODE\n        # Copy args to prevent the function from mutating them.\n        if worker.mode == ray.LOCAL_MODE:\n            return getattr(worker.actors[self._ray_actor_id],\n                           method_name)(*copy.deepcopy(args))\n\n        function_descriptor = FunctionDescriptor(\n            self._ray_module_name, method_name, self._ray_class_name)\n        with self._ray_actor_lock:\n            object_ids = worker.submit_task(\n                function_descriptor,\n                args,\n                actor_id=self._ray_actor_id,\n                actor_handle_id=self._ray_actor_handle_id,\n                actor_counter=self._ray_actor_counter,\n                actor_creation_dummy_object_id=(\n                    self._ray_actor_creation_dummy_object_id),\n                execution_dependencies=[self._ray_actor_cursor],\n                new_actor_handles=self._ray_new_actor_handles,\n                # We add one for the dummy return ID.\n                num_return_vals=num_return_vals + 1,\n                resources={\"CPU\": self._ray_actor_method_cpus},\n                placement_resources={},\n                driver_id=self._ray_actor_driver_id,\n            )\n            # Update the actor counter and cursor to reflect the most recent\n            # invocation.\n            self._ray_actor_counter += 1\n            # The last object returned is the dummy object that should be\n            # passed in to the next actor method. Do not return it to the user.\n            self._ray_actor_cursor = object_ids.pop()\n            # We have notified the backend of the new actor handles to expect\n            # since the last task was submitted, so clear the list.\n            self._ray_new_actor_handles = []\n\n        if len(object_ids) == 1:\n            object_ids = object_ids[0]\n        elif len(object_ids) == 0:\n            object_ids = None\n\n        return object_ids", "response": "Method execution stub for an actor handle."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _serialization_helper(self, ray_forking):\n        if ray_forking:\n            actor_handle_id = compute_actor_handle_id(\n                self._ray_actor_handle_id, self._ray_actor_forks)\n        else:\n            actor_handle_id = self._ray_actor_handle_id\n\n        # Note: _ray_actor_cursor and _ray_actor_creation_dummy_object_id\n        # could be None.\n        state = {\n            \"actor_id\": self._ray_actor_id,\n            \"actor_handle_id\": actor_handle_id,\n            \"module_name\": self._ray_module_name,\n            \"class_name\": self._ray_class_name,\n            \"actor_cursor\": self._ray_actor_cursor,\n            \"actor_method_names\": self._ray_actor_method_names,\n            \"method_signatures\": self._ray_method_signatures,\n            \"method_num_return_vals\": self._ray_method_num_return_vals,\n            # Actors in local mode don't have dummy objects.\n            \"actor_creation_dummy_object_id\": self.\n            _ray_actor_creation_dummy_object_id,\n            \"actor_method_cpus\": self._ray_actor_method_cpus,\n            \"actor_driver_id\": self._ray_actor_driver_id,\n            \"ray_forking\": ray_forking\n        }\n\n        if ray_forking:\n            self._ray_actor_forks += 1\n            new_actor_handle_id = actor_handle_id\n        else:\n            # The execution dependency for a pickled actor handle is never safe\n            # to release, since it could be unpickled and submit another\n            # dependent task at any time. Therefore, we notify the backend of a\n            # random handle ID that will never actually be used.\n            new_actor_handle_id = ActorHandleID(_random_string())\n        # Notify the backend to expect this new actor handle. The backend will\n        # not release the cursor for any new handles until the first task for\n        # each of the new handles is submitted.\n        # NOTE(swang): There is currently no garbage collection for actor\n        # handles until the actor itself is removed.\n        self._ray_new_actor_handles.append(new_actor_handle_id)\n\n        return state", "response": "Internal function that is used by the pickling thread."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _deserialization_helper(self, state, ray_forking):\n        worker = ray.worker.get_global_worker()\n        worker.check_connected()\n\n        if state[\"ray_forking\"]:\n            actor_handle_id = state[\"actor_handle_id\"]\n        else:\n            # Right now, if the actor handle has been pickled, we create a\n            # temporary actor handle id for invocations.\n            # TODO(pcm): This still leads to a lot of actor handles being\n            # created, there should be a better way to handle pickled\n            # actor handles.\n            # TODO(swang): Accessing the worker's current task ID is not\n            # thread-safe.\n            # TODO(swang): Unpickling the same actor handle twice in the same\n            # task will break the application, and unpickling it twice in the\n            # same actor is likely a performance bug. We should consider\n            # logging a warning in these cases.\n            actor_handle_id = compute_actor_handle_id_non_forked(\n                state[\"actor_handle_id\"], worker.current_task_id)\n\n        self.__init__(\n            state[\"actor_id\"],\n            state[\"module_name\"],\n            state[\"class_name\"],\n            state[\"actor_cursor\"],\n            state[\"actor_method_names\"],\n            state[\"method_signatures\"],\n            state[\"method_num_return_vals\"],\n            state[\"actor_creation_dummy_object_id\"],\n            state[\"actor_method_cpus\"],\n            # This is the driver ID of the driver that owns the actor, not\n            # necessarily the driver that owns this actor handle.\n            state[\"actor_driver_id\"],\n            actor_handle_id=actor_handle_id)", "response": "This is the method that is called by the pickler. It is called by the pickler when the actor is pickled."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_data(self, sess, inputs, state_inputs):\n\n        if log_once(\"load_data\"):\n            logger.info(\n                \"Training on concatenated sample batches:\\n\\n{}\\n\".format(\n                    summarize({\n                        \"placeholders\": self.loss_inputs,\n                        \"inputs\": inputs,\n                        \"state_inputs\": state_inputs\n                    })))\n\n        feed_dict = {}\n        assert len(self.loss_inputs) == len(inputs + state_inputs), \\\n            (self.loss_inputs, inputs, state_inputs)\n\n        # Let's suppose we have the following input data, and 2 devices:\n        # 1 2 3 4 5 6 7                              <- state inputs shape\n        # A A A B B B C C C D D D E E E F F F G G G  <- inputs shape\n        # The data is truncated and split across devices as follows:\n        # |---| seq len = 3\n        # |---------------------------------| seq batch size = 6 seqs\n        # |----------------| per device batch size = 9 tuples\n\n        if len(state_inputs) > 0:\n            smallest_array = state_inputs[0]\n            seq_len = len(inputs[0]) // len(state_inputs[0])\n            self._loaded_max_seq_len = seq_len\n        else:\n            smallest_array = inputs[0]\n            self._loaded_max_seq_len = 1\n\n        sequences_per_minibatch = (\n            self.max_per_device_batch_size // self._loaded_max_seq_len * len(\n                self.devices))\n        if sequences_per_minibatch < 1:\n            logger.warn(\n                (\"Target minibatch size is {}, however the rollout sequence \"\n                 \"length is {}, hence the minibatch size will be raised to \"\n                 \"{}.\").format(self.max_per_device_batch_size,\n                               self._loaded_max_seq_len,\n                               self._loaded_max_seq_len * len(self.devices)))\n            sequences_per_minibatch = 1\n\n        if len(smallest_array) < sequences_per_minibatch:\n            # Dynamically shrink the batch size if insufficient data\n            sequences_per_minibatch = make_divisible_by(\n                len(smallest_array), len(self.devices))\n\n        if log_once(\"data_slicing\"):\n            logger.info(\n                (\"Divided {} rollout sequences, each of length {}, among \"\n                 \"{} devices.\").format(\n                     len(smallest_array), self._loaded_max_seq_len,\n                     len(self.devices)))\n\n        if sequences_per_minibatch < len(self.devices):\n            raise ValueError(\n                \"Must load at least 1 tuple sequence per device. Try \"\n                \"increasing `sgd_minibatch_size` or reducing `max_seq_len` \"\n                \"to ensure that at least one sequence fits per device.\")\n        self._loaded_per_device_batch_size = (sequences_per_minibatch // len(\n            self.devices) * self._loaded_max_seq_len)\n\n        if len(state_inputs) > 0:\n            # First truncate the RNN state arrays to the sequences_per_minib.\n            state_inputs = [\n                make_divisible_by(arr, sequences_per_minibatch)\n                for arr in state_inputs\n            ]\n            # Then truncate the data inputs to match\n            inputs = [arr[:len(state_inputs[0]) * seq_len] for arr in inputs]\n            assert len(state_inputs[0]) * seq_len == len(inputs[0]), \\\n                (len(state_inputs[0]), sequences_per_minibatch, seq_len,\n                 len(inputs[0]))\n            for ph, arr in zip(self.loss_inputs, inputs + state_inputs):\n                feed_dict[ph] = arr\n            truncated_len = len(inputs[0])\n        else:\n            for ph, arr in zip(self.loss_inputs, inputs + state_inputs):\n                truncated_arr = make_divisible_by(arr, sequences_per_minibatch)\n                feed_dict[ph] = truncated_arr\n                truncated_len = len(truncated_arr)\n\n        sess.run([t.init_op for t in self._towers], feed_dict=feed_dict)\n\n        self.num_tuples_loaded = truncated_len\n        tuples_per_device = truncated_len // len(self.devices)\n        assert tuples_per_device > 0, \"No data loaded?\"\n        assert tuples_per_device % self._loaded_per_device_batch_size == 0\n        return tuples_per_device", "response": "Bulk loads the specified inputs into device memory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun a single step of SGD.", "response": "def optimize(self, sess, batch_index):\n        \"\"\"Run a single step of SGD.\n\n        Runs a SGD step over a slice of the preloaded batch with size given by\n        self._loaded_per_device_batch_size and offset given by the batch_index\n        argument.\n\n        Updates shared model weights based on the averaged per-device\n        gradients.\n\n        Args:\n            sess: TensorFlow session.\n            batch_index: Offset into the preloaded data. This value must be\n                between `0` and `tuples_per_device`. The amount of data to\n                process is at most `max_per_device_batch_size`.\n\n        Returns:\n            The outputs of extra_ops evaluated over the batch.\n        \"\"\"\n        feed_dict = {\n            self._batch_index: batch_index,\n            self._per_device_batch_size: self._loaded_per_device_batch_size,\n            self._max_seq_len: self._loaded_max_seq_len,\n        }\n        for tower in self._towers:\n            feed_dict.update(tower.loss_graph.extra_compute_grad_feed_dict())\n\n        fetches = {\"train\": self._train_op}\n        for tower in self._towers:\n            fetches.update(tower.loss_graph.extra_compute_grad_fetches())\n\n        return sess.run(fetches, feed_dict=feed_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _next_generation(self, sorted_trials):\n\n        candidate = []\n        next_generation = []\n        num_population = self._next_population_size(len(sorted_trials))\n        top_num = int(max(num_population * self._keep_top_ratio, 2))\n\n        for i in range(top_num):\n            candidate.append(sorted_trials[i].extra_arg)\n            next_generation.append(sorted_trials[i].extra_arg)\n\n        for i in range(top_num, num_population):\n            flip_coin = np.random.uniform()\n            if flip_coin < self._selection_bound:\n                next_generation.append(GeneticSearch._selection(candidate))\n            else:\n                if flip_coin < self._selection_bound + self._crossover_bound:\n                    next_generation.append(GeneticSearch._crossover(candidate))\n                else:\n                    next_generation.append(GeneticSearch._mutation(candidate))\n        return next_generation", "response": "Generate genes for the next generation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nperforming selection action to candidates.", "response": "def _selection(candidate):\n        \"\"\"Perform selection action to candidates.\n\n        For example, new gene = sample_1 + the 5th bit of sample2.\n\n        Args:\n            candidate: List of candidate genes (encodings).\n\n        Examples:\n            >>> # Genes that represent 3 parameters\n            >>> gene1 = np.array([[0, 0, 1], [0, 1], [1, 0]])\n            >>> gene2 = np.array([[0, 1, 0], [1, 0], [0, 1]])\n            >>> new_gene = _selection([gene1, gene2])\n            >>> # new_gene could be gene1 overwritten with the\n            >>> # 2nd parameter of gene2\n            >>> # in which case:\n            >>> #   new_gene[0] = gene1[0]\n            >>> #   new_gene[1] = gene2[1]\n            >>> #   new_gene[2] = gene1[0]\n\n        Returns:\n            New gene (encoding)\n        \"\"\"\n        sample_index1 = np.random.choice(len(candidate))\n        sample_index2 = np.random.choice(len(candidate))\n        sample_1 = candidate[sample_index1]\n        sample_2 = candidate[sample_index2]\n        select_index = np.random.choice(len(sample_1))\n        logger.info(\n            LOGGING_PREFIX + \"Perform selection from %sth to %sth at index=%s\",\n            sample_index2, sample_index1, select_index)\n\n        next_gen = []\n        for i in range(len(sample_1)):\n            if i is select_index:\n                next_gen.append(sample_2[i])\n            else:\n                next_gen.append(sample_1[i])\n        return next_gen"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _crossover(candidate):\n        sample_index1 = np.random.choice(len(candidate))\n        sample_index2 = np.random.choice(len(candidate))\n        sample_1 = candidate[sample_index1]\n        sample_2 = candidate[sample_index2]\n        cross_index = int(len(sample_1) * np.random.uniform(low=0.3, high=0.7))\n        logger.info(\n            LOGGING_PREFIX +\n            \"Perform crossover between %sth and %sth at index=%s\",\n            sample_index1, sample_index2, cross_index)\n\n        next_gen = []\n        for i in range(len(sample_1)):\n            if i > cross_index:\n                next_gen.append(sample_2[i])\n            else:\n                next_gen.append(sample_1[i])\n        return next_gen", "response": "Perform crossover action to candidates."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _mutation(candidate, rate=0.1):\n        sample_index = np.random.choice(len(candidate))\n        sample = candidate[sample_index]\n        idx_list = []\n        for i in range(int(max(len(sample) * rate, 1))):\n            idx = np.random.choice(len(sample))\n            idx_list.append(idx)\n\n            field = sample[idx]  # one-hot encoding\n            field[np.argmax(field)] = 0\n            bit = np.random.choice(field.shape[0])\n            field[bit] = 1\n\n        logger.info(LOGGING_PREFIX + \"Perform mutation on %sth at index=%s\",\n                    sample_index, str(idx_list))\n        return sample", "response": "Perform a random mutation action to candidates."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef list_trials(experiment_path, sort, output, filter_op, columns,\n                result_columns):\n    \"\"\"Lists trials in the directory subtree starting at the given path.\"\"\"\n    if columns:\n        columns = columns.split(\",\")\n    if result_columns:\n        result_columns = result_columns.split(\",\")\n    commands.list_trials(experiment_path, sort, output, filter_op, columns,\n                         result_columns)", "response": "Lists trials in the directory subtree starting at the given path."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlisting experiments in the directory subtree.", "response": "def list_experiments(project_path, sort, output, filter_op, columns):\n    \"\"\"Lists experiments in the directory subtree.\"\"\"\n    if columns:\n        columns = columns.split(\",\")\n    commands.list_experiments(project_path, sort, output, filter_op, columns)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstarting one iteration of training and save remote id.", "response": "def _train(self, trial):\n        \"\"\"Start one iteration of training and save remote id.\"\"\"\n\n        assert trial.status == Trial.RUNNING, trial.status\n        remote = trial.runner.train.remote()\n\n        # Local Mode\n        if isinstance(remote, dict):\n            remote = _LocalWrapper(remote)\n\n        self._running[remote] = trial"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstarts trial and restores last result if trial was paused.", "response": "def _start_trial(self, trial, checkpoint=None):\n        \"\"\"Starts trial and restores last result if trial was paused.\n\n        Raises:\n            ValueError if restoring from checkpoint fails.\n        \"\"\"\n        prior_status = trial.status\n        self.set_status(trial, Trial.RUNNING)\n        trial.runner = self._setup_runner(\n            trial,\n            reuse_allowed=checkpoint is not None\n            or trial._checkpoint.value is not None)\n        if not self.restore(trial, checkpoint):\n            if trial.status == Trial.ERROR:\n                raise RuntimeError(\n                    \"Restore from checkpoint failed for Trial {}.\".format(\n                        str(trial)))\n\n        previous_run = self._find_item(self._paused, trial)\n        if (prior_status == Trial.PAUSED and previous_run):\n            # If Trial was in flight when paused, self._paused stores result.\n            self._paused.pop(previous_run[0])\n            self._running[previous_run[0]] = trial\n        else:\n            self._train(trial)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstops this trial. Stops this trial, releasing all allocating resources. If stopping the trial fails, the run will be marked as terminated in error, but no exception will be thrown. Args: error (bool): Whether to mark this trial as terminated in error. error_msg (str): Optional error message. stop_logger (bool): Whether to shut down the trial logger.", "response": "def _stop_trial(self, trial, error=False, error_msg=None,\n                    stop_logger=True):\n        \"\"\"Stops this trial.\n\n        Stops this trial, releasing all allocating resources. If stopping the\n        trial fails, the run will be marked as terminated in error, but no\n        exception will be thrown.\n\n        Args:\n            error (bool): Whether to mark this trial as terminated in error.\n            error_msg (str): Optional error message.\n            stop_logger (bool): Whether to shut down the trial logger.\n        \"\"\"\n\n        if stop_logger:\n            trial.close_logger()\n\n        if error:\n            self.set_status(trial, Trial.ERROR)\n        else:\n            self.set_status(trial, Trial.TERMINATED)\n\n        try:\n            trial.write_error_log(error_msg)\n            if hasattr(trial, \"runner\") and trial.runner:\n                if (not error and self._reuse_actors\n                        and self._cached_actor is None):\n                    logger.debug(\"Reusing actor for {}\".format(trial.runner))\n                    self._cached_actor = trial.runner\n                else:\n                    logger.info(\n                        \"Destroying actor for trial {}. If your trainable is \"\n                        \"slow to initialize, consider setting \"\n                        \"reuse_actors=True to reduce actor creation \"\n                        \"overheads.\".format(trial))\n                    trial.runner.stop.remote()\n                    trial.runner.__ray_terminate__.remote()\n        except Exception:\n            logger.exception(\"Error stopping runner for Trial %s\", str(trial))\n            self.set_status(trial, Trial.ERROR)\n        finally:\n            trial.runner = None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstart the trial. Will not return resources if trial repeatedly fails on start. Args: trial (Trial): Trial to be started. checkpoint (Checkpoint): A Python object or path storing the state of trial.", "response": "def start_trial(self, trial, checkpoint=None):\n        \"\"\"Starts the trial.\n\n        Will not return resources if trial repeatedly fails on start.\n\n        Args:\n            trial (Trial): Trial to be started.\n            checkpoint (Checkpoint): A Python object or path storing the state\n                of trial.\n        \"\"\"\n\n        self._commit_resources(trial.resources)\n        try:\n            self._start_trial(trial, checkpoint)\n        except Exception as e:\n            logger.exception(\"Error starting runner for Trial %s\", str(trial))\n            error_msg = traceback.format_exc()\n            time.sleep(2)\n            self._stop_trial(trial, error=True, error_msg=error_msg)\n            if isinstance(e, AbortTrialExecution):\n                return  # don't retry fatal Tune errors\n            try:\n                # This forces the trial to not start from checkpoint.\n                trial.clear_checkpoint()\n                logger.info(\n                    \"Trying to start runner for Trial %s without checkpoint.\",\n                    str(trial))\n                self._start_trial(trial)\n            except Exception:\n                logger.exception(\n                    \"Error starting runner for Trial %s, aborting!\",\n                    str(trial))\n                error_msg = traceback.format_exc()\n                self._stop_trial(trial, error=True, error_msg=error_msg)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pause_trial(self, trial):\n\n        trial_future = self._find_item(self._running, trial)\n        if trial_future:\n            self._paused[trial_future[0]] = trial\n        super(RayTrialExecutor, self).pause_trial(trial)", "response": "Pauses the trial.\n\n        If trial is in-flight, preserves return value in separate queue\n        before pausing, which is restored when Trial is resumed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef reset_trial(self, trial, new_config, new_experiment_tag):\n        trial.experiment_tag = new_experiment_tag\n        trial.config = new_config\n        trainable = trial.runner\n        with warn_if_slow(\"reset_config\"):\n            reset_val = ray.get(trainable.reset_config.remote(new_config))\n        return reset_val", "response": "Tries to reset the trial s configuration and experiment tag to new_config."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfetches one result of the running trials. Returns a Result of the most recent trial training run.", "response": "def fetch_result(self, trial):\n        \"\"\"Fetches one result of the running trials.\n\n        Returns:\n            Result of the most recent trial training run.\"\"\"\n        trial_future = self._find_item(self._running, trial)\n        if not trial_future:\n            raise ValueError(\"Trial was not running.\")\n        self._running.pop(trial_future[0])\n        with warn_if_slow(\"fetch_result\"):\n            result = ray.get(trial_future[0])\n\n        # For local mode\n        if isinstance(result, _LocalWrapper):\n            result = result.unwrap()\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef has_resources(self, resources):\n        if time.time() - self._last_resource_refresh > self._refresh_period:\n            self._update_avail_resources()\n\n        currently_available = Resources.subtract(self._avail_resources,\n                                                 self._committed_resources)\n\n        have_space = (\n            resources.cpu_total() <= currently_available.cpu\n            and resources.gpu_total() <= currently_available.gpu and all(\n                resources.get_res_total(res) <= currently_available.get(res)\n                for res in resources.custom_resources))\n\n        if have_space:\n            return True\n\n        can_overcommit = self._queue_trials\n\n        if (resources.cpu_total() > 0 and currently_available.cpu <= 0) or \\\n           (resources.gpu_total() > 0 and currently_available.gpu <= 0) or \\\n           any((resources.get_res_total(res_name) > 0\n                and currently_available.get(res_name) <= 0)\n               for res_name in resources.custom_resources):\n            can_overcommit = False  # requested resource is already saturated\n\n        if can_overcommit:\n            logger.warning(\n                \"Allowing trial to start even though the \"\n                \"cluster does not have enough free resources. Trial actors \"\n                \"may appear to hang until enough resources are added to the \"\n                \"cluster (e.g., via autoscaling). You can disable this \"\n                \"behavior by specifying `queue_trials=False` in \"\n                \"ray.tune.run().\")\n            return True\n\n        return False", "response": "Returns whether this runner has at least the specified resources."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef debug_string(self):\n\n        if self._resources_initialized:\n            status = \"Resources requested: {}/{} CPUs, {}/{} GPUs\".format(\n                self._committed_resources.cpu, self._avail_resources.cpu,\n                self._committed_resources.gpu, self._avail_resources.gpu)\n            customs = \", \".join([\n                \"{}/{} {}\".format(\n                    self._committed_resources.get_res_total(name),\n                    self._avail_resources.get_res_total(name), name)\n                for name in self._avail_resources.custom_resources\n            ])\n            if customs:\n                status += \" ({})\".format(customs)\n            return status\n        else:\n            return \"Resources requested: ?\"", "response": "Returns a human readable message for printing to the console."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef resource_string(self):\n\n        if self._resources_initialized:\n            res_str = \"{} CPUs, {} GPUs\".format(self._avail_resources.cpu,\n                                                self._avail_resources.gpu)\n            if self._avail_resources.custom_resources:\n                custom = \", \".join(\n                    \"{} {}\".format(\n                        self._avail_resources.get_res_total(name), name)\n                    for name in self._avail_resources.custom_resources)\n                res_str += \" ({})\".format(custom)\n            return res_str\n        else:\n            return \"? CPUs, ? GPUs\"", "response": "Returns a string describing the total resources available."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsaving the trial s state to a checkpoint.", "response": "def save(self, trial, storage=Checkpoint.DISK):\n        \"\"\"Saves the trial's state to a checkpoint.\"\"\"\n        trial._checkpoint.storage = storage\n        trial._checkpoint.last_result = trial.last_result\n        if storage == Checkpoint.MEMORY:\n            trial._checkpoint.value = trial.runner.save_to_object.remote()\n        else:\n            # Keeps only highest performing checkpoints if enabled\n            if trial.keep_checkpoints_num:\n                try:\n                    last_attr_val = trial.last_result[\n                        trial.checkpoint_score_attr]\n                    if (trial.compare_checkpoints(last_attr_val)\n                            and not math.isnan(last_attr_val)):\n                        trial.best_checkpoint_attr_value = last_attr_val\n                        self._checkpoint_and_erase(trial)\n                except KeyError:\n                    logger.warning(\n                        \"Result dict has no key: {}. keep\"\n                        \"_checkpoints_num flag will not work\".format(\n                            trial.checkpoint_score_attr))\n            else:\n                with warn_if_slow(\"save_to_disk\"):\n                    trial._checkpoint.value = ray.get(\n                        trial.runner.save.remote())\n\n        return trial._checkpoint.value"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _checkpoint_and_erase(self, trial):\n\n        with warn_if_slow(\"save_to_disk\"):\n            trial._checkpoint.value = ray.get(trial.runner.save.remote())\n\n        if len(trial.history) >= trial.keep_checkpoints_num:\n            ray.get(trial.runner.delete_checkpoint.remote(trial.history[-1]))\n            trial.history.pop()\n\n        trial.history.insert(0, trial._checkpoint.value)", "response": "Checkpoints the model and erases old checkpoints\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrestoring training state from a given model checkpoint.", "response": "def restore(self, trial, checkpoint=None):\n        \"\"\"Restores training state from a given model checkpoint.\n\n        This will also sync the trial results to a new location\n        if restoring on a different node.\n        \"\"\"\n        if checkpoint is None or checkpoint.value is None:\n            checkpoint = trial._checkpoint\n        if checkpoint is None or checkpoint.value is None:\n            return True\n        if trial.runner is None:\n            logger.error(\"Unable to restore - no runner.\")\n            self.set_status(trial, Trial.ERROR)\n            return False\n        try:\n            value = checkpoint.value\n            if checkpoint.storage == Checkpoint.MEMORY:\n                assert type(value) != Checkpoint, type(value)\n                trial.runner.restore_from_object.remote(value)\n            else:\n                worker_ip = ray.get(trial.runner.current_ip.remote())\n                trial.sync_logger_to_new_location(worker_ip)\n                with warn_if_slow(\"restore_from_disk\"):\n                    ray.get(trial.runner.restore.remote(value))\n            trial.last_result = checkpoint.last_result\n            return True\n        except Exception:\n            logger.exception(\"Error restoring runner for Trial %s.\", trial)\n            self.set_status(trial, Trial.ERROR)\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef export_trial_if_needed(self, trial):\n        if trial.export_formats and len(trial.export_formats) > 0:\n            return ray.get(\n                trial.runner.export_model.remote(trial.export_formats))\n        return {}", "response": "Exports model of this trial based on export_formats."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates an actor that will execute a particular instance of the logical operator.", "response": "def __generate_actor(self, instance_id, operator, input, output):\n        \"\"\"Generates an actor that will execute a particular instance of\n        the logical operator\n\n        Attributes:\n            instance_id (UUID): The id of the instance the actor will execute.\n            operator (Operator): The metadata of the logical operator.\n            input (DataInput): The input gate that manages input channels of\n            the instance (see: DataInput in communication.py).\n            input (DataOutput): The output gate that manages output channels\n            of the instance (see: DataOutput in communication.py).\n        \"\"\"\n        actor_id = (operator.id, instance_id)\n        # Record the physical dataflow graph (for debugging purposes)\n        self.__add_channel(actor_id, input, output)\n        # Select actor to construct\n        if operator.type == OpType.Source:\n            source = operator_instance.Source.remote(actor_id, operator, input,\n                                                     output)\n            source.register_handle.remote(source)\n            return source.start.remote()\n        elif operator.type == OpType.Map:\n            map = operator_instance.Map.remote(actor_id, operator, input,\n                                               output)\n            map.register_handle.remote(map)\n            return map.start.remote()\n        elif operator.type == OpType.FlatMap:\n            flatmap = operator_instance.FlatMap.remote(actor_id, operator,\n                                                       input, output)\n            flatmap.register_handle.remote(flatmap)\n            return flatmap.start.remote()\n        elif operator.type == OpType.Filter:\n            filter = operator_instance.Filter.remote(actor_id, operator, input,\n                                                     output)\n            filter.register_handle.remote(filter)\n            return filter.start.remote()\n        elif operator.type == OpType.Reduce:\n            reduce = operator_instance.Reduce.remote(actor_id, operator, input,\n                                                     output)\n            reduce.register_handle.remote(reduce)\n            return reduce.start.remote()\n        elif operator.type == OpType.TimeWindow:\n            pass\n        elif operator.type == OpType.KeyBy:\n            keyby = operator_instance.KeyBy.remote(actor_id, operator, input,\n                                                   output)\n            keyby.register_handle.remote(keyby)\n            return keyby.start.remote()\n        elif operator.type == OpType.Sum:\n            sum = operator_instance.Reduce.remote(actor_id, operator, input,\n                                                  output)\n            # Register target handle at state actor\n            state_actor = operator.state_actor\n            if state_actor is not None:\n                state_actor.register_target.remote(sum)\n            # Register own handle\n            sum.register_handle.remote(sum)\n            return sum.start.remote()\n        elif operator.type == OpType.Sink:\n            pass\n        elif operator.type == OpType.Inspect:\n            inspect = operator_instance.Inspect.remote(actor_id, operator,\n                                                       input, output)\n            inspect.register_handle.remote(inspect)\n            return inspect.start.remote()\n        elif operator.type == OpType.ReadTextFile:\n            # TODO (john): Colocate the source with the input file\n            read = operator_instance.ReadTextFile.remote(\n                actor_id, operator, input, output)\n            read.register_handle.remote(read)\n            return read.start.remote()\n        else:  # TODO (john): Add support for other types of operators\n            sys.exit(\"Unrecognized or unsupported {} operator type.\".format(\n                operator.type))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate one actor for each instance of the given logicalCOOKIE.", "response": "def __generate_actors(self, operator, upstream_channels,\n                          downstream_channels):\n        \"\"\"Generates one actor for each instance of the given logical\n        operator.\n\n        Attributes:\n            operator (Operator): The logical operator metadata.\n            upstream_channels (list): A list of all upstream channels for\n            all instances of the operator.\n            downstream_channels (list): A list of all downstream channels\n            for all instances of the operator.\n        \"\"\"\n        num_instances = operator.num_instances\n        logger.info(\"Generating {} actors of type {}...\".format(\n            num_instances, operator.type))\n        in_channels = upstream_channels.pop(\n            operator.id) if upstream_channels else []\n        handles = []\n        for i in range(num_instances):\n            # Collect input and output channels for the particular instance\n            ip = [\n                channel for channel in in_channels\n                if channel.dst_instance_id == i\n            ] if in_channels else []\n            op = [\n                channel for channels_list in downstream_channels.values()\n                for channel in channels_list if channel.src_instance_id == i\n            ]\n            log = \"Constructed {} input and {} output channels \"\n            log += \"for the {}-th instance of the {} operator.\"\n            logger.debug(log.format(len(ip), len(op), i, operator.type))\n            input_gate = DataInput(ip)\n            output_gate = DataOutput(op, operator.partitioning_strategies)\n            handle = self.__generate_actor(i, operator, input_gate,\n                                           output_gate)\n            if handle:\n                handles.append(handle)\n        return handles"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _generate_channels(self, operator):\n        channels = {}  # destination operator id -> channels\n        strategies = operator.partitioning_strategies\n        for dst_operator, p_scheme in strategies.items():\n            num_dest_instances = self.operators[dst_operator].num_instances\n            entry = channels.setdefault(dst_operator, [])\n            if p_scheme.strategy == PStrategy.Forward:\n                for i in range(operator.num_instances):\n                    # ID of destination instance to connect\n                    id = i % num_dest_instances\n                    channel = DataChannel(self, operator.id, dst_operator, i,\n                                          id)\n                    entry.append(channel)\n            elif p_scheme.strategy in all_to_all_strategies:\n                for i in range(operator.num_instances):\n                    for j in range(num_dest_instances):\n                        channel = DataChannel(self, operator.id, dst_operator,\n                                              i, j)\n                        entry.append(channel)\n            else:\n                # TODO (john): Add support for other partitioning strategies\n                sys.exit(\"Unrecognized or unsupported partitioning strategy.\")\n        return channels", "response": "Generates all output data channels for all instances of the given logical operator."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndeploy and executes the physical dataflow.", "response": "def execute(self):\n        \"\"\"Deploys and executes the physical dataflow.\"\"\"\n        self._collect_garbage()  # Make sure everything is clean\n        # TODO (john): Check if dataflow has any 'logical inconsistencies'\n        # For example, if there is a forward partitioning strategy but\n        # the number of downstream instances is larger than the number of\n        # upstream instances, some of the downstream instances will not be\n        # used at all\n\n        # Each operator instance is implemented as a Ray actor\n        # Actors are deployed in topological order, as we traverse the\n        # logical dataflow from sources to sinks. At each step, data\n        # producers wait for acknowledge from consumers before starting\n        # generating data.\n        upstream_channels = {}\n        for node in nx.topological_sort(self.logical_topo):\n            operator = self.operators[node]\n            # Generate downstream data channels\n            downstream_channels = self._generate_channels(operator)\n            # Instantiate Ray actors\n            handles = self.__generate_actors(operator, upstream_channels,\n                                             downstream_channels)\n            if handles:\n                self.actor_handles.extend(handles)\n            upstream_channels.update(downstream_channels)\n        logger.debug(\"Running...\")\n        return self.actor_handles"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nregisters the given logical operator to the environment and connects it to its upstream operator.", "response": "def __register(self, operator):\n        \"\"\"Registers the given logical operator to the environment and\n        connects it to its upstream operator (if any).\n\n        A call to this function adds a new edge to the logical topology.\n\n        Attributes:\n             operator (Operator): The metadata of the logical operator.\n        \"\"\"\n        self.env.operators[operator.id] = operator\n        self.dst_operator_id = operator.id\n        logger.debug(\"Adding new dataflow edge ({},{}) --> ({},{})\".format(\n            self.src_operator_id,\n            self.env.operators[self.src_operator_id].name,\n            self.dst_operator_id,\n            self.env.operators[self.dst_operator_id].name))\n        # Update logical dataflow graphs\n        self.env._add_edge(self.src_operator_id, self.dst_operator_id)\n        # Keep track of the partitioning strategy and the destination operator\n        src_operator = self.env.operators[self.src_operator_id]\n        if self.is_partitioned is True:\n            partitioning, _ = src_operator._get_partition_strategy(self.id)\n            src_operator._set_partition_strategy(_generate_uuid(),\n                                                 partitioning, operator.id)\n        elif src_operator.type == OpType.KeyBy:\n            # Set the output partitioning strategy to shuffle by key\n            partitioning = PScheme(PStrategy.ShuffleByKey)\n            src_operator._set_partition_strategy(_generate_uuid(),\n                                                 partitioning, operator.id)\n        else:  # No partitioning strategy has been defined - set default\n            partitioning = PScheme(PStrategy.Forward)\n            src_operator._set_partition_strategy(_generate_uuid(),\n                                                 partitioning, operator.id)\n        return self.__expand()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the number of instances for the source operator of the stream.", "response": "def set_parallelism(self, num_instances):\n        \"\"\"Sets the number of instances for the source operator of the stream.\n\n        Attributes:\n             num_instances (int): The level of parallelism for the source\n             operator of the stream.\n        \"\"\"\n        assert (num_instances > 0)\n        self.env._set_parallelism(self.src_operator_id, num_instances)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef map(self, map_fn, name=\"Map\"):\n        op = Operator(\n            _generate_uuid(),\n            OpType.Map,\n            name,\n            map_fn,\n            num_instances=self.env.config.parallelism)\n        return self.__register(op)", "response": "Applies a map operator to the stream."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef flat_map(self, flatmap_fn):\n        op = Operator(\n            _generate_uuid(),\n            OpType.FlatMap,\n            \"FlatMap\",\n            flatmap_fn,\n            num_instances=self.env.config.parallelism)\n        return self.__register(op)", "response": "Applies a flatmap operator to the stream."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef key_by(self, key_selector):\n        op = Operator(\n            _generate_uuid(),\n            OpType.KeyBy,\n            \"KeyBy\",\n            other=key_selector,\n            num_instances=self.env.config.parallelism)\n        return self.__register(op)", "response": "Applies a key_by operator to the stream."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\napplying a rolling sum operator to the stream.", "response": "def reduce(self, reduce_fn):\n        \"\"\"Applies a rolling sum operator to the stream.\n\n        Attributes:\n             sum_attribute_index (int): The index of the attribute to sum\n             (assuming tuple records).\n        \"\"\"\n        op = Operator(\n            _generate_uuid(),\n            OpType.Reduce,\n            \"Sum\",\n            reduce_fn,\n            num_instances=self.env.config.parallelism)\n        return self.__register(op)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sum(self, attribute_selector, state_keeper=None):\n        op = Operator(\n            _generate_uuid(),\n            OpType.Sum,\n            \"Sum\",\n            _sum,\n            other=attribute_selector,\n            state_actor=state_keeper,\n            num_instances=self.env.config.parallelism)\n        return self.__register(op)", "response": "Applies a rolling sum operator to the stream."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\napply a system time window to the stream.", "response": "def time_window(self, window_width_ms):\n        \"\"\"Applies a system time window to the stream.\n\n        Attributes:\n             window_width_ms (int): The length of the window in ms.\n        \"\"\"\n        op = Operator(\n            _generate_uuid(),\n            OpType.TimeWindow,\n            \"TimeWindow\",\n            num_instances=self.env.config.parallelism,\n            other=window_width_ms)\n        return self.__register(op)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\napply a filter to the stream.", "response": "def filter(self, filter_fn):\n        \"\"\"Applies a filter to the stream.\n\n        Attributes:\n             filter_fn (function): The user-defined filter function.\n        \"\"\"\n        op = Operator(\n            _generate_uuid(),\n            OpType.Filter,\n            \"Filter\",\n            filter_fn,\n            num_instances=self.env.config.parallelism)\n        return self.__register(op)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninspect the content of the stream.", "response": "def inspect(self, inspect_logic):\n        \"\"\"Inspects the content of the stream.\n\n        Attributes:\n             inspect_logic (function): The user-defined inspect function.\n        \"\"\"\n        op = Operator(\n            _generate_uuid(),\n            OpType.Inspect,\n            \"Inspect\",\n            inspect_logic,\n            num_instances=self.env.config.parallelism)\n        return self.__register(op)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncloses the stream with a sink operator.", "response": "def sink(self):\n        \"\"\"Closes the stream with a sink operator.\"\"\"\n        op = Operator(\n            _generate_uuid(),\n            OpType.Sink,\n            \"Sink\",\n            num_instances=self.env.config.parallelism)\n        return self.__register(op)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef close_all_files(self):\n        while len(self.open_file_infos) > 0:\n            file_info = self.open_file_infos.pop(0)\n            file_info.file_handle.close()\n            file_info.file_handle = None\n            self.closed_file_infos.append(file_info)\n        self.can_open_more_files = True", "response": "Close all open files."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_log_filenames(self):\n        log_filenames = os.listdir(self.logs_dir)\n\n        for log_filename in log_filenames:\n            full_path = os.path.join(self.logs_dir, log_filename)\n            if full_path not in self.log_filenames:\n                self.log_filenames.add(full_path)\n                self.closed_file_infos.append(\n                    LogFileInfo(\n                        filename=full_path,\n                        size_when_last_opened=0,\n                        file_position=0,\n                        file_handle=None))\n                logger.info(\"Beginning to track file {}\".format(log_filename))", "response": "Update the list of log files to monitor."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nopening some closed files.", "response": "def open_closed_files(self):\n        \"\"\"Open some closed files if they may have new lines.\n\n        Opening more files may require us to close some of the already open\n        files.\n        \"\"\"\n        if not self.can_open_more_files:\n            # If we can't open any more files. Close all of the files.\n            self.close_all_files()\n\n        files_with_no_updates = []\n        while len(self.closed_file_infos) > 0:\n            if (len(self.open_file_infos) >=\n                    ray_constants.LOG_MONITOR_MAX_OPEN_FILES):\n                self.can_open_more_files = False\n                break\n\n            file_info = self.closed_file_infos.pop(0)\n            assert file_info.file_handle is None\n            # Get the file size to see if it has gotten bigger since we last\n            # opened it.\n            try:\n                file_size = os.path.getsize(file_info.filename)\n            except (IOError, OSError) as e:\n                # Catch \"file not found\" errors.\n                if e.errno == errno.ENOENT:\n                    logger.warning(\"Warning: The file {} was not \"\n                                   \"found.\".format(file_info.filename))\n                    self.log_filenames.remove(file_info.filename)\n                    continue\n                raise e\n\n            # If some new lines have been added to this file, try to reopen the\n            # file.\n            if file_size > file_info.size_when_last_opened:\n                try:\n                    f = open(file_info.filename, \"r\")\n                except (IOError, OSError) as e:\n                    if e.errno == errno.ENOENT:\n                        logger.warning(\"Warning: The file {} was not \"\n                                       \"found.\".format(file_info.filename))\n                        self.log_filenames.remove(file_info.filename)\n                        continue\n                    else:\n                        raise e\n\n                f.seek(file_info.file_position)\n                file_info.filesize_when_last_opened = file_size\n                file_info.file_handle = f\n                self.open_file_infos.append(file_info)\n            else:\n                files_with_no_updates.append(file_info)\n\n        # Add the files with no changes back to the list of closed files.\n        self.closed_file_infos += files_with_no_updates"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if any changes to the log files and publish updates to Redis.", "response": "def check_log_files_and_publish_updates(self):\n        \"\"\"Get any changes to the log files and push updates to Redis.\n\n        Returns:\n            True if anything was published and false otherwise.\n        \"\"\"\n        anything_published = False\n        for file_info in self.open_file_infos:\n            assert not file_info.file_handle.closed\n\n            lines_to_publish = []\n            max_num_lines_to_read = 100\n            for _ in range(max_num_lines_to_read):\n                next_line = file_info.file_handle.readline()\n                if next_line == \"\":\n                    break\n                if next_line[-1] == \"\\n\":\n                    next_line = next_line[:-1]\n                lines_to_publish.append(next_line)\n\n            # Publish the lines if this is a worker process.\n            filename = file_info.filename.split(\"/\")[-1]\n            is_worker = (filename.startswith(\"worker\")\n                         and (filename.endswith(\"out\")\n                              or filename.endswith(\"err\")))\n\n            if is_worker and file_info.file_position == 0:\n                if (len(lines_to_publish) > 0 and\n                        lines_to_publish[0].startswith(\"Ray worker pid: \")):\n                    file_info.worker_pid = int(\n                        lines_to_publish[0].split(\" \")[-1])\n                    lines_to_publish = lines_to_publish[1:]\n\n            # Record the current position in the file.\n            file_info.file_position = file_info.file_handle.tell()\n\n            if len(lines_to_publish) > 0 and is_worker:\n                self.redis_client.publish(\n                    ray.gcs_utils.LOG_FILE_CHANNEL,\n                    json.dumps({\n                        \"ip\": self.ip,\n                        \"pid\": file_info.worker_pid,\n                        \"lines\": lines_to_publish\n                    }))\n                anything_published = True\n\n        return anything_published"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns the log monitor.", "response": "def run(self):\n        \"\"\"Run the log monitor.\n\n        This will query Redis once every second to check if there are new log\n        files to monitor. It will also store those log files in Redis.\n        \"\"\"\n        while True:\n            self.update_log_filenames()\n            self.open_closed_files()\n            anything_published = self.check_log_files_and_publish_updates()\n            # If nothing was published, then wait a little bit before checking\n            # for logs to avoid using too much CPU.\n            if not anything_published:\n                time.sleep(0.05)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchains generator given experiment specifications.", "response": "def add_configurations(self, experiments):\n        \"\"\"Chains generator given experiment specifications.\n\n        Arguments:\n            experiments (Experiment | list | dict): Experiments to run.\n        \"\"\"\n        experiment_list = convert_to_experiment_list(experiments)\n        for experiment in experiment_list:\n            self._trial_generator = itertools.chain(\n                self._trial_generator,\n                self._generate_trials(experiment.spec, experiment.name))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprovides a batch of Trial objects to be queued into the TrialRunner.", "response": "def next_trials(self):\n        \"\"\"Provides a batch of Trial objects to be queued into the TrialRunner.\n\n        A batch ends when self._trial_generator returns None.\n\n        Returns:\n            trials (list): Returns a list of trials.\n        \"\"\"\n        trials = []\n\n        for trial in self._trial_generator:\n            if trial is None:\n                return trials\n            trials += [trial]\n\n        self._finished = True\n        return trials"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating trials with configurations from _suggest.", "response": "def _generate_trials(self, experiment_spec, output_path=\"\"):\n        \"\"\"Generates trials with configurations from `_suggest`.\n\n        Creates a trial_id that is passed into `_suggest`.\n\n        Yields:\n            Trial objects constructed according to `spec`\n        \"\"\"\n        if \"run\" not in experiment_spec:\n            raise TuneError(\"Must specify `run` in {}\".format(experiment_spec))\n        for _ in range(experiment_spec.get(\"num_samples\", 1)):\n            trial_id = Trial.generate_id()\n            while True:\n                suggested_config = self._suggest(trial_id)\n                if suggested_config is None:\n                    yield None\n                else:\n                    break\n            spec = copy.deepcopy(experiment_spec)\n            spec[\"config\"] = merge_dicts(spec[\"config\"], suggested_config)\n            flattened_config = resolve_nested_dict(spec[\"config\"])\n            self._counter += 1\n            tag = \"{0}_{1}\".format(\n                str(self._counter), format_vars(flattened_config))\n            yield create_trial_from_spec(\n                spec,\n                output_path,\n                self._parser,\n                experiment_tag=tag,\n                trial_id=trial_id)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a list of variants from a spec with unresolved values.", "response": "def generate_variants(unresolved_spec):\n    \"\"\"Generates variants from a spec (dict) with unresolved values.\n\n    There are two types of unresolved values:\n\n        Grid search: These define a grid search over values. For example, the\n        following grid search values in a spec will produce six distinct\n        variants in combination:\n\n            \"activation\": grid_search([\"relu\", \"tanh\"])\n            \"learning_rate\": grid_search([1e-3, 1e-4, 1e-5])\n\n        Lambda functions: These are evaluated to produce a concrete value, and\n        can express dependencies or conditional distributions between values.\n        They can also be used to express random search (e.g., by calling\n        into the `random` or `np` module).\n\n            \"cpu\": lambda spec: spec.config.num_workers\n            \"batch_size\": lambda spec: random.uniform(1, 1000)\n\n    Finally, to support defining specs in plain JSON / YAML, grid search\n    and lambda functions can also be defined alternatively as follows:\n\n        \"activation\": {\"grid_search\": [\"relu\", \"tanh\"]}\n        \"cpu\": {\"eval\": \"spec.config.num_workers\"}\n    \"\"\"\n    for resolved_vars, spec in _generate_variants(unresolved_spec):\n        assert not _unresolved_values(spec)\n        yield format_vars(resolved_vars), spec"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef resolve_nested_dict(nested_dict):\n    res = {}\n    for k, v in nested_dict.items():\n        if isinstance(v, dict):\n            for k_, v_ in resolve_nested_dict(v).items():\n                res[(k, ) + k_] = v_\n        else:\n            res[(k, )] = v\n    return res", "response": "Flattens a nested dict by joining keys into tuple of paths."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run_board(args):\n    init_config(args)\n\n    # backend service, should import after django settings initialized\n    from backend.collector import CollectorService\n\n    service = CollectorService(\n        args.logdir,\n        args.reload_interval,\n        standalone=False,\n        log_level=args.log_level)\n    service.run()\n\n    # frontend service\n    logger.info(\"Try to start automlboard on port %s\\n\" % args.port)\n    command = [\n        os.path.join(root_path, \"manage.py\"), \"runserver\",\n        \"0.0.0.0:%s\" % args.port, \"--noreload\"\n    ]\n    execute_from_command_line(command)", "response": "Run main entry for AutoMLBoard."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef init_config(args):\n    os.environ[\"AUTOMLBOARD_LOGDIR\"] = args.logdir\n    os.environ[\"AUTOMLBOARD_LOGLEVEL\"] = args.log_level\n    os.environ[\"AUTOMLBOARD_RELOAD_INTERVAL\"] = str(args.reload_interval)\n\n    if args.db:\n        try:\n            db_address_reg = re.compile(r\"(.*)://(.*):(.*)@(.*):(.*)/(.*)\")\n            match = re.match(db_address_reg, args.db_address)\n            os.environ[\"AUTOMLBOARD_DB_ENGINE\"] = match.group(1)\n            os.environ[\"AUTOMLBOARD_DB_USER\"] = match.group(2)\n            os.environ[\"AUTOMLBOARD_DB_PASSWORD\"] = match.group(3)\n            os.environ[\"AUTOMLBOARD_DB_HOST\"] = match.group(4)\n            os.environ[\"AUTOMLBOARD_DB_PORT\"] = match.group(5)\n            os.environ[\"AUTOMLBOARD_DB_NAME\"] = match.group(6)\n            logger.info(\"Using %s as the database backend.\" % match.group(1))\n        except BaseException as e:\n            raise DatabaseError(e)\n    else:\n        logger.info(\"Using sqlite3 as the database backend, \"\n                    \"information will be stored in automlboard.db\")\n\n    os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\",\n                          \"ray.tune.automlboard.settings\")\n    django.setup()\n    command = [os.path.join(root_path, \"manage.py\"), \"migrate\", \"--run-syncdb\"]\n    execute_from_command_line(command)", "response": "Initialize the configuration of the service."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the GPU IDs that are available to the worker.", "response": "def get_gpu_ids():\n    \"\"\"Get the IDs of the GPUs that are available to the worker.\n\n    If the CUDA_VISIBLE_DEVICES environment variable was set when the worker\n    started up, then the IDs returned by this method will be a subset of the\n    IDs in CUDA_VISIBLE_DEVICES. If not, the IDs will fall in the range\n    [0, NUM_GPUS - 1], where NUM_GPUS is the number of GPUs that the node has.\n\n    Returns:\n        A list of GPU IDs.\n    \"\"\"\n    if _mode() == LOCAL_MODE:\n        raise Exception(\"ray.get_gpu_ids() currently does not work in PYTHON \"\n                        \"MODE.\")\n\n    all_resource_ids = global_worker.raylet_client.resource_ids()\n    assigned_ids = [\n        resource_id for resource_id, _ in all_resource_ids.get(\"GPU\", [])\n    ]\n    # If the user had already set CUDA_VISIBLE_DEVICES, then respect that (in\n    # the sense that only GPU IDs that appear in CUDA_VISIBLE_DEVICES should be\n    # returned).\n    if global_worker.original_gpu_ids is not None:\n        assigned_ids = [\n            global_worker.original_gpu_ids[gpu_id] for gpu_id in assigned_ids\n        ]\n\n    return assigned_ids"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn information about failed tasks.", "response": "def error_info():\n    \"\"\"Return information about failed tasks.\"\"\"\n    worker = global_worker\n    worker.check_connected()\n    return (global_state.error_messages(driver_id=worker.task_driver_id) +\n            global_state.error_messages(driver_id=DriverID.nil()))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninitializes the internal serialization library.", "response": "def _initialize_serialization(driver_id, worker=global_worker):\n    \"\"\"Initialize the serialization library.\n\n    This defines a custom serializer for object IDs and also tells ray to\n    serialize several exception classes that we define for error handling.\n    \"\"\"\n    serialization_context = pyarrow.default_serialization_context()\n    # Tell the serialization context to use the cloudpickle version that we\n    # ship with Ray.\n    serialization_context.set_pickle(pickle.dumps, pickle.loads)\n    pyarrow.register_torch_serialization_handlers(serialization_context)\n\n    for id_type in ray._raylet._ID_TYPES:\n        serialization_context.register_type(\n            id_type,\n            \"{}.{}\".format(id_type.__module__, id_type.__name__),\n            pickle=True)\n\n    def actor_handle_serializer(obj):\n        return obj._serialization_helper(True)\n\n    def actor_handle_deserializer(serialized_obj):\n        new_handle = ray.actor.ActorHandle.__new__(ray.actor.ActorHandle)\n        new_handle._deserialization_helper(serialized_obj, True)\n        return new_handle\n\n    # We register this serializer on each worker instead of calling\n    # register_custom_serializer from the driver so that isinstance still\n    # works.\n    serialization_context.register_type(\n        ray.actor.ActorHandle,\n        \"ray.ActorHandle\",\n        pickle=False,\n        custom_serializer=actor_handle_serializer,\n        custom_deserializer=actor_handle_deserializer)\n\n    worker.serialization_context_map[driver_id] = serialization_context\n\n    # Register exception types.\n    for error_cls in RAY_EXCEPTION_TYPES:\n        register_custom_serializer(\n            error_cls,\n            use_dict=True,\n            local=True,\n            driver_id=driver_id,\n            class_id=error_cls.__module__ + \". \" + error_cls.__name__,\n        )\n    # Tell Ray to serialize lambdas with pickle.\n    register_custom_serializer(\n        type(lambda: 0),\n        use_pickle=True,\n        local=True,\n        driver_id=driver_id,\n        class_id=\"lambda\")\n    # Tell Ray to serialize types with pickle.\n    register_custom_serializer(\n        type(int),\n        use_pickle=True,\n        local=True,\n        driver_id=driver_id,\n        class_id=\"type\")\n    # Tell Ray to serialize FunctionSignatures as dictionaries. This is\n    # used when passing around actor handles.\n    register_custom_serializer(\n        ray.signature.FunctionSignature,\n        use_dict=True,\n        local=True,\n        driver_id=driver_id,\n        class_id=\"ray.signature.FunctionSignature\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninitializing a new Ray cluster.", "response": "def init(redis_address=None,\n         num_cpus=None,\n         num_gpus=None,\n         resources=None,\n         object_store_memory=None,\n         redis_max_memory=None,\n         log_to_driver=True,\n         node_ip_address=None,\n         object_id_seed=None,\n         local_mode=False,\n         redirect_worker_output=None,\n         redirect_output=None,\n         ignore_reinit_error=False,\n         num_redis_shards=None,\n         redis_max_clients=None,\n         redis_password=None,\n         plasma_directory=None,\n         huge_pages=False,\n         include_webui=False,\n         driver_id=None,\n         configure_logging=True,\n         logging_level=logging.INFO,\n         logging_format=ray_constants.LOGGER_FORMAT,\n         plasma_store_socket_name=None,\n         raylet_socket_name=None,\n         temp_dir=None,\n         load_code_from_local=False,\n         _internal_config=None):\n    \"\"\"Connect to an existing Ray cluster or start one and connect to it.\n\n    This method handles two cases. Either a Ray cluster already exists and we\n    just attach this driver to it, or we start all of the processes associated\n    with a Ray cluster and attach to the newly started cluster.\n\n    To start Ray and all of the relevant processes, use this as follows:\n\n    .. code-block:: python\n\n        ray.init()\n\n    To connect to an existing Ray cluster, use this as follows (substituting\n    in the appropriate address):\n\n    .. code-block:: python\n\n        ray.init(redis_address=\"123.45.67.89:6379\")\n\n    Args:\n        redis_address (str): The address of the Redis server to connect to. If\n            this address is not provided, then this command will start Redis, a\n            raylet, a plasma store, a plasma manager, and some workers.\n            It will also kill these processes when Python exits.\n        num_cpus (int): Number of cpus the user wishes all raylets to\n            be configured with.\n        num_gpus (int): Number of gpus the user wishes all raylets to\n            be configured with.\n        resources: A dictionary mapping the name of a resource to the quantity\n            of that resource available.\n        object_store_memory: The amount of memory (in bytes) to start the\n            object store with. By default, this is capped at 20GB but can be\n            set higher.\n        redis_max_memory: The max amount of memory (in bytes) to allow each\n            redis shard to use. Once the limit is exceeded, redis will start\n            LRU eviction of entries. This only applies to the sharded redis\n            tables (task, object, and profile tables). By default, this is\n            capped at 10GB but can be set higher.\n        log_to_driver (bool): If true, then output from all of the worker\n            processes on all nodes will be directed to the driver.\n        node_ip_address (str): The IP address of the node that we are on.\n        object_id_seed (int): Used to seed the deterministic generation of\n            object IDs. The same value can be used across multiple runs of the\n            same driver in order to generate the object IDs in a consistent\n            manner. However, the same ID should not be used for different\n            drivers.\n        local_mode (bool): True if the code should be executed serially\n            without Ray. This is useful for debugging.\n        ignore_reinit_error: True if we should suppress errors from calling\n            ray.init() a second time.\n        num_redis_shards: The number of Redis shards to start in addition to\n            the primary Redis shard.\n        redis_max_clients: If provided, attempt to configure Redis with this\n            maxclients number.\n        redis_password (str): Prevents external clients without the password\n            from connecting to Redis if provided.\n        plasma_directory: A directory where the Plasma memory mapped files will\n            be created.\n        huge_pages: Boolean flag indicating whether to start the Object\n            Store with hugetlbfs support. Requires plasma_directory.\n        include_webui: Boolean flag indicating whether to start the web\n            UI, which displays the status of the Ray cluster.\n        driver_id: The ID of driver.\n        configure_logging: True if allow the logging cofiguration here.\n            Otherwise, the users may want to configure it by their own.\n        logging_level: Logging level, default will be logging.INFO.\n        logging_format: Logging format, default contains a timestamp,\n            filename, line number, and message. See ray_constants.py.\n        plasma_store_socket_name (str): If provided, it will specify the socket\n            name used by the plasma store.\n        raylet_socket_name (str): If provided, it will specify the socket path\n            used by the raylet process.\n        temp_dir (str): If provided, it will specify the root temporary\n            directory for the Ray process.\n        load_code_from_local: Whether code should be loaded from a local module\n            or from the GCS.\n        _internal_config (str): JSON configuration for overriding\n            RayConfig defaults. For testing purposes ONLY.\n\n    Returns:\n        Address information about the started processes.\n\n    Raises:\n        Exception: An exception is raised if an inappropriate combination of\n            arguments is passed in.\n    \"\"\"\n\n    if configure_logging:\n        setup_logger(logging_level, logging_format)\n\n    if local_mode:\n        driver_mode = LOCAL_MODE\n    else:\n        driver_mode = SCRIPT_MODE\n\n    if setproctitle is None:\n        logger.warning(\n            \"WARNING: Not updating worker name since `setproctitle` is not \"\n            \"installed. Install this with `pip install setproctitle` \"\n            \"(or ray[debug]) to enable monitoring of worker processes.\")\n\n    if global_worker.connected:\n        if ignore_reinit_error:\n            logger.error(\"Calling ray.init() again after it has already been \"\n                         \"called.\")\n            return\n        else:\n            raise Exception(\"Perhaps you called ray.init twice by accident? \"\n                            \"This error can be suppressed by passing in \"\n                            \"'ignore_reinit_error=True' or by calling \"\n                            \"'ray.shutdown()' prior to 'ray.init()'.\")\n\n    # Convert hostnames to numerical IP address.\n    if node_ip_address is not None:\n        node_ip_address = services.address_to_ip(node_ip_address)\n    if redis_address is not None:\n        redis_address = services.address_to_ip(redis_address)\n\n    global _global_node\n    if driver_mode == LOCAL_MODE:\n        # If starting Ray in LOCAL_MODE, don't start any other processes.\n        _global_node = ray.node.LocalNode()\n    elif redis_address is None:\n        # In this case, we need to start a new cluster.\n        ray_params = ray.parameter.RayParams(\n            redis_address=redis_address,\n            node_ip_address=node_ip_address,\n            object_id_seed=object_id_seed,\n            local_mode=local_mode,\n            driver_mode=driver_mode,\n            redirect_worker_output=redirect_worker_output,\n            redirect_output=redirect_output,\n            num_cpus=num_cpus,\n            num_gpus=num_gpus,\n            resources=resources,\n            num_redis_shards=num_redis_shards,\n            redis_max_clients=redis_max_clients,\n            redis_password=redis_password,\n            plasma_directory=plasma_directory,\n            huge_pages=huge_pages,\n            include_webui=include_webui,\n            object_store_memory=object_store_memory,\n            redis_max_memory=redis_max_memory,\n            plasma_store_socket_name=plasma_store_socket_name,\n            raylet_socket_name=raylet_socket_name,\n            temp_dir=temp_dir,\n            load_code_from_local=load_code_from_local,\n            _internal_config=_internal_config,\n        )\n        # Start the Ray processes. We set shutdown_at_exit=False because we\n        # shutdown the node in the ray.shutdown call that happens in the atexit\n        # handler.\n        _global_node = ray.node.Node(\n            head=True, shutdown_at_exit=False, ray_params=ray_params)\n    else:\n        # In this case, we are connecting to an existing cluster.\n        if num_cpus is not None or num_gpus is not None:\n            raise Exception(\"When connecting to an existing cluster, num_cpus \"\n                            \"and num_gpus must not be provided.\")\n        if resources is not None:\n            raise Exception(\"When connecting to an existing cluster, \"\n                            \"resources must not be provided.\")\n        if num_redis_shards is not None:\n            raise Exception(\"When connecting to an existing cluster, \"\n                            \"num_redis_shards must not be provided.\")\n        if redis_max_clients is not None:\n            raise Exception(\"When connecting to an existing cluster, \"\n                            \"redis_max_clients must not be provided.\")\n        if object_store_memory is not None:\n            raise Exception(\"When connecting to an existing cluster, \"\n                            \"object_store_memory must not be provided.\")\n        if redis_max_memory is not None:\n            raise Exception(\"When connecting to an existing cluster, \"\n                            \"redis_max_memory must not be provided.\")\n        if plasma_directory is not None:\n            raise Exception(\"When connecting to an existing cluster, \"\n                            \"plasma_directory must not be provided.\")\n        if huge_pages:\n            raise Exception(\"When connecting to an existing cluster, \"\n                            \"huge_pages must not be provided.\")\n        if temp_dir is not None:\n            raise Exception(\"When connecting to an existing cluster, \"\n                            \"temp_dir must not be provided.\")\n        if plasma_store_socket_name is not None:\n            raise Exception(\"When connecting to an existing cluster, \"\n                            \"plasma_store_socket_name must not be provided.\")\n        if raylet_socket_name is not None:\n            raise Exception(\"When connecting to an existing cluster, \"\n                            \"raylet_socket_name must not be provided.\")\n        if _internal_config is not None:\n            raise Exception(\"When connecting to an existing cluster, \"\n                            \"_internal_config must not be provided.\")\n\n        # In this case, we only need to connect the node.\n        ray_params = ray.parameter.RayParams(\n            node_ip_address=node_ip_address,\n            redis_address=redis_address,\n            redis_password=redis_password,\n            object_id_seed=object_id_seed,\n            temp_dir=temp_dir,\n            load_code_from_local=load_code_from_local)\n        _global_node = ray.node.Node(\n            ray_params, head=False, shutdown_at_exit=False, connect_only=True)\n\n    connect(\n        _global_node,\n        mode=driver_mode,\n        log_to_driver=log_to_driver,\n        worker=global_worker,\n        driver_id=driver_id)\n\n    for hook in _post_init_hooks:\n        hook()\n\n    return _global_node.address_info"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nshut down the Ray .", "response": "def shutdown(exiting_interpreter=False):\n    \"\"\"Disconnect the worker, and terminate processes started by ray.init().\n\n    This will automatically run at the end when a Python process that uses Ray\n    exits. It is ok to run this twice in a row. The primary use case for this\n    function is to cleanup state between tests.\n\n    Note that this will clear any remote function definitions, actor\n    definitions, and existing actors, so if you wish to use any previously\n    defined remote functions or actors after calling ray.shutdown(), then you\n    need to redefine them. If they were defined in an imported module, then you\n    will need to reload the module.\n\n    Args:\n        exiting_interpreter (bool): True if this is called by the atexit hook\n            and false otherwise. If we are exiting the interpreter, we will\n            wait a little while to print any extra error messages.\n    \"\"\"\n    if exiting_interpreter and global_worker.mode == SCRIPT_MODE:\n        # This is a duration to sleep before shutting down everything in order\n        # to make sure that log messages finish printing.\n        time.sleep(0.5)\n\n    disconnect()\n\n    # Disconnect global state from GCS.\n    global_state.disconnect()\n\n    # Shut down the Ray processes.\n    global _global_node\n    if _global_node is not None:\n        _global_node.kill_all_processes(check_alive=False, allow_graceful=True)\n        _global_node = None\n\n    global_worker.set_mode(None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef print_logs(redis_client, threads_stopped):\n    pubsub_client = redis_client.pubsub(ignore_subscribe_messages=True)\n    pubsub_client.subscribe(ray.gcs_utils.LOG_FILE_CHANNEL)\n    localhost = services.get_node_ip_address()\n    try:\n        # Keep track of the number of consecutive log messages that have been\n        # received with no break in between. If this number grows continually,\n        # then the worker is probably not able to process the log messages as\n        # rapidly as they are coming in.\n        num_consecutive_messages_received = 0\n        while True:\n            # Exit if we received a signal that we should stop.\n            if threads_stopped.is_set():\n                return\n\n            msg = pubsub_client.get_message()\n            if msg is None:\n                num_consecutive_messages_received = 0\n                threads_stopped.wait(timeout=0.01)\n                continue\n            num_consecutive_messages_received += 1\n\n            data = json.loads(ray.utils.decode(msg[\"data\"]))\n            if data[\"ip\"] == localhost:\n                for line in data[\"lines\"]:\n                    print(\"{}{}(pid={}){} {}\".format(\n                        colorama.Style.DIM, colorama.Fore.CYAN, data[\"pid\"],\n                        colorama.Style.RESET_ALL, line))\n            else:\n                for line in data[\"lines\"]:\n                    print(\"{}{}(pid={}, ip={}){} {}\".format(\n                        colorama.Style.DIM, colorama.Fore.CYAN, data[\"pid\"],\n                        data[\"ip\"], colorama.Style.RESET_ALL, line))\n\n            if (num_consecutive_messages_received % 100 == 0\n                    and num_consecutive_messages_received > 0):\n                logger.warning(\n                    \"The driver may not be able to keep up with the \"\n                    \"stdout/stderr of the workers. To avoid forwarding logs \"\n                    \"to the driver, use 'ray.init(log_to_driver=False)'.\")\n    finally:\n        # Close the pubsub client to avoid leaking file descriptors.\n        pubsub_client.close()", "response": "Prints log messages from all of the nodes in the primary Redis shard."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef print_error_messages_raylet(task_error_queue, threads_stopped):\n\n    while True:\n        # Exit if we received a signal that we should stop.\n        if threads_stopped.is_set():\n            return\n\n        try:\n            error, t = task_error_queue.get(block=False)\n        except queue.Empty:\n            threads_stopped.wait(timeout=0.01)\n            continue\n        # Delay errors a little bit of time to attempt to suppress redundant\n        # messages originating from the worker.\n        while t + UNCAUGHT_ERROR_GRACE_PERIOD > time.time():\n            threads_stopped.wait(timeout=1)\n            if threads_stopped.is_set():\n                break\n        if t < last_task_error_raise_time + UNCAUGHT_ERROR_GRACE_PERIOD:\n            logger.debug(\"Suppressing error from worker: {}\".format(error))\n        else:\n            logger.error(\n                \"Possible unhandled error from worker: {}\".format(error))", "response": "Prints the messages received from the given task_error_queue to the given output queue."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef listen_error_messages_raylet(worker, task_error_queue, threads_stopped):\n    worker.error_message_pubsub_client = worker.redis_client.pubsub(\n        ignore_subscribe_messages=True)\n    # Exports that are published after the call to\n    # error_message_pubsub_client.subscribe and before the call to\n    # error_message_pubsub_client.listen will still be processed in the loop.\n\n    # Really we should just subscribe to the errors for this specific job.\n    # However, currently all errors seem to be published on the same channel.\n    error_pubsub_channel = str(\n        ray.gcs_utils.TablePubsub.ERROR_INFO).encode(\"ascii\")\n    worker.error_message_pubsub_client.subscribe(error_pubsub_channel)\n    # worker.error_message_pubsub_client.psubscribe(\"*\")\n\n    try:\n        # Get the exports that occurred before the call to subscribe.\n        error_messages = global_state.error_messages(worker.task_driver_id)\n        for error_message in error_messages:\n            logger.error(error_message)\n\n        while True:\n            # Exit if we received a signal that we should stop.\n            if threads_stopped.is_set():\n                return\n\n            msg = worker.error_message_pubsub_client.get_message()\n            if msg is None:\n                threads_stopped.wait(timeout=0.01)\n                continue\n            gcs_entry = ray.gcs_utils.GcsTableEntry.GetRootAsGcsTableEntry(\n                msg[\"data\"], 0)\n            assert gcs_entry.EntriesLength() == 1\n            error_data = ray.gcs_utils.ErrorTableData.GetRootAsErrorTableData(\n                gcs_entry.Entries(0), 0)\n            driver_id = error_data.DriverId()\n            if driver_id not in [\n                    worker.task_driver_id.binary(),\n                    DriverID.nil().binary()\n            ]:\n                continue\n\n            error_message = ray.utils.decode(error_data.ErrorMessage())\n            if (ray.utils.decode(\n                    error_data.Type()) == ray_constants.TASK_PUSH_ERROR):\n                # Delay it a bit to see if we can suppress it\n                task_error_queue.put((error_message, time.time()))\n            else:\n                logger.error(error_message)\n    finally:\n        # Close the pubsub client to avoid leaking file descriptors.\n        worker.error_message_pubsub_client.close()", "response": "Listen to error messages in the background on the given task_error_queue."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconnect this worker to the Redis and Redis.", "response": "def connect(node,\n            mode=WORKER_MODE,\n            log_to_driver=False,\n            worker=global_worker,\n            driver_id=None,\n            load_code_from_local=False):\n    \"\"\"Connect this worker to the raylet, to Plasma, and to Redis.\n\n    Args:\n        node (ray.node.Node): The node to connect.\n        mode: The mode of the worker. One of SCRIPT_MODE, WORKER_MODE, and\n            LOCAL_MODE.\n        log_to_driver (bool): If true, then output from all of the worker\n            processes on all nodes will be directed to the driver.\n        worker: The ray.Worker instance.\n        driver_id: The ID of driver. If it's None, then we will generate one.\n    \"\"\"\n    # Do some basic checking to make sure we didn't call ray.init twice.\n    error_message = \"Perhaps you called ray.init twice by accident?\"\n    assert not worker.connected, error_message\n    assert worker.cached_functions_to_run is not None, error_message\n\n    # Enable nice stack traces on SIGSEGV etc.\n    if not faulthandler.is_enabled():\n        faulthandler.enable(all_threads=False)\n\n    worker.profiler = profiling.Profiler(worker, worker.threads_stopped)\n\n    # Initialize some fields.\n    if mode is WORKER_MODE:\n        worker.worker_id = _random_string()\n        if setproctitle:\n            setproctitle.setproctitle(\"ray_worker\")\n    else:\n        # This is the code path of driver mode.\n        if driver_id is None:\n            driver_id = DriverID(_random_string())\n\n        if not isinstance(driver_id, DriverID):\n            raise TypeError(\"The type of given driver id must be DriverID.\")\n\n        worker.worker_id = driver_id.binary()\n\n    # When tasks are executed on remote workers in the context of multiple\n    # drivers, the task driver ID is used to keep track of which driver is\n    # responsible for the task so that error messages will be propagated to\n    # the correct driver.\n    if mode != WORKER_MODE:\n        worker.task_driver_id = DriverID(worker.worker_id)\n\n    # All workers start out as non-actors. A worker can be turned into an actor\n    # after it is created.\n    worker.actor_id = ActorID.nil()\n    worker.node = node\n    worker.set_mode(mode)\n\n    # If running Ray in LOCAL_MODE, there is no need to create call\n    # create_worker or to start the worker service.\n    if mode == LOCAL_MODE:\n        return\n\n    # Create a Redis client.\n    # The Redis client can safely be shared between threads. However, that is\n    # not true of Redis pubsub clients. See the documentation at\n    # https://github.com/andymccurdy/redis-py#thread-safety.\n    worker.redis_client = node.create_redis_client()\n\n    # For driver's check that the version information matches the version\n    # information that the Ray cluster was started with.\n    try:\n        ray.services.check_version_info(worker.redis_client)\n    except Exception as e:\n        if mode == SCRIPT_MODE:\n            raise e\n        elif mode == WORKER_MODE:\n            traceback_str = traceback.format_exc()\n            ray.utils.push_error_to_driver_through_redis(\n                worker.redis_client,\n                ray_constants.VERSION_MISMATCH_PUSH_ERROR,\n                traceback_str,\n                driver_id=None)\n\n    worker.lock = threading.RLock()\n\n    # Create an object for interfacing with the global state.\n    global_state._initialize_global_state(\n        node.redis_address, redis_password=node.redis_password)\n\n    # Register the worker with Redis.\n    if mode == SCRIPT_MODE:\n        # The concept of a driver is the same as the concept of a \"job\".\n        # Register the driver/job with Redis here.\n        import __main__ as main\n        driver_info = {\n            \"node_ip_address\": node.node_ip_address,\n            \"driver_id\": worker.worker_id,\n            \"start_time\": time.time(),\n            \"plasma_store_socket\": node.plasma_store_socket_name,\n            \"raylet_socket\": node.raylet_socket_name,\n            \"name\": (main.__file__\n                     if hasattr(main, \"__file__\") else \"INTERACTIVE MODE\")\n        }\n        worker.redis_client.hmset(b\"Drivers:\" + worker.worker_id, driver_info)\n    elif mode == WORKER_MODE:\n        # Register the worker with Redis.\n        worker_dict = {\n            \"node_ip_address\": node.node_ip_address,\n            \"plasma_store_socket\": node.plasma_store_socket_name,\n        }\n        # Check the RedirectOutput key in Redis and based on its value redirect\n        # worker output and error to their own files.\n        # This key is set in services.py when Redis is started.\n        redirect_worker_output_val = worker.redis_client.get(\"RedirectOutput\")\n        if (redirect_worker_output_val is not None\n                and int(redirect_worker_output_val) == 1):\n            log_stdout_file, log_stderr_file = (\n                node.new_worker_redirected_log_file(worker.worker_id))\n            # Redirect stdout/stderr at the file descriptor level. If we simply\n            # set sys.stdout and sys.stderr, then logging from C++ can fail to\n            # be redirected.\n            os.dup2(log_stdout_file.fileno(), sys.stdout.fileno())\n            os.dup2(log_stderr_file.fileno(), sys.stderr.fileno())\n            # We also manually set sys.stdout and sys.stderr because that seems\n            # to have an affect on the output buffering. Without doing this,\n            # stdout and stderr are heavily buffered resulting in seemingly\n            # lost logging statements.\n            sys.stdout = log_stdout_file\n            sys.stderr = log_stderr_file\n            # This should always be the first message to appear in the worker's\n            # stdout and stderr log files. The string \"Ray worker pid:\" is\n            # parsed in the log monitor process.\n            print(\"Ray worker pid: {}\".format(os.getpid()))\n            print(\"Ray worker pid: {}\".format(os.getpid()), file=sys.stderr)\n            sys.stdout.flush()\n            sys.stderr.flush()\n\n            worker_dict[\"stdout_file\"] = os.path.abspath(log_stdout_file.name)\n            worker_dict[\"stderr_file\"] = os.path.abspath(log_stderr_file.name)\n        worker.redis_client.hmset(b\"Workers:\" + worker.worker_id, worker_dict)\n    else:\n        raise Exception(\"This code should be unreachable.\")\n\n    # Create an object store client.\n    worker.plasma_client = thread_safe_client(\n        plasma.connect(node.plasma_store_socket_name, None, 0, 300))\n\n    # If this is a driver, set the current task ID, the task driver ID, and set\n    # the task index to 0.\n    if mode == SCRIPT_MODE:\n        # If the user provided an object_id_seed, then set the current task ID\n        # deterministically based on that seed (without altering the state of\n        # the user's random number generator). Otherwise, set the current task\n        # ID randomly to avoid object ID collisions.\n        numpy_state = np.random.get_state()\n        if node.object_id_seed is not None:\n            np.random.seed(node.object_id_seed)\n        else:\n            # Try to use true randomness.\n            np.random.seed(None)\n        # Reset the state of the numpy random number generator.\n        np.random.set_state(numpy_state)\n\n        # Create an entry for the driver task in the task table. This task is\n        # added immediately with status RUNNING. This allows us to push errors\n        # related to this driver task back to the driver.  For example, if the\n        # driver creates an object that is later evicted, we should notify the\n        # user that we're unable to reconstruct the object, since we cannot\n        # rerun the driver.\n        nil_actor_counter = 0\n\n        function_descriptor = FunctionDescriptor.for_driver_task()\n        driver_task = ray._raylet.Task(\n            worker.task_driver_id,\n            function_descriptor.get_function_descriptor_list(),\n            [],  # arguments.\n            0,  # num_returns.\n            TaskID(_random_string()),  # parent_task_id.\n            0,  # parent_counter.\n            ActorID.nil(),  # actor_creation_id.\n            ObjectID.nil(),  # actor_creation_dummy_object_id.\n            0,  # max_actor_reconstructions.\n            ActorID.nil(),  # actor_id.\n            ActorHandleID.nil(),  # actor_handle_id.\n            nil_actor_counter,  # actor_counter.\n            [],  # new_actor_handles.\n            [],  # execution_dependencies.\n            {},  # resource_map.\n            {},  # placement_resource_map.\n        )\n\n        # Add the driver task to the task table.\n        global_state._execute_command(driver_task.task_id(), \"RAY.TABLE_ADD\",\n                                      ray.gcs_utils.TablePrefix.RAYLET_TASK,\n                                      ray.gcs_utils.TablePubsub.RAYLET_TASK,\n                                      driver_task.task_id().binary(),\n                                      driver_task._serialized_raylet_task())\n\n        # Set the driver's current task ID to the task ID assigned to the\n        # driver task.\n        worker.task_context.current_task_id = driver_task.task_id()\n\n    worker.raylet_client = ray._raylet.RayletClient(\n        node.raylet_socket_name,\n        ClientID(worker.worker_id),\n        (mode == WORKER_MODE),\n        DriverID(worker.current_task_id.binary()),\n    )\n\n    # Start the import thread\n    worker.import_thread = import_thread.ImportThread(worker, mode,\n                                                      worker.threads_stopped)\n    worker.import_thread.start()\n\n    # If this is a driver running in SCRIPT_MODE, start a thread to print error\n    # messages asynchronously in the background. Ideally the scheduler would\n    # push messages to the driver's worker service, but we ran into bugs when\n    # trying to properly shutdown the driver's worker service, so we are\n    # temporarily using this implementation which constantly queries the\n    # scheduler for new error messages.\n    if mode == SCRIPT_MODE:\n        q = queue.Queue()\n        worker.listener_thread = threading.Thread(\n            target=listen_error_messages_raylet,\n            name=\"ray_listen_error_messages\",\n            args=(worker, q, worker.threads_stopped))\n        worker.printer_thread = threading.Thread(\n            target=print_error_messages_raylet,\n            name=\"ray_print_error_messages\",\n            args=(q, worker.threads_stopped))\n        worker.listener_thread.daemon = True\n        worker.listener_thread.start()\n        worker.printer_thread.daemon = True\n        worker.printer_thread.start()\n        if log_to_driver:\n            worker.logger_thread = threading.Thread(\n                target=print_logs,\n                name=\"ray_print_logs\",\n                args=(worker.redis_client, worker.threads_stopped))\n            worker.logger_thread.daemon = True\n            worker.logger_thread.start()\n\n    # If we are using the raylet code path and we are not in local mode, start\n    # a background thread to periodically flush profiling data to the GCS.\n    if mode != LOCAL_MODE:\n        worker.profiler.start_flush_thread()\n\n    if mode == SCRIPT_MODE:\n        # Add the directory containing the script that is running to the Python\n        # paths of the workers. Also add the current directory. Note that this\n        # assumes that the directory structures on the machines in the clusters\n        # are the same.\n        script_directory = os.path.abspath(os.path.dirname(sys.argv[0]))\n        current_directory = os.path.abspath(os.path.curdir)\n        worker.run_function_on_all_workers(\n            lambda worker_info: sys.path.insert(1, script_directory))\n        worker.run_function_on_all_workers(\n            lambda worker_info: sys.path.insert(1, current_directory))\n        # TODO(rkn): Here we first export functions to run, then remote\n        # functions. The order matters. For example, one of the functions to\n        # run may set the Python path, which is needed to import a module used\n        # to define a remote function. We may want to change the order to\n        # simply be the order in which the exports were defined on the driver.\n        # In addition, we will need to retain the ability to decide what the\n        # first few exports are (mostly to set the Python path). Additionally,\n        # note that the first exports to be defined on the driver will be the\n        # ones defined in separate modules that are imported by the driver.\n        # Export cached functions_to_run.\n        for function in worker.cached_functions_to_run:\n            worker.run_function_on_all_workers(function)\n        # Export cached remote functions and actors to the workers.\n        worker.function_actor_manager.export_cached()\n    worker.cached_functions_to_run = None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndisconnecting this worker from the raylet and object store.", "response": "def disconnect():\n    \"\"\"Disconnect this worker from the raylet and object store.\"\"\"\n    # Reset the list of cached remote functions and actors so that if more\n    # remote functions or actors are defined and then connect is called again,\n    # the remote functions will be exported. This is mostly relevant for the\n    # tests.\n    worker = global_worker\n    if worker.connected:\n        # Shutdown all of the threads that we've started. TODO(rkn): This\n        # should be handled cleanly in the worker object's destructor and not\n        # in this disconnect method.\n        worker.threads_stopped.set()\n        if hasattr(worker, \"import_thread\"):\n            worker.import_thread.join_import_thread()\n        if hasattr(worker, \"profiler\") and hasattr(worker.profiler, \"t\"):\n            worker.profiler.join_flush_thread()\n        if hasattr(worker, \"listener_thread\"):\n            worker.listener_thread.join()\n        if hasattr(worker, \"printer_thread\"):\n            worker.printer_thread.join()\n        if hasattr(worker, \"logger_thread\"):\n            worker.logger_thread.join()\n        worker.threads_stopped.clear()\n        worker._session_index += 1\n\n    worker.node = None  # Disconnect the worker from the node.\n    worker.cached_functions_to_run = []\n    worker.function_actor_manager.reset_cache()\n    worker.serialization_context_map.clear()\n\n    if hasattr(worker, \"raylet_client\"):\n        del worker.raylet_client\n    if hasattr(worker, \"plasma_client\"):\n        worker.plasma_client.disconnect()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _try_to_compute_deterministic_class_id(cls, depth=5):\n    # Pickling, loading, and pickling again seems to produce more consistent\n    # results than simply pickling. This is a bit\n    class_id = pickle.dumps(cls)\n    for _ in range(depth):\n        new_class_id = pickle.dumps(pickle.loads(class_id))\n        if new_class_id == class_id:\n            # We appear to have reached a fix point, so use this as the ID.\n            return hashlib.sha1(new_class_id).digest()\n        class_id = new_class_id\n\n    # We have not reached a fixed point, so we may end up with a different\n    # class ID for this custom class on each worker, which could lead to the\n    # same class definition being exported many many times.\n    logger.warning(\n        \"WARNING: Could not produce a deterministic class ID for class \"\n        \"{}\".format(cls))\n    return hashlib.sha1(new_class_id).digest()", "response": "Attempt to produce a deterministic class ID for a given class."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef register_custom_serializer(cls,\n                               use_pickle=False,\n                               use_dict=False,\n                               serializer=None,\n                               deserializer=None,\n                               local=False,\n                               driver_id=None,\n                               class_id=None):\n    \"\"\"Enable serialization and deserialization for a particular class.\n\n    This method runs the register_class function defined below on every worker,\n    which will enable ray to properly serialize and deserialize objects of\n    this class.\n\n    Args:\n        cls (type): The class that ray should use this custom serializer for.\n        use_pickle (bool): If true, then objects of this class will be\n            serialized using pickle.\n        use_dict: If true, then objects of this class be serialized turning\n            their __dict__ fields into a dictionary. Must be False if\n            use_pickle is true.\n        serializer: The custom serializer to use. This should be provided if\n            and only if use_pickle and use_dict are False.\n        deserializer: The custom deserializer to use. This should be provided\n            if and only if use_pickle and use_dict are False.\n        local: True if the serializers should only be registered on the current\n            worker. This should usually be False.\n        driver_id: ID of the driver that we want to register the class for.\n        class_id: ID of the class that we are registering. If this is not\n            specified, we will calculate a new one inside the function.\n\n    Raises:\n        Exception: An exception is raised if pickle=False and the class cannot\n            be efficiently serialized by Ray. This can also raise an exception\n            if use_dict is true and cls is not pickleable.\n    \"\"\"\n    worker = global_worker\n    assert (serializer is None) == (deserializer is None), (\n        \"The serializer/deserializer arguments must both be provided or \"\n        \"both not be provided.\")\n    use_custom_serializer = (serializer is not None)\n\n    assert use_custom_serializer + use_pickle + use_dict == 1, (\n        \"Exactly one of use_pickle, use_dict, or serializer/deserializer must \"\n        \"be specified.\")\n\n    if use_dict:\n        # Raise an exception if cls cannot be serialized efficiently by Ray.\n        serialization.check_serializable(cls)\n\n    if class_id is None:\n        if not local:\n            # In this case, the class ID will be used to deduplicate the class\n            # across workers. Note that cloudpickle unfortunately does not\n            # produce deterministic strings, so these IDs could be different\n            # on different workers. We could use something weaker like\n            # cls.__name__, however that would run the risk of having\n            # collisions.\n            # TODO(rkn): We should improve this.\n            try:\n                # Attempt to produce a class ID that will be the same on each\n                # worker. However, determinism is not guaranteed, and the\n                # result may be different on different workers.\n                class_id = _try_to_compute_deterministic_class_id(cls)\n            except Exception:\n                raise serialization.CloudPickleError(\"Failed to pickle class \"\n                                                     \"'{}'\".format(cls))\n        else:\n            # In this case, the class ID only needs to be meaningful on this\n            # worker and not across workers.\n            class_id = _random_string()\n\n        # Make sure class_id is a string.\n        class_id = ray.utils.binary_to_hex(class_id)\n\n    if driver_id is None:\n        driver_id = worker.task_driver_id\n    assert isinstance(driver_id, DriverID)\n\n    def register_class_for_serialization(worker_info):\n        # TODO(rkn): We need to be more thoughtful about what to do if custom\n        # serializers have already been registered for class_id. In some cases,\n        # we may want to use the last user-defined serializers and ignore\n        # subsequent calls to register_custom_serializer that were made by the\n        # system.\n\n        serialization_context = worker_info[\n            \"worker\"].get_serialization_context(driver_id)\n        serialization_context.register_type(\n            cls,\n            class_id,\n            pickle=use_pickle,\n            custom_serializer=serializer,\n            custom_deserializer=deserializer)\n\n    if not local:\n        worker.run_function_on_all_workers(register_class_for_serialization)\n    else:\n        # Since we are pickling objects of this class, we don't actually need\n        # to ship the class definition.\n        register_class_for_serialization({\"worker\": worker})", "response": "This function registers a custom serializer for a particular class."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a remote object or a list of remote objects from the object store.", "response": "def get(object_ids):\n    \"\"\"Get a remote object or a list of remote objects from the object store.\n\n    This method blocks until the object corresponding to the object ID is\n    available in the local object store. If this object is not in the local\n    object store, it will be shipped from an object store that has it (once the\n    object has been created). If object_ids is a list, then the objects\n    corresponding to each object in the list will be returned.\n\n    Args:\n        object_ids: Object ID of the object to get or a list of object IDs to\n            get.\n\n    Returns:\n        A Python object or a list of Python objects.\n\n    Raises:\n        Exception: An exception is raised if the task that created the object\n            or that created one of the objects raised an exception.\n    \"\"\"\n    worker = global_worker\n    worker.check_connected()\n    with profiling.profile(\"ray.get\"):\n        if worker.mode == LOCAL_MODE:\n            # In LOCAL_MODE, ray.get is the identity operation (the input will\n            # actually be a value not an objectid).\n            return object_ids\n        global last_task_error_raise_time\n        if isinstance(object_ids, list):\n            values = worker.get_object(object_ids)\n            for i, value in enumerate(values):\n                if isinstance(value, RayError):\n                    last_task_error_raise_time = time.time()\n                    raise value\n            return values\n        else:\n            value = worker.get_object([object_ids])[0]\n            if isinstance(value, RayError):\n                # If the result is a RayError, then the task that created\n                # this object failed, and we should propagate the error message\n                # here.\n                last_task_error_raise_time = time.time()\n                raise value\n            return value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nstore an object in the object store.", "response": "def put(value):\n    \"\"\"Store an object in the object store.\n\n    Args:\n        value: The Python object to be stored.\n\n    Returns:\n        The object ID assigned to this value.\n    \"\"\"\n    worker = global_worker\n    worker.check_connected()\n    with profiling.profile(\"ray.put\"):\n        if worker.mode == LOCAL_MODE:\n            # In LOCAL_MODE, ray.put is the identity operation.\n            return value\n        object_id = ray._raylet.compute_put_id(\n            worker.current_task_id,\n            worker.task_context.put_index,\n        )\n        worker.put_object(object_id, value)\n        worker.task_context.put_index += 1\n        return object_id"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwait until the specified list of objects in the object store are ready.", "response": "def wait(object_ids, num_returns=1, timeout=None):\n    \"\"\"Return a list of IDs that are ready and a list of IDs that are not.\n\n    .. warning::\n\n        The **timeout** argument used to be in **milliseconds** (up through\n        ``ray==0.6.1``) and now it is in **seconds**.\n\n    If timeout is set, the function returns either when the requested number of\n    IDs are ready or when the timeout is reached, whichever occurs first. If it\n    is not set, the function simply waits until that number of objects is ready\n    and returns that exact number of object IDs.\n\n    This method returns two lists. The first list consists of object IDs that\n    correspond to objects that are available in the object store. The second\n    list corresponds to the rest of the object IDs (which may or may not be\n    ready).\n\n    Ordering of the input list of object IDs is preserved. That is, if A\n    precedes B in the input list, and both are in the ready list, then A will\n    precede B in the ready list. This also holds true if A and B are both in\n    the remaining list.\n\n    Args:\n        object_ids (List[ObjectID]): List of object IDs for objects that may or\n            may not be ready. Note that these IDs must be unique.\n        num_returns (int): The number of object IDs that should be returned.\n        timeout (float): The maximum amount of time in seconds to wait before\n            returning.\n\n    Returns:\n        A list of object IDs that are ready and a list of the remaining object\n        IDs.\n    \"\"\"\n    worker = global_worker\n\n    if isinstance(object_ids, ObjectID):\n        raise TypeError(\n            \"wait() expected a list of ray.ObjectID, got a single ray.ObjectID\"\n        )\n\n    if not isinstance(object_ids, list):\n        raise TypeError(\n            \"wait() expected a list of ray.ObjectID, got {}\".format(\n                type(object_ids)))\n\n    if isinstance(timeout, int) and timeout != 0:\n        logger.warning(\"The 'timeout' argument now requires seconds instead \"\n                       \"of milliseconds. This message can be suppressed by \"\n                       \"passing in a float.\")\n\n    if timeout is not None and timeout < 0:\n        raise ValueError(\"The 'timeout' argument must be nonnegative. \"\n                         \"Received {}\".format(timeout))\n\n    if worker.mode != LOCAL_MODE:\n        for object_id in object_ids:\n            if not isinstance(object_id, ObjectID):\n                raise TypeError(\"wait() expected a list of ray.ObjectID, \"\n                                \"got list containing {}\".format(\n                                    type(object_id)))\n\n    worker.check_connected()\n    # TODO(swang): Check main thread.\n    with profiling.profile(\"ray.wait\"):\n        # When Ray is run in LOCAL_MODE, all functions are run immediately,\n        # so all objects in object_id are ready.\n        if worker.mode == LOCAL_MODE:\n            return object_ids[:num_returns], object_ids[num_returns:]\n\n        # TODO(rkn): This is a temporary workaround for\n        # https://github.com/ray-project/ray/issues/997. However, it should be\n        # fixed in Arrow instead of here.\n        if len(object_ids) == 0:\n            return [], []\n\n        if len(object_ids) != len(set(object_ids)):\n            raise Exception(\"Wait requires a list of unique object IDs.\")\n        if num_returns <= 0:\n            raise Exception(\n                \"Invalid number of objects to return %d.\" % num_returns)\n        if num_returns > len(object_ids):\n            raise Exception(\"num_returns cannot be greater than the number \"\n                            \"of objects provided to ray.wait.\")\n\n        timeout = timeout if timeout is not None else 10**6\n        timeout_milliseconds = int(timeout * 1000)\n        ready_ids, remaining_ids = worker.raylet_client.wait(\n            object_ids,\n            num_returns,\n            timeout_milliseconds,\n            False,\n            worker.current_task_id,\n        )\n        return ready_ids, remaining_ids"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndefine a remote function or an actor class.", "response": "def remote(*args, **kwargs):\n    \"\"\"Define a remote function or an actor class.\n\n    This can be used with no arguments to define a remote function or actor as\n    follows:\n\n    .. code-block:: python\n\n        @ray.remote\n        def f():\n            return 1\n\n        @ray.remote\n        class Foo(object):\n            def method(self):\n                return 1\n\n    It can also be used with specific keyword arguments:\n\n    * **num_return_vals:** This is only for *remote functions*. It specifies\n      the number of object IDs returned by the remote function invocation.\n    * **num_cpus:** The quantity of CPU cores to reserve for this task or for\n      the lifetime of the actor.\n    * **num_gpus:** The quantity of GPUs to reserve for this task or for the\n      lifetime of the actor.\n    * **resources:** The quantity of various custom resources to reserve for\n      this task or for the lifetime of the actor. This is a dictionary mapping\n      strings (resource names) to numbers.\n    * **max_calls:** Only for *remote functions*. This specifies the maximum\n      number of times that a given worker can execute the given remote function\n      before it must exit (this can be used to address memory leaks in\n      third-party libraries or to reclaim resources that cannot easily be\n      released, e.g., GPU memory that was acquired by TensorFlow). By\n      default this is infinite.\n    * **max_reconstructions**: Only for *actors*. This specifies the maximum\n      number of times that the actor should be reconstructed when it dies\n      unexpectedly. The minimum valid value is 0 (default), which indicates\n      that the actor doesn't need to be reconstructed. And the maximum valid\n      value is ray.ray_constants.INFINITE_RECONSTRUCTIONS.\n\n    This can be done as follows:\n\n    .. code-block:: python\n\n        @ray.remote(num_gpus=1, max_calls=1, num_return_vals=2)\n        def f():\n            return 1, 2\n\n        @ray.remote(num_cpus=2, resources={\"CustomResource\": 1})\n        class Foo(object):\n            def method(self):\n                return 1\n    \"\"\"\n    worker = get_global_worker()\n\n    if len(args) == 1 and len(kwargs) == 0 and callable(args[0]):\n        # This is the case where the decorator is just @ray.remote.\n        return make_decorator(worker=worker)(args[0])\n\n    # Parse the keyword arguments from the decorator.\n    error_string = (\"The @ray.remote decorator must be applied either \"\n                    \"with no arguments and no parentheses, for example \"\n                    \"'@ray.remote', or it must be applied using some of \"\n                    \"the arguments 'num_return_vals', 'num_cpus', 'num_gpus', \"\n                    \"'resources', 'max_calls', \"\n                    \"or 'max_reconstructions', like \"\n                    \"'@ray.remote(num_return_vals=2, \"\n                    \"resources={\\\"CustomResource\\\": 1})'.\")\n    assert len(args) == 0 and len(kwargs) > 0, error_string\n    for key in kwargs:\n        assert key in [\n            \"num_return_vals\", \"num_cpus\", \"num_gpus\", \"resources\",\n            \"max_calls\", \"max_reconstructions\"\n        ], error_string\n\n    num_cpus = kwargs[\"num_cpus\"] if \"num_cpus\" in kwargs else None\n    num_gpus = kwargs[\"num_gpus\"] if \"num_gpus\" in kwargs else None\n    resources = kwargs.get(\"resources\")\n    if not isinstance(resources, dict) and resources is not None:\n        raise Exception(\"The 'resources' keyword argument must be a \"\n                        \"dictionary, but received type {}.\".format(\n                            type(resources)))\n    if resources is not None:\n        assert \"CPU\" not in resources, \"Use the 'num_cpus' argument.\"\n        assert \"GPU\" not in resources, \"Use the 'num_gpus' argument.\"\n\n    # Handle other arguments.\n    num_return_vals = kwargs.get(\"num_return_vals\")\n    max_calls = kwargs.get(\"max_calls\")\n    max_reconstructions = kwargs.get(\"max_reconstructions\")\n\n    return make_decorator(\n        num_return_vals=num_return_vals,\n        num_cpus=num_cpus,\n        num_gpus=num_gpus,\n        resources=resources,\n        max_calls=max_calls,\n        max_reconstructions=max_reconstructions,\n        worker=worker)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the SerializationContext of the given driver.", "response": "def get_serialization_context(self, driver_id):\n        \"\"\"Get the SerializationContext of the driver that this worker is processing.\n\n        Args:\n            driver_id: The ID of the driver that indicates which driver to get\n                the serialization context for.\n\n        Returns:\n            The serialization context of the given driver.\n        \"\"\"\n        # This function needs to be proctected by a lock, because it will be\n        # called by`register_class_for_serialization`, as well as the import\n        # thread, from different threads. Also, this function will recursively\n        # call itself, so we use RLock here.\n        with self.lock:\n            if driver_id not in self.serialization_context_map:\n                _initialize_serialization(driver_id)\n            return self.serialization_context_map[driver_id]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef store_and_register(self, object_id, value, depth=100):\n        counter = 0\n        while True:\n            if counter == depth:\n                raise Exception(\"Ray exceeded the maximum number of classes \"\n                                \"that it will recursively serialize when \"\n                                \"attempting to serialize an object of \"\n                                \"type {}.\".format(type(value)))\n            counter += 1\n            try:\n                if isinstance(value, bytes):\n                    # If the object is a byte array, skip serializing it and\n                    # use a special metadata to indicate it's raw binary. So\n                    # that this object can also be read by Java.\n                    self.plasma_client.put_raw_buffer(\n                        value,\n                        object_id=pyarrow.plasma.ObjectID(object_id.binary()),\n                        metadata=ray_constants.RAW_BUFFER_METADATA,\n                        memcopy_threads=self.memcopy_threads)\n                else:\n                    self.plasma_client.put(\n                        value,\n                        object_id=pyarrow.plasma.ObjectID(object_id.binary()),\n                        memcopy_threads=self.memcopy_threads,\n                        serialization_context=self.get_serialization_context(\n                            self.task_driver_id))\n                break\n            except pyarrow.SerializationCallbackError as e:\n                try:\n                    register_custom_serializer(\n                        type(e.example_object), use_dict=True)\n                    warning_message = (\"WARNING: Serializing objects of type \"\n                                       \"{} by expanding them as dictionaries \"\n                                       \"of their fields. This behavior may \"\n                                       \"be incorrect in some cases.\".format(\n                                           type(e.example_object)))\n                    logger.debug(warning_message)\n                except (serialization.RayNotDictionarySerializable,\n                        serialization.CloudPickleError,\n                        pickle.pickle.PicklingError, Exception):\n                    # We also handle generic exceptions here because\n                    # cloudpickle can fail with many different types of errors.\n                    try:\n                        register_custom_serializer(\n                            type(e.example_object), use_pickle=True)\n                        warning_message = (\"WARNING: Falling back to \"\n                                           \"serializing objects of type {} by \"\n                                           \"using pickle. This may be \"\n                                           \"inefficient.\".format(\n                                               type(e.example_object)))\n                        logger.warning(warning_message)\n                    except serialization.CloudPickleError:\n                        register_custom_serializer(\n                            type(e.example_object),\n                            use_pickle=True,\n                            local=True)\n                        warning_message = (\"WARNING: Pickling the class {} \"\n                                           \"failed, so we are using pickle \"\n                                           \"and only registering the class \"\n                                           \"locally.\".format(\n                                               type(e.example_object)))\n                        logger.warning(warning_message)", "response": "Stores an object in the object store and attempts to register its class if needed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef put_object(self, object_id, value):\n        # Make sure that the value is not an object ID.\n        if isinstance(value, ObjectID):\n            raise TypeError(\n                \"Calling 'put' on an ray.ObjectID is not allowed \"\n                \"(similarly, returning an ray.ObjectID from a remote \"\n                \"function is not allowed). If you really want to \"\n                \"do this, you can wrap the ray.ObjectID in a list and \"\n                \"call 'put' on it (or return it).\")\n\n        # Serialize and put the object in the object store.\n        try:\n            self.store_and_register(object_id, value)\n        except pyarrow.PlasmaObjectExists:\n            # The object already exists in the object store, so there is no\n            # need to add it again. TODO(rkn): We need to compare the hashes\n            # and make sure that the objects are in fact the same. We also\n            # should return an error code to the caller instead of printing a\n            # message.\n            logger.info(\n                \"The object with ID {} already exists in the object store.\"\n                .format(object_id))\n        except TypeError:\n            # This error can happen because one of the members of the object\n            # may not be serializable for cloudpickle. So we need these extra\n            # fallbacks here to start from the beginning. Hopefully the object\n            # could have a `__reduce__` method.\n            register_custom_serializer(type(value), use_pickle=True)\n            warning_message = (\"WARNING: Serializing the class {} failed, \"\n                               \"so are are falling back to cloudpickle.\"\n                               .format(type(value)))\n            logger.warning(warning_message)\n            self.store_and_register(object_id, value)", "response": "Stores the value for the specified object in the local object store with the specified object ID."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_object(self, object_ids):\n        # Make sure that the values are object IDs.\n        for object_id in object_ids:\n            if not isinstance(object_id, ObjectID):\n                raise TypeError(\n                    \"Attempting to call `get` on the value {}, \"\n                    \"which is not an ray.ObjectID.\".format(object_id))\n        # Do an initial fetch for remote objects. We divide the fetch into\n        # smaller fetches so as to not block the manager for a prolonged period\n        # of time in a single call.\n        plain_object_ids = [\n            plasma.ObjectID(object_id.binary()) for object_id in object_ids\n        ]\n        for i in range(0, len(object_ids),\n                       ray._config.worker_fetch_request_size()):\n            self.raylet_client.fetch_or_reconstruct(\n                object_ids[i:(i + ray._config.worker_fetch_request_size())],\n                True)\n\n        # Get the objects. We initially try to get the objects immediately.\n        final_results = self.retrieve_and_deserialize(plain_object_ids, 0)\n        # Construct a dictionary mapping object IDs that we haven't gotten yet\n        # to their original index in the object_ids argument.\n        unready_ids = {\n            plain_object_ids[i].binary(): i\n            for (i, val) in enumerate(final_results)\n            if val is plasma.ObjectNotAvailable\n        }\n\n        if len(unready_ids) > 0:\n            # Try reconstructing any objects we haven't gotten yet. Try to\n            # get them until at least get_timeout_milliseconds\n            # milliseconds passes, then repeat.\n            while len(unready_ids) > 0:\n                object_ids_to_fetch = [\n                    plasma.ObjectID(unready_id)\n                    for unready_id in unready_ids.keys()\n                ]\n                ray_object_ids_to_fetch = [\n                    ObjectID(unready_id) for unready_id in unready_ids.keys()\n                ]\n                fetch_request_size = ray._config.worker_fetch_request_size()\n                for i in range(0, len(object_ids_to_fetch),\n                               fetch_request_size):\n                    self.raylet_client.fetch_or_reconstruct(\n                        ray_object_ids_to_fetch[i:(i + fetch_request_size)],\n                        False,\n                        self.current_task_id,\n                    )\n                results = self.retrieve_and_deserialize(\n                    object_ids_to_fetch,\n                    max([\n                        ray._config.get_timeout_milliseconds(),\n                        int(0.01 * len(unready_ids)),\n                    ]),\n                )\n                # Remove any entries for objects we received during this\n                # iteration so we don't retrieve the same object twice.\n                for i, val in enumerate(results):\n                    if val is not plasma.ObjectNotAvailable:\n                        object_id = object_ids_to_fetch[i].binary()\n                        index = unready_ids[object_id]\n                        final_results[index] = val\n                        unready_ids.pop(object_id)\n\n            # If there were objects that we weren't able to get locally,\n            # let the raylet know that we're now unblocked.\n            self.raylet_client.notify_unblocked(self.current_task_id)\n\n        assert len(final_results) == len(object_ids)\n        return final_results", "response": "Get the value or values in the object store associated with the IDs."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef submit_task(self,\n                    function_descriptor,\n                    args,\n                    actor_id=None,\n                    actor_handle_id=None,\n                    actor_counter=0,\n                    actor_creation_id=None,\n                    actor_creation_dummy_object_id=None,\n                    max_actor_reconstructions=0,\n                    execution_dependencies=None,\n                    new_actor_handles=None,\n                    num_return_vals=None,\n                    resources=None,\n                    placement_resources=None,\n                    driver_id=None):\n        \"\"\"Submit a remote task to the scheduler.\n\n        Tell the scheduler to schedule the execution of the function with\n        function_descriptor with arguments args. Retrieve object IDs for the\n        outputs of the function from the scheduler and immediately return them.\n\n        Args:\n            function_descriptor: The function descriptor to execute.\n            args: The arguments to pass into the function. Arguments can be\n                object IDs or they can be values. If they are values, they must\n                be serializable objects.\n            actor_id: The ID of the actor that this task is for.\n            actor_counter: The counter of the actor task.\n            actor_creation_id: The ID of the actor to create, if this is an\n                actor creation task.\n            actor_creation_dummy_object_id: If this task is an actor method,\n                then this argument is the dummy object ID associated with the\n                actor creation task for the corresponding actor.\n            execution_dependencies: The execution dependencies for this task.\n            num_return_vals: The number of return values this function should\n                have.\n            resources: The resource requirements for this task.\n            placement_resources: The resources required for placing the task.\n                If this is not provided or if it is an empty dictionary, then\n                the placement resources will be equal to resources.\n            driver_id: The ID of the relevant driver. This is almost always the\n                driver ID of the driver that is currently running. However, in\n                the exceptional case that an actor task is being dispatched to\n                an actor created by a different driver, this should be the\n                driver ID of the driver that created the actor.\n\n        Returns:\n            The return object IDs for this task.\n        \"\"\"\n        with profiling.profile(\"submit_task\"):\n            if actor_id is None:\n                assert actor_handle_id is None\n                actor_id = ActorID.nil()\n                actor_handle_id = ActorHandleID.nil()\n            else:\n                assert actor_handle_id is not None\n\n            if actor_creation_id is None:\n                actor_creation_id = ActorID.nil()\n\n            if actor_creation_dummy_object_id is None:\n                actor_creation_dummy_object_id = ObjectID.nil()\n\n            # Put large or complex arguments that are passed by value in the\n            # object store first.\n            args_for_raylet = []\n            for arg in args:\n                if isinstance(arg, ObjectID):\n                    args_for_raylet.append(arg)\n                elif ray._raylet.check_simple_value(arg):\n                    args_for_raylet.append(arg)\n                else:\n                    args_for_raylet.append(put(arg))\n\n            # By default, there are no execution dependencies.\n            if execution_dependencies is None:\n                execution_dependencies = []\n\n            if new_actor_handles is None:\n                new_actor_handles = []\n\n            if driver_id is None:\n                driver_id = self.task_driver_id\n\n            if resources is None:\n                raise ValueError(\"The resources dictionary is required.\")\n            for value in resources.values():\n                assert (isinstance(value, int) or isinstance(value, float))\n                if value < 0:\n                    raise ValueError(\n                        \"Resource quantities must be nonnegative.\")\n                if (value >= 1 and isinstance(value, float)\n                        and not value.is_integer()):\n                    raise ValueError(\n                        \"Resource quantities must all be whole numbers.\")\n\n            # Remove any resources with zero quantity requirements\n            resources = {\n                resource_label: resource_quantity\n                for resource_label, resource_quantity in resources.items()\n                if resource_quantity > 0\n            }\n\n            if placement_resources is None:\n                placement_resources = {}\n\n            # Increment the worker's task index to track how many tasks\n            # have been submitted by the current task so far.\n            self.task_context.task_index += 1\n            # The parent task must be set for the submitted task.\n            assert not self.current_task_id.is_nil()\n            # Current driver id must not be nil when submitting a task.\n            # Because every task must belong to a driver.\n            assert not self.task_driver_id.is_nil()\n            # Submit the task to raylet.\n            function_descriptor_list = (\n                function_descriptor.get_function_descriptor_list())\n            assert isinstance(driver_id, DriverID)\n            task = ray._raylet.Task(\n                driver_id,\n                function_descriptor_list,\n                args_for_raylet,\n                num_return_vals,\n                self.current_task_id,\n                self.task_context.task_index,\n                actor_creation_id,\n                actor_creation_dummy_object_id,\n                max_actor_reconstructions,\n                actor_id,\n                actor_handle_id,\n                actor_counter,\n                new_actor_handles,\n                execution_dependencies,\n                resources,\n                placement_resources,\n            )\n            self.raylet_client.submit_task(task)\n\n            return task.returns()", "response": "This function is called by the scheduler to submit a remote task to the scheduler."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run_function_on_all_workers(self, function,\n                                    run_on_other_drivers=False):\n        \"\"\"Run arbitrary code on all of the workers.\n\n        This function will first be run on the driver, and then it will be\n        exported to all of the workers to be run. It will also be run on any\n        new workers that register later. If ray.init has not been called yet,\n        then cache the function and export it later.\n\n        Args:\n            function (Callable): The function to run on all of the workers. It\n                takes only one argument, a worker info dict. If it returns\n                anything, its return values will not be used.\n            run_on_other_drivers: The boolean that indicates whether we want to\n                run this function on other drivers. One case is we may need to\n                share objects across drivers.\n        \"\"\"\n        # If ray.init has not been called yet, then cache the function and\n        # export it when connect is called. Otherwise, run the function on all\n        # workers.\n        if self.mode is None:\n            self.cached_functions_to_run.append(function)\n        else:\n            # Attempt to pickle the function before we need it. This could\n            # fail, and it is more convenient if the failure happens before we\n            # actually run the function locally.\n            pickled_function = pickle.dumps(function)\n\n            function_to_run_id = hashlib.sha1(pickled_function).digest()\n            key = b\"FunctionsToRun:\" + function_to_run_id\n            # First run the function on the driver.\n            # We always run the task locally.\n            function({\"worker\": self})\n            # Check if the function has already been put into redis.\n            function_exported = self.redis_client.setnx(b\"Lock:\" + key, 1)\n            if not function_exported:\n                # In this case, the function has already been exported, so\n                # we don't need to export it again.\n                return\n\n            check_oversized_pickle(pickled_function, function.__name__,\n                                   \"function\", self)\n\n            # Run the function on all workers.\n            self.redis_client.hmset(\n                key, {\n                    \"driver_id\": self.task_driver_id.binary(),\n                    \"function_id\": function_to_run_id,\n                    \"function\": pickled_function,\n                    \"run_on_other_drivers\": str(run_on_other_drivers)\n                })\n            self.redis_client.rpush(\"Exports\", key)", "response": "Run arbitrary code on all of the workers."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving the arguments for the remote function. This retrieves the values for the arguments to the remote function that were passed in as object IDs. Arguments that were passed by value are not changed. This is called by the worker that is executing the remote function. Args: function_name (str): The name of the remote function whose arguments are being retrieved. serialized_args (List): The arguments to the function. These are either strings representing serialized objects passed by value or they are ray.ObjectIDs. Returns: The retrieved arguments in addition to the arguments that were passed by value. Raises: RayError: This exception is raised if a task that created one of the arguments failed.", "response": "def _get_arguments_for_execution(self, function_name, serialized_args):\n        \"\"\"Retrieve the arguments for the remote function.\n\n        This retrieves the values for the arguments to the remote function that\n        were passed in as object IDs. Arguments that were passed by value are\n        not changed. This is called by the worker that is executing the remote\n        function.\n\n        Args:\n            function_name (str): The name of the remote function whose\n                arguments are being retrieved.\n            serialized_args (List): The arguments to the function. These are\n                either strings representing serialized objects passed by value\n                or they are ray.ObjectIDs.\n\n        Returns:\n            The retrieved arguments in addition to the arguments that were\n                passed by value.\n\n        Raises:\n            RayError: This exception is raised if a task that\n                created one of the arguments failed.\n        \"\"\"\n        arguments = []\n        for (i, arg) in enumerate(serialized_args):\n            if isinstance(arg, ObjectID):\n                # get the object from the local object store\n                argument = self.get_object([arg])[0]\n                if isinstance(argument, RayError):\n                    raise argument\n            else:\n                # pass the argument by value\n                argument = arg\n\n            arguments.append(argument)\n        return arguments"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _store_outputs_in_object_store(self, object_ids, outputs):\n        for i in range(len(object_ids)):\n            if isinstance(outputs[i], ray.actor.ActorHandle):\n                raise Exception(\"Returning an actor handle from a remote \"\n                                \"function is not allowed).\")\n            if outputs[i] is ray.experimental.no_return.NoReturn:\n                if not self.plasma_client.contains(\n                        pyarrow.plasma.ObjectID(object_ids[i].binary())):\n                    raise RuntimeError(\n                        \"Attempting to return 'ray.experimental.NoReturn' \"\n                        \"from a remote function, but the corresponding \"\n                        \"ObjectID does not exist in the local object store.\")\n            else:\n                self.put_object(object_ids[i], outputs[i])", "response": "Store the outputs of a remote function call in the local object store."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexecutes a task and store the result in the object store.", "response": "def _process_task(self, task, function_execution_info):\n        \"\"\"Execute a task assigned to this worker.\n\n        This method deserializes a task from the scheduler, and attempts to\n        execute the task. If the task succeeds, the outputs are stored in the\n        local object store. If the task throws an exception, RayTaskError\n        objects are stored in the object store to represent the failed task\n        (these will be retrieved by calls to get or by subsequent tasks that\n        use the outputs of this task).\n        \"\"\"\n        assert self.current_task_id.is_nil()\n        assert self.task_context.task_index == 0\n        assert self.task_context.put_index == 1\n        if task.actor_id().is_nil():\n            # If this worker is not an actor, check that `task_driver_id`\n            # was reset when the worker finished the previous task.\n            assert self.task_driver_id.is_nil()\n            # Set the driver ID of the current running task. This is\n            # needed so that if the task throws an exception, we propagate\n            # the error message to the correct driver.\n            self.task_driver_id = task.driver_id()\n        else:\n            # If this worker is an actor, task_driver_id wasn't reset.\n            # Check that current task's driver ID equals the previous one.\n            assert self.task_driver_id == task.driver_id()\n\n        self.task_context.current_task_id = task.task_id()\n\n        function_descriptor = FunctionDescriptor.from_bytes_list(\n            task.function_descriptor_list())\n        args = task.arguments()\n        return_object_ids = task.returns()\n        if (not task.actor_id().is_nil()\n                or not task.actor_creation_id().is_nil()):\n            dummy_return_id = return_object_ids.pop()\n        function_executor = function_execution_info.function\n        function_name = function_execution_info.function_name\n\n        # Get task arguments from the object store.\n        try:\n            if function_name != \"__ray_terminate__\":\n                self.reraise_actor_init_error()\n            self.memory_monitor.raise_if_low_memory()\n            with profiling.profile(\"task:deserialize_arguments\"):\n                arguments = self._get_arguments_for_execution(\n                    function_name, args)\n        except Exception as e:\n            self._handle_process_task_failure(\n                function_descriptor, return_object_ids, e,\n                ray.utils.format_error_message(traceback.format_exc()))\n            return\n\n        # Execute the task.\n        try:\n            self._current_task = task\n            with profiling.profile(\"task:execute\"):\n                if (task.actor_id().is_nil()\n                        and task.actor_creation_id().is_nil()):\n                    outputs = function_executor(*arguments)\n                else:\n                    if not task.actor_id().is_nil():\n                        key = task.actor_id()\n                    else:\n                        key = task.actor_creation_id()\n                    outputs = function_executor(dummy_return_id,\n                                                self.actors[key], *arguments)\n        except Exception as e:\n            # Determine whether the exception occured during a task, not an\n            # actor method.\n            task_exception = task.actor_id().is_nil()\n            traceback_str = ray.utils.format_error_message(\n                traceback.format_exc(), task_exception=task_exception)\n            self._handle_process_task_failure(\n                function_descriptor, return_object_ids, e, traceback_str)\n            return\n        finally:\n            self._current_task = None\n\n        # Store the outputs in the local object store.\n        try:\n            with profiling.profile(\"task:store_outputs\"):\n                # If this is an actor task, then the last object ID returned by\n                # the task is a dummy output, not returned by the function\n                # itself. Decrement to get the correct number of return values.\n                num_returns = len(return_object_ids)\n                if num_returns == 1:\n                    outputs = (outputs, )\n                self._store_outputs_in_object_store(return_object_ids, outputs)\n        except Exception as e:\n            self._handle_process_task_failure(\n                function_descriptor, return_object_ids, e,\n                ray.utils.format_error_message(traceback.format_exc()))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _wait_for_and_process_task(self, task):\n        function_descriptor = FunctionDescriptor.from_bytes_list(\n            task.function_descriptor_list())\n        driver_id = task.driver_id()\n\n        # TODO(rkn): It would be preferable for actor creation tasks to share\n        # more of the code path with regular task execution.\n        if not task.actor_creation_id().is_nil():\n            assert self.actor_id.is_nil()\n            self.actor_id = task.actor_creation_id()\n            self.actor_creation_task_id = task.task_id()\n            actor_class = self.function_actor_manager.load_actor_class(\n                driver_id, function_descriptor)\n            self.actors[self.actor_id] = actor_class.__new__(actor_class)\n            self.actor_checkpoint_info[self.actor_id] = ActorCheckpointInfo(\n                num_tasks_since_last_checkpoint=0,\n                last_checkpoint_timestamp=int(1000 * time.time()),\n                checkpoint_ids=[],\n            )\n\n        execution_info = self.function_actor_manager.get_execution_info(\n            driver_id, function_descriptor)\n\n        # Execute the task.\n        function_name = execution_info.function_name\n        extra_data = {\"name\": function_name, \"task_id\": task.task_id().hex()}\n        if task.actor_id().is_nil():\n            if task.actor_creation_id().is_nil():\n                title = \"ray_worker:{}()\".format(function_name)\n                next_title = \"ray_worker\"\n            else:\n                actor = self.actors[task.actor_creation_id()]\n                title = \"ray_{}:{}()\".format(actor.__class__.__name__,\n                                             function_name)\n                next_title = \"ray_{}\".format(actor.__class__.__name__)\n        else:\n            actor = self.actors[task.actor_id()]\n            title = \"ray_{}:{}()\".format(actor.__class__.__name__,\n                                         function_name)\n            next_title = \"ray_{}\".format(actor.__class__.__name__)\n        with profiling.profile(\"task\", extra_data=extra_data):\n            with _changeproctitle(title, next_title):\n                self._process_task(task, execution_info)\n            # Reset the state fields so the next task can run.\n            self.task_context.current_task_id = TaskID.nil()\n            self.task_context.task_index = 0\n            self.task_context.put_index = 1\n            if self.actor_id.is_nil():\n                # Don't need to reset task_driver_id if the worker is an\n                # actor. Because the following tasks should all have the\n                # same driver id.\n                self.task_driver_id = DriverID.nil()\n                # Reset signal counters so that the next task can get\n                # all past signals.\n                ray_signal.reset()\n\n        # Increase the task execution counter.\n        self.function_actor_manager.increase_task_counter(\n            driver_id, function_descriptor)\n\n        reached_max_executions = (self.function_actor_manager.get_task_counter(\n            driver_id, function_descriptor) == execution_info.max_calls)\n        if reached_max_executions:\n            self.raylet_client.disconnect()\n            sys.exit(0)", "response": "Wait for a task to be ready and process it."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_next_task_from_raylet(self):\n        with profiling.profile(\"worker_idle\"):\n            task = self.raylet_client.get_task()\n\n        # Automatically restrict the GPUs available to this task.\n        ray.utils.set_cuda_visible_devices(ray.get_gpu_ids())\n\n        return task", "response": "Get the next task from the raylet."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef main_loop(self):\n\n        def exit(signum, frame):\n            shutdown()\n            sys.exit(0)\n\n        signal.signal(signal.SIGTERM, exit)\n\n        while True:\n            task = self._get_next_task_from_raylet()\n            self._wait_for_and_process_task(task)", "response": "The main loop for the worker."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef address_info(self):\n        return {\n            \"node_ip_address\": self._node_ip_address,\n            \"redis_address\": self._redis_address,\n            \"object_store_address\": self._plasma_store_socket_name,\n            \"raylet_socket_name\": self._raylet_socket_name,\n            \"webui_url\": self._webui_url,\n        }", "response": "Get a dictionary of addresses."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_redis_client(self):\n        return ray.services.create_redis_client(\n            self._redis_address, self._ray_params.redis_password)", "response": "Create a redis client."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _make_inc_temp(self, suffix=\"\", prefix=\"\", directory_name=\"/tmp/ray\"):\n        directory_name = os.path.expanduser(directory_name)\n        index = self._incremental_dict[suffix, prefix, directory_name]\n        # `tempfile.TMP_MAX` could be extremely large,\n        # so using `range` in Python2.x should be avoided.\n        while index < tempfile.TMP_MAX:\n            if index == 0:\n                filename = os.path.join(directory_name, prefix + suffix)\n            else:\n                filename = os.path.join(directory_name,\n                                        prefix + \".\" + str(index) + suffix)\n            index += 1\n            if not os.path.exists(filename):\n                # Save the index.\n                self._incremental_dict[suffix, prefix, directory_name] = index\n                return filename\n\n        raise FileExistsError(errno.EEXIST,\n                              \"No usable temporary filename found\")", "response": "Return a temporary file name. The file is not created."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef new_log_files(self, name, redirect_output=True):\n        if redirect_output is None:\n            redirect_output = self._ray_params.redirect_output\n        if not redirect_output:\n            return None, None\n\n        log_stdout = self._make_inc_temp(\n            suffix=\".out\", prefix=name, directory_name=self._logs_dir)\n        log_stderr = self._make_inc_temp(\n            suffix=\".err\", prefix=name, directory_name=self._logs_dir)\n        # Line-buffer the output (mode 1).\n        log_stdout_file = open(log_stdout, \"a\", buffering=1)\n        log_stderr_file = open(log_stderr, \"a\", buffering=1)\n        return log_stdout_file, log_stderr_file", "response": "Generate partially randomized filenames for log files."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprepare the socket file for raylet and plasma.", "response": "def _prepare_socket_file(self, socket_path, default_prefix):\n        \"\"\"Prepare the socket file for raylet and plasma.\n\n        This method helps to prepare a socket file.\n        1. Make the directory if the directory does not exist.\n        2. If the socket file exists, raise exception.\n\n        Args:\n            socket_path (string): the socket file to prepare.\n        \"\"\"\n        if socket_path is not None:\n            if os.path.exists(socket_path):\n                raise Exception(\"Socket file {} exists!\".format(socket_path))\n            socket_dir = os.path.dirname(socket_path)\n            try_to_create_directory(socket_dir)\n            return socket_path\n        return self._make_inc_temp(\n            prefix=default_prefix, directory_name=self._sockets_dir)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstart the Redis servers.", "response": "def start_redis(self):\n        \"\"\"Start the Redis servers.\"\"\"\n        assert self._redis_address is None\n        redis_log_files = [self.new_log_files(\"redis\")]\n        for i in range(self._ray_params.num_redis_shards):\n            redis_log_files.append(self.new_log_files(\"redis-shard_\" + str(i)))\n\n        (self._redis_address, redis_shards,\n         process_infos) = ray.services.start_redis(\n             self._node_ip_address,\n             redis_log_files,\n             port=self._ray_params.redis_port,\n             redis_shard_ports=self._ray_params.redis_shard_ports,\n             num_redis_shards=self._ray_params.num_redis_shards,\n             redis_max_clients=self._ray_params.redis_max_clients,\n             redirect_worker_output=True,\n             password=self._ray_params.redis_password,\n             include_java=self._ray_params.include_java,\n             redis_max_memory=self._ray_params.redis_max_memory)\n        assert (\n            ray_constants.PROCESS_TYPE_REDIS_SERVER not in self.all_processes)\n        self.all_processes[ray_constants.PROCESS_TYPE_REDIS_SERVER] = (\n            process_infos)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nstart the log monitor process.", "response": "def start_log_monitor(self):\n        \"\"\"Start the log monitor.\"\"\"\n        stdout_file, stderr_file = self.new_log_files(\"log_monitor\")\n        process_info = ray.services.start_log_monitor(\n            self.redis_address,\n            self._logs_dir,\n            stdout_file=stdout_file,\n            stderr_file=stderr_file,\n            redis_password=self._ray_params.redis_password)\n        assert ray_constants.PROCESS_TYPE_LOG_MONITOR not in self.all_processes\n        self.all_processes[ray_constants.PROCESS_TYPE_LOG_MONITOR] = [\n            process_info\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef start_plasma_store(self):\n        stdout_file, stderr_file = self.new_log_files(\"plasma_store\")\n        process_info = ray.services.start_plasma_store(\n            stdout_file=stdout_file,\n            stderr_file=stderr_file,\n            object_store_memory=self._ray_params.object_store_memory,\n            plasma_directory=self._ray_params.plasma_directory,\n            huge_pages=self._ray_params.huge_pages,\n            plasma_store_socket_name=self._plasma_store_socket_name)\n        assert (\n            ray_constants.PROCESS_TYPE_PLASMA_STORE not in self.all_processes)\n        self.all_processes[ray_constants.PROCESS_TYPE_PLASMA_STORE] = [\n            process_info\n        ]", "response": "Start the plasma store."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef start_raylet(self, use_valgrind=False, use_profiler=False):\n        stdout_file, stderr_file = self.new_log_files(\"raylet\")\n        process_info = ray.services.start_raylet(\n            self._redis_address,\n            self._node_ip_address,\n            self._raylet_socket_name,\n            self._plasma_store_socket_name,\n            self._ray_params.worker_path,\n            self._temp_dir,\n            self._ray_params.num_cpus,\n            self._ray_params.num_gpus,\n            self._ray_params.resources,\n            self._ray_params.object_manager_port,\n            self._ray_params.node_manager_port,\n            self._ray_params.redis_password,\n            use_valgrind=use_valgrind,\n            use_profiler=use_profiler,\n            stdout_file=stdout_file,\n            stderr_file=stderr_file,\n            config=self._config,\n            include_java=self._ray_params.include_java,\n            java_worker_options=self._ray_params.java_worker_options,\n            load_code_from_local=self._ray_params.load_code_from_local,\n        )\n        assert ray_constants.PROCESS_TYPE_RAYLET not in self.all_processes\n        self.all_processes[ray_constants.PROCESS_TYPE_RAYLET] = [process_info]", "response": "Start the raylet.\n\n        Args:\n            use_valgrind (bool): True if we should start the process in\n                valgrind.\n            use_profiler (bool): True if we should start the process in the\n                valgrind profiler."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating new logging files for workers to redirect its output.", "response": "def new_worker_redirected_log_file(self, worker_id):\n        \"\"\"Create new logging files for workers to redirect its output.\"\"\"\n        worker_stdout_file, worker_stderr_file = (self.new_log_files(\n            \"worker-\" + ray.utils.binary_to_hex(worker_id), True))\n        return worker_stdout_file, worker_stderr_file"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef start_raylet_monitor(self):\n        stdout_file, stderr_file = self.new_log_files(\"raylet_monitor\")\n        process_info = ray.services.start_raylet_monitor(\n            self._redis_address,\n            stdout_file=stdout_file,\n            stderr_file=stderr_file,\n            redis_password=self._ray_params.redis_password,\n            config=self._config)\n        assert (ray_constants.PROCESS_TYPE_RAYLET_MONITOR not in\n                self.all_processes)\n        self.all_processes[ray_constants.PROCESS_TYPE_RAYLET_MONITOR] = [\n            process_info\n        ]", "response": "Start the raylet monitor."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef start_head_processes(self):\n        logger.info(\n            \"Process STDOUT and STDERR is being redirected to {}.\".format(\n                self._logs_dir))\n        assert self._redis_address is None\n        # If this is the head node, start the relevant head node processes.\n        self.start_redis()\n        self.start_monitor()\n        self.start_raylet_monitor()\n        # The dashboard is Python3.x only.\n        if PY3 and self._ray_params.include_webui:\n            self.start_dashboard()", "response": "Start head processes on the node."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef start_ray_processes(self):\n        logger.info(\n            \"Process STDOUT and STDERR is being redirected to {}.\".format(\n                self._logs_dir))\n\n        self.start_plasma_store()\n        self.start_raylet()\n        if PY3:\n            self.start_reporter()\n\n        if self._ray_params.include_log_monitor:\n            self.start_log_monitor()", "response": "Start all of the processes on the node."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _kill_process_type(self,\n                           process_type,\n                           allow_graceful=False,\n                           check_alive=True,\n                           wait=False):\n        \"\"\"Kill a process of a given type.\n\n        If the process type is PROCESS_TYPE_REDIS_SERVER, then we will kill all\n        of the Redis servers.\n\n        If the process was started in valgrind, then we will raise an exception\n        if the process has a non-zero exit code.\n\n        Args:\n            process_type: The type of the process to kill.\n            allow_graceful (bool): Send a SIGTERM first and give the process\n                time to exit gracefully. If that doesn't work, then use\n                SIGKILL. We usually want to do this outside of tests.\n            check_alive (bool): If true, then we expect the process to be alive\n                and will raise an exception if the process is already dead.\n            wait (bool): If true, then this method will not return until the\n                process in question has exited.\n\n        Raises:\n            This process raises an exception in the following cases:\n                1. The process had already died and check_alive is true.\n                2. The process had been started in valgrind and had a non-zero\n                   exit code.\n        \"\"\"\n        process_infos = self.all_processes[process_type]\n        if process_type != ray_constants.PROCESS_TYPE_REDIS_SERVER:\n            assert len(process_infos) == 1\n        for process_info in process_infos:\n            process = process_info.process\n            # Handle the case where the process has already exited.\n            if process.poll() is not None:\n                if check_alive:\n                    raise Exception(\"Attempting to kill a process of type \"\n                                    \"'{}', but this process is already dead.\"\n                                    .format(process_type))\n                else:\n                    continue\n\n            if process_info.use_valgrind:\n                process.terminate()\n                process.wait()\n                if process.returncode != 0:\n                    message = (\"Valgrind detected some errors in process of \"\n                               \"type {}. Error code {}.\".format(\n                                   process_type, process.returncode))\n                    if process_info.stdout_file is not None:\n                        with open(process_info.stdout_file, \"r\") as f:\n                            message += \"\\nPROCESS STDOUT:\\n\" + f.read()\n                    if process_info.stderr_file is not None:\n                        with open(process_info.stderr_file, \"r\") as f:\n                            message += \"\\nPROCESS STDERR:\\n\" + f.read()\n                    raise Exception(message)\n                continue\n\n            if process_info.use_valgrind_profiler:\n                # Give process signal to write profiler data.\n                os.kill(process.pid, signal.SIGINT)\n                # Wait for profiling data to be written.\n                time.sleep(0.1)\n\n            if allow_graceful:\n                # Allow the process one second to exit gracefully.\n                process.terminate()\n                timer = threading.Timer(1, lambda process: process.kill(),\n                                        [process])\n                try:\n                    timer.start()\n                    process.wait()\n                finally:\n                    timer.cancel()\n\n                if process.poll() is not None:\n                    continue\n\n            # If the process did not exit within one second, force kill it.\n            process.kill()\n            # The reason we usually don't call process.wait() here is that\n            # there's some chance we'd end up waiting a really long time.\n            if wait:\n                process.wait()\n\n        del self.all_processes[process_type]", "response": "Kill a process of a given type."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef kill_redis(self, check_alive=True):\n        self._kill_process_type(\n            ray_constants.PROCESS_TYPE_REDIS_SERVER, check_alive=check_alive)", "response": "Kill the Redis servers."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef kill_plasma_store(self, check_alive=True):\n        self._kill_process_type(\n            ray_constants.PROCESS_TYPE_PLASMA_STORE, check_alive=check_alive)", "response": "Kill the plasma store."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nkilling the raylet. Args: check_alive (bool): Raise an exception if the process was already dead.", "response": "def kill_raylet(self, check_alive=True):\n        \"\"\"Kill the raylet.\n\n        Args:\n            check_alive (bool): Raise an exception if the process was already\n                dead.\n        \"\"\"\n        self._kill_process_type(\n            ray_constants.PROCESS_TYPE_RAYLET, check_alive=check_alive)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nkill the log monitor.", "response": "def kill_log_monitor(self, check_alive=True):\n        \"\"\"Kill the log monitor.\n\n        Args:\n            check_alive (bool): Raise an exception if the process was already\n                dead.\n        \"\"\"\n        self._kill_process_type(\n            ray_constants.PROCESS_TYPE_LOG_MONITOR, check_alive=check_alive)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef kill_reporter(self, check_alive=True):\n        # reporter is started only in PY3.\n        if PY3:\n            self._kill_process_type(\n                ray_constants.PROCESS_TYPE_REPORTER, check_alive=check_alive)", "response": "Kill the reporter.\n\n        Args:\n            check_alive (bool): Raise an exception if the process was already\n                dead."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nkilling the dashboard. Args: check_alive (bool): Raise an exception if the process was already dead.", "response": "def kill_dashboard(self, check_alive=True):\n        \"\"\"Kill the dashboard.\n\n        Args:\n            check_alive (bool): Raise an exception if the process was already\n                dead.\n        \"\"\"\n        self._kill_process_type(\n            ray_constants.PROCESS_TYPE_DASHBOARD, check_alive=check_alive)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nkills the monitor. Args: check_alive (bool): Raise an exception if the process was already dead.", "response": "def kill_monitor(self, check_alive=True):\n        \"\"\"Kill the monitor.\n\n        Args:\n            check_alive (bool): Raise an exception if the process was already\n                dead.\n        \"\"\"\n        self._kill_process_type(\n            ray_constants.PROCESS_TYPE_MONITOR, check_alive=check_alive)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef kill_raylet_monitor(self, check_alive=True):\n        self._kill_process_type(\n            ray_constants.PROCESS_TYPE_RAYLET_MONITOR, check_alive=check_alive)", "response": "Kill the raylet monitor."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef kill_all_processes(self, check_alive=True, allow_graceful=False):\n        # Kill the raylet first. This is important for suppressing errors at\n        # shutdown because we give the raylet a chance to exit gracefully and\n        # clean up its child worker processes. If we were to kill the plasma\n        # store (or Redis) first, that could cause the raylet to exit\n        # ungracefully, leading to more verbose output from the workers.\n        if ray_constants.PROCESS_TYPE_RAYLET in self.all_processes:\n            self._kill_process_type(\n                ray_constants.PROCESS_TYPE_RAYLET,\n                check_alive=check_alive,\n                allow_graceful=allow_graceful)\n\n        # We call \"list\" to copy the keys because we are modifying the\n        # dictionary while iterating over it.\n        for process_type in list(self.all_processes.keys()):\n            self._kill_process_type(\n                process_type,\n                check_alive=check_alive,\n                allow_graceful=allow_graceful)", "response": "Kill all of the processes in the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of the live processes.", "response": "def live_processes(self):\n        \"\"\"Return a list of the live processes.\n\n        Returns:\n            A list of the live processes.\n        \"\"\"\n        result = []\n        for process_type, process_infos in self.all_processes.items():\n            for process_info in process_infos:\n                if process_info.process.poll() is None:\n                    result.append((process_type, process_info.process))\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a large array of noise to be shared by all workers.", "response": "def create_shared_noise(count):\n    \"\"\"Create a large array of noise to be shared by all workers.\"\"\"\n    seed = 123\n    noise = np.random.RandomState(seed).randn(count).astype(np.float32)\n    return noise"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmapping model name to model network configuration.", "response": "def get_model_config(model_name, dataset):\n    \"\"\"Map model name to model network configuration.\"\"\"\n    model_map = _get_model_map(dataset.name)\n    if model_name not in model_map:\n        raise ValueError(\"Invalid model name \\\"%s\\\" for dataset \\\"%s\\\"\" %\n                         (model_name, dataset.name))\n    else:\n        return model_map[model_name]()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nregistering a new model that can be obtained with get_model_config.", "response": "def register_model(model_name, dataset_name, model_func):\n    \"\"\"Register a new model that can be obtained with `get_model_config`.\"\"\"\n    model_map = _get_model_map(dataset_name)\n    if model_name in model_map:\n        raise ValueError(\"Model \\\"%s\\\" is already registered for dataset\"\n                         \"\\\"%s\\\"\" % (model_name, dataset_name))\n    model_map[model_name] = model_func"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rollout(policy, env, timestep_limit=None, add_noise=False, offset=0):\n    env_timestep_limit = env.spec.max_episode_steps\n    timestep_limit = (env_timestep_limit if timestep_limit is None else min(\n        timestep_limit, env_timestep_limit))\n    rews = []\n    t = 0\n    observation = env.reset()\n    for _ in range(timestep_limit or 999999):\n        ac = policy.compute(observation, add_noise=add_noise, update=True)[0]\n        observation, rew, done, _ = env.step(ac)\n        rew -= np.abs(offset)\n        rews.append(rew)\n        t += 1\n        if done:\n            break\n    rews = np.array(rews, dtype=np.float32)\n    return rews, t", "response": "Do a rollout of the object store."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprovide a list of Trial objects to be queued into the TrialRunner.", "response": "def next_trials(self):\n        \"\"\"Provides Trial objects to be queued into the TrialRunner.\n\n        Returns:\n            trials (list): Returns a list of trials.\n        \"\"\"\n        trials = list(self._trial_generator)\n        if self._shuffle:\n            random.shuffle(trials)\n        self._finished = True\n        return trials"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate Trial objects with the variant generation process.", "response": "def _generate_trials(self, unresolved_spec, output_path=\"\"):\n        \"\"\"Generates Trial objects with the variant generation process.\n\n        Uses a fixed point iteration to resolve variants. All trials\n        should be able to be generated at once.\n\n        See also: `ray.tune.suggest.variant_generator`.\n\n        Yields:\n            Trial object\n        \"\"\"\n\n        if \"run\" not in unresolved_spec:\n            raise TuneError(\"Must specify `run` in {}\".format(unresolved_spec))\n        for _ in range(unresolved_spec.get(\"num_samples\", 1)):\n            for resolved_vars, spec in generate_variants(unresolved_spec):\n                experiment_tag = str(self._counter)\n                if resolved_vars:\n                    experiment_tag += \"_{}\".format(resolved_vars)\n                self._counter += 1\n                yield create_trial_from_spec(\n                    spec,\n                    output_path,\n                    self._parser,\n                    experiment_tag=experiment_tag)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the result of applying self. operation to a contiguous subsequence of the array.", "response": "def reduce(self, start=0, end=None):\n        \"\"\"Returns result of applying `self.operation`\n        to a contiguous subsequence of the array.\n\n          self.operation(\n              arr[start], operation(arr[start+1], operation(... arr[end])))\n\n        Parameters\n        ----------\n        start: int\n          beginning of the subsequence\n        end: int\n          end of the subsequences\n\n        Returns\n        -------\n        reduced: obj\n          result of reducing self.operation over the specified range of array\n          elements.\n        \"\"\"\n        if end is None:\n            end = self._capacity - 1\n        if end < 0:\n            end += self._capacity\n        return self._reduce_helper(start, end, 1, 0, self._capacity - 1)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nserialize this policy for Monitor to pick up.", "response": "def set_flushing_policy(flushing_policy):\n    \"\"\"Serialize this policy for Monitor to pick up.\"\"\"\n    if \"RAY_USE_NEW_GCS\" not in os.environ:\n        raise Exception(\n            \"set_flushing_policy() is only available when environment \"\n            \"variable RAY_USE_NEW_GCS is present at both compile and run time.\"\n        )\n    ray.worker.global_worker.check_connected()\n    redis_client = ray.worker.global_worker.redis_client\n\n    serialized = pickle.dumps(flushing_policy)\n    redis_client.set(\"gcs_flushing_policy\", serialized)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_ssh_key():\n    path = os.environ.get(\"TUNE_CLUSTER_SSH_KEY\",\n                          os.path.expanduser(\"~/ray_bootstrap_key.pem\"))\n    if os.path.exists(path):\n        return path\n    return None", "response": "Returns the ssh key to connect to the cluster workers."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef on_trial_complete(self,\n                          trial_id,\n                          result=None,\n                          error=False,\n                          early_terminated=False):\n        \"\"\"Passes the result to HyperOpt unless early terminated or errored.\n\n        The result is internally negated when interacting with HyperOpt\n        so that HyperOpt can \"maximize\" this value, as it minimizes on default.\n        \"\"\"\n        ho_trial = self._get_hyperopt_trial(trial_id)\n        if ho_trial is None:\n            return\n        ho_trial[\"refresh_time\"] = hpo.utils.coarse_utcnow()\n        if error:\n            ho_trial[\"state\"] = hpo.base.JOB_STATE_ERROR\n            ho_trial[\"misc\"][\"error\"] = (str(TuneError), \"Tune Error\")\n        elif early_terminated:\n            ho_trial[\"state\"] = hpo.base.JOB_STATE_ERROR\n            ho_trial[\"misc\"][\"error\"] = (str(TuneError), \"Tune Removed\")\n        else:\n            ho_trial[\"state\"] = hpo.base.JOB_STATE_DONE\n            hp_result = self._to_hyperopt_result(result)\n            ho_trial[\"result\"] = hp_result\n        self._hpopt_trials.refresh()\n        del self._live_trial_mapping[trial_id]", "response": "Passes the result to HyperOpt unless early terminated or errored."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plasma_prefetch(object_id):\n    local_sched_client = ray.worker.global_worker.raylet_client\n    ray_obj_id = ray.ObjectID(object_id)\n    local_sched_client.fetch_or_reconstruct([ray_obj_id], True)", "response": "Tells plasma to prefetch the given object_id."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget an object from plasma without going through object table.", "response": "def plasma_get(object_id):\n    \"\"\"Get an object directly from plasma without going through object table.\n\n    Precondition: plasma_prefetch(object_id) has been called before.\n    \"\"\"\n    client = ray.worker.global_worker.plasma_client\n    plasma_id = ray.pyarrow.plasma.ObjectID(object_id)\n    while not client.contains(plasma_id):\n        pass\n    return client.get(plasma_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nrestores the state of the batched queue for writing.", "response": "def enable_writes(self):\n        \"\"\"Restores the state of the batched queue for writing.\"\"\"\n        self.write_buffer = []\n        self.flush_lock = threading.RLock()\n        self.flush_thread = FlushThread(self.max_batch_time,\n                                        self._flush_writes)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _wait_for_reader(self):\n        if self.max_size <= 0:  # Unlimited queue\n            return\n        if self.write_item_offset - self.cached_remote_offset <= self.max_size:\n            return  # Hasn't reached max size\n        remote_offset = internal_kv._internal_kv_get(self.read_ack_key)\n        if remote_offset is None:\n            # logger.debug(\"[writer] Waiting for reader to start...\")\n            while remote_offset is None:\n                time.sleep(0.01)\n                remote_offset = internal_kv._internal_kv_get(self.read_ack_key)\n        remote_offset = int(remote_offset)\n        if self.write_item_offset - remote_offset > self.max_size:\n            logger.debug(\n                \"[writer] Waiting for reader to catch up {} to {} - {}\".format(\n                    remote_offset, self.write_item_offset, self.max_size))\n            while self.write_item_offset - remote_offset > self.max_size:\n                time.sleep(0.01)\n                remote_offset = int(\n                    internal_kv._internal_kv_get(self.read_ack_key))\n        self.cached_remote_offset = remote_offset", "response": "Checks for backpressure by the downstream reader."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef collect_samples(agents, sample_batch_size, num_envs_per_worker,\n                    train_batch_size):\n    \"\"\"Collects at least train_batch_size samples, never discarding any.\"\"\"\n\n    num_timesteps_so_far = 0\n    trajectories = []\n    agent_dict = {}\n\n    for agent in agents:\n        fut_sample = agent.sample.remote()\n        agent_dict[fut_sample] = agent\n\n    while agent_dict:\n        [fut_sample], _ = ray.wait(list(agent_dict))\n        agent = agent_dict.pop(fut_sample)\n        next_sample = ray_get_and_free(fut_sample)\n        assert next_sample.count >= sample_batch_size * num_envs_per_worker\n        num_timesteps_so_far += next_sample.count\n        trajectories.append(next_sample)\n\n        # Only launch more tasks if we don't already have enough pending\n        pending = len(agent_dict) * sample_batch_size * num_envs_per_worker\n        if num_timesteps_so_far + pending < train_batch_size:\n            fut_sample2 = agent.sample.remote()\n            agent_dict[fut_sample2] = agent\n\n    return SampleBatch.concat_samples(trajectories)", "response": "Collect samples from agents and return a SampleBatch."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncollecting samples from agents and return a new sample batch.", "response": "def collect_samples_straggler_mitigation(agents, train_batch_size):\n    \"\"\"Collects at least train_batch_size samples.\n\n    This is the legacy behavior as of 0.6, and launches extra sample tasks to\n    potentially improve performance but can result in many wasted samples.\n    \"\"\"\n\n    num_timesteps_so_far = 0\n    trajectories = []\n    agent_dict = {}\n\n    for agent in agents:\n        fut_sample = agent.sample.remote()\n        agent_dict[fut_sample] = agent\n\n    while num_timesteps_so_far < train_batch_size:\n        # TODO(pcm): Make wait support arbitrary iterators and remove the\n        # conversion to list here.\n        [fut_sample], _ = ray.wait(list(agent_dict))\n        agent = agent_dict.pop(fut_sample)\n        # Start task with next trajectory and record it in the dictionary.\n        fut_sample2 = agent.sample.remote()\n        agent_dict[fut_sample2] = agent\n\n        next_sample = ray_get_and_free(fut_sample)\n        num_timesteps_so_far += next_sample.count\n        trajectories.append(next_sample)\n\n    logger.info(\"Discarding {} sample tasks\".format(len(agent_dict)))\n    return SampleBatch.concat_samples(trajectories)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nimproving the formatting of an exception thrown by a remote function.", "response": "def format_error_message(exception_message, task_exception=False):\n    \"\"\"Improve the formatting of an exception thrown by a remote function.\n\n    This method takes a traceback from an exception and makes it nicer by\n    removing a few uninformative lines and adding some space to indent the\n    remaining lines nicely.\n\n    Args:\n        exception_message (str): A message generated by traceback.format_exc().\n\n    Returns:\n        A string of the formatted exception message.\n    \"\"\"\n    lines = exception_message.split(\"\\n\")\n    if task_exception:\n        # For errors that occur inside of tasks, remove lines 1 and 2 which are\n        # always the same, they just contain information about the worker code.\n        lines = lines[0:1] + lines[3:]\n        pass\n    return \"\\n\".join(lines)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef push_error_to_driver(worker, error_type, message, driver_id=None):\n    if driver_id is None:\n        driver_id = ray.DriverID.nil()\n    worker.raylet_client.push_error(driver_id, error_type, message,\n                                    time.time())", "response": "Push an error message to the background of the specified driver."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npushes an error message to the driver through Redis.", "response": "def push_error_to_driver_through_redis(redis_client,\n                                       error_type,\n                                       message,\n                                       driver_id=None):\n    \"\"\"Push an error message to the driver to be printed in the background.\n\n    Normally the push_error_to_driver function should be used. However, in some\n    instances, the raylet client is not available, e.g., because the\n    error happens in Python before the driver or worker has connected to the\n    backend processes.\n\n    Args:\n        redis_client: The redis client to use.\n        error_type (str): The type of the error.\n        message (str): The message that will be printed in the background\n            on the driver.\n        driver_id: The ID of the driver to push the error message to. If this\n            is None, then the message will be pushed to all drivers.\n    \"\"\"\n    if driver_id is None:\n        driver_id = ray.DriverID.nil()\n    # Do everything in Python and through the Python Redis client instead\n    # of through the raylet.\n    error_data = ray.gcs_utils.construct_error_message(driver_id, error_type,\n                                                       message, time.time())\n    redis_client.execute_command(\"RAY.TABLE_APPEND\",\n                                 ray.gcs_utils.TablePrefix.ERROR_INFO,\n                                 ray.gcs_utils.TablePubsub.ERROR_INFO,\n                                 driver_id.binary(), error_data)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_cython(obj):\n\n    # TODO(suo): We could split these into two functions, one for Cython\n    # functions and another for Cython methods.\n    # TODO(suo): There doesn't appear to be a Cython function 'type' we can\n    # check against via isinstance. Please correct me if I'm wrong.\n    def check_cython(x):\n        return type(x).__name__ == \"cython_function_or_method\"\n\n    # Check if function or method, respectively\n    return check_cython(obj) or \\\n        (hasattr(obj, \"__func__\") and check_cython(obj.__func__))", "response": "Check if an object is a Cython function or method."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if an object is a function or method.", "response": "def is_function_or_method(obj):\n    \"\"\"Check if an object is a function or method.\n\n    Args:\n        obj: The Python object in question.\n\n    Returns:\n        True if the object is an function or method.\n    \"\"\"\n    return inspect.isfunction(obj) or inspect.ismethod(obj) or is_cython(obj)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef random_string():\n    # Get the state of the numpy random number generator.\n    numpy_state = np.random.get_state()\n    # Try to use true randomness.\n    np.random.seed(None)\n    # Generate the random ID.\n    random_id = np.random.bytes(ray_constants.ID_SIZE)\n    # Reset the state of the numpy random number generator.\n    np.random.set_state(numpy_state)\n    return random_id", "response": "Generate a random string to use as an ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmakes this unicode in Python 2 otherwise leave it as bytes.", "response": "def decode(byte_str, allow_none=False):\n    \"\"\"Make this unicode in Python 3, otherwise leave it as bytes.\n\n    Args:\n        byte_str: The byte string to decode.\n        allow_none: If true, then we will allow byte_str to be None in which\n            case we will return an empty string. TODO(rkn): Remove this flag.\n            This is only here to simplify upgrading to flatbuffers 1.10.0.\n\n    Returns:\n        A byte string in Python 2 and a unicode string in Python 3.\n    \"\"\"\n    if byte_str is None and allow_none:\n        return \"\"\n\n    if not isinstance(byte_str, bytes):\n        raise ValueError(\n            \"The argument {} must be a bytes object.\".format(byte_str))\n    if sys.version_info >= (3, 0):\n        return byte_str.decode(\"ascii\")\n    else:\n        return byte_str"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncoercing *s* to str.", "response": "def ensure_str(s, encoding=\"utf-8\", errors=\"strict\"):\n    \"\"\"Coerce *s* to `str`.\n\n    To keep six with lower version, see Issue 4169, we copy this function\n    from six == 1.12.0.\n\n    TODO(yuhguo): remove this function when six >= 1.12.0.\n\n    For Python 2:\n      - `unicode` -> encoded to `str`\n      - `str` -> `str`\n\n    For Python 3:\n      - `str` -> `str`\n      - `bytes` -> decoded to `str`\n    \"\"\"\n    if six.PY3:\n        text_type = str\n        binary_type = bytes\n    else:\n        text_type = unicode  # noqa: F821\n        binary_type = str\n    if not isinstance(s, (text_type, binary_type)):\n        raise TypeError(\"not expecting type '%s'\" % type(s))\n    if six.PY2 and isinstance(s, text_type):\n        s = s.encode(encoding, errors)\n    elif six.PY3 and isinstance(s, binary_type):\n        s = s.decode(encoding, errors)\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_cuda_visible_devices():\n    gpu_ids_str = os.environ.get(\"CUDA_VISIBLE_DEVICES\", None)\n\n    if gpu_ids_str is None:\n        return None\n\n    if gpu_ids_str == \"\":\n        return []\n\n    return [int(i) for i in gpu_ids_str.split(\",\")]", "response": "Get the device IDs in the CUDA_VISIBLE_DEVICES environment variable."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef resources_from_resource_arguments(default_num_cpus, default_num_gpus,\n                                      default_resources, runtime_num_cpus,\n                                      runtime_num_gpus, runtime_resources):\n    \"\"\"Determine a task's resource requirements.\n\n    Args:\n        default_num_cpus: The default number of CPUs required by this function\n            or actor method.\n        default_num_gpus: The default number of GPUs required by this function\n            or actor method.\n        default_resources: The default custom resources required by this\n            function or actor method.\n        runtime_num_cpus: The number of CPUs requested when the task was\n            invoked.\n        runtime_num_gpus: The number of GPUs requested when the task was\n            invoked.\n        runtime_resources: The custom resources requested when the task was\n            invoked.\n\n    Returns:\n        A dictionary of the resource requirements for the task.\n    \"\"\"\n    if runtime_resources is not None:\n        resources = runtime_resources.copy()\n    elif default_resources is not None:\n        resources = default_resources.copy()\n    else:\n        resources = {}\n\n    if \"CPU\" in resources or \"GPU\" in resources:\n        raise ValueError(\"The resources dictionary must not \"\n                         \"contain the key 'CPU' or 'GPU'\")\n\n    assert default_num_cpus is not None\n    resources[\"CPU\"] = (default_num_cpus\n                        if runtime_num_cpus is None else runtime_num_cpus)\n\n    if runtime_num_gpus is not None:\n        resources[\"GPU\"] = runtime_num_gpus\n    elif default_num_gpus is not None:\n        resources[\"GPU\"] = default_num_gpus\n\n    return resources", "response": "Determine a task s resource requirements from the given arguments."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef vmstat(stat):\n    out = subprocess.check_output([\"vmstat\", \"-s\"])\n    stat = stat.encode(\"ascii\")\n    for line in out.split(b\"\\n\"):\n        line = line.strip()\n        if stat in line:\n            return int(line.split(b\" \")[0])\n    raise ValueError(\"Can't find {} in 'vmstat' output.\".format(stat))", "response": "Run vmstat and get a particular statistic."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning a sysctl command and parse the output.", "response": "def sysctl(command):\n    \"\"\"Run a sysctl command and parse the output.\n\n    Args:\n        command: A sysctl command with an argument, for example,\n            [\"sysctl\", \"hw.memsize\"].\n\n    Returns:\n        The parsed output.\n    \"\"\"\n    out = subprocess.check_output(command)\n    result = out.split(b\" \")[1]\n    try:\n        return int(result)\n    except ValueError:\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the total amount of system memory in bytes.", "response": "def get_system_memory():\n    \"\"\"Return the total amount of system memory in bytes.\n\n    Returns:\n        The total amount of system memory in bytes.\n    \"\"\"\n    # Try to accurately figure out the memory limit if we are in a docker\n    # container. Note that this file is not specific to Docker and its value is\n    # often much larger than the actual amount of memory.\n    docker_limit = None\n    memory_limit_filename = \"/sys/fs/cgroup/memory/memory.limit_in_bytes\"\n    if os.path.exists(memory_limit_filename):\n        with open(memory_limit_filename, \"r\") as f:\n            docker_limit = int(f.read())\n\n    # Use psutil if it is available.\n    psutil_memory_in_bytes = None\n    try:\n        import psutil\n        psutil_memory_in_bytes = psutil.virtual_memory().total\n    except ImportError:\n        pass\n\n    if psutil_memory_in_bytes is not None:\n        memory_in_bytes = psutil_memory_in_bytes\n    elif sys.platform == \"linux\" or sys.platform == \"linux2\":\n        # Handle Linux.\n        bytes_in_kilobyte = 1024\n        memory_in_bytes = vmstat(\"total memory\") * bytes_in_kilobyte\n    else:\n        # Handle MacOS.\n        memory_in_bytes = sysctl([\"sysctl\", \"hw.memsize\"])\n\n    if docker_limit is not None:\n        return min(docker_limit, memory_in_bytes)\n    else:\n        return memory_in_bytes"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_shared_memory_bytes():\n    # Make sure this is only called on Linux.\n    assert sys.platform == \"linux\" or sys.platform == \"linux2\"\n\n    shm_fd = os.open(\"/dev/shm\", os.O_RDONLY)\n    try:\n        shm_fs_stats = os.fstatvfs(shm_fd)\n        # The value shm_fs_stats.f_bsize is the block size and the\n        # value shm_fs_stats.f_bavail is the number of available\n        # blocks.\n        shm_avail = shm_fs_stats.f_bsize * shm_fs_stats.f_bavail\n    finally:\n        os.close(shm_fd)\n\n    return shm_avail", "response": "Get the size of the shared memory file system in bytes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsend a warning message if the pickled object is too large.", "response": "def check_oversized_pickle(pickled, name, obj_type, worker):\n    \"\"\"Send a warning message if the pickled object is too large.\n\n    Args:\n        pickled: the pickled object.\n        name: name of the pickled object.\n        obj_type: type of the pickled object, can be 'function',\n            'remote function', 'actor', or 'object'.\n        worker: the worker used to send warning message.\n    \"\"\"\n    length = len(pickled)\n    if length <= ray_constants.PICKLE_OBJECT_WARNING_SIZE:\n        return\n    warning_message = (\n        \"Warning: The {} {} has size {} when pickled. \"\n        \"It will be stored in Redis, which could cause memory issues. \"\n        \"This may mean that its definition uses a large array or other object.\"\n    ).format(obj_type, name, length)\n    push_error_to_driver(\n        worker,\n        ray_constants.PICKLING_LARGE_OBJECT_PUSH_ERROR,\n        warning_message,\n        driver_id=worker.task_driver_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a thread - safe proxy for the given client.", "response": "def thread_safe_client(client, lock=None):\n    \"\"\"Create a thread-safe proxy which locks every method call\n    for the given client.\n\n    Args:\n        client: the client object to be guarded.\n        lock: the lock object that will be used to lock client's methods.\n            If None, a new lock will be used.\n\n    Returns:\n        A thread-safe proxy for the given client.\n    \"\"\"\n    if lock is None:\n        lock = threading.Lock()\n    return _ThreadSafeProxy(client, lock)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef try_to_create_directory(directory_path):\n    logger = logging.getLogger(\"ray\")\n    directory_path = os.path.expanduser(directory_path)\n    if not os.path.exists(directory_path):\n        try:\n            os.makedirs(directory_path)\n        except OSError as e:\n            if e.errno != errno.EEXIST:\n                raise e\n            logger.warning(\n                \"Attempted to create '{}', but the directory already \"\n                \"exists.\".format(directory_path))\n        # Change the log directory permissions so others can use it. This is\n        # important when multiple people are using the same machine.\n    try:\n        os.chmod(directory_path, 0o0777)\n    except OSError as e:\n        # Silently suppress the PermissionError that is thrown by the chmod.\n        # This is done because the user attempting to change the permissions\n        # on a directory may not own it. The chmod is attempted whether the\n        # directory is new or not to avoid race conditions.\n        # ray-project/ray/#3591\n        if e.errno in [errno.EACCES, errno.EPERM]:\n            pass\n        else:\n            raise", "response": "Attempt to create a directory that is globally readable or writable."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef assemble(self):\n        first_block = ray.get(self.objectids[(0, ) * self.ndim])\n        dtype = first_block.dtype\n        result = np.zeros(self.shape, dtype=dtype)\n        for index in np.ndindex(*self.num_blocks):\n            lower = DistArray.compute_block_lower(index, self.shape)\n            upper = DistArray.compute_block_upper(index, self.shape)\n            result[[slice(l, u) for (l, u) in zip(lower, upper)]] = ray.get(\n                self.objectids[index])\n        return result", "response": "Assemble an array from a distributed array of object IDs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute action log - probabilities from policy logits and actions.", "response": "def multi_log_probs_from_logits_and_actions(policy_logits, actions):\n    \"\"\"Computes action log-probs from policy logits and actions.\n\n  In the notation used throughout documentation and comments, T refers to the\n  time dimension ranging from 0 to T-1. B refers to the batch size and\n  ACTION_SPACE refers to the list of numbers each representing a number of\n  actions.\n\n  Args:\n    policy_logits: A list with length of ACTION_SPACE of float32\n      tensors of shapes\n      [T, B, ACTION_SPACE[0]],\n      ...,\n      [T, B, ACTION_SPACE[-1]]\n      with un-normalized log-probabilities parameterizing a softmax policy.\n    actions: A list with length of ACTION_SPACE of int32\n      tensors of shapes\n      [T, B],\n      ...,\n      [T, B]\n      with actions.\n\n  Returns:\n    A list with length of ACTION_SPACE of float32\n      tensors of shapes\n      [T, B],\n      ...,\n      [T, B]\n      corresponding to the sampling log probability\n      of the chosen action w.r.t. the policy.\n  \"\"\"\n\n    log_probs = []\n    for i in range(len(policy_logits)):\n        log_probs.append(-tf.nn.sparse_softmax_cross_entropy_with_logits(\n            logits=policy_logits[i], labels=actions[i]))\n\n    return log_probs"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_logits(behaviour_policy_logits,\n                target_policy_logits,\n                actions,\n                discounts,\n                rewards,\n                values,\n                bootstrap_value,\n                clip_rho_threshold=1.0,\n                clip_pg_rho_threshold=1.0,\n                name=\"vtrace_from_logits\"):\n    \"\"\"multi_from_logits wrapper used only for tests\"\"\"\n\n    res = multi_from_logits(\n        [behaviour_policy_logits], [target_policy_logits], [actions],\n        discounts,\n        rewards,\n        values,\n        bootstrap_value,\n        clip_rho_threshold=clip_rho_threshold,\n        clip_pg_rho_threshold=clip_pg_rho_threshold,\n        name=name)\n\n    return VTraceFromLogitsReturns(\n        vs=res.vs,\n        pg_advantages=res.pg_advantages,\n        log_rhos=res.log_rhos,\n        behaviour_action_log_probs=tf.squeeze(\n            res.behaviour_action_log_probs, axis=0),\n        target_action_log_probs=tf.squeeze(\n            res.target_action_log_probs, axis=0),\n    )", "response": "wrapper used only for tests"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_importance_weights(log_rhos,\n                            discounts,\n                            rewards,\n                            values,\n                            bootstrap_value,\n                            clip_rho_threshold=1.0,\n                            clip_pg_rho_threshold=1.0,\n                            name=\"vtrace_from_importance_weights\"):\n    r\"\"\"V-trace from log importance weights.\n\n  Calculates V-trace actor critic targets as described in\n\n  \"IMPALA: Scalable Distributed Deep-RL with\n  Importance Weighted Actor-Learner Architectures\"\n  by Espeholt, Soyer, Munos et al.\n\n  In the notation used throughout documentation and comments, T refers to the\n  time dimension ranging from 0 to T-1. B refers to the batch size. This code\n  also supports the case where all tensors have the same number of additional\n  dimensions, e.g., `rewards` is [T, B, C], `values` is [T, B, C],\n  `bootstrap_value` is [B, C].\n\n  Args:\n    log_rhos: A float32 tensor of shape [T, B] representing the\n      log importance sampling weights, i.e.\n      log(target_policy(a) / behaviour_policy(a)). V-trace performs operations\n      on rhos in log-space for numerical stability.\n    discounts: A float32 tensor of shape [T, B] with discounts encountered when\n      following the behaviour policy.\n    rewards: A float32 tensor of shape [T, B] containing rewards generated by\n      following the behaviour policy.\n    values: A float32 tensor of shape [T, B] with the value function estimates\n      wrt. the target policy.\n    bootstrap_value: A float32 of shape [B] with the value function estimate at\n      time T.\n    clip_rho_threshold: A scalar float32 tensor with the clipping threshold for\n      importance weights (rho) when calculating the baseline targets (vs).\n      rho^bar in the paper. If None, no clipping is applied.\n    clip_pg_rho_threshold: A scalar float32 tensor with the clipping threshold\n      on rho_s in \\rho_s \\delta log \\pi(a|x) (r + \\gamma v_{s+1} - V(x_s)). If\n      None, no clipping is applied.\n    name: The name scope that all V-trace operations will be created in.\n\n  Returns:\n    A VTraceReturns namedtuple (vs, pg_advantages) where:\n      vs: A float32 tensor of shape [T, B]. Can be used as target to\n        train a baseline (V(x_t) - vs_t)^2.\n      pg_advantages: A float32 tensor of shape [T, B]. Can be used as the\n        advantage in the calculation of policy gradients.\n  \"\"\"\n    log_rhos = tf.convert_to_tensor(log_rhos, dtype=tf.float32)\n    discounts = tf.convert_to_tensor(discounts, dtype=tf.float32)\n    rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n    values = tf.convert_to_tensor(values, dtype=tf.float32)\n    bootstrap_value = tf.convert_to_tensor(bootstrap_value, dtype=tf.float32)\n    if clip_rho_threshold is not None:\n        clip_rho_threshold = tf.convert_to_tensor(\n            clip_rho_threshold, dtype=tf.float32)\n    if clip_pg_rho_threshold is not None:\n        clip_pg_rho_threshold = tf.convert_to_tensor(\n            clip_pg_rho_threshold, dtype=tf.float32)\n\n    # Make sure tensor ranks are consistent.\n    rho_rank = log_rhos.shape.ndims  # Usually 2.\n    values.shape.assert_has_rank(rho_rank)\n    bootstrap_value.shape.assert_has_rank(rho_rank - 1)\n    discounts.shape.assert_has_rank(rho_rank)\n    rewards.shape.assert_has_rank(rho_rank)\n    if clip_rho_threshold is not None:\n        clip_rho_threshold.shape.assert_has_rank(0)\n    if clip_pg_rho_threshold is not None:\n        clip_pg_rho_threshold.shape.assert_has_rank(0)\n\n    with tf.name_scope(\n            name,\n            values=[log_rhos, discounts, rewards, values, bootstrap_value]):\n        rhos = tf.exp(log_rhos)\n        if clip_rho_threshold is not None:\n            clipped_rhos = tf.minimum(\n                clip_rho_threshold, rhos, name=\"clipped_rhos\")\n\n            tf.summary.histogram(\"clipped_rhos_1000\", tf.minimum(1000.0, rhos))\n            tf.summary.scalar(\n                \"num_of_clipped_rhos\",\n                tf.reduce_sum(\n                    tf.cast(\n                        tf.equal(clipped_rhos, clip_rho_threshold), tf.int32)))\n            tf.summary.scalar(\"size_of_clipped_rhos\", tf.size(clipped_rhos))\n        else:\n            clipped_rhos = rhos\n\n        cs = tf.minimum(1.0, rhos, name=\"cs\")\n        # Append bootstrapped value to get [v1, ..., v_t+1]\n        values_t_plus_1 = tf.concat(\n            [values[1:], tf.expand_dims(bootstrap_value, 0)], axis=0)\n        deltas = clipped_rhos * (\n            rewards + discounts * values_t_plus_1 - values)\n\n        # All sequences are reversed, computation starts from the back.\n        sequences = (\n            tf.reverse(discounts, axis=[0]),\n            tf.reverse(cs, axis=[0]),\n            tf.reverse(deltas, axis=[0]),\n        )\n\n        # V-trace vs are calculated through a scan from the back to the\n        # beginning of the given trajectory.\n        def scanfunc(acc, sequence_item):\n            discount_t, c_t, delta_t = sequence_item\n            return delta_t + discount_t * c_t * acc\n\n        initial_values = tf.zeros_like(bootstrap_value)\n        vs_minus_v_xs = tf.scan(\n            fn=scanfunc,\n            elems=sequences,\n            initializer=initial_values,\n            parallel_iterations=1,\n            back_prop=False,\n            name=\"scan\")\n        # Reverse the results back to original order.\n        vs_minus_v_xs = tf.reverse(vs_minus_v_xs, [0], name=\"vs_minus_v_xs\")\n\n        # Add V(x_s) to get v_s.\n        vs = tf.add(vs_minus_v_xs, values, name=\"vs\")\n\n        # Advantage for policy gradient.\n        vs_t_plus_1 = tf.concat(\n            [vs[1:], tf.expand_dims(bootstrap_value, 0)], axis=0)\n        if clip_pg_rho_threshold is not None:\n            clipped_pg_rhos = tf.minimum(\n                clip_pg_rho_threshold, rhos, name=\"clipped_pg_rhos\")\n        else:\n            clipped_pg_rhos = rhos\n        pg_advantages = (\n            clipped_pg_rhos * (rewards + discounts * vs_t_plus_1 - values))\n\n        # Make sure no gradients backpropagated through the returned values.\n        return VTraceReturns(\n            vs=tf.stop_gradient(vs),\n            pg_advantages=tf.stop_gradient(pg_advantages))", "response": "r Function to calculate the V - trace actor critic targets from the given log importance weights."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_log_rhos(target_action_log_probs, behaviour_action_log_probs):\n    t = tf.stack(target_action_log_probs)\n    b = tf.stack(behaviour_action_log_probs)\n    log_rhos = tf.reduce_sum(t - b, axis=0)\n    return log_rhos", "response": "With the selected log_probs for multi - discrete actions of behaviour\n    and target policies we compute the log_rhos for calculating the vtrace."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef bias_variable(shape):\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)", "response": "bias_variable generates a bias variable of a given shape."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprinting output of given dataframe to fit into terminal size.", "response": "def print_format_output(dataframe):\n    \"\"\"Prints output of given dataframe to fit into terminal.\n\n    Returns:\n        table (pd.DataFrame): Final outputted dataframe.\n        dropped_cols (list): Columns dropped due to terminal size.\n        empty_cols (list): Empty columns (dropped on default).\n    \"\"\"\n    print_df = pd.DataFrame()\n    dropped_cols = []\n    empty_cols = []\n    # column display priority is based on the info_keys passed in\n    for i, col in enumerate(dataframe):\n        if dataframe[col].isnull().all():\n            # Don't add col to print_df if is fully empty\n            empty_cols += [col]\n            continue\n\n        print_df[col] = dataframe[col]\n        test_table = tabulate(print_df, headers=\"keys\", tablefmt=\"psql\")\n        if str(test_table).index(\"\\n\") > TERM_WIDTH:\n            # Drop all columns beyond terminal width\n            print_df.drop(col, axis=1, inplace=True)\n            dropped_cols += list(dataframe.columns)[i:]\n            break\n\n    table = tabulate(\n        print_df, headers=\"keys\", tablefmt=\"psql\", showindex=\"never\")\n\n    print(table)\n    if dropped_cols:\n        print(\"Dropped columns:\", dropped_cols)\n        print(\"Please increase your terminal size to view remaining columns.\")\n    if empty_cols:\n        print(\"Empty columns:\", empty_cols)\n\n    return table, dropped_cols, empty_cols"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef list_trials(experiment_path,\n                sort=None,\n                output=None,\n                filter_op=None,\n                info_keys=None,\n                result_keys=None):\n    \"\"\"Lists trials in the directory subtree starting at the given path.\n\n    Args:\n        experiment_path (str): Directory where trials are located.\n            Corresponds to Experiment.local_dir/Experiment.name.\n        sort (str): Key to sort by.\n        output (str): Name of file where output is saved.\n        filter_op (str): Filter operation in the format\n            \"<column> <operator> <value>\".\n        info_keys (list): Keys that are displayed.\n        result_keys (list): Keys of last result that are displayed.\n    \"\"\"\n    _check_tabulate()\n    experiment_state = _get_experiment_state(\n        experiment_path, exit_on_fail=True)\n\n    checkpoint_dicts = experiment_state[\"checkpoints\"]\n    checkpoint_dicts = [flatten_dict(g) for g in checkpoint_dicts]\n    checkpoints_df = pd.DataFrame(checkpoint_dicts)\n\n    if not info_keys:\n        info_keys = DEFAULT_EXPERIMENT_INFO_KEYS\n    if not result_keys:\n        result_keys = DEFAULT_RESULT_KEYS\n    result_keys = [\"last_result:{}\".format(k) for k in result_keys]\n    col_keys = [\n        k for k in list(info_keys) + result_keys if k in checkpoints_df\n    ]\n    checkpoints_df = checkpoints_df[col_keys]\n\n    if \"last_update_time\" in checkpoints_df:\n        with pd.option_context(\"mode.use_inf_as_null\", True):\n            datetime_series = checkpoints_df[\"last_update_time\"].dropna()\n\n        datetime_series = datetime_series.apply(\n            lambda t: datetime.fromtimestamp(t).strftime(TIMESTAMP_FORMAT))\n        checkpoints_df[\"last_update_time\"] = datetime_series\n\n    if \"logdir\" in checkpoints_df:\n        # logdir often too verbose to view in table, so drop experiment_path\n        checkpoints_df[\"logdir\"] = checkpoints_df[\"logdir\"].str.replace(\n            experiment_path, \"\")\n\n    if filter_op:\n        col, op, val = filter_op.split(\" \")\n        col_type = checkpoints_df[col].dtype\n        if is_numeric_dtype(col_type):\n            val = float(val)\n        elif is_string_dtype(col_type):\n            val = str(val)\n        # TODO(Andrew): add support for datetime and boolean\n        else:\n            raise ValueError(\"Unsupported dtype for \\\"{}\\\": {}\".format(\n                val, col_type))\n        op = OPERATORS[op]\n        filtered_index = op(checkpoints_df[col], val)\n        checkpoints_df = checkpoints_df[filtered_index]\n\n    if sort:\n        if sort not in checkpoints_df:\n            raise KeyError(\"Sort Index \\\"{}\\\" not in: {}\".format(\n                sort, list(checkpoints_df)))\n        checkpoints_df = checkpoints_df.sort_values(by=sort)\n\n    print_format_output(checkpoints_df)\n\n    if output:\n        file_extension = os.path.splitext(output)[1].lower()\n        if file_extension in (\".p\", \".pkl\", \".pickle\"):\n            checkpoints_df.to_pickle(output)\n        elif file_extension == \".csv\":\n            checkpoints_df.to_csv(output, index=False)\n        else:\n            raise ValueError(\"Unsupported filetype: {}\".format(output))\n        print(\"Output saved at:\", output)", "response": "Lists all trials in the given directory subtree."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of all experiments in the project_path.", "response": "def list_experiments(project_path,\n                     sort=None,\n                     output=None,\n                     filter_op=None,\n                     info_keys=None):\n    \"\"\"Lists experiments in the directory subtree.\n\n    Args:\n        project_path (str): Directory where experiments are located.\n            Corresponds to Experiment.local_dir.\n        sort (str): Key to sort by.\n        output (str): Name of file where output is saved.\n        filter_op (str): Filter operation in the format\n            \"<column> <operator> <value>\".\n        info_keys (list): Keys that are displayed.\n    \"\"\"\n    _check_tabulate()\n    base, experiment_folders, _ = next(os.walk(project_path))\n\n    experiment_data_collection = []\n\n    for experiment_dir in experiment_folders:\n        experiment_state = _get_experiment_state(\n            os.path.join(base, experiment_dir))\n        if not experiment_state:\n            logger.debug(\"No experiment state found in %s\", experiment_dir)\n            continue\n\n        checkpoints = pd.DataFrame(experiment_state[\"checkpoints\"])\n        runner_data = experiment_state[\"runner_data\"]\n\n        # Format time-based values.\n        time_values = {\n            \"start_time\": runner_data.get(\"_start_time\"),\n            \"last_updated\": experiment_state.get(\"timestamp\"),\n        }\n\n        formatted_time_values = {\n            key: datetime.fromtimestamp(val).strftime(TIMESTAMP_FORMAT)\n            if val else None\n            for key, val in time_values.items()\n        }\n\n        experiment_data = {\n            \"name\": experiment_dir,\n            \"total_trials\": checkpoints.shape[0],\n            \"running_trials\": (checkpoints[\"status\"] == Trial.RUNNING).sum(),\n            \"terminated_trials\": (\n                checkpoints[\"status\"] == Trial.TERMINATED).sum(),\n            \"error_trials\": (checkpoints[\"status\"] == Trial.ERROR).sum(),\n        }\n        experiment_data.update(formatted_time_values)\n        experiment_data_collection.append(experiment_data)\n\n    if not experiment_data_collection:\n        print(\"No experiments found!\")\n        sys.exit(0)\n\n    info_df = pd.DataFrame(experiment_data_collection)\n    if not info_keys:\n        info_keys = DEFAULT_PROJECT_INFO_KEYS\n    col_keys = [k for k in list(info_keys) if k in info_df]\n    if not col_keys:\n        print(\"None of keys {} in experiment data!\".format(info_keys))\n        sys.exit(0)\n    info_df = info_df[col_keys]\n\n    if filter_op:\n        col, op, val = filter_op.split(\" \")\n        col_type = info_df[col].dtype\n        if is_numeric_dtype(col_type):\n            val = float(val)\n        elif is_string_dtype(col_type):\n            val = str(val)\n        # TODO(Andrew): add support for datetime and boolean\n        else:\n            raise ValueError(\"Unsupported dtype for \\\"{}\\\": {}\".format(\n                val, col_type))\n        op = OPERATORS[op]\n        filtered_index = op(info_df[col], val)\n        info_df = info_df[filtered_index]\n\n    if sort:\n        if sort not in info_df:\n            raise KeyError(\"Sort Index \\\"{}\\\" not in: {}\".format(\n                sort, list(info_df)))\n        info_df = info_df.sort_values(by=sort)\n\n    print_format_output(info_df)\n\n    if output:\n        file_extension = os.path.splitext(output)[1].lower()\n        if file_extension in (\".p\", \".pkl\", \".pickle\"):\n            info_df.to_pickle(output)\n        elif file_extension == \".csv\":\n            info_df.to_csv(output, index=False)\n        else:\n            raise ValueError(\"Unsupported filetype: {}\".format(output))\n        print(\"Output saved at:\", output)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nopen a txt file at the given path where user can add and save notes.", "response": "def add_note(path, filename=\"note.txt\"):\n    \"\"\"Opens a txt file at the given path where user can add and save notes.\n\n    Args:\n        path (str): Directory where note will be saved.\n        filename (str): Name of note. Defaults to \"note.txt\"\n    \"\"\"\n    path = os.path.expanduser(path)\n    assert os.path.isdir(path), \"{} is not a valid directory.\".format(path)\n\n    filepath = os.path.join(path, filename)\n    exists = os.path.isfile(filepath)\n\n    try:\n        subprocess.call([EDITOR, filepath])\n    except Exception as exc:\n        logger.error(\"Editing note failed!\")\n        raise exc\n    if exists:\n        print(\"Note updated at:\", filepath)\n    else:\n        print(\"Note created at:\", filepath)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nresting API to query the job info with the given job_id.", "response": "def query_job(request):\n    \"\"\"Rest API to query the job info, with the given job_id.\n\n    The url pattern should be like this:\n\n    curl http://<server>:<port>/query_job?job_id=<job_id>\n\n    The response may be:\n\n    {\n        \"running_trials\": 0,\n        \"start_time\": \"2018-07-19 20:49:40\",\n        \"current_round\": 1,\n        \"failed_trials\": 0,\n        \"best_trial_id\": \"2067R2ZD\",\n        \"name\": \"asynchyperband_test\",\n        \"job_id\": \"asynchyperband_test\",\n        \"user\": \"Grady\",\n        \"type\": \"RAY TUNE\",\n        \"total_trials\": 4,\n        \"end_time\": \"2018-07-19 20:50:10\",\n        \"progress\": 100,\n        \"success_trials\": 4\n    }\n    \"\"\"\n    job_id = request.GET.get(\"job_id\")\n    jobs = JobRecord.objects.filter(job_id=job_id)\n    trials = TrialRecord.objects.filter(job_id=job_id)\n\n    total_num = len(trials)\n    running_num = sum(t.trial_status == Trial.RUNNING for t in trials)\n    success_num = sum(t.trial_status == Trial.TERMINATED for t in trials)\n    failed_num = sum(t.trial_status == Trial.ERROR for t in trials)\n    if total_num == 0:\n        progress = 0\n    else:\n        progress = int(float(success_num) / total_num * 100)\n\n    if len(jobs) == 0:\n        resp = \"Unkonwn job id %s.\\n\" % job_id\n    else:\n        job = jobs[0]\n        result = {\n            \"job_id\": job.job_id,\n            \"name\": job.name,\n            \"user\": job.user,\n            \"type\": job.type,\n            \"start_time\": job.start_time,\n            \"end_time\": job.end_time,\n            \"success_trials\": success_num,\n            \"failed_trials\": failed_num,\n            \"running_trials\": running_num,\n            \"total_trials\": total_num,\n            \"best_trial_id\": job.best_trial_id,\n            \"progress\": progress\n        }\n        resp = json.dumps(result)\n    return HttpResponse(resp, content_type=\"application/json;charset=utf-8\")"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef query_trial(request):\n    trial_id = request.GET.get(\"trial_id\")\n    trials = TrialRecord.objects \\\n        .filter(trial_id=trial_id) \\\n        .order_by(\"-start_time\")\n    if len(trials) == 0:\n        resp = \"Unkonwn trial id %s.\\n\" % trials\n    else:\n        trial = trials[0]\n        result = {\n            \"trial_id\": trial.trial_id,\n            \"job_id\": trial.job_id,\n            \"trial_status\": trial.trial_status,\n            \"start_time\": trial.start_time,\n            \"end_time\": trial.end_time,\n            \"params\": trial.params\n        }\n        resp = json.dumps(result)\n    return HttpResponse(resp, content_type=\"application/json;charset=utf-8\")", "response": "Rest API to query the trial info with the given trial_id."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef on_trial_result(self, trial_runner, trial, result):\n\n        if trial in self._stopped_trials:\n            assert not self._hard_stop\n            return TrialScheduler.CONTINUE  # fall back to FIFO\n\n        time = result[self._time_attr]\n        self._results[trial].append(result)\n        median_result = self._get_median_result(time)\n        best_result = self._best_result(trial)\n        if self._verbose:\n            logger.info(\"Trial {} best res={} vs median res={} at t={}\".format(\n                trial, best_result, median_result, time))\n        if best_result < median_result and time > self._grace_period:\n            if self._verbose:\n                logger.info(\"MedianStoppingRule: \"\n                            \"early stopping {}\".format(trial))\n            self._stopped_trials.add(trial)\n            if self._hard_stop:\n                return TrialScheduler.STOP\n            else:\n                return TrialScheduler.PAUSE\n        else:\n            return TrialScheduler.CONTINUE", "response": "Callback for early stopping."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef on_trial_remove(self, trial_runner, trial):\n        if trial.status is Trial.PAUSED and trial in self._results:\n            self._completed_trials.add(trial)", "response": "Mark trial as completed if paused and has previously ran."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding a Job instance from a json string.", "response": "def from_json(cls, json_info):\n        \"\"\"Build a Job instance from a json string.\"\"\"\n        if json_info is None:\n            return None\n        return JobRecord(\n            job_id=json_info[\"job_id\"],\n            name=json_info[\"job_name\"],\n            user=json_info[\"user\"],\n            type=json_info[\"type\"],\n            start_time=json_info[\"start_time\"])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_json(cls, json_info):\n        if json_info is None:\n            return None\n        return TrialRecord(\n            trial_id=json_info[\"trial_id\"],\n            job_id=json_info[\"job_id\"],\n            trial_status=json_info[\"status\"],\n            start_time=json_info[\"start_time\"],\n            params=json_info[\"params\"])", "response": "Build a Trial instance from a json string."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nbuild a Result instance from a json string.", "response": "def from_json(cls, json_info):\n        \"\"\"Build a Result instance from a json string.\"\"\"\n        if json_info is None:\n            return None\n        return ResultRecord(\n            trial_id=json_info[\"trial_id\"],\n            timesteps_total=json_info[\"timesteps_total\"],\n            done=json_info.get(\"done\", None),\n            episode_reward_mean=json_info.get(\"episode_reward_mean\", None),\n            mean_accuracy=json_info.get(\"mean_accuracy\", None),\n            mean_loss=json_info.get(\"mean_loss\", None),\n            trainning_iteration=json_info.get(\"training_iteration\", None),\n            timesteps_this_iter=json_info.get(\"timesteps_this_iter\", None),\n            time_this_iter_s=json_info.get(\"time_this_iter_s\", None),\n            time_total_s=json_info.get(\"time_total_s\", None),\n            date=json_info.get(\"date\", None),\n            hostname=json_info.get(\"hostname\", None),\n            node_ip=json_info.get(\"node_ip\", None),\n            config=json_info.get(\"config\", None))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef compute_advantages(rollout, last_r, gamma=0.9, lambda_=1.0, use_gae=True):\n\n    traj = {}\n    trajsize = len(rollout[SampleBatch.ACTIONS])\n    for key in rollout:\n        traj[key] = np.stack(rollout[key])\n\n    if use_gae:\n        assert SampleBatch.VF_PREDS in rollout, \"Values not found!\"\n        vpred_t = np.concatenate(\n            [rollout[SampleBatch.VF_PREDS],\n             np.array([last_r])])\n        delta_t = (\n            traj[SampleBatch.REWARDS] + gamma * vpred_t[1:] - vpred_t[:-1])\n        # This formula for the advantage comes\n        # \"Generalized Advantage Estimation\": https://arxiv.org/abs/1506.02438\n        traj[Postprocessing.ADVANTAGES] = discount(delta_t, gamma * lambda_)\n        traj[Postprocessing.VALUE_TARGETS] = (\n            traj[Postprocessing.ADVANTAGES] +\n            traj[SampleBatch.VF_PREDS]).copy().astype(np.float32)\n    else:\n        rewards_plus_v = np.concatenate(\n            [rollout[SampleBatch.REWARDS],\n             np.array([last_r])])\n        traj[Postprocessing.ADVANTAGES] = discount(rewards_plus_v, gamma)[:-1]\n        # TODO(ekl): support using a critic without GAE\n        traj[Postprocessing.VALUE_TARGETS] = np.zeros_like(\n            traj[Postprocessing.ADVANTAGES])\n\n    traj[Postprocessing.ADVANTAGES] = traj[\n        Postprocessing.ADVANTAGES].copy().astype(np.float32)\n\n    assert all(val.shape[0] == trajsize for val in traj.values()), \\\n        \"Rollout stacked incorrectly!\"\n    return SampleBatch(traj)", "response": "Given a rollout compute its value targets and the advantage."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef xray_heartbeat_batch_handler(self, unused_channel, data):\n\n        gcs_entries = ray.gcs_utils.GcsTableEntry.GetRootAsGcsTableEntry(\n            data, 0)\n        heartbeat_data = gcs_entries.Entries(0)\n\n        message = (ray.gcs_utils.HeartbeatBatchTableData.\n                   GetRootAsHeartbeatBatchTableData(heartbeat_data, 0))\n\n        for j in range(message.BatchLength()):\n            heartbeat_message = message.Batch(j)\n\n            num_resources = heartbeat_message.ResourcesAvailableLabelLength()\n            static_resources = {}\n            dynamic_resources = {}\n            for i in range(num_resources):\n                dyn = heartbeat_message.ResourcesAvailableLabel(i)\n                static = heartbeat_message.ResourcesTotalLabel(i)\n                dynamic_resources[dyn] = (\n                    heartbeat_message.ResourcesAvailableCapacity(i))\n                static_resources[static] = (\n                    heartbeat_message.ResourcesTotalCapacity(i))\n\n            # Update the load metrics for this raylet.\n            client_id = ray.utils.binary_to_hex(heartbeat_message.ClientId())\n            ip = self.raylet_id_to_ip_map.get(client_id)\n            if ip:\n                self.load_metrics.update(ip, static_resources,\n                                         dynamic_resources)\n            else:\n                logger.warning(\n                    \"Monitor: \"\n                    \"could not find ip for client {}\".format(client_id))", "response": "Handle an xray heartbeat batch message from Redis."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _xray_clean_up_entries_for_driver(self, driver_id):\n\n        xray_task_table_prefix = (\n            ray.gcs_utils.TablePrefix_RAYLET_TASK_string.encode(\"ascii\"))\n        xray_object_table_prefix = (\n            ray.gcs_utils.TablePrefix_OBJECT_string.encode(\"ascii\"))\n\n        task_table_objects = self.state.task_table()\n        driver_id_hex = binary_to_hex(driver_id)\n        driver_task_id_bins = set()\n        for task_id_hex, task_info in task_table_objects.items():\n            task_table_object = task_info[\"TaskSpec\"]\n            task_driver_id_hex = task_table_object[\"DriverID\"]\n            if driver_id_hex != task_driver_id_hex:\n                # Ignore tasks that aren't from this driver.\n                continue\n            driver_task_id_bins.add(hex_to_binary(task_id_hex))\n\n        # Get objects associated with the driver.\n        object_table_objects = self.state.object_table()\n        driver_object_id_bins = set()\n        for object_id, _ in object_table_objects.items():\n            task_id_bin = ray._raylet.compute_task_id(object_id).binary()\n            if task_id_bin in driver_task_id_bins:\n                driver_object_id_bins.add(object_id.binary())\n\n        def to_shard_index(id_bin):\n            return binary_to_object_id(id_bin).redis_shard_hash() % len(\n                self.state.redis_clients)\n\n        # Form the redis keys to delete.\n        sharded_keys = [[] for _ in range(len(self.state.redis_clients))]\n        for task_id_bin in driver_task_id_bins:\n            sharded_keys[to_shard_index(task_id_bin)].append(\n                xray_task_table_prefix + task_id_bin)\n        for object_id_bin in driver_object_id_bins:\n            sharded_keys[to_shard_index(object_id_bin)].append(\n                xray_object_table_prefix + object_id_bin)\n\n        # Remove with best effort.\n        for shard_index in range(len(sharded_keys)):\n            keys = sharded_keys[shard_index]\n            if len(keys) == 0:\n                continue\n            redis = self.state.redis_clients[shard_index]\n            num_deleted = redis.delete(*keys)\n            logger.info(\"Monitor: \"\n                        \"Removed {} dead redis entries of the \"\n                        \"driver from redis shard {}.\".format(\n                            num_deleted, shard_index))\n            if num_deleted != len(keys):\n                logger.warning(\"Monitor: \"\n                               \"Failed to remove {} relevant redis \"\n                               \"entries from redis shard {}.\".format(\n                                   len(keys) - num_deleted, shard_index))", "response": "Remove all objects and tasks belonging to this driver from redis."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef xray_driver_removed_handler(self, unused_channel, data):\n        gcs_entries = ray.gcs_utils.GcsTableEntry.GetRootAsGcsTableEntry(\n            data, 0)\n        driver_data = gcs_entries.Entries(0)\n        message = ray.gcs_utils.DriverTableData.GetRootAsDriverTableData(\n            driver_data, 0)\n        driver_id = message.DriverId()\n        logger.info(\"Monitor: \"\n                    \"XRay Driver {} has been removed.\".format(\n                        binary_to_hex(driver_id)))\n        self._xray_clean_up_entries_for_driver(driver_id)", "response": "Handle a notification that a driver has been removed."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process_messages(self, max_messages=10000):\n        subscribe_clients = [self.primary_subscribe_client]\n        for subscribe_client in subscribe_clients:\n            for _ in range(max_messages):\n                message = subscribe_client.get_message()\n                if message is None:\n                    # Continue on to the next subscribe client.\n                    break\n\n                # Parse the message.\n                channel = message[\"channel\"]\n                data = message[\"data\"]\n\n                # Determine the appropriate message handler.\n                if channel == ray.gcs_utils.XRAY_HEARTBEAT_BATCH_CHANNEL:\n                    # Similar functionality as raylet info channel\n                    message_handler = self.xray_heartbeat_batch_handler\n                elif channel == ray.gcs_utils.XRAY_DRIVER_CHANNEL:\n                    # Handles driver death.\n                    message_handler = self.xray_driver_removed_handler\n                else:\n                    raise Exception(\"This code should be unreachable.\")\n\n                # Call the handler.\n                message_handler(channel, data)", "response": "This function reads all messages from the subscription channels and calls the appropriate handlers."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrunning the monitor. This function loops forever, checking for messages about dead database clients and cleaning up state accordingly.", "response": "def run(self):\n        \"\"\"Run the monitor.\n\n        This function loops forever, checking for messages about dead database\n        clients and cleaning up state accordingly.\n        \"\"\"\n        # Initialize the subscription channel.\n        self.subscribe(ray.gcs_utils.XRAY_HEARTBEAT_BATCH_CHANNEL)\n        self.subscribe(ray.gcs_utils.XRAY_DRIVER_CHANNEL)\n\n        # TODO(rkn): If there were any dead clients at startup, we should clean\n        # up the associated state in the state tables.\n\n        # Handle messages from the subscription channels.\n        while True:\n            # Update the mapping from raylet client ID to IP address.\n            # This is only used to update the load metrics for the autoscaler.\n            self.update_raylet_map()\n\n            # Process autoscaling actions\n            if self.autoscaler:\n                self.autoscaler.update()\n\n            self._maybe_flush_gcs()\n\n            # Process a round of messages.\n            self.process_messages()\n\n            # Wait for a heartbeat interval before processing the next round of\n            # messages.\n            time.sleep(ray._config.heartbeat_timeout_milliseconds() * 1e-3)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nviews for the home page.", "response": "def index(request):\n    \"\"\"View for the home page.\"\"\"\n    recent_jobs = JobRecord.objects.order_by(\"-start_time\")[0:100]\n    recent_trials = TrialRecord.objects.order_by(\"-start_time\")[0:500]\n\n    total_num = len(recent_trials)\n    running_num = sum(t.trial_status == Trial.RUNNING for t in recent_trials)\n    success_num = sum(\n        t.trial_status == Trial.TERMINATED for t in recent_trials)\n    failed_num = sum(t.trial_status == Trial.ERROR for t in recent_trials)\n\n    job_records = []\n    for recent_job in recent_jobs:\n        job_records.append(get_job_info(recent_job))\n    context = {\n        \"log_dir\": AUTOMLBOARD_LOG_DIR,\n        \"reload_interval\": AUTOMLBOARD_RELOAD_INTERVAL,\n        \"recent_jobs\": job_records,\n        \"job_num\": len(job_records),\n        \"trial_num\": total_num,\n        \"running_num\": running_num,\n        \"success_num\": success_num,\n        \"failed_num\": failed_num\n    }\n    return render(request, \"index.html\", context)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef job(request):\n    job_id = request.GET.get(\"job_id\")\n    recent_jobs = JobRecord.objects.order_by(\"-start_time\")[0:100]\n    recent_trials = TrialRecord.objects \\\n        .filter(job_id=job_id) \\\n        .order_by(\"-start_time\")\n    trial_records = []\n    for recent_trial in recent_trials:\n        trial_records.append(get_trial_info(recent_trial))\n    current_job = JobRecord.objects \\\n        .filter(job_id=job_id) \\\n        .order_by(\"-start_time\")[0]\n\n    if len(trial_records) > 0:\n        param_keys = trial_records[0][\"params\"].keys()\n    else:\n        param_keys = []\n\n    # TODO: support custom metrics here\n    metric_keys = [\"episode_reward\", \"accuracy\", \"loss\"]\n    context = {\n        \"current_job\": get_job_info(current_job),\n        \"recent_jobs\": recent_jobs,\n        \"recent_trials\": trial_records,\n        \"param_keys\": param_keys,\n        \"param_num\": len(param_keys),\n        \"metric_keys\": metric_keys,\n        \"metric_num\": len(metric_keys)\n    }\n    return render(request, \"job.html\", context)", "response": "View for a single job."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nviews for a single trial.", "response": "def trial(request):\n    \"\"\"View for a single trial.\"\"\"\n    job_id = request.GET.get(\"job_id\")\n    trial_id = request.GET.get(\"trial_id\")\n    recent_trials = TrialRecord.objects \\\n        .filter(job_id=job_id) \\\n        .order_by(\"-start_time\")\n    recent_results = ResultRecord.objects \\\n        .filter(trial_id=trial_id) \\\n        .order_by(\"-date\")[0:2000]\n    current_trial = TrialRecord.objects \\\n        .filter(trial_id=trial_id) \\\n        .order_by(\"-start_time\")[0]\n    context = {\n        \"job_id\": job_id,\n        \"trial_id\": trial_id,\n        \"current_trial\": current_trial,\n        \"recent_results\": recent_results,\n        \"recent_trials\": recent_trials\n    }\n    return render(request, \"trial.html\", context)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_job_info(current_job):\n    trials = TrialRecord.objects.filter(job_id=current_job.job_id)\n    total_num = len(trials)\n    running_num = sum(t.trial_status == Trial.RUNNING for t in trials)\n    success_num = sum(t.trial_status == Trial.TERMINATED for t in trials)\n    failed_num = sum(t.trial_status == Trial.ERROR for t in trials)\n\n    if total_num == 0:\n        progress = 0\n    else:\n        progress = int(float(success_num) / total_num * 100)\n\n    winner = get_winner(trials)\n\n    job_info = {\n        \"job_id\": current_job.job_id,\n        \"job_name\": current_job.name,\n        \"user\": current_job.user,\n        \"type\": current_job.type,\n        \"start_time\": current_job.start_time,\n        \"end_time\": current_job.end_time,\n        \"total_num\": total_num,\n        \"running_num\": running_num,\n        \"success_num\": success_num,\n        \"failed_num\": failed_num,\n        \"best_trial_id\": current_job.best_trial_id,\n        \"progress\": progress,\n        \"winner\": winner\n    }\n\n    return job_info", "response": "Get job information for current job."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_trial_info(current_trial):\n    if current_trial.end_time and (\"_\" in current_trial.end_time):\n        # end time is parsed from result.json and the format\n        # is like: yyyy-mm-dd_hh-MM-ss, which will be converted\n        # to yyyy-mm-dd hh:MM:ss here\n        time_obj = datetime.datetime.strptime(current_trial.end_time,\n                                              \"%Y-%m-%d_%H-%M-%S\")\n        end_time = time_obj.strftime(\"%Y-%m-%d %H:%M:%S\")\n    else:\n        end_time = current_trial.end_time\n\n    if current_trial.metrics:\n        metrics = eval(current_trial.metrics)\n    else:\n        metrics = None\n\n    trial_info = {\n        \"trial_id\": current_trial.trial_id,\n        \"job_id\": current_trial.job_id,\n        \"trial_status\": current_trial.trial_status,\n        \"start_time\": current_trial.start_time,\n        \"end_time\": end_time,\n        \"params\": eval(current_trial.params.encode(\"utf-8\")),\n        \"metrics\": metrics\n    }\n\n    return trial_info", "response": "Get job information for current trial."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_winner(trials):\n    winner = {}\n    # TODO: sort_key should be customized here\n    sort_key = \"accuracy\"\n    if trials and len(trials) > 0:\n        first_metrics = get_trial_info(trials[0])[\"metrics\"]\n        if first_metrics and not first_metrics.get(\"accuracy\", None):\n            sort_key = \"episode_reward\"\n        max_metric = float(\"-Inf\")\n        for t in trials:\n            metrics = get_trial_info(t).get(\"metrics\", None)\n            if metrics and metrics.get(sort_key, None):\n                current_metric = float(metrics[sort_key])\n                if current_metric > max_metric:\n                    winner[\"trial_id\"] = t.trial_id\n                    winner[\"metric\"] = sort_key + \": \" + str(current_metric)\n                    max_metric = current_metric\n    return winner", "response": "Get winner trial of a job."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_parser(parser_creator=None, **kwargs):\n\n    if parser_creator:\n        parser = parser_creator(**kwargs)\n    else:\n        parser = argparse.ArgumentParser(**kwargs)\n\n    # Note: keep this in sync with rllib/train.py\n    parser.add_argument(\n        \"--run\",\n        default=None,\n        type=str,\n        help=\"The algorithm or model to train. This may refer to the name \"\n        \"of a built-on algorithm (e.g. RLLib's DQN or PPO), or a \"\n        \"user-defined trainable function or class registered in the \"\n        \"tune registry.\")\n    parser.add_argument(\n        \"--stop\",\n        default=\"{}\",\n        type=json.loads,\n        help=\"The stopping criteria, specified in JSON. The keys may be any \"\n        \"field returned by 'train()' e.g. \"\n        \"'{\\\"time_total_s\\\": 600, \\\"training_iteration\\\": 100000}' to stop \"\n        \"after 600 seconds or 100k iterations, whichever is reached first.\")\n    parser.add_argument(\n        \"--config\",\n        default=\"{}\",\n        type=json.loads,\n        help=\"Algorithm-specific configuration (e.g. env, hyperparams), \"\n        \"specified in JSON.\")\n    parser.add_argument(\n        \"--resources-per-trial\",\n        default=None,\n        type=json_to_resources,\n        help=\"Override the machine resources to allocate per trial, e.g. \"\n        \"'{\\\"cpu\\\": 64, \\\"gpu\\\": 8}'. Note that GPUs will not be assigned \"\n        \"unless you specify them here. For RLlib, you probably want to \"\n        \"leave this alone and use RLlib configs to control parallelism.\")\n    parser.add_argument(\n        \"--num-samples\",\n        default=1,\n        type=int,\n        help=\"Number of times to repeat each trial.\")\n    parser.add_argument(\n        \"--local-dir\",\n        default=DEFAULT_RESULTS_DIR,\n        type=str,\n        help=\"Local dir to save training results to. Defaults to '{}'.\".format(\n            DEFAULT_RESULTS_DIR))\n    parser.add_argument(\n        \"--upload-dir\",\n        default=\"\",\n        type=str,\n        help=\"Optional URI to sync training results to (e.g. s3://bucket).\")\n    parser.add_argument(\n        \"--trial-name-creator\",\n        default=None,\n        help=\"Optional creator function for the trial string, used in \"\n        \"generating a trial directory.\")\n    parser.add_argument(\n        \"--sync-function\",\n        default=None,\n        help=\"Function for syncing the local_dir to upload_dir. If string, \"\n        \"then it must be a string template for syncer to run and needs to \"\n        \"include replacement fields '{local_dir}' and '{remote_dir}'.\")\n    parser.add_argument(\n        \"--loggers\",\n        default=None,\n        help=\"List of logger creators to be used with each Trial. \"\n        \"Defaults to ray.tune.logger.DEFAULT_LOGGERS.\")\n    parser.add_argument(\n        \"--checkpoint-freq\",\n        default=0,\n        type=int,\n        help=\"How many training iterations between checkpoints. \"\n        \"A value of 0 (default) disables checkpointing.\")\n    parser.add_argument(\n        \"--checkpoint-at-end\",\n        action=\"store_true\",\n        help=\"Whether to checkpoint at the end of the experiment. \"\n        \"Default is False.\")\n    parser.add_argument(\n        \"--keep-checkpoints-num\",\n        default=None,\n        type=int,\n        help=\"Number of last checkpoints to keep. Others get \"\n        \"deleted. Default (None) keeps all checkpoints.\")\n    parser.add_argument(\n        \"--checkpoint-score-attr\",\n        default=\"training_iteration\",\n        type=str,\n        help=\"Specifies by which attribute to rank the best checkpoint. \"\n        \"Default is increasing order. If attribute starts with min- it \"\n        \"will rank attribute in decreasing order. Example: \"\n        \"min-validation_loss\")\n    parser.add_argument(\n        \"--export-formats\",\n        default=None,\n        help=\"List of formats that exported at the end of the experiment. \"\n        \"Default is None. For RLlib, 'checkpoint' and 'model' are \"\n        \"supported for TensorFlow policy graphs.\")\n    parser.add_argument(\n        \"--max-failures\",\n        default=3,\n        type=int,\n        help=\"Try to recover a trial from its last checkpoint at least this \"\n        \"many times. Only applies if checkpointing is enabled.\")\n    parser.add_argument(\n        \"--scheduler\",\n        default=\"FIFO\",\n        type=str,\n        help=\"FIFO (default), MedianStopping, AsyncHyperBand, \"\n        \"HyperBand, or HyperOpt.\")\n    parser.add_argument(\n        \"--scheduler-config\",\n        default=\"{}\",\n        type=json.loads,\n        help=\"Config options to pass to the scheduler.\")\n\n    # Note: this currently only makes sense when running a single trial\n    parser.add_argument(\n        \"--restore\",\n        default=None,\n        type=str,\n        help=\"If specified, restore from this checkpoint.\")\n\n    return parser", "response": "Returns a base argument parser for the ray. tune tool."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_argv(config):\n    argv = []\n    for k, v in config.items():\n        if \"-\" in k:\n            raise ValueError(\"Use '_' instead of '-' in `{}`\".format(k))\n        if v is None:\n            continue\n        if not isinstance(v, bool) or v:  # for argparse flags\n            argv.append(\"--{}\".format(k.replace(\"_\", \"-\")))\n        if isinstance(v, string_types):\n            argv.append(v)\n        elif isinstance(v, bool):\n            pass\n        else:\n            argv.append(json.dumps(v, cls=_SafeFallbackEncoder))\n    return argv", "response": "Converts a dictionary to a command line argument format."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_trial_from_spec(spec, output_path, parser, **trial_kwargs):\n    try:\n        args = parser.parse_args(to_argv(spec))\n    except SystemExit:\n        raise TuneError(\"Error parsing args, see above message\", spec)\n    if \"resources_per_trial\" in spec:\n        trial_kwargs[\"resources\"] = json_to_resources(\n            spec[\"resources_per_trial\"])\n    return Trial(\n        # Submitting trial via server in py2.7 creates Unicode, which does not\n        # convert to string in a straightforward manner.\n        trainable_name=spec[\"run\"],\n        # json.load leads to str -> unicode in py2.7\n        config=spec.get(\"config\", {}),\n        local_dir=os.path.join(args.local_dir, output_path),\n        # json.load leads to str -> unicode in py2.7\n        stopping_criterion=spec.get(\"stop\", {}),\n        checkpoint_freq=args.checkpoint_freq,\n        checkpoint_at_end=args.checkpoint_at_end,\n        keep_checkpoints_num=args.keep_checkpoints_num,\n        checkpoint_score_attr=args.checkpoint_score_attr,\n        export_formats=spec.get(\"export_formats\", []),\n        # str(None) doesn't create None\n        restore_path=spec.get(\"restore\"),\n        upload_dir=args.upload_dir,\n        trial_name_creator=spec.get(\"trial_name_creator\"),\n        loggers=spec.get(\"loggers\"),\n        # str(None) doesn't create None\n        sync_function=spec.get(\"sync_function\"),\n        max_failures=args.max_failures,\n        **trial_kwargs)", "response": "Creates a Trial object from parsing the experiment specification."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npolling for compute zone operation until finished.", "response": "def wait_for_compute_zone_operation(compute, project_name, operation, zone):\n    \"\"\"Poll for compute zone operation until finished.\"\"\"\n    logger.info(\"wait_for_compute_zone_operation: \"\n                \"Waiting for operation {} to finish...\".format(\n                    operation[\"name\"]))\n\n    for _ in range(MAX_POLLS):\n        result = compute.zoneOperations().get(\n            project=project_name, operation=operation[\"name\"],\n            zone=zone).execute()\n        if \"error\" in result:\n            raise Exception(result[\"error\"])\n\n        if result[\"status\"] == \"DONE\":\n            logger.info(\"wait_for_compute_zone_operation: \"\n                        \"Operation {} finished.\".format(operation[\"name\"]))\n            break\n\n        time.sleep(POLL_INTERVAL)\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the task id associated to the generic source of the signal.", "response": "def _get_task_id(source):\n    \"\"\"Return the task id associated to the generic source of the signal.\n\n    Args:\n        source: source of the signal, it can be either an object id returned\n            by a task, a task id, or an actor handle.\n\n    Returns:\n        - If source is an object id, return id of task which creted object.\n        - If source is an actor handle, return id of actor's task creator.\n        - If source is a task id, return same task id.\n    \"\"\"\n    if type(source) is ray.actor.ActorHandle:\n        return source._ray_actor_id\n    else:\n        if type(source) is ray.TaskID:\n            return source\n        else:\n            return ray._raylet.compute_task_id(source)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef send(signal):\n    if hasattr(ray.worker.global_worker, \"actor_creation_task_id\"):\n        source_key = ray.worker.global_worker.actor_id.hex()\n    else:\n        # No actors; this function must have been called from a task\n        source_key = ray.worker.global_worker.current_task_id.hex()\n\n    encoded_signal = ray.utils.binary_to_hex(cloudpickle.dumps(signal))\n    ray.worker.global_worker.redis_client.execute_command(\n        \"XADD \" + source_key + \" * signal \" + encoded_signal)", "response": "Send a signal to the current node of the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget all outstanding signals from sources.", "response": "def receive(sources, timeout=None):\n    \"\"\"Get all outstanding signals from sources.\n\n    A source can be either (1) an object ID returned by the task (we want\n    to receive signals from), or (2) an actor handle.\n\n    When invoked by the same entity E (where E can be an actor, task or\n    driver), for each source S in sources, this function returns all signals\n    generated by S since the last receive() was invoked by E on S. If this is\n    the first call on S, this function returns all past signals generated by S\n    so far. Note that different actors, tasks or drivers that call receive()\n    on the same source S will get independent copies of the signals generated\n    by S.\n\n    Args:\n        sources: List of sources from which the caller waits for signals.\n            A source is either an object ID returned by a task (in this case\n            the object ID is used to identify that task), or an actor handle.\n            If the user passes the IDs of multiple objects returned by the\n            same task, this function returns a copy of the signals generated\n            by that task for each object ID.\n        timeout: Maximum time (in seconds) this function waits to get a signal\n            from a source in sources. If None, the timeout is infinite.\n\n    Returns:\n        A list of pairs (S, sig), where S is a source in the sources argument,\n            and sig is a signal generated by S since the last time receive()\n            was called on S. Thus, for each S in sources, the return list can\n            contain zero or multiple entries.\n    \"\"\"\n\n    # If None, initialize the timeout to a huge value (i.e., over 30,000 years\n    # in this case) to \"approximate\" infinity.\n    if timeout is None:\n        timeout = 10**12\n\n    if timeout < 0:\n        raise ValueError(\"The 'timeout' argument cannot be less than 0.\")\n\n    if not hasattr(ray.worker.global_worker, \"signal_counters\"):\n        ray.worker.global_worker.signal_counters = defaultdict(lambda: b\"0\")\n\n    signal_counters = ray.worker.global_worker.signal_counters\n\n    # Map the ID of each source task to the source itself.\n    task_id_to_sources = defaultdict(lambda: [])\n    for s in sources:\n        task_id_to_sources[_get_task_id(s).hex()].append(s)\n\n    # Construct the redis query.\n    query = \"XREAD BLOCK \"\n    # Multiply by 1000x since timeout is in sec and redis expects ms.\n    query += str(1000 * timeout)\n    query += \" STREAMS \"\n    query += \" \".join([task_id for task_id in task_id_to_sources])\n    query += \" \"\n    query += \" \".join([\n        ray.utils.decode(signal_counters[ray.utils.hex_to_binary(task_id)])\n        for task_id in task_id_to_sources\n    ])\n\n    answers = ray.worker.global_worker.redis_client.execute_command(query)\n    if not answers:\n        return []\n\n    results = []\n    # Decoding is a little bit involved. Iterate through all the answers:\n    for i, answer in enumerate(answers):\n        # Make sure the answer corresponds to a source, s, in sources.\n        task_id = ray.utils.decode(answer[0])\n        task_source_list = task_id_to_sources[task_id]\n        # The list of results for source s is stored in answer[1]\n        for r in answer[1]:\n            for s in task_source_list:\n                if r[1][1].decode(\"ascii\") == ACTOR_DIED_STR:\n                    results.append((s, ActorDiedSignal()))\n                else:\n                    # Now it gets tricky: r[0] is the redis internal sequence\n                    # id\n                    signal_counters[ray.utils.hex_to_binary(task_id)] = r[0]\n                    # r[1] contains a list with elements (key, value), in our\n                    # case we only have one key \"signal\" and the value is the\n                    # signal.\n                    signal = cloudpickle.loads(\n                        ray.utils.hex_to_binary(r[1][1]))\n                    results.append((s, signal))\n\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reset():\n    if hasattr(ray.worker.global_worker, \"signal_counters\"):\n        ray.worker.global_worker.signal_counters = defaultdict(lambda: b\"0\")", "response": "Reset the worker state associated with any signals that this worker has received so far."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef log_once(key):\n\n    global _last_logged\n\n    if _disabled:\n        return False\n    elif key not in _logged:\n        _logged.add(key)\n        _last_logged = time.time()\n        return True\n    elif _periodic_log and time.time() - _last_logged > 60.0:\n        _logged.clear()\n        _last_logged = time.time()\n        return False\n    else:\n        return False", "response": "Returns True if this is the first call for a given key."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a single or a collection of remote objects from the object store.", "response": "def get(object_ids):\n    \"\"\"Get a single or a collection of remote objects from the object store.\n\n    This method is identical to `ray.get` except it adds support for tuples,\n    ndarrays and dictionaries.\n\n    Args:\n        object_ids: Object ID of the object to get, a list, tuple, ndarray of\n            object IDs to get or a dict of {key: object ID}.\n\n    Returns:\n        A Python object, a list of Python objects or a dict of {key: object}.\n    \"\"\"\n    if isinstance(object_ids, (tuple, np.ndarray)):\n        return ray.get(list(object_ids))\n    elif isinstance(object_ids, dict):\n        keys_to_get = [\n            k for k, v in object_ids.items() if isinstance(v, ray.ObjectID)\n        ]\n        ids_to_get = [\n            v for k, v in object_ids.items() if isinstance(v, ray.ObjectID)\n        ]\n        values = ray.get(ids_to_get)\n\n        result = object_ids.copy()\n        for key, value in zip(keys_to_get, values):\n            result[key] = value\n        return result\n    else:\n        return ray.get(object_ids)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef wait(object_ids, num_returns=1, timeout=None):\n    if isinstance(object_ids, (tuple, np.ndarray)):\n        return ray.wait(\n            list(object_ids), num_returns=num_returns, timeout=timeout)\n\n    return ray.wait(object_ids, num_returns=num_returns, timeout=timeout)", "response": "Wait for the specified object IDs to be ready and return a list of IDs that are not ready."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef convert_to_experiment_list(experiments):\n    exp_list = experiments\n\n    # Transform list if necessary\n    if experiments is None:\n        exp_list = []\n    elif isinstance(experiments, Experiment):\n        exp_list = [experiments]\n    elif type(experiments) is dict:\n        exp_list = [\n            Experiment.from_json(name, spec)\n            for name, spec in experiments.items()\n        ]\n\n    # Validate exp_list\n    if (type(exp_list) is list\n            and all(isinstance(exp, Experiment) for exp in exp_list)):\n        if len(exp_list) > 1:\n            logger.warning(\"All experiments will be \"\n                           \"using the same SearchAlgorithm.\")\n    else:\n        raise TuneError(\"Invalid argument: {}\".format(experiments))\n\n    return exp_list", "response": "Converts input from dict single experiment or list of experiments to list of experiments."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating an Experiment object from a JSON configuration.", "response": "def from_json(cls, name, spec):\n        \"\"\"Generates an Experiment object from JSON.\n\n        Args:\n            name (str): Name of Experiment.\n            spec (dict): JSON configuration of experiment.\n        \"\"\"\n        if \"run\" not in spec:\n            raise TuneError(\"No trainable specified!\")\n\n        # Special case the `env` param for RLlib by automatically\n        # moving it into the `config` section.\n        if \"env\" in spec:\n            spec[\"config\"] = spec.get(\"config\", {})\n            spec[\"config\"][\"env\"] = spec[\"env\"]\n            del spec[\"env\"]\n\n        spec = copy.deepcopy(spec)\n\n        run_value = spec.pop(\"run\")\n        try:\n            exp = cls(name, run_value, **spec)\n        except TypeError:\n            raise TuneError(\"Improper argument from JSON: {}.\".format(spec))\n        return exp"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _register_if_needed(cls, run_object):\n\n        if isinstance(run_object, six.string_types):\n            return run_object\n        elif isinstance(run_object, types.FunctionType):\n            if run_object.__name__ == \"<lambda>\":\n                logger.warning(\n                    \"Not auto-registering lambdas - resolving as variant.\")\n                return run_object\n            else:\n                name = run_object.__name__\n                register_trainable(name, run_object)\n                return name\n        elif isinstance(run_object, type):\n            name = run_object.__name__\n            register_trainable(name, run_object)\n            return name\n        else:\n            raise TuneError(\"Improper 'run' - not string nor trainable.\")", "response": "Registers Trainable or Function at runtime."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef tsqr(a):\n    if len(a.shape) != 2:\n        raise Exception(\"tsqr requires len(a.shape) == 2, but a.shape is \"\n                        \"{}\".format(a.shape))\n    if a.num_blocks[1] != 1:\n        raise Exception(\"tsqr requires a.num_blocks[1] == 1, but a.num_blocks \"\n                        \"is {}\".format(a.num_blocks))\n\n    num_blocks = a.num_blocks[0]\n    K = int(np.ceil(np.log2(num_blocks))) + 1\n    q_tree = np.empty((num_blocks, K), dtype=object)\n    current_rs = []\n    for i in range(num_blocks):\n        block = a.objectids[i, 0]\n        q, r = ra.linalg.qr.remote(block)\n        q_tree[i, 0] = q\n        current_rs.append(r)\n    for j in range(1, K):\n        new_rs = []\n        for i in range(int(np.ceil(1.0 * len(current_rs) / 2))):\n            stacked_rs = ra.vstack.remote(*current_rs[(2 * i):(2 * i + 2)])\n            q, r = ra.linalg.qr.remote(stacked_rs)\n            q_tree[i, j] = q\n            new_rs.append(r)\n        current_rs = new_rs\n    assert len(current_rs) == 1, \"len(current_rs) = \" + str(len(current_rs))\n\n    # handle the special case in which the whole DistArray \"a\" fits in one\n    # block and has fewer rows than columns, this is a bit ugly so think about\n    # how to remove it\n    if a.shape[0] >= a.shape[1]:\n        q_shape = a.shape\n    else:\n        q_shape = [a.shape[0], a.shape[0]]\n    q_num_blocks = core.DistArray.compute_num_blocks(q_shape)\n    q_objectids = np.empty(q_num_blocks, dtype=object)\n    q_result = core.DistArray(q_shape, q_objectids)\n\n    # reconstruct output\n    for i in range(num_blocks):\n        q_block_current = q_tree[i, 0]\n        ith_index = i\n        for j in range(1, K):\n            if np.mod(ith_index, 2) == 0:\n                lower = [0, 0]\n                upper = [a.shape[1], core.BLOCK_SIZE]\n            else:\n                lower = [a.shape[1], 0]\n                upper = [2 * a.shape[1], core.BLOCK_SIZE]\n            ith_index //= 2\n            q_block_current = ra.dot.remote(\n                q_block_current,\n                ra.subarray.remote(q_tree[ith_index, j], lower, upper))\n        q_result.objectids[i] = q_block_current\n    r = current_rs[0]\n    return q_result, ray.get(r)", "response": "Perform a QR decomposition of a distributed matrix."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef modified_lu(q):\n    q = q.assemble()\n    m, b = q.shape[0], q.shape[1]\n    S = np.zeros(b)\n\n    q_work = np.copy(q)\n\n    for i in range(b):\n        S[i] = -1 * np.sign(q_work[i, i])\n        q_work[i, i] -= S[i]\n        # Scale ith column of L by diagonal element.\n        q_work[(i + 1):m, i] /= q_work[i, i]\n        # Perform Schur complement update.\n        q_work[(i + 1):m, (i + 1):b] -= np.outer(q_work[(i + 1):m, i],\n                                                 q_work[i, (i + 1):b])\n\n    L = np.tril(q_work)\n    for i in range(b):\n        L[i, i] = 1\n    U = np.triu(q_work)[:b, :]\n    # TODO(rkn): Get rid of the put below.\n    return ray.get(core.numpy_to_dist.remote(ray.put(L))), U, S", "response": "Perform a modified LU decomposition of a matrix q."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _naturalize(string):\n    splits = re.split(\"([0-9]+)\", string)\n    return [int(text) if text.isdigit() else text.lower() for text in splits]", "response": "Provides a natural representation for string for nice sorting."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns path to most recently modified checkpoint.", "response": "def _find_newest_ckpt(ckpt_dir):\n    \"\"\"Returns path to most recently modified checkpoint.\"\"\"\n    full_paths = [\n        os.path.join(ckpt_dir, fname) for fname in os.listdir(ckpt_dir)\n        if fname.startswith(\"experiment_state\") and fname.endswith(\".json\")\n    ]\n    return max(full_paths)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef checkpoint(self):\n        if not self._metadata_checkpoint_dir:\n            return\n        metadata_checkpoint_dir = self._metadata_checkpoint_dir\n        if not os.path.exists(metadata_checkpoint_dir):\n            os.makedirs(metadata_checkpoint_dir)\n        runner_state = {\n            \"checkpoints\": list(\n                self.trial_executor.get_checkpoints().values()),\n            \"runner_data\": self.__getstate__(),\n            \"timestamp\": time.time()\n        }\n        tmp_file_name = os.path.join(metadata_checkpoint_dir,\n                                     \".tmp_checkpoint\")\n        with open(tmp_file_name, \"w\") as f:\n            json.dump(runner_state, f, indent=2, cls=_TuneFunctionEncoder)\n\n        os.rename(\n            tmp_file_name,\n            os.path.join(metadata_checkpoint_dir,\n                         TrialRunner.CKPT_FILE_TMPL.format(self._session_str)))\n        return metadata_checkpoint_dir", "response": "Saves the execution state to self. _metadata_checkpoint_dir."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef restore(cls,\n                metadata_checkpoint_dir,\n                search_alg=None,\n                scheduler=None,\n                trial_executor=None):\n        \"\"\"Restores all checkpointed trials from previous run.\n\n        Requires user to manually re-register their objects. Also stops\n        all ongoing trials.\n\n        Args:\n            metadata_checkpoint_dir (str): Path to metadata checkpoints.\n            search_alg (SearchAlgorithm): Search Algorithm. Defaults to\n                BasicVariantGenerator.\n            scheduler (TrialScheduler): Scheduler for executing\n                the experiment.\n            trial_executor (TrialExecutor): Manage the execution of trials.\n\n        Returns:\n            runner (TrialRunner): A TrialRunner to resume experiments from.\n        \"\"\"\n\n        newest_ckpt_path = _find_newest_ckpt(metadata_checkpoint_dir)\n        with open(newest_ckpt_path, \"r\") as f:\n            runner_state = json.load(f, cls=_TuneFunctionDecoder)\n\n        logger.warning(\"\".join([\n            \"Attempting to resume experiment from {}. \".format(\n                metadata_checkpoint_dir), \"This feature is experimental, \"\n            \"and may not work with all search algorithms. \",\n            \"This will ignore any new changes to the specification.\"\n        ]))\n\n        from ray.tune.suggest import BasicVariantGenerator\n        runner = TrialRunner(\n            search_alg or BasicVariantGenerator(),\n            scheduler=scheduler,\n            trial_executor=trial_executor)\n\n        runner.__setstate__(runner_state[\"runner_data\"])\n\n        trials = []\n        for trial_cp in runner_state[\"checkpoints\"]:\n            new_trial = Trial(trial_cp[\"trainable_name\"])\n            new_trial.__setstate__(trial_cp)\n            trials += [new_trial]\n        for trial in sorted(\n                trials, key=lambda t: t.last_update_time, reverse=True):\n            runner.add_trial(trial)\n        return runner", "response": "Restores all checkpointed trials from previous run."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn whether all trials have finished running.", "response": "def is_finished(self):\n        \"\"\"Returns whether all trials have finished running.\"\"\"\n\n        if self._total_time > self._global_time_limit:\n            logger.warning(\"Exceeded global time limit {} / {}\".format(\n                self._total_time, self._global_time_limit))\n            return True\n\n        trials_done = all(trial.is_finished() for trial in self._trials)\n        return trials_done and self._search_alg.is_finished()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns one iteration of the trial event loop.", "response": "def step(self):\n        \"\"\"Runs one step of the trial event loop.\n\n        Callers should typically run this method repeatedly in a loop. They\n        may inspect or modify the runner's state in between calls to step().\n        \"\"\"\n        if self.is_finished():\n            raise TuneError(\"Called step when all trials finished?\")\n        with warn_if_slow(\"on_step_begin\"):\n            self.trial_executor.on_step_begin()\n        next_trial = self._get_next_trial()  # blocking\n        if next_trial is not None:\n            with warn_if_slow(\"start_trial\"):\n                self.trial_executor.start_trial(next_trial)\n        elif self.trial_executor.get_running_trials():\n            self._process_events()  # blocking\n        else:\n            for trial in self._trials:\n                if trial.status == Trial.PENDING:\n                    if not self.has_resources(trial.resources):\n                        raise TuneError(\n                            (\"Insufficient cluster resources to launch trial: \"\n                             \"trial requested {} but the cluster has only {}. \"\n                             \"Pass `queue_trials=True` in \"\n                             \"ray.tune.run() or on the command \"\n                             \"line to queue trials until the cluster scales \"\n                             \"up. {}\").format(\n                                 trial.resources.summary_string(),\n                                 self.trial_executor.resource_string(),\n                                 trial._get_trainable_cls().resource_help(\n                                     trial.config)))\n                elif trial.status == Trial.PAUSED:\n                    raise TuneError(\n                        \"There are paused trials, but no more pending \"\n                        \"trials with sufficient resources.\")\n\n        try:\n            with warn_if_slow(\"experiment_checkpoint\"):\n                self.checkpoint()\n        except Exception:\n            logger.exception(\"Trial Runner checkpointing failed.\")\n        self._iteration += 1\n\n        if self._server:\n            with warn_if_slow(\"server\"):\n                self._process_requests()\n\n            if self.is_finished():\n                self._server.shutdown()\n        with warn_if_slow(\"on_step_end\"):\n            self.trial_executor.on_step_end()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a new trial to the scheduler.", "response": "def add_trial(self, trial):\n        \"\"\"Adds a new trial to this TrialRunner.\n\n        Trials may be added at any time.\n\n        Args:\n            trial (Trial): Trial to queue.\n        \"\"\"\n        trial.set_verbose(self._verbose)\n        self._trials.append(trial)\n        with warn_if_slow(\"scheduler.on_trial_add\"):\n            self._scheduler_alg.on_trial_add(self, trial)\n        self.trial_executor.try_checkpoint_metadata(trial)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef debug_string(self, max_debug=MAX_DEBUG_TRIALS):\n        messages = self._debug_messages()\n        states = collections.defaultdict(set)\n        limit_per_state = collections.Counter()\n        for t in self._trials:\n            states[t.status].add(t)\n\n        # Show at most max_debug total, but divide the limit fairly\n        while max_debug > 0:\n            start_num = max_debug\n            for s in states:\n                if limit_per_state[s] >= len(states[s]):\n                    continue\n                max_debug -= 1\n                limit_per_state[s] += 1\n            if max_debug == start_num:\n                break\n\n        for local_dir in sorted({t.local_dir for t in self._trials}):\n            messages.append(\"Result logdir: {}\".format(local_dir))\n\n        num_trials_per_state = {\n            state: len(trials)\n            for state, trials in states.items()\n        }\n        total_number_of_trials = sum(num_trials_per_state.values())\n        if total_number_of_trials > 0:\n            messages.append(\"Number of trials: {} ({})\"\n                            \"\".format(total_number_of_trials,\n                                      num_trials_per_state))\n\n        for state, trials in sorted(states.items()):\n            limit = limit_per_state[state]\n            messages.append(\"{} trials:\".format(state))\n            sorted_trials = sorted(\n                trials, key=lambda t: _naturalize(t.experiment_tag))\n            if len(trials) > limit:\n                tail_length = limit // 2\n                first = sorted_trials[:tail_length]\n                for t in first:\n                    messages.append(\" - {}:\\t{}\".format(\n                        t, t.progress_string()))\n                messages.append(\n                    \"  ... {} not shown\".format(len(trials) - tail_length * 2))\n                last = sorted_trials[-tail_length:]\n                for t in last:\n                    messages.append(\" - {}:\\t{}\".format(\n                        t, t.progress_string()))\n            else:\n                for t in sorted_trials:\n                    messages.append(\" - {}:\\t{}\".format(\n                        t, t.progress_string()))\n\n        return \"\\n\".join(messages) + \"\\n\"", "response": "Returns a human readable string for printing to the console."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nblocking if all trials queued have finished and search algorithm is still not finished.", "response": "def _get_next_trial(self):\n        \"\"\"Replenishes queue.\n\n        Blocks if all trials queued have finished, but search algorithm is\n        still not finished.\n        \"\"\"\n        trials_done = all(trial.is_finished() for trial in self._trials)\n        wait_for_trial = trials_done and not self._search_alg.is_finished()\n        self._update_trial_queue(blocking=wait_for_trial)\n        with warn_if_slow(\"choose_trial_to_run\"):\n            trial = self._scheduler_alg.choose_trial_to_run(self)\n        return trial"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntrying to recover trial from checkpoint and notifies SearchAlgorithm and Scheduler.", "response": "def _try_recover(self, trial, error_msg):\n        \"\"\"Tries to recover trial.\n\n        Notifies SearchAlgorithm and Scheduler if failure to recover.\n\n        Args:\n            trial (Trial): Trial to recover.\n            error_msg (str): Error message from prior to invoking this method.\n        \"\"\"\n        try:\n            self.trial_executor.stop_trial(\n                trial,\n                error=error_msg is not None,\n                error_msg=error_msg,\n                stop_logger=False)\n            trial.result_logger.flush()\n            if self.trial_executor.has_resources(trial.resources):\n                logger.info(\"Attempting to recover\"\n                            \" trial state from last checkpoint.\")\n                self.trial_executor.start_trial(trial)\n                if trial.status == Trial.ERROR:\n                    raise RuntimeError(\"Trial did not start correctly.\")\n            else:\n                logger.debug(\"Notifying Scheduler and requeueing trial.\")\n                self._requeue_trial(trial)\n        except Exception:\n            logger.exception(\"Error recovering trial from checkpoint, abort.\")\n            self._scheduler_alg.on_trial_error(self, trial)\n            self._search_alg.on_trial_complete(trial.trial_id, error=True)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _requeue_trial(self, trial):\n        self._scheduler_alg.on_trial_error(self, trial)\n        self.trial_executor.set_status(trial, Trial.PENDING)\n        with warn_if_slow(\"scheduler.on_trial_add\"):\n            self._scheduler_alg.on_trial_add(self, trial)", "response": "Notify TrialScheduler that trial is requeued."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _update_trial_queue(self, blocking=False, timeout=600):\n        trials = self._search_alg.next_trials()\n        if blocking and not trials:\n            start = time.time()\n            # Checking `is_finished` instead of _search_alg.is_finished\n            # is fine because blocking only occurs if all trials are\n            # finished and search_algorithm is not yet finished\n            while (not trials and not self.is_finished()\n                   and time.time() - start < timeout):\n                logger.info(\"Blocking for next trial...\")\n                trials = self._search_alg.next_trials()\n                time.sleep(1)\n\n        for trial in trials:\n            self.add_trial(trial)", "response": "Updates the trial queue if possible."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstopping trial. Trials may be stopped at any time. If trial is in state PENDING or PAUSED, calls `on_trial_remove` for scheduler and `on_trial_complete(..., early_terminated=True) for search_alg. Otherwise waits for result for the trial and calls `on_trial_complete` for scheduler and search_alg if RUNNING.", "response": "def stop_trial(self, trial):\n        \"\"\"Stops trial.\n\n        Trials may be stopped at any time. If trial is in state PENDING\n        or PAUSED, calls `on_trial_remove`  for scheduler and\n        `on_trial_complete(..., early_terminated=True) for search_alg.\n        Otherwise waits for result for the trial and calls\n        `on_trial_complete` for scheduler and search_alg if RUNNING.\n        \"\"\"\n        error = False\n        error_msg = None\n\n        if trial.status in [Trial.ERROR, Trial.TERMINATED]:\n            return\n        elif trial.status in [Trial.PENDING, Trial.PAUSED]:\n            self._scheduler_alg.on_trial_remove(self, trial)\n            self._search_alg.on_trial_complete(\n                trial.trial_id, early_terminated=True)\n        elif trial.status is Trial.RUNNING:\n            try:\n                result = self.trial_executor.fetch_result(trial)\n                trial.update_last_result(result, terminate=True)\n                self._scheduler_alg.on_trial_complete(self, trial, result)\n                self._search_alg.on_trial_complete(\n                    trial.trial_id, result=result)\n            except Exception:\n                error_msg = traceback.format_exc()\n                logger.exception(\"Error processing event.\")\n                self._scheduler_alg.on_trial_error(self, trial)\n                self._search_alg.on_trial_complete(trial.trial_id, error=True)\n                error = True\n\n        self.trial_executor.stop_trial(trial, error=error, error_msg=error_msg)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef example8():\n\n    # See cython_blas.pyx for argument documentation\n    mat = np.array([[[2.0, 2.0], [2.0, 2.0]], [[2.0, 2.0], [2.0, 2.0]]],\n                   dtype=np.float32)\n    result = np.zeros((2, 2), np.float32, order=\"C\")\n\n    run_func(cyth.compute_kernel_matrix,\n             \"L\",\n             \"T\",\n             2,\n             2,\n             1.0,\n             mat,\n             0,\n             2,\n             1.0,\n             result,\n             2\n             )", "response": "Cython with blas. NOTE: requires scipy"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadjusts the given trajectory to encode n - step rewards and new_obs.", "response": "def _adjust_nstep(n_step, gamma, obs, actions, rewards, new_obs, dones):\n    \"\"\"Rewrites the given trajectory fragments to encode n-step rewards.\n\n    reward[i] = (\n        reward[i] * gamma**0 +\n        reward[i+1] * gamma**1 +\n        ... +\n        reward[i+n_step-1] * gamma**(n_step-1))\n\n    The ith new_obs is also adjusted to point to the (i+n_step-1)'th new obs.\n\n    At the end of the trajectory, n is truncated to fit in the traj length.\n    \"\"\"\n\n    assert not any(dones[:-1]), \"Unexpected done in middle of trajectory\"\n\n    traj_length = len(rewards)\n    for i in range(traj_length):\n        for j in range(1, n_step):\n            if i + j < traj_length:\n                new_obs[i] = new_obs[i + j]\n                dones[i] = dones[i + j]\n                rewards[i] += gamma**j * rewards[i + j]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _huber_loss(x, delta=1.0):\n    return tf.where(\n        tf.abs(x) < delta,\n        tf.square(x) * 0.5, delta * (tf.abs(x) - 0.5 * delta))", "response": "Reference to huber loss."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nminimizing objective using optimizer w. r. t. variables var_list while ensure the norm of the gradients for each cluster is clipped to clip_val.", "response": "def _minimize_and_clip(optimizer, objective, var_list, clip_val=10):\n    \"\"\"Minimized `objective` using `optimizer` w.r.t. variables in\n    `var_list` while ensure the norm of the gradients for each\n    variable is clipped to `clip_val`\n    \"\"\"\n    gradients = optimizer.compute_gradients(objective, var_list=var_list)\n    for i, (grad, var) in enumerate(gradients):\n        if grad is not None:\n            gradients[i] = (tf.clip_by_norm(grad, clip_val), var)\n    return gradients"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _scope_vars(scope, trainable_only=False):\n    return tf.get_collection(\n        tf.GraphKeys.TRAINABLE_VARIABLES\n        if trainable_only else tf.GraphKeys.VARIABLES,\n        scope=scope if isinstance(scope, str) else scope.name)", "response": "Returns a list of variables inside a scope."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_custom_getter(self):\n\n        def inner_custom_getter(getter, *args, **kwargs):\n            if not self.use_tf_layers:\n                return getter(*args, **kwargs)\n            requested_dtype = kwargs[\"dtype\"]\n            if not (requested_dtype == tf.float32\n                    and self.variable_dtype == tf.float16):\n                kwargs[\"dtype\"] = self.variable_dtype\n            var = getter(*args, **kwargs)\n            if var.dtype.base_dtype != requested_dtype:\n                var = tf.cast(var, requested_dtype)\n            return var\n\n        return inner_custom_getter", "response": "Returns a custom getter that this class must be called by the custom methods of the base class."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconstructs a conv2d layer on top of cnn.", "response": "def conv(self,\n             num_out_channels,\n             k_height,\n             k_width,\n             d_height=1,\n             d_width=1,\n             mode=\"SAME\",\n             input_layer=None,\n             num_channels_in=None,\n             use_batch_norm=None,\n             stddev=None,\n             activation=\"relu\",\n             bias=0.0):\n        \"\"\"Construct a conv2d layer on top of cnn.\"\"\"\n        if input_layer is None:\n            input_layer = self.top_layer\n        if num_channels_in is None:\n            num_channels_in = self.top_size\n        kernel_initializer = None\n        if stddev is not None:\n            kernel_initializer = tf.truncated_normal_initializer(stddev=stddev)\n        name = \"conv\" + str(self.counts[\"conv\"])\n        self.counts[\"conv\"] += 1\n        with tf.variable_scope(name):\n            strides = [1, d_height, d_width, 1]\n            if self.data_format == \"NCHW\":\n                strides = [strides[0], strides[3], strides[1], strides[2]]\n            if mode != \"SAME_RESNET\":\n                conv = self._conv2d_impl(\n                    input_layer,\n                    num_channels_in,\n                    num_out_channels,\n                    kernel_size=[k_height, k_width],\n                    strides=[d_height, d_width],\n                    padding=mode,\n                    kernel_initializer=kernel_initializer)\n            else:  # Special padding mode for ResNet models\n                if d_height == 1 and d_width == 1:\n                    conv = self._conv2d_impl(\n                        input_layer,\n                        num_channels_in,\n                        num_out_channels,\n                        kernel_size=[k_height, k_width],\n                        strides=[d_height, d_width],\n                        padding=\"SAME\",\n                        kernel_initializer=kernel_initializer)\n                else:\n                    rate = 1  # Unused (for 'a trous' convolutions)\n                    kernel_height_effective = k_height + (k_height - 1) * (\n                        rate - 1)\n                    pad_h_beg = (kernel_height_effective - 1) // 2\n                    pad_h_end = kernel_height_effective - 1 - pad_h_beg\n                    kernel_width_effective = k_width + (k_width - 1) * (\n                        rate - 1)\n                    pad_w_beg = (kernel_width_effective - 1) // 2\n                    pad_w_end = kernel_width_effective - 1 - pad_w_beg\n                    padding = [[0, 0], [pad_h_beg, pad_h_end],\n                               [pad_w_beg, pad_w_end], [0, 0]]\n                    if self.data_format == \"NCHW\":\n                        padding = [\n                            padding[0], padding[3], padding[1], padding[2]\n                        ]\n                    input_layer = tf.pad(input_layer, padding)\n                    conv = self._conv2d_impl(\n                        input_layer,\n                        num_channels_in,\n                        num_out_channels,\n                        kernel_size=[k_height, k_width],\n                        strides=[d_height, d_width],\n                        padding=\"VALID\",\n                        kernel_initializer=kernel_initializer)\n            if use_batch_norm is None:\n                use_batch_norm = self.use_batch_norm\n            if not use_batch_norm:\n                if bias is not None:\n                    biases = self.get_variable(\n                        \"biases\", [num_out_channels],\n                        self.variable_dtype,\n                        self.dtype,\n                        initializer=tf.constant_initializer(bias))\n                    biased = tf.reshape(\n                        tf.nn.bias_add(\n                            conv, biases, data_format=self.data_format),\n                        conv.get_shape())\n                else:\n                    biased = conv\n            else:\n                self.top_layer = conv\n                self.top_size = num_out_channels\n                biased = self.batch_norm(**self.batch_norm_config)\n            if activation == \"relu\":\n                conv1 = tf.nn.relu(biased)\n            elif activation == \"linear\" or activation is None:\n                conv1 = biased\n            elif activation == \"tanh\":\n                conv1 = tf.nn.tanh(biased)\n            else:\n                raise KeyError(\"Invalid activation type \\\"%s\\\"\" % activation)\n            self.top_layer = conv1\n            self.top_size = num_out_channels\n            return conv1"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _pool(self, pool_name, pool_function, k_height, k_width, d_height,\n              d_width, mode, input_layer, num_channels_in):\n        \"\"\"Construct a pooling layer.\"\"\"\n        if input_layer is None:\n            input_layer = self.top_layer\n        else:\n            self.top_size = num_channels_in\n        name = pool_name + str(self.counts[pool_name])\n        self.counts[pool_name] += 1\n        if self.use_tf_layers:\n            pool = pool_function(\n                input_layer, [k_height, k_width], [d_height, d_width],\n                padding=mode,\n                data_format=self.channel_pos,\n                name=name)\n        else:\n            if self.data_format == \"NHWC\":\n                ksize = [1, k_height, k_width, 1]\n                strides = [1, d_height, d_width, 1]\n            else:\n                ksize = [1, 1, k_height, k_width]\n                strides = [1, 1, d_height, d_width]\n            pool = tf.nn.max_pool(\n                input_layer,\n                ksize,\n                strides,\n                padding=mode,\n                data_format=self.data_format,\n                name=name)\n        self.top_layer = pool\n        return pool", "response": "Construct a pooling layer."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconstructs a max pooling layer.", "response": "def mpool(self,\n              k_height,\n              k_width,\n              d_height=2,\n              d_width=2,\n              mode=\"VALID\",\n              input_layer=None,\n              num_channels_in=None):\n        \"\"\"Construct a max pooling layer.\"\"\"\n        return self._pool(\"mpool\", pooling_layers.max_pooling2d, k_height,\n                          k_width, d_height, d_width, mode, input_layer,\n                          num_channels_in)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconstructs an average pooling layer.", "response": "def apool(self,\n              k_height,\n              k_width,\n              d_height=2,\n              d_width=2,\n              mode=\"VALID\",\n              input_layer=None,\n              num_channels_in=None):\n        \"\"\"Construct an average pooling layer.\"\"\"\n        return self._pool(\"apool\", pooling_layers.average_pooling2d, k_height,\n                          k_width, d_height, d_width, mode, input_layer,\n                          num_channels_in)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _batch_norm_without_layers(self, input_layer, decay, use_scale,\n                                   epsilon):\n        \"\"\"Batch normalization on `input_layer` without tf.layers.\"\"\"\n        shape = input_layer.shape\n        num_channels = shape[3] if self.data_format == \"NHWC\" else shape[1]\n        beta = self.get_variable(\n            \"beta\", [num_channels],\n            tf.float32,\n            tf.float32,\n            initializer=tf.zeros_initializer())\n        if use_scale:\n            gamma = self.get_variable(\n                \"gamma\", [num_channels],\n                tf.float32,\n                tf.float32,\n                initializer=tf.ones_initializer())\n        else:\n            gamma = tf.constant(1.0, tf.float32, [num_channels])\n        moving_mean = tf.get_variable(\n            \"moving_mean\", [num_channels],\n            tf.float32,\n            initializer=tf.zeros_initializer(),\n            trainable=False)\n        moving_variance = tf.get_variable(\n            \"moving_variance\", [num_channels],\n            tf.float32,\n            initializer=tf.ones_initializer(),\n            trainable=False)\n        if self.phase_train:\n            bn, batch_mean, batch_variance = tf.nn.fused_batch_norm(\n                input_layer,\n                gamma,\n                beta,\n                epsilon=epsilon,\n                data_format=self.data_format,\n                is_training=True)\n            mean_update = moving_averages.assign_moving_average(\n                moving_mean, batch_mean, decay=decay, zero_debias=False)\n            variance_update = moving_averages.assign_moving_average(\n                moving_variance,\n                batch_variance,\n                decay=decay,\n                zero_debias=False)\n            tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, mean_update)\n            tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, variance_update)\n        else:\n            bn, _, _ = tf.nn.fused_batch_norm(\n                input_layer,\n                gamma,\n                beta,\n                mean=moving_mean,\n                variance=moving_variance,\n                epsilon=epsilon,\n                data_format=self.data_format,\n                is_training=False)\n        return bn", "response": "Batch normalization on input_layer without tf. layers."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef batch_norm(self,\n                   input_layer=None,\n                   decay=0.999,\n                   scale=False,\n                   epsilon=0.001):\n        \"\"\"Adds a Batch Normalization layer.\"\"\"\n        if input_layer is None:\n            input_layer = self.top_layer\n        else:\n            self.top_size = None\n        name = \"batchnorm\" + str(self.counts[\"batchnorm\"])\n        self.counts[\"batchnorm\"] += 1\n\n        with tf.variable_scope(name) as scope:\n            if self.use_tf_layers:\n                bn = tf.contrib.layers.batch_norm(\n                    input_layer,\n                    decay=decay,\n                    scale=scale,\n                    epsilon=epsilon,\n                    is_training=self.phase_train,\n                    fused=True,\n                    data_format=self.data_format,\n                    scope=scope)\n            else:\n                bn = self._batch_norm_without_layers(input_layer, decay, scale,\n                                                     epsilon)\n        self.top_layer = bn\n        self.top_size = bn.shape[\n            3] if self.data_format == \"NHWC\" else bn.shape[1]\n        self.top_size = int(self.top_size)\n        return bn", "response": "Adds a Batch Normalization layer."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef lrn(self, depth_radius, bias, alpha, beta):\n        name = \"lrn\" + str(self.counts[\"lrn\"])\n        self.counts[\"lrn\"] += 1\n        self.top_layer = tf.nn.lrn(\n            self.top_layer, depth_radius, bias, alpha, beta, name=name)\n        return self.top_layer", "response": "Adds a local response normalization layer."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfetch the value of a binary key.", "response": "def _internal_kv_get(key):\n    \"\"\"Fetch the value of a binary key.\"\"\"\n\n    worker = ray.worker.get_global_worker()\n    if worker.mode == ray.worker.LOCAL_MODE:\n        return _local.get(key)\n\n    return worker.redis_client.hget(key, \"value\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _internal_kv_put(key, value, overwrite=False):\n\n    worker = ray.worker.get_global_worker()\n    if worker.mode == ray.worker.LOCAL_MODE:\n        exists = key in _local\n        if not exists or overwrite:\n            _local[key] = value\n        return exists\n\n    if overwrite:\n        updated = worker.redis_client.hset(key, \"value\", value)\n    else:\n        updated = worker.redis_client.hsetnx(key, \"value\", value)\n    return updated == 0", "response": "Globally associates a value with a given binary key."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndeferring init so that we can pass in previously created workers.", "response": "def init(self, aggregators):\n        \"\"\"Deferred init so that we can pass in previously created workers.\"\"\"\n\n        assert len(aggregators) == self.num_aggregation_workers, aggregators\n        if len(self.remote_evaluators) < self.num_aggregation_workers:\n            raise ValueError(\n                \"The number of aggregation workers should not exceed the \"\n                \"number of total evaluation workers ({} vs {})\".format(\n                    self.num_aggregation_workers, len(self.remote_evaluators)))\n\n        assigned_evaluators = collections.defaultdict(list)\n        for i, ev in enumerate(self.remote_evaluators):\n            assigned_evaluators[i % self.num_aggregation_workers].append(ev)\n\n        self.workers = aggregators\n        for i, worker in enumerate(self.workers):\n            worker.init.remote(\n                self.broadcasted_weights, assigned_evaluators[i],\n                self.max_sample_requests_in_flight_per_worker,\n                self.replay_proportion, self.replay_buffer_num_slots,\n                self.train_batch_size, self.sample_batch_size)\n\n        self.agg_tasks = TaskPool()\n        for agg in self.workers:\n            agg.set_weights.remote(self.broadcasted_weights)\n            self.agg_tasks.add(agg, agg.get_train_batches.remote())\n\n        self.initialized = True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfree a list of objects from object store.", "response": "def free(object_ids, local_only=False, delete_creating_tasks=False):\n    \"\"\"Free a list of IDs from object stores.\n\n    This function is a low-level API which should be used in restricted\n    scenarios.\n\n    If local_only is false, the request will be send to all object stores.\n\n    This method will not return any value to indicate whether the deletion is\n    successful or not. This function is an instruction to object store. If\n    the some of the objects are in use, object stores will delete them later\n    when the ref count is down to 0.\n\n    Args:\n        object_ids (List[ObjectID]): List of object IDs to delete.\n        local_only (bool): Whether only deleting the list of objects in local\n            object store or all object stores.\n        delete_creating_tasks (bool): Whether also delete the object creating\n            tasks.\n    \"\"\"\n    worker = ray.worker.get_global_worker()\n\n    if ray.worker._mode() == ray.worker.LOCAL_MODE:\n        return\n\n    if isinstance(object_ids, ray.ObjectID):\n        object_ids = [object_ids]\n\n    if not isinstance(object_ids, list):\n        raise TypeError(\"free() expects a list of ObjectID, got {}\".format(\n            type(object_ids)))\n\n    # Make sure that the values are object IDs.\n    for object_id in object_ids:\n        if not isinstance(object_id, ray.ObjectID):\n            raise TypeError(\"Attempting to call `free` on the value {}, \"\n                            \"which is not an ray.ObjectID.\".format(object_id))\n\n    worker.check_connected()\n    with profiling.profile(\"ray.free\"):\n        if len(object_ids) == 0:\n            return\n\n        worker.raylet_client.free_objects(object_ids, local_only,\n                                          delete_creating_tasks)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run(self):\n        self.collector.start()\n        if self.standalone:\n            self.collector.join()", "response": "Start the collector worker thread."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run(self):\n        self._initialize()\n\n        self._do_collect()\n        while not self._is_finished:\n            time.sleep(self._reload_interval)\n            self._do_collect()\n\n        self.logger.info(\"Collector stopped.\")", "response": "Main event loop for collector thread."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninitializing collector worker thread.", "response": "def _initialize(self):\n        \"\"\"Initialize collector worker thread, Log path will be checked first.\n\n        Records in DB backend will be cleared.\n        \"\"\"\n        if not os.path.exists(self._logdir):\n            raise CollectorError(\"Log directory %s not exists\" % self._logdir)\n\n        self.logger.info(\"Collector started, taking %s as parent directory\"\n                         \"for all job logs.\" % self._logdir)\n\n        # clear old records\n        JobRecord.objects.filter().delete()\n        TrialRecord.objects.filter().delete()\n        ResultRecord.objects.filter().delete()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload information of the job with the given job name.", "response": "def sync_job_info(self, job_name):\n        \"\"\"Load information of the job with the given job name.\n\n        1. Traverse each experiment sub-directory and sync information\n           for each trial.\n        2. Create or update the job information, together with the job\n           meta file.\n\n        Args:\n            job_name (str) name of the Tune experiment\n        \"\"\"\n        job_path = os.path.join(self._logdir, job_name)\n\n        if job_name not in self._monitored_jobs:\n            self._create_job_info(job_path)\n            self._monitored_jobs.add(job_name)\n        else:\n            self._update_job_info(job_path)\n\n        expr_dirs = filter(lambda d: os.path.isdir(os.path.join(job_path, d)),\n                           os.listdir(job_path))\n\n        for expr_dir_name in expr_dirs:\n            self.sync_trial_info(job_path, expr_dir_name)\n\n        self._update_job_info(job_path)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading the trial information from the given experiment directory. Create or update the trial info together with the trial meta file.", "response": "def sync_trial_info(self, job_path, expr_dir_name):\n        \"\"\"Load information of the trial from the given experiment directory.\n\n        Create or update the trial information, together with the trial\n        meta file.\n\n        Args:\n            job_path(str)\n            expr_dir_name(str)\n\n        \"\"\"\n        expr_name = expr_dir_name[-8:]\n        expr_path = os.path.join(job_path, expr_dir_name)\n\n        if expr_name not in self._monitored_trials:\n            self._create_trial_info(expr_path)\n            self._monitored_trials.add(expr_name)\n        else:\n            self._update_trial_info(expr_path)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _create_job_info(self, job_dir):\n        meta = self._build_job_meta(job_dir)\n\n        self.logger.debug(\"Create job: %s\" % meta)\n\n        job_record = JobRecord.from_json(meta)\n        job_record.save()", "response": "Create information for given job."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _update_job_info(cls, job_dir):\n        meta_file = os.path.join(job_dir, JOB_META_FILE)\n        meta = parse_json(meta_file)\n\n        if meta:\n            logging.debug(\"Update job info for %s\" % meta[\"job_id\"])\n            JobRecord.objects \\\n                .filter(job_id=meta[\"job_id\"]) \\\n                .update(end_time=timestamp2date(meta[\"end_time\"]))", "response": "Update the job meta info for given job."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _create_trial_info(self, expr_dir):\n        meta = self._build_trial_meta(expr_dir)\n\n        self.logger.debug(\"Create trial for %s\" % meta)\n\n        trial_record = TrialRecord.from_json(meta)\n        trial_record.save()", "response": "Create trial.\n            Meta file will be loaded if exists and the trial information\n        will be saved in db backend."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate the trial information for given trial.", "response": "def _update_trial_info(self, expr_dir):\n        \"\"\"Update information for given trial.\n\n        Meta file will be loaded if exists, and the trial information\n        in db backend will be updated.\n\n        Args:\n            expr_dir(str)\n        \"\"\"\n        trial_id = expr_dir[-8:]\n\n        meta_file = os.path.join(expr_dir, EXPR_META_FILE)\n        meta = parse_json(meta_file)\n\n        result_file = os.path.join(expr_dir, EXPR_RESULT_FILE)\n        offset = self._result_offsets.get(trial_id, 0)\n        results, new_offset = parse_multiple_json(result_file, offset)\n        self._add_results(results, trial_id)\n        self._result_offsets[trial_id] = new_offset\n\n        if meta:\n            TrialRecord.objects \\\n                .filter(trial_id=trial_id) \\\n                .update(trial_status=meta[\"status\"],\n                        end_time=timestamp2date(meta.get(\"end_time\", None)))\n        elif len(results) > 0:\n            metrics = {\n                \"episode_reward\": results[-1].get(\"episode_reward_mean\", None),\n                \"accuracy\": results[-1].get(\"mean_accuracy\", None),\n                \"loss\": results[-1].get(\"loss\", None)\n            }\n            if results[-1].get(\"done\"):\n                TrialRecord.objects \\\n                    .filter(trial_id=trial_id) \\\n                    .update(trial_status=\"TERMINATED\",\n                            end_time=results[-1].get(\"date\", None),\n                            metrics=str(metrics))\n            else:\n                TrialRecord.objects \\\n                    .filter(trial_id=trial_id) \\\n                    .update(metrics=str(metrics))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild the job meta info for the job.", "response": "def _build_job_meta(cls, job_dir):\n        \"\"\"Build meta file for job.\n\n        Args:\n            job_dir (str): Directory path of the job.\n\n        Return:\n            A dict of job meta info.\n        \"\"\"\n        meta_file = os.path.join(job_dir, JOB_META_FILE)\n        meta = parse_json(meta_file)\n\n        if not meta:\n            job_name = job_dir.split(\"/\")[-1]\n            user = os.environ.get(\"USER\", None)\n            meta = {\n                \"job_id\": job_name,\n                \"job_name\": job_name,\n                \"user\": user,\n                \"type\": \"ray\",\n                \"start_time\": os.path.getctime(job_dir),\n                \"end_time\": None,\n                \"best_trial_id\": None,\n            }\n\n        if meta.get(\"start_time\", None):\n            meta[\"start_time\"] = timestamp2date(meta[\"start_time\"])\n\n        return meta"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _build_trial_meta(cls, expr_dir):\n        meta_file = os.path.join(expr_dir, EXPR_META_FILE)\n        meta = parse_json(meta_file)\n\n        if not meta:\n            job_id = expr_dir.split(\"/\")[-2]\n            trial_id = expr_dir[-8:]\n            params = parse_json(os.path.join(expr_dir, EXPR_PARARM_FILE))\n            meta = {\n                \"trial_id\": trial_id,\n                \"job_id\": job_id,\n                \"status\": \"RUNNING\",\n                \"type\": \"TUNE\",\n                \"start_time\": os.path.getctime(expr_dir),\n                \"end_time\": None,\n                \"progress_offset\": 0,\n                \"result_offset\": 0,\n                \"params\": params\n            }\n\n        if not meta.get(\"start_time\", None):\n            meta[\"start_time\"] = os.path.getctime(expr_dir)\n\n        if isinstance(meta[\"start_time\"], float):\n            meta[\"start_time\"] = timestamp2date(meta[\"start_time\"])\n\n        if meta.get(\"end_time\", None):\n            meta[\"end_time\"] = timestamp2date(meta[\"end_time\"])\n\n        meta[\"params\"] = parse_json(os.path.join(expr_dir, EXPR_PARARM_FILE))\n\n        return meta", "response": "Build the trial meta file for the current experiment."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _add_results(self, results, trial_id):\n        for result in results:\n            self.logger.debug(\"Appending result: %s\" % result)\n            result[\"trial_id\"] = trial_id\n            result_record = ResultRecord.from_json(result)\n            result_record.save()", "response": "Add a list of results into db."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_time_dimension(padded_inputs, seq_lens):\n\n    # Sequence lengths have to be specified for LSTM batch inputs. The\n    # input batch must be padded to the max seq length given here. That is,\n    # batch_size == len(seq_lens) * max(seq_lens)\n    padded_batch_size = tf.shape(padded_inputs)[0]\n    max_seq_len = padded_batch_size // tf.shape(seq_lens)[0]\n\n    # Dynamically reshape the padded batch to introduce a time dimension.\n    new_batch_size = padded_batch_size // max_seq_len\n    new_shape = ([new_batch_size, max_seq_len] +\n                 padded_inputs.get_shape().as_list()[1:])\n    return tf.reshape(padded_inputs, new_shape)", "response": "Adds a time dimension to the padded inputs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef chop_into_sequences(episode_ids,\n                        unroll_ids,\n                        agent_indices,\n                        feature_columns,\n                        state_columns,\n                        max_seq_len,\n                        dynamic_max=True,\n                        _extra_padding=0):\n    \"\"\"Truncate and pad experiences into fixed-length sequences.\n\n    Arguments:\n        episode_ids (list): List of episode ids for each step.\n        unroll_ids (list): List of identifiers for the sample batch. This is\n            used to make sure sequences are cut between sample batches.\n        agent_indices (list): List of agent ids for each step. Note that this\n            has to be combined with episode_ids for uniqueness.\n        feature_columns (list): List of arrays containing features.\n        state_columns (list): List of arrays containing LSTM state values.\n        max_seq_len (int): Max length of sequences before truncation.\n        dynamic_max (bool): Whether to dynamically shrink the max seq len.\n            For example, if max len is 20 and the actual max seq len in the\n            data is 7, it will be shrunk to 7.\n        _extra_padding (int): Add extra padding to the end of sequences.\n\n    Returns:\n        f_pad (list): Padded feature columns. These will be of shape\n            [NUM_SEQUENCES * MAX_SEQ_LEN, ...].\n        s_init (list): Initial states for each sequence, of shape\n            [NUM_SEQUENCES, ...].\n        seq_lens (list): List of sequence lengths, of shape [NUM_SEQUENCES].\n\n    Examples:\n        >>> f_pad, s_init, seq_lens = chop_into_sequences(\n                episode_ids=[1, 1, 5, 5, 5, 5],\n                unroll_ids=[4, 4, 4, 4, 4, 4],\n                agent_indices=[0, 0, 0, 0, 0, 0],\n                feature_columns=[[4, 4, 8, 8, 8, 8],\n                                 [1, 1, 0, 1, 1, 0]],\n                state_columns=[[4, 5, 4, 5, 5, 5]],\n                max_seq_len=3)\n        >>> print(f_pad)\n        [[4, 4, 0, 8, 8, 8, 8, 0, 0],\n         [1, 1, 0, 0, 1, 1, 0, 0, 0]]\n        >>> print(s_init)\n        [[4, 4, 5]]\n        >>> print(seq_lens)\n        [2, 3, 1]\n    \"\"\"\n\n    prev_id = None\n    seq_lens = []\n    seq_len = 0\n    unique_ids = np.add(\n        np.add(episode_ids, agent_indices),\n        np.array(unroll_ids) << 32)\n    for uid in unique_ids:\n        if (prev_id is not None and uid != prev_id) or \\\n                seq_len >= max_seq_len:\n            seq_lens.append(seq_len)\n            seq_len = 0\n        seq_len += 1\n        prev_id = uid\n    if seq_len:\n        seq_lens.append(seq_len)\n    assert sum(seq_lens) == len(unique_ids)\n\n    # Dynamically shrink max len as needed to optimize memory usage\n    if dynamic_max:\n        max_seq_len = max(seq_lens) + _extra_padding\n\n    feature_sequences = []\n    for f in feature_columns:\n        f = np.array(f)\n        f_pad = np.zeros((len(seq_lens) * max_seq_len, ) + np.shape(f)[1:])\n        seq_base = 0\n        i = 0\n        for l in seq_lens:\n            for seq_offset in range(l):\n                f_pad[seq_base + seq_offset] = f[i]\n                i += 1\n            seq_base += max_seq_len\n        assert i == len(unique_ids), f\n        feature_sequences.append(f_pad)\n\n    initial_states = []\n    for s in state_columns:\n        s = np.array(s)\n        s_init = []\n        i = 0\n        for l in seq_lens:\n            s_init.append(s[i])\n            i += l\n        initial_states.append(np.array(s_init))\n\n    return feature_sequences, initial_states, np.array(seq_lens)", "response": "Truncate and pad experiences into fixed - length sequences."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef explore(config, mutations, resample_probability, custom_explore_fn):\n    new_config = copy.deepcopy(config)\n    for key, distribution in mutations.items():\n        if isinstance(distribution, dict):\n            new_config.update({\n                key: explore(config[key], mutations[key], resample_probability,\n                             None)\n            })\n        elif isinstance(distribution, list):\n            if random.random() < resample_probability or \\\n                    config[key] not in distribution:\n                new_config[key] = random.choice(distribution)\n            elif random.random() > 0.5:\n                new_config[key] = distribution[max(\n                    0,\n                    distribution.index(config[key]) - 1)]\n            else:\n                new_config[key] = distribution[min(\n                    len(distribution) - 1,\n                    distribution.index(config[key]) + 1)]\n        else:\n            if random.random() < resample_probability:\n                new_config[key] = distribution()\n            elif random.random() > 0.5:\n                new_config[key] = config[key] * 1.2\n            else:\n                new_config[key] = config[key] * 0.8\n            if type(config[key]) is int:\n                new_config[key] = int(new_config[key])\n    if custom_explore_fn:\n        new_config = custom_explore_fn(new_config)\n        assert new_config is not None, \\\n            \"Custom explore fn failed to return new config\"\n    logger.info(\"[explore] perturbed config from {} -> {}\".format(\n        config, new_config))\n    return new_config", "response": "Explore a hyperparameter configuration."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a tag for the perturbed experiment.", "response": "def make_experiment_tag(orig_tag, config, mutations):\n    \"\"\"Appends perturbed params to the trial name to show in the console.\"\"\"\n\n    resolved_vars = {}\n    for k in mutations.keys():\n        resolved_vars[(\"config\", k)] = config[k]\n    return \"{}@perturbed[{}]\".format(orig_tag, format_vars(resolved_vars))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _log_config_on_step(self, trial_state, new_state, trial,\n                            trial_to_clone, new_config):\n        \"\"\"Logs transition during exploit/exploit step.\n\n        For each step, logs: [target trial tag, clone trial tag, target trial\n        iteration, clone trial iteration, old config, new config].\n        \"\"\"\n        trial_name, trial_to_clone_name = (trial_state.orig_tag,\n                                           new_state.orig_tag)\n        trial_id = \"\".join(itertools.takewhile(str.isdigit, trial_name))\n        trial_to_clone_id = \"\".join(\n            itertools.takewhile(str.isdigit, trial_to_clone_name))\n        trial_path = os.path.join(trial.local_dir,\n                                  \"pbt_policy_\" + trial_id + \".txt\")\n        trial_to_clone_path = os.path.join(\n            trial_to_clone.local_dir,\n            \"pbt_policy_\" + trial_to_clone_id + \".txt\")\n        policy = [\n            trial_name, trial_to_clone_name,\n            trial.last_result[TRAINING_ITERATION],\n            trial_to_clone.last_result[TRAINING_ITERATION],\n            trial_to_clone.config, new_config\n        ]\n        # Log to global file.\n        with open(os.path.join(trial.local_dir, \"pbt_global.txt\"), \"a+\") as f:\n            f.write(json.dumps(policy) + \"\\n\")\n        # Overwrite state in target trial from trial_to_clone.\n        if os.path.exists(trial_to_clone_path):\n            shutil.copyfile(trial_to_clone_path, trial_path)\n        # Log new exploit in target trial log.\n        with open(trial_path, \"a+\") as f:\n            f.write(json.dumps(policy) + \"\\n\")", "response": "Logs the new config for each trial."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntransfers perturbed state from trial_to_clone -> trial.", "response": "def _exploit(self, trial_executor, trial, trial_to_clone):\n        \"\"\"Transfers perturbed state from trial_to_clone -> trial.\n\n        If specified, also logs the updated hyperparam state.\"\"\"\n\n        trial_state = self._trial_state[trial]\n        new_state = self._trial_state[trial_to_clone]\n        if not new_state.last_checkpoint:\n            logger.info(\"[pbt]: no checkpoint for trial.\"\n                        \" Skip exploit for Trial {}\".format(trial))\n            return\n        new_config = explore(trial_to_clone.config, self._hyperparam_mutations,\n                             self._resample_probability,\n                             self._custom_explore_fn)\n        logger.info(\"[exploit] transferring weights from trial \"\n                    \"{} (score {}) -> {} (score {})\".format(\n                        trial_to_clone, new_state.last_score, trial,\n                        trial_state.last_score))\n\n        if self._log_config:\n            self._log_config_on_step(trial_state, new_state, trial,\n                                     trial_to_clone, new_config)\n\n        new_tag = make_experiment_tag(trial_state.orig_tag, new_config,\n                                      self._hyperparam_mutations)\n        reset_successful = trial_executor.reset_trial(trial, new_config,\n                                                      new_tag)\n        if reset_successful:\n            trial_executor.restore(\n                trial, Checkpoint.from_object(new_state.last_checkpoint))\n        else:\n            trial_executor.stop_trial(trial, stop_logger=False)\n            trial.config = new_config\n            trial.experiment_tag = new_tag\n            trial_executor.start_trial(\n                trial, Checkpoint.from_object(new_state.last_checkpoint))\n\n        self._num_perturbations += 1\n        # Transfer over the last perturbation time as well\n        trial_state.last_perturbation_time = new_state.last_perturbation_time"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning trials in the lower and upper quantile of the population.", "response": "def _quantiles(self):\n        \"\"\"Returns trials in the lower and upper `quantile` of the population.\n\n        If there is not enough data to compute this, returns empty lists.\"\"\"\n\n        trials = []\n        for trial, state in self._trial_state.items():\n            if state.last_score is not None and not trial.is_finished():\n                trials.append(trial)\n        trials.sort(key=lambda t: self._trial_state[t].last_score)\n\n        if len(trials) <= 1:\n            return [], []\n        else:\n            return (trials[:int(math.ceil(len(trials) * PBT_QUANTILE))],\n                    trials[int(math.floor(-len(trials) * PBT_QUANTILE)):])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the trial that is currently running.", "response": "def choose_trial_to_run(self, trial_runner):\n        \"\"\"Ensures all trials get fair share of time (as defined by time_attr).\n\n        This enables the PBT scheduler to support a greater number of\n        concurrent trials than can fit in the cluster at any given time.\n        \"\"\"\n\n        candidates = []\n        for trial in trial_runner.get_trials():\n            if trial.status in [Trial.PENDING, Trial.PAUSED] and \\\n                    trial_runner.has_resources(trial.resources):\n                candidates.append(trial)\n        candidates.sort(\n            key=lambda trial: self._trial_state[trial].last_perturbation_time)\n        return candidates[0] if candidates else None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the ith default key pair name and path.", "response": "def key_pair(i, region):\n    \"\"\"Returns the ith default (aws_key_pair_name, key_pair_path).\"\"\"\n    if i == 0:\n        return (\"{}_{}\".format(RAY, region),\n                os.path.expanduser(\"~/.ssh/{}_{}.pem\".format(RAY, region)))\n    return (\"{}_{}_{}\".format(RAY, i, region),\n            os.path.expanduser(\"~/.ssh/{}_{}_{}.pem\".format(RAY, i, region)))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _build_layers(self, inputs, num_outputs, options):\n\n        hiddens = options.get(\"fcnet_hiddens\")\n        activation = get_activation_fn(options.get(\"fcnet_activation\"))\n\n        with tf.name_scope(\"fc_net\"):\n            i = 1\n            last_layer = inputs\n            for size in hiddens:\n                label = \"fc{}\".format(i)\n                last_layer = slim.fully_connected(\n                    last_layer,\n                    size,\n                    weights_initializer=normc_initializer(1.0),\n                    activation_fn=activation,\n                    scope=label)\n                i += 1\n            label = \"fc_out\"\n            output = slim.fully_connected(\n                last_layer,\n                num_outputs,\n                weights_initializer=normc_initializer(0.01),\n                activation_fn=None,\n                scope=label)\n            return output, last_layer", "response": "Builds the list of layers for the given dict inputs."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef with_base_config(base_config, extra_config):\n\n    config = copy.deepcopy(base_config)\n    config.update(extra_config)\n    return config", "response": "Returns the given config dict merged with a base agent conf."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the class of a known agent given its name.", "response": "def get_agent_class(alg):\n    \"\"\"Returns the class of a known agent given its name.\"\"\"\n\n    try:\n        return _get_agent_class(alg)\n    except ImportError:\n        from ray.rllib.agents.mock import _agent_import_failed\n        return _agent_import_failed(traceback.format_exc())"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef determine_ip_address():\n    addrs = [\n        x.address for k, v in psutil.net_if_addrs().items() if k[0] == \"e\"\n        for x in v if x.family == AddressFamily.AF_INET\n    ]\n    return addrs[0]", "response": "Return the first IP address for an ethernet interface on the system."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting any changes to the log files and push updates to Redis.", "response": "def perform_iteration(self):\n        \"\"\"Get any changes to the log files and push updates to Redis.\"\"\"\n        stats = self.get_all_stats()\n\n        self.redis_client.publish(\n            self.redis_key,\n            jsonify_asdict(stats),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nraises an exception if Ray cannot serialize this class efficiently.", "response": "def check_serializable(cls):\n    \"\"\"Throws an exception if Ray cannot serialize this class efficiently.\n\n    Args:\n        cls (type): The class to be serialized.\n\n    Raises:\n        Exception: An exception is raised if Ray cannot serialize this class\n            efficiently.\n    \"\"\"\n    if is_named_tuple(cls):\n        # This case works.\n        return\n    if not hasattr(cls, \"__new__\"):\n        print(\"The class {} does not have a '__new__' attribute and is \"\n              \"probably an old-stye class. Please make it a new-style class \"\n              \"by inheriting from 'object'.\")\n        raise RayNotDictionarySerializable(\"The class {} does not have a \"\n                                           \"'__new__' attribute and is \"\n                                           \"probably an old-style class. We \"\n                                           \"do not support this. Please make \"\n                                           \"it a new-style class by \"\n                                           \"inheriting from 'object'.\"\n                                           .format(cls))\n    try:\n        obj = cls.__new__(cls)\n    except Exception:\n        raise RayNotDictionarySerializable(\"The class {} has overridden \"\n                                           \"'__new__', so Ray may not be able \"\n                                           \"to serialize it efficiently.\"\n                                           .format(cls))\n    if not hasattr(obj, \"__dict__\"):\n        raise RayNotDictionarySerializable(\"Objects of the class {} do not \"\n                                           \"have a '__dict__' attribute, so \"\n                                           \"Ray cannot serialize it \"\n                                           \"efficiently.\".format(cls))\n    if hasattr(obj, \"__slots__\"):\n        raise RayNotDictionarySerializable(\"The class {} uses '__slots__', so \"\n                                           \"Ray may not be able to serialize \"\n                                           \"it efficiently.\".format(cls))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_named_tuple(cls):\n    b = cls.__bases__\n    if len(b) != 1 or b[0] != tuple:\n        return False\n    f = getattr(cls, \"_fields\", None)\n    if not isinstance(f, tuple):\n        return False\n    return all(type(n) == str for n in f)", "response": "Return True if cls is a namedtuple and False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nregister a function or class to the registry.", "response": "def register_trainable(name, trainable):\n    \"\"\"Register a trainable function or class.\n\n    Args:\n        name (str): Name to register.\n        trainable (obj): Function or tune.Trainable class. Functions must\n            take (config, status_reporter) as arguments and will be\n            automatically converted into a class during registration.\n    \"\"\"\n\n    from ray.tune.trainable import Trainable\n    from ray.tune.function_runner import wrap_function\n\n    if isinstance(trainable, type):\n        logger.debug(\"Detected class for trainable.\")\n    elif isinstance(trainable, FunctionType):\n        logger.debug(\"Detected function for trainable.\")\n        trainable = wrap_function(trainable)\n    elif callable(trainable):\n        logger.warning(\n            \"Detected unknown callable for trainable. Converting to class.\")\n        trainable = wrap_function(trainable)\n\n    if not issubclass(trainable, Trainable):\n        raise TypeError(\"Second argument must be convertable to Trainable\",\n                        trainable)\n    _global_registry.register(TRAINABLE_CLASS, name, trainable)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef register_env(name, env_creator):\n\n    if not isinstance(env_creator, FunctionType):\n        raise TypeError(\"Second argument must be a function.\", env_creator)\n    _global_registry.register(ENV_CREATOR, name, env_creator)", "response": "Register a custom environment for use with RLlib.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning optimization stats reported from the policy graph.", "response": "def get_learner_stats(grad_info):\n    \"\"\"Return optimization stats reported from the policy graph.\n\n    Example:\n        >>> grad_info = evaluator.learn_on_batch(samples)\n        >>> print(get_stats(grad_info))\n        {\"vf_loss\": ..., \"policy_loss\": ...}\n    \"\"\"\n\n    if LEARNER_STATS_KEY in grad_info:\n        return grad_info[LEARNER_STATS_KEY]\n\n    multiagent_stats = {}\n    for k, v in grad_info.items():\n        if type(v) is dict:\n            if LEARNER_STATS_KEY in v:\n                multiagent_stats[k] = v[LEARNER_STATS_KEY]\n\n    return multiagent_stats"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngather episode metrics from PolicyEvaluator instances.", "response": "def collect_metrics(local_evaluator=None,\n                    remote_evaluators=[],\n                    timeout_seconds=180):\n    \"\"\"Gathers episode metrics from PolicyEvaluator instances.\"\"\"\n\n    episodes, num_dropped = collect_episodes(\n        local_evaluator, remote_evaluators, timeout_seconds=timeout_seconds)\n    metrics = summarize_episodes(episodes, episodes, num_dropped)\n    return metrics"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef collect_episodes(local_evaluator=None,\n                     remote_evaluators=[],\n                     timeout_seconds=180):\n    \"\"\"Gathers new episodes metrics tuples from the given evaluators.\"\"\"\n\n    pending = [\n        a.apply.remote(lambda ev: ev.get_metrics()) for a in remote_evaluators\n    ]\n    collected, _ = ray.wait(\n        pending, num_returns=len(pending), timeout=timeout_seconds * 1.0)\n    num_metric_batches_dropped = len(pending) - len(collected)\n    if pending and len(collected) == 0:\n        raise ValueError(\n            \"Timed out waiting for metrics from workers. You can configure \"\n            \"this timeout with `collect_metrics_timeout`.\")\n\n    metric_lists = ray_get_and_free(collected)\n    if local_evaluator:\n        metric_lists.append(local_evaluator.get_metrics())\n    episodes = []\n    for metrics in metric_lists:\n        episodes.extend(metrics)\n    return episodes, num_metric_batches_dropped", "response": "Gathers new episodes metrics tuples from the given evaluators."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef summarize_episodes(episodes, new_episodes, num_dropped):\n\n    if num_dropped > 0:\n        logger.warning(\"WARNING: {} workers have NOT returned metrics\".format(\n            num_dropped))\n\n    episodes, estimates = _partition(episodes)\n    new_episodes, _ = _partition(new_episodes)\n\n    episode_rewards = []\n    episode_lengths = []\n    policy_rewards = collections.defaultdict(list)\n    custom_metrics = collections.defaultdict(list)\n    perf_stats = collections.defaultdict(list)\n    for episode in episodes:\n        episode_lengths.append(episode.episode_length)\n        episode_rewards.append(episode.episode_reward)\n        for k, v in episode.custom_metrics.items():\n            custom_metrics[k].append(v)\n        for k, v in episode.perf_stats.items():\n            perf_stats[k].append(v)\n        for (_, policy_id), reward in episode.agent_rewards.items():\n            if policy_id != DEFAULT_POLICY_ID:\n                policy_rewards[policy_id].append(reward)\n    if episode_rewards:\n        min_reward = min(episode_rewards)\n        max_reward = max(episode_rewards)\n    else:\n        min_reward = float(\"nan\")\n        max_reward = float(\"nan\")\n    avg_reward = np.mean(episode_rewards)\n    avg_length = np.mean(episode_lengths)\n\n    for policy_id, rewards in policy_rewards.copy().items():\n        policy_rewards[policy_id] = np.mean(rewards)\n\n    for k, v_list in custom_metrics.copy().items():\n        custom_metrics[k + \"_mean\"] = np.mean(v_list)\n        filt = [v for v in v_list if not np.isnan(v)]\n        if filt:\n            custom_metrics[k + \"_min\"] = np.min(filt)\n            custom_metrics[k + \"_max\"] = np.max(filt)\n        else:\n            custom_metrics[k + \"_min\"] = float(\"nan\")\n            custom_metrics[k + \"_max\"] = float(\"nan\")\n        del custom_metrics[k]\n\n    for k, v_list in perf_stats.copy().items():\n        perf_stats[k] = np.mean(v_list)\n\n    estimators = collections.defaultdict(lambda: collections.defaultdict(list))\n    for e in estimates:\n        acc = estimators[e.estimator_name]\n        for k, v in e.metrics.items():\n            acc[k].append(v)\n    for name, metrics in estimators.items():\n        for k, v_list in metrics.items():\n            metrics[k] = np.mean(v_list)\n        estimators[name] = dict(metrics)\n\n    return dict(\n        episode_reward_max=max_reward,\n        episode_reward_min=min_reward,\n        episode_reward_mean=avg_reward,\n        episode_len_mean=avg_length,\n        episodes_this_iter=len(new_episodes),\n        policy_reward_mean=dict(policy_rewards),\n        custom_metrics=dict(custom_metrics),\n        sampler_perf=dict(perf_stats),\n        off_policy_estimator=dict(estimators),\n        num_metric_batches_dropped=num_dropped)", "response": "Summarizes a set of episode metrics tuples."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndivide metrics data into true rollouts vs off - policy estimates.", "response": "def _partition(episodes):\n    \"\"\"Divides metrics data into true rollouts vs off-policy estimates.\"\"\"\n\n    from ray.rllib.evaluation.sampler import RolloutMetrics\n\n    rollouts, estimates = [], []\n    for e in episodes:\n        if isinstance(e, RolloutMetrics):\n            rollouts.append(e)\n        elif isinstance(e, OffPolicyEstimate):\n            estimates.append(e)\n        else:\n            raise ValueError(\"Unknown metric type: {}\".format(e))\n    return rollouts, estimates"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_status(self, trial, status):\n        trial.status = status\n        if status in [Trial.TERMINATED, Trial.ERROR]:\n            self.try_checkpoint_metadata(trial)", "response": "Sets trial status and checkpoints metadata if needed."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntries to checkpoint the trial metadata.", "response": "def try_checkpoint_metadata(self, trial):\n        \"\"\"Checkpoints metadata.\n\n        Args:\n            trial (Trial): Trial to checkpoint.\n        \"\"\"\n        if trial._checkpoint.storage == Checkpoint.MEMORY:\n            logger.debug(\"Not saving data for trial w/ memory checkpoint.\")\n            return\n        try:\n            logger.debug(\"Saving trial metadata.\")\n            self._cached_trial_state[trial.trial_id] = trial.__getstate__()\n        except Exception:\n            logger.exception(\"Error checkpointing trial metadata.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npauses the trial. We want to release resources (specifically GPUs) when pausing an experiment. This results in PAUSED state that similar to TERMINATED.", "response": "def pause_trial(self, trial):\n        \"\"\"Pauses the trial.\n\n        We want to release resources (specifically GPUs) when pausing an\n        experiment. This results in PAUSED state that similar to TERMINATED.\n        \"\"\"\n        assert trial.status == Trial.RUNNING, trial.status\n        try:\n            self.save(trial, Checkpoint.MEMORY)\n            self.stop_trial(trial, stop_logger=False)\n            self.set_status(trial, Trial.PAUSED)\n        except Exception:\n            logger.exception(\"Error pausing runner.\")\n            self.set_status(trial, Trial.ERROR)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the trial to PENDING to allow scheduler to start.", "response": "def unpause_trial(self, trial):\n        \"\"\"Sets PAUSED trial to pending to allow scheduler to start.\"\"\"\n        assert trial.status == Trial.PAUSED, trial.status\n        self.set_status(trial, Trial.PENDING)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef resume_trial(self, trial):\n\n        assert trial.status == Trial.PAUSED, trial.status\n        self.start_trial(trial)", "response": "Resumes PAUSED trials. This is a blocking call."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef on_trial_complete(self,\n                          trial_id,\n                          result=None,\n                          error=False,\n                          early_terminated=False):\n        \"\"\"Passes the result to Nevergrad unless early terminated or errored.\n\n        The result is internally negated when interacting with Nevergrad\n        so that Nevergrad Optimizers can \"maximize\" this value,\n        as it minimizes on default.\n        \"\"\"\n        ng_trial_info = self._live_trial_mapping.pop(trial_id)\n        if result:\n            self._nevergrad_opt.tell(ng_trial_info, -result[self._reward_attr])", "response": "Passes the result to Nevergrad unless early terminated or errored."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstarts the import thread.", "response": "def start(self):\n        \"\"\"Start the import thread.\"\"\"\n        self.t = threading.Thread(target=self._run, name=\"ray_import_thread\")\n        # Making the thread a daemon causes it to exit\n        # when the main thread exits.\n        self.t.daemon = True\n        self.t.start()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _process_key(self, key):\n        # Handle the driver case first.\n        if self.mode != ray.WORKER_MODE:\n            if key.startswith(b\"FunctionsToRun\"):\n                with profiling.profile(\"fetch_and_run_function\"):\n                    self.fetch_and_execute_function_to_run(key)\n            # Return because FunctionsToRun are the only things that\n            # the driver should import.\n            return\n\n        if key.startswith(b\"RemoteFunction\"):\n            with profiling.profile(\"register_remote_function\"):\n                (self.worker.function_actor_manager.\n                 fetch_and_register_remote_function(key))\n        elif key.startswith(b\"FunctionsToRun\"):\n            with profiling.profile(\"fetch_and_run_function\"):\n                self.fetch_and_execute_function_to_run(key)\n        elif key.startswith(b\"ActorClass\"):\n            # Keep track of the fact that this actor class has been\n            # exported so that we know it is safe to turn this worker\n            # into an actor of that class.\n            self.worker.function_actor_manager.imported_actor_classes.add(key)\n        # TODO(rkn): We may need to bring back the case of\n        # fetching actor classes here.\n        else:\n            raise Exception(\"This code should be unreachable.\")", "response": "Process the given export key from redis."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fetch_and_execute_function_to_run(self, key):\n        (driver_id, serialized_function,\n         run_on_other_drivers) = self.redis_client.hmget(\n             key, [\"driver_id\", \"function\", \"run_on_other_drivers\"])\n\n        if (utils.decode(run_on_other_drivers) == \"False\"\n                and self.worker.mode == ray.SCRIPT_MODE\n                and driver_id != self.worker.task_driver_id.binary()):\n            return\n\n        try:\n            # Deserialize the function.\n            function = pickle.loads(serialized_function)\n            # Run the function.\n            function({\"worker\": self.worker})\n        except Exception:\n            # If an exception was thrown when the function was run, we record\n            # the traceback and notify the scheduler of the failure.\n            traceback_str = traceback.format_exc()\n            # Log the error message.\n            utils.push_error_to_driver(\n                self.worker,\n                ray_constants.FUNCTION_TO_RUN_PUSH_ERROR,\n                traceback_str,\n                driver_id=ray.DriverID(driver_id))", "response": "Fetch and execute a function on the worker."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nclip the given action to the specified range of this policy.", "response": "def clip_action(action, space):\n    \"\"\"Called to clip actions to the specified range of this policy.\n\n    Arguments:\n        action: Single action.\n        space: Action space the actions should be present in.\n\n    Returns:\n        Clipped batch of actions.\n    \"\"\"\n\n    if isinstance(space, gym.spaces.Box):\n        return np.clip(action, space.low, space.high)\n    elif isinstance(space, gym.spaces.Tuple):\n        if type(action) not in (tuple, list):\n            raise ValueError(\"Expected tuple space for actions {}: {}\".format(\n                action, space))\n        out = []\n        for a, s in zip(action, space.spaces):\n            out.append(clip_action(a, s))\n        return out\n    else:\n        return action"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef on_trial_complete(self,\n                          trial_id,\n                          result=None,\n                          error=False,\n                          early_terminated=False):\n        \"\"\"Passes the result to skopt unless early terminated or errored.\n\n        The result is internally negated when interacting with Skopt\n        so that Skopt Optimizers can \"maximize\" this value,\n        as it minimizes on default.\n        \"\"\"\n        skopt_trial_info = self._live_trial_mapping.pop(trial_id)\n        if result:\n            self._skopt_opt.tell(skopt_trial_info, -result[self._reward_attr])", "response": "Passes the result to skopt unless early terminated or errored."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting a hostname to a numerical IP addresses in an address.", "response": "def address_to_ip(address):\n    \"\"\"Convert a hostname to a numerical IP addresses in an address.\n\n    This should be a no-op if address already contains an actual numerical IP\n    address.\n\n    Args:\n        address: This can be either a string containing a hostname (or an IP\n            address) and a port or it can be just an IP address.\n\n    Returns:\n        The same address but with the hostname replaced by a numerical IP\n            address.\n    \"\"\"\n    address_parts = address.split(\":\")\n    ip_address = socket.gethostbyname(address_parts[0])\n    # Make sure localhost isn't resolved to the loopback ip\n    if ip_address == \"127.0.0.1\":\n        ip_address = get_node_ip_address()\n    return \":\".join([ip_address] + address_parts[1:])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_node_ip_address(address=\"8.8.8.8:53\"):\n    ip_address, port = address.split(\":\")\n    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    try:\n        # This command will raise an exception if there is no internet\n        # connection.\n        s.connect((ip_address, int(port)))\n        node_ip_address = s.getsockname()[0]\n    except Exception as e:\n        node_ip_address = \"127.0.0.1\"\n        # [Errno 101] Network is unreachable\n        if e.errno == 101:\n            try:\n                # try get node ip address from host name\n                host_name = socket.getfqdn(socket.gethostname())\n                node_ip_address = socket.gethostbyname(host_name)\n            except Exception:\n                pass\n    finally:\n        s.close()\n\n    return node_ip_address", "response": "This function returns the IP address of the current node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a Redis client.", "response": "def create_redis_client(redis_address, password=None):\n    \"\"\"Create a Redis client.\n\n    Args:\n        The IP address, port, and password of the Redis server.\n\n    Returns:\n        A Redis client.\n    \"\"\"\n    redis_ip_address, redis_port = redis_address.split(\":\")\n    # For this command to work, some other client (on the same machine\n    # as Redis) must have run \"CONFIG SET protected-mode no\".\n    return redis.StrictRedis(\n        host=redis_ip_address, port=int(redis_port), password=password)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef start_ray_process(command,\n                      process_type,\n                      env_updates=None,\n                      cwd=None,\n                      use_valgrind=False,\n                      use_gdb=False,\n                      use_valgrind_profiler=False,\n                      use_perftools_profiler=False,\n                      use_tmux=False,\n                      stdout_file=None,\n                      stderr_file=None):\n    \"\"\"Start one of the Ray processes.\n\n    TODO(rkn): We need to figure out how these commands interact. For example,\n    it may only make sense to start a process in gdb if we also start it in\n    tmux. Similarly, certain combinations probably don't make sense, like\n    simultaneously running the process in valgrind and the profiler.\n\n    Args:\n        command (List[str]): The command to use to start the Ray process.\n        process_type (str): The type of the process that is being started\n            (e.g., \"raylet\").\n        env_updates (dict): A dictionary of additional environment variables to\n            run the command with (in addition to the caller's environment\n            variables).\n        cwd (str): The directory to run the process in.\n        use_valgrind (bool): True if we should start the process in valgrind.\n        use_gdb (bool): True if we should start the process in gdb.\n        use_valgrind_profiler (bool): True if we should start the process in\n            the valgrind profiler.\n        use_perftools_profiler (bool): True if we should profile the process\n            using perftools.\n        use_tmux (bool): True if we should start the process in tmux.\n        stdout_file: A file handle opened for writing to redirect stdout to. If\n            no redirection should happen, then this should be None.\n        stderr_file: A file handle opened for writing to redirect stderr to. If\n            no redirection should happen, then this should be None.\n\n    Returns:\n        Information about the process that was started including a handle to\n            the process that was started.\n    \"\"\"\n    # Detect which flags are set through environment variables.\n    valgrind_env_var = \"RAY_{}_VALGRIND\".format(process_type.upper())\n    if os.environ.get(valgrind_env_var) == \"1\":\n        logger.info(\"Detected environment variable '%s'.\", valgrind_env_var)\n        use_valgrind = True\n    valgrind_profiler_env_var = \"RAY_{}_VALGRIND_PROFILER\".format(\n        process_type.upper())\n    if os.environ.get(valgrind_profiler_env_var) == \"1\":\n        logger.info(\"Detected environment variable '%s'.\",\n                    valgrind_profiler_env_var)\n        use_valgrind_profiler = True\n    perftools_profiler_env_var = \"RAY_{}_PERFTOOLS_PROFILER\".format(\n        process_type.upper())\n    if os.environ.get(perftools_profiler_env_var) == \"1\":\n        logger.info(\"Detected environment variable '%s'.\",\n                    perftools_profiler_env_var)\n        use_perftools_profiler = True\n    tmux_env_var = \"RAY_{}_TMUX\".format(process_type.upper())\n    if os.environ.get(tmux_env_var) == \"1\":\n        logger.info(\"Detected environment variable '%s'.\", tmux_env_var)\n        use_tmux = True\n    gdb_env_var = \"RAY_{}_GDB\".format(process_type.upper())\n    if os.environ.get(gdb_env_var) == \"1\":\n        logger.info(\"Detected environment variable '%s'.\", gdb_env_var)\n        use_gdb = True\n\n    if sum(\n        [use_gdb, use_valgrind, use_valgrind_profiler, use_perftools_profiler\n         ]) > 1:\n        raise ValueError(\n            \"At most one of the 'use_gdb', 'use_valgrind', \"\n            \"'use_valgrind_profiler', and 'use_perftools_profiler' flags can \"\n            \"be used at a time.\")\n    if env_updates is None:\n        env_updates = {}\n    if not isinstance(env_updates, dict):\n        raise ValueError(\"The 'env_updates' argument must be a dictionary.\")\n\n    modified_env = os.environ.copy()\n    modified_env.update(env_updates)\n\n    if use_gdb:\n        if not use_tmux:\n            raise ValueError(\n                \"If 'use_gdb' is true, then 'use_tmux' must be true as well.\")\n\n        # TODO(suquark): Any better temp file creation here?\n        gdb_init_path = \"/tmp/ray/gdb_init_{}_{}\".format(\n            process_type, time.time())\n        ray_process_path = command[0]\n        ray_process_args = command[1:]\n        run_args = \" \".join([\"'{}'\".format(arg) for arg in ray_process_args])\n        with open(gdb_init_path, \"w\") as gdb_init_file:\n            gdb_init_file.write(\"run {}\".format(run_args))\n        command = [\"gdb\", ray_process_path, \"-x\", gdb_init_path]\n\n    if use_valgrind:\n        command = [\n            \"valgrind\", \"--track-origins=yes\", \"--leak-check=full\",\n            \"--show-leak-kinds=all\", \"--leak-check-heuristics=stdstring\",\n            \"--error-exitcode=1\"\n        ] + command\n\n    if use_valgrind_profiler:\n        command = [\"valgrind\", \"--tool=callgrind\"] + command\n\n    if use_perftools_profiler:\n        modified_env[\"LD_PRELOAD\"] = os.environ[\"PERFTOOLS_PATH\"]\n        modified_env[\"CPUPROFILE\"] = os.environ[\"PERFTOOLS_LOGFILE\"]\n\n    if use_tmux:\n        # The command has to be created exactly as below to ensure that it\n        # works on all versions of tmux. (Tested with tmux 1.8-5, travis'\n        # version, and tmux 2.1)\n        command = [\"tmux\", \"new-session\", \"-d\", \"{}\".format(\" \".join(command))]\n\n    process = subprocess.Popen(\n        command,\n        env=modified_env,\n        cwd=cwd,\n        stdout=stdout_file,\n        stderr=stderr_file)\n\n    return ProcessInfo(\n        process=process,\n        stdout_file=stdout_file.name if stdout_file is not None else None,\n        stderr_file=stderr_file.name if stderr_file is not None else None,\n        use_valgrind=use_valgrind,\n        use_gdb=use_gdb,\n        use_valgrind_profiler=use_valgrind_profiler,\n        use_perftools_profiler=use_perftools_profiler,\n        use_tmux=use_tmux)", "response": "Start one Ray process in valgrind and tmux."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwaits for a Redis server to be available.", "response": "def wait_for_redis_to_start(redis_ip_address,\n                            redis_port,\n                            password=None,\n                            num_retries=5):\n    \"\"\"Wait for a Redis server to be available.\n\n    This is accomplished by creating a Redis client and sending a random\n    command to the server until the command gets through.\n\n    Args:\n        redis_ip_address (str): The IP address of the redis server.\n        redis_port (int): The port of the redis server.\n        password (str): The password of the redis server.\n        num_retries (int): The number of times to try connecting with redis.\n            The client will sleep for one second between attempts.\n\n    Raises:\n        Exception: An exception is raised if we could not connect with Redis.\n    \"\"\"\n    redis_client = redis.StrictRedis(\n        host=redis_ip_address, port=redis_port, password=password)\n    # Wait for the Redis server to start.\n    counter = 0\n    while counter < num_retries:\n        try:\n            # Run some random command and see if it worked.\n            logger.info(\n                \"Waiting for redis server at {}:{} to respond...\".format(\n                    redis_ip_address, redis_port))\n            redis_client.client_list()\n        except redis.ConnectionError:\n            # Wait a little bit.\n            time.sleep(1)\n            logger.info(\"Failed to connect to the redis server, retrying.\")\n            counter += 1\n        else:\n            break\n    if counter == num_retries:\n        raise Exception(\"Unable to connect to Redis. If the Redis instance is \"\n                        \"on a different machine, check that your firewall is \"\n                        \"configured properly.\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nattempting to detect the number of GPUs on this machine.", "response": "def _autodetect_num_gpus():\n    \"\"\"Attempt to detect the number of GPUs on this machine.\n\n    TODO(rkn): This currently assumes Nvidia GPUs and Linux.\n\n    Returns:\n        The number of GPUs if any were detected, otherwise 0.\n    \"\"\"\n    proc_gpus_path = \"/proc/driver/nvidia/gpus\"\n    if os.path.isdir(proc_gpus_path):\n        return len(os.listdir(proc_gpus_path))\n    return 0"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _compute_version_info():\n    ray_version = ray.__version__\n    python_version = \".\".join(map(str, sys.version_info[:3]))\n    pyarrow_version = pyarrow.__version__\n    return ray_version, python_version, pyarrow_version", "response": "Compute the versions of Python pyarrow and Ray.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_version_info(redis_client):\n    redis_reply = redis_client.get(\"VERSION_INFO\")\n\n    # Don't do the check if there is no version information in Redis. This\n    # is to make it easier to do things like start the processes by hand.\n    if redis_reply is None:\n        return\n\n    true_version_info = tuple(json.loads(ray.utils.decode(redis_reply)))\n    version_info = _compute_version_info()\n    if version_info != true_version_info:\n        node_ip_address = ray.services.get_node_ip_address()\n        error_message = (\"Version mismatch: The cluster was started with:\\n\"\n                         \"    Ray: \" + true_version_info[0] + \"\\n\"\n                         \"    Python: \" + true_version_info[1] + \"\\n\"\n                         \"    Pyarrow: \" + str(true_version_info[2]) + \"\\n\"\n                         \"This process on node \" + node_ip_address +\n                         \" was started with:\" + \"\\n\"\n                         \"    Ray: \" + version_info[0] + \"\\n\"\n                         \"    Python: \" + version_info[1] + \"\\n\"\n                         \"    Pyarrow: \" + str(version_info[2]))\n        if version_info[:2] != true_version_info[:2]:\n            raise Exception(error_message)\n        else:\n            logger.warning(error_message)", "response": "Check if the version info of this process is correct."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstarts the Redis global state store.", "response": "def start_redis(node_ip_address,\n                redirect_files,\n                port=None,\n                redis_shard_ports=None,\n                num_redis_shards=1,\n                redis_max_clients=None,\n                redirect_worker_output=False,\n                password=None,\n                use_credis=None,\n                redis_max_memory=None,\n                include_java=False):\n    \"\"\"Start the Redis global state store.\n\n    Args:\n        node_ip_address: The IP address of the current node. This is only used\n            for recording the log filenames in Redis.\n        redirect_files: The list of (stdout, stderr) file pairs.\n        port (int): If provided, the primary Redis shard will be started on\n            this port.\n        redis_shard_ports: A list of the ports to use for the non-primary Redis\n            shards.\n        num_redis_shards (int): If provided, the number of Redis shards to\n            start, in addition to the primary one. The default value is one\n            shard.\n        redis_max_clients: If this is provided, Ray will attempt to configure\n            Redis with this maxclients number.\n        redirect_worker_output (bool): True if worker output should be\n            redirected to a file and false otherwise. Workers will have access\n            to this value when they start up.\n        password (str): Prevents external clients without the password\n            from connecting to Redis if provided.\n        use_credis: If True, additionally load the chain-replicated libraries\n            into the redis servers.  Defaults to None, which means its value is\n            set by the presence of \"RAY_USE_NEW_GCS\" in os.environ.\n        redis_max_memory: The max amount of memory (in bytes) to allow each\n            redis shard to use. Once the limit is exceeded, redis will start\n            LRU eviction of entries. This only applies to the sharded redis\n            tables (task, object, and profile tables). By default, this is\n            capped at 10GB but can be set higher.\n        include_java (bool): If True, the raylet backend can also support\n            Java worker.\n\n    Returns:\n        A tuple of the address for the primary Redis shard, a list of\n            addresses for the remaining shards, and the processes that were\n            started.\n    \"\"\"\n\n    if len(redirect_files) != 1 + num_redis_shards:\n        raise ValueError(\"The number of redirect file pairs should be equal \"\n                         \"to the number of redis shards (including the \"\n                         \"primary shard) we will start.\")\n    if redis_shard_ports is None:\n        redis_shard_ports = num_redis_shards * [None]\n    elif len(redis_shard_ports) != num_redis_shards:\n        raise Exception(\"The number of Redis shard ports does not match the \"\n                        \"number of Redis shards.\")\n\n    processes = []\n\n    if use_credis is None:\n        use_credis = (\"RAY_USE_NEW_GCS\" in os.environ)\n    if use_credis:\n        if password is not None:\n            # TODO(pschafhalter) remove this once credis supports\n            # authenticating Redis ports\n            raise Exception(\"Setting the `redis_password` argument is not \"\n                            \"supported in credis. To run Ray with \"\n                            \"password-protected Redis ports, ensure that \"\n                            \"the environment variable `RAY_USE_NEW_GCS=off`.\")\n        assert num_redis_shards == 1, (\n            \"For now, RAY_USE_NEW_GCS supports 1 shard, and credis \"\n            \"supports 1-node chain for that shard only.\")\n\n    if use_credis:\n        redis_executable = CREDIS_EXECUTABLE\n        # TODO(suquark): We need credis here because some symbols need to be\n        # imported from credis dynamically through dlopen when Ray is built\n        # with RAY_USE_NEW_GCS=on. We should remove them later for the primary\n        # shard.\n        # See src/ray/gcs/redis_module/ray_redis_module.cc\n        redis_modules = [CREDIS_MASTER_MODULE, REDIS_MODULE]\n    else:\n        redis_executable = REDIS_EXECUTABLE\n        redis_modules = [REDIS_MODULE]\n\n    redis_stdout_file, redis_stderr_file = redirect_files[0]\n    # Start the primary Redis shard.\n    port, p = _start_redis_instance(\n        redis_executable,\n        modules=redis_modules,\n        port=port,\n        password=password,\n        redis_max_clients=redis_max_clients,\n        # Below we use None to indicate no limit on the memory of the\n        # primary Redis shard.\n        redis_max_memory=None,\n        stdout_file=redis_stdout_file,\n        stderr_file=redis_stderr_file)\n    processes.append(p)\n    redis_address = address(node_ip_address, port)\n\n    # Register the number of Redis shards in the primary shard, so that clients\n    # know how many redis shards to expect under RedisShards.\n    primary_redis_client = redis.StrictRedis(\n        host=node_ip_address, port=port, password=password)\n    primary_redis_client.set(\"NumRedisShards\", str(num_redis_shards))\n\n    # Put the redirect_worker_output bool in the Redis shard so that workers\n    # can access it and know whether or not to redirect their output.\n    primary_redis_client.set(\"RedirectOutput\", 1\n                             if redirect_worker_output else 0)\n\n    # put the include_java bool to primary redis-server, so that other nodes\n    # can access it and know whether or not to enable cross-languages.\n    primary_redis_client.set(\"INCLUDE_JAVA\", 1 if include_java else 0)\n\n    # Store version information in the primary Redis shard.\n    _put_version_info_in_redis(primary_redis_client)\n\n    # Calculate the redis memory.\n    system_memory = ray.utils.get_system_memory()\n    if redis_max_memory is None:\n        redis_max_memory = min(\n            ray_constants.DEFAULT_REDIS_MAX_MEMORY_BYTES,\n            max(\n                int(system_memory * 0.2),\n                ray_constants.REDIS_MINIMUM_MEMORY_BYTES))\n    if redis_max_memory < ray_constants.REDIS_MINIMUM_MEMORY_BYTES:\n        raise ValueError(\"Attempting to cap Redis memory usage at {} bytes, \"\n                         \"but the minimum allowed is {} bytes.\".format(\n                             redis_max_memory,\n                             ray_constants.REDIS_MINIMUM_MEMORY_BYTES))\n\n    # Start other Redis shards. Each Redis shard logs to a separate file,\n    # prefixed by \"redis-<shard number>\".\n    redis_shards = []\n    for i in range(num_redis_shards):\n        redis_stdout_file, redis_stderr_file = redirect_files[i + 1]\n        if use_credis:\n            redis_executable = CREDIS_EXECUTABLE\n            # It is important to load the credis module BEFORE the ray module,\n            # as the latter contains an extern declaration that the former\n            # supplies.\n            redis_modules = [CREDIS_MEMBER_MODULE, REDIS_MODULE]\n        else:\n            redis_executable = REDIS_EXECUTABLE\n            redis_modules = [REDIS_MODULE]\n\n        redis_shard_port, p = _start_redis_instance(\n            redis_executable,\n            modules=redis_modules,\n            port=redis_shard_ports[i],\n            password=password,\n            redis_max_clients=redis_max_clients,\n            redis_max_memory=redis_max_memory,\n            stdout_file=redis_stdout_file,\n            stderr_file=redis_stderr_file)\n        processes.append(p)\n\n        shard_address = address(node_ip_address, redis_shard_port)\n        redis_shards.append(shard_address)\n        # Store redis shard information in the primary redis shard.\n        primary_redis_client.rpush(\"RedisShards\", shard_address)\n\n    if use_credis:\n        # Configure the chain state. The way it is intended to work is\n        # the following:\n        #\n        # PRIMARY_SHARD\n        #\n        # SHARD_1 (master replica) -> SHARD_1 (member replica)\n        #                                        -> SHARD_1 (member replica)\n        #\n        # SHARD_2 (master replica) -> SHARD_2 (member replica)\n        #                                        -> SHARD_2 (member replica)\n        # ...\n        #\n        #\n        # If we have credis members in future, their modules should be:\n        # [CREDIS_MEMBER_MODULE, REDIS_MODULE], and they will be initialized by\n        # execute_command(\"MEMBER.CONNECT_TO_MASTER\", node_ip_address, port)\n        #\n        # Currently we have num_redis_shards == 1, so only one chain will be\n        # created, and the chain only contains master.\n\n        # TODO(suquark): Currently, this is not correct because we are\n        # using the master replica as the primary shard. This should be\n        # fixed later. I had tried to fix it but failed because of heartbeat\n        # issues.\n        primary_client = redis.StrictRedis(\n            host=node_ip_address, port=port, password=password)\n        shard_client = redis.StrictRedis(\n            host=node_ip_address, port=redis_shard_port, password=password)\n        primary_client.execute_command(\"MASTER.ADD\", node_ip_address,\n                                       redis_shard_port)\n        shard_client.execute_command(\"MEMBER.CONNECT_TO_MASTER\",\n                                     node_ip_address, port)\n\n    return redis_address, redis_shards, processes"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstart a Redis server.", "response": "def _start_redis_instance(executable,\n                          modules,\n                          port=None,\n                          redis_max_clients=None,\n                          num_retries=20,\n                          stdout_file=None,\n                          stderr_file=None,\n                          password=None,\n                          redis_max_memory=None):\n    \"\"\"Start a single Redis server.\n\n    Notes:\n        If \"port\" is not None, then we will only use this port and try\n        only once. Otherwise, random ports will be used and the maximum\n        retries count is \"num_retries\".\n\n    Args:\n        executable (str): Full path of the redis-server executable.\n        modules (list of str): A list of pathnames, pointing to the redis\n            module(s) that will be loaded in this redis server.\n        port (int): If provided, start a Redis server with this port.\n        redis_max_clients: If this is provided, Ray will attempt to configure\n            Redis with this maxclients number.\n        num_retries (int): The number of times to attempt to start Redis. If a\n            port is provided, this defaults to 1.\n        stdout_file: A file handle opened for writing to redirect stdout to. If\n            no redirection should happen, then this should be None.\n        stderr_file: A file handle opened for writing to redirect stderr to. If\n            no redirection should happen, then this should be None.\n        password (str): Prevents external clients without the password\n            from connecting to Redis if provided.\n        redis_max_memory: The max amount of memory (in bytes) to allow redis\n            to use, or None for no limit. Once the limit is exceeded, redis\n            will start LRU eviction of entries.\n\n    Returns:\n        A tuple of the port used by Redis and ProcessInfo for the process that\n            was started. If a port is passed in, then the returned port value\n            is the same.\n\n    Raises:\n        Exception: An exception is raised if Redis could not be started.\n    \"\"\"\n    assert os.path.isfile(executable)\n    for module in modules:\n        assert os.path.isfile(module)\n    counter = 0\n    if port is not None:\n        # If a port is specified, then try only once to connect.\n        # This ensures that we will use the given port.\n        num_retries = 1\n    else:\n        port = new_port()\n\n    load_module_args = []\n    for module in modules:\n        load_module_args += [\"--loadmodule\", module]\n\n    while counter < num_retries:\n        if counter > 0:\n            logger.warning(\"Redis failed to start, retrying now.\")\n\n        # Construct the command to start the Redis server.\n        command = [executable]\n        if password:\n            command += [\"--requirepass\", password]\n        command += (\n            [\"--port\", str(port), \"--loglevel\", \"warning\"] + load_module_args)\n        process_info = start_ray_process(\n            command,\n            ray_constants.PROCESS_TYPE_REDIS_SERVER,\n            stdout_file=stdout_file,\n            stderr_file=stderr_file)\n        time.sleep(0.1)\n        # Check if Redis successfully started (or at least if it the executable\n        # did not exit within 0.1 seconds).\n        if process_info.process.poll() is None:\n            break\n        port = new_port()\n        counter += 1\n    if counter == num_retries:\n        raise Exception(\"Couldn't start Redis. Check log files: {} {}\".format(\n            stdout_file.name, stderr_file.name))\n\n    # Create a Redis client just for configuring Redis.\n    redis_client = redis.StrictRedis(\n        host=\"127.0.0.1\", port=port, password=password)\n    # Wait for the Redis server to start.\n    wait_for_redis_to_start(\"127.0.0.1\", port, password=password)\n    # Configure Redis to generate keyspace notifications. TODO(rkn): Change\n    # this to only generate notifications for the export keys.\n    redis_client.config_set(\"notify-keyspace-events\", \"Kl\")\n\n    # Configure Redis to not run in protected mode so that processes on other\n    # hosts can connect to it. TODO(rkn): Do this in a more secure way.\n    redis_client.config_set(\"protected-mode\", \"no\")\n\n    # Discard old task and object metadata.\n    if redis_max_memory is not None:\n        redis_client.config_set(\"maxmemory\", str(redis_max_memory))\n        redis_client.config_set(\"maxmemory-policy\", \"allkeys-lru\")\n        redis_client.config_set(\"maxmemory-samples\", \"10\")\n        logger.info(\"Starting Redis shard with {} GB max memory.\".format(\n            round(redis_max_memory / 1e9, 2)))\n\n    # If redis_max_clients is provided, attempt to raise the number of maximum\n    # number of Redis clients.\n    if redis_max_clients is not None:\n        redis_client.config_set(\"maxclients\", str(redis_max_clients))\n    else:\n        # If redis_max_clients is not provided, determine the current ulimit.\n        # We will use this to attempt to raise the maximum number of Redis\n        # clients.\n        current_max_clients = int(\n            redis_client.config_get(\"maxclients\")[\"maxclients\"])\n        # The below command should be the same as doing ulimit -n.\n        ulimit_n = resource.getrlimit(resource.RLIMIT_NOFILE)[0]\n        # The quantity redis_client_buffer appears to be the required buffer\n        # between the maximum number of redis clients and ulimit -n. That is,\n        # if ulimit -n returns 10000, then we can set maxclients to\n        # 10000 - redis_client_buffer.\n        redis_client_buffer = 32\n        if current_max_clients < ulimit_n - redis_client_buffer:\n            redis_client.config_set(\"maxclients\",\n                                    ulimit_n - redis_client_buffer)\n\n    # Increase the hard and soft limits for the redis client pubsub buffer to\n    # 128MB. This is a hack to make it less likely for pubsub messages to be\n    # dropped and for pubsub connections to therefore be killed.\n    cur_config = (redis_client.config_get(\"client-output-buffer-limit\")[\n        \"client-output-buffer-limit\"])\n    cur_config_list = cur_config.split()\n    assert len(cur_config_list) == 12\n    cur_config_list[8:] = [\"pubsub\", \"134217728\", \"134217728\", \"60\"]\n    redis_client.config_set(\"client-output-buffer-limit\",\n                            \" \".join(cur_config_list))\n    # Put a time stamp in Redis to indicate when it was started.\n    redis_client.set(\"redis_start_time\", time.time())\n    return port, process_info"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef start_log_monitor(redis_address,\n                      logs_dir,\n                      stdout_file=None,\n                      stderr_file=None,\n                      redis_password=None):\n    \"\"\"Start a log monitor process.\n\n    Args:\n        redis_address (str): The address of the Redis instance.\n        logs_dir (str): The directory of logging files.\n        stdout_file: A file handle opened for writing to redirect stdout to. If\n            no redirection should happen, then this should be None.\n        stderr_file: A file handle opened for writing to redirect stderr to. If\n            no redirection should happen, then this should be None.\n        redis_password (str): The password of the redis server.\n\n    Returns:\n        ProcessInfo for the process that was started.\n    \"\"\"\n    log_monitor_filepath = os.path.join(\n        os.path.dirname(os.path.abspath(__file__)), \"log_monitor.py\")\n    command = [\n        sys.executable, \"-u\", log_monitor_filepath,\n        \"--redis-address={}\".format(redis_address),\n        \"--logs-dir={}\".format(logs_dir)\n    ]\n    if redis_password:\n        command += [\"--redis-password\", redis_password]\n    process_info = start_ray_process(\n        command,\n        ray_constants.PROCESS_TYPE_LOG_MONITOR,\n        stdout_file=stdout_file,\n        stderr_file=stderr_file)\n    return process_info", "response": "Start a log monitor process."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nstarting a new reporter process.", "response": "def start_reporter(redis_address,\n                   stdout_file=None,\n                   stderr_file=None,\n                   redis_password=None):\n    \"\"\"Start a reporter process.\n\n    Args:\n        redis_address (str): The address of the Redis instance.\n        stdout_file: A file handle opened for writing to redirect stdout to. If\n            no redirection should happen, then this should be None.\n        stderr_file: A file handle opened for writing to redirect stderr to. If\n            no redirection should happen, then this should be None.\n        redis_password (str): The password of the redis server.\n\n    Returns:\n        ProcessInfo for the process that was started.\n    \"\"\"\n    reporter_filepath = os.path.join(\n        os.path.dirname(os.path.abspath(__file__)), \"reporter.py\")\n    command = [\n        sys.executable, \"-u\", reporter_filepath,\n        \"--redis-address={}\".format(redis_address)\n    ]\n    if redis_password:\n        command += [\"--redis-password\", redis_password]\n\n    try:\n        import psutil  # noqa: F401\n    except ImportError:\n        logger.warning(\"Failed to start the reporter. The reporter requires \"\n                       \"'pip install psutil'.\")\n        return None\n\n    process_info = start_ray_process(\n        command,\n        ray_constants.PROCESS_TYPE_REPORTER,\n        stdout_file=stdout_file,\n        stderr_file=stderr_file)\n    return process_info"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstarting a dashboard process.", "response": "def start_dashboard(redis_address,\n                    temp_dir,\n                    stdout_file=None,\n                    stderr_file=None,\n                    redis_password=None):\n    \"\"\"Start a dashboard process.\n\n    Args:\n        redis_address (str): The address of the Redis instance.\n        temp_dir (str): The temporary directory used for log files and\n            information for this Ray session.\n        stdout_file: A file handle opened for writing to redirect stdout to. If\n            no redirection should happen, then this should be None.\n        stderr_file: A file handle opened for writing to redirect stderr to. If\n            no redirection should happen, then this should be None.\n        redis_password (str): The password of the redis server.\n\n    Returns:\n        ProcessInfo for the process that was started.\n    \"\"\"\n    port = 8080\n    while True:\n        try:\n            port_test_socket = socket.socket()\n            port_test_socket.bind((\"127.0.0.1\", port))\n            port_test_socket.close()\n            break\n        except socket.error:\n            port += 1\n\n    token = ray.utils.decode(binascii.hexlify(os.urandom(24)))\n\n    dashboard_filepath = os.path.join(\n        os.path.dirname(os.path.abspath(__file__)), \"dashboard/dashboard.py\")\n    command = [\n        sys.executable,\n        \"-u\",\n        dashboard_filepath,\n        \"--redis-address={}\".format(redis_address),\n        \"--http-port={}\".format(port),\n        \"--token={}\".format(token),\n        \"--temp-dir={}\".format(temp_dir),\n    ]\n    if redis_password:\n        command += [\"--redis-password\", redis_password]\n\n    if sys.version_info <= (3, 0):\n        return None, None\n    try:\n        import aiohttp  # noqa: F401\n        import psutil  # noqa: F401\n    except ImportError:\n        raise ImportError(\n            \"Failed to start the dashboard. The dashboard requires Python 3 \"\n            \"as well as 'pip install aiohttp psutil'.\")\n\n    process_info = start_ray_process(\n        command,\n        ray_constants.PROCESS_TYPE_DASHBOARD,\n        stdout_file=stdout_file,\n        stderr_file=stderr_file)\n    dashboard_url = \"http://{}:{}/?token={}\".format(\n        ray.services.get_node_ip_address(), port, token)\n    print(\"\\n\" + \"=\" * 70)\n    print(\"View the dashboard at {}\".format(dashboard_url))\n    print(\"=\" * 70 + \"\\n\")\n    return dashboard_url, process_info"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nstarting a new raylet.", "response": "def start_raylet(redis_address,\n                 node_ip_address,\n                 raylet_name,\n                 plasma_store_name,\n                 worker_path,\n                 temp_dir,\n                 num_cpus=None,\n                 num_gpus=None,\n                 resources=None,\n                 object_manager_port=None,\n                 node_manager_port=None,\n                 redis_password=None,\n                 use_valgrind=False,\n                 use_profiler=False,\n                 stdout_file=None,\n                 stderr_file=None,\n                 config=None,\n                 include_java=False,\n                 java_worker_options=None,\n                 load_code_from_local=False):\n    \"\"\"Start a raylet, which is a combined local scheduler and object manager.\n\n    Args:\n        redis_address (str): The address of the primary Redis server.\n        node_ip_address (str): The IP address of this node.\n        raylet_name (str): The name of the raylet socket to create.\n        plasma_store_name (str): The name of the plasma store socket to connect\n             to.\n        worker_path (str): The path of the Python file that new worker\n            processes will execute.\n        temp_dir (str): The path of the temporary directory Ray will use.\n        num_cpus: The CPUs allocated for this raylet.\n        num_gpus: The GPUs allocated for this raylet.\n        resources: The custom resources allocated for this raylet.\n        object_manager_port: The port to use for the object manager. If this is\n            None, then the object manager will choose its own port.\n        node_manager_port: The port to use for the node manager. If this is\n            None, then the node manager will choose its own port.\n        redis_password: The password to use when connecting to Redis.\n        use_valgrind (bool): True if the raylet should be started inside\n            of valgrind. If this is True, use_profiler must be False.\n        use_profiler (bool): True if the raylet should be started inside\n            a profiler. If this is True, use_valgrind must be False.\n        stdout_file: A file handle opened for writing to redirect stdout to. If\n            no redirection should happen, then this should be None.\n        stderr_file: A file handle opened for writing to redirect stderr to. If\n            no redirection should happen, then this should be None.\n        config (dict|None): Optional Raylet configuration that will\n            override defaults in RayConfig.\n        include_java (bool): If True, the raylet backend can also support\n            Java worker.\n        java_worker_options (str): The command options for Java worker.\n    Returns:\n        ProcessInfo for the process that was started.\n    \"\"\"\n    config = config or {}\n    config_str = \",\".join([\"{},{}\".format(*kv) for kv in config.items()])\n\n    if use_valgrind and use_profiler:\n        raise Exception(\"Cannot use valgrind and profiler at the same time.\")\n\n    num_initial_workers = (num_cpus if num_cpus is not None else\n                           multiprocessing.cpu_count())\n\n    static_resources = check_and_update_resources(num_cpus, num_gpus,\n                                                  resources)\n\n    # Limit the number of workers that can be started in parallel by the\n    # raylet. However, make sure it is at least 1.\n    num_cpus_static = static_resources.get(\"CPU\", 0)\n    maximum_startup_concurrency = max(\n        1, min(multiprocessing.cpu_count(), num_cpus_static))\n\n    # Format the resource argument in a form like 'CPU,1.0,GPU,0,Custom,3'.\n    resource_argument = \",\".join(\n        [\"{},{}\".format(*kv) for kv in static_resources.items()])\n\n    gcs_ip_address, gcs_port = redis_address.split(\":\")\n\n    if include_java is True:\n        java_worker_options = (java_worker_options\n                               or DEFAULT_JAVA_WORKER_OPTIONS)\n        java_worker_command = build_java_worker_command(\n            java_worker_options,\n            redis_address,\n            plasma_store_name,\n            raylet_name,\n            redis_password,\n            os.path.join(temp_dir, \"sockets\"),\n        )\n    else:\n        java_worker_command = \"\"\n\n    # Create the command that the Raylet will use to start workers.\n    start_worker_command = (\"{} {} \"\n                            \"--node-ip-address={} \"\n                            \"--object-store-name={} \"\n                            \"--raylet-name={} \"\n                            \"--redis-address={} \"\n                            \"--temp-dir={}\".format(\n                                sys.executable, worker_path, node_ip_address,\n                                plasma_store_name, raylet_name, redis_address,\n                                temp_dir))\n    if redis_password:\n        start_worker_command += \" --redis-password {}\".format(redis_password)\n\n    # If the object manager port is None, then use 0 to cause the object\n    # manager to choose its own port.\n    if object_manager_port is None:\n        object_manager_port = 0\n    # If the node manager port is None, then use 0 to cause the node manager\n    # to choose its own port.\n    if node_manager_port is None:\n        node_manager_port = 0\n\n    if load_code_from_local:\n        start_worker_command += \" --load-code-from-local \"\n\n    command = [\n        RAYLET_EXECUTABLE,\n        \"--raylet_socket_name={}\".format(raylet_name),\n        \"--store_socket_name={}\".format(plasma_store_name),\n        \"--object_manager_port={}\".format(object_manager_port),\n        \"--node_manager_port={}\".format(node_manager_port),\n        \"--node_ip_address={}\".format(node_ip_address),\n        \"--redis_address={}\".format(gcs_ip_address),\n        \"--redis_port={}\".format(gcs_port),\n        \"--num_initial_workers={}\".format(num_initial_workers),\n        \"--maximum_startup_concurrency={}\".format(maximum_startup_concurrency),\n        \"--static_resource_list={}\".format(resource_argument),\n        \"--config_list={}\".format(config_str),\n        \"--python_worker_command={}\".format(start_worker_command),\n        \"--java_worker_command={}\".format(java_worker_command),\n        \"--redis_password={}\".format(redis_password or \"\"),\n        \"--temp_dir={}\".format(temp_dir),\n    ]\n    process_info = start_ray_process(\n        command,\n        ray_constants.PROCESS_TYPE_RAYLET,\n        use_valgrind=use_valgrind,\n        use_gdb=False,\n        use_valgrind_profiler=use_profiler,\n        use_perftools_profiler=(\"RAYLET_PERFTOOLS_PATH\" in os.environ),\n        stdout_file=stdout_file,\n        stderr_file=stderr_file)\n\n    return process_info"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef determine_plasma_store_config(object_store_memory=None,\n                                  plasma_directory=None,\n                                  huge_pages=False):\n    \"\"\"Figure out how to configure the plasma object store.\n\n    This will determine which directory to use for the plasma store (e.g.,\n    /tmp or /dev/shm) and how much memory to start the store with. On Linux,\n    we will try to use /dev/shm unless the shared memory file system is too\n    small, in which case we will fall back to /tmp. If any of the object store\n    memory or plasma directory parameters are specified by the user, then those\n    values will be preserved.\n\n    Args:\n        object_store_memory (int): The user-specified object store memory\n            parameter.\n        plasma_directory (str): The user-specified plasma directory parameter.\n        huge_pages (bool): The user-specified huge pages parameter.\n\n    Returns:\n        A tuple of the object store memory to use and the plasma directory to\n            use. If either of these values is specified by the user, then that\n            value will be preserved.\n    \"\"\"\n    system_memory = ray.utils.get_system_memory()\n\n    # Choose a default object store size.\n    if object_store_memory is None:\n        object_store_memory = int(system_memory * 0.3)\n        # Cap memory to avoid memory waste and perf issues on large nodes\n        if (object_store_memory >\n                ray_constants.DEFAULT_OBJECT_STORE_MAX_MEMORY_BYTES):\n            logger.warning(\n                \"Warning: Capping object memory store to {}GB. \".format(\n                    ray_constants.DEFAULT_OBJECT_STORE_MAX_MEMORY_BYTES // 1e9)\n                + \"To increase this further, specify `object_store_memory` \"\n                \"when calling ray.init() or ray start.\")\n            object_store_memory = (\n                ray_constants.DEFAULT_OBJECT_STORE_MAX_MEMORY_BYTES)\n\n    # Determine which directory to use. By default, use /tmp on MacOS and\n    # /dev/shm on Linux, unless the shared-memory file system is too small,\n    # in which case we default to /tmp on Linux.\n    if plasma_directory is None:\n        if sys.platform == \"linux\" or sys.platform == \"linux2\":\n            shm_avail = ray.utils.get_shared_memory_bytes()\n            # Compare the requested memory size to the memory available in\n            # /dev/shm.\n            if shm_avail > object_store_memory:\n                plasma_directory = \"/dev/shm\"\n            else:\n                plasma_directory = \"/tmp\"\n                logger.warning(\n                    \"WARNING: The object store is using /tmp instead of \"\n                    \"/dev/shm because /dev/shm has only {} bytes available. \"\n                    \"This may slow down performance! You may be able to free \"\n                    \"up space by deleting files in /dev/shm or terminating \"\n                    \"any running plasma_store_server processes. If you are \"\n                    \"inside a Docker container, you may need to pass an \"\n                    \"argument with the flag '--shm-size' to 'docker run'.\".\n                    format(shm_avail))\n        else:\n            plasma_directory = \"/tmp\"\n\n        # Do some sanity checks.\n        if object_store_memory > system_memory:\n            raise Exception(\n                \"The requested object store memory size is greater \"\n                \"than the total available memory.\")\n    else:\n        plasma_directory = os.path.abspath(plasma_directory)\n        logger.warning(\"WARNING: object_store_memory is not verified when \"\n                       \"plasma_directory is set.\")\n\n    if not os.path.isdir(plasma_directory):\n        raise Exception(\n            \"The file {} does not exist or is not a directory.\".format(\n                plasma_directory))\n\n    return object_store_memory, plasma_directory", "response": "Figure out how to configure the object store."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstarting a plasma store process.", "response": "def _start_plasma_store(plasma_store_memory,\n                        use_valgrind=False,\n                        use_profiler=False,\n                        stdout_file=None,\n                        stderr_file=None,\n                        plasma_directory=None,\n                        huge_pages=False,\n                        socket_name=None):\n    \"\"\"Start a plasma store process.\n\n    Args:\n        plasma_store_memory (int): The amount of memory in bytes to start the\n            plasma store with.\n        use_valgrind (bool): True if the plasma store should be started inside\n            of valgrind. If this is True, use_profiler must be False.\n        use_profiler (bool): True if the plasma store should be started inside\n            a profiler. If this is True, use_valgrind must be False.\n        stdout_file: A file handle opened for writing to redirect stdout to. If\n            no redirection should happen, then this should be None.\n        stderr_file: A file handle opened for writing to redirect stderr to. If\n            no redirection should happen, then this should be None.\n        plasma_directory: A directory where the Plasma memory mapped files will\n            be created.\n        huge_pages: a boolean flag indicating whether to start the\n            Object Store with hugetlbfs support. Requires plasma_directory.\n        socket_name (str): If provided, it will specify the socket\n            name used by the plasma store.\n\n    Return:\n        A tuple of the name of the plasma store socket and ProcessInfo for the\n            plasma store process.\n    \"\"\"\n    if use_valgrind and use_profiler:\n        raise Exception(\"Cannot use valgrind and profiler at the same time.\")\n\n    if huge_pages and not (sys.platform == \"linux\"\n                           or sys.platform == \"linux2\"):\n        raise Exception(\"The huge_pages argument is only supported on \"\n                        \"Linux.\")\n\n    if huge_pages and plasma_directory is None:\n        raise Exception(\"If huge_pages is True, then the \"\n                        \"plasma_directory argument must be provided.\")\n\n    if not isinstance(plasma_store_memory, int):\n        raise Exception(\"plasma_store_memory should be an integer.\")\n\n    command = [\n        PLASMA_STORE_EXECUTABLE, \"-s\", socket_name, \"-m\",\n        str(plasma_store_memory)\n    ]\n    if plasma_directory is not None:\n        command += [\"-d\", plasma_directory]\n    if huge_pages:\n        command += [\"-h\"]\n    process_info = start_ray_process(\n        command,\n        ray_constants.PROCESS_TYPE_PLASMA_STORE,\n        use_valgrind=use_valgrind,\n        use_valgrind_profiler=use_profiler,\n        stdout_file=stdout_file,\n        stderr_file=stderr_file)\n    return process_info"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef start_plasma_store(stdout_file=None,\n                       stderr_file=None,\n                       object_store_memory=None,\n                       plasma_directory=None,\n                       huge_pages=False,\n                       plasma_store_socket_name=None):\n    \"\"\"This method starts an object store process.\n\n    Args:\n        stdout_file: A file handle opened for writing to redirect stdout\n            to. If no redirection should happen, then this should be None.\n        stderr_file: A file handle opened for writing to redirect stderr\n            to. If no redirection should happen, then this should be None.\n        object_store_memory: The amount of memory (in bytes) to start the\n            object store with.\n        plasma_directory: A directory where the Plasma memory mapped files will\n            be created.\n        huge_pages: Boolean flag indicating whether to start the Object\n            Store with hugetlbfs support. Requires plasma_directory.\n\n    Returns:\n        ProcessInfo for the process that was started.\n    \"\"\"\n    object_store_memory, plasma_directory = determine_plasma_store_config(\n        object_store_memory, plasma_directory, huge_pages)\n\n    if object_store_memory < ray_constants.OBJECT_STORE_MINIMUM_MEMORY_BYTES:\n        raise ValueError(\"Attempting to cap object store memory usage at {} \"\n                         \"bytes, but the minimum allowed is {} bytes.\".format(\n                             object_store_memory,\n                             ray_constants.OBJECT_STORE_MINIMUM_MEMORY_BYTES))\n\n    # Print the object store memory using two decimal places.\n    object_store_memory_str = (object_store_memory / 10**7) / 10**2\n    logger.info(\"Starting the Plasma object store with {} GB memory \"\n                \"using {}.\".format(\n                    round(object_store_memory_str, 2), plasma_directory))\n    # Start the Plasma store.\n    process_info = _start_plasma_store(\n        object_store_memory,\n        use_profiler=RUN_PLASMA_STORE_PROFILER,\n        stdout_file=stdout_file,\n        stderr_file=stderr_file,\n        plasma_directory=plasma_directory,\n        huge_pages=huge_pages,\n        socket_name=plasma_store_socket_name)\n\n    return process_info", "response": "This method starts the object store process."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef start_worker(node_ip_address,\n                 object_store_name,\n                 raylet_name,\n                 redis_address,\n                 worker_path,\n                 temp_dir,\n                 stdout_file=None,\n                 stderr_file=None):\n    \"\"\"This method starts a worker process.\n\n    Args:\n        node_ip_address (str): The IP address of the node that this worker is\n            running on.\n        object_store_name (str): The socket name of the object store.\n        raylet_name (str): The socket name of the raylet server.\n        redis_address (str): The address that the Redis server is listening on.\n        worker_path (str): The path of the source code which the worker process\n            will run.\n        temp_dir (str): The path of the temp dir.\n        stdout_file: A file handle opened for writing to redirect stdout to. If\n            no redirection should happen, then this should be None.\n        stderr_file: A file handle opened for writing to redirect stderr to. If\n            no redirection should happen, then this should be None.\n\n    Returns:\n        ProcessInfo for the process that was started.\n    \"\"\"\n    command = [\n        sys.executable, \"-u\", worker_path,\n        \"--node-ip-address=\" + node_ip_address,\n        \"--object-store-name=\" + object_store_name,\n        \"--raylet-name=\" + raylet_name,\n        \"--redis-address=\" + str(redis_address), \"--temp-dir=\" + temp_dir\n    ]\n    process_info = start_ray_process(\n        command,\n        ray_constants.PROCESS_TYPE_WORKER,\n        stdout_file=stdout_file,\n        stderr_file=stderr_file)\n    return process_info", "response": "This method starts a worker process."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef start_monitor(redis_address,\n                  stdout_file=None,\n                  stderr_file=None,\n                  autoscaling_config=None,\n                  redis_password=None):\n    \"\"\"Run a process to monitor the other processes.\n\n    Args:\n        redis_address (str): The address that the Redis server is listening on.\n        stdout_file: A file handle opened for writing to redirect stdout to. If\n            no redirection should happen, then this should be None.\n        stderr_file: A file handle opened for writing to redirect stderr to. If\n            no redirection should happen, then this should be None.\n        autoscaling_config: path to autoscaling config file.\n        redis_password (str): The password of the redis server.\n\n    Returns:\n        ProcessInfo for the process that was started.\n    \"\"\"\n    monitor_path = os.path.join(\n        os.path.dirname(os.path.abspath(__file__)), \"monitor.py\")\n    command = [\n        sys.executable, \"-u\", monitor_path,\n        \"--redis-address=\" + str(redis_address)\n    ]\n    if autoscaling_config:\n        command.append(\"--autoscaling-config=\" + str(autoscaling_config))\n    if redis_password:\n        command.append(\"--redis-password=\" + redis_password)\n    process_info = start_ray_process(\n        command,\n        ray_constants.PROCESS_TYPE_MONITOR,\n        stdout_file=stdout_file,\n        stderr_file=stderr_file)\n    return process_info", "response": "Start a monitor process."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstarts a process to monitor the other processes.", "response": "def start_raylet_monitor(redis_address,\n                         stdout_file=None,\n                         stderr_file=None,\n                         redis_password=None,\n                         config=None):\n    \"\"\"Run a process to monitor the other processes.\n\n    Args:\n        redis_address (str): The address that the Redis server is listening on.\n        stdout_file: A file handle opened for writing to redirect stdout to. If\n            no redirection should happen, then this should be None.\n        stderr_file: A file handle opened for writing to redirect stderr to. If\n            no redirection should happen, then this should be None.\n        redis_password (str): The password of the redis server.\n        config (dict|None): Optional configuration that will\n            override defaults in RayConfig.\n\n    Returns:\n        ProcessInfo for the process that was started.\n    \"\"\"\n    gcs_ip_address, gcs_port = redis_address.split(\":\")\n    redis_password = redis_password or \"\"\n    config = config or {}\n    config_str = \",\".join([\"{},{}\".format(*kv) for kv in config.items()])\n    command = [\n        RAYLET_MONITOR_EXECUTABLE,\n        \"--redis_address={}\".format(gcs_ip_address),\n        \"--redis_port={}\".format(gcs_port),\n        \"--config_list={}\".format(config_str),\n    ]\n    if redis_password:\n        command += [redis_password]\n    process_info = start_ray_process(\n        command,\n        ray_constants.PROCESS_TYPE_RAYLET_MONITOR,\n        stdout_file=stdout_file,\n        stderr_file=stderr_file)\n    return process_info"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef restore_original_dimensions(obs, obs_space, tensorlib=tf):\n\n    if hasattr(obs_space, \"original_space\"):\n        return _unpack_obs(obs, obs_space.original_space, tensorlib=tensorlib)\n    else:\n        return obs", "response": "Unpacks Dict and Tuple space observations into their original form."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nunpacks a flattened Dict or Tuple observation array into a new array.", "response": "def _unpack_obs(obs, space, tensorlib=tf):\n    \"\"\"Unpack a flattened Dict or Tuple observation array/tensor.\n\n    Arguments:\n        obs: The flattened observation tensor\n        space: The original space prior to flattening\n        tensorlib: The library used to unflatten (reshape) the array/tensor\n    \"\"\"\n\n    if (isinstance(space, gym.spaces.Dict)\n            or isinstance(space, gym.spaces.Tuple)):\n        prep = get_preprocessor(space)(space)\n        if len(obs.shape) != 2 or obs.shape[1] != prep.shape[0]:\n            raise ValueError(\n                \"Expected flattened obs shape of [None, {}], got {}\".format(\n                    prep.shape[0], obs.shape))\n        assert len(prep.preprocessors) == len(space.spaces), \\\n            (len(prep.preprocessors) == len(space.spaces))\n        offset = 0\n        if isinstance(space, gym.spaces.Tuple):\n            u = []\n            for p, v in zip(prep.preprocessors, space.spaces):\n                obs_slice = obs[:, offset:offset + p.size]\n                offset += p.size\n                u.append(\n                    _unpack_obs(\n                        tensorlib.reshape(obs_slice, [-1] + list(p.shape)),\n                        v,\n                        tensorlib=tensorlib))\n        else:\n            u = OrderedDict()\n            for p, (k, v) in zip(prep.preprocessors, space.spaces.items()):\n                obs_slice = obs[:, offset:offset + p.size]\n                offset += p.size\n                u[k] = _unpack_obs(\n                    tensorlib.reshape(obs_slice, [-1] + list(p.shape)),\n                    v,\n                    tensorlib=tensorlib)\n        return u\n    else:\n        return obs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_aws_format(tags):\n\n    if TAG_RAY_NODE_NAME in tags:\n        tags[\"Name\"] = tags[TAG_RAY_NODE_NAME]\n        del tags[TAG_RAY_NODE_NAME]\n    return tags", "response": "Convert the Ray node name tag to the AWS - specific Name tag."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _node_tag_update_loop(self):\n        while True:\n            self.tag_cache_update_event.wait()\n            self.tag_cache_update_event.clear()\n\n            batch_updates = defaultdict(list)\n\n            with self.tag_cache_lock:\n                for node_id, tags in self.tag_cache_pending.items():\n                    for x in tags.items():\n                        batch_updates[x].append(node_id)\n                    self.tag_cache[node_id].update(tags)\n\n                self.tag_cache_pending = {}\n\n            for (k, v), node_ids in batch_updates.items():\n                m = \"Set tag {}={} on {}\".format(k, v, node_ids)\n                with LogTimer(\"AWSNodeProvider: {}\".format(m)):\n                    if k == TAG_RAY_NODE_NAME:\n                        k = \"Name\"\n                    self.ec2.meta.client.create_tags(\n                        Resources=node_ids,\n                        Tags=[{\n                            \"Key\": k,\n                            \"Value\": v\n                        }],\n                    )\n\n            self.tag_cache_kill_event.wait(timeout=5)\n            if self.tag_cache_kill_event.is_set():\n                return", "response": "This loop updates the AWS tags for a large cluster."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nrefreshes and get info for this node updating the cache.", "response": "def _get_node(self, node_id):\n        \"\"\"Refresh and get info for this node, updating the cache.\"\"\"\n        self.non_terminated_nodes({})  # Side effect: updates cache\n\n        if node_id in self.cached_nodes:\n            return self.cached_nodes[node_id]\n\n        # Node not in {pending, running} -- retry with a point query. This\n        # usually means the node was recently preempted or terminated.\n        matches = list(self.ec2.instances.filter(InstanceIds=[node_id]))\n        assert len(matches) == 1, \"Invalid instance id {}\".format(node_id)\n        return matches[0]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nvalidate export_formats. Raises: ValueError if the format is unknown.", "response": "def validate(export_formats):\n        \"\"\"Validates export_formats.\n\n        Raises:\n            ValueError if the format is unknown.\n        \"\"\"\n        for i in range(len(export_formats)):\n            export_formats[i] = export_formats[i].strip().lower()\n            if export_formats[i] not in [\n                    ExportFormat.CHECKPOINT, ExportFormat.MODEL\n            ]:\n                raise TuneError(\"Unsupported export format: \" +\n                                export_formats[i])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_resources(self, cpu, gpu, **kwargs):\n        if self.status is Trial.RUNNING:\n            raise ValueError(\"Cannot update resources while Trial is running.\")\n        self.resources = Resources(cpu, gpu, **kwargs)", "response": "Update the resources of the current trial."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning whether the trial qualifies for restoring.", "response": "def should_recover(self):\n        \"\"\"Returns whether the trial qualifies for restoring.\n\n        This is if a checkpoint frequency is set and has not failed more than\n        max_failures. This may return true even when there may not yet\n        be a checkpoint.\n        \"\"\"\n        return (self.checkpoint_freq > 0\n                and (self.num_failures < self.max_failures\n                     or self.max_failures < 0))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef compare_checkpoints(self, attr_mean):\n        if self._cmp_greater and attr_mean > self.best_checkpoint_attr_value:\n            return True\n        elif (not self._cmp_greater\n              and attr_mean < self.best_checkpoint_attr_value):\n            return True\n        return False", "response": "Compares two checkpoints based on the attribute value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef preprocess(img):\n    # Crop the image.\n    img = img[35:195]\n    # Downsample by factor of 2.\n    img = img[::2, ::2, 0]\n    # Erase background (background type 1).\n    img[img == 144] = 0\n    # Erase background (background type 2).\n    img[img == 109] = 0\n    # Set everything else (paddles, ball) to 1.\n    img[img != 0] = 1\n    return img.astype(np.float).ravel()", "response": "Preprocess 210x160x3 uint8 frame into 6400 1D float vector."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntaking 1D float array of rewards and compute discounted reward", "response": "def discount_rewards(r):\n    \"\"\"take 1D float array of rewards and compute discounted reward\"\"\"\n    discounted_r = np.zeros_like(r)\n    running_add = 0\n    for t in reversed(range(0, r.size)):\n        # Reset the sum, since this was a game boundary (pong specific!).\n        if r[t] != 0:\n            running_add = 0\n        running_add = running_add * gamma + r[t]\n        discounted_r[t] = running_add\n    return discounted_r"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef policy_backward(eph, epx, epdlogp, model):\n    dW2 = np.dot(eph.T, epdlogp).ravel()\n    dh = np.outer(epdlogp, model[\"W2\"])\n    # Backprop relu.\n    dh[eph <= 0] = 0\n    dW1 = np.dot(dh.T, epx)\n    return {\"W1\": dW1, \"W2\": dW2}", "response": "backward pass. eph is array of intermediate hidden states epx is array of intermediate hidden states epdlogp is array of intermediate hidden states epdlogp is array of intermediate hidden states"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_class(path):\n    class_data = path.split(\".\")\n    if len(class_data) < 2:\n        raise ValueError(\n            \"You need to pass a valid path like mymodule.provider_class\")\n    module_path = \".\".join(class_data[:-1])\n    class_str = class_data[-1]\n    module = importlib.import_module(module_path)\n    return getattr(module, class_str)", "response": "Load a class at runtime given a full path."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nterminating a set of nodes.", "response": "def terminate_nodes(self, node_ids):\n        \"\"\"Terminates a set of nodes. May be overridden with a batch method.\"\"\"\n        for node_id in node_ids:\n            logger.info(\"NodeProvider: \"\n                        \"{}: Terminating node\".format(node_id))\n            self.terminate_node(node_id)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\npass the result to BayesOpt unless early terminated or errored", "response": "def on_trial_complete(self,\n                          trial_id,\n                          result=None,\n                          error=False,\n                          early_terminated=False):\n        \"\"\"Passes the result to BayesOpt unless early terminated or errored\"\"\"\n        if result:\n            self.optimizer.register(\n                params=self._live_trial_mapping[trial_id],\n                target=result[self._reward_attr])\n\n        del self._live_trial_mapping[trial_id]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexecute the method with arg and return the result.", "response": "def _execute_and_seal_error(method, arg, method_name):\n    \"\"\"Execute method with arg and return the result.\n\n    If the method fails, return a RayTaskError so it can be sealed in the\n    resultOID and retried by user.\n    \"\"\"\n    try:\n        return method(arg)\n    except Exception:\n        return ray.worker.RayTaskError(method_name, traceback.format_exc())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_wrapper_by_cls(env, cls):\n    currentenv = env\n    while True:\n        if isinstance(currentenv, cls):\n            return currentenv\n        elif isinstance(currentenv, gym.Wrapper):\n            currentenv = currentenv.env\n        else:\n            return None", "response": "Returns the gym. Wrapper object of the given class."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconfigures environment for DeepMind - style Atari.", "response": "def wrap_deepmind(env, dim=84, framestack=True):\n    \"\"\"Configure environment for DeepMind-style Atari.\n\n    Note that we assume reward clipping is done outside the wrapper.\n\n    Args:\n        dim (int): Dimension to resize observations to (dim x dim).\n        framestack (bool): Whether to framestack observations.\n    \"\"\"\n    env = MonitorEnv(env)\n    env = NoopResetEnv(env, noop_max=30)\n    if \"NoFrameskip\" in env.spec.id:\n        env = MaxAndSkipEnv(env, skip=4)\n    env = EpisodicLifeEnv(env)\n    if \"FIRE\" in env.unwrapped.get_action_meanings():\n        env = FireResetEnv(env)\n    env = WarpFrame(env, dim)\n    # env = ScaledFloatFrame(env)  # TODO: use for dqn?\n    # env = ClipRewardEnv(env)  # reward clipping is handled by policy eval\n    if framestack:\n        env = FrameStack(env, 4)\n    return env"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nnoting Padding is added to match TF conv2d `same` padding. See www.tensorflow.org/versions/r0.12/api_docs/python/nn/convolution Params: in_size (tuple): Rows (Height), Column (Width) for input stride_size (tuple): Rows (Height), Column (Width) for stride filter_size (tuple): Rows (Height), Column (Width) for filter Output: padding (tuple): For input into torch.nn.ZeroPad2d output (tuple): Output shape after padding and convolution", "response": "def valid_padding(in_size, filter_size, stride_size):\n    \"\"\"Note: Padding is added to match TF conv2d `same` padding. See\n    www.tensorflow.org/versions/r0.12/api_docs/python/nn/convolution\n\n    Params:\n        in_size (tuple): Rows (Height), Column (Width) for input\n        stride_size (tuple): Rows (Height), Column (Width) for stride\n        filter_size (tuple): Rows (Height), Column (Width) for filter\n\n    Output:\n        padding (tuple): For input into torch.nn.ZeroPad2d\n        output (tuple): Output shape after padding and convolution\n    \"\"\"\n    in_height, in_width = in_size\n    filter_height, filter_width = filter_size\n    stride_height, stride_width = stride_size\n\n    out_height = np.ceil(float(in_height) / float(stride_height))\n    out_width = np.ceil(float(in_width) / float(stride_width))\n\n    pad_along_height = int(\n        ((out_height - 1) * stride_height + filter_height - in_height))\n    pad_along_width = int(\n        ((out_width - 1) * stride_width + filter_width - in_width))\n    pad_top = pad_along_height // 2\n    pad_bottom = pad_along_height - pad_top\n    pad_left = pad_along_width // 2\n    pad_right = pad_along_width - pad_left\n    padding = (pad_left, pad_right, pad_top, pad_bottom)\n    output = (out_height, out_width)\n    return padding, output"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ray_get_and_free(object_ids):\n\n    global _last_free_time\n    global _to_free\n\n    result = ray.get(object_ids)\n    if type(object_ids) is not list:\n        object_ids = [object_ids]\n    _to_free.extend(object_ids)\n\n    # batch calls to free to reduce overheads\n    now = time.time()\n    if (len(_to_free) > MAX_FREE_QUEUE_SIZE\n            or now - _last_free_time > FREE_DELAY_S):\n        ray.internal.free(_to_free)\n        _to_free = []\n        _last_free_time = now\n\n    return result", "response": "Call ray. get and then queue the object ids for deletion."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef aligned_array(size, dtype, align=64):\n\n    n = size * dtype.itemsize\n    empty = np.empty(n + (align - 1), dtype=np.uint8)\n    data_align = empty.ctypes.data % align\n    offset = 0 if data_align == 0 else (align - data_align)\n    output = empty[offset:offset + n].view(dtype)\n\n    assert len(output) == size, len(output)\n    assert output.ctypes.data % align == 0, output.ctypes.data\n    return output", "response": "Returns an array of a given size that is 64 - byte aligned."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconcatenate arrays ensuring the output is 64 - byte aligned.", "response": "def concat_aligned(items):\n    \"\"\"Concatenate arrays, ensuring the output is 64-byte aligned.\n\n    We only align float arrays; other arrays are concatenated as normal.\n\n    This should be used instead of np.concatenate() to improve performance\n    when the output array is likely to be fed into TensorFlow.\n    \"\"\"\n\n    if len(items) == 0:\n        return []\n    elif len(items) == 1:\n        # we assume the input is aligned. In any case, it doesn't help\n        # performance to force align it since that incurs a needless copy.\n        return items[0]\n    elif (isinstance(items[0], np.ndarray)\n          and items[0].dtype in [np.float32, np.float64, np.uint8]):\n        dtype = items[0].dtype\n        flat = aligned_array(sum(s.size for s in items), dtype)\n        batch_dim = sum(s.shape[0] for s in items)\n        new_shape = (batch_dim, ) + items[0].shape[1:]\n        output = flat.reshape(new_shape)\n        assert output.ctypes.data % 64 == 0, output.ctypes.data\n        np.concatenate(items, out=output)\n        return output\n    else:\n        return np.concatenate(items)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef put(self, item, block=True, timeout=None):\n        if self.maxsize <= 0:\n            self.actor.put.remote(item)\n        elif not block:\n            if not ray.get(self.actor.put.remote(item)):\n                raise Full\n        elif timeout is None:\n            # Polling\n            # Use a not_full condition variable or promise?\n            while not ray.get(self.actor.put.remote(item)):\n                # Consider adding time.sleep here\n                pass\n        elif timeout < 0:\n            raise ValueError(\"'timeout' must be a non-negative number\")\n        else:\n            endtime = time.time() + timeout\n            # Polling\n            # Use a condition variable or switch to promise?\n            success = False\n            while not success and time.time() < endtime:\n                success = ray.get(self.actor.put.remote(item))\n            if not success:\n                raise Full", "response": "Adds an item to the queue."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget an item from the queue.", "response": "def get(self, block=True, timeout=None):\n        \"\"\"Gets an item from the queue.\n\n        Uses polling if block=True, so there is no guarantee of order if\n        multiple consumers get from the same empty queue.\n\n        Returns:\n            The next item in the queue.\n\n        Raises:\n            Empty if the queue is empty and blocking is False.\n        \"\"\"\n        if not block:\n            success, item = ray.get(self.actor.get.remote())\n            if not success:\n                raise Empty\n        elif timeout is None:\n            # Polling\n            # Use a not_empty condition variable or return a promise?\n            success, item = ray.get(self.actor.get.remote())\n            while not success:\n                # Consider adding time.sleep here\n                success, item = ray.get(self.actor.get.remote())\n        elif timeout < 0:\n            raise ValueError(\"'timeout' must be a non-negative number\")\n        else:\n            endtime = time.time() + timeout\n            # Polling\n            # Use a not_full condition variable or return a promise?\n            success = False\n            while not success and time.time() < endtime:\n                success, item = ray.get(self.actor.get.remote())\n            if not success:\n                raise Empty\n        return item"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a function that checks if the specified class provides the overriden method.", "response": "def override(cls):\n    \"\"\"Annotation for documenting method overrides.\n\n    Arguments:\n        cls (type): The superclass that provides the overriden method. If this\n            cls does not actually have the method, an error is raised.\n    \"\"\"\n\n    def check_override(method):\n        if method.__name__ not in dir(cls):\n            raise NameError(\"{} does not override any method of {}\".format(\n                method, cls))\n        return method\n\n    return check_override"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd new trial. On a new trial add to current bracket. Else create new bracket and add to current band. Else create new bracket and add to current iteration. Else create new bracket and add to current bracket. Else create new bracket and add to current bracket. Else create new bracket and add to current bracket.", "response": "def on_trial_add(self, trial_runner, trial):\n        \"\"\"Adds new trial.\n\n        On a new trial add, if current bracket is not filled,\n        add to current bracket. Else, if current band is not filled,\n        create new bracket, add to current bracket.\n        Else, create new iteration, create new bracket, add to bracket.\"\"\"\n\n        cur_bracket = self._state[\"bracket\"]\n        cur_band = self._hyperbands[self._state[\"band_idx\"]]\n        if cur_bracket is None or cur_bracket.filled():\n            retry = True\n            while retry:\n                # if current iteration is filled, create new iteration\n                if self._cur_band_filled():\n                    cur_band = []\n                    self._hyperbands.append(cur_band)\n                    self._state[\"band_idx\"] += 1\n\n                # cur_band will always be less than s_max_1 or else filled\n                s = len(cur_band)\n                assert s < self._s_max_1, \"Current band is filled!\"\n                if self._get_r0(s) == 0:\n                    logger.info(\"Bracket too small - Retrying...\")\n                    cur_bracket = None\n                else:\n                    retry = False\n                    cur_bracket = Bracket(self._time_attr, self._get_n0(s),\n                                          self._get_r0(s), self._max_t_attr,\n                                          self._eta, s)\n                cur_band.append(cur_bracket)\n                self._state[\"bracket\"] = cur_bracket\n\n        self._state[\"bracket\"].add_trial(trial)\n        self._trial_info[trial] = cur_bracket, self._state[\"band_idx\"]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if the current band is filled.", "response": "def _cur_band_filled(self):\n        \"\"\"Checks if the current band is filled.\n\n        The size of the current band should be equal to s_max_1\"\"\"\n\n        cur_band = self._hyperbands[self._state[\"band_idx\"]]\n        return len(cur_band) == self._s_max_1"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef on_trial_result(self, trial_runner, trial, result):\n\n        bracket, _ = self._trial_info[trial]\n        bracket.update_trial_stats(trial, result)\n\n        if bracket.continue_trial(trial):\n            return TrialScheduler.CONTINUE\n\n        action = self._process_bracket(trial_runner, bracket, trial)\n        return action", "response": "Called when trial result is received from bracket iteration."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchoosing trial to run based on trial_runner.", "response": "def choose_trial_to_run(self, trial_runner):\n        \"\"\"Fair scheduling within iteration by completion percentage.\n\n        List of trials not used since all trials are tracked as state\n        of scheduler. If iteration is occupied (ie, no trials to run),\n        then look into next iteration.\n        \"\"\"\n\n        for hyperband in self._hyperbands:\n            # band will have None entries if no resources\n            # are to be allocated to that bracket.\n            scrubbed = [b for b in hyperband if b is not None]\n            for bracket in sorted(\n                    scrubbed, key=lambda b: b.completion_percentage()):\n                for trial in bracket.current_trials():\n                    if (trial.status == Trial.PENDING\n                            and trial_runner.has_resources(trial.resources)):\n                        return trial\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_trial(self, trial):\n        assert not self.filled(), \"Cannot add trial to filled bracket!\"\n        self._live_trials[trial] = None\n        self._all_trials.append(trial)", "response": "Add trial to bracket assuming bracket is not filled."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cur_iter_done(self):\n        return all(\n            self._get_result_time(result) >= self._cumul_r\n            for result in self._live_trials.values())", "response": "Checks if all iterations have completed."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_trial_stats(self, trial, result):\n\n        assert trial in self._live_trials\n        assert self._get_result_time(result) >= 0\n\n        delta = self._get_result_time(result) - \\\n            self._get_result_time(self._live_trials[trial])\n        assert delta >= 0\n        self._completed_progress += delta\n        self._live_trials[trial] = result", "response": "Update trial stats. Called after trial has finished\n        an iteration."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cleanup_full(self, trial_runner):\n        for trial in self.current_trials():\n            if (trial.status == Trial.PAUSED):\n                trial_runner.stop_trial(trial)", "response": "Cleans up bracket after bracket is completely finished."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_client_table(redis_client):\n    NIL_CLIENT_ID = ray.ObjectID.nil().binary()\n    message = redis_client.execute_command(\"RAY.TABLE_LOOKUP\",\n                                           ray.gcs_utils.TablePrefix.CLIENT,\n                                           \"\", NIL_CLIENT_ID)\n\n    # Handle the case where no clients are returned. This should only\n    # occur potentially immediately after the cluster is started.\n    if message is None:\n        return []\n\n    node_info = {}\n    gcs_entry = ray.gcs_utils.GcsTableEntry.GetRootAsGcsTableEntry(message, 0)\n\n    ordered_client_ids = []\n\n    # Since GCS entries are append-only, we override so that\n    # only the latest entries are kept.\n    for i in range(gcs_entry.EntriesLength()):\n        client = (ray.gcs_utils.ClientTableData.GetRootAsClientTableData(\n            gcs_entry.Entries(i), 0))\n\n        resources = {\n            decode(client.ResourcesTotalLabel(i)):\n            client.ResourcesTotalCapacity(i)\n            for i in range(client.ResourcesTotalLabelLength())\n        }\n        client_id = ray.utils.binary_to_hex(client.ClientId())\n\n        # If this client is being removed, then it must\n        # have previously been inserted, and\n        # it cannot have previously been removed.\n        if not client.IsInsertion():\n            assert client_id in node_info, \"Client removed not found!\"\n            assert node_info[client_id][\"IsInsertion\"], (\n                \"Unexpected duplicate removal of client.\")\n        else:\n            ordered_client_ids.append(client_id)\n\n        node_info[client_id] = {\n            \"ClientID\": client_id,\n            \"IsInsertion\": client.IsInsertion(),\n            \"NodeManagerAddress\": decode(\n                client.NodeManagerAddress(), allow_none=True),\n            \"NodeManagerPort\": client.NodeManagerPort(),\n            \"ObjectManagerPort\": client.ObjectManagerPort(),\n            \"ObjectStoreSocketName\": decode(\n                client.ObjectStoreSocketName(), allow_none=True),\n            \"RayletSocketName\": decode(\n                client.RayletSocketName(), allow_none=True),\n            \"Resources\": resources\n        }\n    # NOTE: We return the list comprehension below instead of simply doing\n    # 'list(node_info.values())' in order to have the nodes appear in the order\n    # that they joined the cluster. Python dictionaries do not preserve\n    # insertion order. We could use an OrderedDict, but then we'd have to be\n    # sure to only insert a given node a single time (clients that die appear\n    # twice in the GCS log).\n    return [node_info[client_id] for client_id in ordered_client_ids]", "response": "Read the client table and return a list of node info."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninitializing the GlobalState object by connecting to Redis and then getting all of the Redis shards.", "response": "def _initialize_global_state(self,\n                                 redis_address,\n                                 redis_password=None,\n                                 timeout=20):\n        \"\"\"Initialize the GlobalState object by connecting to Redis.\n\n        It's possible that certain keys in Redis may not have been fully\n        populated yet. In this case, we will retry this method until they have\n        been populated or we exceed a timeout.\n\n        Args:\n            redis_address: The Redis address to connect.\n            redis_password: The password of the redis server.\n        \"\"\"\n        self.redis_client = services.create_redis_client(\n            redis_address, redis_password)\n        start_time = time.time()\n\n        num_redis_shards = None\n        redis_shard_addresses = []\n\n        while time.time() - start_time < timeout:\n            # Attempt to get the number of Redis shards.\n            num_redis_shards = self.redis_client.get(\"NumRedisShards\")\n            if num_redis_shards is None:\n                print(\"Waiting longer for NumRedisShards to be populated.\")\n                time.sleep(1)\n                continue\n            num_redis_shards = int(num_redis_shards)\n            if num_redis_shards < 1:\n                raise Exception(\"Expected at least one Redis shard, found \"\n                                \"{}.\".format(num_redis_shards))\n\n            # Attempt to get all of the Redis shards.\n            redis_shard_addresses = self.redis_client.lrange(\n                \"RedisShards\", start=0, end=-1)\n            if len(redis_shard_addresses) != num_redis_shards:\n                print(\"Waiting longer for RedisShards to be populated.\")\n                time.sleep(1)\n                continue\n\n            # If we got here then we successfully got all of the information.\n            break\n\n        # Check to see if we timed out.\n        if time.time() - start_time >= timeout:\n            raise Exception(\"Timed out while attempting to initialize the \"\n                            \"global state. num_redis_shards = {}, \"\n                            \"redis_shard_addresses = {}\".format(\n                                num_redis_shards, redis_shard_addresses))\n\n        # Get the rest of the information.\n        self.redis_clients = []\n        for shard_address in redis_shard_addresses:\n            self.redis_clients.append(\n                services.create_redis_client(shard_address.decode(),\n                                             redis_password))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexecutes a Redis command on the appropriate Redis shard based on key.", "response": "def _execute_command(self, key, *args):\n        \"\"\"Execute a Redis command on the appropriate Redis shard based on key.\n\n        Args:\n            key: The object ID or the task ID that the query is about.\n            args: The command to run.\n\n        Returns:\n            The value returned by the Redis command.\n        \"\"\"\n        client = self.redis_clients[key.redis_shard_hash() % len(\n            self.redis_clients)]\n        return client.execute_command(*args)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexecute the KEYS command on all Redis shards.", "response": "def _keys(self, pattern):\n        \"\"\"Execute the KEYS command on all Redis shards.\n\n        Args:\n            pattern: The KEYS pattern to query.\n\n        Returns:\n            The concatenated list of results from all shards.\n        \"\"\"\n        result = []\n        for client in self.redis_clients:\n            result.extend(list(client.scan_iter(match=pattern)))\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _object_table(self, object_id):\n        # Allow the argument to be either an ObjectID or a hex string.\n        if not isinstance(object_id, ray.ObjectID):\n            object_id = ray.ObjectID(hex_to_binary(object_id))\n\n        # Return information about a single object ID.\n        message = self._execute_command(object_id, \"RAY.TABLE_LOOKUP\",\n                                        ray.gcs_utils.TablePrefix.OBJECT, \"\",\n                                        object_id.binary())\n        if message is None:\n            return {}\n        gcs_entry = ray.gcs_utils.GcsTableEntry.GetRootAsGcsTableEntry(\n            message, 0)\n\n        assert gcs_entry.EntriesLength() > 0\n\n        entry = ray.gcs_utils.ObjectTableData.GetRootAsObjectTableData(\n            gcs_entry.Entries(0), 0)\n\n        object_info = {\n            \"DataSize\": entry.ObjectSize(),\n            \"Manager\": entry.Manager(),\n        }\n\n        return object_info", "response": "Fetch and parse the object table information for a single object ID."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef object_table(self, object_id=None):\n        self._check_connected()\n        if object_id is not None:\n            # Return information about a single object ID.\n            return self._object_table(object_id)\n        else:\n            # Return the entire object table.\n            object_keys = self._keys(ray.gcs_utils.TablePrefix_OBJECT_string +\n                                     \"*\")\n            object_ids_binary = {\n                key[len(ray.gcs_utils.TablePrefix_OBJECT_string):]\n                for key in object_keys\n            }\n\n            results = {}\n            for object_id_binary in object_ids_binary:\n                results[binary_to_object_id(object_id_binary)] = (\n                    self._object_table(binary_to_object_id(object_id_binary)))\n            return results", "response": "Fetch and parse the object table info for one or more object IDs."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _task_table(self, task_id):\n        assert isinstance(task_id, ray.TaskID)\n        message = self._execute_command(task_id, \"RAY.TABLE_LOOKUP\",\n                                        ray.gcs_utils.TablePrefix.RAYLET_TASK,\n                                        \"\", task_id.binary())\n        if message is None:\n            return {}\n        gcs_entries = ray.gcs_utils.GcsTableEntry.GetRootAsGcsTableEntry(\n            message, 0)\n\n        assert gcs_entries.EntriesLength() == 1\n\n        task_table_message = ray.gcs_utils.Task.GetRootAsTask(\n            gcs_entries.Entries(0), 0)\n\n        execution_spec = task_table_message.TaskExecutionSpec()\n        task_spec = task_table_message.TaskSpecification()\n        task = ray._raylet.Task.from_string(task_spec)\n        function_descriptor_list = task.function_descriptor_list()\n        function_descriptor = FunctionDescriptor.from_bytes_list(\n            function_descriptor_list)\n\n        task_spec_info = {\n            \"DriverID\": task.driver_id().hex(),\n            \"TaskID\": task.task_id().hex(),\n            \"ParentTaskID\": task.parent_task_id().hex(),\n            \"ParentCounter\": task.parent_counter(),\n            \"ActorID\": (task.actor_id().hex()),\n            \"ActorCreationID\": task.actor_creation_id().hex(),\n            \"ActorCreationDummyObjectID\": (\n                task.actor_creation_dummy_object_id().hex()),\n            \"ActorCounter\": task.actor_counter(),\n            \"Args\": task.arguments(),\n            \"ReturnObjectIDs\": task.returns(),\n            \"RequiredResources\": task.required_resources(),\n            \"FunctionID\": function_descriptor.function_id.hex(),\n            \"FunctionHash\": binary_to_hex(function_descriptor.function_hash),\n            \"ModuleName\": function_descriptor.module_name,\n            \"ClassName\": function_descriptor.class_name,\n            \"FunctionName\": function_descriptor.function_name,\n        }\n\n        return {\n            \"ExecutionSpec\": {\n                \"Dependencies\": [\n                    execution_spec.Dependencies(i)\n                    for i in range(execution_spec.DependenciesLength())\n                ],\n                \"LastTimestamp\": execution_spec.LastTimestamp(),\n                \"NumForwards\": execution_spec.NumForwards()\n            },\n            \"TaskSpec\": task_spec_info\n        }", "response": "Fetch and parse the task table information for a single task ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfetches and parse the task table information for one or more task IDs.", "response": "def task_table(self, task_id=None):\n        \"\"\"Fetch and parse the task table information for one or more task IDs.\n\n        Args:\n            task_id: A hex string of the task ID to fetch information about. If\n                this is None, then the task object table is fetched.\n\n        Returns:\n            Information from the task table.\n        \"\"\"\n        self._check_connected()\n        if task_id is not None:\n            task_id = ray.TaskID(hex_to_binary(task_id))\n            return self._task_table(task_id)\n        else:\n            task_table_keys = self._keys(\n                ray.gcs_utils.TablePrefix_RAYLET_TASK_string + \"*\")\n            task_ids_binary = [\n                key[len(ray.gcs_utils.TablePrefix_RAYLET_TASK_string):]\n                for key in task_table_keys\n            ]\n\n            results = {}\n            for task_id_binary in task_ids_binary:\n                results[binary_to_hex(task_id_binary)] = self._task_table(\n                    ray.TaskID(task_id_binary))\n            return results"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfetching and parse the function table.", "response": "def function_table(self, function_id=None):\n        \"\"\"Fetch and parse the function table.\n\n        Returns:\n            A dictionary that maps function IDs to information about the\n                function.\n        \"\"\"\n        self._check_connected()\n        function_table_keys = self.redis_client.keys(\n            ray.gcs_utils.FUNCTION_PREFIX + \"*\")\n        results = {}\n        for key in function_table_keys:\n            info = self.redis_client.hgetall(key)\n            function_info_parsed = {\n                \"DriverID\": binary_to_hex(info[b\"driver_id\"]),\n                \"Module\": decode(info[b\"module\"]),\n                \"Name\": decode(info[b\"name\"])\n            }\n            results[binary_to_hex(info[b\"function_id\"])] = function_info_parsed\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the profile events for a given batch of profile events.", "response": "def _profile_table(self, batch_id):\n        \"\"\"Get the profile events for a given batch of profile events.\n\n        Args:\n            batch_id: An identifier for a batch of profile events.\n\n        Returns:\n            A list of the profile events for the specified batch.\n        \"\"\"\n        # TODO(rkn): This method should support limiting the number of log\n        # events and should also support returning a window of events.\n        message = self._execute_command(batch_id, \"RAY.TABLE_LOOKUP\",\n                                        ray.gcs_utils.TablePrefix.PROFILE, \"\",\n                                        batch_id.binary())\n\n        if message is None:\n            return []\n\n        gcs_entries = ray.gcs_utils.GcsTableEntry.GetRootAsGcsTableEntry(\n            message, 0)\n\n        profile_events = []\n        for i in range(gcs_entries.EntriesLength()):\n            profile_table_message = (\n                ray.gcs_utils.ProfileTableData.GetRootAsProfileTableData(\n                    gcs_entries.Entries(i), 0))\n\n            component_type = decode(profile_table_message.ComponentType())\n            component_id = binary_to_hex(profile_table_message.ComponentId())\n            node_ip_address = decode(\n                profile_table_message.NodeIpAddress(), allow_none=True)\n\n            for j in range(profile_table_message.ProfileEventsLength()):\n                profile_event_message = profile_table_message.ProfileEvents(j)\n\n                profile_event = {\n                    \"event_type\": decode(profile_event_message.EventType()),\n                    \"component_id\": component_id,\n                    \"node_ip_address\": node_ip_address,\n                    \"component_type\": component_type,\n                    \"start_time\": profile_event_message.StartTime(),\n                    \"end_time\": profile_event_message.EndTime(),\n                    \"extra_data\": json.loads(\n                        decode(profile_event_message.ExtraData())),\n                }\n\n                profile_events.append(profile_event)\n\n        return profile_events"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndumps the profile table as a json file.", "response": "def chrome_tracing_dump(self, filename=None):\n        \"\"\"Return a list of profiling events that can viewed as a timeline.\n\n        To view this information as a timeline, simply dump it as a json file\n        by passing in \"filename\" or using using json.dump, and then load go to\n        chrome://tracing in the Chrome web browser and load the dumped file.\n        Make sure to enable \"Flow events\" in the \"View Options\" menu.\n\n        Args:\n            filename: If a filename is provided, the timeline is dumped to that\n                file.\n\n        Returns:\n            If filename is not provided, this returns a list of profiling\n                events. Each profile event is a dictionary.\n        \"\"\"\n        # TODO(rkn): Support including the task specification data in the\n        # timeline.\n        # TODO(rkn): This should support viewing just a window of time or a\n        # limited number of events.\n\n        profile_table = self.profile_table()\n        all_events = []\n\n        for component_id_hex, component_events in profile_table.items():\n            # Only consider workers and drivers.\n            component_type = component_events[0][\"component_type\"]\n            if component_type not in [\"worker\", \"driver\"]:\n                continue\n\n            for event in component_events:\n                new_event = {\n                    # The category of the event.\n                    \"cat\": event[\"event_type\"],\n                    # The string displayed on the event.\n                    \"name\": event[\"event_type\"],\n                    # The identifier for the group of rows that the event\n                    # appears in.\n                    \"pid\": event[\"node_ip_address\"],\n                    # The identifier for the row that the event appears in.\n                    \"tid\": event[\"component_type\"] + \":\" +\n                    event[\"component_id\"],\n                    # The start time in microseconds.\n                    \"ts\": self._seconds_to_microseconds(event[\"start_time\"]),\n                    # The duration in microseconds.\n                    \"dur\": self._seconds_to_microseconds(event[\"end_time\"] -\n                                                         event[\"start_time\"]),\n                    # What is this?\n                    \"ph\": \"X\",\n                    # This is the name of the color to display the box in.\n                    \"cname\": self._default_color_mapping[event[\"event_type\"]],\n                    # The extra user-defined data.\n                    \"args\": event[\"extra_data\"],\n                }\n\n                # Modify the json with the additional user-defined extra data.\n                # This can be used to add fields or override existing fields.\n                if \"cname\" in event[\"extra_data\"]:\n                    new_event[\"cname\"] = event[\"extra_data\"][\"cname\"]\n                if \"name\" in event[\"extra_data\"]:\n                    new_event[\"name\"] = event[\"extra_data\"][\"name\"]\n\n                all_events.append(new_event)\n\n        if filename is not None:\n            with open(filename, \"w\") as outfile:\n                json.dump(all_events, outfile)\n        else:\n            return all_events"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef chrome_tracing_object_transfer_dump(self, filename=None):\n        client_id_to_address = {}\n        for client_info in ray.global_state.client_table():\n            client_id_to_address[client_info[\"ClientID\"]] = \"{}:{}\".format(\n                client_info[\"NodeManagerAddress\"],\n                client_info[\"ObjectManagerPort\"])\n\n        all_events = []\n\n        for key, items in self.profile_table().items():\n            # Only consider object manager events.\n            if items[0][\"component_type\"] != \"object_manager\":\n                continue\n\n            for event in items:\n                if event[\"event_type\"] == \"transfer_send\":\n                    object_id, remote_client_id, _, _ = event[\"extra_data\"]\n\n                elif event[\"event_type\"] == \"transfer_receive\":\n                    object_id, remote_client_id, _, _ = event[\"extra_data\"]\n\n                elif event[\"event_type\"] == \"receive_pull_request\":\n                    object_id, remote_client_id = event[\"extra_data\"]\n\n                else:\n                    assert False, \"This should be unreachable.\"\n\n                # Choose a color by reading the first couple of hex digits of\n                # the object ID as an integer and turning that into a color.\n                object_id_int = int(object_id[:2], 16)\n                color = self._chrome_tracing_colors[object_id_int % len(\n                    self._chrome_tracing_colors)]\n\n                new_event = {\n                    # The category of the event.\n                    \"cat\": event[\"event_type\"],\n                    # The string displayed on the event.\n                    \"name\": event[\"event_type\"],\n                    # The identifier for the group of rows that the event\n                    # appears in.\n                    \"pid\": client_id_to_address[key],\n                    # The identifier for the row that the event appears in.\n                    \"tid\": client_id_to_address[remote_client_id],\n                    # The start time in microseconds.\n                    \"ts\": self._seconds_to_microseconds(event[\"start_time\"]),\n                    # The duration in microseconds.\n                    \"dur\": self._seconds_to_microseconds(event[\"end_time\"] -\n                                                         event[\"start_time\"]),\n                    # What is this?\n                    \"ph\": \"X\",\n                    # This is the name of the color to display the box in.\n                    \"cname\": color,\n                    # The extra user-defined data.\n                    \"args\": event[\"extra_data\"],\n                }\n                all_events.append(new_event)\n\n                # Add another box with a color indicating whether it was a send\n                # or a receive event.\n                if event[\"event_type\"] == \"transfer_send\":\n                    additional_event = new_event.copy()\n                    additional_event[\"cname\"] = \"black\"\n                    all_events.append(additional_event)\n                elif event[\"event_type\"] == \"transfer_receive\":\n                    additional_event = new_event.copy()\n                    additional_event[\"cname\"] = \"grey\"\n                    all_events.append(additional_event)\n                else:\n                    pass\n\n        if filename is not None:\n            with open(filename, \"w\") as outfile:\n                json.dump(all_events, outfile)\n        else:\n            return all_events", "response": "Dump all the object manager events that can viewed as a timeline."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef workers(self):\n        worker_keys = self.redis_client.keys(\"Worker*\")\n        workers_data = {}\n\n        for worker_key in worker_keys:\n            worker_info = self.redis_client.hgetall(worker_key)\n            worker_id = binary_to_hex(worker_key[len(\"Workers:\"):])\n\n            workers_data[worker_id] = {\n                \"node_ip_address\": decode(worker_info[b\"node_ip_address\"]),\n                \"plasma_store_socket\": decode(\n                    worker_info[b\"plasma_store_socket\"])\n            }\n            if b\"stderr_file\" in worker_info:\n                workers_data[worker_id][\"stderr_file\"] = decode(\n                    worker_info[b\"stderr_file\"])\n            if b\"stdout_file\" in worker_info:\n                workers_data[worker_id][\"stdout_file\"] = decode(\n                    worker_info[b\"stdout_file\"])\n        return workers_data", "response": "Get a dictionary mapping worker ID to worker information."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the current total cluster resources.", "response": "def cluster_resources(self):\n        \"\"\"Get the current total cluster resources.\n\n        Note that this information can grow stale as nodes are added to or\n        removed from the cluster.\n\n        Returns:\n            A dictionary mapping resource name to the total quantity of that\n                resource in the cluster.\n        \"\"\"\n        resources = defaultdict(int)\n        clients = self.client_table()\n        for client in clients:\n            # Only count resources from live clients.\n            if client[\"IsInsertion\"]:\n                for key, value in client[\"Resources\"].items():\n                    resources[key] += value\n\n        return dict(resources)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef available_resources(self):\n        available_resources_by_id = {}\n\n        subscribe_clients = [\n            redis_client.pubsub(ignore_subscribe_messages=True)\n            for redis_client in self.redis_clients\n        ]\n        for subscribe_client in subscribe_clients:\n            subscribe_client.subscribe(ray.gcs_utils.XRAY_HEARTBEAT_CHANNEL)\n\n        client_ids = self._live_client_ids()\n\n        while set(available_resources_by_id.keys()) != client_ids:\n            for subscribe_client in subscribe_clients:\n                # Parse client message\n                raw_message = subscribe_client.get_message()\n                if (raw_message is None or raw_message[\"channel\"] !=\n                        ray.gcs_utils.XRAY_HEARTBEAT_CHANNEL):\n                    continue\n                data = raw_message[\"data\"]\n                gcs_entries = (\n                    ray.gcs_utils.GcsTableEntry.GetRootAsGcsTableEntry(\n                        data, 0))\n                heartbeat_data = gcs_entries.Entries(0)\n                message = (ray.gcs_utils.HeartbeatTableData.\n                           GetRootAsHeartbeatTableData(heartbeat_data, 0))\n                # Calculate available resources for this client\n                num_resources = message.ResourcesAvailableLabelLength()\n                dynamic_resources = {}\n                for i in range(num_resources):\n                    resource_id = decode(message.ResourcesAvailableLabel(i))\n                    dynamic_resources[resource_id] = (\n                        message.ResourcesAvailableCapacity(i))\n\n                # Update available resources for this client\n                client_id = ray.utils.binary_to_hex(message.ClientId())\n                available_resources_by_id[client_id] = dynamic_resources\n\n            # Update clients in cluster\n            client_ids = self._live_client_ids()\n\n            # Remove disconnected clients\n            for client_id in available_resources_by_id.keys():\n                if client_id not in client_ids:\n                    del available_resources_by_id[client_id]\n\n        # Calculate total available resources\n        total_available_resources = defaultdict(int)\n        for available_resources in available_resources_by_id.values():\n            for resource_id, num_available in available_resources.items():\n                total_available_resources[resource_id] += num_available\n\n        # Close the pubsub clients to avoid leaking file descriptors.\n        for subscribe_client in subscribe_clients:\n            subscribe_client.close()\n\n        return dict(total_available_resources)", "response": "Get the current available resources for this cluster."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _error_messages(self, driver_id):\n        assert isinstance(driver_id, ray.DriverID)\n        message = self.redis_client.execute_command(\n            \"RAY.TABLE_LOOKUP\", ray.gcs_utils.TablePrefix.ERROR_INFO, \"\",\n            driver_id.binary())\n\n        # If there are no errors, return early.\n        if message is None:\n            return []\n\n        gcs_entries = ray.gcs_utils.GcsTableEntry.GetRootAsGcsTableEntry(\n            message, 0)\n        error_messages = []\n        for i in range(gcs_entries.EntriesLength()):\n            error_data = ray.gcs_utils.ErrorTableData.GetRootAsErrorTableData(\n                gcs_entries.Entries(i), 0)\n            assert driver_id.binary() == error_data.DriverId()\n            error_message = {\n                \"type\": decode(error_data.Type()),\n                \"message\": decode(error_data.ErrorMessage()),\n                \"timestamp\": error_data.Timestamp(),\n            }\n            error_messages.append(error_message)\n        return error_messages", "response": "Get the error messages for a specific driver."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef error_messages(self, driver_id=None):\n        if driver_id is not None:\n            assert isinstance(driver_id, ray.DriverID)\n            return self._error_messages(driver_id)\n\n        error_table_keys = self.redis_client.keys(\n            ray.gcs_utils.TablePrefix_ERROR_INFO_string + \"*\")\n        driver_ids = [\n            key[len(ray.gcs_utils.TablePrefix_ERROR_INFO_string):]\n            for key in error_table_keys\n        ]\n\n        return {\n            binary_to_hex(driver_id): self._error_messages(\n                ray.DriverID(driver_id))\n            for driver_id in driver_ids\n        }", "response": "Returns the error messages for the specified driver."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget checkpoint info for the given actor id.", "response": "def actor_checkpoint_info(self, actor_id):\n        \"\"\"Get checkpoint info for the given actor id.\n         Args:\n            actor_id: Actor's ID.\n         Returns:\n            A dictionary with information about the actor's checkpoint IDs and\n            their timestamps.\n        \"\"\"\n        self._check_connected()\n        message = self._execute_command(\n            actor_id,\n            \"RAY.TABLE_LOOKUP\",\n            ray.gcs_utils.TablePrefix.ACTOR_CHECKPOINT_ID,\n            \"\",\n            actor_id.binary(),\n        )\n        if message is None:\n            return None\n        gcs_entry = ray.gcs_utils.GcsTableEntry.GetRootAsGcsTableEntry(\n            message, 0)\n        entry = (\n            ray.gcs_utils.ActorCheckpointIdData.GetRootAsActorCheckpointIdData(\n                gcs_entry.Entries(0), 0))\n        checkpoint_ids_str = entry.CheckpointIds()\n        num_checkpoints = len(checkpoint_ids_str) // ID_SIZE\n        assert len(checkpoint_ids_str) % ID_SIZE == 0\n        checkpoint_ids = [\n            ray.ActorCheckpointID(\n                checkpoint_ids_str[(i * ID_SIZE):((i + 1) * ID_SIZE)])\n            for i in range(num_checkpoints)\n        ]\n        return {\n            \"ActorID\": ray.utils.binary_to_hex(entry.ActorId()),\n            \"CheckpointIds\": checkpoint_ids,\n            \"Timestamps\": [\n                entry.Timestamps(i) for i in range(num_checkpoints)\n            ],\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_flat_size(self):\n        return sum(\n            np.prod(v.get_shape().as_list()) for v in self.variables.values())", "response": "Returns the total length of all of the flattened variables."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the weights and returns them as a flat array.", "response": "def get_flat(self):\n        \"\"\"Gets the weights and returns them as a flat array.\n\n        Returns:\n            1D Array containing the flattened weights.\n        \"\"\"\n        self._check_sess()\n        return np.concatenate([\n            v.eval(session=self.sess).flatten()\n            for v in self.variables.values()\n        ])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_flat(self, new_weights):\n        self._check_sess()\n        shapes = [v.get_shape().as_list() for v in self.variables.values()]\n        arrays = unflatten(new_weights, shapes)\n        placeholders = [\n            self.placeholders[k] for k, v in self.variables.items()\n        ]\n        self.sess.run(\n            list(self.assignment_nodes.values()),\n            feed_dict=dict(zip(placeholders, arrays)))", "response": "Sets the weights to new_weights converting from a flat array."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a dictionary containing the weights of the network.", "response": "def get_weights(self):\n        \"\"\"Returns a dictionary containing the weights of the network.\n\n        Returns:\n            Dictionary mapping variable names to their weights.\n        \"\"\"\n        self._check_sess()\n        return {\n            k: v.eval(session=self.sess)\n            for k, v in self.variables.items()\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the weights of the current node to new_weights.", "response": "def set_weights(self, new_weights):\n        \"\"\"Sets the weights to new_weights.\n\n        Note:\n            Can set subsets of variables as well, by only passing in the\n            variables you want to be set.\n\n        Args:\n            new_weights (Dict): Dictionary mapping variable names to their\n                weights.\n        \"\"\"\n        self._check_sess()\n        assign_list = [\n            self.assignment_nodes[name] for name in new_weights.keys()\n            if name in self.assignment_nodes\n        ]\n        assert assign_list, (\"No variables in the input matched those in the \"\n                             \"network. Possible cause: Two networks were \"\n                             \"defined in the same TensorFlow graph. To fix \"\n                             \"this, place each network definition in its own \"\n                             \"tf.Graph.\")\n        self.sess.run(\n            assign_list,\n            feed_dict={\n                self.placeholders[name]: value\n                for (name, value) in new_weights.items()\n                if name in self.placeholders\n            })"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef construct_error_message(driver_id, error_type, message, timestamp):\n    builder = flatbuffers.Builder(0)\n    driver_offset = builder.CreateString(driver_id.binary())\n    error_type_offset = builder.CreateString(error_type)\n    message_offset = builder.CreateString(message)\n\n    ray.core.generated.ErrorTableData.ErrorTableDataStart(builder)\n    ray.core.generated.ErrorTableData.ErrorTableDataAddDriverId(\n        builder, driver_offset)\n    ray.core.generated.ErrorTableData.ErrorTableDataAddType(\n        builder, error_type_offset)\n    ray.core.generated.ErrorTableData.ErrorTableDataAddErrorMessage(\n        builder, message_offset)\n    ray.core.generated.ErrorTableData.ErrorTableDataAddTimestamp(\n        builder, timestamp)\n    error_data_offset = ray.core.generated.ErrorTableData.ErrorTableDataEnd(\n        builder)\n    builder.Finish(error_data_offset)\n\n    return bytes(builder.Output())", "response": "Constructs a serialized ErrorTableData object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef flush_task_and_object_metadata_unsafe():\n    ray.worker.global_worker.check_connected()\n\n    def flush_shard(redis_client):\n        # Flush the task table. Note that this also flushes the driver tasks\n        # which may be undesirable.\n        num_task_keys_deleted = 0\n        for key in redis_client.scan_iter(match=TASK_PREFIX + b\"*\"):\n            num_task_keys_deleted += redis_client.delete(key)\n        print(\"Deleted {} task keys from Redis.\".format(num_task_keys_deleted))\n\n        # Flush the object information.\n        num_object_keys_deleted = 0\n        for key in redis_client.scan_iter(match=OBJECT_INFO_PREFIX + b\"*\"):\n            num_object_keys_deleted += redis_client.delete(key)\n        print(\"Deleted {} object info keys from Redis.\".format(\n            num_object_keys_deleted))\n\n        # Flush the object locations.\n        num_object_location_keys_deleted = 0\n        for key in redis_client.scan_iter(match=OBJECT_LOCATION_PREFIX + b\"*\"):\n            num_object_location_keys_deleted += redis_client.delete(key)\n        print(\"Deleted {} object location keys from Redis.\".format(\n            num_object_location_keys_deleted))\n\n    # Loop over the shards and flush all of them.\n    for redis_client in ray.worker.global_state.redis_clients:\n        flush_shard(redis_client)", "response": "This function removes all of the task and object metadata from Redis shards."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef flush_finished_tasks_unsafe():\n    ray.worker.global_worker.check_connected()\n\n    for shard_index in range(len(ray.global_state.redis_clients)):\n        _flush_finished_tasks_unsafe_shard(shard_index)", "response": "This function is used to remove all finished tasks from Redis shards."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef flush_evicted_objects_unsafe():\n    ray.worker.global_worker.check_connected()\n\n    for shard_index in range(len(ray.global_state.redis_clients)):\n        _flush_evicted_objects_unsafe_shard(shard_index)", "response": "This function is used to remove all objects that have been evicted from Redis."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a copy of self using existing input placeholders.", "response": "def copy(self, existing_inputs):\n        \"\"\"Creates a copy of self using existing input placeholders.\"\"\"\n        return PPOPolicyGraph(\n            self.observation_space,\n            self.action_space,\n            self.config,\n            existing_inputs=existing_inputs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef deepnn(x):\n    # Reshape to use within a convolutional neural net.\n    # Last dimension is for \"features\" - there is only one here, since images\n    # are grayscale -- it would be 3 for an RGB image, 4 for RGBA, etc.\n    with tf.name_scope(\"reshape\"):\n        x_image = tf.reshape(x, [-1, 28, 28, 1])\n\n    # First convolutional layer - maps one grayscale image to 32 feature maps.\n    with tf.name_scope(\"conv1\"):\n        W_conv1 = weight_variable([5, 5, 1, 32])\n        b_conv1 = bias_variable([32])\n        h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n\n    # Pooling layer - downsamples by 2X.\n    with tf.name_scope(\"pool1\"):\n        h_pool1 = max_pool_2x2(h_conv1)\n\n    # Second convolutional layer -- maps 32 feature maps to 64.\n    with tf.name_scope(\"conv2\"):\n        W_conv2 = weight_variable([5, 5, 32, 64])\n        b_conv2 = bias_variable([64])\n        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n\n    # Second pooling layer.\n    with tf.name_scope(\"pool2\"):\n        h_pool2 = max_pool_2x2(h_conv2)\n\n    # Fully connected layer 1 -- after 2 round of downsampling, our 28x28 image\n    # is down to 7x7x64 feature maps -- maps this to 1024 features.\n    with tf.name_scope(\"fc1\"):\n        W_fc1 = weight_variable([7 * 7 * 64, 1024])\n        b_fc1 = bias_variable([1024])\n\n        h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\n        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n\n    # Dropout - controls the complexity of the model, prevents co-adaptation of\n    # features.\n    with tf.name_scope(\"dropout\"):\n        keep_prob = tf.placeholder(tf.float32)\n        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n\n    # Map the 1024 features to 10 classes, one for each digit\n    with tf.name_scope(\"fc2\"):\n        W_fc2 = weight_variable([1024, 10])\n        b_fc2 = bias_variable([10])\n\n        y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n    return y_conv, keep_prob", "response": "Builds a graph for a deep net for classifying digits."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting signature parameters from Cython functions.", "response": "def get_signature_params(func):\n    \"\"\"Get signature parameters\n\n    Support Cython functions by grabbing relevant attributes from the Cython\n    function and attaching to a no-op function. This is somewhat brittle, since\n    funcsigs may change, but given that funcsigs is written to a PEP, we hope\n    it is relatively stable. Future versions of Python may allow overloading\n    the inspect 'isfunction' and 'ismethod' functions / create ABC for Python\n    functions. Until then, it appears that Cython won't do anything about\n    compatability with the inspect module.\n\n    Args:\n        func: The function whose signature should be checked.\n\n    Raises:\n        TypeError: A type error if the signature is not supported\n    \"\"\"\n    # The first condition for Cython functions, the latter for Cython instance\n    # methods\n    if is_cython(func):\n        attrs = [\n            \"__code__\", \"__annotations__\", \"__defaults__\", \"__kwdefaults__\"\n        ]\n\n        if all(hasattr(func, attr) for attr in attrs):\n            original_func = func\n\n            def func():\n                return\n\n            for attr in attrs:\n                setattr(func, attr, getattr(original_func, attr))\n        else:\n            raise TypeError(\"{!r} is not a Python function we can process\"\n                            .format(func))\n\n    return list(funcsigs.signature(func).parameters.items())"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if the signature of a function is supported.", "response": "def check_signature_supported(func, warn=False):\n    \"\"\"Check if we support the signature of this function.\n\n    We currently do not allow remote functions to have **kwargs. We also do not\n    support keyword arguments in conjunction with a *args argument.\n\n    Args:\n        func: The function whose signature should be checked.\n        warn: If this is true, a warning will be printed if the signature is\n            not supported. If it is false, an exception will be raised if the\n            signature is not supported.\n\n    Raises:\n        Exception: An exception is raised if the signature is not supported.\n    \"\"\"\n    function_name = func.__name__\n    sig_params = get_signature_params(func)\n\n    has_kwargs_param = False\n    has_kwonly_param = False\n    for keyword_name, parameter in sig_params:\n        if parameter.kind == Parameter.VAR_KEYWORD:\n            has_kwargs_param = True\n        if parameter.kind == Parameter.KEYWORD_ONLY:\n            has_kwonly_param = True\n\n    if has_kwargs_param:\n        message = (\"The function {} has a **kwargs argument, which is \"\n                   \"currently not supported.\".format(function_name))\n        if warn:\n            logger.warning(message)\n        else:\n            raise Exception(message)\n\n    if has_kwonly_param:\n        message = (\"The function {} has a keyword only argument \"\n                   \"(defined after * or *args), which is currently \"\n                   \"not supported.\".format(function_name))\n        if warn:\n            logger.warning(message)\n        else:\n            raise Exception(message)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef extract_signature(func, ignore_first=False):\n    sig_params = get_signature_params(func)\n\n    if ignore_first:\n        if len(sig_params) == 0:\n            raise Exception(\"Methods must take a 'self' argument, but the \"\n                            \"method '{}' does not have one.\".format(\n                                func.__name__))\n        sig_params = sig_params[1:]\n\n    # Construct the argument default values and other argument information.\n    arg_names = []\n    arg_defaults = []\n    arg_is_positionals = []\n    keyword_names = set()\n    for arg_name, parameter in sig_params:\n        arg_names.append(arg_name)\n        arg_defaults.append(parameter.default)\n        arg_is_positionals.append(parameter.kind == parameter.VAR_POSITIONAL)\n        if parameter.kind == Parameter.POSITIONAL_OR_KEYWORD:\n            # Note KEYWORD_ONLY arguments currently unsupported.\n            keyword_names.add(arg_name)\n\n    return FunctionSignature(arg_names, arg_defaults, arg_is_positionals,\n                             keyword_names, func.__name__)", "response": "Extracts the function signature from the function."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extend_args(function_signature, args, kwargs):\n    arg_names = function_signature.arg_names\n    arg_defaults = function_signature.arg_defaults\n    arg_is_positionals = function_signature.arg_is_positionals\n    keyword_names = function_signature.keyword_names\n    function_name = function_signature.function_name\n\n    args = list(args)\n\n    for keyword_name in kwargs:\n        if keyword_name not in keyword_names:\n            raise Exception(\"The name '{}' is not a valid keyword argument \"\n                            \"for the function '{}'.\".format(\n                                keyword_name, function_name))\n\n    # Fill in the remaining arguments.\n    for skipped_name in arg_names[0:len(args)]:\n        if skipped_name in kwargs:\n            raise Exception(\"Positional and keyword value provided for the \"\n                            \"argument '{}' for the function '{}'\".format(\n                                keyword_name, function_name))\n\n    zipped_info = zip(arg_names, arg_defaults, arg_is_positionals)\n    zipped_info = list(zipped_info)[len(args):]\n    for keyword_name, default_value, is_positional in zipped_info:\n        if keyword_name in kwargs:\n            args.append(kwargs[keyword_name])\n        else:\n            if default_value != funcsigs._empty:\n                args.append(default_value)\n            else:\n                # This means that there is a missing argument. Unless this is\n                # the last argument and it is a *args argument in which case it\n                # can be omitted.\n                if not is_positional:\n                    raise Exception(\"No value was provided for the argument \"\n                                    \"'{}' for the function '{}'.\".format(\n                                        keyword_name, function_name))\n\n    no_positionals = len(arg_is_positionals) == 0 or not arg_is_positionals[-1]\n    too_many_arguments = len(args) > len(arg_names) and no_positionals\n    if too_many_arguments:\n        raise Exception(\"Too many arguments were passed to the function '{}'\"\n                        .format(function_name))\n    return args", "response": "Extend the arguments that were passed into a function with the default arguments provided in the function definition."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef wait_for_crm_operation(operation):\n    logger.info(\"wait_for_crm_operation: \"\n                \"Waiting for operation {} to finish...\".format(operation))\n\n    for _ in range(MAX_POLLS):\n        result = crm.operations().get(name=operation[\"name\"]).execute()\n        if \"error\" in result:\n            raise Exception(result[\"error\"])\n\n        if \"done\" in result and result[\"done\"]:\n            logger.info(\"wait_for_crm_operation: Operation done.\")\n            break\n\n        time.sleep(POLL_INTERVAL)\n\n    return result", "response": "Poll for cloud resource manager operation until finished."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef wait_for_compute_global_operation(project_name, operation):\n    logger.info(\"wait_for_compute_global_operation: \"\n                \"Waiting for operation {} to finish...\".format(\n                    operation[\"name\"]))\n\n    for _ in range(MAX_POLLS):\n        result = compute.globalOperations().get(\n            project=project_name,\n            operation=operation[\"name\"],\n        ).execute()\n        if \"error\" in result:\n            raise Exception(result[\"error\"])\n\n        if result[\"status\"] == \"DONE\":\n            logger.info(\"wait_for_compute_global_operation: \"\n                        \"Operation done.\")\n            break\n\n        time.sleep(POLL_INTERVAL)\n\n    return result", "response": "Poll for global compute operation until finished."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the ith default gcp_key_pair_name.", "response": "def key_pair_name(i, region, project_id, ssh_user):\n    \"\"\"Returns the ith default gcp_key_pair_name.\"\"\"\n    key_name = \"{}_gcp_{}_{}_{}\".format(RAY, region, project_id, ssh_user, i)\n    return key_name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning public and private key paths for a given key_name.", "response": "def key_pair_paths(key_name):\n    \"\"\"Returns public and private key paths for a given key_name.\"\"\"\n    public_key_path = os.path.expanduser(\"~/.ssh/{}.pub\".format(key_name))\n    private_key_path = os.path.expanduser(\"~/.ssh/{}.pem\".format(key_name))\n    return public_key_path, private_key_path"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef generate_rsa_key_pair():\n\n    key = rsa.generate_private_key(\n        backend=default_backend(), public_exponent=65537, key_size=2048)\n\n    public_key = key.public_key().public_bytes(\n        serialization.Encoding.OpenSSH,\n        serialization.PublicFormat.OpenSSH).decode(\"utf-8\")\n\n    pem = key.private_bytes(\n        encoding=serialization.Encoding.PEM,\n        format=serialization.PrivateFormat.TraditionalOpenSSL,\n        encryption_algorithm=serialization.NoEncryption()).decode(\"utf-8\")\n\n    return public_key, pem", "response": "Create public and private ssh - keys."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _configure_project(config):\n    project_id = config[\"provider\"].get(\"project_id\")\n    assert config[\"provider\"][\"project_id\"] is not None, (\n        \"'project_id' must be set in the 'provider' section of the autoscaler\"\n        \" config. Notice that the project id must be globally unique.\")\n    project = _get_project(project_id)\n\n    if project is None:\n        #  Project not found, try creating it\n        _create_project(project_id)\n        project = _get_project(project_id)\n\n    assert project is not None, \"Failed to create project\"\n    assert project[\"lifecycleState\"] == \"ACTIVE\", (\n        \"Project status needs to be ACTIVE, got {}\".format(\n            project[\"lifecycleState\"]))\n\n    config[\"provider\"][\"project_id\"] = project[\"projectId\"]\n\n    return config", "response": "Setup a Google Cloud Platform Project."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _configure_key_pair(config):\n\n    if \"ssh_private_key\" in config[\"auth\"]:\n        return config\n\n    ssh_user = config[\"auth\"][\"ssh_user\"]\n\n    project = compute.projects().get(\n        project=config[\"provider\"][\"project_id\"]).execute()\n\n    # Key pairs associated with project meta data. The key pairs are general,\n    # and not just ssh keys.\n    ssh_keys_str = next(\n        (item for item in project[\"commonInstanceMetadata\"].get(\"items\", [])\n         if item[\"key\"] == \"ssh-keys\"), {}).get(\"value\", \"\")\n\n    ssh_keys = ssh_keys_str.split(\"\\n\") if ssh_keys_str else []\n\n    # Try a few times to get or create a good key pair.\n    key_found = False\n    for i in range(10):\n        key_name = key_pair_name(i, config[\"provider\"][\"region\"],\n                                 config[\"provider\"][\"project_id\"], ssh_user)\n        public_key_path, private_key_path = key_pair_paths(key_name)\n\n        for ssh_key in ssh_keys:\n            key_parts = ssh_key.split(\" \")\n            if len(key_parts) != 3:\n                continue\n\n            if key_parts[2] == ssh_user and os.path.exists(private_key_path):\n                # Found a key\n                key_found = True\n                break\n\n        # Create a key since it doesn't exist locally or in GCP\n        if not key_found and not os.path.exists(private_key_path):\n            logger.info(\"_configure_key_pair: \"\n                        \"Creating new key pair {}\".format(key_name))\n            public_key, private_key = generate_rsa_key_pair()\n\n            _create_project_ssh_key_pair(project, public_key, ssh_user)\n\n            with open(private_key_path, \"w\") as f:\n                f.write(private_key)\n            os.chmod(private_key_path, 0o600)\n\n            with open(public_key_path, \"w\") as f:\n                f.write(public_key)\n\n            key_found = True\n\n            break\n\n        if key_found:\n            break\n\n    assert key_found, \"SSH keypair for user {} not found for {}\".format(\n        ssh_user, private_key_path)\n    assert os.path.exists(private_key_path), (\n        \"Private key file {} not found for user {}\"\n        \"\".format(private_key_path, ssh_user))\n\n    logger.info(\"_configure_key_pair: \"\n                \"Private key not specified in config, using\"\n                \"{}\".format(private_key_path))\n\n    config[\"auth\"][\"ssh_private_key\"] = private_key_path\n\n    return config", "response": "Configure SSH access using an existing key pair if possible."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _configure_subnet(config):\n\n    # Rationale: avoid subnet lookup if the network is already\n    # completely manually configured\n    if (\"networkInterfaces\" in config[\"head_node\"]\n            and \"networkInterfaces\" in config[\"worker_nodes\"]):\n        return config\n\n    subnets = _list_subnets(config)\n\n    if not subnets:\n        raise NotImplementedError(\"Should be able to create subnet.\")\n\n    # TODO: make sure that we have usable subnet. Maybe call\n    # compute.subnetworks().listUsable? For some reason it didn't\n    # work out-of-the-box\n    default_subnet = subnets[0]\n\n    if \"networkInterfaces\" not in config[\"head_node\"]:\n        config[\"head_node\"][\"networkInterfaces\"] = [{\n            \"subnetwork\": default_subnet[\"selfLink\"],\n            \"accessConfigs\": [{\n                \"name\": \"External NAT\",\n                \"type\": \"ONE_TO_ONE_NAT\",\n            }],\n        }]\n\n    if \"networkInterfaces\" not in config[\"worker_nodes\"]:\n        config[\"worker_nodes\"][\"networkInterfaces\"] = [{\n            \"subnetwork\": default_subnet[\"selfLink\"],\n            \"accessConfigs\": [{\n                \"name\": \"External NAT\",\n                \"type\": \"ONE_TO_ONE_NAT\",\n            }],\n        }]\n\n    return config", "response": "Pick a reasonable subnet if not specified by the config."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd new IAM roles for the service account.", "response": "def _add_iam_policy_binding(service_account, roles):\n    \"\"\"Add new IAM roles for the service account.\"\"\"\n    project_id = service_account[\"projectId\"]\n    email = service_account[\"email\"]\n    member_id = \"serviceAccount:\" + email\n\n    policy = crm.projects().getIamPolicy(resource=project_id).execute()\n\n    already_configured = True\n\n    for role in roles:\n        role_exists = False\n        for binding in policy[\"bindings\"]:\n            if binding[\"role\"] == role:\n                if member_id not in binding[\"members\"]:\n                    binding[\"members\"].append(member_id)\n                    already_configured = False\n                role_exists = True\n\n        if not role_exists:\n            already_configured = False\n            policy[\"bindings\"].append({\n                \"members\": [member_id],\n                \"role\": role,\n            })\n\n    if already_configured:\n        # In some managed environments, an admin needs to grant the\n        # roles, so only call setIamPolicy if needed.\n        return\n\n    result = crm.projects().setIamPolicy(\n        resource=project_id, body={\n            \"policy\": policy,\n        }).execute()\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _create_project_ssh_key_pair(project, public_key, ssh_user):\n\n    key_parts = public_key.split(\" \")\n\n    # Sanity checks to make sure that the generated key matches expectation\n    assert len(key_parts) == 2, key_parts\n    assert key_parts[0] == \"ssh-rsa\", key_parts\n\n    new_ssh_meta = \"{ssh_user}:ssh-rsa {key_value} {ssh_user}\".format(\n        ssh_user=ssh_user, key_value=key_parts[1])\n\n    common_instance_metadata = project[\"commonInstanceMetadata\"]\n    items = common_instance_metadata.get(\"items\", [])\n\n    ssh_keys_i = next(\n        (i for i, item in enumerate(items) if item[\"key\"] == \"ssh-keys\"), None)\n\n    if ssh_keys_i is None:\n        items.append({\"key\": \"ssh-keys\", \"value\": new_ssh_meta})\n    else:\n        ssh_keys = items[ssh_keys_i]\n        ssh_keys[\"value\"] += \"\\n\" + new_ssh_meta\n        items[ssh_keys_i] = ssh_keys\n\n    common_instance_metadata[\"items\"] = items\n\n    operation = compute.projects().setCommonInstanceMetadata(\n        project=project[\"name\"], body=common_instance_metadata).execute()\n\n    response = wait_for_compute_global_operation(project[\"name\"], operation)\n\n    return response", "response": "Creates an ssh - key pair in the project commonInstanceMetadata."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _remote(self,\n                args=None,\n                kwargs=None,\n                num_return_vals=None,\n                num_cpus=None,\n                num_gpus=None,\n                resources=None):\n        \"\"\"An experimental alternate way to submit remote functions.\"\"\"\n        worker = ray.worker.get_global_worker()\n        worker.check_connected()\n\n        if self._last_export_session < worker._session_index:\n            # If this function was exported in a previous session, we need to\n            # export this function again, because current GCS doesn't have it.\n            self._last_export_session = worker._session_index\n            worker.function_actor_manager.export(self)\n\n        kwargs = {} if kwargs is None else kwargs\n        args = [] if args is None else args\n        args = ray.signature.extend_args(self._function_signature, args,\n                                         kwargs)\n\n        if num_return_vals is None:\n            num_return_vals = self._num_return_vals\n\n        resources = ray.utils.resources_from_resource_arguments(\n            self._num_cpus, self._num_gpus, self._resources, num_cpus,\n            num_gpus, resources)\n        if worker.mode == ray.worker.LOCAL_MODE:\n            # In LOCAL_MODE, remote calls simply execute the function.\n            # We copy the arguments to prevent the function call from\n            # mutating them and to match the usual behavior of\n            # immutable remote objects.\n            result = self._function(*copy.deepcopy(args))\n            return result\n        object_ids = worker.submit_task(\n            self._function_descriptor,\n            args,\n            num_return_vals=num_return_vals,\n            resources=resources)\n        if len(object_ids) == 1:\n            return object_ids[0]\n        elif len(object_ids) > 1:\n            return object_ids", "response": "A remote method that returns a list of objects."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nappend an object to the linked list.", "response": "def append(self, future):\n        \"\"\"Append an object to the linked list.\n\n        Args:\n            future (PlasmaObjectFuture): A PlasmaObjectFuture instance.\n        \"\"\"\n        future.prev = self.tail\n        if self.tail is None:\n            assert self.head is None\n            self.head = future\n        else:\n            self.tail.next = future\n        self.tail = future\n        # Once done, it will be removed from the list.\n        future.add_done_callback(self.remove)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving an object from the linked list.", "response": "def remove(self, future):\n        \"\"\"Remove an object from the linked list.\n\n        Args:\n            future (PlasmaObjectFuture): A PlasmaObjectFuture instance.\n        \"\"\"\n        if self._loop.get_debug():\n            logger.debug(\"Removing %s from the linked list.\", future)\n        if future.prev is None:\n            assert future is self.head\n            self.head = future.next\n            if self.head is None:\n                self.tail = None\n                if not self.cancelled():\n                    self.set_result(None)\n            else:\n                self.head.prev = None\n        elif future.next is None:\n            assert future is self.tail\n            self.tail = future.prev\n            if self.tail is None:\n                self.head = None\n                if not self.cancelled():\n                    self.set_result(None)\n            else:\n                self.tail.prev = None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_result(self, result):\n        for future in self.traverse():\n            # All cancelled futures should have callbacks to removed itself\n            # from this linked list. However, these callbacks are scheduled in\n            # an event loop, so we could still find them in our list.\n            future.set_result(result)\n        if not self.done():\n            super().set_result(result)", "response": "Set the result of the future."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef traverse(self):\n        current = self.head\n        while current is not None:\n            yield current\n            current = current.next", "response": "Traverse this linked list. Yields PlasmaObjectFuture instances."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nturning an object_id into a Future object.", "response": "def as_future(self, object_id, check_ready=True):\n        \"\"\"Turn an object_id into a Future object.\n\n        Args:\n            object_id: A Ray's object_id.\n            check_ready (bool): If true, check if the object_id is ready.\n\n        Returns:\n            PlasmaObjectFuture: A future object that waits the object_id.\n        \"\"\"\n        if not isinstance(object_id, ray.ObjectID):\n            raise TypeError(\"Input should be an ObjectID.\")\n\n        plain_object_id = plasma.ObjectID(object_id.binary())\n        fut = PlasmaObjectFuture(loop=self._loop, object_id=plain_object_id)\n\n        if check_ready:\n            ready, _ = ray.wait([object_id], timeout=0)\n            if ready:\n                if self._loop.get_debug():\n                    logger.debug(\"%s has been ready.\", plain_object_id)\n                self._complete_future(fut)\n                return fut\n\n        if plain_object_id not in self._waiting_dict:\n            linked_list = PlasmaObjectLinkedList(self._loop, plain_object_id)\n            linked_list.add_done_callback(self._unregister_callback)\n            self._waiting_dict[plain_object_id] = linked_list\n        self._waiting_dict[plain_object_id].append(fut)\n        if self._loop.get_debug():\n            logger.debug(\"%s added to the waiting list.\", fut)\n\n        return fut"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_all_trials(self):\n        response = requests.get(urljoin(self._path, \"trials\"))\n        return self._deserialize(response)", "response": "Returns a list of all trials information."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn trial information by trial_id.", "response": "def get_trial(self, trial_id):\n        \"\"\"Returns trial information by trial_id.\"\"\"\n        response = requests.get(\n            urljoin(self._path, \"trials/{}\".format(trial_id)))\n        return self._deserialize(response)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_trial(self, name, specification):\n        payload = {\"name\": name, \"spec\": specification}\n        response = requests.post(urljoin(self._path, \"trials\"), json=payload)\n        return self._deserialize(response)", "response": "Adds a trial by name and specification ( dict )."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstops the trial by trial_id.", "response": "def stop_trial(self, trial_id):\n        \"\"\"Requests to stop trial by trial_id.\"\"\"\n        response = requests.put(\n            urljoin(self._path, \"trials/{}\".format(trial_id)))\n        return self._deserialize(response)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef foreach_worker(self, fn):\n        results = ray.get([w.foreach_worker.remote(fn) for w in self.workers])\n        return results", "response": "Apply the given function to each remote worker."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\napply the given function to each model replica in each worker.", "response": "def foreach_model(self, fn):\n        \"\"\"Apply the given function to each model replica in each worker.\n\n        Returns:\n            List of results from applying the function.\n        \"\"\"\n\n        results = ray.get([w.foreach_model.remote(fn) for w in self.workers])\n        out = []\n        for r in results:\n            out.extend(r)\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\napplies the given function to a single model replica.", "response": "def for_model(self, fn):\n        \"\"\"Apply the given function to a single model replica.\n\n        Returns:\n            Result from applying the function.\n        \"\"\"\n        return ray.get(self.workers[0].for_model.remote(fn))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef step(self, fetch_stats=False):\n        if self.strategy == \"ps\":\n            return _distributed_sgd_step(\n                self.workers,\n                self.ps_list,\n                write_timeline=False,\n                fetch_stats=fetch_stats)\n        else:\n            return _simple_sgd_step(self.workers)", "response": "Run a single SGD step."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwraps for starting a router and register it.", "response": "def start_router(router_class, router_name):\n    \"\"\"Wrapper for starting a router and register it.\n\n    Args:\n        router_class: The router class to instantiate.\n        router_name: The name to give to the router.\n\n    Returns:\n        A handle to newly started router actor.\n    \"\"\"\n    handle = router_class.remote(router_name)\n    ray.experimental.register_actor(router_name, handle)\n    handle.start.remote()\n    return handle"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of one - hot np. array for all parameters.", "response": "def generate_random_one_hot_encoding(self):\n        \"\"\"Returns a list of one-hot encodings for all parameters.\n\n        1 one-hot np.array for 1 parameter,\n        and the 1's place is randomly chosen.\n        \"\"\"\n        encoding = []\n        for ps in self.param_list:\n            one_hot = np.zeros(ps.choices_count())\n            choice = random.randrange(ps.choices_count())\n            one_hot[choice] = 1\n            encoding.append(one_hot)\n        return encoding"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\napplies one hot encoding to generate a specific config.", "response": "def apply_one_hot_encoding(self, one_hot_encoding):\n        \"\"\"Apply one hot encoding to generate a specific config.\n\n\n        Arguments:\n            one_hot_encoding (list): A list of one hot encodings,\n                1 for each parameter. The shape of each encoding\n                should match that ``ParameterSpace``\n\n        Returns:\n            A dict config with specific <name, value> pair\n        \"\"\"\n        config = {}\n        for ps, one_hot in zip(self.param_list, one_hot_encoding):\n            index = np.argmax(one_hot)\n            config[ps.name] = ps.choices[index]\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npin an object in the object store.", "response": "def pin_in_object_store(obj):\n    \"\"\"Pin an object in the object store.\n\n    It will be available as long as the pinning process is alive. The pinned\n    object can be retrieved by calling get_pinned_object on the identifier\n    returned by this call.\n    \"\"\"\n\n    obj_id = ray.put(_to_pinnable(obj))\n    _pinned_objects.append(ray.get(obj_id))\n    return \"{}{}\".format(PINNED_OBJECT_PREFIX,\n                         base64.b64encode(obj_id.binary()).decode(\"utf-8\"))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_pinned_object(pinned_id):\n\n    from ray import ObjectID\n\n    return _from_pinnable(\n        ray.get(\n            ObjectID(base64.b64decode(pinned_id[len(PINNED_OBJECT_PREFIX):]))))", "response": "Retrieve a pinned object from the object store."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a new dict that is d1 and d2 deep merged.", "response": "def merge_dicts(d1, d2):\n    \"\"\"Returns a new dict that is d1 and d2 deep merged.\"\"\"\n    merged = copy.deepcopy(d1)\n    deep_update(merged, d2, True, [])\n    return merged"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating original dict with values from new_dict recursively.", "response": "def deep_update(original, new_dict, new_keys_allowed, whitelist):\n    \"\"\"Updates original dict with values from new_dict recursively.\n    If new key is introduced in new_dict, then if new_keys_allowed is not\n    True, an error will be thrown. Further, for sub-dicts, if the key is\n    in the whitelist, then new subkeys can be introduced.\n\n    Args:\n        original (dict): Dictionary with default values.\n        new_dict (dict): Dictionary with values to be updated\n        new_keys_allowed (bool): Whether new keys are allowed.\n        whitelist (list): List of keys that correspond to dict values\n            where new subkeys can be introduced. This is only at\n            the top level.\n    \"\"\"\n    for k, value in new_dict.items():\n        if k not in original:\n            if not new_keys_allowed:\n                raise Exception(\"Unknown config parameter `{}` \".format(k))\n        if isinstance(original.get(k), dict):\n            if k in whitelist:\n                deep_update(original[k], value, True, [])\n            else:\n                deep_update(original[k], value, new_keys_allowed, [])\n        else:\n            original[k] = value\n    return original"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nnotify that some evaluators may be removed.", "response": "def reset_evaluators(self, evaluators):\n        \"\"\"Notify that some evaluators may be removed.\"\"\"\n        for obj_id, ev in self._tasks.copy().items():\n            if ev not in evaluators:\n                del self._tasks[obj_id]\n                del self._objects[obj_id]\n        ok = []\n        for ev, obj_id in self._fetching:\n            if ev in evaluators:\n                ok.append((ev, obj_id))\n        self._fetching = ok"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef iter_train_batches(self, max_yield=999):\n\n        for ev, sample_batch in self._augment_with_replay(\n                self.sample_tasks.completed_prefetch(\n                    blocking_wait=True, max_yield=max_yield)):\n            sample_batch.decompress_if_needed()\n            self.batch_buffer.append(sample_batch)\n            if sum(b.count\n                   for b in self.batch_buffer) >= self.train_batch_size:\n                train_batch = self.batch_buffer[0].concat_samples(\n                    self.batch_buffer)\n                yield train_batch\n                self.batch_buffer = []\n\n            # If the batch was replayed, skip the update below.\n            if ev is None:\n                continue\n\n            # Put in replay buffer if enabled\n            if self.replay_buffer_num_slots > 0:\n                if len(self.replay_batches) < self.replay_buffer_num_slots:\n                    self.replay_batches.append(sample_batch)\n                else:\n                    self.replay_batches[self.replay_index] = sample_batch\n                    self.replay_index += 1\n                    self.replay_index %= self.replay_buffer_num_slots\n\n            ev.set_weights.remote(self.broadcasted_weights)\n            self.num_weight_syncs += 1\n            self.num_sent_since_broadcast += 1\n\n            # Kick off another sample request\n            self.sample_tasks.add(ev, ev.sample.remote())", "response": "Iterate over train batches."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate or update an autoscaling Ray cluster from a config json.", "response": "def create_or_update_cluster(config_file, override_min_workers,\n                             override_max_workers, no_restart, restart_only,\n                             yes, override_cluster_name):\n    \"\"\"Create or updates an autoscaling Ray cluster from a config json.\"\"\"\n    config = yaml.load(open(config_file).read())\n    if override_min_workers is not None:\n        config[\"min_workers\"] = override_min_workers\n    if override_max_workers is not None:\n        config[\"max_workers\"] = override_max_workers\n    if override_cluster_name is not None:\n        config[\"cluster_name\"] = override_cluster_name\n    config = _bootstrap_config(config)\n    get_or_create_head_node(config, config_file, no_restart, restart_only, yes,\n                            override_cluster_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef teardown_cluster(config_file, yes, workers_only, override_cluster_name):\n\n    config = yaml.load(open(config_file).read())\n    if override_cluster_name is not None:\n        config[\"cluster_name\"] = override_cluster_name\n    validate_config(config)\n    config = fillout_defaults(config)\n\n    confirm(\"This will destroy your cluster\", yes)\n\n    provider = get_node_provider(config[\"provider\"], config[\"cluster_name\"])\n\n    try:\n\n        def remaining_nodes():\n            if workers_only:\n                A = []\n            else:\n                A = [\n                    node_id for node_id in provider.non_terminated_nodes({\n                        TAG_RAY_NODE_TYPE: \"head\"\n                    })\n                ]\n\n            A += [\n                node_id for node_id in provider.non_terminated_nodes({\n                    TAG_RAY_NODE_TYPE: \"worker\"\n                })\n            ]\n            return A\n\n        # Loop here to check that both the head and worker nodes are actually\n        #   really gone\n        A = remaining_nodes()\n        with LogTimer(\"teardown_cluster: Termination done.\"):\n            while A:\n                logger.info(\"teardown_cluster: \"\n                            \"Terminating {} nodes...\".format(len(A)))\n                provider.terminate_nodes(A)\n                time.sleep(1)\n                A = remaining_nodes()\n    finally:\n        provider.cleanup()", "response": "Destroys all nodes of a Ray cluster described by a config json."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef kill_node(config_file, yes, override_cluster_name):\n\n    config = yaml.load(open(config_file).read())\n    if override_cluster_name is not None:\n        config[\"cluster_name\"] = override_cluster_name\n    config = _bootstrap_config(config)\n\n    confirm(\"This will kill a node in your cluster\", yes)\n\n    provider = get_node_provider(config[\"provider\"], config[\"cluster_name\"])\n    try:\n        nodes = provider.non_terminated_nodes({TAG_RAY_NODE_TYPE: \"worker\"})\n        node = random.choice(nodes)\n        logger.info(\"kill_node: Terminating worker {}\".format(node))\n\n        updater = NodeUpdaterThread(\n            node_id=node,\n            provider_config=config[\"provider\"],\n            provider=provider,\n            auth_config=config[\"auth\"],\n            cluster_name=config[\"cluster_name\"],\n            file_mounts=config[\"file_mounts\"],\n            initialization_commands=[],\n            setup_commands=[],\n            runtime_hash=\"\")\n\n        _exec(updater, \"ray stop\", False, False)\n\n        time.sleep(5)\n\n        if config.get(\"provider\", {}).get(\"use_internal_ips\", False) is True:\n            node_ip = provider.internal_ip(node)\n        else:\n            node_ip = provider.external_ip(node)\n    finally:\n        provider.cleanup()\n\n    return node_ip", "response": "Kills a random Raylet worker."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_or_create_head_node(config, config_file, no_restart, restart_only, yes,\n                            override_cluster_name):\n    \"\"\"Create the cluster head node, which in turn creates the workers.\"\"\"\n    provider = get_node_provider(config[\"provider\"], config[\"cluster_name\"])\n    try:\n        head_node_tags = {\n            TAG_RAY_NODE_TYPE: \"head\",\n        }\n        nodes = provider.non_terminated_nodes(head_node_tags)\n        if len(nodes) > 0:\n            head_node = nodes[0]\n        else:\n            head_node = None\n\n        if not head_node:\n            confirm(\"This will create a new cluster\", yes)\n        elif not no_restart:\n            confirm(\"This will restart cluster services\", yes)\n\n        launch_hash = hash_launch_conf(config[\"head_node\"], config[\"auth\"])\n        if head_node is None or provider.node_tags(head_node).get(\n                TAG_RAY_LAUNCH_CONFIG) != launch_hash:\n            if head_node is not None:\n                confirm(\"Head node config out-of-date. It will be terminated\",\n                        yes)\n                logger.info(\n                    \"get_or_create_head_node: \"\n                    \"Terminating outdated head node {}\".format(head_node))\n                provider.terminate_node(head_node)\n            logger.info(\"get_or_create_head_node: Launching new head node...\")\n            head_node_tags[TAG_RAY_LAUNCH_CONFIG] = launch_hash\n            head_node_tags[TAG_RAY_NODE_NAME] = \"ray-{}-head\".format(\n                config[\"cluster_name\"])\n            provider.create_node(config[\"head_node\"], head_node_tags, 1)\n\n        nodes = provider.non_terminated_nodes(head_node_tags)\n        assert len(nodes) == 1, \"Failed to create head node.\"\n        head_node = nodes[0]\n\n        # TODO(ekl) right now we always update the head node even if the hash\n        # matches. We could prompt the user for what they want to do here.\n        runtime_hash = hash_runtime_conf(config[\"file_mounts\"], config)\n        logger.info(\"get_or_create_head_node: Updating files on head node...\")\n\n        # Rewrite the auth config so that the head node can update the workers\n        remote_key_path = \"~/ray_bootstrap_key.pem\"\n        remote_config = copy.deepcopy(config)\n        remote_config[\"auth\"][\"ssh_private_key\"] = remote_key_path\n\n        # Adjust for new file locations\n        new_mounts = {}\n        for remote_path in config[\"file_mounts\"]:\n            new_mounts[remote_path] = remote_path\n        remote_config[\"file_mounts\"] = new_mounts\n        remote_config[\"no_restart\"] = no_restart\n\n        # Now inject the rewritten config and SSH key into the head node\n        remote_config_file = tempfile.NamedTemporaryFile(\n            \"w\", prefix=\"ray-bootstrap-\")\n        remote_config_file.write(json.dumps(remote_config))\n        remote_config_file.flush()\n        config[\"file_mounts\"].update({\n            remote_key_path: config[\"auth\"][\"ssh_private_key\"],\n            \"~/ray_bootstrap_config.yaml\": remote_config_file.name\n        })\n\n        if restart_only:\n            init_commands = config[\"head_start_ray_commands\"]\n        elif no_restart:\n            init_commands = config[\"head_setup_commands\"]\n        else:\n            init_commands = (config[\"head_setup_commands\"] +\n                             config[\"head_start_ray_commands\"])\n\n        updater = NodeUpdaterThread(\n            node_id=head_node,\n            provider_config=config[\"provider\"],\n            provider=provider,\n            auth_config=config[\"auth\"],\n            cluster_name=config[\"cluster_name\"],\n            file_mounts=config[\"file_mounts\"],\n            initialization_commands=config[\"initialization_commands\"],\n            setup_commands=init_commands,\n            runtime_hash=runtime_hash,\n        )\n        updater.start()\n        updater.join()\n\n        # Refresh the node cache so we see the external ip if available\n        provider.non_terminated_nodes(head_node_tags)\n\n        if config.get(\"provider\", {}).get(\"use_internal_ips\", False) is True:\n            head_node_ip = provider.internal_ip(head_node)\n        else:\n            head_node_ip = provider.external_ip(head_node)\n\n        if updater.exitcode != 0:\n            logger.error(\"get_or_create_head_node: \"\n                         \"Updating {} failed\".format(head_node_ip))\n            sys.exit(1)\n        logger.info(\n            \"get_or_create_head_node: \"\n            \"Head node up-to-date, IP address is: {}\".format(head_node_ip))\n\n        monitor_str = \"tail -n 100 -f /tmp/ray/session_*/logs/monitor*\"\n        use_docker = bool(config[\"docker\"][\"container_name\"])\n        if override_cluster_name:\n            modifiers = \" --cluster-name={}\".format(\n                quote(override_cluster_name))\n        else:\n            modifiers = \"\"\n        print(\"To monitor auto-scaling activity, you can run:\\n\\n\"\n              \"  ray exec {} {}{}{}\\n\".format(\n                  config_file, \"--docker \" if use_docker else \" \",\n                  quote(monitor_str), modifiers))\n        print(\"To open a console on the cluster:\\n\\n\"\n              \"  ray attach {}{}\\n\".format(config_file, modifiers))\n\n        print(\"To ssh manually to the cluster, run:\\n\\n\"\n              \"  ssh -i {} {}@{}\\n\".format(config[\"auth\"][\"ssh_private_key\"],\n                                           config[\"auth\"][\"ssh_user\"],\n                                           head_node_ip))\n    finally:\n        provider.cleanup()", "response": "Create the head node which in turn creates the workers."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nattaches to a screen for the specified cluster.", "response": "def attach_cluster(config_file, start, use_tmux, override_cluster_name, new):\n    \"\"\"Attaches to a screen for the specified cluster.\n\n    Arguments:\n        config_file: path to the cluster yaml\n        start: whether to start the cluster if it isn't up\n        use_tmux: whether to use tmux as multiplexer\n        override_cluster_name: set the name of the cluster\n        new: whether to force a new screen\n    \"\"\"\n\n    if use_tmux:\n        if new:\n            cmd = \"tmux new\"\n        else:\n            cmd = \"tmux attach || tmux new\"\n    else:\n        if new:\n            cmd = \"screen -L\"\n        else:\n            cmd = \"screen -L -xRR\"\n\n    exec_cluster(config_file, cmd, False, False, False, False, start,\n                 override_cluster_name, None)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrunning a command on the specified cluster.", "response": "def exec_cluster(config_file, cmd, docker, screen, tmux, stop, start,\n                 override_cluster_name, port_forward):\n    \"\"\"Runs a command on the specified cluster.\n\n    Arguments:\n        config_file: path to the cluster yaml\n        cmd: command to run\n        docker: whether to run command in docker container of config\n        screen: whether to run in a screen\n        tmux: whether to run in a tmux session\n        stop: whether to stop the cluster after command run\n        start: whether to start the cluster if it isn't up\n        override_cluster_name: set the name of the cluster\n        port_forward: port to forward\n    \"\"\"\n    assert not (screen and tmux), \"Can specify only one of `screen` or `tmux`.\"\n\n    config = yaml.load(open(config_file).read())\n    if override_cluster_name is not None:\n        config[\"cluster_name\"] = override_cluster_name\n    config = _bootstrap_config(config)\n\n    head_node = _get_head_node(\n        config, config_file, override_cluster_name, create_if_needed=start)\n\n    provider = get_node_provider(config[\"provider\"], config[\"cluster_name\"])\n    try:\n        updater = NodeUpdaterThread(\n            node_id=head_node,\n            provider_config=config[\"provider\"],\n            provider=provider,\n            auth_config=config[\"auth\"],\n            cluster_name=config[\"cluster_name\"],\n            file_mounts=config[\"file_mounts\"],\n            initialization_commands=[],\n            setup_commands=[],\n            runtime_hash=\"\",\n        )\n\n        def wrap_docker(command):\n            container_name = config[\"docker\"][\"container_name\"]\n            if not container_name:\n                raise ValueError(\"Docker container not specified in config.\")\n            return with_docker_exec(\n                [command], container_name=container_name)[0]\n\n        cmd = wrap_docker(cmd) if docker else cmd\n\n        if stop:\n            shutdown_cmd = (\n                \"ray stop; ray teardown ~/ray_bootstrap_config.yaml \"\n                \"--yes --workers-only\")\n            if docker:\n                shutdown_cmd = wrap_docker(shutdown_cmd)\n            cmd += (\"; {}; sudo shutdown -h now\".format(shutdown_cmd))\n\n        _exec(\n            updater,\n            cmd,\n            screen,\n            tmux,\n            expect_error=stop,\n            port_forward=port_forward)\n\n        if tmux or screen:\n            attach_command_parts = [\"ray attach\", config_file]\n            if override_cluster_name is not None:\n                attach_command_parts.append(\n                    \"--cluster-name={}\".format(override_cluster_name))\n            if tmux:\n                attach_command_parts.append(\"--tmux\")\n            elif screen:\n                attach_command_parts.append(\"--screen\")\n\n            attach_command = \" \".join(attach_command_parts)\n            attach_info = \"Use `{}` to check on command status.\".format(\n                attach_command)\n            logger.info(attach_info)\n    finally:\n        provider.cleanup()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the head node IP for given configuration file if exists.", "response": "def get_head_node_ip(config_file, override_cluster_name):\n    \"\"\"Returns head node IP for given configuration file if exists.\"\"\"\n\n    config = yaml.load(open(config_file).read())\n    if override_cluster_name is not None:\n        config[\"cluster_name\"] = override_cluster_name\n\n    provider = get_node_provider(config[\"provider\"], config[\"cluster_name\"])\n    try:\n        head_node = _get_head_node(config, config_file, override_cluster_name)\n        if config.get(\"provider\", {}).get(\"use_internal_ips\", False) is True:\n            head_node_ip = provider.internal_ip(head_node)\n        else:\n            head_node_ip = provider.external_ip(head_node)\n    finally:\n        provider.cleanup()\n\n    return head_node_ip"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns worker node IPs for given configuration file.", "response": "def get_worker_node_ips(config_file, override_cluster_name):\n    \"\"\"Returns worker node IPs for given configuration file.\"\"\"\n\n    config = yaml.load(open(config_file).read())\n    if override_cluster_name is not None:\n        config[\"cluster_name\"] = override_cluster_name\n\n    provider = get_node_provider(config[\"provider\"], config[\"cluster_name\"])\n    try:\n        nodes = provider.non_terminated_nodes({TAG_RAY_NODE_TYPE: \"worker\"})\n\n        if config.get(\"provider\", {}).get(\"use_internal_ips\", False) is True:\n            return [provider.internal_ip(node) for node in nodes]\n        else:\n            return [provider.external_ip(node) for node in nodes]\n    finally:\n        provider.cleanup()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _train(self):\n        if self._runner.is_alive():\n            # if started and alive, inform the reporter to continue and\n            # generate the next result\n            self._continue_semaphore.release()\n        else:\n            # if not alive, try to start\n            self._status_reporter._start()\n            try:\n                self._runner.start()\n            except RuntimeError:\n                # If this is reached, it means the thread was started and is\n                # now done or has raised an exception.\n                pass\n\n        result = None\n        while result is None and self._runner.is_alive():\n            # fetch the next produced result\n            try:\n                result = self._results_queue.get(\n                    block=True, timeout=RESULT_FETCH_TIMEOUT)\n            except queue.Empty:\n                pass\n\n        # if no result were found, then the runner must no longer be alive\n        if result is None:\n            # Try one last time to fetch results in case results were reported\n            # in between the time of the last check and the termination of the\n            # thread runner.\n            try:\n                result = self._results_queue.get(block=False)\n            except queue.Empty:\n                pass\n\n        # check if error occured inside the thread runner\n        if result is None:\n            # only raise an error from the runner if all results are consumed\n            self._report_thread_runner_error(block=True)\n\n            # Under normal conditions, this code should never be reached since\n            # this branch should only be visited if the runner thread raised\n            # an exception. If no exception were raised, it means that the\n            # runner thread never reported any results which should not be\n            # possible when wrapping functions with `wrap_function`.\n            raise TuneError(\n                (\"Wrapped function ran until completion without reporting \"\n                 \"results or raising an exception.\"))\n\n        else:\n            if not self._error_queue.empty():\n                logger.warning(\n                    (\"Runner error waiting to be raised in main thread. \"\n                     \"Logging all available results first.\"))\n\n        # This keyword appears if the train_func using the Function API\n        # finishes without \"done=True\". This duplicates the last result, but\n        # the TrialRunner will not log this result again.\n        if \"__duplicate__\" in result:\n            new_result = self._last_result.copy()\n            new_result.update(result)\n            result = new_result\n\n        self._last_result = result\n        return result", "response": "Implements train method for a Function API."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build_network(self,\n                      images,\n                      phase_train=True,\n                      nclass=1001,\n                      image_depth=3,\n                      data_type=tf.float32,\n                      data_format=\"NCHW\",\n                      use_tf_layers=True,\n                      fp16_vars=False):\n        \"\"\"Returns logits and aux_logits from images.\"\"\"\n        if data_format == \"NCHW\":\n            images = tf.transpose(images, [0, 3, 1, 2])\n        var_type = tf.float32\n        if data_type == tf.float16 and fp16_vars:\n            var_type = tf.float16\n        network = convnet_builder.ConvNetBuilder(\n            images, image_depth, phase_train, use_tf_layers, data_format,\n            data_type, var_type)\n        with tf.variable_scope(\n                \"cg\", custom_getter=network.get_custom_getter()):\n            self.add_inference(network)\n            # Add the final fully-connected class layer\n            logits = (network.affine(nclass, activation=\"linear\")\n                      if not self.skip_final_affine_layer() else\n                      network.top_layer)\n            aux_logits = None\n            if network.aux_top_layer is not None:\n                with network.switch_to_aux_top_layer():\n                    aux_logits = network.affine(\n                        nclass, activation=\"linear\", stddev=0.001)\n        if data_type == tf.float16:\n            # TODO(reedwm): Determine if we should do this cast here.\n            logits = tf.cast(logits, tf.float32)\n            if aux_logits is not None:\n                aux_logits = tf.cast(aux_logits, tf.float32)\n        return logits, aux_logits", "response": "Builds a convnet network from the given images."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef profile(event_type, extra_data=None):\n    worker = ray.worker.global_worker\n    return RayLogSpanRaylet(worker.profiler, event_type, extra_data=extra_data)", "response": "Profile a span of time."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _periodically_flush_profile_events(self):\n        # Note(rkn): This is run on a background thread in the driver. It uses\n        # the raylet client. This should be ok because it doesn't read\n        # from the raylet client and we have the GIL here. However,\n        # if either of those things changes, then we could run into issues.\n        while True:\n            # Sleep for 1 second. This will be interrupted if\n            # self.threads_stopped is set.\n            self.threads_stopped.wait(timeout=1)\n\n            # Exit if we received a signal that we should stop.\n            if self.threads_stopped.is_set():\n                return\n\n            self.flush_profile_data()", "response": "This function is called by the threads that need to be stopped."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef flush_profile_data(self):\n        with self.lock:\n            events = self.events\n            self.events = []\n\n        if self.worker.mode == ray.WORKER_MODE:\n            component_type = \"worker\"\n        else:\n            component_type = \"driver\"\n\n        self.worker.raylet_client.push_profile_events(\n            component_type, ray.UniqueID(self.worker.worker_id),\n            self.worker.node_ip_address, events)", "response": "Push the logged profiling data to the global control store."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_attribute(self, key, value):\n        if not isinstance(key, str) or not isinstance(value, str):\n            raise ValueError(\"The arguments 'key' and 'value' must both be \"\n                             \"strings. Instead they are {} and {}.\".format(\n                                 key, value))\n        self.extra_data[key] = value", "response": "Add a key - value pair to the extra_data dict."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nforwarding pass for the mixer.", "response": "def forward(self, agent_qs, states):\n        \"\"\"Forward pass for the mixer.\n\n        Arguments:\n            agent_qs: Tensor of shape [B, T, n_agents, n_actions]\n            states: Tensor of shape [B, T, state_dim]\n        \"\"\"\n        bs = agent_qs.size(0)\n        states = states.reshape(-1, self.state_dim)\n        agent_qs = agent_qs.view(-1, 1, self.n_agents)\n        # First layer\n        w1 = th.abs(self.hyper_w_1(states))\n        b1 = self.hyper_b_1(states)\n        w1 = w1.view(-1, self.n_agents, self.embed_dim)\n        b1 = b1.view(-1, 1, self.embed_dim)\n        hidden = F.elu(th.bmm(agent_qs, w1) + b1)\n        # Second layer\n        w_final = th.abs(self.hyper_w_final(states))\n        w_final = w_final.view(-1, self.embed_dim, 1)\n        # State-dependent bias\n        v = self.V(states).view(-1, 1, 1)\n        # Compute final output\n        y = th.bmm(hidden, w_final) + v\n        # Reshape and return\n        q_tot = y.view(bs, -1, 1)\n        return q_tot"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef on_trial_complete(self,\n                          trial_id,\n                          result=None,\n                          error=False,\n                          early_terminated=False):\n        \"\"\"Passes the result to SigOpt unless early terminated or errored.\n\n        If a trial fails, it will be reported as a failed Observation, telling\n        the optimizer that the Suggestion led to a metric failure, which\n        updates the feasible region and improves parameter recommendation.\n\n        Creates SigOpt Observation object for trial.\n        \"\"\"\n        if result:\n            self.conn.experiments(self.experiment.id).observations().create(\n                suggestion=self._live_trial_mapping[trial_id].id,\n                value=result[self._reward_attr],\n            )\n            # Update the experiment object\n            self.experiment = self.conn.experiments(self.experiment.id).fetch()\n        elif error or early_terminated:\n            # Reports a failed Observation\n            self.conn.experiments(self.experiment.id).observations().create(\n                failed=True, suggestion=self._live_trial_mapping[trial_id].id)\n        del self._live_trial_mapping[trial_id]", "response": "Passes the result to SigOpt unless early terminated or errored."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef compute_ranks(x):\n    assert x.ndim == 1\n    ranks = np.empty(len(x), dtype=int)\n    ranks[x.argsort()] = np.arange(len(x))\n    return ranks", "response": "Compute ranks in a sequence of ranks."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bottleneck_block_v1(cnn, depth, depth_bottleneck, stride):\n    input_layer = cnn.top_layer\n    in_size = cnn.top_size\n    name_key = \"resnet_v1\"\n    name = name_key + str(cnn.counts[name_key])\n    cnn.counts[name_key] += 1\n\n    with tf.variable_scope(name):\n        if depth == in_size:\n            if stride == 1:\n                shortcut = input_layer\n            else:\n                shortcut = cnn.apool(\n                    1,\n                    1,\n                    stride,\n                    stride,\n                    input_layer=input_layer,\n                    num_channels_in=in_size)\n        else:\n            shortcut = cnn.conv(\n                depth,\n                1,\n                1,\n                stride,\n                stride,\n                activation=None,\n                use_batch_norm=True,\n                input_layer=input_layer,\n                num_channels_in=in_size,\n                bias=None)\n        cnn.conv(\n            depth_bottleneck,\n            1,\n            1,\n            stride,\n            stride,\n            input_layer=input_layer,\n            num_channels_in=in_size,\n            use_batch_norm=True,\n            bias=None)\n        cnn.conv(\n            depth_bottleneck,\n            3,\n            3,\n            1,\n            1,\n            mode=\"SAME_RESNET\",\n            use_batch_norm=True,\n            bias=None)\n        res = cnn.conv(\n            depth, 1, 1, 1, 1, activation=None, use_batch_norm=True, bias=None)\n        output = tf.nn.relu(shortcut + res)\n        cnn.top_layer = output\n        cnn.top_size = depth", "response": "Bottleneck block with identity short - cut for ResNet v1."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbottleneck block with identity short - cut.", "response": "def bottleneck_block(cnn, depth, depth_bottleneck, stride, pre_activation):\n    \"\"\"Bottleneck block with identity short-cut.\n\n  Args:\n    cnn: the network to append bottleneck blocks.\n    depth: the number of output filters for this bottleneck block.\n    depth_bottleneck: the number of bottleneck filters for this block.\n    stride: Stride used in the first layer of the bottleneck block.\n    pre_activation: use pre_activation structure used in v2 or not.\n  \"\"\"\n    if pre_activation:\n        bottleneck_block_v2(cnn, depth, depth_bottleneck, stride)\n    else:\n        bottleneck_block_v1(cnn, depth, depth_bottleneck, stride)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef residual_block(cnn, depth, stride, pre_activation):\n    input_layer = cnn.top_layer\n    in_size = cnn.top_size\n    if in_size != depth:\n        # Plan A of shortcut.\n        shortcut = cnn.apool(\n            1,\n            1,\n            stride,\n            stride,\n            input_layer=input_layer,\n            num_channels_in=in_size)\n        padding = (depth - in_size) // 2\n        if cnn.channel_pos == \"channels_last\":\n            shortcut = tf.pad(shortcut,\n                              [[0, 0], [0, 0], [0, 0], [padding, padding]])\n        else:\n            shortcut = tf.pad(shortcut,\n                              [[0, 0], [padding, padding], [0, 0], [0, 0]])\n    else:\n        shortcut = input_layer\n    if pre_activation:\n        res = cnn.batch_norm(input_layer)\n        res = tf.nn.relu(res)\n    else:\n        res = input_layer\n    cnn.conv(\n        depth,\n        3,\n        3,\n        stride,\n        stride,\n        input_layer=res,\n        num_channels_in=in_size,\n        use_batch_norm=True,\n        bias=None)\n    if pre_activation:\n        res = cnn.conv(\n            depth,\n            3,\n            3,\n            1,\n            1,\n            activation=None,\n            use_batch_norm=False,\n            bias=None)\n        output = shortcut + res\n    else:\n        res = cnn.conv(\n            depth, 3, 3, 1, 1, activation=None, use_batch_norm=True, bias=None)\n        output = tf.nn.relu(shortcut + res)\n    cnn.top_layer = output\n    cnn.top_size = depth", "response": "Residual block with identity short - cut."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\napplying updates from the rs and buffer of another filter.", "response": "def apply_changes(self, other, with_buffer=False):\n        \"\"\"Applies updates from the buffer of another filter.\n\n        Params:\n            other (MeanStdFilter): Other filter to apply info from\n            with_buffer (bool): Flag for specifying if the buffer should be\n                copied from other.\n\n        Examples:\n            >>> a = MeanStdFilter(())\n            >>> a(1)\n            >>> a(2)\n            >>> print([a.rs.n, a.rs.mean, a.buffer.n])\n            [2, 1.5, 2]\n            >>> b = MeanStdFilter(())\n            >>> b(10)\n            >>> a.apply_changes(b, with_buffer=False)\n            >>> print([a.rs.n, a.rs.mean, a.buffer.n])\n            [3, 4.333333333333333, 2]\n            >>> a.apply_changes(b, with_buffer=True)\n            >>> print([a.rs.n, a.rs.mean, a.buffer.n])\n            [4, 5.75, 1]\n        \"\"\"\n        self.rs.update(other.buffer)\n        if with_buffer:\n            self.buffer = other.buffer.copy()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a copy of the filter.", "response": "def copy(self):\n        \"\"\"Returns a copy of Filter.\"\"\"\n        other = MeanStdFilter(self.shape)\n        other.sync(self)\n        return other"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef as_serializable(self):\n        other = MeanStdFilter(self.shape)\n        other.sync(self)\n        return other", "response": "Returns non - concurrent version of current class"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef copy(self):\n        other = ConcurrentMeanStdFilter(self.shape)\n        other.sync(self)\n        return other", "response": "Returns a copy of this Filter."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_general_int(s):\n    mo = re.match(r\"(\\d+)([KkMGT]?)$\", s)\n    if mo:\n        i, suffix = mo.group(1, 2)\n        v = int(i)\n        if suffix:\n            if suffix == \"K\" or suffix == \"k\":\n                v *= 1024\n            elif suffix == \"M\":\n                v *= (1024 * 1024)\n            elif suffix == \"G\":\n                v *= (1024 * 1024 * 1024)\n            elif suffix == \"T\":\n                v *= (1024 * 1024 * 1024 * 1024)\n            else:\n                raise ValueError(\"invalid integer string %s\" % s)\n        return v\n    else:\n        v = int(s)\n    return v", "response": "Parse integer with power - of - 2 suffix eg. 32k."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing all_reduce_spec and return a list of AllReduceSpecTuple s.", "response": "def parse_all_reduce_spec(all_reduce_spec):\n    \"\"\"Parse all_reduce_spec.\n\n  Args:\n    all_reduce_spec: a string specifying a combination of all-reduce\n      algorithms to apply for gradient reduction.\n\n  Returns:\n    a list of AllReduceSpecTuple.\n\n  Raises:\n    ValueError: all_reduce_spec is not well-formed.\n\n  An all_reduce_spec has BNF form:\n     int ::= positive whole number\n     g_int ::= int[KkMGT]?\n     alg_spec ::= alg | alg#int\n     range_spec ::= alg_spec | alg_spec/alg_spec\n     spec ::= range_spec | range_spec:g_int:range_spec\n\n  Not all syntactically correct specifications are supported.\n  Examples of supported all_reduce_spec strings, with semantics explained:\n\n    \"xring\" == apply ring all-reduce to all tensors\n    \"xring#2\" == apply ring all-reduce to all tensors, using two simultaneous\n            transfer rings, each operating on 1/2 of each tensor.\n    \"nccl\"  == apply NCCL all-reduce to all tensors (only works within\n            a single worker process where all devices are GPUs)\n    \"nccl/xring\" == apply NCCL all-reduce to all tensors within each worker\n            to produce at least one full-reduced (locally) value,\n            then apply ring all-reduce to one such value from each\n            worker, then apply NCCL broadcast to propagate those globally\n            reduced values back to every device within each worker.\n    \"pscpu\" == Shuffle reduce using worker CPUs as the gather devices: each\n            distributed tensor is reduced by copying all instances to\n            one of the worker CPUs, computing the reduction there, then\n            copying back to each participating device.  Tensor reductions\n            are assigned to specific CPUs round-robin.\n    \"psgpu#4\" == Arrange all GPUs across all workers into groups of 4.\n            Each distributed tensor is shuffle reduced against one\n            such group of 4 GPUs, selected round-robin.  That is, each\n            tensor is split across 4 shards for the reduction.\n    \"pscpu:2k:pscpu#2:64k:xring\" == Apply single-shard pscpu to\n            tensors of size <= 2048 elements, apply 2-shard pscpu to\n            tensors up to size 64k elements, apply xring to larger tensors.\n    \"pscpu/pscpu#2\" == Use shuffle gather to locally reduce each tensor on\n            the worker's CPU, then use 2-shard shuffle to reduce those\n            locally reduced tensors across workers (on the worker CPUs), then\n            scatter the globally reduced values locally from each worker CPU.\n  \"\"\"\n    range_parts = all_reduce_spec.split(\":\") + [\"-1\"]\n    if len(range_parts) % 2:\n        raise ValueError(\n            \"all_reduce_spec not well formed: %s\" % all_reduce_spec)\n    limit = 0\n    spec = []\n    alg = None\n    shards = 1\n    for i, range_part in enumerate(range_parts):\n        if i % 2 == 1:\n            try:\n                limit = parse_general_int(range_part)\n                spec.append(\n                    AllReduceSpecTuple(alg=alg, shards=shards, limit=limit))\n            except ValueError:\n                raise ValueError(\n                    \"all_reduce_spec (%s) contains non-integer range %s\" %\n                    (all_reduce_spec, range_part))\n        else:\n            alg = range_part\n            alg_parts = range_part.split(\"#\")\n            alg = alg_parts[0]\n            if len(alg_parts) > 1:\n                try:\n                    shards = int(alg_parts[1])\n                except ValueError:\n                    raise ValueError(\n                        \"all_reduce_spec (%s) contains non-integer \"\n                        \"shards %s\" % all_reduce_spec, alg_parts[1])\n            else:\n                shards = 1\n            if alg not in [\n                    \"nccl\", \"nccl/xring\", \"nccl/rechd\", \"nccl/pscpu\", \"xring\",\n                    \"pscpu\", \"psgpu\", \"pscpu/pscpu\"\n            ]:\n                raise ValueError(\"all_reduce_spec (%s) contains invalid alg %s\"\n                                 % (all_reduce_spec, alg))\n    return spec"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding a list of device name prefixes for all_reduce.", "response": "def build_all_reduce_device_prefixes(job_name, num_tasks):\n    \"\"\"Build list of device prefix names for all_reduce.\n\n  Args:\n    job_name: \"worker\", \"ps\" or \"localhost\".\n    num_tasks: number of jobs across which device names should be generated.\n\n  Returns:\n     A list of device name prefix strings. Each element spells out the full\n     host name without adding the device.\n     e.g. \"/job:worker/task:0\"\n  \"\"\"\n    if job_name != \"localhost\":\n        return [\"/job:%s/task:%d\" % (job_name, d) for d in range(0, num_tasks)]\n    else:\n        assert num_tasks == 1\n        return [\"/job:%s\" % job_name]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef group_device_names(devices, group_size):\n    num_devices = len(devices)\n    if group_size > num_devices:\n        raise ValueError(\n            \"only %d devices, but group_size=%d\" % (num_devices, group_size))\n    num_groups = (\n        num_devices // group_size + (1 if\n                                     (num_devices % group_size != 0) else 0))\n    groups = [[] for i in range(num_groups)]\n    for i in range(0, num_groups * group_size):\n        groups[i % num_groups].append(devices[i % num_devices])\n    return groups", "response": "Group the names of a list of strings naming devices into groups of group_size."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef split_grads_by_size(threshold_size, device_grads):\n    small_grads = []\n    large_grads = []\n    for dl in device_grads:\n        small_dl = []\n        large_dl = []\n        for (g, v) in dl:\n            tensor_size = g.get_shape().num_elements()\n            if tensor_size <= threshold_size:\n                small_dl.append([g, v])\n            else:\n                large_dl.append([g, v])\n        if small_dl:\n            small_grads.append(small_dl)\n        if large_dl:\n            large_grads.append(large_dl)\n    return small_grads, large_grads", "response": "Break gradients into two sets according to tensor size."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef aggregate_single_gradient(grad_and_vars, use_mean, check_inf_nan):\n    grads = [g for g, _ in grad_and_vars]\n    grad = tf.add_n(grads)\n\n    if use_mean and len(grads) > 1:\n        grad = tf.multiply(grad, 1.0 / len(grads))\n\n    v = grad_and_vars[0][1]\n    if check_inf_nan:\n        has_nan_or_inf = tf.logical_not(tf.reduce_all(tf.is_finite(grads)))\n        return (grad, v), has_nan_or_inf\n    else:\n        return (grad, v), None", "response": "Calculates the average gradient for a single variable across all towers."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef aggregate_gradients_using_copy_with_device_selection(\n        tower_grads, avail_devices, use_mean=True, check_inf_nan=False):\n    \"\"\"Aggregate gradients, controlling device for the aggregation.\n\n  Args:\n    tower_grads: List of lists of (gradient, variable) tuples. The outer list\n      is over towers. The inner list is over individual gradients.\n    use_mean: if True, mean is taken, else sum of gradients is taken.\n    check_inf_nan: If true, check grads for nans and infs.\n\n  Returns:\n    The tuple ([(average_gradient, variable),], has_nan_or_inf) where the\n      gradient has been averaged across all towers. The variable is chosen from\n      the first tower. The has_nan_or_inf indicates the grads has nan or inf.\n  \"\"\"\n    agg_grads = []\n    has_nan_or_inf_list = []\n    for i, single_grads in enumerate(zip(*tower_grads)):\n        with tf.device(avail_devices[i % len(avail_devices)]):\n            grad_and_var, has_nan_or_inf = aggregate_single_gradient(\n                single_grads, use_mean, check_inf_nan)\n            agg_grads.append(grad_and_var)\n            has_nan_or_inf_list.append(has_nan_or_inf)\n    return agg_grads", "response": "Aggregate gradients over all towers and compute the variable for each tower."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsum the gradients and variables of the specified all - reduce algorithm.", "response": "def sum_grad_and_var_all_reduce(grad_and_vars,\n                                num_workers,\n                                alg,\n                                gpu_indices,\n                                aux_devices=None,\n                                num_shards=1):\n    \"\"\"Apply all-reduce algorithm over specified gradient tensors.\"\"\"\n    with tf.name_scope(\"allreduce\"):\n        # Note that each grad_and_vars looks like the following:\n        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n        scaled_grads = [g for g, _ in grad_and_vars]\n        if alg == \"nccl\":\n            from tensorflow.python.ops import nccl_ops\n            summed_grads = nccl_ops.all_sum(scaled_grads)\n        elif alg == \"simple\":\n            summed_grads = build_reduce_sum(scaled_grads)\n        elif alg == \"trivial\":\n            summed_grads = build_trivial_sum(scaled_grads)\n        elif alg == \"xring\":\n            summed_grads = all_reduce.build_ring_all_reduce(\n                scaled_grads, num_workers, num_shards, gpu_indices, tf.add)\n        elif alg == \"nccl/xring\":\n            summed_grads = all_reduce.build_nccl_then_ring(\n                scaled_grads, num_shards, tf.add)\n        elif alg == \"nccl/rechd\":\n            summed_grads = all_reduce.build_nccl_then_recursive_hd(\n                scaled_grads, tf.add)\n        elif alg == \"nccl/pscpu\":\n            summed_grads = all_reduce.build_nccl_then_shuffle(\n                scaled_grads, aux_devices, tf.add, tf.add_n)\n        elif alg == \"pscpu/pscpu\":\n            summed_grads = all_reduce.build_shuffle_then_shuffle(\n                scaled_grads,\n                aux_devices,\n                # TODO(tucker): devise a way of better specifying the device\n                # for the second level.\n                [aux_devices[0]],\n                tf.add_n)\n        elif alg in [\"pscpu\", \"psgpu\"]:\n            summed_grads = all_reduce.build_shuffle_all_reduce(\n                scaled_grads, aux_devices, tf.add_n)\n        else:\n            raise ValueError(\"unsupported all_reduce alg: \", alg)\n\n        result = []\n        for (_, v), g in zip(grad_and_vars, summed_grads):\n            result.append([g, v])\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sum_gradients_all_reduce(dev_prefixes,\n                             tower_grads,\n                             num_workers,\n                             alg,\n                             num_shards,\n                             gpu_indices,\n                             agg_small_grads_max_bytes=0):\n    \"\"\"Apply all-reduce algorithm over specified gradient tensors.\n\n  Args:\n    dev_prefixes: list of prefix strings to use to generate PS device names.\n    tower_grads: the gradients to reduce.\n    num_workers: number of worker processes across entire job.\n    alg: the all-reduce algorithm to apply.\n    num_shards: alg-specific sharding factor.\n    gpu_indices: indices of local GPUs in order usable for ring-reduce.\n    agg_small_grads_max_bytes: largest tensor eligible for aggregation,\n      in number of bytes.\n\n  Returns:\n    list of reduced tensors, packing values\n  \"\"\"\n    alg_contains_shuffle = contains_any(alg, [\"pscpu\", \"psgpu\"])\n    is_hierarchical = \"/\" in alg\n    if \"pscpu\" in alg:\n        aux_devices = [prefix + \"/cpu:0\" for prefix in dev_prefixes]\n    elif \"psgpu\" in alg:\n        aux_devices = [\n            prefix + \"/gpu:%d\" % i for i in range(len(gpu_indices))\n            for prefix in dev_prefixes\n        ]\n    else:\n        aux_devices = [\"/job:localhost/cpu:0\"]\n    aux_device_groups = group_device_names(\n        aux_devices, num_shards if alg_contains_shuffle else 1)\n    group_index = 0\n    if agg_small_grads_max_bytes > 0:\n        tower_grads, packing = pack_small_tensors(\n            tower_grads, max_bytes=agg_small_grads_max_bytes)\n\n    else:\n        packing = None\n    new_tower_grads = []\n    if alg == \"better\":\n        raw_devices = [\"/gpu:%i\" % (i) for i in gpu_indices]\n        agg_grads = aggregate_gradients_using_copy_with_device_selection(\n            tower_grads, raw_devices)\n        for arr in tower_grads:\n            new_tower_grads.append(\n                [(g, v) for (_, v), (g, _) in zip(arr, agg_grads)])\n    else:\n        reduced_gv_list = []\n        for grad_and_vars in zip(*tower_grads):\n            reduced_gv_list.append(\n                sum_grad_and_var_all_reduce(\n                    grad_and_vars, num_workers, alg, gpu_indices, aux_devices\n                    if is_hierarchical else aux_device_groups[group_index],\n                    num_shards))\n            group_index = (group_index + 1) % len(aux_device_groups)\n        new_tower_grads = [list(x) for x in zip(*reduced_gv_list)]\n    return new_tower_grads, packing", "response": "This function calculates the sum of gradients over all - reduce processes across all - reduce devices."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nextracting consecutive ranges and singles from index_list.", "response": "def extract_ranges(index_list, range_size_limit=32):\n    \"\"\"Extract consecutive ranges and singles from index_list.\n\n  Args:\n    index_list: List of monotone increasing non-negative integers.\n    range_size_limit: Largest size range to return.  If a larger\n      consecutive range exists it will be returned as multiple\n      ranges.\n\n  Returns:\n   ranges, singles where ranges is a list of [first, last] pairs of\n     consecutive elements in index_list, and singles is all of the\n     other elements, in original order.\n  \"\"\"\n    if not index_list:\n        return [], []\n    first = index_list[0]\n    last = first\n    ranges = []\n    singles = []\n    for i in index_list[1:]:\n        if i == last + 1 and (last - first) <= range_size_limit:\n            last = i\n        else:\n            if last > first:\n                ranges.append([first, last])\n            else:\n                singles.append(first)\n            first = i\n            last = i\n    if last > first:\n        ranges.append([first, last])\n    else:\n        singles.append(first)\n    return ranges, singles"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nforming the concatenation of a range of small tensors.", "response": "def pack_range(key, packing, grad_vars, rng):\n    \"\"\"Form the concatenation of a specified range of gradient tensors.\n\n  Args:\n    key: Value under which to store meta-data in packing that will be used\n      later to restore the grad_var list structure.\n    packing: Dict holding data describing packed ranges of small tensors.\n    grad_vars: List of (grad, var) pairs for one tower.\n    rng: A pair of integers giving the first, last indices of a consecutive\n      range of tensors to be packed.\n\n  Returns:\n    A tensor that is the concatenation of all the specified small tensors.\n  \"\"\"\n    to_pack = grad_vars[rng[0]:rng[1] + 1]\n    members = []\n    variables = []\n    restore_shapes = []\n    with tf.name_scope(\"pack\"):\n        for g, v in to_pack:\n            variables.append(v)\n            restore_shapes.append(g.shape)\n            with tf.device(g.device):\n                members.append(tf.reshape(g, [-1]))\n        packing[key] = GradPackTuple(\n            indices=range(rng[0], rng[1] + 1),\n            vars=variables,\n            shapes=restore_shapes)\n        with tf.device(members[0].device):\n            return tf.concat(members, 0)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unpack_grad_tuple(gv, gpt):\n    elt_widths = [x.num_elements() for x in gpt.shapes]\n    with tf.device(gv[0][0].device):\n        with tf.name_scope(\"unpack\"):\n            splits = tf.split(gv[0], elt_widths)\n            unpacked_gv = []\n            for idx, s in enumerate(splits):\n                unpacked_gv.append((tf.reshape(s, gpt.shapes[idx]),\n                                    gpt.vars[idx]))\n    return unpacked_gv", "response": "Unpack a previously packed collection of gradient tensors into a list of tuples corresponding to the values that were\n     originally packed into gv."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconcatenating gradients together more intelligently.", "response": "def pack_small_tensors(tower_grads, max_bytes=0):\n    \"\"\"Concatenate gradients together more intelligently.\n\n  Does binpacking\n  Args:\n    tower_grads: List of lists of (gradient, variable) tuples.\n    max_bytes: Int giving max number of bytes in a tensor that\n      may be considered small.\n  \"\"\"\n    assert max_bytes >= 0\n    orig_grads = [g for g, _ in tower_grads[0]]\n    # Check to make sure sizes are accurate; not entirely important\n    assert all(g.dtype == tf.float32 for g in orig_grads)\n    sizes = [4 * g.shape.num_elements() for g in orig_grads]\n    print_stats(sizes)\n    small_ranges = []\n    large_indices = []\n    new_sizes = []\n\n    def end_interval(indices, small_ranges, large_indices):\n        if len(indices) > 1:\n            small_ranges.insert(0, [indices[0], indices[-1]])\n        else:\n            large_indices.insert(0, indices[0])\n\n    cur_range = []\n    cur_size = 0\n    for i, s in reversed(list(enumerate(sizes))):\n        if cur_size > max_bytes:\n            end_interval(cur_range, small_ranges, large_indices)\n            new_sizes.insert(0, cur_size)\n            cur_range = []\n            cur_size = 0\n        cur_range.insert(0, i)\n        cur_size += s\n    end_interval(cur_range, small_ranges, large_indices)\n    new_sizes.insert(0, cur_size)\n\n    print_stats(new_sizes)\n    num_gv = len(orig_grads)\n    packing = {}\n    if len(small_ranges):\n        new_tower_grads = []\n        for dev_idx, gv_list in enumerate(tower_grads):\n            assert len(gv_list) == num_gv, (\n                \"Possible cause: \"\n                \"Networks constructed on different workers \"\n                \"don't have the same number of variables. \"\n                \"If you use tf.GraphKeys or tf.global_variables() \"\n                \"with multiple graphs per worker during network \"\n                \"construction, you need to use \"\n                \"appropriate scopes, see \"\n                \"https://github.com/ray-project/ray/issues/3136\")\n            new_gv_list = []\n            for r in small_ranges:\n                key = \"%d:%d\" % (dev_idx, len(new_gv_list))\n                new_gv_list.append((pack_range(key, packing, gv_list, r),\n                                    \"packing_var_placeholder\"))\n            for i in large_indices:\n                new_gv_list.append(gv_list[i])\n            new_tower_grads.append(new_gv_list)\n        return new_tower_grads, packing\n    else:\n        return tower_grads, None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef unpack_small_tensors(tower_grads, packing):\n    if not packing:\n        return tower_grads\n    new_tower_grads = []\n    num_devices = len(tower_grads)\n    num_packed = len(packing.keys()) // num_devices\n    for dev_idx, gv_list in enumerate(tower_grads):\n        new_gv_list = gv_list[num_packed:]\n        for i in xrange(0, num_packed):\n            k = \"%d:%d\" % (dev_idx, i)\n            gpt = packing[k]\n            gv = unpack_grad_tuple(gv_list[i], gpt)\n            for gi, idx in enumerate(gpt.indices):\n                assert idx == gpt.indices[gi]\n                new_gv_list.insert(idx, gv[gi])\n        new_tower_grads.append(new_gv_list)\n    return new_tower_grads", "response": "Unpacks small tensors into a list of lists of gradients and variables."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninitialize the object variables.", "response": "def _init(self):\n        \"\"\"CSV outputted with Headers as first set of results.\"\"\"\n        # Note that we assume params.json was already created by JsonLogger\n        progress_file = os.path.join(self.logdir, \"progress.csv\")\n        self._continuing = os.path.exists(progress_file)\n        self._file = open(progress_file, \"a\")\n        self._csv_out = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sync_results_to_new_location(self, worker_ip):\n        if worker_ip != self._log_syncer.worker_ip:\n            self._log_syncer.set_worker_ip(worker_ip)\n            self._log_syncer.sync_to_worker_if_possible()", "response": "Sends the current log directory to the remote node."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninserts value into config by path generating intermediate dictionaries.", "response": "def deep_insert(path_list, value, config):\n    \"\"\"Inserts value into config by path, generating intermediate dictionaries.\n\n    Example:\n        >>> deep_insert(path.split(\".\"), value, {})\n    \"\"\"\n    if len(path_list) > 1:\n        inside_config = config.setdefault(path_list[0], {})\n        deep_insert(path_list[1:], value, inside_config)\n    else:\n        config[path_list[0]] = value"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a FunctionDescriptor instance from list of bytes. This function is used to create the function descriptor from backend data. Args: cls: Current class which is required argument for classmethod. function_descriptor_list: list of bytes to represent the function descriptor. Returns: The FunctionDescriptor instance created from the bytes list.", "response": "def from_bytes_list(cls, function_descriptor_list):\n        \"\"\"Create a FunctionDescriptor instance from list of bytes.\n\n        This function is used to create the function descriptor from\n        backend data.\n\n        Args:\n            cls: Current class which is required argument for classmethod.\n            function_descriptor_list: list of bytes to represent the\n                function descriptor.\n\n        Returns:\n            The FunctionDescriptor instance created from the bytes list.\n        \"\"\"\n        assert isinstance(function_descriptor_list, list)\n        if len(function_descriptor_list) == 0:\n            # This is a function descriptor of driver task.\n            return FunctionDescriptor.for_driver_task()\n        elif (len(function_descriptor_list) == 3\n              or len(function_descriptor_list) == 4):\n            module_name = ensure_str(function_descriptor_list[0])\n            class_name = ensure_str(function_descriptor_list[1])\n            function_name = ensure_str(function_descriptor_list[2])\n            if len(function_descriptor_list) == 4:\n                return cls(module_name, function_name, class_name,\n                           function_descriptor_list[3])\n            else:\n                return cls(module_name, function_name, class_name)\n        else:\n            raise Exception(\n                \"Invalid input for FunctionDescriptor.from_bytes_list\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a FunctionDescriptor instance from a python function instance.", "response": "def from_function(cls, function):\n        \"\"\"Create a FunctionDescriptor from a function instance.\n\n        This function is used to create the function descriptor from\n        a python function. If a function is a class function, it should\n        not be used by this function.\n\n        Args:\n            cls: Current class which is required argument for classmethod.\n            function: the python function used to create the function\n                descriptor.\n\n        Returns:\n            The FunctionDescriptor instance created according to the function.\n        \"\"\"\n        module_name = function.__module__\n        function_name = function.__name__\n        class_name = \"\"\n\n        function_source_hasher = hashlib.sha1()\n        try:\n            # If we are running a script or are in IPython, include the source\n            # code in the hash.\n            source = inspect.getsource(function)\n            if sys.version_info[0] >= 3:\n                source = source.encode()\n            function_source_hasher.update(source)\n            function_source_hash = function_source_hasher.digest()\n        except (IOError, OSError, TypeError):\n            # Source code may not be available:\n            # e.g. Cython or Python interpreter.\n            function_source_hash = b\"\"\n\n        return cls(module_name, function_name, class_name,\n                   function_source_hash)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a FunctionDescriptor instance from a class.", "response": "def from_class(cls, target_class):\n        \"\"\"Create a FunctionDescriptor from a class.\n\n        Args:\n            cls: Current class which is required argument for classmethod.\n            target_class: the python class used to create the function\n                descriptor.\n\n        Returns:\n            The FunctionDescriptor instance created according to the class.\n        \"\"\"\n        module_name = target_class.__module__\n        class_name = target_class.__name__\n        return cls(module_name, \"__init__\", class_name)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_for_driver_task(self):\n        return all(\n            len(x) == 0\n            for x in [self.module_name, self.class_name, self.function_name])", "response": "See whether this function descriptor is for a driver task or not."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the function id of the current function descriptor.", "response": "def _get_function_id(self):\n        \"\"\"Calculate the function id of current function descriptor.\n\n        This function id is calculated from all the fields of function\n        descriptor.\n\n        Returns:\n            ray.ObjectID to represent the function descriptor.\n        \"\"\"\n        if self.is_for_driver_task:\n            return ray.FunctionID.nil()\n        function_id_hash = hashlib.sha1()\n        # Include the function module and name in the hash.\n        function_id_hash.update(self.module_name.encode(\"ascii\"))\n        function_id_hash.update(self.function_name.encode(\"ascii\"))\n        function_id_hash.update(self.class_name.encode(\"ascii\"))\n        function_id_hash.update(self._function_source_hash)\n        # Compute the function ID.\n        function_id = function_id_hash.digest()\n        return ray.FunctionID(function_id)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_function_descriptor_list(self):\n        descriptor_list = []\n        if self.is_for_driver_task:\n            # Driver task returns an empty list.\n            return descriptor_list\n        else:\n            descriptor_list.append(self.module_name.encode(\"ascii\"))\n            descriptor_list.append(self.class_name.encode(\"ascii\"))\n            descriptor_list.append(self.function_name.encode(\"ascii\"))\n            if len(self._function_source_hash) != 0:\n                descriptor_list.append(self._function_source_hash)\n            return descriptor_list", "response": "Return a list of bytes representing the function descriptor."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef export_cached(self):\n        for remote_function in self._functions_to_export:\n            self._do_export(remote_function)\n        self._functions_to_export = None\n        for info in self._actors_to_export:\n            (key, actor_class_info) = info\n            self._publish_actor_class_to_key(key, actor_class_info)", "response": "Export cached remote functions to the local cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef export(self, remote_function):\n        if self._worker.mode is None:\n            # If the worker isn't connected, cache the function\n            # and export it later.\n            self._functions_to_export.append(remote_function)\n            return\n        if self._worker.mode != ray.worker.SCRIPT_MODE:\n            # Don't need to export if the worker is not a driver.\n            return\n        self._do_export(remote_function)", "response": "Exports a remote function."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _do_export(self, remote_function):\n        if self._worker.load_code_from_local:\n            return\n        # Work around limitations of Python pickling.\n        function = remote_function._function\n        function_name_global_valid = function.__name__ in function.__globals__\n        function_name_global_value = function.__globals__.get(\n            function.__name__)\n        # Allow the function to reference itself as a global variable\n        if not is_cython(function):\n            function.__globals__[function.__name__] = remote_function\n        try:\n            pickled_function = pickle.dumps(function)\n        finally:\n            # Undo our changes\n            if function_name_global_valid:\n                function.__globals__[function.__name__] = (\n                    function_name_global_value)\n            else:\n                del function.__globals__[function.__name__]\n\n        check_oversized_pickle(pickled_function,\n                               remote_function._function_name,\n                               \"remote function\", self._worker)\n        key = (b\"RemoteFunction:\" + self._worker.task_driver_id.binary() + b\":\"\n               + remote_function._function_descriptor.function_id.binary())\n        self._worker.redis_client.hmset(\n            key, {\n                \"driver_id\": self._worker.task_driver_id.binary(),\n                \"function_id\": remote_function._function_descriptor.\n                function_id.binary(),\n                \"name\": remote_function._function_name,\n                \"module\": function.__module__,\n                \"function\": pickled_function,\n                \"max_calls\": remote_function._max_calls\n            })\n        self._worker.redis_client.rpush(\"Exports\", key)", "response": "Pickle a remote function and export it to redis."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfetches a remote function from the scheduler and register it with the scheduler.", "response": "def fetch_and_register_remote_function(self, key):\n        \"\"\"Import a remote function.\"\"\"\n        (driver_id_str, function_id_str, function_name, serialized_function,\n         num_return_vals, module, resources,\n         max_calls) = self._worker.redis_client.hmget(key, [\n             \"driver_id\", \"function_id\", \"name\", \"function\", \"num_return_vals\",\n             \"module\", \"resources\", \"max_calls\"\n         ])\n        function_id = ray.FunctionID(function_id_str)\n        driver_id = ray.DriverID(driver_id_str)\n        function_name = decode(function_name)\n        max_calls = int(max_calls)\n        module = decode(module)\n\n        # This is a placeholder in case the function can't be unpickled. This\n        # will be overwritten if the function is successfully registered.\n        def f():\n            raise Exception(\"This function was not imported properly.\")\n\n        # This function is called by ImportThread. This operation needs to be\n        # atomic. Otherwise, there is race condition. Another thread may use\n        # the temporary function above before the real function is ready.\n        with self.lock:\n            self._function_execution_info[driver_id][function_id] = (\n                FunctionExecutionInfo(\n                    function=f,\n                    function_name=function_name,\n                    max_calls=max_calls))\n            self._num_task_executions[driver_id][function_id] = 0\n\n            try:\n                function = pickle.loads(serialized_function)\n            except Exception:\n                # If an exception was thrown when the remote function was\n                # imported, we record the traceback and notify the scheduler\n                # of the failure.\n                traceback_str = format_error_message(traceback.format_exc())\n                # Log the error message.\n                push_error_to_driver(\n                    self._worker,\n                    ray_constants.REGISTER_REMOTE_FUNCTION_PUSH_ERROR,\n                    \"Failed to unpickle the remote function '{}' with \"\n                    \"function ID {}. Traceback:\\n{}\".format(\n                        function_name, function_id.hex(), traceback_str),\n                    driver_id=driver_id)\n            else:\n                # The below line is necessary. Because in the driver process,\n                # if the function is defined in the file where the python\n                # script was started from, its module is `__main__`.\n                # However in the worker process, the `__main__` module is a\n                # different module, which is `default_worker.py`\n                function.__module__ = module\n                self._function_execution_info[driver_id][function_id] = (\n                    FunctionExecutionInfo(\n                        function=function,\n                        function_name=function_name,\n                        max_calls=max_calls))\n                # Add the function to the function table.\n                self._worker.redis_client.rpush(\n                    b\"FunctionTable:\" + function_id.binary(),\n                    self._worker.worker_id)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the FunctionExecutionInfo object for a remote function.", "response": "def get_execution_info(self, driver_id, function_descriptor):\n        \"\"\"Get the FunctionExecutionInfo of a remote function.\n\n        Args:\n            driver_id: ID of the driver that the function belongs to.\n            function_descriptor: The FunctionDescriptor of the function to get.\n\n        Returns:\n            A FunctionExecutionInfo object.\n        \"\"\"\n        if self._worker.load_code_from_local:\n            # Load function from local code.\n            # Currently, we don't support isolating code by drivers,\n            # thus always set driver ID to NIL here.\n            driver_id = ray.DriverID.nil()\n            if not function_descriptor.is_actor_method():\n                self._load_function_from_local(driver_id, function_descriptor)\n        else:\n            # Load function from GCS.\n            # Wait until the function to be executed has actually been\n            # registered on this worker. We will push warnings to the user if\n            # we spend too long in this loop.\n            # The driver function may not be found in sys.path. Try to load\n            # the function from GCS.\n            with profiling.profile(\"wait_for_function\"):\n                self._wait_for_function(function_descriptor, driver_id)\n        try:\n            function_id = function_descriptor.function_id\n            info = self._function_execution_info[driver_id][function_id]\n        except KeyError as e:\n            message = (\"Error occurs in get_execution_info: \"\n                       \"driver_id: %s, function_descriptor: %s. Message: %s\" %\n                       (driver_id, function_descriptor, e))\n            raise KeyError(message)\n        return info"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwaiting until the function to be executed is present on this worker. This method will simply loop until the import thread has imported the relevant function. If we spend too long in this loop, that may indicate a problem somewhere and we will push an error message to the user. If this worker is an actor, then this will wait until the actor has been defined. Args: function_descriptor : The FunctionDescriptor of the function that we want to execute. driver_id (str): The ID of the driver to push the error message to if this times out.", "response": "def _wait_for_function(self, function_descriptor, driver_id, timeout=10):\n        \"\"\"Wait until the function to be executed is present on this worker.\n\n        This method will simply loop until the import thread has imported the\n        relevant function. If we spend too long in this loop, that may indicate\n        a problem somewhere and we will push an error message to the user.\n\n        If this worker is an actor, then this will wait until the actor has\n        been defined.\n\n        Args:\n            function_descriptor : The FunctionDescriptor of the function that\n                we want to execute.\n            driver_id (str): The ID of the driver to push the error message to\n                if this times out.\n        \"\"\"\n        start_time = time.time()\n        # Only send the warning once.\n        warning_sent = False\n        while True:\n            with self.lock:\n                if (self._worker.actor_id.is_nil()\n                        and (function_descriptor.function_id in\n                             self._function_execution_info[driver_id])):\n                    break\n                elif not self._worker.actor_id.is_nil() and (\n                        self._worker.actor_id in self._worker.actors):\n                    break\n            if time.time() - start_time > timeout:\n                warning_message = (\"This worker was asked to execute a \"\n                                   \"function that it does not have \"\n                                   \"registered. You may have to restart \"\n                                   \"Ray.\")\n                if not warning_sent:\n                    ray.utils.push_error_to_driver(\n                        self._worker,\n                        ray_constants.WAIT_FOR_FUNCTION_PUSH_ERROR,\n                        warning_message,\n                        driver_id=driver_id)\n                warning_sent = True\n            time.sleep(0.001)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _publish_actor_class_to_key(self, key, actor_class_info):\n        # We set the driver ID here because it may not have been available when\n        # the actor class was defined.\n        self._worker.redis_client.hmset(key, actor_class_info)\n        self._worker.redis_client.rpush(\"Exports\", key)", "response": "Push an actor class definition to Redis."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_actor_class(self, driver_id, function_descriptor):\n        function_id = function_descriptor.function_id\n        # Check if the actor class already exists in the cache.\n        actor_class = self._loaded_actor_classes.get(function_id, None)\n        if actor_class is None:\n            # Load actor class.\n            if self._worker.load_code_from_local:\n                driver_id = ray.DriverID.nil()\n                # Load actor class from local code.\n                actor_class = self._load_actor_from_local(\n                    driver_id, function_descriptor)\n            else:\n                # Load actor class from GCS.\n                actor_class = self._load_actor_class_from_gcs(\n                    driver_id, function_descriptor)\n            # Save the loaded actor class in cache.\n            self._loaded_actor_classes[function_id] = actor_class\n\n            # Generate execution info for the methods of this actor class.\n            module_name = function_descriptor.module_name\n            actor_class_name = function_descriptor.class_name\n            actor_methods = inspect.getmembers(\n                actor_class, predicate=is_function_or_method)\n            for actor_method_name, actor_method in actor_methods:\n                method_descriptor = FunctionDescriptor(\n                    module_name, actor_method_name, actor_class_name)\n                method_id = method_descriptor.function_id\n                executor = self._make_actor_method_executor(\n                    actor_method_name,\n                    actor_method,\n                    actor_imported=True,\n                )\n                self._function_execution_info[driver_id][method_id] = (\n                    FunctionExecutionInfo(\n                        function=executor,\n                        function_name=actor_method_name,\n                        max_calls=0,\n                    ))\n                self._num_task_executions[driver_id][method_id] = 0\n            self._num_task_executions[driver_id][function_id] = 0\n        return actor_class", "response": "Loads the actor class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading actor class from local code.", "response": "def _load_actor_from_local(self, driver_id, function_descriptor):\n        \"\"\"Load actor class from local code.\"\"\"\n        module_name, class_name = (function_descriptor.module_name,\n                                   function_descriptor.class_name)\n        try:\n            module = importlib.import_module(module_name)\n            actor_class = getattr(module, class_name)\n            if isinstance(actor_class, ray.actor.ActorClass):\n                return actor_class._modified_class\n            else:\n                return actor_class\n        except Exception:\n            logger.exception(\n                \"Failed to load actor_class %s.\".format(class_name))\n            raise Exception(\n                \"Actor {} failed to be imported from local code.\".format(\n                    class_name))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _load_actor_class_from_gcs(self, driver_id, function_descriptor):\n        key = (b\"ActorClass:\" + driver_id.binary() + b\":\" +\n               function_descriptor.function_id.binary())\n        # Wait for the actor class key to have been imported by the\n        # import thread. TODO(rkn): It shouldn't be possible to end\n        # up in an infinite loop here, but we should push an error to\n        # the driver if too much time is spent here.\n        while key not in self.imported_actor_classes:\n            time.sleep(0.001)\n\n        # Fetch raw data from GCS.\n        (driver_id_str, class_name, module, pickled_class,\n         actor_method_names) = self._worker.redis_client.hmget(\n             key, [\n                 \"driver_id\", \"class_name\", \"module\", \"class\",\n                 \"actor_method_names\"\n             ])\n\n        class_name = ensure_str(class_name)\n        module_name = ensure_str(module)\n        driver_id = ray.DriverID(driver_id_str)\n        actor_method_names = json.loads(ensure_str(actor_method_names))\n\n        actor_class = None\n        try:\n            with self.lock:\n                actor_class = pickle.loads(pickled_class)\n        except Exception:\n            logger.exception(\n                \"Failed to load actor class %s.\".format(class_name))\n            # The actor class failed to be unpickled, create a fake actor\n            # class instead (just to produce error messages and to prevent\n            # the driver from hanging).\n            actor_class = self._create_fake_actor_class(\n                class_name, actor_method_names)\n            # If an exception was thrown when the actor was imported, we record\n            # the traceback and notify the scheduler of the failure.\n            traceback_str = ray.utils.format_error_message(\n                traceback.format_exc())\n            # Log the error message.\n            push_error_to_driver(\n                self._worker, ray_constants.REGISTER_ACTOR_PUSH_ERROR,\n                \"Failed to unpickle actor class '{}' for actor ID {}. \"\n                \"Traceback:\\n{}\".format(class_name,\n                                        self._worker.actor_id.hex(),\n                                        traceback_str), driver_id)\n            # TODO(rkn): In the future, it might make sense to have the worker\n            # exit here. However, currently that would lead to hanging if\n            # someone calls ray.get on a method invoked on the actor.\n\n        # The below line is necessary. Because in the driver process,\n        # if the function is defined in the file where the python script\n        # was started from, its module is `__main__`.\n        # However in the worker process, the `__main__` module is a\n        # different module, which is `default_worker.py`\n        actor_class.__module__ = module_name\n        return actor_class", "response": "Load actor class from GCS."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates an executor that wraps a user - defined actor method and returns the return values.", "response": "def _make_actor_method_executor(self, method_name, method, actor_imported):\n        \"\"\"Make an executor that wraps a user-defined actor method.\n\n        The wrapped method updates the worker's internal state and performs any\n        necessary checkpointing operations.\n\n        Args:\n            method_name (str): The name of the actor method.\n            method (instancemethod): The actor method to wrap. This should be a\n                method defined on the actor class and should therefore take an\n                instance of the actor as the first argument.\n            actor_imported (bool): Whether the actor has been imported.\n                Checkpointing operations will not be run if this is set to\n                False.\n\n        Returns:\n            A function that executes the given actor method on the worker's\n                stored instance of the actor. The function also updates the\n                worker's internal state to record the executed method.\n        \"\"\"\n\n        def actor_method_executor(dummy_return_id, actor, *args):\n            # Update the actor's task counter to reflect the task we're about\n            # to execute.\n            self._worker.actor_task_counter += 1\n\n            # Execute the assigned method and save a checkpoint if necessary.\n            try:\n                if is_class_method(method):\n                    method_returns = method(*args)\n                else:\n                    method_returns = method(actor, *args)\n            except Exception as e:\n                # Save the checkpoint before allowing the method exception\n                # to be thrown, but don't save the checkpoint for actor\n                # creation task.\n                if (isinstance(actor, ray.actor.Checkpointable)\n                        and self._worker.actor_task_counter != 1):\n                    self._save_and_log_checkpoint(actor)\n                raise e\n            else:\n                # Handle any checkpointing operations before storing the\n                # method's return values.\n                # NOTE(swang): If method_returns is a pointer to the actor's\n                # state and the checkpointing operations can modify the return\n                # values if they mutate the actor's state. Is this okay?\n                if isinstance(actor, ray.actor.Checkpointable):\n                    # If this is the first task to execute on the actor, try to\n                    # resume from a checkpoint.\n                    if self._worker.actor_task_counter == 1:\n                        if actor_imported:\n                            self._restore_and_log_checkpoint(actor)\n                    else:\n                        # Save the checkpoint before returning the method's\n                        # return values.\n                        self._save_and_log_checkpoint(actor)\n                return method_returns\n\n        return actor_method_executor"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _save_and_log_checkpoint(self, actor):\n        actor_id = self._worker.actor_id\n        checkpoint_info = self._worker.actor_checkpoint_info[actor_id]\n        checkpoint_info.num_tasks_since_last_checkpoint += 1\n        now = int(1000 * time.time())\n        checkpoint_context = ray.actor.CheckpointContext(\n            actor_id, checkpoint_info.num_tasks_since_last_checkpoint,\n            now - checkpoint_info.last_checkpoint_timestamp)\n        # If we should take a checkpoint, notify raylet to prepare a checkpoint\n        # and then call `save_checkpoint`.\n        if actor.should_checkpoint(checkpoint_context):\n            try:\n                now = int(1000 * time.time())\n                checkpoint_id = (self._worker.raylet_client.\n                                 prepare_actor_checkpoint(actor_id))\n                checkpoint_info.checkpoint_ids.append(checkpoint_id)\n                actor.save_checkpoint(actor_id, checkpoint_id)\n                if (len(checkpoint_info.checkpoint_ids) >\n                        ray._config.num_actor_checkpoints_to_keep()):\n                    actor.checkpoint_expired(\n                        actor_id,\n                        checkpoint_info.checkpoint_ids.pop(0),\n                    )\n                checkpoint_info.num_tasks_since_last_checkpoint = 0\n                checkpoint_info.last_checkpoint_timestamp = now\n            except Exception:\n                # Checkpoint save or reload failed. Notify the driver.\n                traceback_str = ray.utils.format_error_message(\n                    traceback.format_exc())\n                ray.utils.push_error_to_driver(\n                    self._worker,\n                    ray_constants.CHECKPOINT_PUSH_ERROR,\n                    traceback_str,\n                    driver_id=self._worker.task_driver_id)", "response": "Save an actor checkpoint if necessary and log any errors."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _restore_and_log_checkpoint(self, actor):\n        actor_id = self._worker.actor_id\n        try:\n            checkpoints = ray.actor.get_checkpoints_for_actor(actor_id)\n            if len(checkpoints) > 0:\n                # If we found previously saved checkpoints for this actor,\n                # call the `load_checkpoint` callback.\n                checkpoint_id = actor.load_checkpoint(actor_id, checkpoints)\n                if checkpoint_id is not None:\n                    # Check that the returned checkpoint id is in the\n                    # `available_checkpoints` list.\n                    msg = (\n                        \"`load_checkpoint` must return a checkpoint id that \" +\n                        \"exists in the `available_checkpoints` list, or eone.\")\n                    assert any(checkpoint_id == checkpoint.checkpoint_id\n                               for checkpoint in checkpoints), msg\n                    # Notify raylet that this actor has been resumed from\n                    # a checkpoint.\n                    (self._worker.raylet_client.\n                     notify_actor_resumed_from_checkpoint(\n                         actor_id, checkpoint_id))\n        except Exception:\n            # Checkpoint save or reload failed. Notify the driver.\n            traceback_str = ray.utils.format_error_message(\n                traceback.format_exc())\n            ray.utils.push_error_to_driver(\n                self._worker,\n                ray_constants.CHECKPOINT_PUSH_ERROR,\n                traceback_str,\n                driver_id=self._worker.task_driver_id)", "response": "Restore an actor from a checkpoint if available and log any errors."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprocess observations from the environment and prepare for policy evaluation.", "response": "def _process_observations(base_env, policies, batch_builder_pool,\n                          active_episodes, unfiltered_obs, rewards, dones,\n                          infos, off_policy_actions, horizon, preprocessors,\n                          obs_filters, unroll_length, pack, callbacks,\n                          soft_horizon):\n    \"\"\"Record new data from the environment and prepare for policy evaluation.\n\n    Returns:\n        active_envs: set of non-terminated env ids\n        to_eval: map of policy_id to list of agent PolicyEvalData\n        outputs: list of metrics and samples to return from the sampler\n    \"\"\"\n\n    active_envs = set()\n    to_eval = defaultdict(list)\n    outputs = []\n\n    # For each environment\n    for env_id, agent_obs in unfiltered_obs.items():\n        new_episode = env_id not in active_episodes\n        episode = active_episodes[env_id]\n        if not new_episode:\n            episode.length += 1\n            episode.batch_builder.count += 1\n            episode._add_agent_rewards(rewards[env_id])\n\n        if (episode.batch_builder.total() > max(1000, unroll_length * 10)\n                and log_once(\"large_batch_warning\")):\n            logger.warning(\n                \"More than {} observations for {} env steps \".format(\n                    episode.batch_builder.total(),\n                    episode.batch_builder.count) + \"are buffered in \"\n                \"the sampler. If this is more than you expected, check that \"\n                \"that you set a horizon on your environment correctly. Note \"\n                \"that in multi-agent environments, `sample_batch_size` sets \"\n                \"the batch size based on environment steps, not the steps of \"\n                \"individual agents, which can result in unexpectedly large \"\n                \"batches.\")\n\n        # Check episode termination conditions\n        if dones[env_id][\"__all__\"] or episode.length >= horizon:\n            hit_horizon = (episode.length >= horizon\n                           and not dones[env_id][\"__all__\"])\n            all_done = True\n            atari_metrics = _fetch_atari_metrics(base_env)\n            if atari_metrics is not None:\n                for m in atari_metrics:\n                    outputs.append(\n                        m._replace(custom_metrics=episode.custom_metrics))\n            else:\n                outputs.append(\n                    RolloutMetrics(episode.length, episode.total_reward,\n                                   dict(episode.agent_rewards),\n                                   episode.custom_metrics, {}))\n        else:\n            hit_horizon = False\n            all_done = False\n            active_envs.add(env_id)\n\n        # For each agent in the environment\n        for agent_id, raw_obs in agent_obs.items():\n            policy_id = episode.policy_for(agent_id)\n            prep_obs = _get_or_raise(preprocessors,\n                                     policy_id).transform(raw_obs)\n            if log_once(\"prep_obs\"):\n                logger.info(\"Preprocessed obs: {}\".format(summarize(prep_obs)))\n\n            filtered_obs = _get_or_raise(obs_filters, policy_id)(prep_obs)\n            if log_once(\"filtered_obs\"):\n                logger.info(\"Filtered obs: {}\".format(summarize(filtered_obs)))\n\n            agent_done = bool(all_done or dones[env_id].get(agent_id))\n            if not agent_done:\n                to_eval[policy_id].append(\n                    PolicyEvalData(env_id, agent_id, filtered_obs,\n                                   infos[env_id].get(agent_id, {}),\n                                   episode.rnn_state_for(agent_id),\n                                   episode.last_action_for(agent_id),\n                                   rewards[env_id][agent_id] or 0.0))\n\n            last_observation = episode.last_observation_for(agent_id)\n            episode._set_last_observation(agent_id, filtered_obs)\n            episode._set_last_raw_obs(agent_id, raw_obs)\n            episode._set_last_info(agent_id, infos[env_id].get(agent_id, {}))\n\n            # Record transition info if applicable\n            if (last_observation is not None and infos[env_id].get(\n                    agent_id, {}).get(\"training_enabled\", True)):\n                episode.batch_builder.add_values(\n                    agent_id,\n                    policy_id,\n                    t=episode.length - 1,\n                    eps_id=episode.episode_id,\n                    agent_index=episode._agent_index(agent_id),\n                    obs=last_observation,\n                    actions=episode.last_action_for(agent_id),\n                    rewards=rewards[env_id][agent_id],\n                    prev_actions=episode.prev_action_for(agent_id),\n                    prev_rewards=episode.prev_reward_for(agent_id),\n                    dones=(False\n                           if (hit_horizon and soft_horizon) else agent_done),\n                    infos=infos[env_id].get(agent_id, {}),\n                    new_obs=filtered_obs,\n                    **episode.last_pi_info_for(agent_id))\n\n        # Invoke the step callback after the step is logged to the episode\n        if callbacks.get(\"on_episode_step\"):\n            callbacks[\"on_episode_step\"]({\"env\": base_env, \"episode\": episode})\n\n        # Cut the batch if we're not packing multiple episodes into one,\n        # or if we've exceeded the requested batch size.\n        if episode.batch_builder.has_pending_data():\n            if dones[env_id][\"__all__\"]:\n                episode.batch_builder.check_missing_dones()\n            if (all_done and not pack) or \\\n                    episode.batch_builder.count >= unroll_length:\n                outputs.append(episode.batch_builder.build_and_reset(episode))\n            elif all_done:\n                # Make sure postprocessor stays within one episode\n                episode.batch_builder.postprocess_batch_so_far(episode)\n\n        if all_done:\n            # Handle episode termination\n            batch_builder_pool.append(episode.batch_builder)\n            if callbacks.get(\"on_episode_end\"):\n                callbacks[\"on_episode_end\"]({\n                    \"env\": base_env,\n                    \"policy\": policies,\n                    \"episode\": episode\n                })\n            if hit_horizon and soft_horizon:\n                episode.soft_reset()\n                resetted_obs = agent_obs\n            else:\n                del active_episodes[env_id]\n                resetted_obs = base_env.try_reset(env_id)\n            if resetted_obs is None:\n                # Reset not supported, drop this env from the ready list\n                if horizon != float(\"inf\"):\n                    raise ValueError(\n                        \"Setting episode horizon requires reset() support \"\n                        \"from the environment.\")\n            elif resetted_obs != ASYNC_RESET_RETURN:\n                # Creates a new episode if this is not async return\n                # If reset is async, we will get its result in some future poll\n                episode = active_episodes[env_id]\n                for agent_id, raw_obs in resetted_obs.items():\n                    policy_id = episode.policy_for(agent_id)\n                    policy = _get_or_raise(policies, policy_id)\n                    prep_obs = _get_or_raise(preprocessors,\n                                             policy_id).transform(raw_obs)\n                    filtered_obs = _get_or_raise(obs_filters,\n                                                 policy_id)(prep_obs)\n                    episode._set_last_observation(agent_id, filtered_obs)\n                    to_eval[policy_id].append(\n                        PolicyEvalData(\n                            env_id, agent_id, filtered_obs,\n                            episode.last_info_for(agent_id) or {},\n                            episode.rnn_state_for(agent_id),\n                            np.zeros_like(\n                                _flatten_action(policy.action_space.sample())),\n                            0.0))\n\n    return active_envs, to_eval, outputs"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute actions on observation batches to get next actions.", "response": "def _do_policy_eval(tf_sess, to_eval, policies, active_episodes):\n    \"\"\"Call compute actions on observation batches to get next actions.\n\n    Returns:\n        eval_results: dict of policy to compute_action() outputs.\n    \"\"\"\n\n    eval_results = {}\n\n    if tf_sess:\n        builder = TFRunBuilder(tf_sess, \"policy_eval\")\n        pending_fetches = {}\n    else:\n        builder = None\n\n    if log_once(\"compute_actions_input\"):\n        logger.info(\"Inputs to compute_actions():\\n\\n{}\\n\".format(\n            summarize(to_eval)))\n\n    for policy_id, eval_data in to_eval.items():\n        rnn_in_cols = _to_column_format([t.rnn_state for t in eval_data])\n        policy = _get_or_raise(policies, policy_id)\n        if builder and (policy.compute_actions.__code__ is\n                        TFPolicyGraph.compute_actions.__code__):\n            # TODO(ekl): how can we make info batch available to TF code?\n            pending_fetches[policy_id] = policy._build_compute_actions(\n                builder, [t.obs for t in eval_data],\n                rnn_in_cols,\n                prev_action_batch=[t.prev_action for t in eval_data],\n                prev_reward_batch=[t.prev_reward for t in eval_data])\n        else:\n            eval_results[policy_id] = policy.compute_actions(\n                [t.obs for t in eval_data],\n                rnn_in_cols,\n                prev_action_batch=[t.prev_action for t in eval_data],\n                prev_reward_batch=[t.prev_reward for t in eval_data],\n                info_batch=[t.info for t in eval_data],\n                episodes=[active_episodes[t.env_id] for t in eval_data])\n    if builder:\n        for k, v in pending_fetches.items():\n            eval_results[k] = builder.get(v)\n\n    if log_once(\"compute_actions_result\"):\n        logger.info(\"Outputs of compute_actions():\\n\\n{}\\n\".format(\n            summarize(eval_results)))\n\n    return eval_results"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprocessing the output of policy neural network evaluation.", "response": "def _process_policy_eval_results(to_eval, eval_results, active_episodes,\n                                 active_envs, off_policy_actions, policies,\n                                 clip_actions):\n    \"\"\"Process the output of policy neural network evaluation.\n\n    Records policy evaluation results into the given episode objects and\n    returns replies to send back to agents in the env.\n\n    Returns:\n        actions_to_send: nested dict of env id -> agent id -> agent replies.\n    \"\"\"\n\n    actions_to_send = defaultdict(dict)\n    for env_id in active_envs:\n        actions_to_send[env_id] = {}  # at minimum send empty dict\n\n    for policy_id, eval_data in to_eval.items():\n        rnn_in_cols = _to_column_format([t.rnn_state for t in eval_data])\n        actions, rnn_out_cols, pi_info_cols = eval_results[policy_id]\n        if len(rnn_in_cols) != len(rnn_out_cols):\n            raise ValueError(\"Length of RNN in did not match RNN out, got: \"\n                             \"{} vs {}\".format(rnn_in_cols, rnn_out_cols))\n        # Add RNN state info\n        for f_i, column in enumerate(rnn_in_cols):\n            pi_info_cols[\"state_in_{}\".format(f_i)] = column\n        for f_i, column in enumerate(rnn_out_cols):\n            pi_info_cols[\"state_out_{}\".format(f_i)] = column\n        # Save output rows\n        actions = _unbatch_tuple_actions(actions)\n        policy = _get_or_raise(policies, policy_id)\n        for i, action in enumerate(actions):\n            env_id = eval_data[i].env_id\n            agent_id = eval_data[i].agent_id\n            if clip_actions:\n                actions_to_send[env_id][agent_id] = clip_action(\n                    action, policy.action_space)\n            else:\n                actions_to_send[env_id][agent_id] = action\n            episode = active_episodes[env_id]\n            episode._set_rnn_state(agent_id, [c[i] for c in rnn_out_cols])\n            episode._set_last_pi_info(\n                agent_id, {k: v[i]\n                           for k, v in pi_info_cols.items()})\n            if env_id in off_policy_actions and \\\n                    agent_id in off_policy_actions[env_id]:\n                episode._set_last_action(agent_id,\n                                         off_policy_actions[env_id][agent_id])\n            else:\n                episode._set_last_action(agent_id, action)\n\n    return actions_to_send"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _fetch_atari_metrics(base_env):\n    unwrapped = base_env.get_unwrapped()\n    if not unwrapped:\n        return None\n    atari_out = []\n    for u in unwrapped:\n        monitor = get_wrapper_by_cls(u, MonitorEnv)\n        if not monitor:\n            return None\n        for eps_rew, eps_len in monitor.next_episode_results():\n            atari_out.append(RolloutMetrics(eps_len, eps_rew, {}, {}, {}))\n    return atari_out", "response": "Fetch the metrics for the given base environment."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compare_version(a, b):\n  aa = string.split(a, \".\")\n  bb = string.split(b, \".\")\n  for i in range(0, 4):\n    if aa[i] != bb[i]:\n      return cmp(int(aa[i]), int(bb[i]))\n  return 0", "response": "Compare two version number strings of the form W. X. Y. Z.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate CMake instance and execute configure step", "response": "def configure_cmake(self):\n        \"\"\"Create CMake instance and execute configure step\n        \"\"\"\n        cmake = CMake(self)\n        cmake.definitions[\"FLATBUFFERS_BUILD_TESTS\"] = False\n        cmake.definitions[\"FLATBUFFERS_BUILD_SHAREDLIB\"] = self.options.shared\n        cmake.definitions[\"FLATBUFFERS_BUILD_FLATLIB\"] = not self.options.shared\n        cmake.configure()\n        return cmake"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncopy Flatbuffers artifacts to package folder", "response": "def package(self):\n        \"\"\"Copy Flatbuffers' artifacts to package folder\n        \"\"\"\n        cmake = self.configure_cmake()\n        cmake.install()\n        self.copy(pattern=\"LICENSE.txt\", dst=\"licenses\")\n        self.copy(pattern=\"FindFlatBuffers.cmake\", dst=os.path.join(\"lib\", \"cmake\", \"flatbuffers\"), src=\"CMake\")\n        self.copy(pattern=\"flathash*\", dst=\"bin\", src=\"bin\")\n        self.copy(pattern=\"flatc*\", dst=\"bin\", src=\"bin\")\n        if self.settings.os == \"Windows\" and self.options.shared:\n            if self.settings.compiler == \"Visual Studio\":\n                shutil.move(os.path.join(self.package_folder, \"lib\", \"%s.dll\" % self.name),\n                            os.path.join(self.package_folder, \"bin\", \"%s.dll\" % self.name))\n            elif self.settings.compiler == \"gcc\":\n                shutil.move(os.path.join(self.package_folder, \"lib\", \"lib%s.dll\" % self.name),\n                            os.path.join(self.package_folder, \"bin\", \"lib%s.dll\" % self.name))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef package_info(self):\n        self.cpp_info.libs = tools.collect_libs(self)\n        self.user_info.flatc = os.path.join(self.package_folder, \"bin\", \"flatc\")", "response": "Collect built libraries names and solve flatc path."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\noffset provides access into the Table s vtable.", "response": "def Offset(self, vtableOffset):\n        \"\"\"Offset provides access into the Table's vtable.\n\n        Deprecated fields are ignored by checking the vtable's length.\"\"\"\n\n        vtable = self.Pos - self.Get(N.SOffsetTFlags, self.Pos)\n        vtableEnd = self.Get(N.VOffsetTFlags, vtable)\n        if vtableOffset < vtableEnd:\n            return self.Get(N.VOffsetTFlags, vtable + vtableOffset)\n        return 0"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef Indirect(self, off):\n        N.enforce_number(off, N.UOffsetTFlags)\n        return off + encode.Get(N.UOffsetTFlags.packer_type, self.Bytes, off)", "response": "Indirect retrieves the relative offset stored at offset."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef String(self, off):\n        N.enforce_number(off, N.UOffsetTFlags)\n        off += encode.Get(N.UOffsetTFlags.packer_type, self.Bytes, off)\n        start = off + N.UOffsetTFlags.bytewidth\n        length = encode.Get(N.UOffsetTFlags.packer_type, self.Bytes, off)\n        return bytes(self.Bytes[start:start+length])", "response": "String gets a string from data stored inside the flatbuffer."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Vector(self, off):\n        N.enforce_number(off, N.UOffsetTFlags)\n\n        off += self.Pos\n        x = off + self.Get(N.UOffsetTFlags, off)\n        # data starts after metadata containing the vector length\n        x += N.UOffsetTFlags.bytewidth\n        return x", "response": "Vector retrieves the start of data of the vector whose offset is\n           stored at off."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Get(self, flags, off):\n        N.enforce_number(off, N.UOffsetTFlags)\n        return flags.py_type(encode.Get(flags.packer_type, self.Bytes, off))", "response": "Get retrieves a value of the type specified by flags at the given offset."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef GetVectorAsNumpy(self, flags, off):\n        offset = self.Vector(off)\n        length = self.VectorLen(off) # TODO: length accounts for bytewidth, right?\n        numpy_dtype = N.to_numpy_type(flags)\n        return encode.GetVectorAsNumpy(numpy_dtype, self.Bytes, length, offset)", "response": "GetVectorAsNumpy returns the vector that starts at Vector ( off ) with the type specified by flags."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef GetVOffsetTSlot(self, slot, d):\n\n        N.enforce_number(slot, N.VOffsetTFlags)\n        N.enforce_number(d, N.VOffsetTFlags)\n\n        off = self.Offset(slot)\n        if off == 0:\n                return d\n        return off", "response": "GetVOffsetTSlot retrieves the VOffsetT that the given vtable location contains."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndecode a vector of numpy_type from buf starting at buf [ head count offset.", "response": "def GetVectorAsNumpy(numpy_type, buf, count, offset):\n    \"\"\" GetVecAsNumpy decodes values starting at buf[head] as\n    `numpy_type`, where `numpy_type` is a numpy dtype. \"\"\"\n    if np is not None:\n        # TODO: could set .flags.writeable = False to make users jump through\n        #       hoops before modifying...\n        return np.frombuffer(buf, dtype=numpy_type, count=count, offset=offset)\n    else:\n        raise NumpyRequiredForThisFeature('Numpy was not found.')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef Write(packer_type, buf, head, n):\n    packer_type.pack_into(buf, head, n)", "response": "Encodes n bytes at buf at head using packer_type."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef main():\n  if len(sys.argv) < 2:\n    sys.stderr.write('Usage: run_flatc.py flatbuffers_dir [flatc_args]\\n')\n    return 1\n  cwd = os.getcwd()\n  flatc = ''\n  flatbuffers_dir = sys.argv[1]\n  for path in FLATC_SEARCH_PATHS:\n    current = os.path.join(flatbuffers_dir, path,\n                           'flatc' + EXECUTABLE_EXTENSION)\n    if os.path.exists(current):\n      flatc = current\n      break\n  if not flatc:\n    sys.stderr.write('flatc not found\\n')\n    return 1\n  command = [flatc] + sys.argv[2:]\n  return subprocess.call(command)", "response": "Script that finds and runs flatc built from source."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef import_numpy():\n    try:\n        imp.find_module('numpy')\n        numpy_exists = True\n    except ImportError:\n        numpy_exists = False\n\n    if numpy_exists:\n        # We do this outside of try/except block in case numpy exists\n        # but is not installed correctly. We do not want to catch an\n        # incorrect installation which would manifest as an\n        # ImportError.\n        import numpy as np\n    else:\n        np = None\n\n    return np", "response": "Imports the numpy module if it exists on the system otherwise returns None."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef StartObject(self, numfields):\n\n        self.assertNotNested()\n\n        # use 32-bit offsets so that arithmetic doesn't overflow.\n        self.current_vtable = [0 for _ in range_func(numfields)]\n        self.objectEnd = self.Offset()\n        self.nested = True", "response": "Initialize bookkeeping for writing a new object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef WriteVtable(self):\n\n        # Prepend a zero scalar to the object. Later in this function we'll\n        # write an offset here that points to the object's vtable:\n        self.PrependSOffsetTRelative(0)\n\n        objectOffset = self.Offset()\n        existingVtable = None\n\n        # Trim trailing 0 offsets.\n        while self.current_vtable and self.current_vtable[-1] == 0:\n            self.current_vtable.pop()\n\n        # Search backwards through existing vtables, because similar vtables\n        # are likely to have been recently appended. See\n        # BenchmarkVtableDeduplication for a case in which this heuristic\n        # saves about 30% of the time used in writing objects with duplicate\n        # tables.\n\n        i = len(self.vtables) - 1\n        while i >= 0:\n            # Find the other vtable, which is associated with `i`:\n            vt2Offset = self.vtables[i]\n            vt2Start = len(self.Bytes) - vt2Offset\n            vt2Len = encode.Get(packer.voffset, self.Bytes, vt2Start)\n\n            metadata = VtableMetadataFields * N.VOffsetTFlags.bytewidth\n            vt2End = vt2Start + vt2Len\n            vt2 = self.Bytes[vt2Start+metadata:vt2End]\n\n            # Compare the other vtable to the one under consideration.\n            # If they are equal, store the offset and break:\n            if vtableEqual(self.current_vtable, objectOffset, vt2):\n                existingVtable = vt2Offset\n                break\n\n            i -= 1\n\n        if existingVtable is None:\n            # Did not find a vtable, so write this one to the buffer.\n\n            # Write out the current vtable in reverse , because\n            # serialization occurs in last-first order:\n            i = len(self.current_vtable) - 1\n            while i >= 0:\n                off = 0\n                if self.current_vtable[i] != 0:\n                    # Forward reference to field;\n                    # use 32bit number to ensure no overflow:\n                    off = objectOffset - self.current_vtable[i]\n\n                self.PrependVOffsetT(off)\n                i -= 1\n\n            # The two metadata fields are written last.\n\n            # First, store the object bytesize:\n            objectSize = UOffsetTFlags.py_type(objectOffset - self.objectEnd)\n            self.PrependVOffsetT(VOffsetTFlags.py_type(objectSize))\n\n            # Second, store the vtable bytesize:\n            vBytes = len(self.current_vtable) + VtableMetadataFields\n            vBytes *= N.VOffsetTFlags.bytewidth\n            self.PrependVOffsetT(VOffsetTFlags.py_type(vBytes))\n\n            # Next, write the offset to the new vtable in the\n            # already-allocated SOffsetT at the beginning of this object:\n            objectStart = SOffsetTFlags.py_type(len(self.Bytes) - objectOffset)\n            encode.Write(packer.soffset, self.Bytes, objectStart,\n                         SOffsetTFlags.py_type(self.Offset() - objectOffset))\n\n            # Finally, store this vtable in memory for future\n            # deduplication:\n            self.vtables.append(self.Offset())\n        else:\n            # Found a duplicate vtable.\n\n            objectStart = SOffsetTFlags.py_type(len(self.Bytes) - objectOffset)\n            self.head = UOffsetTFlags.py_type(objectStart)\n\n            # Write the offset to the found vtable in the\n            # already-allocated SOffsetT at the beginning of this object:\n            encode.Write(packer.soffset, self.Bytes, self.Head(),\n                         SOffsetTFlags.py_type(existingVtable - objectOffset))\n\n        self.current_vtable = None\n        return objectOffset", "response": "Writes out the vtable for the current object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngrow the byteslice to fit the size of the new buffer.", "response": "def growByteBuffer(self):\n        \"\"\"Doubles the size of the byteslice, and copies the old data towards\n           the end of the new buffer (since we build the buffer backwards).\"\"\"\n        if len(self.Bytes) == Builder.MAX_BUFFER_SIZE:\n            msg = \"flatbuffers: cannot grow buffer beyond 2 gigabytes\"\n            raise BuilderSizeError(msg)\n\n        newSize = min(len(self.Bytes) * 2, Builder.MAX_BUFFER_SIZE)\n        if newSize == 0:\n            newSize = 1\n        bytes2 = bytearray(newSize)\n        bytes2[newSize-len(self.Bytes):] = self.Bytes\n        self.Bytes = bytes2"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npadding the array with zeros at the current offset.", "response": "def Pad(self, n):\n        \"\"\"Pad places zeros at the current offset.\"\"\"\n        for i in range_func(n):\n            self.Place(0, N.Uint8Flags)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef Prep(self, size, additionalBytes):\n\n        # Track the biggest thing we've ever aligned to.\n        if size > self.minalign:\n            self.minalign = size\n\n        # Find the amount of alignment needed such that `size` is properly\n        # aligned after `additionalBytes`:\n        alignSize = (~(len(self.Bytes) - self.Head() + additionalBytes)) + 1\n        alignSize &= (size - 1)\n\n        # Reallocate the buffer if needed:\n        while self.Head() < alignSize+size+additionalBytes:\n            oldBufSize = len(self.Bytes)\n            self.growByteBuffer()\n            updated_head = self.head + len(self.Bytes) - oldBufSize\n            self.head = UOffsetTFlags.py_type(updated_head)\n        self.Pad(alignSize)", "response": "Prepares to write an element of size after additionalBytes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef PrependSOffsetTRelative(self, off):\n\n        # Ensure alignment is already done:\n        self.Prep(N.SOffsetTFlags.bytewidth, 0)\n        if not (off <= self.Offset()):\n            msg = \"flatbuffers: Offset arithmetic error.\"\n            raise OffsetArithmeticError(msg)\n        off2 = self.Offset() - off + N.SOffsetTFlags.bytewidth\n        self.PlaceSOffsetT(off2)", "response": "PrependSOffsetTRelative prepends an SOffsetT relative to where it will be written."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef PrependUOffsetTRelative(self, off):\n\n        # Ensure alignment is already done:\n        self.Prep(N.UOffsetTFlags.bytewidth, 0)\n        if not (off <= self.Offset()):\n            msg = \"flatbuffers: Offset arithmetic error.\"\n            raise OffsetArithmeticError(msg)\n        off2 = self.Offset() - off + N.UOffsetTFlags.bytewidth\n        self.PlaceUOffsetT(off2)", "response": "Prepends an unsigned offset into vector data relative to where it is written."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninitializing the bookkeeping for writing a new vector.", "response": "def StartVector(self, elemSize, numElems, alignment):\n        \"\"\"\n        StartVector initializes bookkeeping for writing a new vector.\n\n        A vector has the following format:\n          - <UOffsetT: number of elements in this vector>\n          - <T: data>+, where T is the type of elements of this vector.\n        \"\"\"\n\n        self.assertNotNested()\n        self.nested = True\n        self.Prep(N.Uint32Flags.bytewidth, elemSize*numElems)\n        self.Prep(alignment, elemSize*numElems)  # In case alignment > int.\n        return self.Offset()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nend the vector construction.", "response": "def EndVector(self, vectorNumElems):\n        \"\"\"EndVector writes data necessary to finish vector construction.\"\"\"\n\n        self.assertNested()\n        ## @cond FLATBUFFERS_INTERNAL\n        self.nested = False\n        ## @endcond\n        # we already made space for this, so write without PrependUint32\n        self.PlaceUOffsetT(vectorNumElems)\n        return self.Offset()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef CreateByteVector(self, x):\n\n        self.assertNotNested()\n        ## @cond FLATBUFFERS_INTERNAL\n        self.nested = True\n        ## @endcond\n\n        if not isinstance(x, compat.binary_types):\n            raise TypeError(\"non-byte vector passed to CreateByteVector\")\n\n        self.Prep(N.UOffsetTFlags.bytewidth, len(x)*N.Uint8Flags.bytewidth)\n\n        l = UOffsetTFlags.py_type(len(x))\n        ## @cond FLATBUFFERS_INTERNAL\n        self.head = UOffsetTFlags.py_type(self.Head() - l)\n        ## @endcond\n        self.Bytes[self.Head():self.Head()+l] = x\n\n        return self.EndVector(len(x))", "response": "CreateString writes a byte vector."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef CreateNumpyVector(self, x):\n\n        if np is None:\n            # Numpy is required for this feature\n            raise NumpyRequiredForThisFeature(\"Numpy was not found.\")\n\n        if not isinstance(x, np.ndarray):\n            raise TypeError(\"non-numpy-ndarray passed to CreateNumpyVector\")\n\n        if x.dtype.kind not in ['b', 'i', 'u', 'f']:\n            raise TypeError(\"numpy-ndarray holds elements of unsupported datatype\")\n\n        if x.ndim > 1:\n            raise TypeError(\"multidimensional-ndarray passed to CreateNumpyVector\")\n\n        self.StartVector(x.itemsize, x.size, x.dtype.alignment)\n\n        # Ensure little endian byte ordering\n        if x.dtype.str[0] == \"<\":\n            x_lend = x\n        else:\n            x_lend = x.byteswap(inplace=False)\n\n        # Calculate total length\n        l = UOffsetTFlags.py_type(x_lend.itemsize * x_lend.size)\n        ## @cond FLATBUFFERS_INTERNAL\n        self.head = UOffsetTFlags.py_type(self.Head() - l)\n        ## @endcond\n\n        # tobytes ensures c_contiguous ordering\n        self.Bytes[self.Head():self.Head()+l] = x_lend.tobytes(order='C')\n        \n        return self.EndVector(x.size)", "response": "Creates a Numpy array into the buffer."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nensures that a Struct is always stored inline.", "response": "def assertStructIsInline(self, obj):\n        \"\"\"\n        Structs are always stored inline, so need to be created right\n        where they are used. You'll get this error if you created it\n        elsewhere.\n        \"\"\"\n\n        N.enforce_number(obj, N.UOffsetTFlags)\n        if obj != self.Offset():\n            msg = (\"flatbuffers: Tried to write a Struct at an Offset that \"\n                   \"is different from the current Offset of the Builder.\")\n            raise StructIsNotInlineError(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef Slot(self, slotnum):\n        self.assertNested()\n        self.current_vtable[slotnum] = self.Offset()", "response": "Sets the vtable key voffset to the current location in the available buffer."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinishing finalizes a buffer pointing to the given rootTable.", "response": "def __Finish(self, rootTable, sizePrefix):\n        \"\"\"Finish finalizes a buffer, pointing to the given `rootTable`.\"\"\"\n        N.enforce_number(rootTable, N.UOffsetTFlags)\n        prepSize = N.UOffsetTFlags.bytewidth\n        if sizePrefix:\n            prepSize += N.Int32Flags.bytewidth\n        self.Prep(self.minalign, prepSize)\n        self.PrependUOffsetTRelative(rootTable)\n        if sizePrefix:\n            size = len(self.Bytes) - self.Head()\n            N.enforce_number(size, N.Int32Flags)\n            self.PrependInt32(size)\n        self.finished = True\n        return self.Head()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef PrependStructSlot(self, v, x, d):\n\n        N.enforce_number(d, N.UOffsetTFlags)\n        if x != d:\n            self.assertStructIsInline(x)\n            self.Slot(v)", "response": "PrependStructSlot prepends a struct onto the object at vtable slot o."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef Place(self, x, flags):\n\n        N.enforce_number(x, flags)\n        self.head = self.head - flags.bytewidth\n        encode.Write(flags.packer_type, self.Bytes, self.Head(), x)", "response": "Place prepends a value specified by flags to the Builder."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef user_config_dir(appname=None, appauthor=None, version=None, roaming=False):\n    if system in [\"win32\", \"darwin\"]:\n        path = user_data_dir(appname, appauthor, None, roaming)\n    else:\n        path = os.getenv('XDG_CONFIG_HOME', os.path.expanduser(\"~/.config\"))\n        if appname:\n            path = os.path.join(path, appname)\n    if appname and version:\n        path = os.path.join(path, version)\n    return path", "response": "r Return full path to the user - specific config dir for this application."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef request(method, url, **kwargs):\n\n    # By using the 'with' statement we are sure the session is closed, thus we\n    # avoid leaving sockets open which can trigger a ResourceWarning in some\n    # cases, and look like a memory leak in others.\n    with sessions.Session() as session:\n        return session.request(method=method, url=url, **kwargs)", "response": "Constructs and sends a new HTTP Request object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dump(o, f):\n\n    if not f.write:\n        raise TypeError(\"You can only dump an object to a file descriptor\")\n    d = dumps(o)\n    f.write(d)\n    return d", "response": "Writes out dictionary o into a toml file f"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dumps(o, encoder=None):\n\n    retval = \"\"\n    if encoder is None:\n        encoder = TomlEncoder(o.__class__)\n    addtoretval, sections = encoder.dump_sections(o, \"\")\n    retval += addtoretval\n    while sections:\n        newsections = encoder.get_empty_table()\n        for section in sections:\n            addtoretval, addtosections = encoder.dump_sections(\n                sections[section], section)\n\n            if addtoretval or (not addtoretval and not addtosections):\n                if retval and retval[-2:] != \"\\n\\n\":\n                    retval += \"\\n\"\n                retval += \"[\" + section + \"]\\n\"\n                if addtoretval:\n                    retval += addtoretval\n            for s in addtosections:\n                newsections[section + \".\" + s] = addtosections[s]\n        sections = newsections\n    return retval", "response": "Stringifies input dict into toml"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndump inline table in its compact syntax instead of expanding into subsection.", "response": "def dump_inline_table(self, section):\n        \"\"\"Preserve inline table in its compact syntax instead of expanding\n        into subsection.\n\n        https://github.com/toml-lang/toml#user-content-inline-table\n        \"\"\"\n        retval = \"\"\n        if isinstance(section, dict):\n            val_list = []\n            for k, v in section.items():\n                val = self.dump_inline_table(v)\n                val_list.append(k + \" = \" + val)\n            retval += \"{ \" + \", \".join(val_list) + \" }\\n\"\n            return retval\n        else:\n            return unicode(self.dump_value(section))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_in_virtualenv():\n\n    pipenv_active = os.environ.get(\"PIPENV_ACTIVE\", False)\n    virtual_env = None\n    use_system = False\n    ignore_virtualenvs = bool(os.environ.get(\"PIPENV_IGNORE_VIRTUALENVS\", False))\n\n    if not pipenv_active and not ignore_virtualenvs:\n        virtual_env = os.environ.get(\"VIRTUAL_ENV\")\n        use_system = bool(virtual_env)\n    return (use_system or virtual_env) and not (pipenv_active or ignore_virtualenvs)", "response": "Check whether we are in a virtual environment."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef unpackb(packed, **kwargs):\n    unpacker = Unpacker(None, **kwargs)\n    unpacker.feed(packed)\n    try:\n        ret = unpacker._unpack()\n    except OutOfData:\n        raise UnpackValueError(\"Data is not enough.\")\n    if unpacker._got_extradata():\n        raise ExtraData(ret, unpacker._get_extradata())\n    return ret", "response": "Unpack an object from packed."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _consume(self):\n        self._stream_offset += self._buff_i - self._buf_checkpoint\n        self._buf_checkpoint = self._buff_i", "response": "Consume the next set of entries from the buffer."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nestablishing a new connection and set nodelay settings on it.", "response": "def _new_conn(self):\n        \"\"\" Establish a socket connection and set nodelay settings on it.\n\n        :return: New socket connection.\n        \"\"\"\n        extra_kw = {}\n        if self.source_address:\n            extra_kw['source_address'] = self.source_address\n\n        if self.socket_options:\n            extra_kw['socket_options'] = self.socket_options\n\n        try:\n            conn = connection.create_connection(\n                (self._dns_host, self.port), self.timeout, **extra_kw)\n\n        except SocketTimeout as e:\n            raise ConnectTimeoutError(\n                self, \"Connection to %s timed out. (connect timeout=%s)\" %\n                (self.host, self.timeout))\n\n        except SocketError as e:\n            raise NewConnectionError(\n                self, \"Failed to establish a new connection: %s\" % e)\n\n        return conn"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef request_chunked(self, method, url, body=None, headers=None):\n        headers = HTTPHeaderDict(headers if headers is not None else {})\n        skip_accept_encoding = 'accept-encoding' in headers\n        skip_host = 'host' in headers\n        self.putrequest(\n            method,\n            url,\n            skip_accept_encoding=skip_accept_encoding,\n            skip_host=skip_host\n        )\n        for header, value in headers.items():\n            self.putheader(header, value)\n        if 'transfer-encoding' not in headers:\n            self.putheader('Transfer-Encoding', 'chunked')\n        self.endheaders()\n\n        if body is not None:\n            stringish_types = six.string_types + (bytes,)\n            if isinstance(body, stringish_types):\n                body = (body,)\n            for chunk in body:\n                if not chunk:\n                    continue\n                if not isinstance(chunk, bytes):\n                    chunk = chunk.encode('utf8')\n                len_str = hex(len(chunk))[2:]\n                self.send(len_str.encode('utf-8'))\n                self.send(b'\\r\\n')\n                self.send(chunk)\n                self.send(b'\\r\\n')\n\n        # After the if clause, to always have a closed body\n        self.send(b'0\\r\\n\\r\\n')", "response": "This method sends the request body with chunked encoding and not as one block."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the certificate for the current object.", "response": "def set_cert(self, key_file=None, cert_file=None,\n                 cert_reqs=None, ca_certs=None,\n                 assert_hostname=None, assert_fingerprint=None,\n                 ca_cert_dir=None):\n        \"\"\"\n        This method should only be called once, before the connection is used.\n        \"\"\"\n        # If cert_reqs is not provided, we can try to guess. If the user gave\n        # us a cert database, we assume they want to use it: otherwise, if\n        # they gave us an SSL Context object we should use whatever is set for\n        # it.\n        if cert_reqs is None:\n            if ca_certs or ca_cert_dir:\n                cert_reqs = 'CERT_REQUIRED'\n            elif self.ssl_context is not None:\n                cert_reqs = self.ssl_context.verify_mode\n\n        self.key_file = key_file\n        self.cert_file = cert_file\n        self.cert_reqs = cert_reqs\n        self.assert_hostname = assert_hostname\n        self.assert_fingerprint = assert_fingerprint\n        self.ca_certs = ca_certs and os.path.expanduser(ca_certs)\n        self.ca_cert_dir = ca_cert_dir and os.path.expanduser(ca_cert_dir)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef prettify_exc(error):\n    matched_exceptions = [k for k in KNOWN_EXCEPTIONS.keys() if k in error]\n    if not matched_exceptions:\n        return \"{}\".format(vistir.misc.decode_for_output(error))\n    errors = []\n    for match in matched_exceptions:\n        _, error, info = error.rpartition(KNOWN_EXCEPTIONS[match])\n        errors.append(\"{} {}\".format(error, info))\n\n    return \"\\n\".join(errors)", "response": "Catch known errors and prettify them instead of showing the\n    entire traceback for better UX"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the OS appropriate handle for the corresponding output stream.", "response": "def get_stream_handle(stream=sys.stdout):\n    \"\"\"\n    Get the OS appropriate handle for the corresponding output stream.\n\n    :param str stream: The the stream to get the handle for\n    :return: A handle to the appropriate stream, either a ctypes buffer\n             or **sys.stdout** or **sys.stderr**.\n    \"\"\"\n    handle = stream\n    if os.name == \"nt\":\n        from ctypes import windll\n\n        handle_id = WIN_STDOUT_HANDLE_ID\n        handle = windll.kernel32.GetStdHandle(handle_id)\n    return handle"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nhide the console cursor on the given stream.", "response": "def hide_cursor(stream=sys.stdout):\n    \"\"\"\n    Hide the console cursor on the given stream\n\n    :param stream: The name of the stream to get the handle for\n    :return: None\n    :rtype: None\n    \"\"\"\n\n    handle = get_stream_handle(stream=stream)\n    if os.name == \"nt\":\n        from ctypes import windll\n\n        cursor_info = CONSOLE_CURSOR_INFO()\n        windll.kernel32.GetConsoleCursorInfo(handle, ctypes.byref(cursor_info))\n        cursor_info.visible = False\n        windll.kernel32.SetConsoleCursorInfo(handle, ctypes.byref(cursor_info))\n    else:\n        handle.write(\"\\033[?25l\")\n        handle.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the completion results for click. core. Choice", "response": "def choice_complete(self, ctx, incomplete):\n    \"\"\"Returns the completion results for click.core.Choice\n\n    Parameters\n    ----------\n    ctx : click.core.Context\n        The current context\n    incomplete :\n        The string to complete\n\n    Returns\n    -------\n    [(str, str)]\n        A list of completion results\n    \"\"\"\n    return [\n        (c, None) for c in self.choices\n        if completion_configuration.match_incomplete(c, incomplete)\n    ]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef patch():\n    import click\n    click.types.ParamType.complete = param_type_complete\n    click.types.Choice.complete = choice_complete\n    click.core.MultiCommand.get_command_short_help = multicommand_get_command_short_help\n    click.core._bashcomplete = _shellcomplete", "response": "Patch click to support different types of parameter type choice and multi - command."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_expr(tokens, options):\n    seq = parse_seq(tokens, options)\n    if tokens.current() != '|':\n        return seq\n    result = [Required(*seq)] if len(seq) > 1 else seq\n    while tokens.current() == '|':\n        tokens.move()\n        seq = parse_seq(tokens, options)\n        result += [Required(*seq)] if len(seq) > 1 else seq\n    return [Either(*result)] if len(result) > 1 else result", "response": "expr ::= seq ( '|' seq )* ;"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing a sequence of items.", "response": "def parse_seq(tokens, options):\n    \"\"\"seq ::= ( atom [ '...' ] )* ;\"\"\"\n    result = []\n    while tokens.current() not in [None, ']', ')', '|']:\n        atom = parse_atom(tokens, options)\n        if tokens.current() == '...':\n            atom = [OneOrMore(*atom)]\n            tokens.move()\n        result += atom\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses command - line argument vector.", "response": "def parse_argv(tokens, options, options_first=False):\n    \"\"\"Parse command-line argument vector.\n\n    If options_first:\n        argv ::= [ long | shorts ]* [ argument ]* [ '--' [ argument ]* ] ;\n    else:\n        argv ::= [ long | shorts | argument ]* [ '--' [ argument ]* ] ;\n\n    \"\"\"\n    parsed = []\n    while tokens.current() is not None:\n        if tokens.current() == '--':\n            return parsed + [Argument(None, v) for v in tokens]\n        elif tokens.current().startswith('--'):\n            parsed += parse_long(tokens, options)\n        elif tokens.current().startswith('-') and tokens.current() != '-':\n            parsed += parse_shorts(tokens, options)\n        elif options_first:\n            return parsed + [Argument(None, v) for v in tokens]\n        else:\n            parsed.append(Argument(None, tokens.move()))\n    return parsed"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef unnest(elem):\n\n    if isinstance(elem, Iterable) and not isinstance(elem, six.string_types):\n        elem, target = tee(elem, 2)\n    else:\n        target = elem\n    for el in target:\n        if isinstance(el, Iterable) and not isinstance(el, six.string_types):\n            el, el_copy = tee(el, 2)\n            for sub in unnest(el_copy):\n                yield sub\n        else:\n            yield el", "response": "Flatten an arbitrarily nested iterable into a single list of entries."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns a command in a non - blocking manner.", "response": "def run(\n    cmd,\n    env=None,\n    return_object=False,\n    block=True,\n    cwd=None,\n    verbose=False,\n    nospin=False,\n    spinner_name=None,\n    combine_stderr=True,\n    display_limit=200,\n    write_to_stdout=True,\n):\n    \"\"\"Use `subprocess.Popen` to get the output of a command and decode it.\n\n    :param list cmd: A list representing the command you want to run.\n    :param dict env: Additional environment settings to pass through to the subprocess.\n    :param bool return_object: When True, returns the whole subprocess instance\n    :param bool block: When False, returns a potentially still-running :class:`subprocess.Popen` instance\n    :param str cwd: Current working directory contect to use for spawning the subprocess.\n    :param bool verbose: Whether to print stdout in real time when non-blocking.\n    :param bool nospin: Whether to disable the cli spinner.\n    :param str spinner_name: The name of the spinner to use if enabled, defaults to bouncingBar\n    :param bool combine_stderr: Optionally merge stdout and stderr in the subprocess, false if nonblocking.\n    :param int dispay_limit: The max width of output lines to display when using a spinner.\n    :param bool write_to_stdout: Whether to write to stdout when using a spinner, default True.\n    :returns: A 2-tuple of (output, error) or a :class:`subprocess.Popen` object.\n\n    .. Warning:: Merging standard out and standarad error in a nonblocking subprocess\n        can cause errors in some cases and may not be ideal. Consider disabling\n        this functionality.\n    \"\"\"\n\n    _env = os.environ.copy()\n    if env:\n        _env.update(env)\n    if six.PY2:\n        fs_encode = partial(to_bytes, encoding=locale_encoding)\n        _env = {fs_encode(k): fs_encode(v) for k, v in _env.items()}\n    else:\n        _env = {k: fs_str(v) for k, v in _env.items()}\n    if not spinner_name:\n        spinner_name = \"bouncingBar\"\n    if six.PY2:\n        if isinstance(cmd, six.string_types):\n            cmd = cmd.encode(\"utf-8\")\n        elif isinstance(cmd, (list, tuple)):\n            cmd = [c.encode(\"utf-8\") for c in cmd]\n    if not isinstance(cmd, Script):\n        cmd = Script.parse(cmd)\n    if block or not return_object:\n        combine_stderr = False\n    start_text = \"\"\n    with spinner(\n        spinner_name=spinner_name,\n        start_text=start_text,\n        nospin=nospin,\n        write_to_stdout=write_to_stdout,\n    ) as sp:\n        return _create_subprocess(\n            cmd,\n            env=_env,\n            return_object=return_object,\n            block=block,\n            cwd=cwd,\n            verbose=verbose,\n            spinner=sp,\n            combine_stderr=combine_stderr,\n            start_text=start_text,\n            write_to_stdout=True,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_path(python):\n\n    python = Path(python).as_posix()\n    out, err = run(\n        [python, \"-c\", \"import json, sys; print(json.dumps(sys.path))\"], nospin=True\n    )\n    if out:\n        return json.loads(out)\n    else:\n        return []", "response": "Load the sys. path from the given python executable s environment as json\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_bytes(string, encoding=\"utf-8\", errors=\"ignore\"):\n\n    if not errors:\n        if encoding.lower() == \"utf-8\":\n            errors = \"surrogateescape\" if six.PY3 else \"ignore\"\n        else:\n            errors = \"strict\"\n    if isinstance(string, bytes):\n        if encoding.lower() == \"utf-8\":\n            return string\n        else:\n            return string.decode(\"utf-8\").encode(encoding, errors)\n    elif isinstance(string, memoryview):\n        return bytes(string)\n    elif not isinstance(string, six.string_types):\n        try:\n            if six.PY3:\n                return six.text_type(string).encode(encoding, errors)\n            else:\n                return bytes(string)\n        except UnicodeEncodeError:\n            if isinstance(string, Exception):\n                return b\" \".join(to_bytes(arg, encoding, errors) for arg in string)\n            return six.text_type(string).encode(encoding, errors)\n    else:\n        return string.encode(encoding, errors)", "response": "Force a value to bytes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nforce a value to a text - type.", "response": "def to_text(string, encoding=\"utf-8\", errors=None):\n    \"\"\"Force a value to a text-type.\n\n    :param string: Some input that can be converted to a unicode representation.\n    :type string: str or bytes unicode\n    :param encoding: The encoding to use for conversions, defaults to \"utf-8\"\n    :param encoding: str, optional\n    :return: The unicode representation of the string\n    :rtype: str\n    \"\"\"\n\n    if not errors:\n        if encoding.lower() == \"utf-8\":\n            errors = \"surrogateescape\" if six.PY3 else \"ignore\"\n        else:\n            errors = \"strict\"\n    if issubclass(type(string), six.text_type):\n        return string\n    try:\n        if not issubclass(type(string), six.string_types):\n            if six.PY3:\n                if isinstance(string, bytes):\n                    string = six.text_type(string, encoding, errors)\n                else:\n                    string = six.text_type(string)\n            elif hasattr(string, \"__unicode__\"):\n                string = six.text_type(string)\n            else:\n                string = six.text_type(bytes(string), encoding, errors)\n        else:\n            string = string.decode(encoding, errors)\n    except UnicodeDecodeError:\n        string = \" \".join(to_text(arg, encoding, errors) for arg in string)\n    return string"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef divide(n, iterable):\n\n    seq = tuple(iterable)\n    q, r = divmod(len(seq), n)\n\n    ret = []\n    for i in range(n):\n        start = (i * q) + (i if i < r else r)\n        stop = ((i + 1) * q) + (i + 1 if i + 1 < r else r)\n        ret.append(iter(seq[start:stop]))\n\n    return ret", "response": "split an iterable into n groups per https://more - itertools. io / en / latest / api. html#grouping\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndetermine the proper output encoding for terminal rendering", "response": "def getpreferredencoding():\n    \"\"\"Determine the proper output encoding for terminal rendering\"\"\"\n\n    # Borrowed from Invoke\n    # (see https://github.com/pyinvoke/invoke/blob/93af29d/invoke/runners.py#L881)\n    _encoding = locale.getpreferredencoding(False)\n    if six.PY2 and not sys.platform == \"win32\":\n        _default_encoding = locale.getdefaultlocale()[1]\n        if _default_encoding is not None:\n            _encoding = _default_encoding\n    return _encoding"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef decode_for_output(output, target_stream=None, translation_map=None):\n\n    if not isinstance(output, six.string_types):\n        return output\n    encoding = None\n    if target_stream is not None:\n        encoding = getattr(target_stream, \"encoding\", None)\n    encoding = get_output_encoding(encoding)\n    try:\n        output = _encode(output, encoding=encoding, translation_map=translation_map)\n    except (UnicodeDecodeError, UnicodeEncodeError):\n        output = to_native_string(output)\n        output = _encode(\n            output, encoding=encoding, errors=\"replace\", translation_map=translation_map\n        )\n    return to_text(output, encoding=encoding, errors=\"replace\")", "response": "Given a string output decode it to a terminal\nTaxonomy"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives an encoding name get the canonical version of the codec.", "response": "def get_canonical_encoding_name(name):\n    # type: (str) -> str\n    \"\"\"\n    Given an encoding name, get the canonical name from a codec lookup.\n\n    :param str name: The name of the codec to lookup\n    :return: The canonical version of the codec name\n    :rtype: str\n    \"\"\"\n\n    import codecs\n\n    try:\n        codec = codecs.lookup(name)\n    except LookupError:\n        return name\n    else:\n        return codec.name"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngives a stream wrap it in a StreamWrapper instance and return the wrapped stream.", "response": "def get_wrapped_stream(stream):\n    \"\"\"\n    Given a stream, wrap it in a `StreamWrapper` instance and return the wrapped stream.\n\n    :param stream: A stream instance to wrap\n    :returns: A new, wrapped stream\n    :rtype: :class:`StreamWrapper`\n    \"\"\"\n\n    if stream is None:\n        raise TypeError(\"must provide a stream to wrap\")\n    encoding = getattr(stream, \"encoding\", None)\n    encoding = get_output_encoding(encoding)\n    return StreamWrapper(stream, encoding, \"replace\", line_buffering=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_connection_dropped(conn):  # Platform-specific\n    sock = getattr(conn, 'sock', False)\n    if sock is False:  # Platform-specific: AppEngine\n        return False\n    if sock is None:  # Connection already closed (such as by httplib).\n        return True\n    try:\n        # Returns True if readable, which here means it's been dropped\n        return wait_for_read(sock, timeout=0.0)\n    except NoWayToWaitForSocketError:  # Platform-specific: AppEngine\n        return False", "response": "Returns True if the connection is dropped and should be closed."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a connection to the specified address and return the socket object.", "response": "def create_connection(address, timeout=socket._GLOBAL_DEFAULT_TIMEOUT,\n                      source_address=None, socket_options=None):\n    \"\"\"Connect to *address* and return the socket object.\n\n    Convenience function.  Connect to *address* (a 2-tuple ``(host,\n    port)``) and return the socket object.  Passing the optional\n    *timeout* parameter will set the timeout on the socket instance\n    before attempting to connect.  If no *timeout* is supplied, the\n    global default timeout setting returned by :func:`getdefaulttimeout`\n    is used.  If *source_address* is set it must be a tuple of (host, port)\n    for the socket to bind as a source address before making the connection.\n    An host of '' or port 0 tells the OS to use the default.\n    \"\"\"\n\n    host, port = address\n    if host.startswith('['):\n        host = host.strip('[]')\n    err = None\n\n    # Using the value from allowed_gai_family() in the context of getaddrinfo lets\n    # us select whether to work with IPv4 DNS records, IPv6 records, or both.\n    # The original create_connection function always returns all records.\n    family = allowed_gai_family()\n\n    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n        af, socktype, proto, canonname, sa = res\n        sock = None\n        try:\n            sock = socket.socket(af, socktype, proto)\n\n            # If provided, set socket level options before connecting.\n            _set_socket_options(sock, socket_options)\n\n            if timeout is not socket._GLOBAL_DEFAULT_TIMEOUT:\n                sock.settimeout(timeout)\n            if source_address:\n                sock.bind(source_address)\n            sock.connect(sa)\n            return sock\n\n        except socket.error as e:\n            err = e\n            if sock is not None:\n                sock.close()\n                sock = None\n\n    if err is not None:\n        raise err\n\n    raise socket.error(\"getaddrinfo returns an empty list\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns True if the system can bind an IPv6 address.", "response": "def _has_ipv6(host):\n    \"\"\" Returns True if the system can bind an IPv6 address. \"\"\"\n    sock = None\n    has_ipv6 = False\n\n    # App Engine doesn't support IPV6 sockets and actually has a quota on the\n    # number of sockets that can be used, so just early out here instead of\n    # creating a socket needlessly.\n    # See https://github.com/urllib3/urllib3/issues/1446\n    if _appengine_environ.is_appengine_sandbox():\n        return False\n\n    if socket.has_ipv6:\n        # has_ipv6 returns true if cPython was compiled with IPv6 support.\n        # It does not tell us if the system has IPv6 support enabled. To\n        # determine that we must bind to an IPv6 address.\n        # https://github.com/shazow/urllib3/pull/611\n        # https://bugs.python.org/issue658327\n        try:\n            sock = socket.socket(socket.AF_INET6)\n            sock.bind((host, 0))\n            has_ipv6 = True\n        except Exception:\n            pass\n\n    if sock:\n        sock.close()\n    return has_ipv6"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntaking a string like abc. 1. twelve and turns it into tuple.", "response": "def _parse_local_version(local):\n    \"\"\"\n    Takes a string like abc.1.twelve and turns it into (\"abc\", 1, \"twelve\").\n    \"\"\"\n    if local is not None:\n        return tuple(\n            part.lower() if not part.isdigit() else int(part)\n            for part in _local_version_separators.split(local)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if unicode string only contains ASCII characters.", "response": "def unicode_is_ascii(u_string):\n    \"\"\"Determine if unicode string only contains ASCII characters.\n\n    :param str u_string: unicode string to check. Must be unicode\n        and not Python 2 `str`.\n    :rtype: bool\n    \"\"\"\n    assert isinstance(u_string, str)\n    try:\n        u_string.encode('ascii')\n        return True\n    except UnicodeEncodeError:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nraises an option parsing error using parser. error.", "response": "def raise_option_error(parser, option, msg):\n    \"\"\"\n    Raise an option parsing error using parser.error().\n\n    Args:\n      parser: an OptionParser instance.\n      option: an Option instance.\n      msg: the error text.\n    \"\"\"\n    msg = '{} error: {}'.format(option, msg)\n    msg = textwrap.fill(' '.join(msg.split()))\n    parser.error(msg)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate an OptionGroup object from a dict with name and options keys and a ConfigOptionParser object", "response": "def make_option_group(group, parser):\n    # type: (Dict[str, Any], ConfigOptionParser) -> OptionGroup\n    \"\"\"\n    Return an OptionGroup object\n    group  -- assumed to be dict with 'name' and 'options' keys\n    parser -- an optparse Parser\n    \"\"\"\n    option_group = OptionGroup(parser, group['name'])\n    for option in group['options']:\n        option_group.add_option(option())\n    return option_group"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_install_build_global(options, check_options=None):\n    # type: (Values, Optional[Values]) -> None\n    \"\"\"Disable wheels if per-setup.py call options are set.\n\n    :param options: The OptionParser options to update.\n    :param check_options: The options to check, if not supplied defaults to\n        options.\n    \"\"\"\n    if check_options is None:\n        check_options = options\n\n    def getname(n):\n        return getattr(check_options, n, None)\n    names = [\"build_options\", \"global_options\", \"install_options\"]\n    if any(map(getname, names)):\n        control = options.format_control\n        control.disallow_binaries()\n        warnings.warn(\n            'Disabling all use of wheels due to the use of --build-options '\n            '/ --global-options / --install-options.', stacklevel=2,\n        )", "response": "Disable wheels if per - setup. py call options are set."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfunction for determining if custom platform options are allowed.", "response": "def check_dist_restriction(options, check_target=False):\n    # type: (Values, bool) -> None\n    \"\"\"Function for determining if custom platform options are allowed.\n\n    :param options: The OptionParser options.\n    :param check_target: Whether or not to check if --target is being used.\n    \"\"\"\n    dist_restriction_set = any([\n        options.python_version,\n        options.platform,\n        options.abi,\n        options.implementation,\n    ])\n\n    binary_only = FormatControl(set(), {':all:'})\n    sdist_dependencies_allowed = (\n        options.format_control != binary_only and\n        not options.ignore_dependencies\n    )\n\n    # Installations or downloads using dist restrictions must not combine\n    # source distributions and dist-specific wheels, as they are not\n    # gauranteed to be locally compatible.\n    if dist_restriction_set and sdist_dependencies_allowed:\n        raise CommandError(\n            \"When restricting platform and interpreter constraints using \"\n            \"--python-version, --platform, --abi, or --implementation, \"\n            \"either --no-deps must be set, or --only-binary=:all: must be \"\n            \"set and --no-binary must not be set (or must be set to \"\n            \":none:).\"\n        )\n\n    if check_target:\n        if dist_restriction_set and not options.target_dir:\n            raise CommandError(\n                \"Can not use any platform or abi specific options unless \"\n                \"installing via '--target'\"\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef no_cache_dir_callback(option, opt, value, parser):\n    # The value argument will be None if --no-cache-dir is passed via the\n    # command-line, since the option doesn't accept arguments.  However,\n    # the value can be non-None if the option is triggered e.g. by an\n    # environment variable, like PIP_NO_CACHE_DIR=true.\n    if value is not None:\n        # Then parse the string value to get argument error-checking.\n        try:\n            strtobool(value)\n        except ValueError as exc:\n            raise_option_error(parser, option=option, msg=str(exc))\n\n    # Originally, setting PIP_NO_CACHE_DIR to a value that strtobool()\n    # converted to 0 (like \"false\" or \"no\") caused cache_dir to be disabled\n    # rather than enabled (logic would say the latter).  Thus, we disable\n    # the cache directory not just on values that parse to True, but (for\n    # backwards compatibility reasons) also on values that parse to False.\n    # In other words, always set it to False if the option is provided in\n    # some (valid) form.\n    parser.values.cache_dir = False", "response": "This is a callback for the no - cache - dir option."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef no_use_pep517_callback(option, opt, value, parser):\n    # Since --no-use-pep517 doesn't accept arguments, the value argument\n    # will be None if --no-use-pep517 is passed via the command-line.\n    # However, the value can be non-None if the option is triggered e.g.\n    # by an environment variable, for example \"PIP_NO_USE_PEP517=true\".\n    if value is not None:\n        msg = \"\"\"A value was passed for --no-use-pep517,\n        probably using either the PIP_NO_USE_PEP517 environment variable\n        or the \"no-use-pep517\" config file option. Use an appropriate value\n        of the PIP_USE_PEP517 environment variable or the \"use-pep517\"\n        config file option instead.\n        \"\"\"\n        raise_option_error(parser, option=option, msg=msg)\n\n    # Otherwise, --no-use-pep517 was passed via the command-line.\n    parser.values.use_pep517 = False", "response": "This is a callback for the no - use - pep517 option."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _merge_hash(option, opt_str, value, parser):\n    # type: (Option, str, str, OptionParser) -> None\n    \"\"\"Given a value spelled \"algo:digest\", append the digest to a list\n    pointed to in a dict by the algo name.\"\"\"\n    if not parser.values.hashes:\n        parser.values.hashes = {}  # type: ignore\n    try:\n        algo, digest = value.split(':', 1)\n    except ValueError:\n        parser.error('Arguments to %s must be a hash name '\n                     'followed by a value, like --hash=sha256:abcde...' %\n                     opt_str)\n    if algo not in STRONG_HASHES:\n        parser.error('Allowed hash algorithms for %s are %s.' %\n                     (opt_str, ', '.join(STRONG_HASHES)))\n    parser.values.hashes.setdefault(algo, []).append(digest)", "response": "Given a value spelled \"algo : digest append the digest to a list\n    pointed to by the algo name."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef populate_source(cls, source):\n        # Only URL pararemter is mandatory, let the KeyError be thrown.\n        if \"name\" not in source:\n            source[\"name\"] = get_url_name(source[\"url\"])\n        if \"verify_ssl\" not in source:\n            source[\"verify_ssl\"] = \"https://\" in source[\"url\"]\n        if not isinstance(source[\"verify_ssl\"], bool):\n            source[\"verify_ssl\"] = source[\"verify_ssl\"].lower() == \"true\"\n        return source", "response": "Populate source with missing values from the existing fields."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the pinned version of an InstallRequirement.", "response": "def get_pinned_version(ireq):\n    \"\"\"Get the pinned version of an InstallRequirement.\n\n    An InstallRequirement is considered pinned if:\n\n    - Is not editable\n    - It has exactly one specifier\n    - That specifier is \"==\"\n    - The version does not contain a wildcard\n\n    Examples:\n        django==1.8   # pinned\n        django>1.8    # NOT pinned\n        django~=1.8   # NOT pinned\n        django==1.*   # NOT pinned\n\n    Raises `TypeError` if the input is not a valid InstallRequirement, or\n    `ValueError` if the InstallRequirement is not pinned.\n    \"\"\"\n    try:\n        specifier = ireq.specifier\n    except AttributeError:\n        raise TypeError(\"Expected InstallRequirement, not {}\".format(\n            type(ireq).__name__,\n        ))\n\n    if ireq.editable:\n        raise ValueError(\"InstallRequirement is editable\")\n    if not specifier:\n        raise ValueError(\"InstallRequirement has no version specification\")\n    if len(specifier._specs) != 1:\n        raise ValueError(\"InstallRequirement has multiple specifications\")\n\n    op, version = next(iter(specifier._specs))._spec\n    if op not in ('==', '===') or version.endswith('.*'):\n        raise ValueError(\"InstallRequirement not pinned (is {0!r})\".format(\n            op + version,\n        ))\n\n    return version"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a new requirement object with extras removed.", "response": "def strip_extras(requirement):\n    \"\"\"Returns a new requirement object with extras removed.\n    \"\"\"\n    line = requirement.as_line()\n    new = type(requirement).from_line(line)\n    new.extras = None\n    return new"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _subst_vars(path, local_vars):\n    def _replacer(matchobj):\n        name = matchobj.group(1)\n        if name in local_vars:\n            return local_vars[name]\n        elif name in os.environ:\n            return os.environ[name]\n        return matchobj.group(0)\n    return _VAR_REPL.sub(_replacer, path)", "response": "In the string path replace tokens like some. thing with the\n    corresponding value from the map local_vars."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the path of the Makefile.", "response": "def get_makefile_filename():\n    \"\"\"Return the path of the Makefile.\"\"\"\n    if _PYTHON_BUILD:\n        return os.path.join(_PROJECT_BASE, \"Makefile\")\n    if hasattr(sys, 'abiflags'):\n        config_dir_name = 'config-%s%s' % (_PY_VERSION_SHORT, sys.abiflags)\n    else:\n        config_dir_name = 'config'\n    return os.path.join(get_path('stdlib'), config_dir_name, 'Makefile')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _init_posix(vars):\n    # load the installed Makefile:\n    makefile = get_makefile_filename()\n    try:\n        _parse_makefile(makefile, vars)\n    except IOError as e:\n        msg = \"invalid Python installation: unable to open %s\" % makefile\n        if hasattr(e, \"strerror\"):\n            msg = msg + \" (%s)\" % e.strerror\n        raise IOError(msg)\n    # load the installed pyconfig.h:\n    config_h = get_config_h_filename()\n    try:\n        with open(config_h) as f:\n            parse_config_h(f, vars)\n    except IOError as e:\n        msg = \"invalid Python installation: unable to open %s\" % config_h\n        if hasattr(e, \"strerror\"):\n            msg = msg + \" (%s)\" % e.strerror\n        raise IOError(msg)\n    # On AIX, there are wrong paths to the linker scripts in the Makefile\n    # -- these paths are relative to the Python source, but when installed\n    # the scripts are in another directory.\n    if _PYTHON_BUILD:\n        vars['LDSHARED'] = vars['BLDSHARED']", "response": "Initialize the module as appropriate for POSIX systems."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninitialize the module as appropriate for NT", "response": "def _init_non_posix(vars):\n    \"\"\"Initialize the module as appropriate for NT\"\"\"\n    # set basic install directories\n    vars['LIBDEST'] = get_path('stdlib')\n    vars['BINLIBDEST'] = get_path('platstdlib')\n    vars['INCLUDEPY'] = get_path('include')\n    vars['SO'] = '.pyd'\n    vars['EXE'] = '.exe'\n    vars['VERSION'] = _PY_VERSION_SHORT_NO_DOT\n    vars['BINDIR'] = os.path.dirname(_safe_realpath(sys.executable))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing a config. h - style file.", "response": "def parse_config_h(fp, vars=None):\n    \"\"\"Parse a config.h-style file.\n\n    A dictionary containing name/value pairs is returned.  If an\n    optional dictionary is passed in as the second argument, it is\n    used instead of a new dictionary.\n    \"\"\"\n    if vars is None:\n        vars = {}\n    define_rx = re.compile(\"#define ([A-Z][A-Za-z0-9_]+) (.*)\\n\")\n    undef_rx = re.compile(\"/[*] #undef ([A-Z][A-Za-z0-9_]+) [*]/\\n\")\n\n    while True:\n        line = fp.readline()\n        if not line:\n            break\n        m = define_rx.match(line)\n        if m:\n            n, v = m.group(1, 2)\n            try:\n                v = int(v)\n            except ValueError:\n                pass\n            vars[n] = v\n        else:\n            m = undef_rx.match(line)\n            if m:\n                vars[m.group(1)] = 0\n    return vars"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_config_h_filename():\n    if _PYTHON_BUILD:\n        if os.name == \"nt\":\n            inc_dir = os.path.join(_PROJECT_BASE, \"PC\")\n        else:\n            inc_dir = _PROJECT_BASE\n    else:\n        inc_dir = get_path('platinclude')\n    return os.path.join(inc_dir, 'pyconfig.h')", "response": "Return the path of pyconfig. h."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a mapping containing an install scheme name to a list of paths.", "response": "def get_paths(scheme=_get_default_scheme(), vars=None, expand=True):\n    \"\"\"Return a mapping containing an install scheme.\n\n    ``scheme`` is the install scheme name. If not provided, it will\n    return the default scheme for the current platform.\n    \"\"\"\n    _ensure_cfg_read()\n    if expand:\n        return _expand_vars(scheme, vars)\n    else:\n        return dict(_SCHEMES.items(scheme))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_path(name, scheme=_get_default_scheme(), vars=None, expand=True):\n    return get_paths(scheme, vars, expand)[name]", "response": "Return a path corresponding to the given name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndisplay all information sysconfig detains.", "response": "def _main():\n    \"\"\"Display all information sysconfig detains.\"\"\"\n    print('Platform: \"%s\"' % get_platform())\n    print('Python version: \"%s\"' % get_python_version())\n    print('Current installation scheme: \"%s\"' % _get_default_scheme())\n    print()\n    _print_dict('Paths', get_paths())\n    print()\n    _print_dict('Variables', get_config_vars())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef inc(self, exception=None):  # type: (Optional[ParseError.__class__]) -> bool\n        try:\n            self._idx, self._current = next(self._chars)\n\n            return True\n        except StopIteration:\n            self._idx = len(self)\n            self._current = self.EOF\n            if exception:\n                raise self.parse_error(exception)\n\n            return False", "response": "Increments the parser if the end of the input has not been reached. Returns whether or not it was able to advance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nincrementing the parser by n characters and returns True if the parser is finished.", "response": "def inc_n(self, n, exception=None):  # type: (int, Exception) -> bool\n        \"\"\"\n        Increments the parser by n characters\n        if the end of the input has not been reached.\n        \"\"\"\n        for _ in range(n):\n            if not self.inc(exception=exception):\n                return False\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconsume characters until min or max is satisfied.", "response": "def consume(self, chars, min=0, max=-1):\n        \"\"\"\n        Consume chars until min/max is satisfied is valid.\n        \"\"\"\n        while self.current in chars and max != 0:\n            min -= 1\n            max -= 1\n            if not self.inc():\n                break\n\n        # failed to consume minimum number of characters\n        if min > 0:\n            self.parse_error(UnexpectedCharError)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_error(\n        self, exception=ParseError, *args\n    ):  # type: (ParseError.__class__, ...) -> ParseError\n        \"\"\"\n        Creates a generic \"parse error\" at the current position.\n        \"\"\"\n        line, col = self._to_linecol()\n\n        return exception(line, col, *args)", "response": "Creates a generic parse error at the current position."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nyield sorted ( command name command summary ) tuples.", "response": "def get_summaries(ordered=True):\n    \"\"\"Yields sorted (command name, command summary) tuples.\"\"\"\n\n    if ordered:\n        cmditems = _sort_commands(commands_dict, commands_order)\n    else:\n        cmditems = commands_dict.items()\n\n    for name, command_class in cmditems:\n        yield (name, command_class.summary)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_similar_commands(name):\n    from difflib import get_close_matches\n\n    name = name.lower()\n\n    close_commands = get_close_matches(name, commands_dict.keys())\n\n    if close_commands:\n        return close_commands[0]\n    else:\n        return False", "response": "Return the command that is similar to the given command name."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bind(self, environment):\n        rv = object.__new__(self.__class__)\n        rv.__dict__.update(self.__dict__)\n        rv.environment = environment\n        return rv", "response": "Create a copy of this extension bound to another environment."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef attr(self, name, lineno=None):\n        return nodes.ExtensionAttribute(self.identifier, name, lineno=lineno)", "response": "Return an attribute node for the current extension."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncall a method of the extension. This is a shortcut for :meth:`attr` + :class:`jinja2.nodes.Call`.", "response": "def call_method(self, name, args=None, kwargs=None, dyn_args=None,\n                    dyn_kwargs=None, lineno=None):\n        \"\"\"Call a method of the extension.  This is a shortcut for\n        :meth:`attr` + :class:`jinja2.nodes.Call`.\n        \"\"\"\n        if args is None:\n            args = []\n        if kwargs is None:\n            kwargs = []\n        return nodes.Call(self.attr(name, lineno=lineno), args, kwargs,\n                          dyn_args, dyn_kwargs, lineno=lineno)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse(self, parser):\n        lineno = next(parser.stream).lineno\n        num_called_num = False\n\n        # find all the variables referenced.  Additionally a variable can be\n        # defined in the body of the trans block too, but this is checked at\n        # a later state.\n        plural_expr = None\n        plural_expr_assignment = None\n        variables = {}\n        trimmed = None\n        while parser.stream.current.type != 'block_end':\n            if variables:\n                parser.stream.expect('comma')\n\n            # skip colon for python compatibility\n            if parser.stream.skip_if('colon'):\n                break\n\n            name = parser.stream.expect('name')\n            if name.value in variables:\n                parser.fail('translatable variable %r defined twice.' %\n                            name.value, name.lineno,\n                            exc=TemplateAssertionError)\n\n            # expressions\n            if parser.stream.current.type == 'assign':\n                next(parser.stream)\n                variables[name.value] = var = parser.parse_expression()\n            elif trimmed is None and name.value in ('trimmed', 'notrimmed'):\n                trimmed = name.value == 'trimmed'\n                continue\n            else:\n                variables[name.value] = var = nodes.Name(name.value, 'load')\n\n            if plural_expr is None:\n                if isinstance(var, nodes.Call):\n                    plural_expr = nodes.Name('_trans', 'load')\n                    variables[name.value] = plural_expr\n                    plural_expr_assignment = nodes.Assign(\n                        nodes.Name('_trans', 'store'), var)\n                else:\n                    plural_expr = var\n                num_called_num = name.value == 'num'\n\n        parser.stream.expect('block_end')\n\n        plural = None\n        have_plural = False\n        referenced = set()\n\n        # now parse until endtrans or pluralize\n        singular_names, singular = self._parse_block(parser, True)\n        if singular_names:\n            referenced.update(singular_names)\n            if plural_expr is None:\n                plural_expr = nodes.Name(singular_names[0], 'load')\n                num_called_num = singular_names[0] == 'num'\n\n        # if we have a pluralize block, we parse that too\n        if parser.stream.current.test('name:pluralize'):\n            have_plural = True\n            next(parser.stream)\n            if parser.stream.current.type != 'block_end':\n                name = parser.stream.expect('name')\n                if name.value not in variables:\n                    parser.fail('unknown variable %r for pluralization' %\n                                name.value, name.lineno,\n                                exc=TemplateAssertionError)\n                plural_expr = variables[name.value]\n                num_called_num = name.value == 'num'\n            parser.stream.expect('block_end')\n            plural_names, plural = self._parse_block(parser, False)\n            next(parser.stream)\n            referenced.update(plural_names)\n        else:\n            next(parser.stream)\n\n        # register free names as simple name expressions\n        for var in referenced:\n            if var not in variables:\n                variables[var] = nodes.Name(var, 'load')\n\n        if not have_plural:\n            plural_expr = None\n        elif plural_expr is None:\n            parser.fail('pluralize without variables', lineno)\n\n        if trimmed is None:\n            trimmed = self.environment.policies['ext.i18n.trimmed']\n        if trimmed:\n            singular = self._trim_whitespace(singular)\n            if plural:\n                plural = self._trim_whitespace(plural)\n\n        node = self._make_node(singular, plural, variables, plural_expr,\n                               bool(referenced),\n                               num_called_num and have_plural)\n        node.set_lineno(lineno)\n        if plural_expr_assignment is not None:\n            return [plural_expr_assignment, node]\n        else:\n            return node", "response": "Parse a translatable tag."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _parse_block(self, parser, allow_pluralize):\n        referenced = []\n        buf = []\n        while 1:\n            if parser.stream.current.type == 'data':\n                buf.append(parser.stream.current.value.replace('%', '%%'))\n                next(parser.stream)\n            elif parser.stream.current.type == 'variable_begin':\n                next(parser.stream)\n                name = parser.stream.expect('name').value\n                referenced.append(name)\n                buf.append('%%(%s)s' % name)\n                parser.stream.expect('variable_end')\n            elif parser.stream.current.type == 'block_begin':\n                next(parser.stream)\n                if parser.stream.current.test('name:endtrans'):\n                    break\n                elif parser.stream.current.test('name:pluralize'):\n                    if allow_pluralize:\n                        break\n                    parser.fail('a translatable section can have only one '\n                                'pluralize section')\n                parser.fail('control structures in translatable sections are '\n                            'not allowed')\n            elif parser.stream.eos:\n                parser.fail('unclosed translation block')\n            else:\n                assert False, 'internal parser error'\n\n        return referenced, concat(buf)", "response": "Parse until the next block tag with a given name."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _make_node(self, singular, plural, variables, plural_expr,\n                   vars_referenced, num_called_num):\n        \"\"\"Generates a useful node from the data provided.\"\"\"\n        # no variables referenced?  no need to escape for old style\n        # gettext invocations only if there are vars.\n        if not vars_referenced and not self.environment.newstyle_gettext:\n            singular = singular.replace('%%', '%')\n            if plural:\n                plural = plural.replace('%%', '%')\n\n        # singular only:\n        if plural_expr is None:\n            gettext = nodes.Name('gettext', 'load')\n            node = nodes.Call(gettext, [nodes.Const(singular)],\n                              [], None, None)\n\n        # singular and plural\n        else:\n            ngettext = nodes.Name('ngettext', 'load')\n            node = nodes.Call(ngettext, [\n                nodes.Const(singular),\n                nodes.Const(plural),\n                plural_expr\n            ], [], None, None)\n\n        # in case newstyle gettext is used, the method is powerful\n        # enough to handle the variable expansion and autoescape\n        # handling itself\n        if self.environment.newstyle_gettext:\n            for key, value in iteritems(variables):\n                # the function adds that later anyways in case num was\n                # called num, so just skip it.\n                if num_called_num and key == 'num':\n                    continue\n                node.kwargs.append(nodes.Keyword(key, value))\n\n        # otherwise do that here\n        else:\n            # mark the return value as safe if we are in an\n            # environment with autoescaping turned on\n            node = nodes.MarkSafeIfAutoescape(node)\n            if variables:\n                node = nodes.Mod(node, nodes.Dict([\n                    nodes.Pair(nodes.Const(key), value)\n                    for key, value in variables.items()\n                ]))\n        return nodes.Output([node])", "response": "Generates a useful node from the data provided."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_cli_string(path=None, action=None, key=None, value=None, quote=None):\n    command = ['dotenv']\n    if quote:\n        command.append('-q %s' % quote)\n    if path:\n        command.append('-f %s' % path)\n    if action:\n        command.append(action)\n        if key:\n            command.append(key)\n            if value:\n                if ' ' in value:\n                    command.append('\"%s\"' % value)\n                else:\n                    command.append(value)\n\n    return ' '.join(command).strip()", "response": "Returns a string suitable for running a fabric task\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef merge_hooks(request_hooks, session_hooks, dict_class=OrderedDict):\n    if session_hooks is None or session_hooks.get('response') == []:\n        return request_hooks\n\n    if request_hooks is None or request_hooks.get('response') == []:\n        return session_hooks\n\n    return merge_setting(request_hooks, session_hooks, dict_class)", "response": "Merges two request and session hooks."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreceives a Response. Returns a redirect URI or None.", "response": "def get_redirect_target(self, resp):\n        \"\"\"Receives a Response. Returns a redirect URI or ``None``\"\"\"\n        # Due to the nature of how requests processes redirects this method will\n        # be called at least once upon the original response and at least twice\n        # on each subsequent redirect response (if any).\n        # If a custom mixin is used to handle this logic, it may be advantageous\n        # to cache the redirect location onto the response object as a private\n        # attribute.\n        if resp.is_redirect:\n            location = resp.headers['location']\n            # Currently the underlying http module on py3 decode headers\n            # in latin1, but empirical evidence suggests that latin1 is very\n            # rarely used with non-ASCII characters in HTTP headers.\n            # It is more likely to get UTF8 header rather than latin1.\n            # This causes incorrect handling of UTF8 encoded location headers.\n            # To solve this, we re-encode the location in latin1.\n            if is_py3:\n                location = location.encode('latin1')\n            return to_native_string(location, 'utf8')\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndecides whether Authorization header should be removed when redirecting.", "response": "def should_strip_auth(self, old_url, new_url):\n        \"\"\"Decide whether Authorization header should be removed when redirecting\"\"\"\n        old_parsed = urlparse(old_url)\n        new_parsed = urlparse(new_url)\n        if old_parsed.hostname != new_parsed.hostname:\n            return True\n        # Special case: allow http -> https redirect when using the standard\n        # ports. This isn't specified by RFC 7235, but is kept to avoid\n        # breaking backwards compatibility with older versions of requests\n        # that allowed any redirects on the same host.\n        if (old_parsed.scheme == 'http' and old_parsed.port in (80, None)\n                and new_parsed.scheme == 'https' and new_parsed.port in (443, None)):\n            return False\n\n        # Handle default port usage corresponding to scheme.\n        changed_port = old_parsed.port != new_parsed.port\n        changed_scheme = old_parsed.scheme != new_parsed.scheme\n        default_port = (DEFAULT_PORTS.get(old_parsed.scheme, None), None)\n        if (not changed_scheme and old_parsed.port in default_port\n                and new_parsed.port in default_port):\n            return False\n\n        # Standard case: root URI must match\n        return changed_port or changed_scheme"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rebuild_auth(self, prepared_request, response):\n        headers = prepared_request.headers\n        url = prepared_request.url\n\n        if 'Authorization' in headers and self.should_strip_auth(response.request.url, url):\n            # If we get redirected to a new host, we should strip out any\n            # authentication headers.\n            del headers['Authorization']\n\n        # .netrc might have more auth for us on our new host.\n        new_auth = get_netrc_auth(url) if self.trust_env else None\n        if new_auth is not None:\n            prepared_request.prepare_auth(new_auth)\n\n        return", "response": "Rebuilds the authentication headers for the current request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconstruct a PreparedRequest for the given request.", "response": "def prepare_request(self, request):\n        \"\"\"Constructs a :class:`PreparedRequest <PreparedRequest>` for\n        transmission and returns it. The :class:`PreparedRequest` has settings\n        merged from the :class:`Request <Request>` instance and those of the\n        :class:`Session`.\n\n        :param request: :class:`Request` instance to prepare with this\n            session's settings.\n        :rtype: requests.PreparedRequest\n        \"\"\"\n        cookies = request.cookies or {}\n\n        # Bootstrap CookieJar.\n        if not isinstance(cookies, cookielib.CookieJar):\n            cookies = cookiejar_from_dict(cookies)\n\n        # Merge with session cookies\n        merged_cookies = merge_cookies(\n            merge_cookies(RequestsCookieJar(), self.cookies), cookies)\n\n        # Set environment's basic authentication if not explicitly set.\n        auth = request.auth\n        if self.trust_env and not auth and not self.auth:\n            auth = get_netrc_auth(request.url)\n\n        p = PreparedRequest()\n        p.prepare(\n            method=request.method.upper(),\n            url=request.url,\n            files=request.files,\n            data=request.data,\n            json=request.json,\n            headers=merge_setting(request.headers, self.headers, dict_class=CaseInsensitiveDict),\n            params=merge_setting(request.params, self.params),\n            auth=merge_setting(auth, self.auth),\n            cookies=merged_cookies,\n            hooks=merge_hooks(request.hooks, self.hooks),\n        )\n        return p"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconstruct a new request object and sends it to the server.", "response": "def request(self, method, url,\n            params=None, data=None, headers=None, cookies=None, files=None,\n            auth=None, timeout=None, allow_redirects=True, proxies=None,\n            hooks=None, stream=None, verify=None, cert=None, json=None):\n        \"\"\"Constructs a :class:`Request <Request>`, prepares it and sends it.\n        Returns :class:`Response <Response>` object.\n\n        :param method: method for the new :class:`Request` object.\n        :param url: URL for the new :class:`Request` object.\n        :param params: (optional) Dictionary or bytes to be sent in the query\n            string for the :class:`Request`.\n        :param data: (optional) Dictionary, list of tuples, bytes, or file-like\n            object to send in the body of the :class:`Request`.\n        :param json: (optional) json to send in the body of the\n            :class:`Request`.\n        :param headers: (optional) Dictionary of HTTP Headers to send with the\n            :class:`Request`.\n        :param cookies: (optional) Dict or CookieJar object to send with the\n            :class:`Request`.\n        :param files: (optional) Dictionary of ``'filename': file-like-objects``\n            for multipart encoding upload.\n        :param auth: (optional) Auth tuple or callable to enable\n            Basic/Digest/Custom HTTP Auth.\n        :param timeout: (optional) How long to wait for the server to send\n            data before giving up, as a float, or a :ref:`(connect timeout,\n            read timeout) <timeouts>` tuple.\n        :type timeout: float or tuple\n        :param allow_redirects: (optional) Set to True by default.\n        :type allow_redirects: bool\n        :param proxies: (optional) Dictionary mapping protocol or protocol and\n            hostname to the URL of the proxy.\n        :param stream: (optional) whether to immediately download the response\n            content. Defaults to ``False``.\n        :param verify: (optional) Either a boolean, in which case it controls whether we verify\n            the server's TLS certificate, or a string, in which case it must be a path\n            to a CA bundle to use. Defaults to ``True``.\n        :param cert: (optional) if String, path to ssl client cert file (.pem).\n            If Tuple, ('cert', 'key') pair.\n        :rtype: requests.Response\n        \"\"\"\n        # Create the Request.\n        req = Request(\n            method=method.upper(),\n            url=url,\n            headers=headers,\n            files=files,\n            data=data or {},\n            json=json,\n            params=params or {},\n            auth=auth,\n            cookies=cookies,\n            hooks=hooks,\n        )\n        prep = self.prepare_request(req)\n\n        proxies = proxies or {}\n\n        settings = self.merge_environment_settings(\n            prep.url, proxies, stream, verify, cert\n        )\n\n        # Send the request.\n        send_kwargs = {\n            'timeout': timeout,\n            'allow_redirects': allow_redirects,\n        }\n        send_kwargs.update(settings)\n        resp = self.send(prep, **send_kwargs)\n\n        return resp"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef options(self, url, **kwargs):\n\n        kwargs.setdefault('allow_redirects', True)\n        return self.request('OPTIONS', url, **kwargs)", "response": "r Sends an OPTIONS request. Returns a new Response object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsends a PreparedRequest object to the appropriate adapter.", "response": "def send(self, request, **kwargs):\n        \"\"\"Send a given PreparedRequest.\n\n        :rtype: requests.Response\n        \"\"\"\n        # Set defaults that the hooks can utilize to ensure they always have\n        # the correct parameters to reproduce the previous request.\n        kwargs.setdefault('stream', self.stream)\n        kwargs.setdefault('verify', self.verify)\n        kwargs.setdefault('cert', self.cert)\n        kwargs.setdefault('proxies', self.proxies)\n\n        # It's possible that users might accidentally send a Request object.\n        # Guard against that specific failure case.\n        if isinstance(request, Request):\n            raise ValueError('You can only send PreparedRequests.')\n\n        # Set up variables needed for resolve_redirects and dispatching of hooks\n        allow_redirects = kwargs.pop('allow_redirects', True)\n        stream = kwargs.get('stream')\n        hooks = request.hooks\n\n        # Get the appropriate adapter to use\n        adapter = self.get_adapter(url=request.url)\n\n        # Start time (approximately) of the request\n        start = preferred_clock()\n\n        # Send the request\n        r = adapter.send(request, **kwargs)\n\n        # Total elapsed time of the request (approximately)\n        elapsed = preferred_clock() - start\n        r.elapsed = timedelta(seconds=elapsed)\n\n        # Response manipulation hooks\n        r = dispatch_hook('response', hooks, r, **kwargs)\n\n        # Persist cookies\n        if r.history:\n\n            # If the hooks create history then we want those cookies too\n            for resp in r.history:\n                extract_cookies_to_jar(self.cookies, resp.request, resp.raw)\n\n        extract_cookies_to_jar(self.cookies, request, r.raw)\n\n        # Redirect resolving generator.\n        gen = self.resolve_redirects(r, request, **kwargs)\n\n        # Resolve redirects if allowed.\n        history = [resp for resp in gen] if allow_redirects else []\n\n        # Shuffle things around if there's history.\n        if history:\n            # Insert the first (original) request at the start\n            history.insert(0, r)\n            # Get the last request made\n            r = history.pop()\n            r.history = history\n\n        # If redirects aren't being followed, store the response on the Request for Response.next().\n        if not allow_redirects:\n            try:\n                r._next = next(self.resolve_redirects(r, request, yield_requests=True, **kwargs))\n            except StopIteration:\n                pass\n\n        if not stream:\n            r.content\n\n        return r"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef merge_environment_settings(self, url, proxies, stream, verify, cert):\n        # Gather clues from the surrounding environment.\n        if self.trust_env:\n            # Set environment's proxies.\n            no_proxy = proxies.get('no_proxy') if proxies is not None else None\n            env_proxies = get_environ_proxies(url, no_proxy=no_proxy)\n            for (k, v) in env_proxies.items():\n                proxies.setdefault(k, v)\n\n            # Look for requests environment configuration and be compatible\n            # with cURL.\n            if verify is True or verify is None:\n                verify = (os.environ.get('REQUESTS_CA_BUNDLE') or\n                          os.environ.get('CURL_CA_BUNDLE'))\n\n        # Merge all the kwargs.\n        proxies = merge_setting(proxies, self.proxies)\n        stream = merge_setting(stream, self.stream)\n        verify = merge_setting(verify, self.verify)\n        cert = merge_setting(cert, self.cert)\n\n        return {'verify': verify, 'proxies': proxies, 'stream': stream,\n                'cert': cert}", "response": "Merge the environment settings with some kwargs."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_adapter(self, url):\n        for (prefix, adapter) in self.adapters.items():\n\n            if url.lower().startswith(prefix.lower()):\n                return adapter\n\n        # Nothing matches :-/\n        raise InvalidSchema(\"No connection adapters were found for '%s'\" % url)", "response": "Returns the appropriate connection adapter for the given URL."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mount(self, prefix, adapter):\n        self.adapters[prefix] = adapter\n        keys_to_move = [k for k in self.adapters if len(k) < len(prefix)]\n\n        for key in keys_to_move:\n            self.adapters[key] = self.adapters.pop(key)", "response": "Registers a connection adapter to a prefix."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a console output to a string.", "response": "def console_to_str(data):\n    # type: (bytes) -> Text\n    \"\"\"Return a string, safe for output, of subprocess output.\n\n    We assume the data is in the locale preferred encoding.\n    If it won't decode properly, we warn the user but decode as\n    best we can.\n\n    We also ensure that the output can be safely written to\n    standard output without encoding errors.\n    \"\"\"\n\n    # First, get the encoding we assume. This is the preferred\n    # encoding for the locale, unless that is not found, or\n    # it is ASCII, in which case assume UTF-8\n    encoding = locale.getpreferredencoding()\n    if (not encoding) or codecs.lookup(encoding).name == \"ascii\":\n        encoding = \"utf-8\"\n\n    # Now try to decode the data - if we fail, warn the user and\n    # decode with replacement.\n    try:\n        decoded_data = data.decode(encoding)\n    except UnicodeDecodeError:\n        logger.warning(\n            \"Subprocess output does not appear to be encoded as %s\",\n            encoding,\n        )\n        decoded_data = data.decode(encoding, errors=backslashreplace_decode)\n\n    # Make sure we can print the output, by encoding it to the output\n    # encoding with replacement of unencodable characters, and then\n    # decoding again.\n    # We use stderr's encoding because it's less likely to be\n    # redirected and if we don't find an encoding we skip this\n    # step (on the assumption that output is wrapped by something\n    # that won't fail).\n    # The double getattr is to deal with the possibility that we're\n    # being called in a situation where sys.__stderr__ doesn't exist,\n    # or doesn't have an encoding attribute. Neither of these cases\n    # should occur in normal pip use, but there's no harm in checking\n    # in case people use pip in (unsupported) unusual situations.\n    output_encoding = getattr(getattr(sys, \"__stderr__\", None),\n                              \"encoding\", None)\n\n    if output_encoding:\n        output_encoded = decoded_data.encode(\n            output_encoding,\n            errors=\"backslashreplace\"\n        )\n        decoded_data = output_encoded.decode(output_encoding)\n\n    return decoded_data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_path_uid(path):\n    # type: (str) -> int\n    \"\"\"\n    Return path's uid.\n\n    Does not follow symlinks:\n        https://github.com/pypa/pip/pull/935#discussion_r5307003\n\n    Placed this function in compat due to differences on AIX and\n    Jython, that should eventually go away.\n\n    :raises OSError: When path is a symlink or can't be read.\n    \"\"\"\n    if hasattr(os, 'O_NOFOLLOW'):\n        fd = os.open(path, os.O_RDONLY | os.O_NOFOLLOW)\n        file_uid = os.fstat(fd).st_uid\n        os.close(fd)\n    else:  # AIX and Jython\n        # WARNING: time of check vulnerability, but best we can do w/o NOFOLLOW\n        if not os.path.islink(path):\n            # older versions of Jython don't have `os.fstat`\n            file_uid = os.stat(path).st_uid\n        else:\n            # raise OSError for parity with os.O_NOFOLLOW above\n            raise OSError(\n                \"%s is a symlink; Will not return uid for symlinks\" % path\n            )\n    return file_uid", "response": "Returns the uid of the file in the order of the path."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexpands ~ and ~user constructions.", "response": "def expanduser(path):\n    # type: (str) -> str\n    \"\"\"\n    Expand ~ and ~user constructions.\n\n    Includes a workaround for https://bugs.python.org/issue14768\n    \"\"\"\n    expanded = os.path.expanduser(path)\n    if path.startswith('~/') and expanded.startswith('//'):\n        expanded = expanded[1:]\n    return expanded"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if file1 and file2 are the same file on Windows and False otherwise.", "response": "def samefile(file1, file2):\n    # type: (str, str) -> bool\n    \"\"\"Provide an alternative for os.path.samefile on Windows/Python2\"\"\"\n    if hasattr(os.path, 'samefile'):\n        return os.path.samefile(file1, file2)\n    else:\n        path1 = os.path.normcase(os.path.abspath(file1))\n        path2 = os.path.normcase(os.path.abspath(file2))\n        return path1 == path2"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmutating the current entry to ensure that the smallest amount of updates possible to the existing lockfile.", "response": "def ensure_least_updates_possible(self):\n        \"\"\"\n        Mutate the current entry to ensure that we are making the smallest amount of\n        changes possible to the existing lockfile -- this will keep the old locked\n        versions of packages if they satisfy new constraints.\n\n        :return: None\n        \"\"\"\n        constraints = self.get_constraints()\n        can_use_original = True\n        can_use_updated = True\n        satisfied_by_versions = set()\n        for constraint in constraints:\n            if not constraint.specifier.contains(self.original_version):\n                self.can_use_original = False\n            if not constraint.specifier.contains(self.updated_version):\n                self.can_use_updated = False\n            satisfied_by_value = getattr(constraint, \"satisfied_by\", None)\n            if satisfied_by_value:\n                satisfied_by = \"{0}\".format(\n                    self.clean_specifier(str(satisfied_by_value.version))\n                )\n                satisfied_by_versions.add(satisfied_by)\n        if can_use_original:\n            self.entry_dict = self.lockfile_dict.copy()\n        elif can_use_updated:\n            if len(satisfied_by_versions) == 1:\n                self.entry_dict[\"version\"] = next(iter(\n                    sat_by for sat_by in satisfied_by_versions if sat_by\n                ), None)\n                hashes = None\n                if self.lockfile_entry.specifiers == satisfied_by:\n                    ireq = self.lockfile_entry.as_ireq()\n                    if not self.lockfile_entry.hashes and self.resolver._should_include_hash(ireq):\n                        hashes = self.resolver.get_hash(ireq)\n                    else:\n                        hashes = self.lockfile_entry.hashes\n                else:\n                    if self.resolver._should_include_hash(constraint):\n                        hashes = self.resolver.get_hash(constraint)\n                if hashes:\n                    self.entry_dict[\"hashes\"] = list(hashes)\n                    self._entry.hashes = frozenset(hashes)\n        else:\n            # check for any parents, since they depend on this and the current\n            # installed versions are not compatible with the new version, so\n            # we will need to update the top level dependency if possible\n            self.check_flattened_parents()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve all of the relevant constraints aggregated from the pipfile resolver and parent dependencies and their respective conflict resolution where possible.", "response": "def get_constraints(self):\n        \"\"\"\n        Retrieve all of the relevant constraints, aggregated from the pipfile, resolver,\n        and parent dependencies and their respective conflict resolution where possible.\n\n        :return: A set of **InstallRequirement** instances representing constraints\n        :rtype: Set\n        \"\"\"\n        constraints = {\n            c for c in self.resolver.parsed_constraints\n            if c and c.name == self.entry.name\n        }\n        pipfile_constraint = self.get_pipfile_constraint()\n        if pipfile_constraint:\n            constraints.add(pipfile_constraint)\n        return constraints"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef constraint_from_parent_conflicts(self):\n        # ensure that we satisfy the parent dependencies of this dep\n        from pipenv.vendor.packaging.specifiers import Specifier\n        parent_dependencies = set()\n        has_mismatch = False\n        can_use_original = True\n        for p in self.parent_deps:\n            # updated dependencies should be satisfied since they were resolved already\n            if p.is_updated:\n                continue\n            # parents with no requirements can't conflict\n            if not p.requirements:\n                continue\n            needed = p.requirements.get(\"dependencies\", [])\n            entry_ref = p.get_dependency(self.name)\n            required = entry_ref.get(\"required_version\", \"*\")\n            required = self.clean_specifier(required)\n            parent_requires = self.make_requirement(self.name, required)\n            parent_dependencies.add(\"{0} => {1} ({2})\".format(p.name, self.name, required))\n            if not parent_requires.requirement.specifier.contains(self.original_version):\n                can_use_original = False\n            if not parent_requires.requirement.specifier.contains(self.updated_version):\n                has_mismatch = True\n        if has_mismatch and not can_use_original:\n            from pipenv.exceptions import DependencyConflict\n            msg = (\n                \"Cannot resolve {0} ({1}) due to conflicting parent dependencies: \"\n                \"\\n\\t{2}\".format(\n                    self.name, self.updated_version, \"\\n\\t\".join(parent_dependencies)\n                )\n            )\n            raise DependencyConflict(msg)\n        elif can_use_original:\n            return self.lockfile_entry.as_ireq()\n        return self.entry.as_ireq()", "response": "Given a resolved entry with multiple parent dependencies with different constraints returns a new InstallRequirement that satisfies all of the parent constraints."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks that all constraints for the current version are satisfied by the resolution provided", "response": "def validate_constraints(self):\n        \"\"\"\n        Retrieves the full set of available constraints and iterate over them, validating\n        that they exist and that they are not causing unresolvable conflicts.\n\n        :return: True if the constraints are satisfied by the resolution provided\n        :raises: :exc:`pipenv.exceptions.DependencyConflict` if the constraints dont exist\n        \"\"\"\n        constraints = self.get_constraints()\n        for constraint in constraints:\n            try:\n                constraint.check_if_exists(False)\n            except Exception:\n                from pipenv.exceptions import DependencyConflict\n                msg = (\n                    \"Cannot resolve conflicting version {0}{1} while {1}{2} is \"\n                    \"locked.\".format(\n                        self.name, self.updated_specifier, self.old_name, self.old_specifiers\n                    )\n                )\n                raise DependencyConflict(msg)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef inject_into_urllib3():\n    util.ssl_.SSLContext = SecureTransportContext\n    util.HAS_SNI = HAS_SNI\n    util.ssl_.HAS_SNI = HAS_SNI\n    util.IS_SECURETRANSPORT = True\n    util.ssl_.IS_SECURETRANSPORT = True", "response": "Monkey - patch urllib3 with SecureTransport - backed SSL - support."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nextracting the SSL context and SNI from the URLLib3 object.", "response": "def extract_from_urllib3():\n    \"\"\"\n    Undo monkey-patching by :func:`inject_into_urllib3`.\n    \"\"\"\n    util.ssl_.SSLContext = orig_util_SSLContext\n    util.HAS_SNI = orig_util_HAS_SNI\n    util.ssl_.HAS_SNI = orig_util_HAS_SNI\n    util.IS_SECURETRANSPORT = False\n    util.ssl_.IS_SECURETRANSPORT = False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _read_callback(connection_id, data_buffer, data_length_pointer):\n    wrapped_socket = None\n    try:\n        wrapped_socket = _connection_refs.get(connection_id)\n        if wrapped_socket is None:\n            return SecurityConst.errSSLInternal\n        base_socket = wrapped_socket.socket\n\n        requested_length = data_length_pointer[0]\n\n        timeout = wrapped_socket.gettimeout()\n        error = None\n        read_count = 0\n\n        try:\n            while read_count < requested_length:\n                if timeout is None or timeout >= 0:\n                    if not util.wait_for_read(base_socket, timeout):\n                        raise socket.error(errno.EAGAIN, 'timed out')\n\n                remaining = requested_length - read_count\n                buffer = (ctypes.c_char * remaining).from_address(\n                    data_buffer + read_count\n                )\n                chunk_size = base_socket.recv_into(buffer, remaining)\n                read_count += chunk_size\n                if not chunk_size:\n                    if not read_count:\n                        return SecurityConst.errSSLClosedGraceful\n                    break\n        except (socket.error) as e:\n            error = e.errno\n\n            if error is not None and error != errno.EAGAIN:\n                data_length_pointer[0] = read_count\n                if error == errno.ECONNRESET or error == errno.EPIPE:\n                    return SecurityConst.errSSLClosedAbort\n                raise\n\n        data_length_pointer[0] = read_count\n\n        if read_count != requested_length:\n            return SecurityConst.errSSLWouldBlock\n\n        return 0\n    except Exception as e:\n        if wrapped_socket is not None:\n            wrapped_socket._exception = e\n        return SecurityConst.errSSLInternal", "response": "This is the SecureTransport read callback. This is called by ST to request that the socket read from the server. This is called by ST to request that the socket read from the server."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _write_callback(connection_id, data_buffer, data_length_pointer):\n    wrapped_socket = None\n    try:\n        wrapped_socket = _connection_refs.get(connection_id)\n        if wrapped_socket is None:\n            return SecurityConst.errSSLInternal\n        base_socket = wrapped_socket.socket\n\n        bytes_to_write = data_length_pointer[0]\n        data = ctypes.string_at(data_buffer, bytes_to_write)\n\n        timeout = wrapped_socket.gettimeout()\n        error = None\n        sent = 0\n\n        try:\n            while sent < bytes_to_write:\n                if timeout is None or timeout >= 0:\n                    if not util.wait_for_write(base_socket, timeout):\n                        raise socket.error(errno.EAGAIN, 'timed out')\n                chunk_sent = base_socket.send(data)\n                sent += chunk_sent\n\n                # This has some needless copying here, but I'm not sure there's\n                # much value in optimising this data path.\n                data = data[chunk_sent:]\n        except (socket.error) as e:\n            error = e.errno\n\n            if error is not None and error != errno.EAGAIN:\n                data_length_pointer[0] = sent\n                if error == errno.ECONNRESET or error == errno.EPIPE:\n                    return SecurityConst.errSSLClosedAbort\n                raise\n\n        data_length_pointer[0] = sent\n\n        if sent != bytes_to_write:\n            return SecurityConst.errSSLWouldBlock\n\n        return 0\n    except Exception as e:\n        if wrapped_socket is not None:\n            wrapped_socket._exception = e\n        return SecurityConst.errSSLInternal", "response": "This is the callback function that is called by ST to request that the secure transport actually wants to send the data on the network. This is called by ST to request that the secure transport actually wants to send the data on the network."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _set_ciphers(self):\n        ciphers = (Security.SSLCipherSuite * len(CIPHER_SUITES))(*CIPHER_SUITES)\n        result = Security.SSLSetEnabledCiphers(\n            self.context, ciphers, len(CIPHER_SUITES)\n        )\n        _assert_no_error(result)", "response": "Sets up the allowed ciphers."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncall when we have set custom validation. We do this in two cases: first, when cert validation is entirely disabled; and second, when using a custom trust DB.", "response": "def _custom_validate(self, verify, trust_bundle):\n        \"\"\"\n        Called when we have set custom validation. We do this in two cases:\n        first, when cert validation is entirely disabled; and second, when\n        using a custom trust DB.\n        \"\"\"\n        # If we disabled cert validation, just say: cool.\n        if not verify:\n            return\n\n        # We want data in memory, so load it up.\n        if os.path.isfile(trust_bundle):\n            with open(trust_bundle, 'rb') as f:\n                trust_bundle = f.read()\n\n        cert_array = None\n        trust = Security.SecTrustRef()\n\n        try:\n            # Get a CFArray that contains the certs we want.\n            cert_array = _cert_array_from_pem(trust_bundle)\n\n            # Ok, now the hard part. We want to get the SecTrustRef that ST has\n            # created for this connection, shove our CAs into it, tell ST to\n            # ignore everything else it knows, and then ask if it can build a\n            # chain. This is a buuuunch of code.\n            result = Security.SSLCopyPeerTrust(\n                self.context, ctypes.byref(trust)\n            )\n            _assert_no_error(result)\n            if not trust:\n                raise ssl.SSLError(\"Failed to copy trust reference\")\n\n            result = Security.SecTrustSetAnchorCertificates(trust, cert_array)\n            _assert_no_error(result)\n\n            result = Security.SecTrustSetAnchorCertificatesOnly(trust, True)\n            _assert_no_error(result)\n\n            trust_result = Security.SecTrustResultType()\n            result = Security.SecTrustEvaluate(\n                trust, ctypes.byref(trust_result)\n            )\n            _assert_no_error(result)\n        finally:\n            if trust:\n                CoreFoundation.CFRelease(trust)\n\n            if cert_array is not None:\n                CoreFoundation.CFRelease(cert_array)\n\n        # Ok, now we can look at what the result was.\n        successes = (\n            SecurityConst.kSecTrustResultUnspecified,\n            SecurityConst.kSecTrustResultProceed\n        )\n        if trust_result.value not in successes:\n            raise ssl.SSLError(\n                \"certificate verify failed, error code: %d\" %\n                trust_result.value\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndumping the bytecode into the file or file like object passed.", "response": "def write_bytecode(self, f):\n        \"\"\"Dump the bytecode into the file or file like object passed.\"\"\"\n        if self.code is None:\n            raise TypeError('can\\'t write empty bucket')\n        f.write(bc_magic)\n        pickle.dump(self.checksum, f, 2)\n        marshal_dump(self.code, f)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the unique hash key for this template name.", "response": "def get_cache_key(self, name, filename=None):\n        \"\"\"Returns the unique hash key for this template name.\"\"\"\n        hash = sha1(name.encode('utf-8'))\n        if filename is not None:\n            filename = '|' + filename\n            if isinstance(filename, text_type):\n                filename = filename.encode('utf-8')\n            hash.update(filename)\n        return hash.hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a cache bucket for the given template. All arguments are are mandatory.", "response": "def get_bucket(self, environment, name, filename, source):\n        \"\"\"Return a cache bucket for the given template.  All arguments are\n        mandatory but filename may be `None`.\n        \"\"\"\n        key = self.get_cache_key(name, filename)\n        checksum = self.get_source_checksum(source)\n        bucket = Bucket(environment, key, checksum)\n        self.load_bytecode(bucket)\n        return bucket"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef lookup(label):\n    # Only strip ASCII whitespace: U+0009, U+000A, U+000C, U+000D, and U+0020.\n    label = ascii_lower(label.strip('\\t\\n\\f\\r '))\n    name = LABELS.get(label)\n    if name is None:\n        return None\n    encoding = CACHE.get(name)\n    if encoding is None:\n        if name == 'x-user-defined':\n            from .x_user_defined import codec_info\n        else:\n            python_name = PYTHON_NAMES.get(name, name)\n            # Any python_name value that gets to here should be valid.\n            codec_info = codecs.lookup(python_name)\n        encoding = Encoding(name, codec_info)\n        CACHE[name] = encoding\n    return encoding", "response": "Look for an encoding by its label."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_encoding(encoding_or_label):\n    if hasattr(encoding_or_label, 'codec_info'):\n        return encoding_or_label\n\n    encoding = lookup(encoding_or_label)\n    if encoding is None:\n        raise LookupError('Unknown encoding label: %r' % encoding_or_label)\n    return encoding", "response": "Returns the encoding object for the given label."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndecoding a single string.", "response": "def decode(input, fallback_encoding, errors='replace'):\n    \"\"\"\n    Decode a single string.\n\n    :param input: A byte string\n    :param fallback_encoding:\n        An :class:`Encoding` object or a label string.\n        The encoding to use if :obj:`input` does note have a BOM.\n    :param errors: Type of error handling. See :func:`codecs.register`.\n    :raises: :exc:`~exceptions.LookupError` for an unknown encoding label.\n    :return:\n        A ``(output, encoding)`` tuple of an Unicode string\n        and an :obj:`Encoding`.\n\n    \"\"\"\n    # Fail early if `encoding` is an invalid label.\n    fallback_encoding = _get_encoding(fallback_encoding)\n    bom_encoding, input = _detect_bom(input)\n    encoding = bom_encoding or fallback_encoding\n    return encoding.codec_info.decode(input, errors)[0], encoding"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _detect_bom(input):\n    if input.startswith(b'\\xFF\\xFE'):\n        return _UTF16LE, input[2:]\n    if input.startswith(b'\\xFE\\xFF'):\n        return _UTF16BE, input[2:]\n    if input.startswith(b'\\xEF\\xBB\\xBF'):\n        return UTF8, input[3:]\n    return None, input", "response": "Detect the BOM in the input."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef encode(input, encoding=UTF8, errors='strict'):\n    return _get_encoding(encoding).codec_info.encode(input, errors)[0]", "response": "Encode a Unicode string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef iter_decode(input, fallback_encoding, errors='replace'):\n\n    decoder = IncrementalDecoder(fallback_encoding, errors)\n    generator = _iter_decode_generator(input, decoder)\n    encoding = next(generator)\n    return generator, encoding", "response": "Return an iterator over the input and the encoding that is being used."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _iter_decode_generator(input, decoder):\n    decode = decoder.decode\n    input = iter(input)\n    for chunck in input:\n        output = decode(chunck)\n        if output:\n            assert decoder.encoding is not None\n            yield decoder.encoding\n            yield output\n            break\n    else:\n        # Input exhausted without determining the encoding\n        output = decode(b'', final=True)\n        assert decoder.encoding is not None\n        yield decoder.encoding\n        if output:\n            yield output\n        return\n\n    for chunck in input:\n        output = decode(chunck)\n        if output:\n            yield output\n    output = decode(b'', final=True)\n    if output:\n        yield output", "response": "Return a generator that yields the encoding and output chukns as Unicode strings."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef iter_encode(input, encoding=UTF8, errors='strict'):\n    # Fail early if `encoding` is an invalid label.\n    encode = IncrementalEncoder(encoding, errors).encode\n    return _iter_encode_generator(input, encode)", "response": "Encode a sequence of Unicode strings."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndecode one chunk of input.", "response": "def decode(self, input, final=False):\n        \"\"\"Decode one chunk of the input.\n\n        :param input: A byte string.\n        :param final:\n            Indicate that no more input is available.\n            Must be :obj:`True` if this is the last call.\n        :returns: An Unicode string.\n\n        \"\"\"\n        decoder = self._decoder\n        if decoder is not None:\n            return decoder(input, final)\n\n        input = self._buffer + input\n        encoding, input = _detect_bom(input)\n        if encoding is None:\n            if len(input) < 3 and not final:  # Not enough data yet.\n                self._buffer = input\n                return ''\n            else:  # No BOM\n                encoding = self._fallback_encoding\n        decoder = encoding.codec_info.incrementaldecoder(self._errors).decode\n        self._decoder = decoder\n        self.encoding = encoding\n        return decoder(input, final)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving a list of Python tuples create an associated CFDictionary.", "response": "def _cf_dictionary_from_tuples(tuples):\n    \"\"\"\n    Given a list of Python tuples, create an associated CFDictionary.\n    \"\"\"\n    dictionary_size = len(tuples)\n\n    # We need to get the dictionary keys and values out in the same order.\n    keys = (t[0] for t in tuples)\n    values = (t[1] for t in tuples)\n    cf_keys = (CoreFoundation.CFTypeRef * dictionary_size)(*keys)\n    cf_values = (CoreFoundation.CFTypeRef * dictionary_size)(*values)\n\n    return CoreFoundation.CFDictionaryCreate(\n        CoreFoundation.kCFAllocatorDefault,\n        cf_keys,\n        cf_values,\n        dictionary_size,\n        CoreFoundation.kCFTypeDictionaryKeyCallBacks,\n        CoreFoundation.kCFTypeDictionaryValueCallBacks,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _cf_string_to_unicode(value):\n    value_as_void_p = ctypes.cast(value, ctypes.POINTER(ctypes.c_void_p))\n\n    string = CoreFoundation.CFStringGetCStringPtr(\n        value_as_void_p,\n        CFConst.kCFStringEncodingUTF8\n    )\n    if string is None:\n        buffer = ctypes.create_string_buffer(1024)\n        result = CoreFoundation.CFStringGetCString(\n            value_as_void_p,\n            buffer,\n            1024,\n            CFConst.kCFStringEncodingUTF8\n        )\n        if not result:\n            raise OSError('Error copying C string from CFStringRef')\n        string = buffer.value\n    if string is not None:\n        string = string.decode('utf-8')\n    return string", "response": "Converts a CFString object to a Unicode string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _assert_no_error(error, exception_class=None):\n    if error == 0:\n        return\n\n    cf_error_string = Security.SecCopyErrorMessageString(error, None)\n    output = _cf_string_to_unicode(cf_error_string)\n    CoreFoundation.CFRelease(cf_error_string)\n\n    if output is None or output == u'':\n        output = u'OSStatus %s' % error\n\n    if exception_class is None:\n        exception_class = ssl.SSLError\n\n    raise exception_class(output)", "response": "Checks the return code and raises an exception if there is an error to\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _cert_array_from_pem(pem_bundle):\n    # Normalize the PEM bundle's line endings.\n    pem_bundle = pem_bundle.replace(b\"\\r\\n\", b\"\\n\")\n\n    der_certs = [\n        base64.b64decode(match.group(1))\n        for match in _PEM_CERTS_RE.finditer(pem_bundle)\n    ]\n    if not der_certs:\n        raise ssl.SSLError(\"No root certificates specified\")\n\n    cert_array = CoreFoundation.CFArrayCreateMutable(\n        CoreFoundation.kCFAllocatorDefault,\n        0,\n        ctypes.byref(CoreFoundation.kCFTypeArrayCallBacks)\n    )\n    if not cert_array:\n        raise ssl.SSLError(\"Unable to allocate memory!\")\n\n    try:\n        for der_bytes in der_certs:\n            certdata = _cf_data_from_bytes(der_bytes)\n            if not certdata:\n                raise ssl.SSLError(\"Unable to allocate memory!\")\n            cert = Security.SecCertificateCreateWithData(\n                CoreFoundation.kCFAllocatorDefault, certdata\n            )\n            CoreFoundation.CFRelease(certdata)\n            if not cert:\n                raise ssl.SSLError(\"Unable to build cert object!\")\n\n            CoreFoundation.CFArrayAppendValue(cert_array, cert)\n            CoreFoundation.CFRelease(cert)\n    except Exception:\n        # We need to free the array before the exception bubbles further.\n        # We only want to do that if an error occurs: otherwise, the caller\n        # should free.\n        CoreFoundation.CFRelease(cert_array)\n\n    return cert_array", "response": "Given a PEM format returns a CFArray of certs that can be used to validate a cert chain."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a temporary Mac keychain that we can use to work with.", "response": "def _temporary_keychain():\n    \"\"\"\n    This function creates a temporary Mac keychain that we can use to work with\n    credentials. This keychain uses a one-time password and a temporary file to\n    store the data. We expect to have one keychain per socket. The returned\n    SecKeychainRef must be freed by the caller, including calling\n    SecKeychainDelete.\n\n    Returns a tuple of the SecKeychainRef and the path to the temporary\n    directory that contains it.\n    \"\"\"\n    # Unfortunately, SecKeychainCreate requires a path to a keychain. This\n    # means we cannot use mkstemp to use a generic temporary file. Instead,\n    # we're going to create a temporary directory and a filename to use there.\n    # This filename will be 8 random bytes expanded into base64. We also need\n    # some random bytes to password-protect the keychain we're creating, so we\n    # ask for 40 random bytes.\n    random_bytes = os.urandom(40)\n    filename = base64.b16encode(random_bytes[:8]).decode('utf-8')\n    password = base64.b16encode(random_bytes[8:])  # Must be valid UTF-8\n    tempdirectory = tempfile.mkdtemp()\n\n    keychain_path = os.path.join(tempdirectory, filename).encode('utf-8')\n\n    # We now want to create the keychain itself.\n    keychain = Security.SecKeychainRef()\n    status = Security.SecKeychainCreate(\n        keychain_path,\n        len(password),\n        password,\n        False,\n        None,\n        ctypes.byref(keychain)\n    )\n    _assert_no_error(status)\n\n    # Having created the keychain, we want to pass it off to the caller.\n    return keychain, tempdirectory"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _load_items_from_file(keychain, path):\n    certificates = []\n    identities = []\n    result_array = None\n\n    with open(path, 'rb') as f:\n        raw_filedata = f.read()\n\n    try:\n        filedata = CoreFoundation.CFDataCreate(\n            CoreFoundation.kCFAllocatorDefault,\n            raw_filedata,\n            len(raw_filedata)\n        )\n        result_array = CoreFoundation.CFArrayRef()\n        result = Security.SecItemImport(\n            filedata,  # cert data\n            None,  # Filename, leaving it out for now\n            None,  # What the type of the file is, we don't care\n            None,  # what's in the file, we don't care\n            0,  # import flags\n            None,  # key params, can include passphrase in the future\n            keychain,  # The keychain to insert into\n            ctypes.byref(result_array)  # Results\n        )\n        _assert_no_error(result)\n\n        # A CFArray is not very useful to us as an intermediary\n        # representation, so we are going to extract the objects we want\n        # and then free the array. We don't need to keep hold of keys: the\n        # keychain already has them!\n        result_count = CoreFoundation.CFArrayGetCount(result_array)\n        for index in range(result_count):\n            item = CoreFoundation.CFArrayGetValueAtIndex(\n                result_array, index\n            )\n            item = ctypes.cast(item, CoreFoundation.CFTypeRef)\n\n            if _is_cert(item):\n                CoreFoundation.CFRetain(item)\n                certificates.append(item)\n            elif _is_identity(item):\n                CoreFoundation.CFRetain(item)\n                identities.append(item)\n    finally:\n        if result_array:\n            CoreFoundation.CFRelease(result_array)\n\n        CoreFoundation.CFRelease(filedata)\n\n    return (identities, certificates)", "response": "Loads all the trust objects from a single file into arrays and returns a tuple of lists."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _load_client_cert_chain(keychain, *paths):\n    # Ok, the strategy.\n    #\n    # This relies on knowing that macOS will not give you a SecIdentityRef\n    # unless you have imported a key into a keychain. This is a somewhat\n    # artificial limitation of macOS (for example, it doesn't necessarily\n    # affect iOS), but there is nothing inside Security.framework that lets you\n    # get a SecIdentityRef without having a key in a keychain.\n    #\n    # So the policy here is we take all the files and iterate them in order.\n    # Each one will use SecItemImport to have one or more objects loaded from\n    # it. We will also point at a keychain that macOS can use to work with the\n    # private key.\n    #\n    # Once we have all the objects, we'll check what we actually have. If we\n    # already have a SecIdentityRef in hand, fab: we'll use that. Otherwise,\n    # we'll take the first certificate (which we assume to be our leaf) and\n    # ask the keychain to give us a SecIdentityRef with that cert's associated\n    # key.\n    #\n    # We'll then return a CFArray containing the trust chain: one\n    # SecIdentityRef and then zero-or-more SecCertificateRef objects. The\n    # responsibility for freeing this CFArray will be with the caller. This\n    # CFArray must remain alive for the entire connection, so in practice it\n    # will be stored with a single SSLSocket, along with the reference to the\n    # keychain.\n    certificates = []\n    identities = []\n\n    # Filter out bad paths.\n    paths = (path for path in paths if path)\n\n    try:\n        for file_path in paths:\n            new_identities, new_certs = _load_items_from_file(\n                keychain, file_path\n            )\n            identities.extend(new_identities)\n            certificates.extend(new_certs)\n\n        # Ok, we have everything. The question is: do we have an identity? If\n        # not, we want to grab one from the first cert we have.\n        if not identities:\n            new_identity = Security.SecIdentityRef()\n            status = Security.SecIdentityCreateWithCertificate(\n                keychain,\n                certificates[0],\n                ctypes.byref(new_identity)\n            )\n            _assert_no_error(status)\n            identities.append(new_identity)\n\n            # We now want to release the original certificate, as we no longer\n            # need it.\n            CoreFoundation.CFRelease(certificates.pop(0))\n\n        # We now need to build a new CFArray that holds the trust chain.\n        trust_chain = CoreFoundation.CFArrayCreateMutable(\n            CoreFoundation.kCFAllocatorDefault,\n            0,\n            ctypes.byref(CoreFoundation.kCFTypeArrayCallBacks),\n        )\n        for item in itertools.chain(identities, certificates):\n            # ArrayAppendValue does a CFRetain on the item. That's fine,\n            # because the finally block will release our other refs to them.\n            CoreFoundation.CFArrayAppendValue(trust_chain, item)\n\n        return trust_chain\n    finally:\n        for obj in itertools.chain(identities, certificates):\n            CoreFoundation.CFRelease(obj)", "response": "Load certificates and maybe keys from a number of files."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn Truthy redirect location string if we got a redirect status code and valid location.", "response": "def get_redirect_location(self):\n        \"\"\"\n        Should we redirect and where to?\n\n        :returns: Truthy redirect location string if we got a redirect status\n            code and valid location. ``None`` if redirect status and no\n            location. ``False`` if not a redirect status code.\n        \"\"\"\n        if self.status in self.REDIRECT_STATUSES:\n            return self.headers.get('location')\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninitializes the length value for the response content if available.", "response": "def _init_length(self, request_method):\n        \"\"\"\n        Set initial length value for Response content if available.\n        \"\"\"\n        length = self.headers.get('content-length')\n\n        if length is not None:\n            if self.chunked:\n                # This Response will fail with an IncompleteRead if it can't be\n                # received as chunked. This method falls back to attempt reading\n                # the response before raising an exception.\n                log.warning(\"Received response with both Content-Length and \"\n                            \"Transfer-Encoding set. This is expressly forbidden \"\n                            \"by RFC 7230 sec 3.3.2. Ignoring Content-Length and \"\n                            \"attempting to process response as Transfer-Encoding: \"\n                            \"chunked.\")\n                return None\n\n            try:\n                # RFC 7230 section 3.3.2 specifies multiple content lengths can\n                # be sent in a single Content-Length header\n                # (e.g. Content-Length: 42, 42). This line ensures the values\n                # are all valid ints and that as long as the `set` length is 1,\n                # all values are the same. Otherwise, the header is invalid.\n                lengths = set([int(val) for val in length.split(',')])\n                if len(lengths) > 1:\n                    raise InvalidHeader(\"Content-Length contained multiple \"\n                                        \"unmatching values (%s)\" % length)\n                length = lengths.pop()\n            except ValueError:\n                length = None\n            else:\n                if length < 0:\n                    length = None\n\n        # Convert status to int for comparison\n        # In some cases, httplib returns a status of \"_UNKNOWN\"\n        try:\n            status = int(self.status)\n        except ValueError:\n            status = 0\n\n        # Check for responses that shouldn't include a body\n        if status in (204, 304) or 100 <= status < 200 or request_method == 'HEAD':\n            length = 0\n\n        return length"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _init_decoder(self):\n        # Note: content-encoding value should be case-insensitive, per RFC 7230\n        # Section 3.2\n        content_encoding = self.headers.get('content-encoding', '').lower()\n        if self._decoder is None:\n            if content_encoding in self.CONTENT_DECODERS:\n                self._decoder = _get_decoder(content_encoding)\n            elif ',' in content_encoding:\n                encodings = [e.strip() for e in content_encoding.split(',') if e.strip() in self.CONTENT_DECODERS]\n                if len(encodings):\n                    self._decoder = _get_decoder(content_encoding)", "response": "Initialize the decoder attribute if necessary."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nflush the decoder. Should only be called if the decoder is actually being used.", "response": "def _flush_decoder(self):\n        \"\"\"\n        Flushes the decoder. Should only be called if the decoder is actually\n        being used.\n        \"\"\"\n        if self._decoder:\n            buf = self._decoder.decompress(b'')\n            return buf + self._decoder.flush()\n\n        return b''"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncatching exceptions and return the appropriate exception.", "response": "def _error_catcher(self):\n        \"\"\"\n        Catch low-level python exceptions, instead re-raising urllib3\n        variants, so that low-level exceptions are not leaked in the\n        high-level api.\n\n        On exit, release the connection back to the pool.\n        \"\"\"\n        clean_exit = False\n\n        try:\n            try:\n                yield\n\n            except SocketTimeout:\n                # FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\n                # there is yet no clean way to get at it from this context.\n                raise ReadTimeoutError(self._pool, None, 'Read timed out.')\n\n            except BaseSSLError as e:\n                # FIXME: Is there a better way to differentiate between SSLErrors?\n                if 'read operation timed out' not in str(e):  # Defensive:\n                    # This shouldn't happen but just in case we're missing an edge\n                    # case, let's avoid swallowing SSL errors.\n                    raise\n\n                raise ReadTimeoutError(self._pool, None, 'Read timed out.')\n\n            except (HTTPException, SocketError) as e:\n                # This includes IncompleteRead.\n                raise ProtocolError('Connection broken: %r' % e, e)\n\n            # If no exception is thrown, we should avoid cleaning up\n            # unnecessarily.\n            clean_exit = True\n        finally:\n            # If we didn't terminate cleanly, we need to throw away our\n            # connection.\n            if not clean_exit:\n                # The response may not be closed but we're not going to use it\n                # anymore so close it now to ensure that the connection is\n                # released back to the pool.\n                if self._original_response:\n                    self._original_response.close()\n\n                # Closing the response may not actually be sufficient to close\n                # everything, so if we have a hold of the connection close that\n                # too.\n                if self._connection:\n                    self._connection.close()\n\n            # If we hold the original response but it's closed now, we should\n            # return the connection back to the pool.\n            if self._original_response and self._original_response.isclosed():\n                self.release_conn()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads the content of the response and return it as a dict.", "response": "def read(self, amt=None, decode_content=None, cache_content=False):\n        \"\"\"\n        Similar to :meth:`httplib.HTTPResponse.read`, but with two additional\n        parameters: ``decode_content`` and ``cache_content``.\n\n        :param amt:\n            How much of the content to read. If specified, caching is skipped\n            because it doesn't make sense to cache partial content as the full\n            response.\n\n        :param decode_content:\n            If True, will attempt to decode the body based on the\n            'content-encoding' header.\n\n        :param cache_content:\n            If True, will save the returned data such that the same result is\n            returned despite of the state of the underlying file object. This\n            is useful if you want the ``.data`` property to continue working\n            after having ``.read()`` the file object. (Overridden if ``amt`` is\n            set.)\n        \"\"\"\n        self._init_decoder()\n        if decode_content is None:\n            decode_content = self.decode_content\n\n        if self._fp is None:\n            return\n\n        flush_decoder = False\n        data = None\n\n        with self._error_catcher():\n            if amt is None:\n                # cStringIO doesn't like amt=None\n                data = self._fp.read()\n                flush_decoder = True\n            else:\n                cache_content = False\n                data = self._fp.read(amt)\n                if amt != 0 and not data:  # Platform-specific: Buggy versions of Python.\n                    # Close the connection when no data is returned\n                    #\n                    # This is redundant to what httplib/http.client _should_\n                    # already do.  However, versions of python released before\n                    # December 15, 2012 (http://bugs.python.org/issue16298) do\n                    # not properly close the connection in all cases. There is\n                    # no harm in redundantly calling close.\n                    self._fp.close()\n                    flush_decoder = True\n                    if self.enforce_content_length and self.length_remaining not in (0, None):\n                        # This is an edge case that httplib failed to cover due\n                        # to concerns of backward compatibility. We're\n                        # addressing it here to make sure IncompleteRead is\n                        # raised during streaming, so all calls with incorrect\n                        # Content-Length are caught.\n                        raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n\n        if data:\n            self._fp_bytes_read += len(data)\n            if self.length_remaining is not None:\n                self.length_remaining -= len(data)\n\n            data = self._decode(data, decode_content, flush_decoder)\n\n            if cache_content:\n                self._body = data\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngives an : class : httplib. HTTPResponse instance r return a new instance of ResponseCls with the contents of r.", "response": "def from_httplib(ResponseCls, r, **response_kw):\n        \"\"\"\n        Given an :class:`httplib.HTTPResponse` instance ``r``, return a\n        corresponding :class:`urllib3.response.HTTPResponse` object.\n\n        Remaining parameters are passed to the HTTPResponse constructor, along\n        with ``original_response=r``.\n        \"\"\"\n        headers = r.msg\n\n        if not isinstance(headers, HTTPHeaderDict):\n            if PY3:  # Python 3\n                headers = HTTPHeaderDict(headers.items())\n            else:  # Python 2\n                headers = HTTPHeaderDict.from_httplib(headers)\n\n        # HTTPResponse objects in Python 3 don't have a .strict attribute\n        strict = getattr(r, 'strict', 0)\n        resp = ResponseCls(body=r,\n                           headers=headers,\n                           status=r.status,\n                           version=r.version,\n                           reason=r.reason,\n                           strict=strict,\n                           original_response=r,\n                           **response_kw)\n        return resp"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read_chunked(self, amt=None, decode_content=None):\n        self._init_decoder()\n        # FIXME: Rewrite this method and make it a class with a better structured logic.\n        if not self.chunked:\n            raise ResponseNotChunked(\n                \"Response is not chunked. \"\n                \"Header 'transfer-encoding: chunked' is missing.\")\n        if not self.supports_chunked_reads():\n            raise BodyNotHttplibCompatible(\n                \"Body should be httplib.HTTPResponse like. \"\n                \"It should have have an fp attribute which returns raw chunks.\")\n\n        with self._error_catcher():\n            # Don't bother reading the body of a HEAD request.\n            if self._original_response and is_response_to_head(self._original_response):\n                self._original_response.close()\n                return\n\n            # If a response is already read and closed\n            # then return immediately.\n            if self._fp.fp is None:\n                return\n\n            while True:\n                self._update_chunk_length()\n                if self.chunk_left == 0:\n                    break\n                chunk = self._handle_chunk(amt)\n                decoded = self._decode(chunk, decode_content=decode_content,\n                                       flush_decoder=False)\n                if decoded:\n                    yield decoded\n\n            if decode_content:\n                # On CPython and PyPy, we should never need to flush the\n                # decoder. However, on Jython we *might* need to, so\n                # lets defensively do it anyway.\n                decoded = self._flush_decoder()\n                if decoded:  # Platform-specific: Jython.\n                    yield decoded\n\n            # Chunk content ends with \\r\\n: discard it.\n            while True:\n                line = self._fp.fp.readline()\n                if not line:\n                    # Some sites may not end with '\\r\\n'.\n                    break\n                if line == b'\\r\\n':\n                    break\n\n            # We read everything; close the \"file\".\n            if self._original_response:\n                self._original_response.close()", "response": "A method that returns a list of bytes."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef geturl(self):\n        if self.retries is not None and len(self.retries.history):\n            return self.retries.history[-1].redirect_location\n        else:\n            return self._request_url", "response": "Returns the URL that was generated by this response."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets ok finalizer to a spinner.", "response": "def ok(self, text=u\"OK\", err=False):\n        \"\"\"Set Ok (success) finalizer to a spinner.\"\"\"\n        # Do not display spin text for ok state\n        self._text = None\n\n        _text = to_text(text) if text else u\"OK\"\n        err = err or not self.write_to_stdout\n        self._freeze(_text, err=err)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting fail finalizer to a spinner.", "response": "def fail(self, text=u\"FAIL\", err=False):\n        \"\"\"Set fail finalizer to a spinner.\"\"\"\n        # Do not display spin text for fail state\n        self._text = None\n\n        _text = text if text else u\"FAIL\"\n        err = err or not self.write_to_stdout\n        self._freeze(_text, err=err)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting error text in the terminal without breaking the spinner.", "response": "def write_err(self, text):\n        \"\"\"Write error text in the terminal without breaking the spinner.\"\"\"\n        stderr = self.stderr\n        if self.stderr.closed:\n            stderr = sys.stderr\n        stderr.write(decode_output(u\"\\r\", target_stream=stderr))\n        stderr.write(decode_output(CLEAR_LINE, target_stream=stderr))\n        if text is None:\n            text = \"\"\n        text = decode_output(u\"{0}\\n\".format(text), target_stream=stderr)\n        self.stderr.write(text)\n        self.out_buff.write(decode_output(text, target_stream=self.out_buff))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nstopping spinner compose last frame and freeze it.", "response": "def _freeze(self, final_text, err=False):\n        \"\"\"Stop spinner, compose last frame and 'freeze' it.\"\"\"\n        if not final_text:\n            final_text = \"\"\n        target = self.stderr if err else self.stdout\n        if target.closed:\n            target = sys.stderr if err else sys.stdout\n        text = to_text(final_text)\n        last_frame = self._compose_out(text, mode=\"last\")\n        self._last_frame = decode_output(last_frame, target_stream=target)\n\n        # Should be stopped here, otherwise prints after\n        # self._freeze call will mess up the spinner\n        self.stop()\n        target.write(self._last_frame)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef describe_token_expr(expr):\n    if ':' in expr:\n        type, value = expr.split(':', 1)\n        if type == 'name':\n            return value\n    else:\n        type = expr\n    return _describe_token_type(type)", "response": "Like describe_token but for token expressions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a lexer which is probably cached.", "response": "def get_lexer(environment):\n    \"\"\"Return a lexer which is probably cached.\"\"\"\n    key = (environment.block_start_string,\n           environment.block_end_string,\n           environment.variable_start_string,\n           environment.variable_end_string,\n           environment.comment_start_string,\n           environment.comment_end_string,\n           environment.line_statement_prefix,\n           environment.line_comment_prefix,\n           environment.trim_blocks,\n           environment.lstrip_blocks,\n           environment.newline_sequence,\n           environment.keep_trailing_newline)\n    lexer = _lexer_cache.get(key)\n    if lexer is None:\n        lexer = Lexer(environment)\n        _lexer_cache[key] = lexer\n    return lexer"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncall tokeniter + tokenize and wraps it in a TokenStream", "response": "def tokenize(self, source, name=None, filename=None, state=None):\n        \"\"\"Calls tokeniter + tokenize and wraps it in a token stream.\n        \"\"\"\n        stream = self.tokeniter(source, name, filename, state)\n        return TokenStream(self.wrap(stream, name, filename), name, filename)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef wrap(self, stream, name=None, filename=None):\n        for lineno, token, value in stream:\n            if token in ignored_tokens:\n                continue\n            elif token == 'linestatement_begin':\n                token = 'block_begin'\n            elif token == 'linestatement_end':\n                token = 'block_end'\n            # we are not interested in those tokens in the parser\n            elif token in ('raw_begin', 'raw_end'):\n                continue\n            elif token == 'data':\n                value = self._normalize_newlines(value)\n            elif token == 'keyword':\n                token = value\n            elif token == 'name':\n                value = str(value)\n                if check_ident and not value.isidentifier():\n                    raise TemplateSyntaxError(\n                        'Invalid character in identifier',\n                        lineno, name, filename)\n            elif token == 'string':\n                # try to unescape string\n                try:\n                    value = self._normalize_newlines(value[1:-1]) \\\n                        .encode('ascii', 'backslashreplace') \\\n                        .decode('unicode-escape')\n                except Exception as e:\n                    msg = str(e).split(':')[-1].strip()\n                    raise TemplateSyntaxError(msg, lineno, name, filename)\n            elif token == 'integer':\n                value = int(value)\n            elif token == 'float':\n                value = float(value)\n            elif token == 'operator':\n                token = operators[value]\n            yield Token(lineno, token, value)", "response": "This is called by tokenize and parses the tokens in the given stream and yields a Token object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload the PEP 517 - specific Toml file and return a tuple of the names of the build and requirements.", "response": "def load_pyproject_toml(\n    use_pep517,  # type: Optional[bool]\n    pyproject_toml,  # type: str\n    setup_py,  # type: str\n    req_name  # type: str\n):\n    # type: (...) -> Optional[Tuple[List[str], str, List[str]]]\n    \"\"\"Load the pyproject.toml file.\n\n    Parameters:\n        use_pep517 - Has the user requested PEP 517 processing? None\n                     means the user hasn't explicitly specified.\n        pyproject_toml - Location of the project's pyproject.toml file\n        setup_py - Location of the project's setup.py file\n        req_name - The name of the requirement we're processing (for\n                   error reporting)\n\n    Returns:\n        None if we should use the legacy code path, otherwise a tuple\n        (\n            requirements from pyproject.toml,\n            name of PEP 517 backend,\n            requirements we should check are installed after setting\n                up the build environment\n        )\n    \"\"\"\n    has_pyproject = os.path.isfile(pyproject_toml)\n    has_setup = os.path.isfile(setup_py)\n\n    if has_pyproject:\n        with io.open(pyproject_toml, encoding=\"utf-8\") as f:\n            pp_toml = pytoml.load(f)\n        build_system = pp_toml.get(\"build-system\")\n    else:\n        build_system = None\n\n    # The following cases must use PEP 517\n    # We check for use_pep517 being non-None and falsey because that means\n    # the user explicitly requested --no-use-pep517.  The value 0 as\n    # opposed to False can occur when the value is provided via an\n    # environment variable or config file option (due to the quirk of\n    # strtobool() returning an integer in pip's configuration code).\n    if has_pyproject and not has_setup:\n        if use_pep517 is not None and not use_pep517:\n            raise InstallationError(\n                \"Disabling PEP 517 processing is invalid: \"\n                \"project does not have a setup.py\"\n            )\n        use_pep517 = True\n    elif build_system and \"build-backend\" in build_system:\n        if use_pep517 is not None and not use_pep517:\n            raise InstallationError(\n                \"Disabling PEP 517 processing is invalid: \"\n                \"project specifies a build backend of {} \"\n                \"in pyproject.toml\".format(\n                    build_system[\"build-backend\"]\n                )\n            )\n        use_pep517 = True\n\n    # If we haven't worked out whether to use PEP 517 yet,\n    # and the user hasn't explicitly stated a preference,\n    # we do so if the project has a pyproject.toml file.\n    elif use_pep517 is None:\n        use_pep517 = has_pyproject\n\n    # At this point, we know whether we're going to use PEP 517.\n    assert use_pep517 is not None\n\n    # If we're using the legacy code path, there is nothing further\n    # for us to do here.\n    if not use_pep517:\n        return None\n\n    if build_system is None:\n        # Either the user has a pyproject.toml with no build-system\n        # section, or the user has no pyproject.toml, but has opted in\n        # explicitly via --use-pep517.\n        # In the absence of any explicit backend specification, we\n        # assume the setuptools backend that most closely emulates the\n        # traditional direct setup.py execution, and require wheel and\n        # a version of setuptools that supports that backend.\n\n        build_system = {\n            \"requires\": [\"setuptools>=40.8.0\", \"wheel\"],\n            \"build-backend\": \"setuptools.build_meta:__legacy__\",\n        }\n\n    # If we're using PEP 517, we have build system information (either\n    # from pyproject.toml, or defaulted by the code above).\n    # Note that at this point, we do not know if the user has actually\n    # specified a backend, though.\n    assert build_system is not None\n\n    # Ensure that the build-system section in pyproject.toml conforms\n    # to PEP 518.\n    error_template = (\n        \"{package} has a pyproject.toml file that does not comply \"\n        \"with PEP 518: {reason}\"\n    )\n\n    # Specifying the build-system table but not the requires key is invalid\n    if \"requires\" not in build_system:\n        raise InstallationError(\n            error_template.format(package=req_name, reason=(\n                \"it has a 'build-system' table but not \"\n                \"'build-system.requires' which is mandatory in the table\"\n            ))\n        )\n\n    # Error out if requires is not a list of strings\n    requires = build_system[\"requires\"]\n    if not _is_list_of_str(requires):\n        raise InstallationError(error_template.format(\n            package=req_name,\n            reason=\"'build-system.requires' is not a list of strings.\",\n        ))\n\n    backend = build_system.get(\"build-backend\")\n    check = []  # type: List[str]\n    if backend is None:\n        # If the user didn't specify a backend, we assume they want to use\n        # the setuptools backend. But we can't be sure they have included\n        # a version of setuptools which supplies the backend, or wheel\n        # (which is needed by the backend) in their requirements. So we\n        # make a note to check that those requirements are present once\n        # we have set up the environment.\n        # This is quite a lot of work to check for a very specific case. But\n        # the problem is, that case is potentially quite common - projects that\n        # adopted PEP 518 early for the ability to specify requirements to\n        # execute setup.py, but never considered needing to mention the build\n        # tools themselves. The original PEP 518 code had a similar check (but\n        # implemented in a different way).\n        backend = \"setuptools.build_meta:__legacy__\"\n        check = [\"setuptools>=40.8.0\", \"wheel\"]\n\n    return (requires, backend, check)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\niterates through CPython versions available for Pipenv to install.", "response": "def iter_installable_versions(self):\n        \"\"\"Iterate through CPython versions available for Pipenv to install.\n        \"\"\"\n        for name in self._pyenv('install', '--list').out.splitlines():\n            try:\n                version = Version.parse(name.strip())\n            except ValueError:\n                continue\n            yield version"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding a version in pyenv from the version supplied.", "response": "def find_version_to_install(self, name):\n        \"\"\"Find a version in pyenv from the version supplied.\n\n        A ValueError is raised if a matching version cannot be found.\n        \"\"\"\n        version = Version.parse(name)\n        if version.patch is not None:\n            return name\n        try:\n            best_match = max((\n                inst_version\n                for inst_version in self.iter_installable_versions()\n                if inst_version.matches_minor(version)\n            ), key=operator.attrgetter('cmpkey'))\n        except ValueError:\n            raise ValueError(\n                'no installable version found for {0!r}'.format(name),\n            )\n        return best_match"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninstalling the given version with pyenv.", "response": "def install(self, version):\n        \"\"\"Install the given version with pyenv.\n\n        The version must be a ``Version`` instance representing a version\n        found in pyenv.\n\n        A ValueError is raised if the given version does not have a match in\n        pyenv. A PyenvError is raised if the pyenv command fails.\n        \"\"\"\n        c = self._pyenv(\n            'install', '-s', str(version),\n            timeout=PIPENV_INSTALL_TIMEOUT,\n        )\n        return c"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse an editable requirement into a tuple of package name url extras options and set of editable options.", "response": "def parse_editable(editable_req):\n    # type: (str) -> Tuple[Optional[str], str, Optional[Set[str]]]\n    \"\"\"Parses an editable requirement into:\n        - a requirement name\n        - an URL\n        - extras\n        - editable options\n    Accepted requirements:\n        svn+http://blahblah@rev#egg=Foobar[baz]&subdirectory=version_subdir\n        .[some_extra]\n    \"\"\"\n\n    url = editable_req\n\n    # If a file path is specified with extras, strip off the extras.\n    url_no_extras, extras = _strip_extras(url)\n\n    if os.path.isdir(url_no_extras):\n        if not os.path.exists(os.path.join(url_no_extras, 'setup.py')):\n            msg = (\n                'File \"setup.py\" not found. Directory cannot be installed '\n                'in editable mode: {}'.format(os.path.abspath(url_no_extras))\n            )\n            pyproject_path = make_pyproject_path(url_no_extras)\n            if os.path.isfile(pyproject_path):\n                msg += (\n                    '\\n(A \"pyproject.toml\" file was found, but editable '\n                    'mode currently requires a setup.py based build.)'\n                )\n            raise InstallationError(msg)\n\n        # Treating it as code that has already been checked out\n        url_no_extras = path_to_url(url_no_extras)\n\n    if url_no_extras.lower().startswith('file:'):\n        package_name = Link(url_no_extras).egg_fragment\n        if extras:\n            return (\n                package_name,\n                url_no_extras,\n                Requirement(\"placeholder\" + extras.lower()).extras,\n            )\n        else:\n            return package_name, url_no_extras, None\n\n    for version_control in vcs:\n        if url.lower().startswith('%s:' % version_control):\n            url = '%s+%s' % (version_control, url)\n            break\n\n    if '+' not in url:\n        raise InstallationError(\n            '%s should either be a path to a local project or a VCS url '\n            'beginning with svn+, git+, hg+, or bzr+' %\n            editable_req\n        )\n\n    vc_type = url.split('+', 1)[0].lower()\n\n    if not vcs.get_backend(vc_type):\n        error_message = 'For --editable=%s only ' % editable_req + \\\n            ', '.join([backend.name + '+URL' for backend in vcs.backends]) + \\\n            ' is currently supported'\n        raise InstallationError(error_message)\n\n    package_name = Link(url).egg_fragment\n    if not package_name:\n        raise InstallationError(\n            \"Could not detect requirement name for '%s', please specify one \"\n            \"with #egg=your_package_name\" % editable_req\n        )\n    return package_name, url, None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef deduce_helpful_msg(req):\n    # type: (str) -> str\n    \"\"\"Returns helpful msg in case requirements file does not exist,\n    or cannot be parsed.\n\n    :params req: Requirements file path\n    \"\"\"\n    msg = \"\"\n    if os.path.exists(req):\n        msg = \" It does exist.\"\n        # Try to parse and check if it is a requirements file.\n        try:\n            with open(req, 'r') as fp:\n                # parse first line only\n                next(parse_requirements(fp.read()))\n                msg += \" The argument you provided \" + \\\n                    \"(%s) appears to be a\" % (req) + \\\n                    \" requirements file. If that is the\" + \\\n                    \" case, use the '-r' flag to install\" + \\\n                    \" the packages specified within it.\"\n        except RequirementParseError:\n            logger.debug(\"Cannot parse '%s' as requirements \\\n            file\" % (req), exc_info=True)\n    else:\n        msg += \" File '%s' does not exist.\" % (req)\n    return msg", "response": "Returns helpful msg in case requirements file does not exist or cannot be parsed."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef install_req_from_line(\n    name,  # type: str\n    comes_from=None,  # type: Optional[Union[str, InstallRequirement]]\n    use_pep517=None,  # type: Optional[bool]\n    isolated=False,  # type: bool\n    options=None,  # type: Optional[Dict[str, Any]]\n    wheel_cache=None,  # type: Optional[WheelCache]\n    constraint=False  # type: bool\n):\n    # type: (...) -> InstallRequirement\n    \"\"\"Creates an InstallRequirement from a name, which might be a\n    requirement, directory containing 'setup.py', filename, or URL.\n    \"\"\"\n    if is_url(name):\n        marker_sep = '; '\n    else:\n        marker_sep = ';'\n    if marker_sep in name:\n        name, markers_as_string = name.split(marker_sep, 1)\n        markers_as_string = markers_as_string.strip()\n        if not markers_as_string:\n            markers = None\n        else:\n            markers = Marker(markers_as_string)\n    else:\n        markers = None\n    name = name.strip()\n    req_as_string = None\n    path = os.path.normpath(os.path.abspath(name))\n    link = None\n    extras_as_string = None\n\n    if is_url(name):\n        link = Link(name)\n    else:\n        p, extras_as_string = _strip_extras(path)\n        looks_like_dir = os.path.isdir(p) and (\n            os.path.sep in name or\n            (os.path.altsep is not None and os.path.altsep in name) or\n            name.startswith('.')\n        )\n        if looks_like_dir:\n            if not is_installable_dir(p):\n                raise InstallationError(\n                    \"Directory %r is not installable. Neither 'setup.py' \"\n                    \"nor 'pyproject.toml' found.\" % name\n                )\n            link = Link(path_to_url(p))\n        elif is_archive_file(p):\n            if not os.path.isfile(p):\n                logger.warning(\n                    'Requirement %r looks like a filename, but the '\n                    'file does not exist',\n                    name\n                )\n            link = Link(path_to_url(p))\n\n    # it's a local file, dir, or url\n    if link:\n        # Handle relative file URLs\n        if link.scheme == 'file' and re.search(r'\\.\\./', link.url):\n            link = Link(\n                path_to_url(os.path.normpath(os.path.abspath(link.path))))\n        # wheel file\n        if link.is_wheel:\n            wheel = Wheel(link.filename)  # can raise InvalidWheelFilename\n            req_as_string = \"%s==%s\" % (wheel.name, wheel.version)\n        else:\n            # set the req to the egg fragment.  when it's not there, this\n            # will become an 'unnamed' requirement\n            req_as_string = link.egg_fragment\n\n    # a requirement specifier\n    else:\n        req_as_string = name\n\n    if extras_as_string:\n        extras = Requirement(\"placeholder\" + extras_as_string.lower()).extras\n    else:\n        extras = ()\n    if req_as_string is not None:\n        try:\n            req = Requirement(req_as_string)\n        except InvalidRequirement:\n            if os.path.sep in req_as_string:\n                add_msg = \"It looks like a path.\"\n                add_msg += deduce_helpful_msg(req_as_string)\n            elif ('=' in req_as_string and\n                  not any(op in req_as_string for op in operators)):\n                add_msg = \"= is not a valid operator. Did you mean == ?\"\n            else:\n                add_msg = \"\"\n            raise InstallationError(\n                \"Invalid requirement: '%s'\\n%s\" % (req_as_string, add_msg)\n            )\n    else:\n        req = None\n\n    return InstallRequirement(\n        req, comes_from, link=link, markers=markers,\n        use_pep517=use_pep517, isolated=isolated,\n        options=options if options else {},\n        wheel_cache=wheel_cache,\n        constraint=constraint,\n        extras=extras,\n    )", "response": "Creates an InstallRequirement object from a name which might be a directory containing setup. py filename or URL."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the file path based on the URL.", "response": "def url_to_file_path(url, filecache):\n    \"\"\"Return the file cache path based on the URL.\n\n    This does not ensure the file exists!\n    \"\"\"\n    key = CacheController.cache_url(url)\n    return filecache._fn(key)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write_pid_to_pidfile(pidfile_path):\n    open_flags = (os.O_CREAT | os.O_EXCL | os.O_WRONLY)\n    open_mode = 0o644\n    pidfile_fd = os.open(pidfile_path, open_flags, open_mode)\n    pidfile = os.fdopen(pidfile_fd, 'w')\n\n    # According to the FHS 2.3 section on PID files in /var/run:\n    #\n    #   The file must consist of the process identifier in\n    #   ASCII-encoded decimal, followed by a newline character. For\n    #   example, if crond was process number 25, /var/run/crond.pid\n    #   would contain three characters: two, five, and newline.\n\n    pid = os.getpid()\n    pidfile.write(\"%s\\n\" % pid)\n    pidfile.close()", "response": "Write the PID in the named PID file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving the named PID file from the hierarchy.", "response": "def remove_existing_pidfile(pidfile_path):\n    \"\"\" Remove the named PID file if it exists.\n\n        Removing a PID file that doesn't already exist puts us in the\n        desired state, so we ignore the condition if the file does not\n        exist.\n\n        \"\"\"\n    try:\n        os.remove(pidfile_path)\n    except OSError as exc:\n        if exc.errno == errno.ENOENT:\n            pass\n        else:\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreleasing the lock. Removes the PID file to release the lock, or raises an error if the current process does not hold the lock.", "response": "def release(self):\n        \"\"\" Release the lock.\n\n            Removes the PID file to release the lock, or raises an\n            error if the current process does not hold the lock.\n\n            \"\"\"\n        if not self.is_locked():\n            raise NotLocked(\"%s is not locked\" % self.path)\n        if not self.i_am_locking():\n            raise NotMyLock(\"%s is locked, but not by me\" % self.path)\n        remove_existing_pidfile(self.path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef export(self, location):\n        # Remove the location to make sure Bazaar can export it correctly\n        if os.path.exists(location):\n            rmtree(location)\n\n        with TempDirectory(kind=\"export\") as temp_dir:\n            self.unpack(temp_dir.path)\n\n            self.run_command(\n                ['export', location],\n                cwd=temp_dir.path, show_stdout=False,\n            )", "response": "Export the Bazaar repository at the url to the destination location"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef search_packages_info(query):\n    installed = {}\n    for p in pkg_resources.working_set:\n        installed[canonicalize_name(p.project_name)] = p\n\n    query_names = [canonicalize_name(name) for name in query]\n\n    for dist in [installed[pkg] for pkg in query_names if pkg in installed]:\n        package = {\n            'name': dist.project_name,\n            'version': dist.version,\n            'location': dist.location,\n            'requires': [dep.project_name for dep in dist.requires()],\n        }\n        file_list = None\n        metadata = None\n        if isinstance(dist, pkg_resources.DistInfoDistribution):\n            # RECORDs should be part of .dist-info metadatas\n            if dist.has_metadata('RECORD'):\n                lines = dist.get_metadata_lines('RECORD')\n                paths = [l.split(',')[0] for l in lines]\n                paths = [os.path.join(dist.location, p) for p in paths]\n                file_list = [os.path.relpath(p, dist.location) for p in paths]\n\n            if dist.has_metadata('METADATA'):\n                metadata = dist.get_metadata('METADATA')\n        else:\n            # Otherwise use pip's log for .egg-info's\n            if dist.has_metadata('installed-files.txt'):\n                paths = dist.get_metadata_lines('installed-files.txt')\n                paths = [os.path.join(dist.egg_info, p) for p in paths]\n                file_list = [os.path.relpath(p, dist.location) for p in paths]\n\n            if dist.has_metadata('PKG-INFO'):\n                metadata = dist.get_metadata('PKG-INFO')\n\n        if dist.has_metadata('entry_points.txt'):\n            entry_points = dist.get_metadata_lines('entry_points.txt')\n            package['entry_points'] = entry_points\n\n        if dist.has_metadata('INSTALLER'):\n            for line in dist.get_metadata_lines('INSTALLER'):\n                if line.strip():\n                    package['installer'] = line.strip()\n                    break\n\n        # @todo: Should pkg_resources.Distribution have a\n        # `get_pkg_info` method?\n        feed_parser = FeedParser()\n        feed_parser.feed(metadata)\n        pkg_info_dict = feed_parser.close()\n        for key in ('metadata-version', 'summary',\n                    'home-page', 'author', 'author-email', 'license'):\n            package[key] = pkg_info_dict.get(key)\n\n        # It looks like FeedParser cannot deal with repeated headers\n        classifiers = []\n        for line in metadata.splitlines():\n            if line.startswith('Classifier: '):\n                classifiers.append(line[len('Classifier: '):])\n        package['classifiers'] = classifiers\n\n        if file_list:\n            package['files'] = sorted(file_list)\n        yield package", "response": "Gather details from installed distributions. Print distribution name,\n    version, location, and installed files. Installed files requires a\n    pip generated 'installed-files.txt' in the distributions '.egg-info'\n    directory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprinting the informations from installed distributions found.", "response": "def print_results(distributions, list_files=False, verbose=False):\n    \"\"\"\n    Print the informations from installed distributions found.\n    \"\"\"\n    results_printed = False\n    for i, dist in enumerate(distributions):\n        results_printed = True\n        if i > 0:\n            logger.info(\"---\")\n\n        name = dist.get('name', '')\n        required_by = [\n            pkg.project_name for pkg in pkg_resources.working_set\n            if name in [required.name for required in pkg.requires()]\n        ]\n\n        logger.info(\"Name: %s\", name)\n        logger.info(\"Version: %s\", dist.get('version', ''))\n        logger.info(\"Summary: %s\", dist.get('summary', ''))\n        logger.info(\"Home-page: %s\", dist.get('home-page', ''))\n        logger.info(\"Author: %s\", dist.get('author', ''))\n        logger.info(\"Author-email: %s\", dist.get('author-email', ''))\n        logger.info(\"License: %s\", dist.get('license', ''))\n        logger.info(\"Location: %s\", dist.get('location', ''))\n        logger.info(\"Requires: %s\", ', '.join(dist.get('requires', [])))\n        logger.info(\"Required-by: %s\", ', '.join(required_by))\n\n        if verbose:\n            logger.info(\"Metadata-Version: %s\",\n                        dist.get('metadata-version', ''))\n            logger.info(\"Installer: %s\", dist.get('installer', ''))\n            logger.info(\"Classifiers:\")\n            for classifier in dist.get('classifiers', []):\n                logger.info(\"  %s\", classifier)\n            logger.info(\"Entry-points:\")\n            for entry in dist.get('entry_points', []):\n                logger.info(\"  %s\", entry.strip())\n        if list_files:\n            logger.info(\"Files:\")\n            for line in dist.get('files', []):\n                logger.info(\"  %s\", line.strip())\n            if \"files\" not in dist:\n                logger.info(\"Cannot locate installed-files.txt\")\n    return results_printed"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fail(self, msg, lineno=None, exc=TemplateSyntaxError):\n        if lineno is None:\n            lineno = self.stream.current.lineno\n        raise exc(msg, lineno, self.name, self.filename)", "response": "Convenience method that raises exc with the message passed to the next entry in the sequence."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalls when the parser encounters an unknown tag.", "response": "def fail_unknown_tag(self, name, lineno=None):\n        \"\"\"Called if the parser encounters an unknown tag.  Tries to fail\n        with a human readable error message that could help to identify\n        the problem.\n        \"\"\"\n        return self._fail_ut_eof(name, self._end_token_stack, lineno)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fail_eof(self, end_tokens=None, lineno=None):\n        stack = list(self._end_token_stack)\n        if end_tokens is not None:\n            stack.append(end_tokens)\n        return self._fail_ut_eof(None, stack, lineno)", "response": "Like fail_unknown_tag but for end of template situations."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_tuple_end(self, extra_end_rules=None):\n        if self.stream.current.type in ('variable_end', 'block_end', 'rparen'):\n            return True\n        elif extra_end_rules is not None:\n            return self.stream.current.test_any(extra_end_rules)\n        return False", "response": "Is we at the end of a tuple?"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef free_identifier(self, lineno=None):\n        self._last_identifier += 1\n        rv = object.__new__(nodes.InternalName)\n        nodes.Node.__init__(rv, 'fi%d' % self._last_identifier, lineno=lineno)\n        return rv", "response": "Return a new free identifier as : class ~jinja2. nodes. InternalName."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing a single statement.", "response": "def parse_statement(self):\n        \"\"\"Parse a single statement.\"\"\"\n        token = self.stream.current\n        if token.type != 'name':\n            self.fail('tag name expected', token.lineno)\n        self._tag_stack.append(token.value)\n        pop_tag = True\n        try:\n            if token.value in _statement_keywords:\n                return getattr(self, 'parse_' + self.stream.current.value)()\n            if token.value == 'call':\n                return self.parse_call_block()\n            if token.value == 'filter':\n                return self.parse_filter_block()\n            ext = self.extensions.get(token.value)\n            if ext is not None:\n                return ext(self)\n\n            # did not work out, remove the token we pushed by accident\n            # from the stack so that the unknown tag fail function can\n            # produce a proper error message.\n            self._tag_stack.pop()\n            pop_tag = False\n            self.fail_unknown_tag(token.value, token.lineno)\n        finally:\n            if pop_tag:\n                self._tag_stack.pop()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse multiple statements into a list of template data structures.", "response": "def parse_statements(self, end_tokens, drop_needle=False):\n        \"\"\"Parse multiple statements into a list until one of the end tokens\n        is reached.  This is used to parse the body of statements as it also\n        parses template data if appropriate.  The parser checks first if the\n        current token is a colon and skips it if there is one.  Then it checks\n        for the block end and parses until if one of the `end_tokens` is\n        reached.  Per default the active token in the stream at the end of\n        the call is the matched end token.  If this is not wanted `drop_needle`\n        can be set to `True` and the end token is removed.\n        \"\"\"\n        # the first token may be a colon for python compatibility\n        self.stream.skip_if('colon')\n\n        # in the future it would be possible to add whole code sections\n        # by adding some sort of end of statement token and parsing those here.\n        self.stream.expect('block_end')\n        result = self.subparse(end_tokens)\n\n        # we reached the end of the template too early, the subparser\n        # does not check for this, so we do that now\n        if self.stream.current.type == 'eof':\n            self.fail_eof(end_tokens)\n\n        if drop_needle:\n            next(self.stream)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing an assign statement.", "response": "def parse_set(self):\n        \"\"\"Parse an assign statement.\"\"\"\n        lineno = next(self.stream).lineno\n        target = self.parse_assign_target(with_namespace=True)\n        if self.stream.skip_if('assign'):\n            expr = self.parse_tuple()\n            return nodes.Assign(target, expr, lineno=lineno)\n        filter_node = self.parse_filter(None)\n        body = self.parse_statements(('name:endset',),\n                                     drop_needle=True)\n        return nodes.AssignBlock(target, filter_node, body, lineno=lineno)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses a for loop.", "response": "def parse_for(self):\n        \"\"\"Parse a for loop.\"\"\"\n        lineno = self.stream.expect('name:for').lineno\n        target = self.parse_assign_target(extra_end_rules=('name:in',))\n        self.stream.expect('name:in')\n        iter = self.parse_tuple(with_condexpr=False,\n                                extra_end_rules=('name:recursive',))\n        test = None\n        if self.stream.skip_if('name:if'):\n            test = self.parse_expression()\n        recursive = self.stream.skip_if('name:recursive')\n        body = self.parse_statements(('name:endfor', 'name:else'))\n        if next(self.stream).value == 'endfor':\n            else_ = []\n        else:\n            else_ = self.parse_statements(('name:endfor',), drop_needle=True)\n        return nodes.For(target, iter, body, else_, test,\n                         recursive, lineno=lineno)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing an if construct.", "response": "def parse_if(self):\n        \"\"\"Parse an if construct.\"\"\"\n        node = result = nodes.If(lineno=self.stream.expect('name:if').lineno)\n        while 1:\n            node.test = self.parse_tuple(with_condexpr=False)\n            node.body = self.parse_statements(('name:elif', 'name:else',\n                                               'name:endif'))\n            node.elif_ = []\n            node.else_ = []\n            token = next(self.stream)\n            if token.test('name:elif'):\n                node = nodes.If(lineno=self.stream.current.lineno)\n                result.elif_.append(node)\n                continue\n            elif token.test('name:else'):\n                result.else_ = self.parse_statements(('name:endif',),\n                                                     drop_needle=True)\n            break\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_assign_target(self, with_tuple=True, name_only=False,\n                            extra_end_rules=None, with_namespace=False):\n        \"\"\"Parse an assignment target.  As Jinja2 allows assignments to\n        tuples, this function can parse all allowed assignment targets.  Per\n        default assignments to tuples are parsed, that can be disable however\n        by setting `with_tuple` to `False`.  If only assignments to names are\n        wanted `name_only` can be set to `True`.  The `extra_end_rules`\n        parameter is forwarded to the tuple parsing function.  If\n        `with_namespace` is enabled, a namespace assignment may be parsed.\n        \"\"\"\n        if with_namespace and self.stream.look().type == 'dot':\n            token = self.stream.expect('name')\n            next(self.stream)  # dot\n            attr = self.stream.expect('name')\n            target = nodes.NSRef(token.value, attr.value, lineno=token.lineno)\n        elif name_only:\n            token = self.stream.expect('name')\n            target = nodes.Name(token.value, 'store', lineno=token.lineno)\n        else:\n            if with_tuple:\n                target = self.parse_tuple(simplified=True,\n                                          extra_end_rules=extra_end_rules)\n            else:\n                target = self.parse_primary()\n            target.set_ctx('store')\n        if not target.can_assign():\n            self.fail('can\\'t assign to %r' % target.__class__.\n                      __name__.lower(), target.lineno)\n        return target", "response": "Parse an assignment target."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the whole template into a Template node.", "response": "def parse(self):\n        \"\"\"Parse the whole template into a `Template` node.\"\"\"\n        result = nodes.Template(self.subparse(), lineno=1)\n        result.set_environment(self.environment)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_trace(self):\n        '''This returns an abbreviated stack trace with lines that only concern\n        the caller. In other words, the stack trace inside the Pexpect module\n        is not included. '''\n\n        tblist = traceback.extract_tb(sys.exc_info()[2])\n        tblist = [item for item in tblist if ('pexpect/__init__' not in item[0])\n                                           and ('pexpect/expect' not in item[0])]\n        tblist = traceback.format_list(tblist)\n        return ''.join(tblist)", "response": "This returns an abbreviated stack trace with lines that only concern\n        the caller."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_uri(uri):\n    groups = URI.match(uri).groups()\n    return (groups[1], groups[3], groups[4], groups[6], groups[8])", "response": "Parses a URI using the regex given in Appendix B of RFC 3986."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _urlnorm(cls, uri):\n        (scheme, authority, path, query, fragment) = parse_uri(uri)\n        if not scheme or not authority:\n            raise Exception(\"Only absolute URIs are allowed. uri = %s\" % uri)\n\n        scheme = scheme.lower()\n        authority = authority.lower()\n\n        if not path:\n            path = \"/\"\n\n        # Could do syntax based normalization of the URI before\n        # computing the digest. See Section 6.2.2 of Std 66.\n        request_uri = query and \"?\".join([path, query]) or path\n        defrag_uri = scheme + \"://\" + authority + request_uri\n\n        return defrag_uri", "response": "Normalize the URL to create a safe key for the cache"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a cached response if it exists in the cache otherwise return False.", "response": "def cached_request(self, request):\n        \"\"\"\n        Return a cached response if it exists in the cache, otherwise\n        return False.\n        \"\"\"\n        cache_url = self.cache_url(request.url)\n        logger.debug('Looking up \"%s\" in the cache', cache_url)\n        cc = self.parse_cache_control(request.headers)\n\n        # Bail out if the request insists on fresh data\n        if \"no-cache\" in cc:\n            logger.debug('Request header has \"no-cache\", cache bypassed')\n            return False\n\n        if \"max-age\" in cc and cc[\"max-age\"] == 0:\n            logger.debug('Request header has \"max_age\" as 0, cache bypassed')\n            return False\n\n        # Request allows serving from the cache, let's see if we find something\n        cache_data = self.cache.get(cache_url)\n        if cache_data is None:\n            logger.debug(\"No cache entry available\")\n            return False\n\n        # Check whether it can be deserialized\n        resp = self.serializer.loads(request, cache_data)\n        if not resp:\n            logger.warning(\"Cache entry deserialization failed, entry ignored\")\n            return False\n\n        # If we have a cached 301, return it immediately. We don't\n        # need to test our response for other headers b/c it is\n        # intrinsically \"cacheable\" as it is Permanent.\n        # See:\n        #   https://tools.ietf.org/html/rfc7231#section-6.4.2\n        #\n        # Client can try to refresh the value by repeating the request\n        # with cache busting headers as usual (ie no-cache).\n        if resp.status == 301:\n            msg = (\n                'Returning cached \"301 Moved Permanently\" response '\n                \"(ignoring date and etag information)\"\n            )\n            logger.debug(msg)\n            return resp\n\n        headers = CaseInsensitiveDict(resp.headers)\n        if not headers or \"date\" not in headers:\n            if \"etag\" not in headers:\n                # Without date or etag, the cached response can never be used\n                # and should be deleted.\n                logger.debug(\"Purging cached response: no date or etag\")\n                self.cache.delete(cache_url)\n            logger.debug(\"Ignoring cached response: no date\")\n            return False\n\n        now = time.time()\n        date = calendar.timegm(parsedate_tz(headers[\"date\"]))\n        current_age = max(0, now - date)\n        logger.debug(\"Current age based on date: %i\", current_age)\n\n        # TODO: There is an assumption that the result will be a\n        #       urllib3 response object. This may not be best since we\n        #       could probably avoid instantiating or constructing the\n        #       response until we know we need it.\n        resp_cc = self.parse_cache_control(headers)\n\n        # determine freshness\n        freshness_lifetime = 0\n\n        # Check the max-age pragma in the cache control header\n        if \"max-age\" in resp_cc:\n            freshness_lifetime = resp_cc[\"max-age\"]\n            logger.debug(\"Freshness lifetime from max-age: %i\", freshness_lifetime)\n\n        # If there isn't a max-age, check for an expires header\n        elif \"expires\" in headers:\n            expires = parsedate_tz(headers[\"expires\"])\n            if expires is not None:\n                expire_time = calendar.timegm(expires) - date\n                freshness_lifetime = max(0, expire_time)\n                logger.debug(\"Freshness lifetime from expires: %i\", freshness_lifetime)\n\n        # Determine if we are setting freshness limit in the\n        # request. Note, this overrides what was in the response.\n        if \"max-age\" in cc:\n            freshness_lifetime = cc[\"max-age\"]\n            logger.debug(\n                \"Freshness lifetime from request max-age: %i\", freshness_lifetime\n            )\n\n        if \"min-fresh\" in cc:\n            min_fresh = cc[\"min-fresh\"]\n            # adjust our current age by our min fresh\n            current_age += min_fresh\n            logger.debug(\"Adjusted current age from min-fresh: %i\", current_age)\n\n        # Return entry if it is fresh enough\n        if freshness_lifetime > current_age:\n            logger.debug('The response is \"fresh\", returning cached response')\n            logger.debug(\"%i > %i\", freshness_lifetime, current_age)\n            return resp\n\n        # we're not fresh. If we don't have an Etag, clear it out\n        if \"etag\" not in headers:\n            logger.debug('The cached response is \"stale\" with no etag, purging')\n            self.cache.delete(cache_url)\n\n        # return the original handler\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cache_response(self, request, response, body=None, status_codes=None):\n        # From httplib2: Don't cache 206's since we aren't going to\n        #                handle byte range requests\n        cacheable_status_codes = status_codes or self.cacheable_status_codes\n        if response.status not in cacheable_status_codes:\n            logger.debug(\n                \"Status code %s not in %s\", response.status, cacheable_status_codes\n            )\n            return\n\n        response_headers = CaseInsensitiveDict(response.headers)\n\n        # If we've been given a body, our response has a Content-Length, that\n        # Content-Length is valid then we can check to see if the body we've\n        # been given matches the expected size, and if it doesn't we'll just\n        # skip trying to cache it.\n        if (\n            body is not None\n            and \"content-length\" in response_headers\n            and response_headers[\"content-length\"].isdigit()\n            and int(response_headers[\"content-length\"]) != len(body)\n        ):\n            return\n\n        cc_req = self.parse_cache_control(request.headers)\n        cc = self.parse_cache_control(response_headers)\n\n        cache_url = self.cache_url(request.url)\n        logger.debug('Updating cache with response from \"%s\"', cache_url)\n\n        # Delete it from the cache if we happen to have it stored there\n        no_store = False\n        if \"no-store\" in cc:\n            no_store = True\n            logger.debug('Response header has \"no-store\"')\n        if \"no-store\" in cc_req:\n            no_store = True\n            logger.debug('Request header has \"no-store\"')\n        if no_store and self.cache.get(cache_url):\n            logger.debug('Purging existing cache entry to honor \"no-store\"')\n            self.cache.delete(cache_url)\n        if no_store:\n            return\n\n        # If we've been given an etag, then keep the response\n        if self.cache_etags and \"etag\" in response_headers:\n            logger.debug(\"Caching due to etag\")\n            self.cache.set(\n                cache_url, self.serializer.dumps(request, response, body=body)\n            )\n\n        # Add to the cache any 301s. We do this before looking that\n        # the Date headers.\n        elif response.status == 301:\n            logger.debug(\"Caching permanant redirect\")\n            self.cache.set(cache_url, self.serializer.dumps(request, response))\n\n        # Add to the cache if the response headers demand it. If there\n        # is no date header then we can't do anything about expiring\n        # the cache.\n        elif \"date\" in response_headers:\n            # cache when there is a max-age > 0\n            if \"max-age\" in cc and cc[\"max-age\"] > 0:\n                logger.debug(\"Caching b/c date exists and max-age > 0\")\n                self.cache.set(\n                    cache_url, self.serializer.dumps(request, response, body=body)\n                )\n\n            # If the request can expire, it means we should cache it\n            # in the meantime.\n            elif \"expires\" in response_headers:\n                if response_headers[\"expires\"]:\n                    logger.debug(\"Caching b/c of expires header\")\n                    self.cache.set(\n                        cache_url, self.serializer.dumps(request, response, body=body)\n                    )", "response": "This function is used to cache the response."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate our cached response with the new request and response.", "response": "def update_cached_response(self, request, response):\n        \"\"\"On a 304 we will get a new set of headers that we want to\n        update our cached value with, assuming we have one.\n\n        This should only ever be called when we've sent an ETag and\n        gotten a 304 as the response.\n        \"\"\"\n        cache_url = self.cache_url(request.url)\n\n        cached_response = self.serializer.loads(request, self.cache.get(cache_url))\n\n        if not cached_response:\n            # we didn't have a cached response\n            return response\n\n        # Lets update our headers with the headers from the new request:\n        # http://tools.ietf.org/html/draft-ietf-httpbis-p4-conditional-26#section-4.1\n        #\n        # The server isn't supposed to send headers that would make\n        # the cached body invalid. But... just in case, we'll be sure\n        # to strip out ones we know that might be problmatic due to\n        # typical assumptions.\n        excluded_headers = [\"content-length\"]\n\n        cached_response.headers.update(\n            dict(\n                (k, v)\n                for k, v in response.headers.items()\n                if k.lower() not in excluded_headers\n            )\n        )\n\n        # we want a 200 b/c we have content via the cache\n        cached_response.status = 200\n\n        # update our cache\n        self.cache.set(cache_url, self.serializer.dumps(request, cached_response))\n\n        return cached_response"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef close (self):\n        if self.child_fd == -1:\n            return\n\n        self.flush()\n        os.close(self.child_fd)\n        self.child_fd = -1\n        self.closed = True", "response": "Close the file descriptor."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if the file descriptor is still valid.", "response": "def isalive (self):\n        '''This checks if the file descriptor is still valid. If :func:`os.fstat`\n        does not raise an exception then we assume it is alive. '''\n\n        if self.child_fd == -1:\n            return False\n        try:\n            os.fstat(self.child_fd)\n            return True\n        except:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read_nonblocking(self, size=1, timeout=-1):\n        if os.name == 'posix':\n            if timeout == -1:\n                timeout = self.timeout\n            rlist = [self.child_fd]\n            wlist = []\n            xlist = []\n            if self.use_poll:\n                rlist = poll_ignore_interrupts(rlist, timeout)\n            else:\n                rlist, wlist, xlist = select_ignore_interrupts(\n                    rlist, wlist, xlist, timeout\n                )\n            if self.child_fd not in rlist:\n                raise TIMEOUT('Timeout exceeded.')\n        return super(fdspawn, self).read_nonblocking(size)", "response": "Reads from the file descriptor and returns the result as a string."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clear_caches():\n    from jinja2.environment import _spontaneous_environments\n    from jinja2.lexer import _lexer_cache\n    _spontaneous_environments.clear()\n    _lexer_cache.clear()", "response": "Jinja2 keeps internal caches for environments and lexers."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef import_string(import_name, silent=False):\n    try:\n        if ':' in import_name:\n            module, obj = import_name.split(':', 1)\n        elif '.' in import_name:\n            items = import_name.split('.')\n            module = '.'.join(items[:-1])\n            obj = items[-1]\n        else:\n            return __import__(import_name)\n        return getattr(__import__(module, None, None, [obj]), obj)\n    except (ImportError, AttributeError):\n        if not silent:\n            raise", "response": "Imports an object based on a string."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef unicode_urlencode(obj, charset='utf-8', for_qs=False):\n    if not isinstance(obj, string_types):\n        obj = text_type(obj)\n    if isinstance(obj, text_type):\n        obj = obj.encode(charset)\n    safe = not for_qs and b'/' or b''\n    rv = text_type(url_quote(obj, safe))\n    if for_qs:\n        rv = rv.replace('%20', '+')\n    return rv", "response": "URL encodes a single bytestring or unicode string with the given charset if applicable to URL safe quoting under all supported Python versions."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndoes not use in templates.", "response": "def htmlsafe_json_dumps(obj, dumper=None, **kwargs):\n    \"\"\"Works exactly like :func:`dumps` but is safe for use in ``<script>``\n    tags.  It accepts the same arguments and returns a JSON string.  Note that\n    this is available in templates through the ``|tojson`` filter which will\n    also mark the result as safe.  Due to how this function escapes certain\n    characters this is safe even if used outside of ``<script>`` tags.\n\n    The following characters are escaped in strings:\n\n    -   ``<``\n    -   ``>``\n    -   ``&``\n    -   ``'``\n\n    This makes it safe to embed such strings in any place in HTML with the\n    notable exception of double quoted attributes.  In that case single\n    quote your attributes or HTML escape it in addition.\n    \"\"\"\n    if dumper is None:\n        dumper = json.dumps\n    rv = dumper(obj, **kwargs) \\\n        .replace(u'<', u'\\\\u003c') \\\n        .replace(u'>', u'\\\\u003e') \\\n        .replace(u'&', u'\\\\u0026') \\\n        .replace(u\"'\", u'\\\\u0027')\n    return Markup(rv)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a shallow copy of the instance.", "response": "def copy(self):\n        \"\"\"Return a shallow copy of the instance.\"\"\"\n        rv = self.__class__(self.capacity)\n        rv._mapping.update(self._mapping)\n        rv._queue = deque(self._queue)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the value of the specified key to the given default value. If the key is not in the cache otherwise leave unchanged. Return the value of this key.", "response": "def setdefault(self, key, default=None):\n        \"\"\"Set `default` if the key is not in the cache otherwise\n        leave unchanged. Return the value of this key.\n        \"\"\"\n        self._wlock.acquire()\n        try:\n            try:\n                return self[key]\n            except KeyError:\n                self[key] = default\n                return default\n        finally:\n            self._wlock.release()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef items(self):\n        result = [(key, self._mapping[key]) for key in list(self._queue)]\n        result.reverse()\n        return result", "response": "Return a list of items."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef libname_from_dir(dirname):\n    parts = []\n    for part in dirname.split('-'):\n        if part[0].isdigit():\n            break\n        parts.append(part)\n    return '-'.join(parts)", "response": "Reconstruct the library name without it s version"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef license_destination(vendor_dir, libname, filename):\n    normal = vendor_dir / libname\n    if normal.is_dir():\n        return normal / filename\n    lowercase = vendor_dir / libname.lower().replace('-', '_')\n    if lowercase.is_dir():\n        return lowercase / filename\n    rename_dict = LIBRARY_RENAMES if vendor_dir.name != 'patched' else PATCHED_RENAMES\n    # Short circuit all logic if we are renaming the whole library\n    if libname in rename_dict:\n        return vendor_dir / rename_dict[libname] / filename\n    if libname in LIBRARY_DIRNAMES:\n        override = vendor_dir / LIBRARY_DIRNAMES[libname]\n        if not override.exists() and override.parent.exists():\n            # for flattened subdeps, specifically backports/weakref.py\n            return (\n                vendor_dir / override.parent\n            ) / '{0}.{1}'.format(override.name, filename)\n        return vendor_dir / LIBRARY_DIRNAMES[libname] / filename\n    # fallback to libname.LICENSE (used for nondirs)\n    return vendor_dir / '{}.{}'.format(libname, filename)", "response": "Given the vendor directory and the filename of a new license return the destination of the new license"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _convert_hashes(values):\n    hashes = {}\n    if not values:\n        return hashes\n    for value in values:\n        try:\n            name, value = value.split(\":\", 1)\n        except ValueError:\n            name = \"sha256\"\n        if name not in hashes:\n            hashes[name] = []\n        hashes[name].append(value)\n    return hashes", "response": "Convert Pipfile. lock hash lines into InstallRequirement option format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nhacking to hide noise generated by `setup.py develop`. There isn't a good way to suppress them now, so let's monky-patch. See https://bugs.python.org/issue25392.", "response": "def _suppress_distutils_logs():\n    \"\"\"Hack to hide noise generated by `setup.py develop`.\n\n    There isn't a good way to suppress them now, so let's monky-patch.\n    See https://bugs.python.org/issue25392.\n    \"\"\"\n    f = distutils.log.Log._log\n\n    def _log(log, level, msg, args):\n        if level >= distutils.log.ERROR:\n            f(log, level, msg, args)\n\n    distutils.log.Log._log = _log\n    yield\n    distutils.log.Log._log = f"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind this package s. egg - info directory.", "response": "def _find_egg_info(ireq):\n    \"\"\"Find this package's .egg-info directory.\n\n    Due to how sdists are designed, the .egg-info directory cannot be reliably\n    found without running setup.py to aggregate all configurations. This\n    function instead uses some heuristics to locate the egg-info directory\n    that most likely represents this package.\n\n    The best .egg-info directory's path is returned as a string. None is\n    returned if no matches can be found.\n    \"\"\"\n    root = ireq.setup_py_dir\n\n    directory_iterator = _iter_egg_info_directories(root, ireq.name)\n    try:\n        top_egg_info = next(directory_iterator)\n    except StopIteration:   # No egg-info found. Wat.\n        return None\n    directory_iterator = itertools.chain([top_egg_info], directory_iterator)\n\n    # Read the sdist's PKG-INFO to determine which egg_info is best.\n    pkg_info = _read_pkg_info(root)\n\n    # PKG-INFO not readable. Just return whatever comes first, I guess.\n    if pkg_info is None:\n        return top_egg_info\n\n    # Walk the sdist to find the egg-info with matching PKG-INFO.\n    for directory in directory_iterator:\n        egg_pkg_info = _read_pkg_info(directory)\n        if egg_pkg_info == pkg_info:\n            return directory\n\n    # Nothing matches...? Use the first one we found, I guess.\n    return top_egg_info"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fs_str(string):\n\n    if isinstance(string, str):\n        return string\n    assert not isinstance(string, bytes)\n    return string.encode(_fs_encoding)", "response": "Encodes a string into the proper filesystem encoding"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_path(path):\n\n    if isinstance(path, (six.string_types, bytes)):\n        return path\n    path_type = type(path)\n    try:\n        path_repr = path_type.__fspath__(path)\n    except AttributeError:\n        return\n    if isinstance(path_repr, (six.string_types, bytes)):\n        return path_repr\n    return", "response": "Get the string value from a path - like object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nencodes a filesystem path to the proper filesystem encoding", "response": "def fs_encode(path):\n    \"\"\"\n    Encode a filesystem path to the proper filesystem encoding\n\n    :param Union[str, bytes] path: A string-like path\n    :returns: A bytes-encoded filesystem path representation\n    \"\"\"\n\n    path = _get_path(path)\n    if path is None:\n        raise TypeError(\"expected a valid path to encode\")\n    if isinstance(path, six.text_type):\n        path = path.encode(_fs_encoding, _fs_encode_errors)\n    return path"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndecoding a filesystem path using the proper filesystem encoding", "response": "def fs_decode(path):\n    \"\"\"\n    Decode a filesystem path using the proper filesystem encoding\n\n    :param path: The filesystem path to decode from bytes or string\n    :return: [description]\n    :rtype: [type]\n    \"\"\"\n\n    path = _get_path(path)\n    if path is None:\n        raise TypeError(\"expected a valid path to decode\")\n    if isinstance(path, six.binary_type):\n        path = path.decode(_fs_encoding, _fs_decode_errors)\n    return path"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbuilds a collection of traces for each package in the graph.", "response": "def trace_graph(graph):\n    \"\"\"Build a collection of \"traces\" for each package.\n\n    A trace is a list of names that eventually leads to the package. For\n    example, if A and B are root dependencies, A depends on C and D, B\n    depends on C, and C depends on D, the return value would be like::\n\n        {\n            None: [],\n            \"A\": [None],\n            \"B\": [None],\n            \"C\": [[None, \"A\"], [None, \"B\"]],\n            \"D\": [[None, \"B\", \"C\"], [None, \"A\"]],\n        }\n    \"\"\"\n    result = {None: []}\n    for vertex in graph:\n        result[vertex] = []\n        for root in graph.iter_children(None):\n            paths = []\n            _trace_visit_vertex(graph, root, vertex, {None}, [None], paths)\n            result[vertex].extend(paths)\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _validate_timeout(cls, value, name):\n        if value is _Default:\n            return cls.DEFAULT_TIMEOUT\n\n        if value is None or value is cls.DEFAULT_TIMEOUT:\n            return value\n\n        if isinstance(value, bool):\n            raise ValueError(\"Timeout cannot be a boolean value. It must \"\n                             \"be an int, float or None.\")\n        try:\n            float(value)\n        except (TypeError, ValueError):\n            raise ValueError(\"Timeout value %s was %s, but it must be an \"\n                             \"int, float or None.\" % (name, value))\n\n        try:\n            if value <= 0:\n                raise ValueError(\"Attempted to set %s timeout to %s, but the \"\n                                 \"timeout cannot be set to a value less \"\n                                 \"than or equal to 0.\" % (name, value))\n        except TypeError:  # Python 3\n            raise ValueError(\"Timeout value %s was %s, but it must be an \"\n                             \"int, float or None.\" % (name, value))\n\n        return value", "response": "Validate that a timeout attribute is valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef clone(self):\n        # We can't use copy.deepcopy because that will also create a new object\n        # for _GLOBAL_DEFAULT_TIMEOUT, which socket.py uses as a sentinel to\n        # detect the user default.\n        return Timeout(connect=self._connect, read=self._read,\n                       total=self.total)", "response": "Create a copy of the timeout object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef start_connect(self):\n        if self._start_connect is not None:\n            raise TimeoutStateError(\"Timeout timer has already been started.\")\n        self._start_connect = current_time()\n        return self._start_connect", "response": "Start the timeout clock used during a connect."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef connect_timeout(self):\n        if self.total is None:\n            return self._connect\n\n        if self._connect is None or self._connect is self.DEFAULT_TIMEOUT:\n            return self.total\n\n        return min(self._connect, self.total)", "response": "Get the value to use when setting a connection timeout."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the value for the read timeout.", "response": "def read_timeout(self):\n        \"\"\" Get the value for the read timeout.\n\n        This assumes some time has elapsed in the connection timeout and\n        computes the read timeout appropriately.\n\n        If self.total is set, the read timeout is dependent on the amount of\n        time taken by the connect timeout. If the connection time has not been\n        established, a :exc:`~urllib3.exceptions.TimeoutStateError` will be\n        raised.\n\n        :return: Value to use for the read timeout.\n        :rtype: int, float, :attr:`Timeout.DEFAULT_TIMEOUT` or None\n        :raises urllib3.exceptions.TimeoutStateError: If :meth:`start_connect`\n            has not yet been called on this object.\n        \"\"\"\n        if (self.total is not None and\n                self.total is not self.DEFAULT_TIMEOUT and\n                self._read is not None and\n                self._read is not self.DEFAULT_TIMEOUT):\n            # In case the connect timeout has not yet been established.\n            if self._start_connect is None:\n                return self._read\n            return max(0, min(self.total - self.get_connect_duration(),\n                              self._read))\n        elif self.total is not None and self.total is not self.DEFAULT_TIMEOUT:\n            return max(0, self.total - self.get_connect_duration())\n        else:\n            return self._read"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _new_conn(self):\n        extra_kw = {}\n        if self.source_address:\n            extra_kw['source_address'] = self.source_address\n\n        if self.socket_options:\n            extra_kw['socket_options'] = self.socket_options\n\n        try:\n            conn = socks.create_connection(\n                (self.host, self.port),\n                proxy_type=self._socks_options['socks_version'],\n                proxy_addr=self._socks_options['proxy_host'],\n                proxy_port=self._socks_options['proxy_port'],\n                proxy_username=self._socks_options['username'],\n                proxy_password=self._socks_options['password'],\n                proxy_rdns=self._socks_options['rdns'],\n                timeout=self.timeout,\n                **extra_kw\n            )\n\n        except SocketTimeout as e:\n            raise ConnectTimeoutError(\n                self, \"Connection to %s timed out. (connect timeout=%s)\" %\n                (self.host, self.timeout))\n\n        except socks.ProxyError as e:\n            # This is fragile as hell, but it seems to be the only way to raise\n            # useful errors here.\n            if e.socket_err:\n                error = e.socket_err\n                if isinstance(error, SocketTimeout):\n                    raise ConnectTimeoutError(\n                        self,\n                        \"Connection to %s timed out. (connect timeout=%s)\" %\n                        (self.host, self.timeout)\n                    )\n                else:\n                    raise NewConnectionError(\n                        self,\n                        \"Failed to establish a new connection: %s\" % error\n                    )\n            else:\n                raise NewConnectionError(\n                    self,\n                    \"Failed to establish a new connection: %s\" % e\n                )\n\n        except SocketError as e:  # Defensive: PySocks should catch all these.\n            raise NewConnectionError(\n                self, \"Failed to establish a new connection: %s\" % e)\n\n        return conn", "response": "Establish a new connection via the SOCKS proxy."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_requirement_info(dist):\n    # type: (Distribution) -> RequirementInfo\n    \"\"\"\n    Compute and return values (req, editable, comments) for use in\n    FrozenRequirement.from_dist().\n    \"\"\"\n    if not dist_is_editable(dist):\n        return (None, False, [])\n\n    location = os.path.normcase(os.path.abspath(dist.location))\n\n    from pipenv.patched.notpip._internal.vcs import vcs, RemoteNotFoundError\n    vc_type = vcs.get_backend_type(location)\n\n    if not vc_type:\n        req = dist.as_requirement()\n        logger.debug(\n            'No VCS found for editable requirement {!r} in: {!r}', req,\n            location,\n        )\n        comments = [\n            '# Editable install with no version control ({})'.format(req)\n        ]\n        return (location, True, comments)\n\n    try:\n        req = vc_type.get_src_requirement(location, dist.project_name)\n    except RemoteNotFoundError:\n        req = dist.as_requirement()\n        comments = [\n            '# Editable {} install with no remote ({})'.format(\n                vc_type.__name__, req,\n            )\n        ]\n        return (location, True, comments)\n\n    except BadCommand:\n        logger.warning(\n            'cannot determine version of editable source in %s '\n            '(%s command not found in path)',\n            location,\n            vc_type.name,\n        )\n        return (None, True, [])\n\n    except InstallationError as exc:\n        logger.warning(\n            \"Error when trying to get requirement for VCS system %s, \"\n            \"falling back to uneditable format\", exc\n        )\n    else:\n        if req is not None:\n            return (req, True, [])\n\n    logger.warning(\n        'Could not determine repository location of %s', location\n    )\n    comments = ['## !! Could not determine repository location']\n\n    return (None, False, comments)", "response": "Returns a tuple of the location of the dist and whether it is editable or not."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndetects the proc filesystem style.", "response": "def detect_proc():\n    \"\"\"Detect /proc filesystem style.\n\n    This checks the /proc/{pid} directory for possible formats. Returns one of\n    the followings as str:\n\n    * `stat`: Linux-style, i.e. ``/proc/{pid}/stat``.\n    * `status`: BSD-style, i.e. ``/proc/{pid}/status``.\n    \"\"\"\n    pid = os.getpid()\n    for name in ('stat', 'status'):\n        if os.path.exists(os.path.join('/proc', str(pid), name)):\n            return name\n    raise ProcFormatError('unsupported proc format')"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntries to find the process tree via the process interface.", "response": "def get_process_mapping():\n    \"\"\"Try to look up the process tree via the /proc interface.\n    \"\"\"\n    stat_name = detect_proc()\n    self_tty = _get_stat(os.getpid(), stat_name)[0]\n    processes = {}\n    for pid in os.listdir('/proc'):\n        if not pid.isdigit():\n            continue\n        try:\n            tty, ppid = _get_stat(pid, stat_name)\n            if tty != self_tty:\n                continue\n            args = _get_cmdline(pid)\n            processes[pid] = Process(args=args, pid=pid, ppid=ppid)\n        except IOError:\n            # Process has disappeared - just ignore it.\n            continue\n    return processes"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating and return a temporary file.", "response": "def NamedTemporaryFile(\n    mode=\"w+b\",\n    buffering=-1,\n    encoding=None,\n    newline=None,\n    suffix=None,\n    prefix=None,\n    dir=None,\n    delete=True,\n    wrapper_class_override=None,\n):\n    \"\"\"Create and return a temporary file.\n    Arguments:\n    'prefix', 'suffix', 'dir' -- as for mkstemp.\n    'mode' -- the mode argument to io.open (default \"w+b\").\n    'buffering' -- the buffer size argument to io.open (default -1).\n    'encoding' -- the encoding argument to io.open (default None)\n    'newline' -- the newline argument to io.open (default None)\n    'delete' -- whether the file is deleted on close (default True).\n    The file is created as mkstemp() would do it.\n    Returns an object with a file-like interface; the name of the file\n    is accessible as its 'name' attribute.  The file will be automatically\n    deleted when it is closed unless the 'delete' argument is set to False.\n    \"\"\"\n    prefix, suffix, dir, output_type = _sanitize_params(prefix, suffix, dir)\n    flags = _bin_openflags\n    # Setting O_TEMPORARY in the flags causes the OS to delete\n    # the file when it is closed.  This is only supported by Windows.\n    if not wrapper_class_override:\n        wrapper_class_override = _TemporaryFileWrapper\n    if os.name == \"nt\" and delete:\n        flags |= os.O_TEMPORARY\n    if sys.version_info < (3, 5):\n        (fd, name) = _mkstemp_inner(dir, prefix, suffix, flags)\n    else:\n        (fd, name) = _mkstemp_inner(dir, prefix, suffix, flags, output_type)\n    try:\n        file = io.open(fd, mode, buffering=buffering, newline=newline, encoding=encoding)\n        if wrapper_class_override is not None:\n            return type(str(\"_TempFileWrapper\"), (wrapper_class_override, object), {})(\n                file, name, delete\n            )\n        else:\n            return _TemporaryFileWrapper(file, name, delete)\n\n    except BaseException:\n        os.unlink(name)\n        os.close(fd)\n        raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fast_exit(code):\n    sys.stdout.flush()\n    sys.stderr.flush()\n    os._exit(code)", "response": "Exit without garbage collection"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef augment_usage_errors(ctx, param=None):\n    try:\n        yield\n    except BadParameter as e:\n        if e.ctx is None:\n            e.ctx = ctx\n        if param is not None and e.param is None:\n            e.param = param\n        raise\n    except UsageError as e:\n        if e.ctx is None:\n            e.ctx = ctx\n        raise", "response": "Context manager that attaches extra information to exceptions that are not in use."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive a sequence of parameters in the order as should be considered for processing and an iterable of parameters that exist, this returns a list in the correct order as they should be processed.", "response": "def iter_params_for_processing(invocation_order, declaration_order):\n    \"\"\"Given a sequence of parameters in the order as should be considered\n    for processing and an iterable of parameters that exist, this returns\n    a list in the correct order as they should be processed.\n    \"\"\"\n    def sort_key(item):\n        try:\n            idx = invocation_order.index(item)\n        except ValueError:\n            idx = float('inf')\n        return (not item.is_eager, idx)\n\n    return sorted(declaration_order, key=sort_key)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_root(self):\n        node = self\n        while node.parent is not None:\n            node = node.parent\n        return node", "response": "Finds the root context."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_object(self, object_type):\n        node = self\n        while node is not None:\n            if isinstance(node.obj, object_type):\n                return node.obj\n            node = node.parent", "response": "Finds the closest object of a given type."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ensure_object(self, object_type):\n        rv = self.find_object(object_type)\n        if rv is None:\n            self.obj = rv = object_type()\n        return rv", "response": "Like find_object but sets the innermost object to a\n            new instance of object_type."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef lookup_default(self, name):\n        if self.default_map is not None:\n            rv = self.default_map.get(name)\n            if callable(rv):\n                rv = rv()\n            return rv", "response": "Looks up the default for a parameter name."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef invoke(*args, **kwargs):\n        self, callback = args[:2]\n        ctx = self\n\n        # It's also possible to invoke another command which might or\n        # might not have a callback.  In that case we also fill\n        # in defaults and make a new context for this command.\n        if isinstance(callback, Command):\n            other_cmd = callback\n            callback = other_cmd.callback\n            ctx = Context(other_cmd, info_name=other_cmd.name, parent=self)\n            if callback is None:\n                raise TypeError('The given command does not have a '\n                                'callback that can be invoked.')\n\n            for param in other_cmd.params:\n                if param.name not in kwargs and param.expose_value:\n                    kwargs[param.name] = param.get_default(ctx)\n\n        args = args[2:]\n        with augment_usage_errors(self):\n            with ctx:\n                return callback(*args, **kwargs)", "response": "Invoke a command callback in exactly the way it expects."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef main(self, args=None, prog_name=None, complete_var=None,\n             standalone_mode=True, **extra):\n        \"\"\"This is the way to invoke a script with all the bells and\n        whistles as a command line application.  This will always terminate\n        the application after a call.  If this is not wanted, ``SystemExit``\n        needs to be caught.\n\n        This method is also available by directly calling the instance of\n        a :class:`Command`.\n\n        .. versionadded:: 3.0\n           Added the `standalone_mode` flag to control the standalone mode.\n\n        :param args: the arguments that should be used for parsing.  If not\n                     provided, ``sys.argv[1:]`` is used.\n        :param prog_name: the program name that should be used.  By default\n                          the program name is constructed by taking the file\n                          name from ``sys.argv[0]``.\n        :param complete_var: the environment variable that controls the\n                             bash completion support.  The default is\n                             ``\"_<prog_name>_COMPLETE\"`` with prog_name in\n                             uppercase.\n        :param standalone_mode: the default behavior is to invoke the script\n                                in standalone mode.  Click will then\n                                handle exceptions and convert them into\n                                error messages and the function will never\n                                return but shut down the interpreter.  If\n                                this is set to `False` they will be\n                                propagated to the caller and the return\n                                value of this function is the return value\n                                of :meth:`invoke`.\n        :param extra: extra keyword arguments are forwarded to the context\n                      constructor.  See :class:`Context` for more information.\n        \"\"\"\n        # If we are in Python 3, we will verify that the environment is\n        # sane at this point or reject further execution to avoid a\n        # broken script.\n        if not PY2:\n            _verify_python3_env()\n        else:\n            _check_for_unicode_literals()\n\n        if args is None:\n            args = get_os_args()\n        else:\n            args = list(args)\n\n        if prog_name is None:\n            prog_name = make_str(os.path.basename(\n                sys.argv and sys.argv[0] or __file__))\n\n        # Hook for the Bash completion.  This only activates if the Bash\n        # completion is actually enabled, otherwise this is quite a fast\n        # noop.\n        _bashcomplete(self, prog_name, complete_var)\n\n        try:\n            try:\n                with self.make_context(prog_name, args, **extra) as ctx:\n                    rv = self.invoke(ctx)\n                    if not standalone_mode:\n                        return rv\n                    # it's not safe to `ctx.exit(rv)` here!\n                    # note that `rv` may actually contain data like \"1\" which\n                    # has obvious effects\n                    # more subtle case: `rv=[None, None]` can come out of\n                    # chained commands which all returned `None` -- so it's not\n                    # even always obvious that `rv` indicates success/failure\n                    # by its truthiness/falsiness\n                    ctx.exit()\n            except (EOFError, KeyboardInterrupt):\n                echo(file=sys.stderr)\n                raise Abort()\n            except ClickException as e:\n                if not standalone_mode:\n                    raise\n                e.show()\n                sys.exit(e.exit_code)\n            except IOError as e:\n                if e.errno == errno.EPIPE:\n                    sys.stdout = PacifyFlushWrapper(sys.stdout)\n                    sys.stderr = PacifyFlushWrapper(sys.stderr)\n                    sys.exit(1)\n                else:\n                    raise\n        except Exit as e:\n            if standalone_mode:\n                sys.exit(e.exit_code)\n            else:\n                # in non-standalone mode, return the exit code\n                # note that this is only reached if `self.invoke` above raises\n                # an Exit explicitly -- thus bypassing the check there which\n                # would return its result\n                # the results of non-standalone execution may therefore be\n                # somewhat ambiguous: if there are codepaths which lead to\n                # `ctx.exit(1)` and to `return 1`, the caller won't be able to\n                # tell the difference between the two\n                return e.exit_code\n        except Abort:\n            if not standalone_mode:\n                raise\n            echo('Aborted!', file=sys.stderr)\n            sys.exit(1)", "response": "This is the main method of the command line application."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting the usage line into the formatter.", "response": "def format_usage(self, ctx, formatter):\n        \"\"\"Writes the usage line into the formatter.\"\"\"\n        pieces = self.collect_usage_pieces(ctx)\n        formatter.write_usage(ctx.command_path, ' '.join(pieces))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef collect_usage_pieces(self, ctx):\n        rv = [self.options_metavar]\n        for param in self.get_params(ctx):\n            rv.extend(param.get_usage_pieces(ctx))\n        return rv", "response": "Returns all the pieces that go into the usage line and returns\n        it as a list of strings."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_help_option_names(self, ctx):\n        all_names = set(ctx.help_option_names)\n        for param in self.params:\n            all_names.difference_update(param.opts)\n            all_names.difference_update(param.secondary_opts)\n        return all_names", "response": "Returns the names for the help option."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the help option object.", "response": "def get_help_option(self, ctx):\n        \"\"\"Returns the help option object.\"\"\"\n        help_options = self.get_help_option_names(ctx)\n        if not help_options or not self.add_help_option:\n            return\n\n        def show_help(ctx, param, value):\n            if value and not ctx.resilient_parsing:\n                echo(ctx.get_help(), color=ctx.color)\n                ctx.exit()\n        return Option(help_options, is_flag=True,\n                      is_eager=True, expose_value=False,\n                      callback=show_help,\n                      help='Show this message and exit.')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate the underlying option parser for this command.", "response": "def make_parser(self, ctx):\n        \"\"\"Creates the underlying option parser for this command.\"\"\"\n        parser = OptionParser(ctx)\n        for param in self.get_params(ctx):\n            param.add_to_parser(parser, ctx)\n        return parser"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_help(self, ctx):\n        formatter = ctx.make_formatter()\n        self.format_help(ctx, formatter)\n        return formatter.getvalue().rstrip('\\n')", "response": "Formats the help into a string and returns it"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the short help string for the command or makes it by shortening the long help string.", "response": "def get_short_help_str(self, limit=45):\n        \"\"\"Gets short help for the command or makes it by shortening the long help string.\"\"\"\n        return self.short_help or self.help and make_default_short_help(self.help, limit) or ''"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite the help text into the formatter if it exists.", "response": "def format_help(self, ctx, formatter):\n        \"\"\"Writes the help into the formatter if it exists.\n\n        This calls into the following methods:\n\n        -   :meth:`format_usage`\n        -   :meth:`format_help_text`\n        -   :meth:`format_options`\n        -   :meth:`format_epilog`\n        \"\"\"\n        self.format_usage(ctx, formatter)\n        self.format_help_text(ctx, formatter)\n        self.format_options(ctx, formatter)\n        self.format_epilog(ctx, formatter)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef format_help_text(self, ctx, formatter):\n        if self.help:\n            formatter.write_paragraph()\n            with formatter.indentation():\n                help_text = self.help\n                if self.deprecated:\n                    help_text += DEPRECATED_HELP_NOTICE\n                formatter.write_text(help_text)\n        elif self.deprecated:\n            formatter.write_paragraph()\n            with formatter.indentation():\n                formatter.write_text(DEPRECATED_HELP_NOTICE)", "response": "Writes the help text to the formatter if it exists."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef format_options(self, ctx, formatter):\n        opts = []\n        for param in self.get_params(ctx):\n            rv = param.get_help_record(ctx)\n            if rv is not None:\n                opts.append(rv)\n\n        if opts:\n            with formatter.section('Options'):\n                formatter.write_dl(opts)", "response": "Writes all the options into the formatter if they exist."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef format_epilog(self, ctx, formatter):\n        if self.epilog:\n            formatter.write_paragraph()\n            with formatter.indentation():\n                formatter.write_text(self.epilog)", "response": "Writes the epilog into the formatter if it exists."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef invoke(self, ctx):\n        _maybe_show_deprecated_notice(self)\n        if self.callback is not None:\n            return ctx.invoke(self.callback, **ctx.params)", "response": "Invoke the callback on the related object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a result callback to the chain command.", "response": "def resultcallback(self, replace=False):\n        \"\"\"Adds a result callback to the chain command.  By default if a\n        result callback is already registered this will chain them but\n        this can be disabled with the `replace` parameter.  The result\n        callback is invoked with the return value of the subcommand\n        (or the list of return values from all subcommands if chaining\n        is enabled) as well as the parameters as they would be passed\n        to the main callback.\n\n        Example::\n\n            @click.group()\n            @click.option('-i', '--input', default=23)\n            def cli(input):\n                return 42\n\n            @cli.resultcallback()\n            def process_result(result, input):\n                return result + input\n\n        .. versionadded:: 3.0\n\n        :param replace: if set to `True` an already existing result\n                        callback will be removed.\n        \"\"\"\n        def decorator(f):\n            old_callback = self.result_callback\n            if old_callback is None or replace:\n                self.result_callback = f\n                return f\n            def function(__value, *args, **kwargs):\n                return f(old_callback(__value, *args, **kwargs),\n                         *args, **kwargs)\n            self.result_callback = rv = update_wrapper(function, f)\n            return rv\n        return decorator"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef format_commands(self, ctx, formatter):\n        commands = []\n        for subcommand in self.list_commands(ctx):\n            cmd = self.get_command(ctx, subcommand)\n            # What is this, the tool lied about a command.  Ignore it\n            if cmd is None:\n                continue\n            if cmd.hidden:\n                continue\n\n            commands.append((subcommand, cmd))\n\n        # allow for 3 times the default spacing\n        if len(commands):\n            limit = formatter.width - 6 - max(len(cmd[0]) for cmd in commands)\n\n            rows = []\n            for subcommand, cmd in commands:\n                help = cmd.get_short_help_str(limit)\n                rows.append((subcommand, help))\n\n            if rows:\n                with formatter.section('Commands'):\n                    formatter.write_dl(rows)", "response": "Extra format methods for multi - command commands."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nregistering another command with this group.", "response": "def add_command(self, cmd, name=None):\n        \"\"\"Registers another :class:`Command` with this group.  If the name\n        is not provided, the name of the command is used.\n        \"\"\"\n        name = name or cmd.name\n        if name is None:\n            raise TypeError('Command has no name.')\n        _check_multicommand(self, name, cmd, register=True)\n        self.commands[name] = cmd"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef command(self, *args, **kwargs):\n        def decorator(f):\n            cmd = command(*args, **kwargs)(f)\n            self.add_command(cmd)\n            return cmd\n        return decorator", "response": "A decorator for declaring and attaching a command to the group."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef group(self, *args, **kwargs):\n        def decorator(f):\n            cmd = group(*args, **kwargs)(f)\n            self.add_command(cmd)\n            return cmd\n        return decorator", "response": "A decorator for declaring and attaching a group to the current instance."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive a context variable this calculates the default value.", "response": "def get_default(self, ctx):\n        \"\"\"Given a context variable this calculates the default value.\"\"\"\n        # Otherwise go with the regular default.\n        if callable(self.default):\n            rv = self.default()\n        else:\n            rv = self.default\n        return self.type_cast_value(ctx, rv)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving a value this runs it properly through the type system. This handles things like nargs multiple and composite types.", "response": "def type_cast_value(self, ctx, value):\n        \"\"\"Given a value this runs it properly through the type system.\n        This automatically handles things like `nargs` and `multiple` as\n        well as composite types.\n        \"\"\"\n        if self.type.is_composite:\n            if self.nargs <= 1:\n                raise TypeError('Attempted to invoke composite type '\n                                'but nargs has been set to %s.  This is '\n                                'not supported; nargs needs to be set to '\n                                'a fixed value > 1.' % self.nargs)\n            if self.multiple:\n                return tuple(self.type(x or (), self, ctx) for x in value or ())\n            return self.type(value or (), self, ctx)\n\n        def _convert(value, level):\n            if level == 0:\n                return self.type(value, self, ctx)\n            return tuple(_convert(x, level - 1) for x in value or ())\n        return _convert(value, (self.nargs != 1) + bool(self.multiple))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_error_hint(self, ctx):\n        hint_list = self.opts or [self.human_readable_name]\n        return ' / '.join('\"%s\"' % x for x in hint_list)", "response": "Get a stringified version of the param for use in error messages to\n        indicate which param caused the error."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef prompt_for_value(self, ctx):\n        # Calculate the default before prompting anything to be stable.\n        default = self.get_default(ctx)\n\n        # If this is a prompt for a flag we need to handle this\n        # differently.\n        if self.is_bool_flag:\n            return confirm(self.prompt, default)\n\n        return prompt(self.prompt, default=default, type=self.type,\n                      hide_input=self.hide_input, show_choices=self.show_choices,\n                      confirmation_prompt=self.confirmation_prompt,\n                      value_proc=lambda x: self.process_value(ctx, x))", "response": "Prompt the user for a value."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind all matching dependencies using the supplied finder and the given ireq.", "response": "def find_all_matches(finder, ireq, pre=False):\n    # type: (PackageFinder, InstallRequirement, bool) -> List[InstallationCandidate]\n    \"\"\"Find all matching dependencies using the supplied finder and the\n    given ireq.\n\n    :param finder: A package finder for discovering matching candidates.\n    :type finder: :class:`~pip._internal.index.PackageFinder`\n    :param ireq: An install requirement.\n    :type ireq: :class:`~pip._internal.req.req_install.InstallRequirement`\n    :return: A list of matching candidates.\n    :rtype: list[:class:`~pip._internal.index.InstallationCandidate`]\n    \"\"\"\n\n\n    candidates = clean_requires_python(finder.find_all_candidates(ireq.name))\n    versions = {candidate.version for candidate in candidates}\n    allowed_versions = _get_filtered_versions(ireq, versions, pre)\n    if not pre and not allowed_versions:\n        allowed_versions = _get_filtered_versions(ireq, versions, True)\n    candidates = {c for c in candidates if c.version in allowed_versions}\n    return candidates"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_abstract_dependencies(reqs, sources=None, parent=None):\n\n    deps = []\n    from .requirements import Requirement\n\n    for req in reqs:\n        if isinstance(req, pip_shims.shims.InstallRequirement):\n            requirement = Requirement.from_line(\n                \"{0}{1}\".format(req.name, req.specifier)\n            )\n            if req.link:\n                requirement.req.link = req.link\n                requirement.markers = req.markers\n                requirement.req.markers = req.markers\n                requirement.extras = req.extras\n                requirement.req.extras = req.extras\n        elif isinstance(req, Requirement):\n            requirement = copy.deepcopy(req)\n        else:\n            requirement = Requirement.from_line(req)\n        dep = AbstractDependency.from_requirement(requirement, parent=parent)\n        deps.append(dep)\n    return deps", "response": "Given a set of requirements convert each requirement to an AbstractDependency."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget all dependencies for a given install requirement.", "response": "def get_dependencies(ireq, sources=None, parent=None):\n    # type: (Union[InstallRequirement, InstallationCandidate], Optional[List[Dict[S, Union[S, bool]]]], Optional[AbstractDependency]) -> Set[S, ...]\n    \"\"\"Get all dependencies for a given install requirement.\n\n    :param ireq: A single InstallRequirement\n    :type ireq: :class:`~pip._internal.req.req_install.InstallRequirement`\n    :param sources: Pipfile-formatted sources, defaults to None\n    :type sources: list[dict], optional\n    :param parent: The parent of this list of dependencies, defaults to None\n    :type parent: :class:`~pip._internal.req.req_install.InstallRequirement`\n    :return: A set of dependency lines for generating new InstallRequirements.\n    :rtype: set(str)\n    \"\"\"\n    if not isinstance(ireq, pip_shims.shims.InstallRequirement):\n        name = getattr(\n            ireq, \"project_name\",\n            getattr(ireq, \"project\", ireq.name),\n        )\n        version = getattr(ireq, \"version\", None)\n        if not version:\n            ireq = pip_shims.shims.InstallRequirement.from_line(\"{0}\".format(name))\n        else:\n            ireq = pip_shims.shims.InstallRequirement.from_line(\"{0}=={1}\".format(name, version))\n    pip_options = get_pip_options(sources=sources)\n    getters = [\n        get_dependencies_from_cache,\n        get_dependencies_from_wheel_cache,\n        get_dependencies_from_json,\n        functools.partial(get_dependencies_from_index, pip_options=pip_options)\n    ]\n    for getter in getters:\n        deps = getter(ireq)\n        if deps is not None:\n            return deps\n    raise RuntimeError('failed to get dependencies for {}'.format(ireq))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_dependencies_from_wheel_cache(ireq):\n\n    if ireq.editable or not is_pinned_requirement(ireq):\n        return\n    matches = WHEEL_CACHE.get(ireq.link, name_from_req(ireq.req))\n    if matches:\n        matches = set(matches)\n        if not DEPENDENCY_CACHE.get(ireq):\n            DEPENDENCY_CACHE[ireq] = [format_requirement(m) for m in matches]\n        return matches\n    return", "response": "Retrieves the set of dependencies for the given install requirement from the wheel cache."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving the dependencies for the given install requirement from the json api.", "response": "def get_dependencies_from_json(ireq):\n    \"\"\"Retrieves dependencies for the given install requirement from the json api.\n\n    :param ireq: A single InstallRequirement\n    :type ireq: :class:`~pip._internal.req.req_install.InstallRequirement`\n    :return: A set of dependency lines for generating new InstallRequirements.\n    :rtype: set(str) or None\n    \"\"\"\n\n    if ireq.editable or not is_pinned_requirement(ireq):\n        return\n\n    # It is technically possible to parse extras out of the JSON API's\n    # requirement format, but it is such a chore let's just use the simple API.\n    if ireq.extras:\n        return\n\n    session = requests.session()\n    atexit.register(session.close)\n    version = str(ireq.req.specifier).lstrip(\"=\")\n\n    def gen(ireq):\n        info = None\n        try:\n            info = session.get(\n                \"https://pypi.org/pypi/{0}/{1}/json\".format(ireq.req.name, version)\n            ).json()[\"info\"]\n        finally:\n            session.close()\n        requires_dist = info.get(\"requires_dist\", info.get(\"requires\"))\n        if not requires_dist:   # The API can return None for this.\n            return\n        for requires in requires_dist:\n            i = pip_shims.shims.InstallRequirement.from_line(requires)\n            # See above, we don't handle requirements with extras.\n            if not _marker_contains_extra(i):\n                yield format_requirement(i)\n\n    if ireq not in DEPENDENCY_CACHE:\n        try:\n            reqs = DEPENDENCY_CACHE[ireq] = list(gen(ireq))\n        except JSONDecodeError:\n            return\n        req_iter = iter(reqs)\n    else:\n        req_iter = gen(ireq)\n    return set(req_iter)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving the set of lines that depend on the given install requirement.", "response": "def get_dependencies_from_cache(ireq):\n    \"\"\"Retrieves dependencies for the given install requirement from the dependency cache.\n\n    :param ireq: A single InstallRequirement\n    :type ireq: :class:`~pip._internal.req.req_install.InstallRequirement`\n    :return: A set of dependency lines for generating new InstallRequirements.\n    :rtype: set(str) or None\n    \"\"\"\n    if ireq.editable or not is_pinned_requirement(ireq):\n        return\n    if ireq not in DEPENDENCY_CACHE:\n        return\n    cached = set(DEPENDENCY_CACHE[ireq])\n\n    # Preserving sanity: Run through the cache and make sure every entry if\n    # valid. If this fails, something is wrong with the cache. Drop it.\n    try:\n        broken = False\n        for line in cached:\n            dep_ireq = pip_shims.shims.InstallRequirement.from_line(line)\n            name = canonicalize_name(dep_ireq.name)\n            if _marker_contains_extra(dep_ireq):\n                broken = True   # The \"extra =\" marker breaks everything.\n            elif name == canonicalize_name(ireq.name):\n                broken = True   # A package cannot depend on itself.\n            if broken:\n                break\n    except Exception:\n        broken = True\n\n    if broken:\n        del DEPENDENCY_CACHE[ireq]\n        return\n\n    return cached"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_dependencies_from_index(dep, sources=None, pip_options=None, wheel_cache=None):\n\n    finder = get_finder(sources=sources, pip_options=pip_options)\n    if not wheel_cache:\n        wheel_cache = WHEEL_CACHE\n    dep.is_direct = True\n    reqset = pip_shims.shims.RequirementSet()\n    reqset.add_requirement(dep)\n    requirements = None\n    setup_requires = {}\n    with temp_environ(), start_resolver(finder=finder, wheel_cache=wheel_cache) as resolver:\n        os.environ['PIP_EXISTS_ACTION'] = 'i'\n        dist = None\n        if dep.editable and not dep.prepared and not dep.req:\n            with cd(dep.setup_py_dir):\n                from setuptools.dist import distutils\n                try:\n                    dist = distutils.core.run_setup(dep.setup_py)\n                except (ImportError, TypeError, AttributeError):\n                    dist = None\n                else:\n                    setup_requires[dist.get_name()] = dist.setup_requires\n                if not dist:\n                    try:\n                        dist = dep.get_dist()\n                    except (TypeError, ValueError, AttributeError):\n                        pass\n                    else:\n                        setup_requires[dist.get_name()] = dist.setup_requires\n        resolver.require_hashes = False\n        try:\n            results = resolver._resolve_one(reqset, dep)\n        except Exception:\n            # FIXME: Needs to bubble the exception somehow to the user.\n            results = []\n        finally:\n            try:\n                wheel_cache.cleanup()\n            except AttributeError:\n                pass\n        resolver_requires_python = getattr(resolver, \"requires_python\", None)\n        requires_python = getattr(reqset, \"requires_python\", resolver_requires_python)\n        if requires_python:\n            add_marker = fix_requires_python_marker(requires_python)\n            reqset.remove(dep)\n            if dep.req.marker:\n                dep.req.marker._markers.extend(['and',].extend(add_marker._markers))\n            else:\n                dep.req.marker = add_marker\n            reqset.add(dep)\n        requirements = set()\n        for r in results:\n            if requires_python:\n                if r.req.marker:\n                    r.req.marker._markers.extend(['and',].extend(add_marker._markers))\n                else:\n                    r.req.marker = add_marker\n            requirements.add(format_requirement(r))\n        for section in setup_requires:\n            python_version = section\n            not_python = not is_python(section)\n\n            # This is for cleaning up :extras: formatted markers\n            # by adding them to the results of the resolver\n            # since any such extra would have been returned as a result anyway\n            for value in setup_requires[section]:\n\n                # This is a marker.\n                if is_python(section):\n                    python_version = value[1:-1]\n                else:\n                    not_python = True\n\n                if ':' not in value and not_python:\n                    try:\n                        requirement_str = \"{0}{1}\".format(value, python_version).replace(\":\", \";\")\n                        requirements.add(format_requirement(make_install_requirement(requirement_str).ireq))\n                    # Anything could go wrong here -- can't be too careful.\n                    except Exception:\n                        pass\n\n    if not dep.editable and is_pinned_requirement(dep) and requirements is not None:\n        DEPENDENCY_CACHE[dep] = list(requirements)\n    return requirements", "response": "Returns a set of lines that are required by pip to install the given install requirement."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds a pip command from a list of sources", "response": "def get_pip_options(args=[], sources=None, pip_command=None):\n    \"\"\"Build a pip command from a list of sources\n\n    :param args: positional arguments passed through to the pip parser\n    :param sources: A list of pipfile-formatted sources, defaults to None\n    :param sources: list[dict], optional\n    :param pip_command: A pre-built pip command instance\n    :type pip_command: :class:`~pip._internal.cli.base_command.Command`\n    :return: An instance of pip_options using the supplied arguments plus sane defaults\n    :rtype: :class:`~pip._internal.cli.cmdoptions`\n    \"\"\"\n\n    if not pip_command:\n        pip_command = get_pip_command()\n    if not sources:\n        sources = [\n            {\"url\": \"https://pypi.org/simple\", \"name\": \"pypi\", \"verify_ssl\": True}\n        ]\n    _ensure_dir(CACHE_DIR)\n    pip_args = args\n    pip_args = prepare_pip_source_args(sources, pip_args)\n    pip_options, _ = pip_command.parser.parse_args(pip_args)\n    pip_options.cache_dir = CACHE_DIR\n    return pip_options"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a pip. shims. PackageFinder instance for looking up candidates to install .", "response": "def get_finder(sources=None, pip_command=None, pip_options=None):\n    # type: (List[Dict[S, Union[S, bool]]], Optional[Command], Any) -> PackageFinder\n    \"\"\"Get a package finder for looking up candidates to install\n\n    :param sources: A list of pipfile-formatted sources, defaults to None\n    :param sources: list[dict], optional\n    :param pip_command: A pip command instance, defaults to None\n    :type pip_command: :class:`~pip._internal.cli.base_command.Command`\n    :param pip_options: A pip options, defaults to None\n    :type pip_options: :class:`~pip._internal.cli.cmdoptions`\n    :return: A package finder\n    :rtype: :class:`~pip._internal.index.PackageFinder`\n    \"\"\"\n\n    if not pip_command:\n        pip_command = get_pip_command()\n    if not sources:\n        sources = [\n            {\"url\": \"https://pypi.org/simple\", \"name\": \"pypi\", \"verify_ssl\": True}\n        ]\n    if not pip_options:\n        pip_options = get_pip_options(sources=sources, pip_command=pip_command)\n    session = pip_command._build_session(pip_options)\n    atexit.register(session.close)\n    finder = pip_shims.shims.PackageFinder(\n        find_links=[],\n        index_urls=[s.get(\"url\") for s in sources],\n        trusted_hosts=[],\n        allow_all_prereleases=pip_options.pre,\n        session=session,\n    )\n    return finder"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef start_resolver(finder=None, wheel_cache=None):\n\n    pip_command = get_pip_command()\n    pip_options = get_pip_options(pip_command=pip_command)\n\n    if not finder:\n        finder = get_finder(pip_command=pip_command, pip_options=pip_options)\n\n    if not wheel_cache:\n        wheel_cache = WHEEL_CACHE\n    _ensure_dir(fs_str(os.path.join(wheel_cache.cache_dir, \"wheels\")))\n\n    download_dir = PKGS_DOWNLOAD_DIR\n    _ensure_dir(download_dir)\n\n    _build_dir = create_tracked_tempdir(fs_str(\"build\"))\n    _source_dir = create_tracked_tempdir(fs_str(\"source\"))\n    preparer = partialclass(\n        pip_shims.shims.RequirementPreparer,\n        build_dir=_build_dir,\n        src_dir=_source_dir,\n        download_dir=download_dir,\n        wheel_download_dir=WHEEL_DOWNLOAD_DIR,\n        progress_bar=\"off\",\n        build_isolation=False,\n    )\n    resolver = partialclass(\n        pip_shims.shims.Resolver,\n        finder=finder,\n        session=finder.session,\n        upgrade_strategy=\"to-satisfy-only\",\n        force_reinstall=True,\n        ignore_dependencies=False,\n        ignore_requires_python=True,\n        ignore_installed=True,\n        isolated=False,\n        wheel_cache=wheel_cache,\n        use_user_site=False,\n    )\n    try:\n        if packaging.version.parse(pip_shims.shims.pip_version) >= packaging.version.parse('18'):\n            with pip_shims.shims.RequirementTracker() as req_tracker:\n                preparer = preparer(req_tracker=req_tracker)\n                yield resolver(preparer=preparer)\n        else:\n            preparer = preparer()\n            yield resolver(preparer=preparer)\n    finally:\n        finder.session.close()", "response": "Start a new resolver for the given package finder."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef as_cache_key(self, ireq):\n        name, version, extras = as_tuple(ireq)\n        if not extras:\n            extras_string = \"\"\n        else:\n            extras_string = \"[{}]\".format(\",\".join(extras))\n        return name, \"{}{}\".format(version, extras_string)", "response": "Returns a tuple containing the name version and extras of the current version of the current ireq."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread the cached contents into memory.", "response": "def read_cache(self):\n        \"\"\"Reads the cached contents into memory.\"\"\"\n        if os.path.exists(self._cache_file):\n            self._cache = read_cache_file(self._cache_file)\n        else:\n            self._cache = {}"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite the cache to disk as JSON.", "response": "def write_cache(self):\n        \"\"\"Writes the cache to disk as JSON.\"\"\"\n        doc = {\n            '__format__': 1,\n            'dependencies': self._cache,\n        }\n        with open(self._cache_file, 'w') as f:\n            json.dump(doc, f, sort_keys=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef reverse_dependencies(self, ireqs):\n        ireqs_as_cache_values = [self.as_cache_key(ireq) for ireq in ireqs]\n        return self._reverse_dependencies(ireqs_as_cache_values)", "response": "Returns a lookup table of reverse dependencies for all the given ireqs."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a lookup table of reverse dependencies for all the given cache keys.", "response": "def _reverse_dependencies(self, cache_keys):\n        \"\"\"\n        Returns a lookup table of reverse dependencies for all the given cache keys.\n\n        Example input:\n\n            [('pep8', '1.5.7'),\n             ('flake8', '2.4.0'),\n             ('mccabe', '0.3'),\n             ('pyflakes', '0.8.1')]\n\n        Example output:\n\n            {'pep8': ['flake8'],\n             'flake8': [],\n             'mccabe': ['flake8'],\n             'pyflakes': ['flake8']}\n\n        \"\"\"\n        # First, collect all the dependencies into a sequence of (parent, child) tuples, like [('flake8', 'pep8'),\n        # ('flake8', 'mccabe'), ...]\n        return lookup_table((key_from_req(Requirement(dep_name)), name)\n                            for name, version_and_extras in cache_keys\n                            for dep_name in self.cache[name][version_and_extras])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngiving a requirement return its cache key.", "response": "def as_cache_key(self, ireq):\n        \"\"\"Given a requirement, return its cache key.\n\n        This behavior is a little weird in order to allow backwards\n        compatibility with cache files. For a requirement without extras, this\n        will return, for example::\n\n            (\"ipython\", \"2.1.0\")\n\n        For a requirement with extras, the extras will be comma-separated and\n        appended to the version, inside brackets, like so::\n\n            (\"ipython\", \"2.1.0[nbconvert,notebook]\")\n        \"\"\"\n        extras = tuple(sorted(ireq.extras))\n        if not extras:\n            extras_string = \"\"\n        else:\n            extras_string = \"[{}]\".format(\",\".join(extras))\n        name = key_from_req(ireq.req)\n        version = get_pinned_version(ireq)\n        return name, \"{}{}\".format(version, extras_string)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef locked(path, timeout=None):\n    def decor(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            lock = FileLock(path, timeout=timeout)\n            lock.acquire()\n            try:\n                return func(*args, **kwargs)\n            finally:\n                lock.release()\n        return wrapper\n    return decor", "response": "Decorator which enables locks for a file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a TreeWalker class for various types of tree with built - in support.", "response": "def getTreeWalker(treeType, implementation=None, **kwargs):\n    \"\"\"Get a TreeWalker class for various types of tree with built-in support\n\n    :arg str treeType: the name of the tree type required (case-insensitive).\n        Supported values are:\n\n        * \"dom\": The xml.dom.minidom DOM implementation\n        * \"etree\": A generic walker for tree implementations exposing an\n          elementtree-like interface (known to work with ElementTree,\n          cElementTree and lxml.etree).\n        * \"lxml\": Optimized walker for lxml.etree\n        * \"genshi\": a Genshi stream\n\n    :arg implementation: A module implementing the tree type e.g.\n        xml.etree.ElementTree or cElementTree (Currently applies to the \"etree\"\n        tree type only).\n\n    :arg kwargs: keyword arguments passed to the etree walker--for other\n        walkers, this has no effect\n\n    :returns: a TreeWalker class\n\n    \"\"\"\n\n    treeType = treeType.lower()\n    if treeType not in treeWalkerCache:\n        if treeType == \"dom\":\n            from . import dom\n            treeWalkerCache[treeType] = dom.TreeWalker\n        elif treeType == \"genshi\":\n            from . import genshi\n            treeWalkerCache[treeType] = genshi.TreeWalker\n        elif treeType == \"lxml\":\n            from . import etree_lxml\n            treeWalkerCache[treeType] = etree_lxml.TreeWalker\n        elif treeType == \"etree\":\n            from . import etree\n            if implementation is None:\n                implementation = default_etree\n            # XXX: NEVER cache here, caching is done in the etree submodule\n            return etree.getETreeModule(implementation, **kwargs).TreeWalker\n    return treeWalkerCache.get(treeType)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pprint(walker):\n    output = []\n    indent = 0\n    for token in concatenateCharacterTokens(walker):\n        type = token[\"type\"]\n        if type in (\"StartTag\", \"EmptyTag\"):\n            # tag name\n            if token[\"namespace\"] and token[\"namespace\"] != constants.namespaces[\"html\"]:\n                if token[\"namespace\"] in constants.prefixes:\n                    ns = constants.prefixes[token[\"namespace\"]]\n                else:\n                    ns = token[\"namespace\"]\n                name = \"%s %s\" % (ns, token[\"name\"])\n            else:\n                name = token[\"name\"]\n            output.append(\"%s<%s>\" % (\" \" * indent, name))\n            indent += 2\n            # attributes (sorted for consistent ordering)\n            attrs = token[\"data\"]\n            for (namespace, localname), value in sorted(attrs.items()):\n                if namespace:\n                    if namespace in constants.prefixes:\n                        ns = constants.prefixes[namespace]\n                    else:\n                        ns = namespace\n                    name = \"%s %s\" % (ns, localname)\n                else:\n                    name = localname\n                output.append(\"%s%s=\\\"%s\\\"\" % (\" \" * indent, name, value))\n            # self-closing\n            if type == \"EmptyTag\":\n                indent -= 2\n\n        elif type == \"EndTag\":\n            indent -= 2\n\n        elif type == \"Comment\":\n            output.append(\"%s<!-- %s -->\" % (\" \" * indent, token[\"data\"]))\n\n        elif type == \"Doctype\":\n            if token[\"name\"]:\n                if token[\"publicId\"]:\n                    output.append(\"\"\"%s<!DOCTYPE %s \"%s\" \"%s\">\"\"\" %\n                                  (\" \" * indent,\n                                   token[\"name\"],\n                                   token[\"publicId\"],\n                                   token[\"systemId\"] if token[\"systemId\"] else \"\"))\n                elif token[\"systemId\"]:\n                    output.append(\"\"\"%s<!DOCTYPE %s \"\" \"%s\">\"\"\" %\n                                  (\" \" * indent,\n                                   token[\"name\"],\n                                   token[\"systemId\"]))\n                else:\n                    output.append(\"%s<!DOCTYPE %s>\" % (\" \" * indent,\n                                                       token[\"name\"]))\n            else:\n                output.append(\"%s<!DOCTYPE >\" % (\" \" * indent,))\n\n        elif type == \"Characters\":\n            output.append(\"%s\\\"%s\\\"\" % (\" \" * indent, token[\"data\"]))\n\n        elif type == \"SpaceCharacters\":\n            assert False, \"concatenateCharacterTokens should have got rid of all Space tokens\"\n\n        else:\n            raise ValueError(\"Unknown token type, %s\" % type)\n\n    return \"\\n\".join(output)", "response": "Pretty printer for tree walkers"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nhiding the spinner to allow for custom writing to the terminal.", "response": "def hide(self):\n        \"\"\"Hide the spinner to allow for custom writing to the terminal.\"\"\"\n        thr_is_alive = self._spin_thread and self._spin_thread.is_alive()\n\n        if thr_is_alive and not self._hide_spin.is_set():\n            # set the hidden spinner flag\n            self._hide_spin.set()\n\n            # clear the current line\n            sys.stdout.write(\"\\r\")\n            self._clear_line()\n\n            # flush the stdout buffer so the current line can be rewritten to\n            sys.stdout.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nshow the hidden spinner.", "response": "def show(self):\n        \"\"\"Show the hidden spinner.\"\"\"\n        thr_is_alive = self._spin_thread and self._spin_thread.is_alive()\n\n        if thr_is_alive and self._hide_spin.is_set():\n            # clear the hidden spinner flag\n            self._hide_spin.clear()\n\n            # clear the current line so the spinner is not appended to it\n            sys.stdout.write(\"\\r\")\n            self._clear_line()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write(self, text):\n        # similar to tqdm.write()\n        # https://pypi.python.org/pypi/tqdm#writing-messages\n        sys.stdout.write(\"\\r\")\n        self._clear_line()\n\n        _text = to_unicode(text)\n        if PY2:\n            _text = _text.encode(ENCODING)\n\n        # Ensure output is bytes for Py2 and Unicode for Py3\n        assert isinstance(_text, builtin_str)\n\n        sys.stdout.write(\"{0}\\n\".format(_text))", "response": "Write text in the terminal without breaking the spinner."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _freeze(self, final_text):\n        text = to_unicode(final_text)\n        self._last_frame = self._compose_out(text, mode=\"last\")\n\n        # Should be stopped here, otherwise prints after\n        # self._freeze call will mess up the spinner\n        self.stop()\n        sys.stdout.write(self._last_frame)", "response": "Stop spinner compose last frame and freeze it."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_args(self):\n        # type: () -> List[str]\n        \"\"\"\n        Return the VCS-specific command arguments.\n        \"\"\"\n        args = []  # type: List[str]\n        rev = self.arg_rev\n        if rev is not None:\n            args += self.vcs.get_base_rev_args(rev)\n        args += self.extra_args\n\n        return args", "response": "Return the VCS - specific command arguments."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmake a new version of the current instance but with a new revision.", "response": "def make_new(self, rev):\n        # type: (str) -> RevOptions\n        \"\"\"\n        Make a copy of the current instance, but with a new rev.\n\n        Args:\n          rev: the name of the revision for the new object.\n        \"\"\"\n        return self.vcs.make_rev_options(rev, extra_args=self.extra_args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_backend_type(self, location):\n        # type: (str) -> Optional[Type[VersionControl]]\n        \"\"\"\n        Return the type of the version control backend if found at given\n        location, e.g. vcs.get_backend_type('/path/to/vcs/checkout')\n        \"\"\"\n        for vc_type in self._registry.values():\n            if vc_type.controls_location(location):\n                logger.debug('Determine that %s uses VCS: %s',\n                             location, vc_type.name)\n                return vc_type\n        return None", "response": "Returns the type of the version control backend at given location."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning True if the repository is local.", "response": "def _is_local_repository(cls, repo):\n        # type: (str) -> bool\n        \"\"\"\n           posix absolute paths start with os.path.sep,\n           win32 ones start with drive (like c:\\\\folder)\n        \"\"\"\n        drive, tail = os.path.splitdrive(repo)\n        return repo.startswith(os.path.sep) or bool(drive)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_url_rev_and_auth(self, url):\n        # type: (str) -> Tuple[str, Optional[str], AuthInfo]\n        \"\"\"\n        Parse the repository URL to use, and return the URL, revision,\n        and auth info to use.\n\n        Returns: (url, rev, (username, password)).\n        \"\"\"\n        scheme, netloc, path, query, frag = urllib_parse.urlsplit(url)\n        if '+' not in scheme:\n            raise ValueError(\n                \"Sorry, {!r} is a malformed VCS url. \"\n                \"The format is <vcs>+<protocol>://<url>, \"\n                \"e.g. svn+http://myrepo/svn/MyApp#egg=MyApp\".format(url)\n            )\n        # Remove the vcs prefix.\n        scheme = scheme.split('+', 1)[1]\n        netloc, user_pass = self.get_netloc_and_auth(netloc, scheme)\n        rev = None\n        if '@' in path:\n            path, rev = path.rsplit('@', 1)\n        url = urllib_parse.urlunsplit((scheme, netloc, path, query, ''))\n        return url, rev, user_pass", "response": "Parses the URL to use and return the URL revision and auth info to use."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_url_rev_options(self, url):\n        # type: (str) -> Tuple[str, RevOptions]\n        \"\"\"\n        Return the URL and RevOptions object to use in obtain() and in\n        some cases export(), as a tuple (url, rev_options).\n        \"\"\"\n        url, rev, user_pass = self.get_url_rev_and_auth(url)\n        username, password = user_pass\n        extra_args = self.make_rev_args(username, password)\n        rev_options = self.make_rev_options(rev, extra_args=extra_args)\n\n        return url, rev_options", "response": "Get the URL and RevOptions object to use in obtain and export as a tuple."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomparing two repo URLs ignoring incidental differences.", "response": "def compare_urls(self, url1, url2):\n        # type: (str, str) -> bool\n        \"\"\"\n        Compare two repo URLs for identity, ignoring incidental differences.\n        \"\"\"\n        return (self.normalize_url(url1) == self.normalize_url(url2))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninstalls or update in editable mode the package represented by this VersionControl object. Args: dest: the repository directory in which to install or update.", "response": "def obtain(self, dest):\n        # type: (str) -> None\n        \"\"\"\n        Install or update in editable mode the package represented by this\n        VersionControl object.\n\n        Args:\n          dest: the repository directory in which to install or update.\n        \"\"\"\n        url, rev_options = self.get_url_rev_options(self.url)\n\n        if not os.path.exists(dest):\n            self.fetch_new(dest, url, rev_options)\n            return\n\n        rev_display = rev_options.to_display()\n        if self.is_repository_directory(dest):\n            existing_url = self.get_remote_url(dest)\n            if self.compare_urls(existing_url, url):\n                logger.debug(\n                    '%s in %s exists, and has correct URL (%s)',\n                    self.repo_name.title(),\n                    display_path(dest),\n                    url,\n                )\n                if not self.is_commit_id_equal(dest, rev_options.rev):\n                    logger.info(\n                        'Updating %s %s%s',\n                        display_path(dest),\n                        self.repo_name,\n                        rev_display,\n                    )\n                    self.update(dest, url, rev_options)\n                else:\n                    logger.info('Skipping because already up-to-date.')\n                return\n\n            logger.warning(\n                '%s %s in %s exists with URL %s',\n                self.name,\n                self.repo_name,\n                display_path(dest),\n                existing_url,\n            )\n            prompt = ('(s)witch, (i)gnore, (w)ipe, (b)ackup ',\n                      ('s', 'i', 'w', 'b'))\n        else:\n            logger.warning(\n                'Directory %s already exists, and is not a %s %s.',\n                dest,\n                self.name,\n                self.repo_name,\n            )\n            # https://github.com/python/mypy/issues/1174\n            prompt = ('(i)gnore, (w)ipe, (b)ackup ',  # type: ignore\n                      ('i', 'w', 'b'))\n\n        logger.warning(\n            'The plan is to install the %s repository %s',\n            self.name,\n            url,\n        )\n        response = ask_path_exists('What to do?  %s' % prompt[0], prompt[1])\n\n        if response == 'a':\n            sys.exit(-1)\n\n        if response == 'w':\n            logger.warning('Deleting %s', display_path(dest))\n            rmtree(dest)\n            self.fetch_new(dest, url, rev_options)\n            return\n\n        if response == 'b':\n            dest_dir = backup_dir(dest)\n            logger.warning(\n                'Backing up %s to %s', display_path(dest), dest_dir,\n            )\n            shutil.move(dest, dest_dir)\n            self.fetch_new(dest, url, rev_options)\n            return\n\n        # Do nothing if the response is \"i\".\n        if response == 's':\n            logger.info(\n                'Switching %s %s to %s%s',\n                self.repo_name,\n                display_path(dest),\n                url,\n                rev_display,\n            )\n            self.switch(dest, url, rev_options)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun a VCS command and returns the output of the command.", "response": "def run_command(\n        cls,\n        cmd,  # type: List[str]\n        show_stdout=True,  # type: bool\n        cwd=None,  # type: Optional[str]\n        on_returncode='raise',  # type: str\n        extra_ok_returncodes=None,  # type: Optional[Iterable[int]]\n        command_desc=None,  # type: Optional[str]\n        extra_environ=None,  # type: Optional[Mapping[str, Any]]\n        spinner=None  # type: Optional[SpinnerInterface]\n    ):\n        # type: (...) -> Optional[Text]\n        \"\"\"\n        Run a VCS subcommand\n        This is simply a wrapper around call_subprocess that adds the VCS\n        command name, and checks that the VCS is available\n        \"\"\"\n        cmd = [cls.name] + cmd\n        try:\n            return call_subprocess(cmd, show_stdout, cwd,\n                                   on_returncode=on_returncode,\n                                   extra_ok_returncodes=extra_ok_returncodes,\n                                   command_desc=command_desc,\n                                   extra_environ=extra_environ,\n                                   unset_environ=cls.unset_environ,\n                                   spinner=spinner)\n        except OSError as e:\n            # errno.ENOENT = no such file or directory\n            # In other words, the VCS executable isn't available\n            if e.errno == errno.ENOENT:\n                raise BadCommand(\n                    'Cannot find command %r - do you have '\n                    '%r installed and in your '\n                    'PATH?' % (cls.name, cls.name))\n            else:\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_repository_directory(cls, path):\n        # type: (str) -> bool\n        \"\"\"\n        Return whether a directory path is a repository directory.\n        \"\"\"\n        logger.debug('Checking in %s for %s (%s)...',\n                     path, cls.dirname, cls.name)\n        return os.path.exists(os.path.join(path, cls.dirname))", "response": "Returns whether a directory path is a repository directory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate the fully qualified name of the files created by { console gui } _scripts for the given dist.", "response": "def _script_names(dist, script_name, is_gui):\n    \"\"\"Create the fully qualified name of the files created by\n    {console,gui}_scripts for the given ``dist``.\n    Returns the list of file names\n    \"\"\"\n    if dist_in_usersite(dist):\n        bin_dir = bin_user\n    else:\n        bin_dir = bin_py\n    exe_name = os.path.join(bin_dir, script_name)\n    paths_to_remove = [exe_name]\n    if WINDOWS:\n        paths_to_remove.append(exe_name + '.exe')\n        paths_to_remove.append(exe_name + '.exe.manifest')\n        if is_gui:\n            paths_to_remove.append(exe_name + '-script.pyw')\n        else:\n            paths_to_remove.append(exe_name + '-script.py')\n    return paths_to_remove"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncompacting a path set to contain the minimal number of paths necessary to contain all paths in the set.", "response": "def compact(paths):\n    \"\"\"Compact a path set to contain the minimal number of paths\n    necessary to contain all paths in the set. If /a/path/ and\n    /a/path/to/a/file.txt are both in the set, leave only the\n    shorter path.\"\"\"\n\n    sep = os.path.sep\n    short_paths = set()\n    for path in sorted(paths, key=len):\n        should_skip = any(\n            path.startswith(shortpath.rstrip(\"*\")) and\n            path[len(shortpath.rstrip(\"*\").rstrip(sep))] == sep\n            for shortpath in short_paths\n        )\n        if not should_skip:\n            short_paths.add(path)\n    return short_paths"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a set containing the paths that need to be renamed.", "response": "def compress_for_rename(paths):\n    \"\"\"Returns a set containing the paths that need to be renamed.\n\n    This set may include directories when the original sequence of paths\n    included every file on disk.\n    \"\"\"\n    case_map = dict((os.path.normcase(p), p) for p in paths)\n    remaining = set(case_map)\n    unchecked = sorted(set(os.path.split(p)[0]\n                           for p in case_map.values()), key=len)\n    wildcards = set()\n\n    def norm_join(*a):\n        return os.path.normcase(os.path.join(*a))\n\n    for root in unchecked:\n        if any(os.path.normcase(root).startswith(w)\n               for w in wildcards):\n            # This directory has already been handled.\n            continue\n\n        all_files = set()\n        all_subdirs = set()\n        for dirname, subdirs, files in os.walk(root):\n            all_subdirs.update(norm_join(root, dirname, d)\n                               for d in subdirs)\n            all_files.update(norm_join(root, dirname, f)\n                             for f in files)\n        # If all the files we found are in our remaining set of files to\n        # remove, then remove them from the latter set and add a wildcard\n        # for the directory.\n        if not (all_files - remaining):\n            remaining.difference_update(all_files)\n            wildcards.add(root + os.sep)\n\n    return set(map(case_map.__getitem__, remaining)) | wildcards"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compress_for_output_listing(paths):\n\n    will_remove = list(paths)\n    will_skip = set()\n\n    # Determine folders and files\n    folders = set()\n    files = set()\n    for path in will_remove:\n        if path.endswith(\".pyc\"):\n            continue\n        if path.endswith(\"__init__.py\") or \".dist-info\" in path:\n            folders.add(os.path.dirname(path))\n        files.add(path)\n\n    _normcased_files = set(map(os.path.normcase, files))\n\n    folders = compact(folders)\n\n    # This walks the tree using os.walk to not miss extra folders\n    # that might get added.\n    for folder in folders:\n        for dirpath, _, dirfiles in os.walk(folder):\n            for fname in dirfiles:\n                if fname.endswith(\".pyc\"):\n                    continue\n\n                file_ = os.path.join(dirpath, fname)\n                if (os.path.isfile(file_) and\n                        os.path.normcase(file_) not in _normcased_files):\n                    # We are skipping this file. Add it to the set.\n                    will_skip.add(file_)\n\n    will_remove = files | {\n        os.path.join(folder, \"*\") for folder in folders\n    }\n\n    return will_remove, will_skip", "response": "This function compresses the list of paths to display to a user - specified set of paths that would be deleted and files that would be skipped in the package."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nstashing a directory. Directories are stashed adjacent to their original location if possible or else moved into the user s temp dir if possible.", "response": "def _get_directory_stash(self, path):\n        \"\"\"Stashes a directory.\n\n        Directories are stashed adjacent to their original location if\n        possible, or else moved/copied into the user's temp dir.\"\"\"\n\n        try:\n            save_dir = AdjacentTempDirectory(path)\n            save_dir.create()\n        except OSError:\n            save_dir = TempDirectory(kind=\"uninstall\")\n            save_dir.create()\n        self._save_dirs[os.path.normcase(path)] = save_dir\n\n        return save_dir.path"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstashing a file. AttributeNames.", "response": "def _get_file_stash(self, path):\n        \"\"\"Stashes a file.\n\n        If no root has been provided, one will be created for the directory\n        in the user's temp directory.\"\"\"\n        path = os.path.normcase(path)\n        head, old_head = os.path.dirname(path), None\n        save_dir = None\n\n        while head != old_head:\n            try:\n                save_dir = self._save_dirs[head]\n                break\n            except KeyError:\n                pass\n            head, old_head = os.path.dirname(head), head\n        else:\n            # Did not find any suitable root\n            head = os.path.dirname(path)\n            save_dir = TempDirectory(kind='uninstall')\n            save_dir.create()\n            self._save_dirs[head] = save_dir\n\n        relpath = os.path.relpath(path, head)\n        if relpath and relpath != os.path.curdir:\n            return os.path.join(save_dir.path, relpath)\n        return save_dir.path"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef stash(self, path):\n        if os.path.isdir(path):\n            new_path = self._get_directory_stash(path)\n        else:\n            new_path = self._get_file_stash(path)\n\n        self._moves.append((path, new_path))\n        if os.path.isdir(path) and os.path.isdir(new_path):\n            # If we're moving a directory, we need to\n            # remove the destination first or else it will be\n            # moved to inside the existing directory.\n            # We just created new_path ourselves, so it will\n            # be removable.\n            os.rmdir(new_path)\n        renames(path, new_path)\n        return new_path", "response": "Stashes the directory or file and returns its new location."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncommit the uninstall by removing stashed files.", "response": "def commit(self):\n        \"\"\"Commits the uninstall by removing stashed files.\"\"\"\n        for _, save_dir in self._save_dirs.items():\n            save_dir.cleanup()\n        self._moves = []\n        self._save_dirs = {}"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nundoes the uninstall by moving stashed files back.", "response": "def rollback(self):\n        \"\"\"Undoes the uninstall by moving stashed files back.\"\"\"\n        for p in self._moves:\n            logging.info(\"Moving to %s\\n from %s\", *p)\n\n        for new_path, path in self._moves:\n            try:\n                logger.debug('Replacing %s from %s', new_path, path)\n                if os.path.isfile(new_path):\n                    os.unlink(new_path)\n                elif os.path.isdir(new_path):\n                    rmtree(new_path)\n                renames(path, new_path)\n            except OSError as ex:\n                logger.error(\"Failed to restore %s\", new_path)\n                logger.debug(\"Exception: %s\", ex)\n\n        self.commit()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nremove paths in self. paths with confirmation ( unless auto_confirm is True.", "response": "def remove(self, auto_confirm=False, verbose=False):\n        \"\"\"Remove paths in ``self.paths`` with confirmation (unless\n        ``auto_confirm`` is True).\"\"\"\n\n        if not self.paths:\n            logger.info(\n                \"Can't uninstall '%s'. No files were found to uninstall.\",\n                self.dist.project_name,\n            )\n            return\n\n        dist_name_version = (\n            self.dist.project_name + \"-\" + self.dist.version\n        )\n        logger.info('Uninstalling %s:', dist_name_version)\n\n        with indent_log():\n            if auto_confirm or self._allowed_to_proceed(verbose):\n                moved = self._moved_paths\n\n                for_rename = compress_for_rename(self.paths)\n\n                for path in sorted(compact(for_rename)):\n                    moved.stash(path)\n                    logger.debug('Removing file or directory %s', path)\n\n                for pth in self.pth.values():\n                    pth.remove()\n\n                logger.info('Successfully uninstalled %s', dist_name_version)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndisplaying which files would be deleted and prompt for confirmation", "response": "def _allowed_to_proceed(self, verbose):\n        \"\"\"Display which files would be deleted and prompt for confirmation\n        \"\"\"\n\n        def _display(msg, paths):\n            if not paths:\n                return\n\n            logger.info(msg)\n            with indent_log():\n                for path in sorted(compact(paths)):\n                    logger.info(path)\n\n        if not verbose:\n            will_remove, will_skip = compress_for_output_listing(self.paths)\n        else:\n            # In verbose mode, display all the files that are going to be\n            # deleted.\n            will_remove = list(self.paths)\n            will_skip = set()\n\n        _display('Would remove:', will_remove)\n        _display('Would not remove (might be manually added):', will_skip)\n        _display('Would not remove (outside of prefix):', self._refuse)\n        if verbose:\n            _display('Will actually move:', compress_for_rename(self.paths))\n\n        return ask('Proceed (y/n)? ', ('y', 'n')) == 'y'"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rollback(self):\n        if not self._moved_paths.can_rollback:\n            logger.error(\n                \"Can't roll back %s; was not uninstalled\",\n                self.dist.project_name,\n            )\n            return False\n        logger.info('Rolling back uninstall of %s', self.dist.project_name)\n        self._moved_paths.rollback()\n        for pth in self.pth.values():\n            pth.rollback()", "response": "Rollback the changes made by remove."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a namedtuple of author name and email.", "response": "def author(self):\n        \"\"\"\n            >>> package = yarg.get('yarg')\n            >>> package.author\n            Author(name=u'Kura', email=u'kura@kura.io')\n        \"\"\"\n        author = namedtuple('Author', 'name email')\n        return author(name=self._package['author'],\n                      email=self._package['author_email'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef maintainer(self):\n        maintainer = namedtuple('Maintainer', 'name email')\n        return maintainer(name=self._package['maintainer'],\n                          email=self._package['maintainer_email'])", "response": "Return a new Maintainer instance for the current user."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef license_from_classifiers(self):\n        if len(self.classifiers) > 0:\n            for c in self.classifiers:\n                if c.startswith(\"License\"):\n                    return c.split(\" :: \")[-1]", "response": "Return the License from the classifiers."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef downloads(self):\n        _downloads = self._package['downloads']\n        downloads = namedtuple('Downloads', 'day week month')\n        return downloads(day=_downloads['last_day'],\n                         week=_downloads['last_week'],\n                         month=_downloads['last_month'])", "response": "Get a namedtuple of downloads for the current locale."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef python_versions(self):\n        version_re = re.compile(r\"\"\"Programming Language \\:\\: \"\"\"\n                                \"\"\"Python \\:\\: \\d\\.\\d\"\"\")\n        return [c.split(' :: ')[-1] for c in self.classifiers\n                if version_re.match(c)]", "response": "Return a list of Python version strings that are part of the release classifiers that are part of the release."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef release_ids(self):\n        r = [(k, self._releases[k][0]['upload_time'])\n             for k in self._releases.keys()\n             if len(self._releases[k]) > 0]\n        return [k[0] for k in sorted(r, key=lambda k: k[1])]", "response": "Return a list of all release IDs that are available on the system."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef release(self, release_id):\n        if release_id not in self.release_ids:\n            return None\n        return [Release(release_id, r) for r in self._releases[release_id]]", "response": "Return a list of Release objects for each file in a PyPI release."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_executable_file(path):\n    # follow symlinks,\n    fpath = os.path.realpath(path)\n\n    if not os.path.isfile(fpath):\n        # non-files (directories, fifo, etc.)\n        return False\n\n    mode = os.stat(fpath).st_mode\n\n    if (sys.platform.startswith('sunos')\n            and os.getuid() == 0):\n        # When root on Solaris, os.X_OK is True for *all* files, irregardless\n        # of their executability -- instead, any permission bit of any user,\n        # group, or other is fine enough.\n        #\n        # (This may be true for other \"Unix98\" OS's such as HP-UX and AIX)\n        return bool(mode & (stat.S_IXUSR |\n                            stat.S_IXGRP |\n                            stat.S_IXOTH))\n\n    return os.access(fpath, os.X_OK)", "response": "Checks that the given path is an executable regular file or a symlink towards one."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef which(filename, env=None):\n    '''This takes a given filename; tries to find it in the environment path;\n    then checks if it is executable. This returns the full path to the filename\n    if found and executable. Otherwise this returns None.'''\n\n    # Special case where filename contains an explicit path.\n    if os.path.dirname(filename) != '' and is_executable_file(filename):\n        return filename\n    if env is None:\n        env = os.environ\n    p = env.get('PATH')\n    if not p:\n        p = os.defpath\n    pathlist = p.split(os.pathsep)\n    for path in pathlist:\n        ff = os.path.join(path, filename)\n        if is_executable_file(ff):\n            return ff\n    return None", "response": "This takes a given filename and checks if it is executable. This returns the full path to the filename if found and None otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef select_ignore_interrupts(iwtd, owtd, ewtd, timeout=None):\n\n    '''This is a wrapper around select.select() that ignores signals. If\n    select.select raises a select.error exception and errno is an EINTR\n    error then it is ignored. Mainly this is used to ignore sigwinch\n    (terminal resize). '''\n\n    # if select() is interrupted by a signal (errno==EINTR) then\n    # we loop back and enter the select() again.\n    if timeout is not None:\n        end_time = time.time() + timeout\n    while True:\n        try:\n            return select.select(iwtd, owtd, ewtd, timeout)\n        except InterruptedError:\n            err = sys.exc_info()[1]\n            if err.args[0] == errno.EINTR:\n                # if we loop back we have to subtract the\n                # amount of time we already waited.\n                if timeout is not None:\n                    timeout = end_time - time.time()\n                    if timeout < 0:\n                        return([], [], [])\n            else:\n                # something else caused the select.error, so\n                # this actually is an exception.\n                raise", "response": "This is a wrapper around select. select that ignores signals."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef poll_ignore_interrupts(fds, timeout=None):\n    '''Simple wrapper around poll to register file descriptors and\n    ignore signals.'''\n\n    if timeout is not None:\n        end_time = time.time() + timeout\n\n    poller = select.poll()\n    for fd in fds:\n        poller.register(fd, select.POLLIN | select.POLLPRI | select.POLLHUP | select.POLLERR)\n\n    while True:\n        try:\n            timeout_ms = None if timeout is None else timeout * 1000\n            results = poller.poll(timeout_ms)\n            return [afd for afd, _ in results]\n        except InterruptedError:\n            err = sys.exc_info()[1]\n            if err.args[0] == errno.EINTR:\n                # if we loop back we have to subtract the\n                # amount of time we already waited.\n                if timeout is not None:\n                    timeout = end_time - time.time()\n                    if timeout < 0:\n                        return []\n            else:\n                # something else caused the select.error, so\n                # this actually is an exception.\n                raise", "response": "Simple wrapper around poll to register file descriptors and\n            ignore signals."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntrying to suggest a semantic version for a version for which _suggest_normalized_version couldn t come up with anything.", "response": "def _suggest_semantic_version(s):\n    \"\"\"\n    Try to suggest a semantic form for a version for which\n    _suggest_normalized_version couldn't come up with anything.\n    \"\"\"\n    result = s.strip().lower()\n    for pat, repl in _REPLACEMENTS:\n        result = pat.sub(repl, result)\n    if not result:\n        result = '0.0.0'\n\n    # Now look for numeric prefix, and separate it out from\n    # the rest.\n    #import pdb; pdb.set_trace()\n    m = _NUMERIC_PREFIX.match(result)\n    if not m:\n        prefix = '0.0.0'\n        suffix = result\n    else:\n        prefix = m.groups()[0].split('.')\n        prefix = [int(i) for i in prefix]\n        while len(prefix) < 3:\n            prefix.append(0)\n        if len(prefix) == 3:\n            suffix = result[m.end():]\n        else:\n            suffix = '.'.join([str(i) for i in prefix[3:]]) + result[m.end():]\n            prefix = prefix[:3]\n        prefix = '.'.join([str(i) for i in prefix])\n        suffix = suffix.strip()\n    if suffix:\n        #import pdb; pdb.set_trace()\n        # massage the suffix.\n        for pat, repl in _SUFFIX_REPLACEMENTS:\n            suffix = pat.sub(repl, suffix)\n\n    if not suffix:\n        result = prefix\n    else:\n        sep = '-' if 'dev' in suffix else '+'\n        result = prefix + sep + suffix\n    if not is_semver(result):\n        result = None\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _suggest_normalized_version(s):\n    try:\n        _normalized_key(s)\n        return s   # already rational\n    except UnsupportedVersionError:\n        pass\n\n    rs = s.lower()\n\n    # part of this could use maketrans\n    for orig, repl in (('-alpha', 'a'), ('-beta', 'b'), ('alpha', 'a'),\n                       ('beta', 'b'), ('rc', 'c'), ('-final', ''),\n                       ('-pre', 'c'),\n                       ('-release', ''), ('.release', ''), ('-stable', ''),\n                       ('+', '.'), ('_', '.'), (' ', ''), ('.final', ''),\n                       ('final', '')):\n        rs = rs.replace(orig, repl)\n\n    # if something ends with dev or pre, we add a 0\n    rs = re.sub(r\"pre$\", r\"pre0\", rs)\n    rs = re.sub(r\"dev$\", r\"dev0\", rs)\n\n    # if we have something like \"b-2\" or \"a.2\" at the end of the\n    # version, that is probably beta, alpha, etc\n    # let's remove the dash or dot\n    rs = re.sub(r\"([abc]|rc)[\\-\\.](\\d+)$\", r\"\\1\\2\", rs)\n\n    # 1.0-dev-r371 -> 1.0.dev371\n    # 0.1-dev-r79 -> 0.1.dev79\n    rs = re.sub(r\"[\\-\\.](dev)[\\-\\.]?r?(\\d+)$\", r\".\\1\\2\", rs)\n\n    # Clean: 2.0.a.3, 2.0.b1, 0.9.0~c1\n    rs = re.sub(r\"[.~]?([abc])\\.?\", r\"\\1\", rs)\n\n    # Clean: v0.3, v1.0\n    if rs.startswith('v'):\n        rs = rs[1:]\n\n    # Clean leading '0's on numbers.\n    #TODO: unintended side-effect on, e.g., \"2003.05.09\"\n    # PyPI stats: 77 (~2%) better\n    rs = re.sub(r\"\\b0+(\\d+)(?!\\d)\", r\"\\1\", rs)\n\n    # Clean a/b/c with no version. E.g. \"1.0a\" -> \"1.0a0\". Setuptools infers\n    # zero.\n    # PyPI stats: 245 (7.56%) better\n    rs = re.sub(r\"(\\d+[abc])$\", r\"\\g<1>0\", rs)\n\n    # the 'dev-rNNN' tag is a dev tag\n    rs = re.sub(r\"\\.?(dev-r|dev\\.r)\\.?(\\d+)$\", r\".dev\\2\", rs)\n\n    # clean the - when used as a pre delimiter\n    rs = re.sub(r\"-(a|b|c)(\\d+)$\", r\"\\1\\2\", rs)\n\n    # a terminal \"dev\" or \"devel\" can be changed into \".dev0\"\n    rs = re.sub(r\"[\\.\\-](dev|devel)$\", r\".dev0\", rs)\n\n    # a terminal \"dev\" can be changed into \".dev0\"\n    rs = re.sub(r\"(?![\\.\\-])dev$\", r\".dev0\", rs)\n\n    # a terminal \"final\" or \"stable\" can be removed\n    rs = re.sub(r\"(final|stable)$\", \"\", rs)\n\n    # The 'r' and the '-' tags are post release tags\n    #   0.4a1.r10       ->  0.4a1.post10\n    #   0.9.33-17222    ->  0.9.33.post17222\n    #   0.9.33-r17222   ->  0.9.33.post17222\n    rs = re.sub(r\"\\.?(r|-|-r)\\.?(\\d+)$\", r\".post\\2\", rs)\n\n    # Clean 'r' instead of 'dev' usage:\n    #   0.9.33+r17222   ->  0.9.33.dev17222\n    #   1.0dev123       ->  1.0.dev123\n    #   1.0.git123      ->  1.0.dev123\n    #   1.0.bzr123      ->  1.0.dev123\n    #   0.1a0dev.123    ->  0.1a0.dev123\n    # PyPI stats:  ~150 (~4%) better\n    rs = re.sub(r\"\\.?(dev|git|bzr)\\.?(\\d+)$\", r\".dev\\2\", rs)\n\n    # Clean '.pre' (normalized from '-pre' above) instead of 'c' usage:\n    #   0.2.pre1        ->  0.2c1\n    #   0.2-c1         ->  0.2c1\n    #   1.0preview123   ->  1.0c123\n    # PyPI stats: ~21 (0.62%) better\n    rs = re.sub(r\"\\.?(pre|preview|-c)(\\d+)$\", r\"c\\g<2>\", rs)\n\n    # Tcl/Tk uses \"px\" for their post release markers\n    rs = re.sub(r\"p(\\d+)$\", r\".post\\1\", rs)\n\n    try:\n        _normalized_key(rs)\n    except UnsupportedVersionError:\n        rs = None\n    return rs", "response": "Suggest a normalized version close to the given version string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if the provided version matches the constraints.", "response": "def match(self, version):\n        \"\"\"\n        Check if the provided version matches the constraints.\n\n        :param version: The version to match against this instance.\n        :type version: String or :class:`Version` instance.\n        \"\"\"\n        if isinstance(version, string_types):\n            version = self.version_class(version)\n        for operator, constraint, prefix in self._parts:\n            f = self._operators.get(operator)\n            if isinstance(f, string_types):\n                f = getattr(self, f)\n            if not f:\n                msg = ('%r not implemented '\n                       'for %s' % (operator, self.__class__.__name__))\n                raise NotImplementedError(msg)\n            if not f(version, constraint, prefix):\n                return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd or updates a key to the given. env file.", "response": "def set_key(dotenv_path, key_to_set, value_to_set, quote_mode=\"always\"):\n    \"\"\"\n    Adds or Updates a key/value to the given .env\n\n    If the .env path given doesn't exist, fails instead of risking creating\n    an orphan .env somewhere in the filesystem\n    \"\"\"\n    value_to_set = value_to_set.strip(\"'\").strip('\"')\n    if not os.path.exists(dotenv_path):\n        warnings.warn(\"can't write to %s - it doesn't exist.\" % dotenv_path)\n        return None, key_to_set, value_to_set\n\n    if \" \" in value_to_set:\n        quote_mode = \"always\"\n\n    line_template = '{}=\"{}\"\\n' if quote_mode == \"always\" else '{}={}\\n'\n    line_out = line_template.format(key_to_set, value_to_set)\n\n    with rewrite(dotenv_path) as (source, dest):\n        replaced = False\n        for mapping in parse_stream(source):\n            if mapping.key == key_to_set:\n                dest.write(line_out)\n                replaced = True\n            else:\n                dest.write(mapping.original)\n        if not replaced:\n            dest.write(line_out)\n\n    return True, key_to_set, value_to_set"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef unset_key(dotenv_path, key_to_unset, quote_mode=\"always\"):\n    if not os.path.exists(dotenv_path):\n        warnings.warn(\"can't delete from %s - it doesn't exist.\" % dotenv_path)\n        return None, key_to_unset\n\n    removed = False\n    with rewrite(dotenv_path) as (source, dest):\n        for mapping in parse_stream(source):\n            if mapping.key == key_to_unset:\n                removed = True\n            else:\n                dest.write(mapping.original)\n\n    if not removed:\n        warnings.warn(\"key %s not removed from %s - key doesn't exist.\" % (key_to_unset, dotenv_path))\n        return None, key_to_unset\n\n    return removed, key_to_unset", "response": "Removes a given key from the given. env file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nyield the directories starting from the given path up to the root ArcGIS s directory structure.", "response": "def _walk_to_root(path):\n    \"\"\"\n    Yield directories starting from the given directory up to the root\n    \"\"\"\n    if not os.path.exists(path):\n        raise IOError('Starting path not found')\n\n    if os.path.isfile(path):\n        path = os.path.dirname(path)\n\n    last_dir = None\n    current_dir = os.path.abspath(path)\n    while last_dir != current_dir:\n        yield current_dir\n        parent_dir = os.path.abspath(os.path.join(current_dir, os.path.pardir))\n        last_dir, current_dir = current_dir, parent_dir"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run_command(command, env):\n    # copy the current environment variables and add the vales from\n    # `env`\n    cmd_env = os.environ.copy()\n    cmd_env.update(env)\n\n    p = Popen(command,\n              universal_newlines=True,\n              bufsize=0,\n              shell=False,\n              env=cmd_env)\n    _, _ = p.communicate()\n\n    return p.returncode", "response": "Run a command in a sub process with the variables from env added in the current environment variables."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dict(self):\n        if self._dict:\n            return self._dict\n\n        values = OrderedDict(self.parse())\n        self._dict = resolve_nested_variables(values)\n        return self._dict", "response": "Return dotenv as dict"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_as_environment_variables(self, override=False):\n        for k, v in self.dict().items():\n            if k in os.environ and not override:\n                continue\n            # With Python2 on Windows, force environment variables to str to avoid\n            # \"TypeError: environment can only contain strings\" in Python's subprocess.py.\n            if PY2 and WIN:\n                if isinstance(k, text_type) or isinstance(v, text_type):\n                    k = k.encode('ascii')\n                    v = v.encode('ascii')\n            os.environ[k] = v\n\n        return True", "response": "Load the current dotenv as system environemt variable."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget an all - lowercase version of the requirement s name.", "response": "def _key_from_req(req):\n    \"\"\"Get an all-lowercase version of the requirement's name.\"\"\"\n    if hasattr(req, 'key'):\n        # from pkg_resources, such as installed dists for pip-sync\n        key = req.key\n    else:\n        # from packaging, such as install requirements from requirements.txt\n        key = req.name\n\n    key = key.replace('_', '-').lower()\n    return key"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_cache(self):\n        if os.path.exists(self._cache_file):\n            self._cache = _read_cache_file(self._cache_file)\n        else:\n            self._cache = {}", "response": "Reads the cached contents into memory."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmonkey - patch urllib3 with PyOpenSSL - backed SSL - support.", "response": "def inject_into_urllib3():\n    'Monkey-patch urllib3 with PyOpenSSL-backed SSL-support.'\n\n    _validate_dependencies_met()\n\n    util.ssl_.SSLContext = PyOpenSSLContext\n    util.HAS_SNI = HAS_SNI\n    util.ssl_.HAS_SNI = HAS_SNI\n    util.IS_PYOPENSSL = True\n    util.ssl_.IS_PYOPENSSL = True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nverify that PyOpenSSL s package - level dependencies have been met.", "response": "def _validate_dependencies_met():\n    \"\"\"\n    Verifies that PyOpenSSL's package-level dependencies have been met.\n    Throws `ImportError` if they are not met.\n    \"\"\"\n    # Method added in `cryptography==1.1`; not available in older versions\n    from cryptography.x509.extensions import Extensions\n    if getattr(Extensions, \"get_extension_for_class\", None) is None:\n        raise ImportError(\"'cryptography' module missing required functionality.  \"\n                          \"Try upgrading to v1.3.4 or newer.\")\n\n    # pyOpenSSL 0.14 and above use cryptography for OpenSSL bindings. The _x509\n    # attribute is only present on those versions.\n    from OpenSSL.crypto import X509\n    x509 = X509()\n    if getattr(x509, \"_x509\", None) is None:\n        raise ImportError(\"'pyOpenSSL' module missing required functionality. \"\n                          \"Try upgrading to v0.14 or newer.\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _dnsname_to_stdlib(name):\n    def idna_encode(name):\n        \"\"\"\n        Borrowed wholesale from the Python Cryptography Project. It turns out\n        that we can't just safely call `idna.encode`: it can explode for\n        wildcard names. This avoids that problem.\n        \"\"\"\n        from pipenv.patched.notpip._vendor import idna\n\n        try:\n            for prefix in [u'*.', u'.']:\n                if name.startswith(prefix):\n                    name = name[len(prefix):]\n                    return prefix.encode('ascii') + idna.encode(name)\n            return idna.encode(name)\n        except idna.core.IDNAError:\n            return None\n\n    name = idna_encode(name)\n    if name is None:\n        return None\n    elif sys.version_info >= (3, 0):\n        name = name.decode('utf-8')\n    return name", "response": "Converts a DNSName SubjectAlternativeName field to the form used by the standard library on Python 3."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_subj_alt_name(peer_cert):\n    # Pass the cert to cryptography, which has much better APIs for this.\n    if hasattr(peer_cert, \"to_cryptography\"):\n        cert = peer_cert.to_cryptography()\n    else:\n        # This is technically using private APIs, but should work across all\n        # relevant versions before PyOpenSSL got a proper API for this.\n        cert = _Certificate(openssl_backend, peer_cert._x509)\n\n    # We want to find the SAN extension. Ask Cryptography to locate it (it's\n    # faster than looping in Python)\n    try:\n        ext = cert.extensions.get_extension_for_class(\n            x509.SubjectAlternativeName\n        ).value\n    except x509.ExtensionNotFound:\n        # No such extension, return the empty list.\n        return []\n    except (x509.DuplicateExtension, UnsupportedExtension,\n            x509.UnsupportedGeneralNameType, UnicodeError) as e:\n        # A problem has been found with the quality of the certificate. Assume\n        # no SAN field is present.\n        log.warning(\n            \"A problem was encountered with the certificate that prevented \"\n            \"urllib3 from finding the SubjectAlternativeName field. This can \"\n            \"affect certificate validation. The error was %s\",\n            e,\n        )\n        return []\n\n    # We want to return dNSName and iPAddress fields. We need to cast the IPs\n    # back to strings because the match_hostname function wants them as\n    # strings.\n    # Sadly the DNS names need to be idna encoded and then, on Python 3, UTF-8\n    # decoded. This is pretty frustrating, but that's what the standard library\n    # does with certificates, and so we need to attempt to do the same.\n    # We also want to skip over names which cannot be idna encoded.\n    names = [\n        ('DNS', name) for name in map(_dnsname_to_stdlib, ext.get_values_for_type(x509.DNSName))\n        if name is not None\n    ]\n    names.extend(\n        ('IP Address', str(name))\n        for name in ext.get_values_for_type(x509.IPAddress)\n    )\n\n    return names", "response": "Given a PyOpenSSL certificate provides all the subject alternative names."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfeed a character with known length", "response": "def feed(self, char, char_len):\n        \"\"\"feed a character with known length\"\"\"\n        if char_len == 2:\n            # we only care about 2-bytes character in our distribution analysis\n            order = self.get_order(char)\n        else:\n            order = -1\n        if order >= 0:\n            self._total_chars += 1\n            # order is valid\n            if order < self._table_size:\n                if 512 > self._char_to_freq_order[order]:\n                    self._freq_chars += 1"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn confidence based on existing data", "response": "def get_confidence(self):\n        \"\"\"return confidence based on existing data\"\"\"\n        # if we didn't receive any character in our consideration range,\n        # return negative answer\n        if self._total_chars <= 0 or self._freq_chars <= self.MINIMUM_DATA_THRESHOLD:\n            return self.SURE_NO\n\n        if self._total_chars != self._freq_chars:\n            r = (self._freq_chars / ((self._total_chars - self._freq_chars)\n                 * self.typical_distribution_ratio))\n            if r < self.SURE_YES:\n                return r\n\n        # normalize confidence (we don't want to be 100% sure)\n        return self.SURE_YES"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting all outline tables to inline tables.", "response": "def convert_toml_outline_tables(parsed):\n    \"\"\"Converts all outline tables to inline tables.\"\"\"\n    def convert_tomlkit_table(section):\n        for key, value in section._body:\n            if not key:\n                continue\n            if hasattr(value, \"keys\") and not isinstance(value, tomlkit.items.InlineTable):\n                table = tomlkit.inline_table()\n                table.update(value.value)\n                section[key.key] = table\n\n    def convert_toml_table(section):\n        for package, value in section.items():\n            if hasattr(value, \"keys\") and not isinstance(value, toml.decoder.InlineTableDict):\n                table = toml.TomlDecoder().get_empty_inline_table()\n                table.update(value)\n                section[package] = table\n\n    is_tomlkit_parsed = isinstance(parsed, tomlkit.container.Container)\n    for section in (\"packages\", \"dev-packages\"):\n        table_data = parsed.get(section, {})\n        if not table_data:\n            continue\n        if is_tomlkit_parsed:\n            convert_tomlkit_table(table_data)\n        else:\n            convert_toml_table(table_data)\n\n    return parsed"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run_command(cmd, *args, **kwargs):\n\n    from pipenv.vendor import delegator\n    from ._compat import decode_for_output\n    from .cmdparse import Script\n    catch_exceptions = kwargs.pop(\"catch_exceptions\", True)\n    if isinstance(cmd, (six.string_types, list, tuple)):\n        cmd = Script.parse(cmd)\n    if not isinstance(cmd, Script):\n        raise TypeError(\"Command input must be a string, list or tuple\")\n    if \"env\" not in kwargs:\n        kwargs[\"env\"] = os.environ.copy()\n    kwargs[\"env\"][\"PYTHONIOENCODING\"] = \"UTF-8\"\n    try:\n        cmd_string = cmd.cmdify()\n    except TypeError:\n        click_echo(\"Error turning command into string: {0}\".format(cmd), err=True)\n        sys.exit(1)\n    if environments.is_verbose():\n        click_echo(\"Running command: $ {0}\".format(cmd_string, err=True))\n    c = delegator.run(cmd_string, *args, **kwargs)\n    return_code = c.return_code\n    if environments.is_verbose():\n        click_echo(\"Command output: {0}\".format(\n            crayons.blue(decode_for_output(c.out))\n        ), err=True)\n    if not c.ok and catch_exceptions:\n        raise PipenvCmdError(cmd_string, c.out, c.err, return_code)\n    return c", "response": "Runs a command and returns the output and error."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing a Python version output returned by python - version.", "response": "def parse_python_version(output):\n    \"\"\"Parse a Python version output returned by `python --version`.\n\n    Return a dict with three keys: major, minor, and micro. Each value is a\n    string containing a version part.\n\n    Note: The micro part would be `'0'` if it's missing from the input string.\n    \"\"\"\n    version_line = output.split(\"\\n\", 1)[0]\n    version_pattern = re.compile(\n        r\"\"\"\n        ^                   # Beginning of line.\n        Python              # Literally \"Python\".\n        \\s                  # Space.\n        (?P<major>\\d+)      # Major = one or more digits.\n        \\.                  # Dot.\n        (?P<minor>\\d+)      # Minor = one or more digits.\n        (?:                 # Unnamed group for dot-micro.\n            \\.              # Dot.\n            (?P<micro>\\d+)  # Micro = one or more digit.\n        )?                  # Micro is optional because pypa/pipenv#1893.\n        .*                  # Trailing garbage.\n        $                   # End of line.\n    \"\"\",\n        re.VERBOSE,\n    )\n\n    match = version_pattern.match(version_line)\n    if not match:\n        return None\n    return match.groupdict(default=\"0\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef escape_grouped_arguments(s):\n    if s is None:\n        return None\n\n    # Additional escaping for windows paths\n    if os.name == \"nt\":\n        s = \"{}\".format(s.replace(\"\\\\\", \"\\\\\\\\\"))\n    return '\"' + s.replace(\"'\", \"'\\\\''\") + '\"'", "response": "Prepares a string for the shell"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef venv_resolve_deps(\n    deps,\n    which,\n    project,\n    pre=False,\n    clear=False,\n    allow_global=False,\n    pypi_mirror=None,\n    dev=False,\n    pipfile=None,\n    lockfile=None,\n    keep_outdated=False\n):\n    \"\"\"\n    Resolve dependencies for a pipenv project, acts as a portal to the target environment.\n\n    Regardless of whether a virtual environment is present or not, this will spawn\n    a subproces which is isolated to the target environment and which will perform\n    dependency resolution.  This function reads the output of that call and mutates\n    the provided lockfile accordingly, returning nothing.\n\n    :param List[:class:`~requirementslib.Requirement`] deps: A list of dependencies to resolve.\n    :param Callable which: [description]\n    :param project: The pipenv Project instance to use during resolution\n    :param Optional[bool] pre: Whether to resolve pre-release candidates, defaults to False\n    :param Optional[bool] clear: Whether to clear the cache during resolution, defaults to False\n    :param Optional[bool] allow_global: Whether to use *sys.executable* as the python binary, defaults to False\n    :param Optional[str] pypi_mirror: A URL to substitute any time *pypi.org* is encountered, defaults to None\n    :param Optional[bool] dev: Whether to target *dev-packages* or not, defaults to False\n    :param pipfile: A Pipfile section to operate on, defaults to None\n    :type pipfile: Optional[Dict[str, Union[str, Dict[str, bool, List[str]]]]]\n    :param Dict[str, Any] lockfile: A project lockfile to mutate, defaults to None\n    :param bool keep_outdated: Whether to retain outdated dependencies and resolve with them in mind, defaults to False\n    :raises RuntimeError: Raised on resolution failure\n    :return: Nothing\n    :rtype: None\n    \"\"\"\n\n    from .vendor.vistir.misc import fs_str\n    from .vendor.vistir.compat import Path, JSONDecodeError, NamedTemporaryFile\n    from .vendor.vistir.path import create_tracked_tempdir\n    from . import resolver\n    from ._compat import decode_for_output\n    import json\n\n    results = []\n    pipfile_section = \"dev-packages\" if dev else \"packages\"\n    lockfile_section = \"develop\" if dev else \"default\"\n    if not deps:\n        if not project.pipfile_exists:\n            return None\n        deps = project.parsed_pipfile.get(pipfile_section, {})\n    if not deps:\n        return None\n\n    if not pipfile:\n        pipfile = getattr(project, pipfile_section, {})\n    if not lockfile:\n        lockfile = project._lockfile\n    req_dir = create_tracked_tempdir(prefix=\"pipenv\", suffix=\"requirements\")\n    cmd = [\n        which(\"python\", allow_global=allow_global),\n        Path(resolver.__file__.rstrip(\"co\")).as_posix()\n    ]\n    if pre:\n        cmd.append(\"--pre\")\n    if clear:\n        cmd.append(\"--clear\")\n    if allow_global:\n        cmd.append(\"--system\")\n    if dev:\n        cmd.append(\"--dev\")\n    target_file = NamedTemporaryFile(prefix=\"resolver\", suffix=\".json\", delete=False)\n    target_file.close()\n    cmd.extend([\"--write\", make_posix(target_file.name)])\n    with temp_environ():\n        os.environ.update({fs_str(k): fs_str(val) for k, val in os.environ.items()})\n        if pypi_mirror:\n            os.environ[\"PIPENV_PYPI_MIRROR\"] = str(pypi_mirror)\n        os.environ[\"PIPENV_VERBOSITY\"] = str(environments.PIPENV_VERBOSITY)\n        os.environ[\"PIPENV_REQ_DIR\"] = fs_str(req_dir)\n        os.environ[\"PIP_NO_INPUT\"] = fs_str(\"1\")\n        os.environ[\"PIPENV_SITE_DIR\"] = get_pipenv_sitedir()\n        if keep_outdated:\n            os.environ[\"PIPENV_KEEP_OUTDATED\"] = fs_str(\"1\")\n        with create_spinner(text=decode_for_output(\"Locking...\")) as sp:\n            # This conversion is somewhat slow on local and file-type requirements since\n            # we now download those requirements / make temporary folders to perform\n            # dependency resolution on them, so we are including this step inside the\n            # spinner context manager for the UX improvement\n            sp.write(decode_for_output(\"Building requirements...\"))\n            deps = convert_deps_to_pip(\n                deps, project, r=False, include_index=True\n            )\n            constraints = set(deps)\n            os.environ[\"PIPENV_PACKAGES\"] = str(\"\\n\".join(constraints))\n            sp.write(decode_for_output(\"Resolving dependencies...\"))\n            c = resolve(cmd, sp)\n            results = c.out.strip()\n            sp.green.ok(environments.PIPENV_SPINNER_OK_TEXT.format(\"Success!\"))\n    try:\n        with open(target_file.name, \"r\") as fh:\n            results = json.load(fh)\n    except (IndexError, JSONDecodeError):\n        click_echo(c.out.strip(), err=True)\n        click_echo(c.err.strip(), err=True)\n        if os.path.exists(target_file.name):\n            os.unlink(target_file.name)\n        raise RuntimeError(\"There was a problem with locking.\")\n    if os.path.exists(target_file.name):\n        os.unlink(target_file.name)\n    if environments.is_verbose():\n        click_echo(results, err=True)\n\n    if lockfile_section not in lockfile:\n        lockfile[lockfile_section] = {}\n    prepare_lockfile(results, pipfile, lockfile[lockfile_section])", "response": "Resolve dependencies for a virtual environment."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving a list of dependencies return a list of dependencies using pip - tools.", "response": "def resolve_deps(\n    deps,\n    which,\n    project,\n    sources=None,\n    python=False,\n    clear=False,\n    pre=False,\n    allow_global=False,\n    req_dir=None\n):\n    \"\"\"Given a list of dependencies, return a resolved list of dependencies,\n    using pip-tools -- and their hashes, using the warehouse API / pip.\n    \"\"\"\n    index_lookup = {}\n    markers_lookup = {}\n    python_path = which(\"python\", allow_global=allow_global)\n    if not os.environ.get(\"PIP_SRC\"):\n        os.environ[\"PIP_SRC\"] = project.virtualenv_src_location\n    backup_python_path = sys.executable\n    results = []\n    resolver = None\n    if not deps:\n        return results, resolver\n    # First (proper) attempt:\n    req_dir = req_dir if req_dir else os.environ.get(\"req_dir\", None)\n    if not req_dir:\n        from .vendor.vistir.path import create_tracked_tempdir\n        req_dir = create_tracked_tempdir(prefix=\"pipenv-\", suffix=\"-requirements\")\n    with HackedPythonVersion(python_version=python, python_path=python_path):\n        try:\n            results, hashes, markers_lookup, resolver, skipped = actually_resolve_deps(\n                deps,\n                index_lookup,\n                markers_lookup,\n                project,\n                sources,\n                clear,\n                pre,\n                req_dir=req_dir,\n            )\n        except RuntimeError:\n            # Don't exit here, like usual.\n            results = None\n    # Second (last-resort) attempt:\n    if results is None:\n        with HackedPythonVersion(\n            python_version=\".\".join([str(s) for s in sys.version_info[:3]]),\n            python_path=backup_python_path,\n        ):\n            try:\n                # Attempt to resolve again, with different Python version information,\n                # particularly for particularly particular packages.\n                results, hashes, markers_lookup, resolver, skipped = actually_resolve_deps(\n                    deps,\n                    index_lookup,\n                    markers_lookup,\n                    project,\n                    sources,\n                    clear,\n                    pre,\n                    req_dir=req_dir,\n                )\n            except RuntimeError:\n                sys.exit(1)\n    return results, resolver"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef convert_deps_to_pip(deps, project=None, r=True, include_index=True):\n    from .vendor.requirementslib.models.requirements import Requirement\n\n    dependencies = []\n    for dep_name, dep in deps.items():\n        if project:\n            project.clear_pipfile_cache()\n        indexes = getattr(project, \"pipfile_sources\", []) if project is not None else []\n        new_dep = Requirement.from_pipfile(dep_name, dep)\n        if new_dep.index:\n            include_index = True\n        req = new_dep.as_line(sources=indexes if include_index else None).strip()\n        dependencies.append(req)\n    if not r:\n        return dependencies\n\n    # Write requirements.txt to tmp directory.\n    from .vendor.vistir.path import create_tracked_tempfile\n    f = create_tracked_tempfile(suffix=\"-requirements.txt\", delete=False)\n    f.write(\"\\n\".join(dependencies).encode(\"utf-8\"))\n    f.close()\n    return f.name", "response": "Converts a Pipfile - formatted dependency to a pip - formatted one."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_required_version(version, specified_version):\n    # Certain packages may be defined with multiple values.\n    if isinstance(specified_version, dict):\n        specified_version = specified_version.get(\"version\", \"\")\n    if specified_version.startswith(\"==\"):\n        return version.strip() == specified_version.split(\"==\")[1].strip()\n\n    return True", "response": "Check to see if there s a hard requirement for version in Pipfile."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndetermining if a path can potentially be installed", "response": "def is_installable_file(path):\n    \"\"\"Determine if a path can potentially be installed\"\"\"\n    from .vendor.pip_shims.shims import is_installable_dir, is_archive_file\n    from .patched.notpip._internal.utils.packaging import specifiers\n    from ._compat import Path\n\n    if hasattr(path, \"keys\") and any(\n        key for key in path.keys() if key in [\"file\", \"path\"]\n    ):\n        path = urlparse(path[\"file\"]).path if \"file\" in path else path[\"path\"]\n    if not isinstance(path, six.string_types) or path == \"*\":\n        return False\n\n    # If the string starts with a valid specifier operator, test if it is a valid\n    # specifier set before making a path object (to avoid breaking windows)\n    if any(path.startswith(spec) for spec in \"!=<>~\"):\n        try:\n            specifiers.SpecifierSet(path)\n        # If this is not a valid specifier, just move on and try it as a path\n        except specifiers.InvalidSpecifier:\n            pass\n        else:\n            return False\n\n    if not os.path.exists(os.path.abspath(path)):\n        return False\n\n    lookup_path = Path(path)\n    absolute_path = \"{0}\".format(lookup_path.absolute())\n    if lookup_path.is_dir() and is_installable_dir(absolute_path):\n        return True\n\n    elif lookup_path.is_file() and is_archive_file(absolute_path):\n        return True\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndetermining if a package name is for a File dependency.", "response": "def is_file(package):\n    \"\"\"Determine if a package name is for a File dependency.\"\"\"\n    if hasattr(package, \"keys\"):\n        return any(key for key in package.keys() if key in [\"file\", \"path\"])\n\n    if os.path.exists(str(package)):\n        return True\n\n    for start in SCHEME_LIST:\n        if str(package).startswith(start):\n            return True\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pep423_name(name):\n    name = name.lower()\n    if any(i not in name for i in (VCS_LIST + SCHEME_LIST)):\n        return name.replace(\"_\", \"-\")\n\n    else:\n        return name", "response": "Normalize package name to PEP 423 style standard."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef proper_case(package_name):\n    # Hit the simple API.\n    r = _get_requests_session().get(\n        \"https://pypi.org/pypi/{0}/json\".format(package_name), timeout=0.3, stream=True\n    )\n    if not r.ok:\n        raise IOError(\n            \"Unable to find package {0} in PyPI repository.\".format(package_name)\n        )\n\n    r = parse.parse(\"https://pypi.org/pypi/{name}/json\", r.url)\n    good_name = r[\"name\"]\n    return good_name", "response": "Properly case project name from pypi. org."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive an executable name search the given location for an executable", "response": "def find_windows_executable(bin_path, exe_name):\n    \"\"\"Given an executable name, search the given location for an executable\"\"\"\n    requested_path = get_windows_path(bin_path, exe_name)\n    if os.path.isfile(requested_path):\n        return requested_path\n\n    try:\n        pathext = os.environ[\"PATHEXT\"]\n    except KeyError:\n        pass\n    else:\n        for ext in pathext.split(os.pathsep):\n            path = get_windows_path(bin_path, exe_name + ext.strip().lower())\n            if os.path.isfile(path):\n                return path\n\n    return find_executable(exe_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_canonical_names(packages):\n    from .vendor.packaging.utils import canonicalize_name\n\n    if not isinstance(packages, Sequence):\n        if not isinstance(packages, six.string_types):\n            return packages\n        packages = [packages]\n    return set([canonicalize_name(pkg) for pkg in packages if pkg])", "response": "Canonicalize a list of packages and return a set of canonical names"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the path of a Pipfile in parent directories.", "response": "def find_requirements(max_depth=3):\n    \"\"\"Returns the path of a Pipfile in parent directories.\"\"\"\n    i = 0\n    for c, d, f in walk_up(os.getcwd()):\n        i += 1\n        if i < max_depth:\n            if \"requirements.txt\":\n                r = os.path.join(c, \"requirements.txt\")\n                if os.path.isfile(r):\n                    return r\n\n    raise RuntimeError(\"No requirements.txt found!\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef temp_environ():\n    environ = dict(os.environ)\n    try:\n        yield\n\n    finally:\n        os.environ.clear()\n        os.environ.update(environ)", "response": "Temporarily sets the environment variables temporarily."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nallowing the ability to set os.environ temporarily", "response": "def temp_path():\n    \"\"\"Allow the ability to set os.environ temporarily\"\"\"\n    path = [p for p in sys.path]\n    try:\n        yield\n    finally:\n        sys.path = [p for p in path]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if a given string is a valid url", "response": "def is_valid_url(url):\n    \"\"\"Checks if a given string is an url\"\"\"\n    pieces = urlparse(url)\n    return all([pieces.scheme, pieces.netloc])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef download_file(url, filename):\n    r = _get_requests_session().get(url, stream=True)\n    if not r.ok:\n        raise IOError(\"Unable to download file\")\n\n    with open(filename, \"wb\") as f:\n        f.write(r.content)", "response": "Downloads file from url to a path with filename"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef normalize_drive(path):\n    if os.name != \"nt\" or not isinstance(path, six.string_types):\n        return path\n\n    drive, tail = os.path.splitdrive(path)\n    # Only match (lower cased) local drives (e.g. 'c:'), not UNC mounts.\n    if drive.islower() and len(drive) == 2 and drive[1] == \":\":\n        return \"{}{}\".format(drive.upper(), tail)\n\n    return path", "response": "Normalize drive in path so they stay consistent."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_readonly_path(fn):\n    if os.path.exists(fn):\n        return (os.stat(fn).st_mode & stat.S_IREAD) or not os.access(fn, os.W_OK)\n\n    return False", "response": "Check if a provided path exists and is readonly."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef safe_expandvars(value):\n    if isinstance(value, six.string_types):\n        return os.path.expandvars(value)\n    return value", "response": "Call os. path. expandvars if value is a string otherwise do nothing."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntake a pipfile entry and normalize its markers", "response": "def translate_markers(pipfile_entry):\n    \"\"\"Take a pipfile entry and normalize its markers\n\n    Provide a pipfile entry which may have 'markers' as a key or it may have\n    any valid key from `packaging.markers.marker_context.keys()` and standardize\n    the format into {'markers': 'key == \"some_value\"'}.\n\n    :param pipfile_entry: A dictionariy of keys and values representing a pipfile entry\n    :type pipfile_entry: dict\n    :returns: A normalized dictionary with cleaned marker entries\n    \"\"\"\n    if not isinstance(pipfile_entry, Mapping):\n        raise TypeError(\"Entry is not a pipfile formatted mapping.\")\n    from .vendor.distlib.markers import DEFAULT_CONTEXT as marker_context\n    from .vendor.packaging.markers import Marker\n    from .vendor.vistir.misc import dedup\n\n    allowed_marker_keys = [\"markers\"] + [k for k in marker_context.keys()]\n    provided_keys = list(pipfile_entry.keys()) if hasattr(pipfile_entry, \"keys\") else []\n    pipfile_markers = [k for k in provided_keys if k in allowed_marker_keys]\n    new_pipfile = dict(pipfile_entry).copy()\n    marker_set = set()\n    if \"markers\" in new_pipfile:\n        marker = str(Marker(new_pipfile.pop(\"markers\")))\n        if 'extra' not in marker:\n            marker_set.add(marker)\n    for m in pipfile_markers:\n        entry = \"{0}\".format(pipfile_entry[m])\n        if m != \"markers\":\n            marker_set.add(str(Marker(\"{0}{1}\".format(m, entry))))\n            new_pipfile.pop(m)\n    if marker_set:\n        new_pipfile[\"markers\"] = str(Marker(\" or \".join(\n            \"{0}\".format(s) if \" and \" in s else s\n            for s in sorted(dedup(marker_set))\n        ))).replace('\"', \"'\")\n    return new_pipfile"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if a given path is a virtual environment s root.", "response": "def is_virtual_environment(path):\n    \"\"\"Check if a given path is a virtual environment's root.\n\n    This is done by checking if the directory contains a Python executable in\n    its bin/Scripts directory. Not technically correct, but good enough for\n    general usage.\n    \"\"\"\n    if not path.is_dir():\n        return False\n    for bindir_name in ('bin', 'Scripts'):\n        for python in path.joinpath(bindir_name).glob('python*'):\n            try:\n                exeness = python.is_file() and os.access(str(python), os.X_OK)\n            except OSError:\n                exeness = False\n            if exeness:\n                return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting a temporary sys.version_info tuple :param version_tuple: a fake sys.version_info tuple", "response": "def sys_version(version_tuple):\n    \"\"\"\n    Set a temporary sys.version_info tuple\n\n    :param version_tuple: a fake sys.version_info tuple\n    \"\"\"\n\n    old_version = sys.version_info\n    sys.version_info = version_tuple\n    yield\n    sys.version_info = old_version"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive a set and some arbitrary element add the element to the set", "response": "def add_to_set(original_set, element):\n    \"\"\"Given a set and some arbitrary element, add the element(s) to the set\"\"\"\n    if not element:\n        return original_set\n    if isinstance(element, Set):\n        original_set |= element\n    elif isinstance(element, (list, tuple)):\n        original_set |= set(element)\n    else:\n        original_set.add(element)\n    return original_set"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncompares two URLs ignoring auth and query and fragment", "response": "def is_url_equal(url, other_url):\n    # type: (str, str) -> bool\n    \"\"\"\n    Compare two urls by scheme, host, and path, ignoring auth\n\n    :param str url: The initial URL to compare\n    :param str url: Second url to compare to the first\n    :return: Whether the URLs are equal without **auth**, **query**, and **fragment**\n    :rtype: bool\n\n    >>> is_url_equal(\"https://user:pass@mydomain.com/some/path?some_query\",\n                     \"https://user2:pass2@mydomain.com/some/path\")\n    True\n\n    >>> is_url_equal(\"https://user:pass@mydomain.com/some/path?some_query\",\n                 \"https://mydomain.com/some?some_query\")\n    False\n    \"\"\"\n    if not isinstance(url, six.string_types):\n        raise TypeError(\"Expected string for url, received {0!r}\".format(url))\n    if not isinstance(other_url, six.string_types):\n        raise TypeError(\"Expected string for url, received {0!r}\".format(other_url))\n    parsed_url = urllib3_util.parse_url(url)\n    parsed_other_url = urllib3_util.parse_url(other_url)\n    unparsed = parsed_url._replace(auth=None, query=None, fragment=None).url\n    unparsed_other = parsed_other_url._replace(auth=None, query=None, fragment=None).url\n    return unparsed == unparsed_other"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a path with possible windows - style separators to a posix - style path.", "response": "def make_posix(path):\n    # type: (str) -> str\n    \"\"\"\n    Convert a path with possible windows-style separators to a posix-style path\n    (with **/** separators instead of **\\\\** separators).\n\n    :param Text path: A path to convert.\n    :return: A converted posix-style path\n    :rtype: Text\n\n    >>> make_posix(\"c:/users/user/venvs/some_venv\\\\Lib\\\\site-packages\")\n    \"c:/users/user/venvs/some_venv/Lib/site-packages\"\n\n    >>> make_posix(\"c:\\\\users\\\\user\\\\venvs\\\\some_venv\")\n    \"c:/users/user/venvs/some_venv\"\n    \"\"\"\n    if not isinstance(path, six.string_types):\n        raise TypeError(\"Expected a string for path, received {0!r}...\".format(path))\n    starts_with_sep = path.startswith(os.path.sep)\n    separated = normalize_path(path).split(os.path.sep)\n    if isinstance(separated, (list, tuple)):\n        path = posixpath.join(*separated)\n        if starts_with_sep:\n            path = \"/{0}\".format(path)\n    return path"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_python(finder, line=None):\n\n    if line and not isinstance(line, six.string_types):\n        raise TypeError(\n            \"Invalid python search type: expected string, received {0!r}\".format(line)\n        )\n    if line and os.path.isabs(line):\n        if os.name == \"nt\":\n            line = posixpath.join(*line.split(os.path.sep))\n        return line\n    if not finder:\n        from pipenv.vendor.pythonfinder import Finder\n        finder = Finder(global_search=True)\n    if not line:\n        result = next(iter(finder.find_all_python_versions()), None)\n    elif line and line[0].isdigit() or re.match(r'[\\d\\.]+', line):\n        result = finder.find_python_version(line)\n    else:\n        result = finder.find_python_version(name=line)\n    if not result:\n        result = finder.which(line)\n    if not result and not line.startswith(\"python\"):\n        line = \"python{0}\".format(line)\n        result = find_python(finder, line)\n    if not result:\n        result = next(iter(finder.find_all_python_versions()), None)\n    if result:\n        if not isinstance(result, six.string_types):\n            return result.path.as_posix()\n        return result\n    return", "response": "Given a python finder instance and an optional line find a corresponding python object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck whether the input is a request for python or notself.", "response": "def is_python_command(line):\n    \"\"\"\n    Given an input, checks whether the input is a request for python or notself.\n\n    This can be a version, a python runtime name, or a generic 'python' or 'pythonX.Y'\n\n    :param str line: A potential request to find python\n    :returns: Whether the line is a python lookup\n    :rtype: bool\n    \"\"\"\n\n    if not isinstance(line, six.string_types):\n        raise TypeError(\"Not a valid command to check: {0!r}\".format(line))\n\n    from pipenv.vendor.pythonfinder.utils import PYTHON_IMPLEMENTATIONS\n    is_version = re.match(r'[\\d\\.]+', line)\n    if (line.startswith(\"python\") or is_version or\n            any(line.startswith(v) for v in PYTHON_IMPLEMENTATIONS)):\n        return True\n    # we are less sure about this but we can guess\n    if line.startswith(\"py\"):\n        return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_hash(self, ireq, ireq_hashes=None):\n\n        # We _ALWAYS MUST PRIORITIZE_ the inclusion of hashes from local sources\n        # PLEASE *DO NOT MODIFY THIS* TO CHECK WHETHER AN IREQ ALREADY HAS A HASH\n        # RESOLVED. The resolver will pull hashes from PyPI and only from PyPI.\n        # The entire purpose of this approach is to include missing hashes.\n        # This fixes a race condition in resolution for missing dependency caches\n        # see pypa/pipenv#3289\n        if not self._should_include_hash(ireq):\n            return set()\n        elif self._should_include_hash(ireq) and (\n            not ireq_hashes or ireq.link.scheme == \"file\"\n        ):\n            if not ireq_hashes:\n                ireq_hashes = set()\n            new_hashes = self.resolver.repository._hash_cache.get_hash(ireq.link)\n            ireq_hashes = add_to_set(ireq_hashes, new_hashes)\n        else:\n            ireq_hashes = set(ireq_hashes)\n        # The _ONLY CASE_ where we flat out set the value is if it isn't present\n        # It's a set, so otherwise we *always* need to do a union update\n        if ireq not in self.hashes:\n            return ireq_hashes\n        else:\n            return self.hashes[ireq] | ireq_hashes", "response": "Retrieves hashes for a specific InstallRequirement instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_process_mapping():\n    for impl in (proc, ps):\n        try:\n            mapping = impl.get_process_mapping()\n        except EnvironmentError:\n            continue\n        return mapping\n    raise ShellDetectionFailure('compatible proc fs or ps utility is required')", "response": "Select a way to obtain process information from the system."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\niterates over the process commands in the tree.", "response": "def _iter_process_command(mapping, pid, max_depth):\n    \"\"\"Iterator to traverse up the tree, yielding `argv[0]` of each process.\n    \"\"\"\n    for _ in range(max_depth):\n        try:\n            proc = mapping[pid]\n        except KeyError:    # We've reached the root process. Give up.\n            break\n        try:\n            cmd = proc.args[0]\n        except IndexError:  # Process has no name? Whatever, ignore it.\n            pass\n        else:\n            yield cmd\n        pid = proc.ppid"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_login_shell(proc_cmd):\n    login_shell = os.environ.get('SHELL', '')\n    if login_shell:\n        proc_cmd = login_shell\n    else:\n        proc_cmd = proc_cmd[1:]\n    return (os.path.basename(proc_cmd).lower(), proc_cmd)", "response": "Form shell information from the SHELL environment variable if possible."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the shell that the pid or os. getpid() is running in.", "response": "def get_shell(pid=None, max_depth=6):\n    \"\"\"Get the shell that the supplied pid or os.getpid() is running in.\n    \"\"\"\n    pid = str(pid or os.getpid())\n    mapping = _get_process_mapping()\n    for proc_cmd in _iter_process_command(mapping, pid, max_depth):\n        if proc_cmd.startswith('-'):    # Login shell! Let's use this.\n            return _get_login_shell(proc_cmd)\n        name = os.path.basename(proc_cmd).lower()\n        if name in SHELL_NAMES:     # The inner-most (non-login) shell.\n            return (name, proc_cmd)\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsends a PreparedRequest object. Returns a Response object.", "response": "def send(self, request, stream=False, timeout=None, verify=True,\n             cert=None, proxies=None):\n        \"\"\"Sends PreparedRequest object. Returns Response object.\n\n        :param request: The :class:`PreparedRequest <PreparedRequest>` being sent.\n        :param stream: (optional) Whether to stream the request content.\n        :param timeout: (optional) How long to wait for the server to send\n            data before giving up, as a float, or a :ref:`(connect timeout,\n            read timeout) <timeouts>` tuple.\n        :type timeout: float or tuple\n        :param verify: (optional) Either a boolean, in which case it controls whether we verify\n            the server's TLS certificate, or a string, in which case it must be a path\n            to a CA bundle to use\n        :param cert: (optional) Any user-provided SSL certificate to be trusted.\n        :param proxies: (optional) The proxies dictionary to apply to the request.\n        \"\"\"\n        raise NotImplementedError"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef init_poolmanager(self, connections, maxsize, block=DEFAULT_POOLBLOCK, **pool_kwargs):\n        # save these values for pickling\n        self._pool_connections = connections\n        self._pool_maxsize = maxsize\n        self._pool_block = block\n\n        self.poolmanager = PoolManager(num_pools=connections, maxsize=maxsize,\n                                       block=block, strict=True, **pool_kwargs)", "response": "Initializes a urllib3 PoolManager."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef proxy_manager_for(self, proxy, **proxy_kwargs):\n        if proxy in self.proxy_manager:\n            manager = self.proxy_manager[proxy]\n        elif proxy.lower().startswith('socks'):\n            username, password = get_auth_from_url(proxy)\n            manager = self.proxy_manager[proxy] = SOCKSProxyManager(\n                proxy,\n                username=username,\n                password=password,\n                num_pools=self._pool_connections,\n                maxsize=self._pool_maxsize,\n                block=self._pool_block,\n                **proxy_kwargs\n            )\n        else:\n            proxy_headers = self.proxy_headers(proxy)\n            manager = self.proxy_manager[proxy] = proxy_from_url(\n                proxy,\n                proxy_headers=proxy_headers,\n                num_pools=self._pool_connections,\n                maxsize=self._pool_maxsize,\n                block=self._pool_block,\n                **proxy_kwargs)\n\n        return manager", "response": "Returns a urllib3 ProxyManager for the given proxy."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build_response(self, req, resp):\n        response = Response()\n\n        # Fallback to None if there's no status_code, for whatever reason.\n        response.status_code = getattr(resp, 'status', None)\n\n        # Make headers case-insensitive.\n        response.headers = CaseInsensitiveDict(getattr(resp, 'headers', {}))\n\n        # Set encoding.\n        response.encoding = get_encoding_from_headers(response.headers)\n        response.raw = resp\n        response.reason = response.raw.reason\n\n        if isinstance(req.url, bytes):\n            response.url = req.url.decode('utf-8')\n        else:\n            response.url = req.url\n\n        # Add new cookies from the server.\n        extract_cookies_to_jar(response.cookies, req, resp)\n\n        # Give the Response some context.\n        response.request = req\n        response.connection = self\n\n        return response", "response": "Builds a response object from a urllib3 prepared request and response object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_connection(self, url, proxies=None):\n        proxy = select_proxy(url, proxies)\n\n        if proxy:\n            proxy = prepend_scheme_if_needed(proxy, 'http')\n            proxy_url = parse_url(proxy)\n            if not proxy_url.host:\n                raise InvalidProxyURL(\"Please check proxy URL. It is malformed\"\n                                      \" and could be missing the host.\")\n            proxy_manager = self.proxy_manager_for(proxy)\n            conn = proxy_manager.connection_from_url(url)\n        else:\n            # Only scheme should be lower case\n            parsed = urlparse(url)\n            url = parsed.geturl()\n            conn = self.poolmanager.connection_from_url(url)\n\n        return conn", "response": "Returns a urllib3 connection for the given URL."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef close(self):\n        self.poolmanager.clear()\n        for proxy in self.proxy_manager.values():\n            proxy.clear()", "response": "Disposes of any internal state."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nobtain the url to use when making the final request.", "response": "def request_url(self, request, proxies):\n        \"\"\"Obtain the url to use when making the final request.\n\n        If the message is being sent through a HTTP proxy, the full URL has to\n        be used. Otherwise, we should only use the path portion of the URL.\n\n        This should not be called from user code, and is only exposed for use\n        when subclassing the\n        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.\n\n        :param request: The :class:`PreparedRequest <PreparedRequest>` being sent.\n        :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs.\n        :rtype: str\n        \"\"\"\n        proxy = select_proxy(request.url, proxies)\n        scheme = urlparse(request.url).scheme\n\n        is_proxied_http_request = (proxy and scheme != 'https')\n        using_socks_proxy = False\n        if proxy:\n            proxy_scheme = urlparse(proxy).scheme.lower()\n            using_socks_proxy = proxy_scheme.startswith('socks')\n\n        url = request.path_url\n        if is_proxied_http_request and not using_socks_proxy:\n            url = urldefragauth(request.url)\n\n        return url"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef strip_ssh_from_git_uri(uri):\n    # type: (S) -> S\n    \"\"\"Return git+ssh:// formatted URI to git+git@ format\"\"\"\n    if isinstance(uri, six.string_types):\n        if \"git+ssh://\" in uri:\n            parsed = urlparse(uri)\n            # split the path on the first separating / so we can put the first segment\n            # into the 'netloc' section with a : separator\n            path_part, _, path = parsed.path.lstrip(\"/\").partition(\"/\")\n            path = \"/{0}\".format(path)\n            parsed = parsed._replace(\n                netloc=\"{0}:{1}\".format(parsed.netloc, path_part), path=path\n            )\n            uri = urlunparse(parsed).replace(\"git+ssh://\", \"git+\", 1)\n    return uri", "response": "Return git + ssh:// formatted URI to git + git@ format"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncleaning VCS uris from pipenv. patched. notpip format", "response": "def add_ssh_scheme_to_git_uri(uri):\n    # type: (S) -> S\n    \"\"\"Cleans VCS uris from pipenv.patched.notpip format\"\"\"\n    if isinstance(uri, six.string_types):\n        # Add scheme for parsing purposes, this is also what pip does\n        if uri.startswith(\"git+\") and \"://\" not in uri:\n            uri = uri.replace(\"git+\", \"git+ssh://\", 1)\n            parsed = urlparse(uri)\n            if \":\" in parsed.netloc:\n                netloc, _, path_start = parsed.netloc.rpartition(\":\")\n                path = \"/{0}{1}\".format(path_start, parsed.path)\n                uri = urlunparse(parsed._replace(netloc=netloc, path=path))\n    return uri"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_vcs(pipfile_entry):\n    # type: (PipfileType) -> bool\n    \"\"\"Determine if dictionary entry from Pipfile is for a vcs dependency.\"\"\"\n    if isinstance(pipfile_entry, Mapping):\n        return any(key for key in pipfile_entry.keys() if key in VCS_LIST)\n\n    elif isinstance(pipfile_entry, six.string_types):\n        if not is_valid_url(pipfile_entry) and pipfile_entry.startswith(\"git+\"):\n            pipfile_entry = add_ssh_scheme_to_git_uri(pipfile_entry)\n\n        parsed_entry = urlsplit(pipfile_entry)\n        return parsed_entry.scheme in VCS_SCHEMES\n    return False", "response": "Determine if Pipfile entry is for a vcs dependency."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsplits on multiple given separators.", "response": "def multi_split(s, split):\n    # type: (S, Iterable[S]) -> List[S]\n    \"\"\"Splits on multiple given separators.\"\"\"\n    for r in split:\n        s = s.replace(r, \"|\")\n    return [i for i in s.split(\"|\") if len(i) > 0]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef convert_entry_to_path(path):\n    # type: (Dict[S, Union[S, bool, Tuple[S], List[S]]]) -> S\n    \"\"\"Convert a pipfile entry to a string\"\"\"\n\n    if not isinstance(path, Mapping):\n        raise TypeError(\"expecting a mapping, received {0!r}\".format(path))\n\n    if not any(key in path for key in [\"file\", \"path\"]):\n        raise ValueError(\"missing path-like entry in supplied mapping {0!r}\".format(path))\n\n    if \"file\" in path:\n        path = vistir.path.url_to_path(path[\"file\"])\n\n    elif \"path\" in path:\n        path = path[\"path\"]\n    return path", "response": "Convert a pipfile entry to a string"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndetermining if a path can potentially be installed", "response": "def is_installable_file(path):\n    # type: (PipfileType) -> bool\n    \"\"\"Determine if a path can potentially be installed\"\"\"\n    from packaging import specifiers\n\n    if isinstance(path, Mapping):\n        path = convert_entry_to_path(path)\n\n    # If the string starts with a valid specifier operator, test if it is a valid\n    # specifier set before making a path object (to avoid breaking windows)\n    if any(path.startswith(spec) for spec in \"!=<>~\"):\n        try:\n            specifiers.SpecifierSet(path)\n        # If this is not a valid specifier, just move on and try it as a path\n        except specifiers.InvalidSpecifier:\n            pass\n        else:\n            return False\n\n    parsed = urlparse(path)\n    is_local = (\n        not parsed.scheme\n        or parsed.scheme == \"file\"\n        or (len(parsed.scheme) == 1 and os.name == \"nt\")\n    )\n    if parsed.scheme and parsed.scheme == \"file\":\n        path = vistir.compat.fs_decode(vistir.path.url_to_path(path))\n    normalized_path = vistir.path.normalize_path(path)\n    if is_local and not os.path.exists(normalized_path):\n        return False\n\n    is_archive = pip_shims.shims.is_archive_file(normalized_path)\n    is_local_project = os.path.isdir(normalized_path) and is_installable_dir(\n        normalized_path\n    )\n    if is_local and is_local_project or is_archive:\n        return True\n\n    if not is_local and pip_shims.shims.is_archive_file(parsed.path):\n        return True\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a value from a nested object via a tuple representing the tree structure of the object.", "response": "def get_path(root, path, default=_UNSET):\n    \"\"\"Retrieve a value from a nested object via a tuple representing the\n    lookup path.\n    >>> root = {'a': {'b': {'c': [[1], [2], [3]]}}}\n    >>> get_path(root, ('a', 'b', 'c', 2, 0))\n    3\n    The path format is intentionally consistent with that of\n    :func:`remap`.\n    One of get_path's chief aims is improved error messaging. EAFP is\n    great, but the error messages are not.\n    For instance, ``root['a']['b']['c'][2][1]`` gives back\n    ``IndexError: list index out of range``\n    What went out of range where? get_path currently raises\n    ``PathAccessError: could not access 2 from path ('a', 'b', 'c', 2,\n    1), got error: IndexError('list index out of range',)``, a\n    subclass of IndexError and KeyError.\n    You can also pass a default that covers the entire operation,\n    should the lookup fail at any level.\n    Args:\n       root: The target nesting of dictionaries, lists, or other\n          objects supporting ``__getitem__``.\n       path (tuple): A list of strings and integers to be successively\n          looked up within *root*.\n       default: The value to be returned should any\n          ``PathAccessError`` exceptions be raised.\n    \"\"\"\n    if isinstance(path, six.string_types):\n        path = path.split(\".\")\n    cur = root\n    try:\n        for seg in path:\n            try:\n                cur = cur[seg]\n            except (KeyError, IndexError) as exc:\n                raise PathAccessError(exc, seg, path)\n            except TypeError as exc:\n                # either string index in a list, or a parent that\n                # doesn't support indexing\n                try:\n                    seg = int(seg)\n                    cur = cur[seg]\n                except (ValueError, KeyError, IndexError, TypeError):\n                    if not getattr(cur, \"__iter__\", None):\n                        exc = TypeError(\"%r object is not indexable\" % type(cur).__name__)\n                    raise PathAccessError(exc, seg, path)\n    except PathAccessError:\n        if default is _UNSET:\n            raise\n        return default\n    return cur"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a comparison of actual and expected hash values.", "response": "def _hash_comparison(self):\n        \"\"\"\n        Return a comparison of actual and expected hash values.\n\n        Example::\n\n               Expected sha256 abcdeabcdeabcdeabcdeabcdeabcdeabcdeabcdeabcde\n                            or 123451234512345123451234512345123451234512345\n                    Got        bcdefbcdefbcdefbcdefbcdefbcdefbcdefbcdefbcdef\n\n        \"\"\"\n        def hash_then_or(hash_name):\n            # For now, all the decent hashes have 6-char names, so we can get\n            # away with hard-coding space literals.\n            return chain([hash_name], repeat('    or'))\n\n        lines = []\n        for hash_name, expecteds in iteritems(self.allowed):\n            prefix = hash_then_or(hash_name)\n            lines.extend(('        Expected %s %s' % (next(prefix), e))\n                         for e in expecteds)\n            lines.append('             Got        %s\\n' %\n                         self.gots[hash_name].hexdigest())\n            prefix = '    or'\n        return '\\n'.join(lines)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nresets the UniversalDetector and all of its probers to their initial states.", "response": "def reset(self):\n        \"\"\"\n        Reset the UniversalDetector and all of its probers back to their\n        initial states.  This is called by ``__init__``, so you only need to\n        call this directly in between analyses of different documents.\n        \"\"\"\n        self.result = {'encoding': None, 'confidence': 0.0, 'language': None}\n        self.done = False\n        self._got_data = False\n        self._has_win_bytes = False\n        self._input_state = InputState.PURE_ASCII\n        self._last_char = b''\n        if self._esc_charset_prober:\n            self._esc_charset_prober.reset()\n        for prober in self._charset_probers:\n            prober.reset()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfeeding a chunk of a unicode object into the relevant base - level ISO - 10646 - UCS - 4 - 3412 - UCS - 4 - 3412 - UCS - 4 - 3412 - UCS - 4 - 3412 - UCS - 4 - 3412 - UCS - 4 - 3412 - UCS - 4 - 3412 - UCS - 4 - 3412 - UCS - 4 - 3412 - UCS - 4 - 3412.", "response": "def feed(self, byte_str):\n        \"\"\"\n        Takes a chunk of a document and feeds it through all of the relevant\n        charset probers.\n\n        After calling ``feed``, you can check the value of the ``done``\n        attribute to see if you need to continue feeding the\n        ``UniversalDetector`` more data, or if it has made a prediction\n        (in the ``result`` attribute).\n\n        .. note::\n           You should always call ``close`` when you're done feeding in your\n           document if ``done`` is not already ``True``.\n        \"\"\"\n        if self.done:\n            return\n\n        if not len(byte_str):\n            return\n\n        if not isinstance(byte_str, bytearray):\n            byte_str = bytearray(byte_str)\n\n        # First check for known BOMs, since these are guaranteed to be correct\n        if not self._got_data:\n            # If the data starts with BOM, we know it is UTF\n            if byte_str.startswith(codecs.BOM_UTF8):\n                # EF BB BF  UTF-8 with BOM\n                self.result = {'encoding': \"UTF-8-SIG\",\n                               'confidence': 1.0,\n                               'language': ''}\n            elif byte_str.startswith((codecs.BOM_UTF32_LE,\n                                      codecs.BOM_UTF32_BE)):\n                # FF FE 00 00  UTF-32, little-endian BOM\n                # 00 00 FE FF  UTF-32, big-endian BOM\n                self.result = {'encoding': \"UTF-32\",\n                               'confidence': 1.0,\n                               'language': ''}\n            elif byte_str.startswith(b'\\xFE\\xFF\\x00\\x00'):\n                # FE FF 00 00  UCS-4, unusual octet order BOM (3412)\n                self.result = {'encoding': \"X-ISO-10646-UCS-4-3412\",\n                               'confidence': 1.0,\n                               'language': ''}\n            elif byte_str.startswith(b'\\x00\\x00\\xFF\\xFE'):\n                # 00 00 FF FE  UCS-4, unusual octet order BOM (2143)\n                self.result = {'encoding': \"X-ISO-10646-UCS-4-2143\",\n                               'confidence': 1.0,\n                               'language': ''}\n            elif byte_str.startswith((codecs.BOM_LE, codecs.BOM_BE)):\n                # FF FE  UTF-16, little endian BOM\n                # FE FF  UTF-16, big endian BOM\n                self.result = {'encoding': \"UTF-16\",\n                               'confidence': 1.0,\n                               'language': ''}\n\n            self._got_data = True\n            if self.result['encoding'] is not None:\n                self.done = True\n                return\n\n        # If none of those matched and we've only see ASCII so far, check\n        # for high bytes and escape sequences\n        if self._input_state == InputState.PURE_ASCII:\n            if self.HIGH_BYTE_DETECTOR.search(byte_str):\n                self._input_state = InputState.HIGH_BYTE\n            elif self._input_state == InputState.PURE_ASCII and \\\n                    self.ESC_DETECTOR.search(self._last_char + byte_str):\n                self._input_state = InputState.ESC_ASCII\n\n        self._last_char = byte_str[-1:]\n\n        # If we've seen escape sequences, use the EscCharSetProber, which\n        # uses a simple state machine to check for known escape sequences in\n        # HZ and ISO-2022 encodings, since those are the only encodings that\n        # use such sequences.\n        if self._input_state == InputState.ESC_ASCII:\n            if not self._esc_charset_prober:\n                self._esc_charset_prober = EscCharSetProber(self.lang_filter)\n            if self._esc_charset_prober.feed(byte_str) == ProbingState.FOUND_IT:\n                self.result = {'encoding':\n                               self._esc_charset_prober.charset_name,\n                               'confidence':\n                               self._esc_charset_prober.get_confidence(),\n                               'language':\n                               self._esc_charset_prober.language}\n                self.done = True\n        # If we've seen high bytes (i.e., those with values greater than 127),\n        # we need to do more complicated checks using all our multi-byte and\n        # single-byte probers that are left.  The single-byte probers\n        # use character bigram distributions to determine the encoding, whereas\n        # the multi-byte probers use a combination of character unigram and\n        # bigram distributions.\n        elif self._input_state == InputState.HIGH_BYTE:\n            if not self._charset_probers:\n                self._charset_probers = [MBCSGroupProber(self.lang_filter)]\n                # If we're checking non-CJK encodings, use single-byte prober\n                if self.lang_filter & LanguageFilter.NON_CJK:\n                    self._charset_probers.append(SBCSGroupProber())\n                self._charset_probers.append(Latin1Prober())\n            for prober in self._charset_probers:\n                if prober.feed(byte_str) == ProbingState.FOUND_IT:\n                    self.result = {'encoding': prober.charset_name,\n                                   'confidence': prober.get_confidence(),\n                                   'language': prober.language}\n                    self.done = True\n                    break\n            if self.WIN_BYTE_DETECTOR.search(byte_str):\n                self._has_win_bytes = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstops analyzing the current document and come up with a final AttributeNames prediction.", "response": "def close(self):\n        \"\"\"\n        Stop analyzing the current document and come up with a final\n        prediction.\n\n        :returns:  The ``result`` attribute, a ``dict`` with the keys\n                   `encoding`, `confidence`, and `language`.\n        \"\"\"\n        # Don't bother with checks if we're already done\n        if self.done:\n            return self.result\n        self.done = True\n\n        if not self._got_data:\n            self.logger.debug('no data received!')\n\n        # Default to ASCII if it is all we've seen so far\n        elif self._input_state == InputState.PURE_ASCII:\n            self.result = {'encoding': 'ascii',\n                           'confidence': 1.0,\n                           'language': ''}\n\n        # If we have seen non-ASCII, return the best that met MINIMUM_THRESHOLD\n        elif self._input_state == InputState.HIGH_BYTE:\n            prober_confidence = None\n            max_prober_confidence = 0.0\n            max_prober = None\n            for prober in self._charset_probers:\n                if not prober:\n                    continue\n                prober_confidence = prober.get_confidence()\n                if prober_confidence > max_prober_confidence:\n                    max_prober_confidence = prober_confidence\n                    max_prober = prober\n            if max_prober and (max_prober_confidence > self.MINIMUM_THRESHOLD):\n                charset_name = max_prober.charset_name\n                lower_charset_name = max_prober.charset_name.lower()\n                confidence = max_prober.get_confidence()\n                # Use Windows encoding name instead of ISO-8859 if we saw any\n                # extra Windows-specific bytes\n                if lower_charset_name.startswith('iso-8859'):\n                    if self._has_win_bytes:\n                        charset_name = self.ISO_WIN_MAP.get(lower_charset_name,\n                                                            charset_name)\n                self.result = {'encoding': charset_name,\n                               'confidence': confidence,\n                               'language': max_prober.language}\n\n        # Log all prober confidences if none met MINIMUM_THRESHOLD\n        if self.logger.getEffectiveLevel() == logging.DEBUG:\n            if self.result['encoding'] is None:\n                self.logger.debug('no probers hit minimum threshold')\n                for group_prober in self._charset_probers:\n                    if not group_prober:\n                        continue\n                    if isinstance(group_prober, CharSetGroupProber):\n                        for prober in group_prober.probers:\n                            self.logger.debug('%s %s confidence = %s',\n                                              prober.charset_name,\n                                              prober.language,\n                                              prober.get_confidence())\n                    else:\n                        self.logger.debug('%s %s confidence = %s',\n                                          prober.charset_name,\n                                          prober.language,\n                                          prober.get_confidence())\n        return self.result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bash(command=\"bash\"):\n    bashrc = os.path.join(os.path.dirname(__file__), 'bashrc.sh')\n    child = pexpect.spawn(command, ['--rcfile', bashrc], echo=False,\n                          encoding='utf-8')\n\n    # If the user runs 'env', the value of PS1 will be in the output. To avoid\n    # replwrap seeing that as the next prompt, we'll embed the marker characters\n    # for invisible characters in the prompt; these show up when inspecting the\n    # environment variable, but not when bash displays the prompt.\n    ps1 = PEXPECT_PROMPT[:5] + u'\\\\[\\\\]' + PEXPECT_PROMPT[5:]\n    ps2 = PEXPECT_CONTINUATION_PROMPT[:5] + u'\\\\[\\\\]' + PEXPECT_CONTINUATION_PROMPT[5:]\n    prompt_change = u\"PS1='{0}' PS2='{1}' PROMPT_COMMAND=''\".format(ps1, ps2)\n\n    return REPLWrapper(child, u'\\\\$', prompt_change,\n                       extra_init_cmd=\"export PAGER=cat\")", "response": "Start a bash shell and return a REPLWrapper object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend a command to the REPL wait for and return the output.", "response": "def run_command(self, command, timeout=-1):\n        \"\"\"Send a command to the REPL, wait for and return output.\n\n        :param str command: The command to send. Trailing newlines are not needed.\n          This should be a complete block of input that will trigger execution;\n          if a continuation prompt is found after sending input, :exc:`ValueError`\n          will be raised.\n        :param int timeout: How long to wait for the next prompt. -1 means the\n          default from the :class:`pexpect.spawn` object (default 30 seconds).\n          None means to wait indefinitely.\n        \"\"\"\n        # Split up multiline commands and feed them in bit-by-bit\n        cmdlines = command.splitlines()\n        # splitlines ignores trailing newlines - add it back in manually\n        if command.endswith('\\n'):\n            cmdlines.append('')\n        if not cmdlines:\n            raise ValueError(\"No command was given\")\n\n        res = []\n        self.child.sendline(cmdlines[0])\n        for line in cmdlines[1:]:\n            self._expect_prompt(timeout=timeout)\n            res.append(self.child.before)\n            self.child.sendline(line)\n\n        # Command was fully submitted, now wait for the next prompt\n        if self._expect_prompt(timeout=timeout) == 1:\n            # We got the continuation prompt - command was incomplete\n            self.child.kill(signal.SIGINT)\n            self._expect_prompt(timeout=1)\n            raise ValueError(\"Continuation prompt found - input was incomplete:\\n\"\n                             + command)\n        return u''.join(res + [self.child.before])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing hashes from self. line and set them on the current object.", "response": "def parse_hashes(self):\n        # type: () -> None\n        \"\"\"\n        Parse hashes from *self.line* and set them on the current object.\n        :returns: Nothing\n        :rtype: None\n        \"\"\"\n\n        line, hashes = self.split_hashes(self.line)\n        self.hashes = hashes\n        self.line = line"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses extras from self. line and sets them on the current object .", "response": "def parse_extras(self):\n        # type: () -> None\n        \"\"\"\n        Parse extras from *self.line* and set them on the current object\n        :returns: Nothing\n        :rtype: None\n        \"\"\"\n\n        extras = None\n        if \"@\" in self.line or self.is_vcs or self.is_url:\n            line = \"{0}\".format(self.line)\n            uri = URI.parse(line)\n            name = uri.name\n            if name:\n                self._name = name\n            if uri.host and uri.path and uri.scheme:\n                self.line = uri.to_string(\n                    escape_password=False, direct=False, strip_ssh=uri.is_implicit_ssh\n                )\n            else:\n                self.line, extras = pip_shims.shims._strip_extras(self.line)\n        else:\n            self.line, extras = pip_shims.shims._strip_extras(self.line)\n        extras_set = set()  # type: Set[STRING_TYPE]\n        if extras is not None:\n            extras_set = set(parse_extras(extras))\n        if self._name:\n            self._name, name_extras = pip_shims.shims._strip_extras(self._name)\n            if name_extras:\n                name_extras = set(parse_extras(name_extras))\n                extras_set |= name_extras\n        if extras_set is not None:\n            self.extras = tuple(sorted(extras_set))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting self. is_direct_url if given a **PEP - 508 ** style URL", "response": "def get_url(self):\n        # type: () -> STRING_TYPE\n        \"\"\"Sets ``self.name`` if given a **PEP-508** style URL\"\"\"\n\n        line = self.line\n        try:\n            parsed = URI.parse(line)\n            line = parsed.to_string(escape_password=False, direct=False, strip_ref=True)\n        except ValueError:\n            pass\n        else:\n            self._parsed_url = parsed\n            return line\n        if self.vcs is not None and self.line.startswith(\"{0}+\".format(self.vcs)):\n            _, _, _parseable = self.line.partition(\"+\")\n            parsed = urllib_parse.urlparse(add_ssh_scheme_to_git_uri(_parseable))\n            line, _ = split_ref_from_uri(line)\n        else:\n            parsed = urllib_parse.urlparse(add_ssh_scheme_to_git_uri(line))\n        if \"@\" in self.line and parsed.scheme == \"\":\n            name, _, url = self.line.partition(\"@\")\n            if self._name is None:\n                url = url.strip()\n                self._name = name.strip()\n                if is_valid_url(url):\n                    self.is_direct_url = True\n            line = url.strip()\n            parsed = urllib_parse.urlparse(line)\n            url_path = parsed.path\n            if \"@\" in url_path:\n                url_path, _, _ = url_path.rpartition(\"@\")\n            parsed = parsed._replace(path=url_path)\n        self._parsed_url = parsed\n        return line"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef requirement_info(self):\n        # type: () -> Tuple[Optional[S], Tuple[Optional[S], ...], Optional[S]]\n        \"\"\"\n        Generates a 3-tuple of the requisite *name*, *extras* and *url* to generate a\n        :class:`~packaging.requirements.Requirement` out of.\n\n        :return: A Tuple of an optional name, a Tuple of extras, and an optional URL.\n        :rtype: Tuple[Optional[S], Tuple[Optional[S], ...], Optional[S]]\n        \"\"\"\n\n        # Direct URLs can be converted to packaging requirements directly, but\n        # only if they are `file://` (with only two slashes)\n        name = None  # type: Optional[S]\n        extras = ()  # type: Tuple[Optional[S], ...]\n        url = None  # type: Optional[STRING_TYPE]\n        # if self.is_direct_url:\n        if self._name:\n            name = canonicalize_name(self._name)\n        if self.is_file or self.is_url or self.is_path or self.is_file_url or self.is_vcs:\n            url = \"\"\n            if self.is_vcs:\n                url = self.url if self.url else self.uri\n                if self.is_direct_url:\n                    url = self.link.url_without_fragment\n            else:\n                if self.link:\n                    url = self.link.url_without_fragment\n                elif self.url:\n                    url = self.url\n                    if self.ref:\n                        url = \"{0}@{1}\".format(url, self.ref)\n                else:\n                    url = self.uri\n            if self.link and name is None:\n                self._name = self.link.egg_fragment\n                if self._name:\n                    name = canonicalize_name(self._name)\n        return name, extras, url", "response": "Returns a 3 - tuple of the requisite name extras and URL."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if the given line is installable.", "response": "def line_is_installable(self):\n        # type: () -> bool\n        \"\"\"\n        This is a safeguard against decoy requirements when a user installs a package\n        whose name coincides with the name of a folder in the cwd, e.g. install *alembic*\n        when there is a folder called *alembic* in the working directory.\n\n        In this case we first need to check that the given requirement is a valid\n        URL, VCS requirement, or installable filesystem path before deciding to treat it\n        as a file requirement over a named requirement.\n        \"\"\"\n        line = self.line\n        if is_file_url(line):\n            link = create_link(line)\n            line = link.url_without_fragment\n            line, _ = split_ref_from_uri(line)\n        if (\n            is_vcs(line)\n            or (\n                is_valid_url(line)\n                and (not is_file_url(line) or is_installable_file(line))\n            )\n            or is_installable_file(line)\n        ):\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fork_pty():\n    '''This implements a substitute for the forkpty system call. This\n    should be more portable than the pty.fork() function. Specifically,\n    this should work on Solaris.\n\n    Modified 10.06.05 by Geoff Marshall: Implemented __fork_pty() method to\n    resolve the issue with Python's pty.fork() not supporting Solaris,\n    particularly ssh. Based on patch to posixmodule.c authored by Noah\n    Spurrier::\n\n        http://mail.python.org/pipermail/python-dev/2003-May/035281.html\n\n    '''\n\n    parent_fd, child_fd = os.openpty()\n    if parent_fd < 0 or child_fd < 0:\n        raise OSError(\"os.openpty() failed\")\n\n    pid = os.fork()\n    if pid == CHILD:\n        # Child.\n        os.close(parent_fd)\n        pty_make_controlling_tty(child_fd)\n\n        os.dup2(child_fd, STDIN_FILENO)\n        os.dup2(child_fd, STDOUT_FILENO)\n        os.dup2(child_fd, STDERR_FILENO)\n\n    else:\n        # Parent.\n        os.close(child_fd)\n\n    return pid, parent_fd", "response": "This method implements a substitute for the forkpty system call."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef safecall(func):\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except Exception:\n            pass\n    return wrapper", "response": "Wraps a function so that it swallows exceptions."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a value into a valid string.", "response": "def make_str(value):\n    \"\"\"Converts a value into a valid string.\"\"\"\n    if isinstance(value, bytes):\n        try:\n            return value.decode(get_filesystem_encoding())\n        except UnicodeError:\n            return value.decode('utf-8', 'replace')\n    return text_type(value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_default_short_help(help, max_length=45):\n    words = help.split()\n    total_length = 0\n    result = []\n    done = False\n\n    for word in words:\n        if word[-1:] == '.':\n            done = True\n        new_length = result and 1 + len(word) or len(word)\n        if total_length + new_length > max_length:\n            result.append('...')\n            done = True\n        else:\n            if result:\n                result.append(' ')\n            result.append(word)\n        if done:\n            break\n        total_length += new_length\n\n    return ''.join(result)", "response": "Return a condensed version of help string."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a system stream for byte processing.", "response": "def get_binary_stream(name):\n    \"\"\"Returns a system stream for byte processing.  This essentially\n    returns the stream from the sys module with the given name but it\n    solves some compatibility issues between different Python versions.\n    Primarily this function is necessary for getting binary streams on\n    Python 3.\n\n    :param name: the name of the stream to open.  Valid names are ``'stdin'``,\n                 ``'stdout'`` and ``'stderr'``\n    \"\"\"\n    opener = binary_streams.get(name)\n    if opener is None:\n        raise TypeError('Unknown standard stream %r' % name)\n    return opener()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a system stream for text processing.", "response": "def get_text_stream(name, encoding=None, errors='strict'):\n    \"\"\"Returns a system stream for text processing.  This usually returns\n    a wrapped stream around a binary stream returned from\n    :func:`get_binary_stream` but it also can take shortcuts on Python 3\n    for already correctly configured streams.\n\n    :param name: the name of the stream to open.  Valid names are ``'stdin'``,\n                 ``'stdout'`` and ``'stderr'``\n    :param encoding: overrides the detected default encoding.\n    :param errors: overrides the default error mode.\n    \"\"\"\n    opener = text_streams.get(name)\n    if opener is None:\n        raise TypeError('Unknown standard stream %r' % name)\n    return opener(encoding, errors)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nopen a file and return a file object.", "response": "def open_file(filename, mode='r', encoding=None, errors='strict',\n              lazy=False, atomic=False):\n    \"\"\"This is similar to how the :class:`File` works but for manual\n    usage.  Files are opened non lazy by default.  This can open regular\n    files as well as stdin/stdout if ``'-'`` is passed.\n\n    If stdin/stdout is returned the stream is wrapped so that the context\n    manager will not close the stream accidentally.  This makes it possible\n    to always use the function like this without having to worry to\n    accidentally close a standard stream::\n\n        with open_file(filename) as f:\n            ...\n\n    .. versionadded:: 3.0\n\n    :param filename: the name of the file to open (or ``'-'`` for stdin/stdout).\n    :param mode: the mode in which to open the file.\n    :param encoding: the encoding to use.\n    :param errors: the error handling for this file.\n    :param lazy: can be flipped to true to open the file lazily.\n    :param atomic: in atomic mode writes go into a temporary file and it's\n                   moved on close.\n    \"\"\"\n    if lazy:\n        return LazyFile(filename, mode, encoding, errors, atomic=atomic)\n    f, should_close = open_stream(filename, mode, encoding, errors,\n                                  atomic=atomic)\n    if not should_close:\n        f = KeepOpenFile(f)\n    return f"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nformat a filename for user display.", "response": "def format_filename(filename, shorten=False):\n    \"\"\"Formats a filename for user display.  The main purpose of this\n    function is to ensure that the filename can be displayed at all.  This\n    will decode the filename to unicode if necessary in a way that it will\n    not fail.  Optionally, it can shorten the filename to not include the\n    full path to the filename.\n\n    :param filename: formats a filename for UI display.  This will also convert\n                     the filename into unicode without failing.\n    :param shorten: this optionally shortens the filename to strip of the\n                    path that leads up to it.\n    \"\"\"\n    if shorten:\n        filename = os.path.basename(filename)\n    return filename_to_ui(filename)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nopening the file if it s not yet opened.", "response": "def open(self):\n        \"\"\"Opens the file if it's not yet open.  This call might fail with\n        a :exc:`FileError`.  Not handling this error will produce an error\n        that Click shows.\n        \"\"\"\n        if self._f is not None:\n            return self._f\n        try:\n            rv, self.should_close = open_stream(self.name, self.mode,\n                                                self.encoding,\n                                                self.errors,\n                                                atomic=self.atomic)\n        except (IOError, OSError) as e:\n            from .exceptions import FileError\n            raise FileError(self.name, hint=get_streerror(e))\n        self._f = rv\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nversion of shlex. split that silently accept incomplete strings.", "response": "def split_args(line):\n    \"\"\"Version of shlex.split that silently accept incomplete strings.\n\n    Parameters\n    ----------\n    line : str\n        The string to split\n\n    Returns\n    -------\n    [str]\n        The line split in separated arguments\n    \"\"\"\n    lex = shlex.shlex(line, posix=True)\n    lex.whitespace_split = True\n    lex.commenters = ''\n    res = []\n    try:\n        while True:\n            res.append(next(lex))\n    except ValueError:  # No closing quotation\n        pass\n    except StopIteration:  # End of loop\n        pass\n    if lex.token:\n        res.append(lex.token)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprompt for confirmation of a single user.", "response": "def confirm(text, default=False, abort=False, prompt_suffix=': ',\n            show_default=True, err=False):\n    \"\"\"Prompts for confirmation (yes/no question).\n\n    If the user aborts the input by sending a interrupt signal this\n    function will catch it and raise a :exc:`Abort` exception.\n\n    .. versionadded:: 4.0\n       Added the `err` parameter.\n\n    :param text: the question to ask.\n    :param default: the default for the prompt.\n    :param abort: if this is set to `True` a negative answer aborts the\n                  exception by raising :exc:`Abort`.\n    :param prompt_suffix: a suffix that should be added to the prompt.\n    :param show_default: shows or hides the default value in the prompt.\n    :param err: if set to true the file defaults to ``stderr`` instead of\n                ``stdout``, the same as with echo.\n    \"\"\"\n    prompt = _build_prompt(text, prompt_suffix, show_default,\n                           default and 'Y/n' or 'y/N')\n    while 1:\n        try:\n            # Write the prompt separately so that we get nice\n            # coloring through colorama on Windows\n            echo(prompt, nl=False, err=err)\n            value = visible_prompt_func('').lower().strip()\n        except (KeyboardInterrupt, EOFError):\n            raise Abort()\n        if value in ('y', 'yes'):\n            rv = True\n        elif value in ('n', 'no'):\n            rv = False\n        elif value == '':\n            rv = default\n        else:\n            echo('Error: invalid input', err=err)\n            continue\n        break\n    if abort and not rv:\n        raise Abort()\n    return rv"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef clear():\n    if not isatty(sys.stdout):\n        return\n    # If we're on Windows and we don't have colorama available, then we\n    # clear the screen by shelling out.  Otherwise we can use an escape\n    # sequence.\n    if WIN:\n        os.system('cls')\n    else:\n        sys.stdout.write('\\033[2J\\033[1;1H')", "response": "Clears the terminal screen."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef edit(text=None, editor=None, env=None, require_save=True,\n         extension='.txt', filename=None):\n    r\"\"\"Edits the given text in the defined editor.  If an editor is given\n    (should be the full path to the executable but the regular operating\n    system search path is used for finding the executable) it overrides\n    the detected editor.  Optionally, some environment variables can be\n    used.  If the editor is closed without changes, `None` is returned.  In\n    case a file is edited directly the return value is always `None` and\n    `require_save` and `extension` are ignored.\n\n    If the editor cannot be opened a :exc:`UsageError` is raised.\n\n    Note for Windows: to simplify cross-platform usage, the newlines are\n    automatically converted from POSIX to Windows and vice versa.  As such,\n    the message here will have ``\\n`` as newline markers.\n\n    :param text: the text to edit.\n    :param editor: optionally the editor to use.  Defaults to automatic\n                   detection.\n    :param env: environment variables to forward to the editor.\n    :param require_save: if this is true, then not saving in the editor\n                         will make the return value become `None`.\n    :param extension: the extension to tell the editor about.  This defaults\n                      to `.txt` but changing this might change syntax\n                      highlighting.\n    :param filename: if provided it will edit this file instead of the\n                     provided text contents.  It will not use a temporary\n                     file as an indirection in that case.\n    \"\"\"\n    from ._termui_impl import Editor\n    editor = Editor(editor=editor, env=env, require_save=require_save,\n                    extension=extension)\n    if filename is None:\n        return editor.edit(text)\n    editor.edit_file(filename)", "response": "r Edits the given text in the specified editor."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getchar(echo=False):\n    f = _getchar\n    if f is None:\n        from ._termui_impl import getchar as f\n    return f(echo)", "response": "Fetches a single character from the terminal and returns it."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a shallow copy of this graph.", "response": "def copy(self):\n        \"\"\"Return a shallow copy of this graph.\n        \"\"\"\n        other = DirectedGraph()\n        other._vertices = set(self._vertices)\n        other._forwards = {k: set(v) for k, v in self._forwards.items()}\n        other._backwards = {k: set(v) for k, v in self._backwards.items()}\n        return other"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a new vertex to the graph.", "response": "def add(self, key):\n        \"\"\"Add a new vertex to the graph.\n        \"\"\"\n        if key in self._vertices:\n            raise ValueError('vertex exists')\n        self._vertices.add(key)\n        self._forwards[key] = set()\n        self._backwards[key] = set()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove(self, key):\n        self._vertices.remove(key)\n        for f in self._forwards.pop(key):\n            self._backwards[f].remove(key)\n        for t in self._backwards.pop(key):\n            self._forwards[t].remove(key)", "response": "Removes a vertex from the graph disconnecting all edges from it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconnecting two existing vertices.", "response": "def connect(self, f, t):\n        \"\"\"Connect two existing vertices.\n\n        Nothing happens if the vertices are already connected.\n        \"\"\"\n        if t not in self._vertices:\n            raise KeyError(t)\n        self._forwards[f].add(t)\n        self._backwards[t].add(f)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompare two digests of equal length in constant time.", "response": "def _const_compare_digest_backport(a, b):\n    \"\"\"\n    Compare two digests of equal length in constant time.\n\n    The digests must be of type str/bytes.\n    Returns True if the digests match, and False otherwise.\n    \"\"\"\n    result = abs(len(a) - len(b))\n    for l, r in zip(bytearray(a), bytearray(b)):\n        result |= l ^ r\n    return result == 0"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if given fingerprint matches the supplied certificate.", "response": "def assert_fingerprint(cert, fingerprint):\n    \"\"\"\n    Checks if given fingerprint matches the supplied certificate.\n\n    :param cert:\n        Certificate as bytes object.\n    :param fingerprint:\n        Fingerprint as string of hexdigits, can be interspersed by colons.\n    \"\"\"\n\n    fingerprint = fingerprint.replace(':', '').lower()\n    digest_length = len(fingerprint)\n    hashfunc = HASHFUNC_MAP.get(digest_length)\n    if not hashfunc:\n        raise SSLError(\n            'Fingerprint of invalid length: {0}'.format(fingerprint))\n\n    # We need encode() here for py32; works on py2 and p33.\n    fingerprint_bytes = unhexlify(fingerprint.encode())\n\n    cert_digest = hashfunc(cert).digest()\n\n    if not _const_compare_digest(cert_digest, fingerprint_bytes):\n        raise SSLError('Fingerprints did not match. Expected \"{0}\", got \"{1}\".'\n                       .format(fingerprint, hexlify(cert_digest)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nresolving the argument to a numeric constant which can be passed to the ssl module.", "response": "def resolve_cert_reqs(candidate):\n    \"\"\"\n    Resolves the argument to a numeric constant, which can be passed to\n    the wrap_socket function/method from the ssl module.\n    Defaults to :data:`ssl.CERT_NONE`.\n    If given a string it is assumed to be the name of the constant in the\n    :mod:`ssl` module or its abbreviation.\n    (So you can specify `REQUIRED` instead of `CERT_REQUIRED`.\n    If it's neither `None` nor a string we assume it is already the numeric\n    constant which can directly be passed to wrap_socket.\n    \"\"\"\n    if candidate is None:\n        return CERT_NONE\n\n    if isinstance(candidate, str):\n        res = getattr(ssl, candidate, None)\n        if res is None:\n            res = getattr(ssl, 'CERT_' + candidate)\n        return res\n\n    return candidate"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef resolve_ssl_version(candidate):\n    if candidate is None:\n        return PROTOCOL_SSLv23\n\n    if isinstance(candidate, str):\n        res = getattr(ssl, candidate, None)\n        if res is None:\n            res = getattr(ssl, 'PROTOCOL_' + candidate)\n        return res\n\n    return candidate", "response": "resolve_ssl_version returns the protocol version of the candidate"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ssl_wrap_socket(sock, keyfile=None, certfile=None, cert_reqs=None,\n                    ca_certs=None, server_hostname=None,\n                    ssl_version=None, ciphers=None, ssl_context=None,\n                    ca_cert_dir=None):\n    \"\"\"\n    All arguments except for server_hostname, ssl_context, and ca_cert_dir have\n    the same meaning as they do when using :func:`ssl.wrap_socket`.\n\n    :param server_hostname:\n        When SNI is supported, the expected hostname of the certificate\n    :param ssl_context:\n        A pre-made :class:`SSLContext` object. If none is provided, one will\n        be created using :func:`create_urllib3_context`.\n    :param ciphers:\n        A string of ciphers we wish the client to support.\n    :param ca_cert_dir:\n        A directory containing CA certificates in multiple separate files, as\n        supported by OpenSSL's -CApath flag or the capath argument to\n        SSLContext.load_verify_locations().\n    \"\"\"\n    context = ssl_context\n    if context is None:\n        # Note: This branch of code and all the variables in it are no longer\n        # used by urllib3 itself. We should consider deprecating and removing\n        # this code.\n        context = create_urllib3_context(ssl_version, cert_reqs,\n                                         ciphers=ciphers)\n\n    if ca_certs or ca_cert_dir:\n        try:\n            context.load_verify_locations(ca_certs, ca_cert_dir)\n        except IOError as e:  # Platform-specific: Python 2.7\n            raise SSLError(e)\n        # Py33 raises FileNotFoundError which subclasses OSError\n        # These are not equivalent unless we check the errno attribute\n        except OSError as e:  # Platform-specific: Python 3.3 and beyond\n            if e.errno == errno.ENOENT:\n                raise SSLError(e)\n            raise\n    elif getattr(context, 'load_default_certs', None) is not None:\n        # try to load OS default certs; works well on Windows (require Python3.4+)\n        context.load_default_certs()\n\n    if certfile:\n        context.load_cert_chain(certfile, keyfile)\n\n    # If we detect server_hostname is an IP address then the SNI\n    # extension should not be used according to RFC3546 Section 3.1\n    # We shouldn't warn the user if SNI isn't available but we would\n    # not be using SNI anyways due to IP address for server_hostname.\n    if ((server_hostname is not None and not is_ipaddress(server_hostname))\n            or IS_SECURETRANSPORT):\n        if HAS_SNI and server_hostname is not None:\n            return context.wrap_socket(sock, server_hostname=server_hostname)\n\n        warnings.warn(\n            'An HTTPS request has been made, but the SNI (Server Name '\n            'Indication) extension to TLS is not available on this platform. '\n            'This may cause the server to present an incorrect TLS '\n            'certificate, which can cause validation failures. You can upgrade to '\n            'a newer version of Python to solve this. For more information, see '\n            'https://urllib3.readthedocs.io/en/latest/advanced-usage.html'\n            '#ssl-warnings',\n            SNIMissingWarning\n        )\n\n    return context.wrap_socket(sock)", "response": "Wrap a socket with SSL."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_ipaddress(hostname):\n    if six.PY3 and isinstance(hostname, bytes):\n        # IDN A-label bytes are ASCII compatible.\n        hostname = hostname.decode('ascii')\n\n    families = [socket.AF_INET]\n    if hasattr(socket, 'AF_INET6'):\n        families.append(socket.AF_INET6)\n\n    for af in families:\n        try:\n            inet_pton(af, hostname)\n        except (socket.error, ValueError, OSError):\n            pass\n        else:\n            return True\n    return False", "response": "Detects whether the given hostname is an IP address."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_backoff_time(self):\n        # We want to consider only the last consecutive errors sequence (Ignore redirects).\n        consecutive_errors_len = len(list(takewhile(lambda x: x.redirect_location is None,\n                                                    reversed(self.history))))\n        if consecutive_errors_len <= 1:\n            return 0\n\n        backoff_value = self.backoff_factor * (2 ** (consecutive_errors_len - 1))\n        return min(self.BACKOFF_MAX, backoff_value)", "response": "Returns the current backoff time for the current resource."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the value of Retry - After in seconds.", "response": "def get_retry_after(self, response):\n        \"\"\" Get the value of Retry-After in seconds. \"\"\"\n\n        retry_after = response.getheader(\"Retry-After\")\n\n        if retry_after is None:\n            return None\n\n        return self.parse_retry_after(retry_after)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsleeping between retry attempts.", "response": "def sleep(self, response=None):\n        \"\"\" Sleep between retry attempts.\n\n        This method will respect a server's ``Retry-After`` response header\n        and sleep the duration of the time requested. If that is not present, it\n        will use an exponential backoff. By default, the backoff factor is 0 and\n        this method will return immediately.\n        \"\"\"\n\n        if response:\n            slept = self.sleep_for_retry(response)\n            if slept:\n                return\n\n        self._sleep_backoff()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if a given HTTP method should be retried upon depending if it is included on the method whitelist.", "response": "def _is_method_retryable(self, method):\n        \"\"\" Checks if a given HTTP method should be retried upon, depending if\n        it is included on the method whitelist.\n        \"\"\"\n        if self.method_whitelist and method.upper() not in self.method_whitelist:\n            return False\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nam this status code retryable?", "response": "def is_retry(self, method, status_code, has_retry_after=False):\n        \"\"\" Is this method/status code retryable? (Based on whitelists and control\n        variables such as the number of total retries to allow, whether to\n        respect the Retry-After header, whether this header is present, and\n        whether the returned status code is on the list of status codes to\n        be retried upon on the presence of the aforementioned header)\n        \"\"\"\n        if not self._is_method_retryable(method):\n            return False\n\n        if self.status_forcelist and status_code in self.status_forcelist:\n            return True\n\n        return (self.total and self.respect_retry_after_header and\n                has_retry_after and (status_code in self.RETRY_AFTER_STATUS_CODES))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_exhausted(self):\n        retry_counts = (self.total, self.connect, self.read, self.redirect, self.status)\n        retry_counts = list(filter(None, retry_counts))\n        if not retry_counts:\n            return False\n\n        return min(retry_counts) < 0", "response": "Return True if we have too many retries."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nincrement the retry counters for the given method and url.", "response": "def increment(self, method=None, url=None, response=None, error=None,\n                  _pool=None, _stacktrace=None):\n        \"\"\" Return a new Retry object with incremented retry counters.\n\n        :param response: A response object, or None, if the server did not\n            return a response.\n        :type response: :class:`~urllib3.response.HTTPResponse`\n        :param Exception error: An error encountered during the request, or\n            None if the response was received successfully.\n\n        :return: A new ``Retry`` object.\n        \"\"\"\n        if self.total is False and error:\n            # Disabled, indicate to re-raise the error.\n            raise six.reraise(type(error), error, _stacktrace)\n\n        total = self.total\n        if total is not None:\n            total -= 1\n\n        connect = self.connect\n        read = self.read\n        redirect = self.redirect\n        status_count = self.status\n        cause = 'unknown'\n        status = None\n        redirect_location = None\n\n        if error and self._is_connection_error(error):\n            # Connect retry?\n            if connect is False:\n                raise six.reraise(type(error), error, _stacktrace)\n            elif connect is not None:\n                connect -= 1\n\n        elif error and self._is_read_error(error):\n            # Read retry?\n            if read is False or not self._is_method_retryable(method):\n                raise six.reraise(type(error), error, _stacktrace)\n            elif read is not None:\n                read -= 1\n\n        elif response and response.get_redirect_location():\n            # Redirect retry?\n            if redirect is not None:\n                redirect -= 1\n            cause = 'too many redirects'\n            redirect_location = response.get_redirect_location()\n            status = response.status\n\n        else:\n            # Incrementing because of a server error like a 500 in\n            # status_forcelist and a the given method is in the whitelist\n            cause = ResponseError.GENERIC_ERROR\n            if response and response.status:\n                if status_count is not None:\n                    status_count -= 1\n                cause = ResponseError.SPECIFIC_ERROR.format(\n                    status_code=response.status)\n                status = response.status\n\n        history = self.history + (RequestHistory(method, url, error, status, redirect_location),)\n\n        new_retry = self.new(\n            total=total,\n            connect=connect, read=read, redirect=redirect, status=status_count,\n            history=history)\n\n        if new_retry.is_exhausted():\n            raise MaxRetryError(_pool, url, error or ResponseError(cause))\n\n        log.debug(\"Incremented Retry for (url='%s'): %r\", url, new_retry)\n\n        return new_retry"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build_response(\n        self, request, response, from_cache=False, cacheable_methods=None\n    ):\n        \"\"\"\n        Build a response by making a request or using the cache.\n\n        This will end up calling send and returning a potentially\n        cached response\n        \"\"\"\n        cacheable = cacheable_methods or self.cacheable_methods\n        if not from_cache and request.method in cacheable:\n            # Check for any heuristics that might update headers\n            # before trying to cache.\n            if self.heuristic:\n                response = self.heuristic.apply(response)\n\n            # apply any expiration heuristics\n            if response.status == 304:\n                # We must have sent an ETag request. This could mean\n                # that we've been expired already or that we simply\n                # have an etag. In either case, we want to try and\n                # update the cache if that is the case.\n                cached_response = self.controller.update_cached_response(\n                    request, response\n                )\n\n                if cached_response is not response:\n                    from_cache = True\n\n                # We are done with the server response, read a\n                # possible response body (compliant servers will\n                # not return one, but we cannot be 100% sure) and\n                # release the connection back to the pool.\n                response.read(decode_content=False)\n                response.release_conn()\n\n                response = cached_response\n\n            # We always cache the 301 responses\n            elif response.status == 301:\n                self.controller.cache_response(request, response)\n            else:\n                # Wrap the response file with a wrapper that will cache the\n                #   response when the stream has been consumed.\n                response._fp = CallbackFileWrapper(\n                    response._fp,\n                    functools.partial(\n                        self.controller.cache_response, request, response\n                    ),\n                )\n                if response.chunked:\n                    super_update_chunk_length = response._update_chunk_length\n\n                    def _update_chunk_length(self):\n                        super_update_chunk_length()\n                        if self.chunk_left == 0:\n                            self._fp._close()\n\n                    response._update_chunk_length = types.MethodType(\n                        _update_chunk_length, response\n                    )\n\n        resp = super(CacheControlAdapter, self).build_response(request, response)\n\n        # See if we should invalidate the cache.\n        if request.method in self.invalidating_methods and resp.ok:\n            cache_url = self.controller.cache_url(request.url)\n            self.cache.delete(cache_url)\n\n        # Give the request a from_cache attr to let people use it\n        resp.from_cache = from_cache\n\n        return resp", "response": "Builds a response object from the request and response."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_process_mapping():\n    with open('/proc/{0}/stat'.format(os.getpid())) as f:\n        self_tty = f.read().split()[STAT_TTY]\n    processes = {}\n    for pid in os.listdir('/proc'):\n        if not pid.isdigit():\n            continue\n        try:\n            stat = '/proc/{0}/stat'.format(pid)\n            cmdline = '/proc/{0}/cmdline'.format(pid)\n            with open(stat) as fstat, open(cmdline) as fcmdline:\n                stat = re.findall(r'\\(.+\\)|\\S+', fstat.read())\n                cmd = fcmdline.read().split('\\x00')[:-1]\n            ppid = stat[STAT_PPID]\n            tty = stat[STAT_TTY]\n            if tty == self_tty:\n                processes[pid] = Process(\n                    args=tuple(cmd), pid=pid, ppid=ppid,\n                )\n        except IOError:\n            # Process has disappeared - just ignore it.\n            continue\n    return processes", "response": "Try to find the process tree via Linux s proc - file and return a dict of Process objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a hash and length for a file", "response": "def rehash(path, blocksize=1 << 20):\n    # type: (str, int) -> Tuple[str, str]\n    \"\"\"Return (hash, length) for path using hashlib.sha256()\"\"\"\n    h = hashlib.sha256()\n    length = 0\n    with open(path, 'rb') as f:\n        for block in read_chunks(f, size=blocksize):\n            length += len(block)\n            h.update(block)\n    digest = 'sha256=' + urlsafe_b64encode(\n        h.digest()\n    ).decode('latin1').rstrip('=')\n    # unicode/str python2 issues\n    return (digest, str(length))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef replace_python_tag(wheelname, new_tag):\n    # type: (str, str) -> str\n    \"\"\"Replace the Python tag in a wheel file name with a new value.\n    \"\"\"\n    parts = wheelname.split('-')\n    parts[-3] = new_tag\n    return '-'.join(parts)", "response": "Replace the Python tag in a wheel file name with a new value."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef message_about_scripts_not_on_PATH(scripts):\n    # type: (Sequence[str]) -> Optional[str]\n    \"\"\"Determine if any scripts are not on PATH and format a warning.\n\n    Returns a warning message if one or more scripts are not on PATH,\n    otherwise None.\n    \"\"\"\n    if not scripts:\n        return None\n\n    # Group scripts by the path they were installed in\n    grouped_by_dir = collections.defaultdict(set)  # type: Dict[str, set]\n    for destfile in scripts:\n        parent_dir = os.path.dirname(destfile)\n        script_name = os.path.basename(destfile)\n        grouped_by_dir[parent_dir].add(script_name)\n\n    # We don't want to warn for directories that are on PATH.\n    not_warn_dirs = [\n        os.path.normcase(i).rstrip(os.sep) for i in\n        os.environ.get(\"PATH\", \"\").split(os.pathsep)\n    ]\n    # If an executable sits with sys.executable, we don't warn for it.\n    #     This covers the case of venv invocations without activating the venv.\n    executable_loc = os.environ.get(\"PIP_PYTHON_PATH\", sys.executable)\n    not_warn_dirs.append(os.path.normcase(os.path.dirname(executable_loc)))\n    warn_for = {\n        parent_dir: scripts for parent_dir, scripts in grouped_by_dir.items()\n        if os.path.normcase(parent_dir) not in not_warn_dirs\n    }\n    if not warn_for:\n        return None\n\n    # Format a message\n    msg_lines = []\n    for parent_dir, scripts in warn_for.items():\n        scripts = sorted(scripts)\n        if len(scripts) == 1:\n            start_text = \"script {} is\".format(scripts[0])\n        else:\n            start_text = \"scripts {} are\".format(\n                \", \".join(scripts[:-1]) + \" and \" + scripts[-1]\n            )\n\n        msg_lines.append(\n            \"The {} installed in '{}' which is not on PATH.\"\n            .format(start_text, parent_dir)\n        )\n\n    last_line_fmt = (\n        \"Consider adding {} to PATH or, if you prefer \"\n        \"to suppress this warning, use --no-warn-script-location.\"\n    )\n    if len(msg_lines) == 1:\n        msg_lines.append(last_line_fmt.format(\"this directory\"))\n    else:\n        msg_lines.append(last_line_fmt.format(\"these directories\"))\n\n    # Returns the formatted multiline message\n    return \"\\n\".join(msg_lines)", "response": "Returns a warning message if any scripts are not on PATH and None otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sorted_outrows(outrows):\n    # type: (Iterable[InstalledCSVRow]) -> List[InstalledCSVRow]\n    \"\"\"\n    Return the given rows of a RECORD file in sorted order.\n\n    Each row is a 3-tuple (path, hash, size) and corresponds to a record of\n    a RECORD file (see PEP 376 and PEP 427 for details).  For the rows\n    passed to this function, the size can be an integer as an int or string,\n    or the empty string.\n    \"\"\"\n    # Normally, there should only be one row per path, in which case the\n    # second and third elements don't come into play when sorting.\n    # However, in cases in the wild where a path might happen to occur twice,\n    # we don't want the sort operation to trigger an error (but still want\n    # determinism).  Since the third element can be an int or string, we\n    # coerce each element to a string to avoid a TypeError in this case.\n    # For additional background, see--\n    # https://github.com/pypa/pip/issues/5868\n    return sorted(outrows, key=lambda row: tuple(str(x) for x in row))", "response": "Returns the given rows of a RECORD file in sorted order."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_csv_rows_for_installed(\n    old_csv_rows,  # type: Iterable[List[str]]\n    installed,  # type: Dict[str, str]\n    changed,  # type: set\n    generated,  # type: List[str]\n    lib_dir,  # type: str\n):\n    # type: (...) -> List[InstalledCSVRow]\n    \"\"\"\n    :param installed: A map from archive RECORD path to installation RECORD\n        path.\n    \"\"\"\n    installed_rows = []  # type: List[InstalledCSVRow]\n    for row in old_csv_rows:\n        if len(row) > 3:\n            logger.warning(\n                'RECORD line has more than three elements: {}'.format(row)\n            )\n        # Make a copy because we are mutating the row.\n        row = list(row)\n        old_path = row[0]\n        new_path = installed.pop(old_path, old_path)\n        row[0] = new_path\n        if new_path in changed:\n            digest, length = rehash(new_path)\n            row[1] = digest\n            row[2] = length\n        installed_rows.append(tuple(row))\n    for f in generated:\n        digest, length = rehash(f)\n        installed_rows.append((normpath(f, lib_dir), digest, str(length)))\n    for f in installed:\n        installed_rows.append((installed[f], '', ''))\n    return installed_rows", "response": "Returns a list of tuples where each tuple is a row of the CSV file that is used to install the RECORD."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef move_wheel_files(\n    name,  # type: str\n    req,  # type: Requirement\n    wheeldir,  # type: str\n    user=False,  # type: bool\n    home=None,  # type: Optional[str]\n    root=None,  # type: Optional[str]\n    pycompile=True,  # type: bool\n    scheme=None,  # type: Optional[Mapping[str, str]]\n    isolated=False,  # type: bool\n    prefix=None,  # type: Optional[str]\n    warn_script_location=True  # type: bool\n):\n    # type: (...) -> None\n    \"\"\"Install a wheel\"\"\"\n    # TODO: Investigate and break this up.\n    # TODO: Look into moving this into a dedicated class for representing an\n    #       installation.\n\n    if not scheme:\n        scheme = distutils_scheme(\n            name, user=user, home=home, root=root, isolated=isolated,\n            prefix=prefix,\n        )\n\n    if root_is_purelib(name, wheeldir):\n        lib_dir = scheme['purelib']\n    else:\n        lib_dir = scheme['platlib']\n\n    info_dir = []  # type: List[str]\n    data_dirs = []\n    source = wheeldir.rstrip(os.path.sep) + os.path.sep\n\n    # Record details of the files moved\n    #   installed = files copied from the wheel to the destination\n    #   changed = files changed while installing (scripts #! line typically)\n    #   generated = files newly generated during the install (script wrappers)\n    installed = {}  # type: Dict[str, str]\n    changed = set()\n    generated = []  # type: List[str]\n\n    # Compile all of the pyc files that we're going to be installing\n    if pycompile:\n        with captured_stdout() as stdout:\n            with warnings.catch_warnings():\n                warnings.filterwarnings('ignore')\n                compileall.compile_dir(source, force=True, quiet=True)\n        logger.debug(stdout.getvalue())\n\n    def record_installed(srcfile, destfile, modified=False):\n        \"\"\"Map archive RECORD paths to installation RECORD paths.\"\"\"\n        oldpath = normpath(srcfile, wheeldir)\n        newpath = normpath(destfile, lib_dir)\n        installed[oldpath] = newpath\n        if modified:\n            changed.add(destfile)\n\n    def clobber(source, dest, is_base, fixer=None, filter=None):\n        ensure_dir(dest)  # common for the 'include' path\n\n        for dir, subdirs, files in os.walk(source):\n            basedir = dir[len(source):].lstrip(os.path.sep)\n            destdir = os.path.join(dest, basedir)\n            if is_base and basedir.split(os.path.sep, 1)[0].endswith('.data'):\n                continue\n            for s in subdirs:\n                destsubdir = os.path.join(dest, basedir, s)\n                if is_base and basedir == '' and destsubdir.endswith('.data'):\n                    data_dirs.append(s)\n                    continue\n                elif (is_base and\n                        s.endswith('.dist-info') and\n                        canonicalize_name(s).startswith(\n                            canonicalize_name(req.name))):\n                    assert not info_dir, ('Multiple .dist-info directories: ' +\n                                          destsubdir + ', ' +\n                                          ', '.join(info_dir))\n                    info_dir.append(destsubdir)\n            for f in files:\n                # Skip unwanted files\n                if filter and filter(f):\n                    continue\n                srcfile = os.path.join(dir, f)\n                destfile = os.path.join(dest, basedir, f)\n                # directory creation is lazy and after the file filtering above\n                # to ensure we don't install empty dirs; empty dirs can't be\n                # uninstalled.\n                ensure_dir(destdir)\n\n                # copyfile (called below) truncates the destination if it\n                # exists and then writes the new contents. This is fine in most\n                # cases, but can cause a segfault if pip has loaded a shared\n                # object (e.g. from pyopenssl through its vendored urllib3)\n                # Since the shared object is mmap'd an attempt to call a\n                # symbol in it will then cause a segfault. Unlinking the file\n                # allows writing of new contents while allowing the process to\n                # continue to use the old copy.\n                if os.path.exists(destfile):\n                    os.unlink(destfile)\n\n                # We use copyfile (not move, copy, or copy2) to be extra sure\n                # that we are not moving directories over (copyfile fails for\n                # directories) as well as to ensure that we are not copying\n                # over any metadata because we want more control over what\n                # metadata we actually copy over.\n                shutil.copyfile(srcfile, destfile)\n\n                # Copy over the metadata for the file, currently this only\n                # includes the atime and mtime.\n                st = os.stat(srcfile)\n                if hasattr(os, \"utime\"):\n                    os.utime(destfile, (st.st_atime, st.st_mtime))\n\n                # If our file is executable, then make our destination file\n                # executable.\n                if os.access(srcfile, os.X_OK):\n                    st = os.stat(srcfile)\n                    permissions = (\n                        st.st_mode | stat.S_IXUSR | stat.S_IXGRP | stat.S_IXOTH\n                    )\n                    os.chmod(destfile, permissions)\n\n                changed = False\n                if fixer:\n                    changed = fixer(destfile)\n                record_installed(srcfile, destfile, changed)\n\n    clobber(source, lib_dir, True)\n\n    assert info_dir, \"%s .dist-info directory not found\" % req\n\n    # Get the defined entry points\n    ep_file = os.path.join(info_dir[0], 'entry_points.txt')\n    console, gui = get_entrypoints(ep_file)\n\n    def is_entrypoint_wrapper(name):\n        # EP, EP.exe and EP-script.py are scripts generated for\n        # entry point EP by setuptools\n        if name.lower().endswith('.exe'):\n            matchname = name[:-4]\n        elif name.lower().endswith('-script.py'):\n            matchname = name[:-10]\n        elif name.lower().endswith(\".pya\"):\n            matchname = name[:-4]\n        else:\n            matchname = name\n        # Ignore setuptools-generated scripts\n        return (matchname in console or matchname in gui)\n\n    for datadir in data_dirs:\n        fixer = None\n        filter = None\n        for subdir in os.listdir(os.path.join(wheeldir, datadir)):\n            fixer = None\n            if subdir == 'scripts':\n                fixer = fix_script\n                filter = is_entrypoint_wrapper\n            source = os.path.join(wheeldir, datadir, subdir)\n            dest = scheme[subdir]\n            clobber(source, dest, False, fixer=fixer, filter=filter)\n\n    maker = ScriptMaker(None, scheme['scripts'])\n\n    # Ensure old scripts are overwritten.\n    # See https://github.com/pypa/pip/issues/1800\n    maker.clobber = True\n\n    # Ensure we don't generate any variants for scripts because this is almost\n    # never what somebody wants.\n    # See https://bitbucket.org/pypa/distlib/issue/35/\n    maker.variants = {''}\n\n    # This is required because otherwise distlib creates scripts that are not\n    # executable.\n    # See https://bitbucket.org/pypa/distlib/issue/32/\n    maker.set_mode = True\n\n    # Simplify the script and fix the fact that the default script swallows\n    # every single stack trace.\n    # See https://bitbucket.org/pypa/distlib/issue/34/\n    # See https://bitbucket.org/pypa/distlib/issue/33/\n    def _get_script_text(entry):\n        if entry.suffix is None:\n            raise InstallationError(\n                \"Invalid script entry point: %s for req: %s - A callable \"\n                \"suffix is required. Cf https://packaging.python.org/en/\"\n                \"latest/distributing.html#console-scripts for more \"\n                \"information.\" % (entry, req)\n            )\n        return maker.script_template % {\n            \"module\": entry.prefix,\n            \"import_name\": entry.suffix.split(\".\")[0],\n            \"func\": entry.suffix,\n        }\n    # ignore type, because mypy disallows assigning to a method,\n    # see https://github.com/python/mypy/issues/2427\n    maker._get_script_text = _get_script_text  # type: ignore\n    maker.script_template = r\"\"\"# -*- coding: utf-8 -*-\nimport re\nimport sys\n\nfrom %(module)s import %(import_name)s\n\nif __name__ == '__main__':\n    sys.argv[0] = re.sub(r'(-script\\.pyw?|\\.exe)?$', '', sys.argv[0])\n    sys.exit(%(func)s())\n\"\"\"\n\n    # Special case pip and setuptools to generate versioned wrappers\n    #\n    # The issue is that some projects (specifically, pip and setuptools) use\n    # code in setup.py to create \"versioned\" entry points - pip2.7 on Python\n    # 2.7, pip3.3 on Python 3.3, etc. But these entry points are baked into\n    # the wheel metadata at build time, and so if the wheel is installed with\n    # a *different* version of Python the entry points will be wrong. The\n    # correct fix for this is to enhance the metadata to be able to describe\n    # such versioned entry points, but that won't happen till Metadata 2.0 is\n    # available.\n    # In the meantime, projects using versioned entry points will either have\n    # incorrect versioned entry points, or they will not be able to distribute\n    # \"universal\" wheels (i.e., they will need a wheel per Python version).\n    #\n    # Because setuptools and pip are bundled with _ensurepip and virtualenv,\n    # we need to use universal wheels. So, as a stopgap until Metadata 2.0, we\n    # override the versioned entry points in the wheel and generate the\n    # correct ones. This code is purely a short-term measure until Metadata 2.0\n    # is available.\n    #\n    # To add the level of hack in this section of code, in order to support\n    # ensurepip this code will look for an ``ENSUREPIP_OPTIONS`` environment\n    # variable which will control which version scripts get installed.\n    #\n    # ENSUREPIP_OPTIONS=altinstall\n    #   - Only pipX.Y and easy_install-X.Y will be generated and installed\n    # ENSUREPIP_OPTIONS=install\n    #   - pipX.Y, pipX, easy_install-X.Y will be generated and installed. Note\n    #     that this option is technically if ENSUREPIP_OPTIONS is set and is\n    #     not altinstall\n    # DEFAULT\n    #   - The default behavior is to install pip, pipX, pipX.Y, easy_install\n    #     and easy_install-X.Y.\n    pip_script = console.pop('pip', None)\n    if pip_script:\n        if \"ENSUREPIP_OPTIONS\" not in os.environ:\n            spec = 'pip = ' + pip_script\n            generated.extend(maker.make(spec))\n\n        if os.environ.get(\"ENSUREPIP_OPTIONS\", \"\") != \"altinstall\":\n            spec = 'pip%s = %s' % (sys.version[:1], pip_script)\n            generated.extend(maker.make(spec))\n\n        spec = 'pip%s = %s' % (sys.version[:3], pip_script)\n        generated.extend(maker.make(spec))\n        # Delete any other versioned pip entry points\n        pip_ep = [k for k in console if re.match(r'pip(\\d(\\.\\d)?)?$', k)]\n        for k in pip_ep:\n            del console[k]\n    easy_install_script = console.pop('easy_install', None)\n    if easy_install_script:\n        if \"ENSUREPIP_OPTIONS\" not in os.environ:\n            spec = 'easy_install = ' + easy_install_script\n            generated.extend(maker.make(spec))\n\n        spec = 'easy_install-%s = %s' % (sys.version[:3], easy_install_script)\n        generated.extend(maker.make(spec))\n        # Delete any other versioned easy_install entry points\n        easy_install_ep = [\n            k for k in console if re.match(r'easy_install(-\\d\\.\\d)?$', k)\n        ]\n        for k in easy_install_ep:\n            del console[k]\n\n    # Generate the console and GUI entry points specified in the wheel\n    if len(console) > 0:\n        generated_console_scripts = maker.make_multiple(\n            ['%s = %s' % kv for kv in console.items()]\n        )\n        generated.extend(generated_console_scripts)\n\n        if warn_script_location:\n            msg = message_about_scripts_not_on_PATH(generated_console_scripts)\n            if msg is not None:\n                logger.warning(msg)\n\n    if len(gui) > 0:\n        generated.extend(\n            maker.make_multiple(\n                ['%s = %s' % kv for kv in gui.items()],\n                {'gui': True}\n            )\n        )\n\n    # Record pip as the installer\n    installer = os.path.join(info_dir[0], 'INSTALLER')\n    temp_installer = os.path.join(info_dir[0], 'INSTALLER.pip')\n    with open(temp_installer, 'wb') as installer_file:\n        installer_file.write(b'pip\\n')\n    shutil.move(temp_installer, installer)\n    generated.append(installer)\n\n    # Record details of all files installed\n    record = os.path.join(info_dir[0], 'RECORD')\n    temp_record = os.path.join(info_dir[0], 'RECORD.pip')\n    with open_for_csv(record, 'r') as record_in:\n        with open_for_csv(temp_record, 'w+') as record_out:\n            reader = csv.reader(record_in)\n            outrows = get_csv_rows_for_installed(\n                reader, installed=installed, changed=changed,\n                generated=generated, lib_dir=lib_dir,\n            )\n            writer = csv.writer(record_out)\n            # Sort to simplify testing.\n            for row in sorted_outrows(outrows):\n                writer.writerow(row)\n    shutil.move(temp_record, record)", "response": "Move a wheel to a new version of the current version of the current version."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef wheel_version(source_dir):\n    # type: (Optional[str]) -> Optional[Tuple[int, ...]]\n    \"\"\"\n    Return the Wheel-Version of an extracted wheel, if possible.\n\n    Otherwise, return None if we couldn't parse / extract it.\n    \"\"\"\n    try:\n        dist = [d for d in pkg_resources.find_on_path(None, source_dir)][0]\n\n        wheel_data = dist.get_metadata('WHEEL')\n        wheel_data = Parser().parsestr(wheel_data)\n\n        version = wheel_data['Wheel-Version'].strip()\n        version = tuple(map(int, version.split('.')))\n        return version\n    except Exception:\n        return None", "response": "Returns the Wheel - Version of an extracted wheel if possible."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _contains_egg_info(\n        s, _egg_info_re=re.compile(r'([a-z0-9_.]+)-([a-z0-9_.!+-]+)', re.I)):\n    \"\"\"Determine whether the string looks like an egg_info.\n\n    :param s: The string to parse. E.g. foo-2.1\n    \"\"\"\n    return bool(_egg_info_re.search(s))", "response": "Returns True if the string looks like an egg_info.\n\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndetermine if the given InstallRequirement object should be used for ephemeral cache.", "response": "def should_use_ephemeral_cache(\n    req,  # type: InstallRequirement\n    format_control,  # type: FormatControl\n    autobuilding,  # type: bool\n    cache_available  # type: bool\n):\n    # type: (...) -> Optional[bool]\n    \"\"\"\n    Return whether to build an InstallRequirement object using the\n    ephemeral cache.\n\n    :param cache_available: whether a cache directory is available for the\n        autobuilding=True case.\n\n    :return: True or False to build the requirement with ephem_cache=True\n        or False, respectively; or None not to build the requirement.\n    \"\"\"\n    if req.constraint:\n        return None\n    if req.is_wheel:\n        if not autobuilding:\n            logger.info(\n                'Skipping %s, due to already being wheel.', req.name,\n            )\n        return None\n    if not autobuilding:\n        return False\n\n    if req.editable or not req.source_dir:\n        return None\n\n    if req.link and not req.link.is_artifact:\n        # VCS checkout. Build wheel just for this run.\n        return True\n\n    if \"binary\" not in format_control.get_allowed_formats(\n            canonicalize_name(req.name)):\n        logger.info(\n            \"Skipping bdist_wheel for %s, due to binaries \"\n            \"being disabled for it.\", req.name,\n        )\n        return None\n\n    link = req.link\n    base, ext = link.splitext()\n    if cache_available and _contains_egg_info(base):\n        return False\n\n    # Otherwise, build the wheel just for this run using the ephemeral\n    # cache since we are either in the case of e.g. a local directory, or\n    # no cache directory is available to use.\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef format_command(\n    command_args,  # type: List[str]\n    command_output,  # type: str\n):\n    # type: (...) -> str\n    \"\"\"\n    Format command information for logging.\n    \"\"\"\n    text = 'Command arguments: {}\\n'.format(command_args)\n\n    if not command_output:\n        text += 'Command output: None'\n    elif logger.getEffectiveLevel() > logging.DEBUG:\n        text += 'Command output: [use --verbose to show]'\n    else:\n        if not command_output.endswith('\\n'):\n            command_output += '\\n'\n        text += (\n            'Command output:\\n{}'\n            '-----------------------------------------'\n        ).format(command_output)\n\n    return text", "response": "Formats the command information for logging."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the path to the wheel in the temporary build directory.", "response": "def get_legacy_build_wheel_path(\n    names,  # type: List[str]\n    temp_dir,  # type: str\n    req,  # type: InstallRequirement\n    command_args,  # type: List[str]\n    command_output,  # type: str\n):\n    # type: (...) -> Optional[str]\n    \"\"\"\n    Return the path to the wheel in the temporary build directory.\n    \"\"\"\n    # Sort for determinism.\n    names = sorted(names)\n    if not names:\n        msg = (\n            'Legacy build of wheel for {!r} created no files.\\n'\n        ).format(req.name)\n        msg += format_command(command_args, command_output)\n        logger.warning(msg)\n        return None\n\n    if len(names) > 1:\n        msg = (\n            'Legacy build of wheel for {!r} created more than one file.\\n'\n            'Filenames (choosing first): {}\\n'\n        ).format(req.name, names)\n        msg += format_command(command_args, command_output)\n        logger.warning(msg)\n\n    return os.path.join(temp_dir, names[0])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef support_index_min(self, tags=None):\n        # type: (Optional[List[Pep425Tag]]) -> Optional[int]\n        \"\"\"\n        Return the lowest index that one of the wheel's file_tag combinations\n        achieves in the supported_tags list e.g. if there are 8 supported tags,\n        and one of the file tags is first in the list, then return 0.  Returns\n        None is the wheel is not supported.\n        \"\"\"\n        if tags is None:  # for mock\n            tags = pep425tags.get_supported()\n        indexes = [tags.index(c) for c in self.file_tags if c in tags]\n        return min(indexes) if indexes else None", "response": "Returns the lowest index that one of the wheel s file_tag combinations\n            achieves in the supported_tags list e. g. 0 if the wheel is not supported."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nam this wheel supported on this system?", "response": "def supported(self, tags=None):\n        # type: (Optional[List[Pep425Tag]]) -> bool\n        \"\"\"Is this wheel supported on this system?\"\"\"\n        if tags is None:  # for mock\n            tags = pep425tags.get_supported()\n        return bool(set(tags).intersection(self.file_tags))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding one wheel. :return: The filename of the built wheel, or None if the build failed.", "response": "def _build_one(self, req, output_dir, python_tag=None):\n        \"\"\"Build one wheel.\n\n        :return: The filename of the built wheel, or None if the build failed.\n        \"\"\"\n        # Install build deps into temporary directory (PEP 518)\n        with req.build_env:\n            return self._build_one_inside_env(req, output_dir,\n                                              python_tag=python_tag)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _build_one_pep517(self, req, tempd, python_tag=None):\n        assert req.metadata_directory is not None\n        try:\n            req.spin_message = 'Building wheel for %s (PEP 517)' % (req.name,)\n            logger.debug('Destination directory: %s', tempd)\n            wheel_name = req.pep517_backend.build_wheel(\n                tempd,\n                metadata_directory=req.metadata_directory\n            )\n            if python_tag:\n                # General PEP 517 backends don't necessarily support\n                # a \"--python-tag\" option, so we rename the wheel\n                # file directly.\n                new_name = replace_python_tag(wheel_name, python_tag)\n                os.rename(\n                    os.path.join(tempd, wheel_name),\n                    os.path.join(tempd, new_name)\n                )\n                # Reassign to simplify the return at the end of function\n                wheel_name = new_name\n        except Exception:\n            logger.error('Failed building wheel for %s', req.name)\n            return None\n        return os.path.join(tempd, wheel_name)", "response": "Build one InstallRequirement using the PEP 517 build process."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild one InstallRequirement using the legacy build process.", "response": "def _build_one_legacy(self, req, tempd, python_tag=None):\n        \"\"\"Build one InstallRequirement using the \"legacy\" build process.\n\n        Returns path to wheel if successfully built. Otherwise, returns None.\n        \"\"\"\n        base_args = self._base_setup_args(req)\n\n        spin_message = 'Building wheel for %s (setup.py)' % (req.name,)\n        with open_spinner(spin_message) as spinner:\n            logger.debug('Destination directory: %s', tempd)\n            wheel_args = base_args + ['bdist_wheel', '-d', tempd] \\\n                + self.build_options\n\n            if python_tag is not None:\n                wheel_args += [\"--python-tag\", python_tag]\n\n            try:\n                output = call_subprocess(wheel_args, cwd=req.setup_py_dir,\n                                         show_stdout=False, spinner=spinner)\n            except Exception:\n                spinner.finish(\"error\")\n                logger.error('Failed building wheel for %s', req.name)\n                return None\n            names = os.listdir(tempd)\n            wheel_path = get_legacy_build_wheel_path(\n                names=names,\n                temp_dir=tempd,\n                req=req,\n                command_args=wheel_args,\n                command_output=output,\n            )\n            return wheel_path"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build(\n        self,\n        requirements,  # type: Iterable[InstallRequirement]\n        session,  # type: PipSession\n        autobuilding=False  # type: bool\n    ):\n        # type: (...) -> List[InstallRequirement]\n        \"\"\"Build wheels.\n\n        :param unpack: If True, replace the sdist we built from with the\n            newly built wheel, in preparation for installation.\n        :return: True if all the wheels built correctly.\n        \"\"\"\n        buildset = []\n        format_control = self.finder.format_control\n        # Whether a cache directory is available for autobuilding=True.\n        cache_available = bool(self._wheel_dir or self.wheel_cache.cache_dir)\n\n        for req in requirements:\n            ephem_cache = should_use_ephemeral_cache(\n                req, format_control=format_control, autobuilding=autobuilding,\n                cache_available=cache_available,\n            )\n            if ephem_cache is None:\n                continue\n\n            buildset.append((req, ephem_cache))\n\n        if not buildset:\n            return []\n\n        # Is any wheel build not using the ephemeral cache?\n        if any(not ephem_cache for _, ephem_cache in buildset):\n            have_directory_for_build = self._wheel_dir or (\n                autobuilding and self.wheel_cache.cache_dir\n            )\n            assert have_directory_for_build\n\n        # TODO by @pradyunsg\n        # Should break up this method into 2 separate methods.\n\n        # Build the wheels.\n        logger.info(\n            'Building wheels for collected packages: %s',\n            ', '.join([req.name for (req, _) in buildset]),\n        )\n        _cache = self.wheel_cache  # shorter name\n        with indent_log():\n            build_success, build_failure = [], []\n            for req, ephem in buildset:\n                python_tag = None\n                if autobuilding:\n                    python_tag = pep425tags.implementation_tag\n                    if ephem:\n                        output_dir = _cache.get_ephem_path_for_link(req.link)\n                    else:\n                        output_dir = _cache.get_path_for_link(req.link)\n                    try:\n                        ensure_dir(output_dir)\n                    except OSError as e:\n                        logger.warning(\"Building wheel for %s failed: %s\",\n                                       req.name, e)\n                        build_failure.append(req)\n                        continue\n                else:\n                    output_dir = self._wheel_dir\n                wheel_file = self._build_one(\n                    req, output_dir,\n                    python_tag=python_tag,\n                )\n                if wheel_file:\n                    build_success.append(req)\n                    if autobuilding:\n                        # XXX: This is mildly duplicative with prepare_files,\n                        # but not close enough to pull out to a single common\n                        # method.\n                        # The code below assumes temporary source dirs -\n                        # prevent it doing bad things.\n                        if req.source_dir and not os.path.exists(os.path.join(\n                                req.source_dir, PIP_DELETE_MARKER_FILENAME)):\n                            raise AssertionError(\n                                \"bad source dir - missing marker\")\n                        # Delete the source we built the wheel from\n                        req.remove_temporary_source()\n                        # set the build directory again - name is known from\n                        # the work prepare_files did.\n                        req.source_dir = req.build_location(\n                            self.preparer.build_dir\n                        )\n                        # Update the link for this.\n                        req.link = Link(path_to_url(wheel_file))\n                        assert req.link.is_wheel\n                        # extract the wheel into the dir\n                        unpack_url(\n                            req.link, req.source_dir, None, False,\n                            session=session,\n                        )\n                else:\n                    build_failure.append(req)\n\n        # notify success/failure\n        if build_success:\n            logger.info(\n                'Successfully built %s',\n                ' '.join([req.name for req in build_success]),\n            )\n        if build_failure:\n            logger.info(\n                'Failed to build %s',\n                ' '.join([req.name for req in build_failure]),\n            )\n        # Return a list of requirements that failed to build\n        return build_failure", "response": "Build the wheels for the given set of requirements."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_pypirc_command(self):\n        from distutils.core import Distribution\n        from distutils.config import PyPIRCCommand\n        d = Distribution()\n        return PyPIRCCommand(d)", "response": "Get the distutils command for interacting with PyPI configurations."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading the PyPI access configuration as supported by distutils getting PyPI to do the actual work.", "response": "def read_configuration(self):\n        \"\"\"\n        Read the PyPI access configuration as supported by distutils, getting\n        PyPI to do the actual work. This populates ``username``, ``password``,\n        ``realm`` and ``url`` attributes from the configuration.\n        \"\"\"\n        # get distutils to do the work\n        c = self._get_pypirc_command()\n        c.repository = self.url\n        cfg = c._read_pypirc()\n        self.username = cfg.get('username')\n        self.password = cfg.get('password')\n        self.realm = cfg.get('realm', 'pypi')\n        self.url = cfg.get('repository', self.url)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsaving the PyPI access configuration.", "response": "def save_configuration(self):\n        \"\"\"\n        Save the PyPI access configuration. You must have set ``username`` and\n        ``password`` attributes before calling this method.\n\n        Again, distutils is used to do the actual work.\n        \"\"\"\n        self.check_credentials()\n        # get distutils to do the work\n        c = self._get_pypirc_command()\n        c._store_pypirc(self.username, self.password)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_credentials(self):\n        if self.username is None or self.password is None:\n            raise DistlibException('username and password must be set')\n        pm = HTTPPasswordMgr()\n        _, netloc, _, _, _, _ = urlparse(self.url)\n        pm.add_password(self.realm, netloc, self.username, self.password)\n        self.password_handler = HTTPBasicAuthHandler(pm)", "response": "Check that username and password have been set and raise an exception if not."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef register(self, metadata):\n        self.check_credentials()\n        metadata.validate()\n        d = metadata.todict()\n        d[':action'] = 'verify'\n        request = self.encode_request(d.items(), [])\n        response = self.send_request(request)\n        d[':action'] = 'submit'\n        request = self.encode_request(d.items(), [])\n        return self.send_request(request)", "response": "Register a distribution on PyPI using the provided metadata."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _reader(self, name, stream, outbuf):\n        while True:\n            s = stream.readline()\n            if not s:\n                break\n            s = s.decode('utf-8').rstrip()\n            outbuf.append(s)\n            logger.debug('%s: %s' % (name, s))\n        stream.close()", "response": "A thread that reads lines of from a subprocess into a buffer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_sign_command(self, filename, signer, sign_password,\n                         keystore=None):\n        \"\"\"\n        Return a suitable command for signing a file.\n\n        :param filename: The pathname to the file to be signed.\n        :param signer: The identifier of the signer of the file.\n        :param sign_password: The passphrase for the signer's\n                              private key used for signing.\n        :param keystore: The path to a directory which contains the keys\n                         used in verification. If not specified, the\n                         instance's ``gpg_home`` attribute is used instead.\n        :return: The signing command as a list suitable to be\n                 passed to :class:`subprocess.Popen`.\n        \"\"\"\n        cmd = [self.gpg, '--status-fd', '2', '--no-tty']\n        if keystore is None:\n            keystore = self.gpg_home\n        if keystore:\n            cmd.extend(['--homedir', keystore])\n        if sign_password is not None:\n            cmd.extend(['--batch', '--passphrase-fd', '0'])\n        td = tempfile.mkdtemp()\n        sf = os.path.join(td, os.path.basename(filename) + '.asc')\n        cmd.extend(['--detach-sign', '--armor', '--local-user',\n                    signer, '--output', sf, filename])\n        logger.debug('invoking: %s', ' '.join(cmd))\n        return cmd, sf", "response": "Returns a command to sign a file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run_command(self, cmd, input_data=None):\n        kwargs = {\n            'stdout': subprocess.PIPE,\n            'stderr': subprocess.PIPE,\n        }\n        if input_data is not None:\n            kwargs['stdin'] = subprocess.PIPE\n        stdout = []\n        stderr = []\n        p = subprocess.Popen(cmd, **kwargs)\n        # We don't use communicate() here because we may need to\n        # get clever with interacting with the command\n        t1 = Thread(target=self._reader, args=('stdout', p.stdout, stdout))\n        t1.start()\n        t2 = Thread(target=self._reader, args=('stderr', p.stderr, stderr))\n        t2.start()\n        if input_data is not None:\n            p.stdin.write(input_data)\n            p.stdin.close()\n\n        p.wait()\n        t1.join()\n        t2.join()\n        return p.returncode, stdout, stderr", "response": "Runs a command in a child process passing it any input data specified."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsign a file. :param filename: The pathname to the file to be signed. :param signer: The identifier of the signer of the file. :param sign_password: The passphrase for the signer's private key used for signing. :param keystore: The path to a directory which contains the keys used in signing. If not specified, the instance's ``gpg_home`` attribute is used instead. :return: The absolute pathname of the file where the signature is stored.", "response": "def sign_file(self, filename, signer, sign_password, keystore=None):\n        \"\"\"\n        Sign a file.\n\n        :param filename: The pathname to the file to be signed.\n        :param signer: The identifier of the signer of the file.\n        :param sign_password: The passphrase for the signer's\n                              private key used for signing.\n        :param keystore: The path to a directory which contains the keys\n                         used in signing. If not specified, the instance's\n                         ``gpg_home`` attribute is used instead.\n        :return: The absolute pathname of the file where the signature is\n                 stored.\n        \"\"\"\n        cmd, sig_file = self.get_sign_command(filename, signer, sign_password,\n                                              keystore)\n        rc, stdout, stderr = self.run_command(cmd,\n                                              sign_password.encode('utf-8'))\n        if rc != 0:\n            raise DistlibException('sign command failed with error '\n                                   'code %s' % rc)\n        return sig_file"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef upload_documentation(self, metadata, doc_dir):\n        self.check_credentials()\n        if not os.path.isdir(doc_dir):\n            raise DistlibException('not a directory: %r' % doc_dir)\n        fn = os.path.join(doc_dir, 'index.html')\n        if not os.path.exists(fn):\n            raise DistlibException('not found: %r' % fn)\n        metadata.validate()\n        name, version = metadata.name, metadata.version\n        zip_data = zip_dir(doc_dir).getvalue()\n        fields = [(':action', 'doc_upload'),\n                  ('name', name), ('version', version)]\n        files = [('content', name, zip_data)]\n        request = self.encode_request(fields, files)\n        return self.send_request(request)", "response": "Uploads documentation to the index."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list suitable to be passed to subprocess. Popen to verify a file.", "response": "def get_verify_command(self, signature_filename, data_filename,\n                           keystore=None):\n        \"\"\"\n        Return a suitable command for verifying a file.\n\n        :param signature_filename: The pathname to the file containing the\n                                   signature.\n        :param data_filename: The pathname to the file containing the\n                              signed data.\n        :param keystore: The path to a directory which contains the keys\n                         used in verification. If not specified, the\n                         instance's ``gpg_home`` attribute is used instead.\n        :return: The verifying command as a list suitable to be\n                 passed to :class:`subprocess.Popen`.\n        \"\"\"\n        cmd = [self.gpg, '--status-fd', '2', '--no-tty']\n        if keystore is None:\n            keystore = self.gpg_home\n        if keystore:\n            cmd.extend(['--homedir', keystore])\n        cmd.extend(['--verify', signature_filename, data_filename])\n        logger.debug('invoking: %s', ' '.join(cmd))\n        return cmd"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef verify_signature(self, signature_filename, data_filename,\n                         keystore=None):\n        \"\"\"\n        Verify a signature for a file.\n\n        :param signature_filename: The pathname to the file containing the\n                                   signature.\n        :param data_filename: The pathname to the file containing the\n                              signed data.\n        :param keystore: The path to a directory which contains the keys\n                         used in verification. If not specified, the\n                         instance's ``gpg_home`` attribute is used instead.\n        :return: True if the signature was verified, else False.\n        \"\"\"\n        if not self.gpg:\n            raise DistlibException('verification unavailable because gpg '\n                                   'unavailable')\n        cmd = self.get_verify_command(signature_filename, data_filename,\n                                      keystore)\n        rc, stdout, stderr = self.run_command(cmd)\n        if rc not in (0, 1):\n            raise DistlibException('verify command failed with error '\n                             'code %s' % rc)\n        return rc == 0", "response": "Verify a signature for a file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndownload a file from a URL and save it to a file.", "response": "def download_file(self, url, destfile, digest=None, reporthook=None):\n        \"\"\"\n        This is a convenience method for downloading a file from an URL.\n        Normally, this will be a file from the index, though currently\n        no check is made for this (i.e. a file can be downloaded from\n        anywhere).\n\n        The method is just like the :func:`urlretrieve` function in the\n        standard library, except that it allows digest computation to be\n        done during download and checking that the downloaded data\n        matched any expected value.\n\n        :param url: The URL of the file to be downloaded (assumed to be\n                    available via an HTTP GET request).\n        :param destfile: The pathname where the downloaded file is to be\n                         saved.\n        :param digest: If specified, this must be a (hasher, value)\n                       tuple, where hasher is the algorithm used (e.g.\n                       ``'md5'``) and ``value`` is the expected value.\n        :param reporthook: The same as for :func:`urlretrieve` in the\n                           standard library.\n        \"\"\"\n        if digest is None:\n            digester = None\n            logger.debug('No digest specified')\n        else:\n            if isinstance(digest, (list, tuple)):\n                hasher, digest = digest\n            else:\n                hasher = 'md5'\n            digester = getattr(hashlib, hasher)()\n            logger.debug('Digest specified: %s' % digest)\n        # The following code is equivalent to urlretrieve.\n        # We need to do it this way so that we can compute the\n        # digest of the file as we go.\n        with open(destfile, 'wb') as dfp:\n            # addinfourl is not a context manager on 2.x\n            # so we have to use try/finally\n            sfp = self.send_request(Request(url))\n            try:\n                headers = sfp.info()\n                blocksize = 8192\n                size = -1\n                read = 0\n                blocknum = 0\n                if \"content-length\" in headers:\n                    size = int(headers[\"Content-Length\"])\n                if reporthook:\n                    reporthook(blocknum, blocksize, size)\n                while True:\n                    block = sfp.read(blocksize)\n                    if not block:\n                        break\n                    read += len(block)\n                    dfp.write(block)\n                    if digester:\n                        digester.update(block)\n                    blocknum += 1\n                    if reporthook:\n                        reporthook(blocknum, blocksize, size)\n            finally:\n                sfp.close()\n\n        # check that we got the whole file, if we can\n        if size >= 0 and read < size:\n            raise DistlibException(\n                'retrieval incomplete: got only %d out of %d bytes'\n                % (read, size))\n        # if we have a digest, it must match.\n        if digester:\n            actual = digester.hexdigest()\n            if digest != actual:\n                raise DistlibException('%s digest mismatch for %s: expected '\n                                       '%s, got %s' % (hasher, destfile,\n                                                       digest, actual))\n            logger.debug('Digest verified: %s', digest)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending a standard library request to PyPI and return its response.", "response": "def send_request(self, req):\n        \"\"\"\n        Send a standard library :class:`Request` to PyPI and return its\n        response.\n\n        :param req: The request to send.\n        :return: The HTTP response from PyPI (a standard library HTTPResponse).\n        \"\"\"\n        handlers = []\n        if self.password_handler:\n            handlers.append(self.password_handler)\n        if self.ssl_verifier:\n            handlers.append(self.ssl_verifier)\n        opener = build_opener(*handlers)\n        return opener.open(req)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef encode_request(self, fields, files):\n        # Adapted from packaging, which in turn was adapted from\n        # http://code.activestate.com/recipes/146306\n\n        parts = []\n        boundary = self.boundary\n        for k, values in fields:\n            if not isinstance(values, (list, tuple)):\n                values = [values]\n\n            for v in values:\n                parts.extend((\n                    b'--' + boundary,\n                    ('Content-Disposition: form-data; name=\"%s\"' %\n                     k).encode('utf-8'),\n                    b'',\n                    v.encode('utf-8')))\n        for key, filename, value in files:\n            parts.extend((\n                b'--' + boundary,\n                ('Content-Disposition: form-data; name=\"%s\"; filename=\"%s\"' %\n                 (key, filename)).encode('utf-8'),\n                b'',\n                value))\n\n        parts.extend((b'--' + boundary + b'--', b''))\n\n        body = b'\\r\\n'.join(parts)\n        ct = b'multipart/form-data; boundary=' + boundary\n        headers = {\n            'Content-type': ct,\n            'Content-length': str(len(body))\n        }\n        return Request(self.url, body, headers)", "response": "Encode fields and files for posting to an HTTP server."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndo the completion for bash", "response": "def do_bash_complete(cli, prog_name):\n    \"\"\"Do the completion for bash\n\n    Parameters\n    ----------\n    cli : click.Command\n        The main click Command of the program\n    prog_name : str\n        The program name on the command line\n\n    Returns\n    -------\n    bool\n        True if the completion was successful, False otherwise\n    \"\"\"\n    comp_words = os.environ['COMP_WORDS']\n    try:\n        cwords = shlex.split(comp_words)\n        quoted = False\n    except ValueError:  # No closing quotation\n        cwords = split_args(comp_words)\n        quoted = True\n    cword = int(os.environ['COMP_CWORD'])\n    args = cwords[1:cword]\n    try:\n        incomplete = cwords[cword]\n    except IndexError:\n        incomplete = ''\n    choices = get_choices(cli, prog_name, args, incomplete)\n\n    if quoted:\n        echo('\\t'.join(opt for opt, _ in choices), nl=False)\n    else:\n        echo('\\t'.join(re.sub(r\"\"\"([\\s\\\\\"'()])\"\"\", r'\\\\\\1', opt) for opt, _ in choices), nl=False)\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndo the fish completion of the base class", "response": "def do_fish_complete(cli, prog_name):\n    \"\"\"Do the fish completion\n\n    Parameters\n    ----------\n    cli : click.Command\n        The main click Command of the program\n    prog_name : str\n        The program name on the command line\n\n    Returns\n    -------\n    bool\n        True if the completion was successful, False otherwise\n    \"\"\"\n    commandline = os.environ['COMMANDLINE']\n    args = split_args(commandline)[1:]\n    if args and not commandline.endswith(' '):\n        incomplete = args[-1]\n        args = args[:-1]\n    else:\n        incomplete = ''\n\n    for item, help in get_choices(cli, prog_name, args, incomplete):\n        if help:\n            echo(\"%s\\t%s\" % (item, re.sub('\\s', ' ', help)))\n        else:\n            echo(item)\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndo the powershell completion", "response": "def do_powershell_complete(cli, prog_name):\n    \"\"\"Do the powershell completion\n\n    Parameters\n    ----------\n    cli : click.Command\n        The main click Command of the program\n    prog_name : str\n        The program name on the command line\n\n    Returns\n    -------\n    bool\n        True if the completion was successful, False otherwise\n    \"\"\"\n    commandline = os.environ['COMMANDLINE']\n    args = split_args(commandline)[1:]\n    quote = single_quote\n    incomplete = ''\n    if args and not commandline.endswith(' '):\n        incomplete = args[-1]\n        args = args[:-1]\n        quote_pos = commandline.rfind(incomplete) - 1\n        if quote_pos >= 0 and commandline[quote_pos] == '\"':\n            quote = double_quote\n\n    for item, help in get_choices(cli, prog_name, args, incomplete):\n        echo(quote(item))\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_code(shell=None, prog_name=None, env_name=None, extra_env=None):\n    from jinja2 import Environment, FileSystemLoader\n    if shell in [None, 'auto']:\n        shell = get_auto_shell()\n    if not isinstance(shell, Shell):\n        shell = Shell[shell]\n    prog_name = prog_name or click.get_current_context().find_root().info_name\n    env_name = env_name or '_%s_COMPLETE' % prog_name.upper().replace('-', '_')\n    extra_env = extra_env if extra_env else {}\n    env = Environment(loader=FileSystemLoader(os.path.dirname(__file__)))\n    template = env.get_template('%s.j2' % shell.name)\n    return template.render(prog_name=prog_name, complete_var=env_name, extra_env=extra_env)", "response": "Returns the completion code to be evaluated by the shell\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninstalling the completion of the current user s application.", "response": "def install(shell=None, prog_name=None, env_name=None, path=None, append=None, extra_env=None):\n    \"\"\"Install the completion\n\n    Parameters\n    ----------\n    shell : Shell\n        The shell type targeted. It will be guessed with get_auto_shell() if the value is None (Default value = None)\n    prog_name : str\n        The program name on the command line. It will be automatically computed if the value is None\n        (Default value = None)\n    env_name : str\n        The environment variable name used to control the completion. It will be automatically computed if the value is\n        None (Default value = None)\n    path : str\n        The installation path of the code to be evaluated by the shell. The standard installation path is used if the\n        value is None (Default value = None)\n    append : bool\n        Whether to append the content to the file or to override it. The default behavior depends on the shell type\n        (Default value = None)\n    extra_env : dict\n        A set of environment variables and their values to be added to the generated code (Default value = None)\n    \"\"\"\n    prog_name = prog_name or click.get_current_context().find_root().info_name\n    shell = shell or get_auto_shell()\n    if append is None and path is not None:\n        append = True\n    if append is not None:\n        mode = 'a' if append else 'w'\n    else:\n        mode = None\n\n    if shell == 'fish':\n        path = path or os.path.expanduser('~') + '/.config/fish/completions/%s.fish' % prog_name\n        mode = mode or 'w'\n    elif shell == 'bash':\n        path = path or os.path.expanduser('~') + '/.bash_completion'\n        mode = mode or 'a'\n    elif shell == 'zsh':\n        ohmyzsh = os.path.expanduser('~') + '/.oh-my-zsh'\n        if os.path.exists(ohmyzsh):\n            path = path or ohmyzsh + '/completions/_%s' % prog_name\n            mode = mode or 'w'\n        else:\n            path = path or os.path.expanduser('~') + '/.zshrc'\n            mode = mode or 'a'\n    elif shell == 'powershell':\n        subprocess.check_call(['powershell', 'Set-ExecutionPolicy Unrestricted -Scope CurrentUser'])\n        path = path or subprocess.check_output(['powershell', '-NoProfile', 'echo $profile']).strip() if install else ''\n        mode = mode or 'a'\n    else:\n        raise click.ClickException('%s is not supported.' % shell)\n\n    if append is not None:\n        mode = 'a' if append else 'w'\n    else:\n        mode = mode\n    d = os.path.dirname(path)\n    if not os.path.exists(d):\n        os.makedirs(d)\n    f = open(path, mode)\n    f.write(get_code(shell, prog_name, env_name, extra_env))\n    f.write(\"\\n\")\n    f.close()\n    return shell, path"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getTreeBuilder(treeType, implementation=None, **kwargs):\n\n    treeType = treeType.lower()\n    if treeType not in treeBuilderCache:\n        if treeType == \"dom\":\n            from . import dom\n            # Come up with a sane default (pref. from the stdlib)\n            if implementation is None:\n                from xml.dom import minidom\n                implementation = minidom\n            # NEVER cache here, caching is done in the dom submodule\n            return dom.getDomModule(implementation, **kwargs).TreeBuilder\n        elif treeType == \"lxml\":\n            from . import etree_lxml\n            treeBuilderCache[treeType] = etree_lxml.TreeBuilder\n        elif treeType == \"etree\":\n            from . import etree\n            if implementation is None:\n                implementation = default_etree\n            # NEVER cache here, caching is done in the etree submodule\n            return etree.getETreeModule(implementation, **kwargs).TreeBuilder\n        else:\n            raise ValueError(\"\"\"Unrecognised treebuilder \"%s\" \"\"\" % treeType)\n    return treeBuilderCache.get(treeType)", "response": "Returns a TreeBuilder class for various types of trees with built - in support."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef choose_boundary():\n    boundary = binascii.hexlify(os.urandom(16))\n    if six.PY3:\n        boundary = boundary.decode('ascii')\n    return boundary", "response": "A simple replacement for mimetools. choose_boundary."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef iter_field_objects(fields):\n    if isinstance(fields, dict):\n        i = six.iteritems(fields)\n    else:\n        i = iter(fields)\n\n    for field in i:\n        if isinstance(field, RequestField):\n            yield field\n        else:\n            yield RequestField.from_tuples(*field)", "response": "Iterate over fields.\n\n    Supports list of (k, v) tuples and dicts, and lists of\n    :class:`~urllib3.fields.RequestField`."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nencode a dictionary of fields using the multipart - form - data MIME format.", "response": "def encode_multipart_formdata(fields, boundary=None):\n    \"\"\"\n    Encode a dictionary of ``fields`` using the multipart/form-data MIME format.\n\n    :param fields:\n        Dictionary of fields or list of (key, :class:`~urllib3.fields.RequestField`).\n\n    :param boundary:\n        If not specified, then a random boundary will be generated using\n        :func:`urllib3.filepost.choose_boundary`.\n    \"\"\"\n    body = BytesIO()\n    if boundary is None:\n        boundary = choose_boundary()\n\n    for field in iter_field_objects(fields):\n        body.write(b('--%s\\r\\n' % (boundary)))\n\n        writer(body).write(field.render_headers())\n        data = field.data\n\n        if isinstance(data, int):\n            data = str(data)  # Backwards compatibility\n\n        if isinstance(data, six.text_type):\n            writer(body).write(data)\n        else:\n            body.write(data)\n\n        body.write(b'\\r\\n')\n\n    body.write(b('--%s--\\r\\n' % (boundary)))\n\n    content_type = str('multipart/form-data; boundary=%s' % boundary)\n\n    return body.getvalue(), content_type"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a resource finder for a given package.", "response": "def finder(package):\n    \"\"\"\n    Return a resource finder for a package.\n    :param package: The name of the package.\n    :return: A :class:`ResourceFinder` instance for the package.\n    \"\"\"\n    if package in _finder_cache:\n        result = _finder_cache[package]\n    else:\n        if package not in sys.modules:\n            __import__(package)\n        module = sys.modules[package]\n        path = getattr(module, '__path__', None)\n        if path is None:\n            raise DistlibException('You cannot get a finder for a module, '\n                                   'only for a package')\n        loader = getattr(module, '__loader__', None)\n        finder_maker = _finder_registry.get(type(loader))\n        if finder_maker is None:\n            raise DistlibException('Unable to locate finder for %r' % package)\n        result = finder_maker(module)\n        _finder_cache[package] = result\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef finder_for_path(path):\n    result = None\n    # calls any path hooks, gets importer into cache\n    pkgutil.get_importer(path)\n    loader = sys.path_importer_cache.get(path)\n    finder = _finder_registry.get(type(loader))\n    if finder:\n        module = _dummy_module\n        module.__file__ = os.path.join(path, '')\n        module.__loader__ = loader\n        result = finder(module)\n    return result", "response": "Returns a resource finder for a path."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a resource into the cache.", "response": "def get(self, resource):\n        \"\"\"\n        Get a resource into the cache,\n\n        :param resource: A :class:`Resource` instance.\n        :return: The pathname of the resource in the cache.\n        \"\"\"\n        prefix, path = resource.finder.get_cache_info(resource)\n        if prefix is None:\n            result = path\n        else:\n            result = os.path.join(self.base, self.prefix_to_dir(prefix), path)\n            dirname = os.path.dirname(result)\n            if not os.path.isdir(dirname):\n                os.makedirs(dirname)\n            if not os.path.exists(result):\n                stale = True\n            else:\n                stale = self.is_stale(resource, path)\n            if stale:\n                # write the bytes of the resource to the cache location\n                with open(result, 'wb') as f:\n                    f.write(resource.bytes)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_sax(walker, handler):\n    handler.startDocument()\n    for prefix, namespace in prefix_mapping.items():\n        handler.startPrefixMapping(prefix, namespace)\n\n    for token in walker:\n        type = token[\"type\"]\n        if type == \"Doctype\":\n            continue\n        elif type in (\"StartTag\", \"EmptyTag\"):\n            attrs = AttributesNSImpl(token[\"data\"],\n                                     unadjustForeignAttributes)\n            handler.startElementNS((token[\"namespace\"], token[\"name\"]),\n                                   token[\"name\"],\n                                   attrs)\n            if type == \"EmptyTag\":\n                handler.endElementNS((token[\"namespace\"], token[\"name\"]),\n                                     token[\"name\"])\n        elif type == \"EndTag\":\n            handler.endElementNS((token[\"namespace\"], token[\"name\"]),\n                                 token[\"name\"])\n        elif type in (\"Characters\", \"SpaceCharacters\"):\n            handler.characters(token[\"data\"])\n        elif type == \"Comment\":\n            pass\n        else:\n            assert False, \"Unknown token type\"\n\n    for prefix, namespace in prefix_mapping.items():\n        handler.endPrefixMapping(prefix)\n    handler.endDocument()", "response": "Convert a treewalker to SAX - like content handler based on the handler."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsleep a random amount of time between wait_random_min and wait_random_max", "response": "def random_sleep(self, previous_attempt_number, delay_since_first_attempt_ms):\n        \"\"\"Sleep a random amount of time between wait_random_min and wait_random_max\"\"\"\n        return random.randint(self._wait_random_min, self._wait_random_max)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef incrementing_sleep(self, previous_attempt_number, delay_since_first_attempt_ms):\n        result = self._wait_incrementing_start + (self._wait_incrementing_increment * (previous_attempt_number - 1))\n        if result < 0:\n            result = 0\n        return result", "response": "Increment the amount of time after each attempt."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the value of this Attempt instance or raise an Exception.", "response": "def get(self, wrap_exception=False):\n        \"\"\"\n        Return the return value of this Attempt instance or raise an Exception.\n        If wrap_exception is true, this Attempt is wrapped inside of a\n        RetryError before being raised.\n        \"\"\"\n        if self.has_exception:\n            if wrap_exception:\n                raise RetryError(self)\n            else:\n                six.reraise(self.value[0], self.value[1], self.value[2])\n        else:\n            return self.value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntesting if the attribute given is an internal python attribute.", "response": "def is_internal_attribute(obj, attr):\n    \"\"\"Test if the attribute given is an internal python attribute.  For\n    example this function returns `True` for the `func_code` attribute of\n    python objects.  This is useful if the environment method\n    :meth:`~SandboxedEnvironment.is_safe_attribute` is overridden.\n\n    >>> from jinja2.sandbox import is_internal_attribute\n    >>> is_internal_attribute(str, \"mro\")\n    True\n    >>> is_internal_attribute(str, \"upper\")\n    False\n    \"\"\"\n    if isinstance(obj, types.FunctionType):\n        if attr in UNSAFE_FUNCTION_ATTRIBUTES:\n            return True\n    elif isinstance(obj, types.MethodType):\n        if attr in UNSAFE_FUNCTION_ATTRIBUTES or \\\n           attr in UNSAFE_METHOD_ATTRIBUTES:\n            return True\n    elif isinstance(obj, type):\n        if attr == 'mro':\n            return True\n    elif isinstance(obj, (types.CodeType, types.TracebackType, types.FrameType)):\n        return True\n    elif isinstance(obj, types.GeneratorType):\n        if attr in UNSAFE_GENERATOR_ATTRIBUTES:\n            return True\n    elif hasattr(types, 'CoroutineType') and isinstance(obj, types.CoroutineType):\n        if attr in UNSAFE_COROUTINE_ATTRIBUTES:\n            return True\n    elif hasattr(types, 'AsyncGeneratorType') and isinstance(obj, types.AsyncGeneratorType):\n        if attr in UNSAFE_ASYNC_GENERATOR_ATTRIBUTES:\n            return True\n    return attr.startswith('__')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef modifies_known_mutable(obj, attr):\n    for typespec, unsafe in _mutable_spec:\n        if isinstance(obj, typespec):\n            return attr in unsafe\n    return False", "response": "Checks if an attribute on a builtin mutable object is modified if called."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_safe_attribute(self, obj, attr, value):\n        return not (attr.startswith('_') or is_internal_attribute(obj, attr))", "response": "This method is used to check if the passed in object attribute is safe to access."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef call_binop(self, context, operator, left, right):\n        return self.binop_table[operator](left, right)", "response": "This function is used to call the builtin binary operator on the current canonetags."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsubscribes an object from sandboxed code and prefer the attribute.", "response": "def getattr(self, obj, attribute):\n        \"\"\"Subscribe an object from sandboxed code and prefer the\n        attribute.  The attribute passed *must* be a bytestring.\n        \"\"\"\n        try:\n            value = getattr(obj, attribute)\n        except AttributeError:\n            try:\n                return obj[attribute]\n            except (TypeError, LookupError):\n                pass\n        else:\n            if self.is_safe_attribute(obj, attribute, value):\n                return value\n            return self.unsafe_undefined(obj, attribute)\n        return self.undefined(obj=obj, name=attribute)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns an undefined object for unsafe attributes.", "response": "def unsafe_undefined(self, obj, attribute):\n        \"\"\"Return an undefined object for unsafe attributes.\"\"\"\n        return self.undefined('access to attribute %r of %r '\n                              'object is unsafe.' % (\n            attribute,\n            obj.__class__.__name__\n        ), name=attribute, obj=obj, exc=SecurityError)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef format_string(self, s, args, kwargs):\n        if isinstance(s, Markup):\n            formatter = SandboxedEscapeFormatter(self, s.escape)\n        else:\n            formatter = SandboxedFormatter(self)\n        kwargs = _MagicFormatMapping(args, kwargs)\n        rv = formatter.vformat(s, args, kwargs)\n        return type(s)(rv)", "response": "Format a string with the arguments and kwargs."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalls an object from sandboxed code.", "response": "def call(__self, __context, __obj, *args, **kwargs):\n        \"\"\"Call an object from sandboxed code.\"\"\"\n        fmt = inspect_format_method(__obj)\n        if fmt is not None:\n            return __self.format_string(fmt, args, kwargs)\n\n        # the double prefixes are to avoid double keyword argument\n        # errors when proxying the call.\n        if not __self.is_safe_callable(__obj):\n            raise SecurityError('%r is not safely callable' % (__obj,))\n        return __context.call(__obj, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a new attribute on a base class.", "response": "def attrib(\n    default=NOTHING,\n    validator=None,\n    repr=True,\n    cmp=True,\n    hash=None,\n    init=True,\n    convert=None,\n    metadata=None,\n    type=None,\n    converter=None,\n    factory=None,\n    kw_only=False,\n):\n    \"\"\"\n    Create a new attribute on a class.\n\n    ..  warning::\n\n        Does *not* do anything unless the class is also decorated with\n        :func:`attr.s`!\n\n    :param default: A value that is used if an ``attrs``-generated ``__init__``\n        is used and no value is passed while instantiating or the attribute is\n        excluded using ``init=False``.\n\n        If the value is an instance of :class:`Factory`, its callable will be\n        used to construct a new value (useful for mutable data types like lists\n        or dicts).\n\n        If a default is not set (or set manually to ``attr.NOTHING``), a value\n        *must* be supplied when instantiating; otherwise a :exc:`TypeError`\n        will be raised.\n\n        The default can also be set using decorator notation as shown below.\n\n    :type default: Any value.\n\n    :param callable factory: Syntactic sugar for\n        ``default=attr.Factory(callable)``.\n\n    :param validator: :func:`callable` that is called by ``attrs``-generated\n        ``__init__`` methods after the instance has been initialized.  They\n        receive the initialized instance, the :class:`Attribute`, and the\n        passed value.\n\n        The return value is *not* inspected so the validator has to throw an\n        exception itself.\n\n        If a ``list`` is passed, its items are treated as validators and must\n        all pass.\n\n        Validators can be globally disabled and re-enabled using\n        :func:`get_run_validators`.\n\n        The validator can also be set using decorator notation as shown below.\n\n    :type validator: ``callable`` or a ``list`` of ``callable``\\\\ s.\n\n    :param bool repr: Include this attribute in the generated ``__repr__``\n        method.\n    :param bool cmp: Include this attribute in the generated comparison methods\n        (``__eq__`` et al).\n    :param hash: Include this attribute in the generated ``__hash__``\n        method.  If ``None`` (default), mirror *cmp*'s value.  This is the\n        correct behavior according the Python spec.  Setting this value to\n        anything else than ``None`` is *discouraged*.\n    :type hash: ``bool`` or ``None``\n    :param bool init: Include this attribute in the generated ``__init__``\n        method.  It is possible to set this to ``False`` and set a default\n        value.  In that case this attributed is unconditionally initialized\n        with the specified default value or factory.\n    :param callable converter: :func:`callable` that is called by\n        ``attrs``-generated ``__init__`` methods to converter attribute's value\n        to the desired format.  It is given the passed-in value, and the\n        returned value will be used as the new value of the attribute.  The\n        value is converted before being passed to the validator, if any.\n    :param metadata: An arbitrary mapping, to be used by third-party\n        components.  See :ref:`extending_metadata`.\n    :param type: The type of the attribute.  In Python 3.6 or greater, the\n        preferred method to specify the type is using a variable annotation\n        (see `PEP 526 <https://www.python.org/dev/peps/pep-0526/>`_).\n        This argument is provided for backward compatibility.\n        Regardless of the approach used, the type will be stored on\n        ``Attribute.type``.\n\n        Please note that ``attrs`` doesn't do anything with this metadata by\n        itself. You can use it as part of your own code or for\n        :doc:`static type checking <types>`.\n    :param kw_only: Make this attribute keyword-only (Python 3+)\n        in the generated ``__init__`` (if ``init`` is ``False``, this\n        parameter is ignored).\n\n    .. versionadded:: 15.2.0 *convert*\n    .. versionadded:: 16.3.0 *metadata*\n    .. versionchanged:: 17.1.0 *validator* can be a ``list`` now.\n    .. versionchanged:: 17.1.0\n       *hash* is ``None`` and therefore mirrors *cmp* by default.\n    .. versionadded:: 17.3.0 *type*\n    .. deprecated:: 17.4.0 *convert*\n    .. versionadded:: 17.4.0 *converter* as a replacement for the deprecated\n       *convert* to achieve consistency with other noun-based arguments.\n    .. versionadded:: 18.1.0\n       ``factory=f`` is syntactic sugar for ``default=attr.Factory(f)``.\n    .. versionadded:: 18.2.0 *kw_only*\n    \"\"\"\n    if hash is not None and hash is not True and hash is not False:\n        raise TypeError(\n            \"Invalid value for hash.  Must be True, False, or None.\"\n        )\n\n    if convert is not None:\n        if converter is not None:\n            raise RuntimeError(\n                \"Can't pass both `convert` and `converter`.  \"\n                \"Please use `converter` only.\"\n            )\n        warnings.warn(\n            \"The `convert` argument is deprecated in favor of `converter`.  \"\n            \"It will be removed after 2019/01.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        converter = convert\n\n    if factory is not None:\n        if default is not NOTHING:\n            raise ValueError(\n                \"The `default` and `factory` arguments are mutually \"\n                \"exclusive.\"\n            )\n        if not callable(factory):\n            raise ValueError(\"The `factory` argument must be a callable.\")\n        default = Factory(factory)\n\n    if metadata is None:\n        metadata = {}\n\n    return _CountingAttr(\n        default=default,\n        validator=validator,\n        repr=repr,\n        cmp=cmp,\n        hash=hash,\n        init=init,\n        converter=converter,\n        metadata=metadata,\n        type=type,\n        kw_only=kw_only,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a tuple subclass to hold Attribute s for an attrs class.", "response": "def _make_attr_tuple_class(cls_name, attr_names):\n    \"\"\"\n    Create a tuple subclass to hold `Attribute`s for an `attrs` class.\n\n    The subclass is a bare tuple with properties for names.\n\n    class MyClassAttributes(tuple):\n        __slots__ = ()\n        x = property(itemgetter(0))\n    \"\"\"\n    attr_class_name = \"{}Attributes\".format(cls_name)\n    attr_class_template = [\n        \"class {}(tuple):\".format(attr_class_name),\n        \"    __slots__ = ()\",\n    ]\n    if attr_names:\n        for i, attr_name in enumerate(attr_names):\n            attr_class_template.append(\n                _tuple_property_pat.format(index=i, attr_name=attr_name)\n            )\n    else:\n        attr_class_template.append(\"    pass\")\n    globs = {\"_attrs_itemgetter\": itemgetter, \"_attrs_property\": property}\n    eval(compile(\"\\n\".join(attr_class_template), \"\", \"exec\"), globs)\n\n    return globs[attr_class_name]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_annotations(cls):\n    anns = getattr(cls, \"__annotations__\", None)\n    if anns is None:\n        return {}\n\n    # Verify that the annotations aren't merely inherited.\n    for base_cls in cls.__mro__[1:]:\n        if anns is getattr(base_cls, \"__annotations__\", None):\n            return {}\n\n    return anns", "response": "Get annotations for cls."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _transform_attrs(cls, these, auto_attribs, kw_only):\n    cd = cls.__dict__\n    anns = _get_annotations(cls)\n\n    if these is not None:\n        ca_list = [(name, ca) for name, ca in iteritems(these)]\n\n        if not isinstance(these, ordered_dict):\n            ca_list.sort(key=_counter_getter)\n    elif auto_attribs is True:\n        ca_names = {\n            name\n            for name, attr in cd.items()\n            if isinstance(attr, _CountingAttr)\n        }\n        ca_list = []\n        annot_names = set()\n        for attr_name, type in anns.items():\n            if _is_class_var(type):\n                continue\n            annot_names.add(attr_name)\n            a = cd.get(attr_name, NOTHING)\n            if not isinstance(a, _CountingAttr):\n                if a is NOTHING:\n                    a = attrib()\n                else:\n                    a = attrib(default=a)\n            ca_list.append((attr_name, a))\n\n        unannotated = ca_names - annot_names\n        if len(unannotated) > 0:\n            raise UnannotatedAttributeError(\n                \"The following `attr.ib`s lack a type annotation: \"\n                + \", \".join(\n                    sorted(unannotated, key=lambda n: cd.get(n).counter)\n                )\n                + \".\"\n            )\n    else:\n        ca_list = sorted(\n            (\n                (name, attr)\n                for name, attr in cd.items()\n                if isinstance(attr, _CountingAttr)\n            ),\n            key=lambda e: e[1].counter,\n        )\n\n    own_attrs = [\n        Attribute.from_counting_attr(\n            name=attr_name, ca=ca, type=anns.get(attr_name)\n        )\n        for attr_name, ca in ca_list\n    ]\n\n    base_attrs = []\n    base_attr_map = {}  # A dictionary of base attrs to their classes.\n    taken_attr_names = {a.name: a for a in own_attrs}\n\n    # Traverse the MRO and collect attributes.\n    for base_cls in cls.__mro__[1:-1]:\n        sub_attrs = getattr(base_cls, \"__attrs_attrs__\", None)\n        if sub_attrs is not None:\n            for a in sub_attrs:\n                prev_a = taken_attr_names.get(a.name)\n                # Only add an attribute if it hasn't been defined before.  This\n                # allows for overwriting attribute definitions by subclassing.\n                if prev_a is None:\n                    base_attrs.append(a)\n                    taken_attr_names[a.name] = a\n                    base_attr_map[a.name] = base_cls\n\n    attr_names = [a.name for a in base_attrs + own_attrs]\n\n    AttrsClass = _make_attr_tuple_class(cls.__name__, attr_names)\n\n    if kw_only:\n        own_attrs = [a._assoc(kw_only=True) for a in own_attrs]\n        base_attrs = [a._assoc(kw_only=True) for a in base_attrs]\n\n    attrs = AttrsClass(base_attrs + own_attrs)\n\n    had_default = False\n    was_kw_only = False\n    for a in attrs:\n        if (\n            was_kw_only is False\n            and had_default is True\n            and a.default is NOTHING\n            and a.init is True\n            and a.kw_only is False\n        ):\n            raise ValueError(\n                \"No mandatory attributes allowed after an attribute with a \"\n                \"default value or factory.  Attribute in question: %r\" % (a,)\n            )\n        elif (\n            had_default is False\n            and a.default is not NOTHING\n            and a.init is not False\n            and\n            # Keyword-only attributes without defaults can be specified\n            # after keyword-only attributes with defaults.\n            a.kw_only is False\n        ):\n            had_default = True\n        if was_kw_only is True and a.kw_only is False and a.init is True:\n            raise ValueError(\n                \"Non keyword-only attributes are not allowed after a \"\n                \"keyword-only attribute (unless they are init=False).  \"\n                \"Attribute in question: {a!r}\".format(a=a)\n            )\n        if was_kw_only is False and a.init is True and a.kw_only is True:\n            was_kw_only = True\n\n    return _Attributes((attrs, base_attrs, base_attr_map))", "response": "Transform all _CountingAttr s on a class into _Attributes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a tuple of all values of obj s attrs.", "response": "def _attrs_to_tuple(obj, attrs):\n    \"\"\"\n    Create a tuple of all values of *obj*'s *attrs*.\n    \"\"\"\n    return tuple(getattr(obj, a.name) for a in attrs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _add_hash(cls, attrs):\n    cls.__hash__ = _make_hash(attrs, frozen=False, cache_hash=False)\n    return cls", "response": "Add a hash method to cls."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds comparison methods to cls.", "response": "def _add_cmp(cls, attrs=None):\n    \"\"\"\n    Add comparison methods to *cls*.\n    \"\"\"\n    if attrs is None:\n        attrs = cls.__attrs_attrs__\n\n    cls.__eq__, cls.__ne__, cls.__lt__, cls.__le__, cls.__gt__, cls.__ge__ = _make_cmp(  # noqa\n        attrs\n    )\n\n    return cls"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmaking a repr method for attr_names adding ns to the full name.", "response": "def _make_repr(attrs, ns):\n    \"\"\"\n    Make a repr method for *attr_names* adding *ns* to the full name.\n    \"\"\"\n    attr_names = tuple(a.name for a in attrs if a.repr)\n\n    def __repr__(self):\n        \"\"\"\n        Automatically created by attrs.\n        \"\"\"\n        try:\n            working_set = _already_repring.working_set\n        except AttributeError:\n            working_set = set()\n            _already_repring.working_set = working_set\n\n        if id(self) in working_set:\n            return \"...\"\n        real_cls = self.__class__\n        if ns is None:\n            qualname = getattr(real_cls, \"__qualname__\", None)\n            if qualname is not None:\n                class_name = qualname.rsplit(\">.\", 1)[-1]\n            else:\n                class_name = real_cls.__name__\n        else:\n            class_name = ns + \".\" + real_cls.__name__\n\n        # Since 'self' remains on the stack (i.e.: strongly referenced) for the\n        # duration of this call, it's safe to depend on id(...) stability, and\n        # not need to track the instance and therefore worry about properties\n        # like weakref- or hash-ability.\n        working_set.add(id(self))\n        try:\n            result = [class_name, \"(\"]\n            first = True\n            for name in attr_names:\n                if first:\n                    first = False\n                else:\n                    result.append(\", \")\n                result.extend((name, \"=\", repr(getattr(self, name, NOTHING))))\n            return \"\".join(result) + \")\"\n        finally:\n            working_set.remove(id(self))\n\n    return __repr__"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _add_repr(cls, ns=None, attrs=None):\n    if attrs is None:\n        attrs = cls.__attrs_attrs__\n\n    cls.__repr__ = _make_repr(attrs, ns)\n    return cls", "response": "Add a repr method to cls."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the tuple of attrs attributes for a class.", "response": "def fields(cls):\n    \"\"\"\n    Return the tuple of ``attrs`` attributes for a class.\n\n    The tuple also allows accessing the fields by their names (see below for\n    examples).\n\n    :param type cls: Class to introspect.\n\n    :raise TypeError: If *cls* is not a class.\n    :raise attr.exceptions.NotAnAttrsClassError: If *cls* is not an ``attrs``\n        class.\n\n    :rtype: tuple (with name accessors) of :class:`attr.Attribute`\n\n    ..  versionchanged:: 16.2.0 Returned tuple allows accessing the fields\n        by name.\n    \"\"\"\n    if not isclass(cls):\n        raise TypeError(\"Passed object must be a class.\")\n    attrs = getattr(cls, \"__attrs_attrs__\", None)\n    if attrs is None:\n        raise NotAnAttrsClassError(\n            \"{cls!r} is not an attrs-decorated class.\".format(cls=cls)\n        )\n    return attrs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fields_dict(cls):\n    if not isclass(cls):\n        raise TypeError(\"Passed object must be a class.\")\n    attrs = getattr(cls, \"__attrs_attrs__\", None)\n    if attrs is None:\n        raise NotAnAttrsClassError(\n            \"{cls!r} is not an attrs-decorated class.\".format(cls=cls)\n        )\n    return ordered_dict(((a.name, a) for a in attrs))", "response": "Returns an ordered dictionary of attrs attributes for a class."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nvalidates all attributes on inst that have a validator.", "response": "def validate(inst):\n    \"\"\"\n    Validate all attributes on *inst* that have a validator.\n\n    Leaves all exceptions through.\n\n    :param inst: Instance of a class with ``attrs`` attributes.\n    \"\"\"\n    if _config._run_validators is False:\n        return\n\n    for a in fields(inst.__class__):\n        v = a.validator\n        if v is not None:\n            v(inst, a, getattr(inst, a.name))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a script that can be used to initialize a new object.", "response": "def _attrs_to_init_script(\n    attrs, frozen, slots, post_init, cache_hash, base_attr_map, is_exc\n):\n    \"\"\"\n    Return a script of an initializer for *attrs* and a dict of globals.\n\n    The globals are expected by the generated script.\n\n    If *frozen* is True, we cannot set the attributes directly so we use\n    a cached ``object.__setattr__``.\n    \"\"\"\n    lines = []\n    any_slot_ancestors = any(\n        _is_slot_attr(a.name, base_attr_map) for a in attrs\n    )\n    if frozen is True:\n        if slots is True:\n            lines.append(\n                # Circumvent the __setattr__ descriptor to save one lookup per\n                # assignment.\n                # Note _setattr will be used again below if cache_hash is True\n                \"_setattr = _cached_setattr.__get__(self, self.__class__)\"\n            )\n\n            def fmt_setter(attr_name, value_var):\n                return \"_setattr('%(attr_name)s', %(value_var)s)\" % {\n                    \"attr_name\": attr_name,\n                    \"value_var\": value_var,\n                }\n\n            def fmt_setter_with_converter(attr_name, value_var):\n                conv_name = _init_converter_pat.format(attr_name)\n                return \"_setattr('%(attr_name)s', %(conv)s(%(value_var)s))\" % {\n                    \"attr_name\": attr_name,\n                    \"value_var\": value_var,\n                    \"conv\": conv_name,\n                }\n\n        else:\n            # Dict frozen classes assign directly to __dict__.\n            # But only if the attribute doesn't come from an ancestor slot\n            # class.\n            # Note _inst_dict will be used again below if cache_hash is True\n            lines.append(\"_inst_dict = self.__dict__\")\n            if any_slot_ancestors:\n                lines.append(\n                    # Circumvent the __setattr__ descriptor to save one lookup\n                    # per assignment.\n                    \"_setattr = _cached_setattr.__get__(self, self.__class__)\"\n                )\n\n            def fmt_setter(attr_name, value_var):\n                if _is_slot_attr(attr_name, base_attr_map):\n                    res = \"_setattr('%(attr_name)s', %(value_var)s)\" % {\n                        \"attr_name\": attr_name,\n                        \"value_var\": value_var,\n                    }\n                else:\n                    res = \"_inst_dict['%(attr_name)s'] = %(value_var)s\" % {\n                        \"attr_name\": attr_name,\n                        \"value_var\": value_var,\n                    }\n                return res\n\n            def fmt_setter_with_converter(attr_name, value_var):\n                conv_name = _init_converter_pat.format(attr_name)\n                if _is_slot_attr(attr_name, base_attr_map):\n                    tmpl = \"_setattr('%(attr_name)s', %(c)s(%(value_var)s))\"\n                else:\n                    tmpl = \"_inst_dict['%(attr_name)s'] = %(c)s(%(value_var)s)\"\n                return tmpl % {\n                    \"attr_name\": attr_name,\n                    \"value_var\": value_var,\n                    \"c\": conv_name,\n                }\n\n    else:\n        # Not frozen.\n        def fmt_setter(attr_name, value):\n            return \"self.%(attr_name)s = %(value)s\" % {\n                \"attr_name\": attr_name,\n                \"value\": value,\n            }\n\n        def fmt_setter_with_converter(attr_name, value_var):\n            conv_name = _init_converter_pat.format(attr_name)\n            return \"self.%(attr_name)s = %(conv)s(%(value_var)s)\" % {\n                \"attr_name\": attr_name,\n                \"value_var\": value_var,\n                \"conv\": conv_name,\n            }\n\n    args = []\n    kw_only_args = []\n    attrs_to_validate = []\n\n    # This is a dictionary of names to validator and converter callables.\n    # Injecting this into __init__ globals lets us avoid lookups.\n    names_for_globals = {}\n    annotations = {\"return\": None}\n\n    for a in attrs:\n        if a.validator:\n            attrs_to_validate.append(a)\n        attr_name = a.name\n        arg_name = a.name.lstrip(\"_\")\n        has_factory = isinstance(a.default, Factory)\n        if has_factory and a.default.takes_self:\n            maybe_self = \"self\"\n        else:\n            maybe_self = \"\"\n        if a.init is False:\n            if has_factory:\n                init_factory_name = _init_factory_pat.format(a.name)\n                if a.converter is not None:\n                    lines.append(\n                        fmt_setter_with_converter(\n                            attr_name,\n                            init_factory_name + \"({0})\".format(maybe_self),\n                        )\n                    )\n                    conv_name = _init_converter_pat.format(a.name)\n                    names_for_globals[conv_name] = a.converter\n                else:\n                    lines.append(\n                        fmt_setter(\n                            attr_name,\n                            init_factory_name + \"({0})\".format(maybe_self),\n                        )\n                    )\n                names_for_globals[init_factory_name] = a.default.factory\n            else:\n                if a.converter is not None:\n                    lines.append(\n                        fmt_setter_with_converter(\n                            attr_name,\n                            \"attr_dict['{attr_name}'].default\".format(\n                                attr_name=attr_name\n                            ),\n                        )\n                    )\n                    conv_name = _init_converter_pat.format(a.name)\n                    names_for_globals[conv_name] = a.converter\n                else:\n                    lines.append(\n                        fmt_setter(\n                            attr_name,\n                            \"attr_dict['{attr_name}'].default\".format(\n                                attr_name=attr_name\n                            ),\n                        )\n                    )\n        elif a.default is not NOTHING and not has_factory:\n            arg = \"{arg_name}=attr_dict['{attr_name}'].default\".format(\n                arg_name=arg_name, attr_name=attr_name\n            )\n            if a.kw_only:\n                kw_only_args.append(arg)\n            else:\n                args.append(arg)\n            if a.converter is not None:\n                lines.append(fmt_setter_with_converter(attr_name, arg_name))\n                names_for_globals[\n                    _init_converter_pat.format(a.name)\n                ] = a.converter\n            else:\n                lines.append(fmt_setter(attr_name, arg_name))\n        elif has_factory:\n            arg = \"{arg_name}=NOTHING\".format(arg_name=arg_name)\n            if a.kw_only:\n                kw_only_args.append(arg)\n            else:\n                args.append(arg)\n            lines.append(\n                \"if {arg_name} is not NOTHING:\".format(arg_name=arg_name)\n            )\n            init_factory_name = _init_factory_pat.format(a.name)\n            if a.converter is not None:\n                lines.append(\n                    \"    \" + fmt_setter_with_converter(attr_name, arg_name)\n                )\n                lines.append(\"else:\")\n                lines.append(\n                    \"    \"\n                    + fmt_setter_with_converter(\n                        attr_name,\n                        init_factory_name + \"({0})\".format(maybe_self),\n                    )\n                )\n                names_for_globals[\n                    _init_converter_pat.format(a.name)\n                ] = a.converter\n            else:\n                lines.append(\"    \" + fmt_setter(attr_name, arg_name))\n                lines.append(\"else:\")\n                lines.append(\n                    \"    \"\n                    + fmt_setter(\n                        attr_name,\n                        init_factory_name + \"({0})\".format(maybe_self),\n                    )\n                )\n            names_for_globals[init_factory_name] = a.default.factory\n        else:\n            if a.kw_only:\n                kw_only_args.append(arg_name)\n            else:\n                args.append(arg_name)\n            if a.converter is not None:\n                lines.append(fmt_setter_with_converter(attr_name, arg_name))\n                names_for_globals[\n                    _init_converter_pat.format(a.name)\n                ] = a.converter\n            else:\n                lines.append(fmt_setter(attr_name, arg_name))\n\n        if a.init is True and a.converter is None and a.type is not None:\n            annotations[arg_name] = a.type\n\n    if attrs_to_validate:  # we can skip this if there are no validators.\n        names_for_globals[\"_config\"] = _config\n        lines.append(\"if _config._run_validators is True:\")\n        for a in attrs_to_validate:\n            val_name = \"__attr_validator_{}\".format(a.name)\n            attr_name = \"__attr_{}\".format(a.name)\n            lines.append(\n                \"    {}(self, {}, self.{})\".format(val_name, attr_name, a.name)\n            )\n            names_for_globals[val_name] = a.validator\n            names_for_globals[attr_name] = a\n    if post_init:\n        lines.append(\"self.__attrs_post_init__()\")\n\n    # because this is set only after __attrs_post_init is called, a crash\n    # will result if post-init tries to access the hash code.  This seemed\n    # preferable to setting this beforehand, in which case alteration to\n    # field values during post-init combined with post-init accessing the\n    # hash code would result in silent bugs.\n    if cache_hash:\n        if frozen:\n            if slots:\n                # if frozen and slots, then _setattr defined above\n                init_hash_cache = \"_setattr('%s', %s)\"\n            else:\n                # if frozen and not slots, then _inst_dict defined above\n                init_hash_cache = \"_inst_dict['%s'] = %s\"\n        else:\n            init_hash_cache = \"self.%s = %s\"\n        lines.append(init_hash_cache % (_hash_cache_field, \"None\"))\n\n    # For exceptions we rely on BaseException.__init__ for proper\n    # initialization.\n    if is_exc:\n        vals = \",\".join(\"self.\" + a.name for a in attrs if a.init)\n\n        lines.append(\"BaseException.__init__(self, %s)\" % (vals,))\n\n    args = \", \".join(args)\n    if kw_only_args:\n        if PY2:\n            raise PythonTooOldError(\n                \"Keyword-only arguments only work on Python 3 and later.\"\n            )\n\n        args += \"{leading_comma}*, {kw_only_args}\".format(\n            leading_comma=\", \" if args else \"\",\n            kw_only_args=\", \".join(kw_only_args),\n        )\n    return (\n        \"\"\"\\\ndef __init__(self, {args}):\n    {lines}\n\"\"\".format(\n            args=args, lines=\"\\n    \".join(lines) if lines else \"pass\"\n        ),\n        names_for_globals,\n        annotations,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make_class(name, attrs, bases=(object,), **attributes_arguments):\n    if isinstance(attrs, dict):\n        cls_dict = attrs\n    elif isinstance(attrs, (list, tuple)):\n        cls_dict = dict((a, attrib()) for a in attrs)\n    else:\n        raise TypeError(\"attrs argument must be a dict or a list.\")\n\n    post_init = cls_dict.pop(\"__attrs_post_init__\", None)\n    type_ = type(\n        name,\n        bases,\n        {} if post_init is None else {\"__attrs_post_init__\": post_init},\n    )\n    # For pickling to work, the __module__ variable needs to be set to the\n    # frame where the class is created.  Bypass this step in environments where\n    # sys._getframe is not defined (Jython for example) or sys._getframe is not\n    # defined for arguments greater than 0 (IronPython).\n    try:\n        type_.__module__ = sys._getframe(1).f_globals.get(\n            \"__name__\", \"__main__\"\n        )\n    except (AttributeError, ValueError):\n        pass\n\n    return _attrs(these=cls_dict, **attributes_arguments)(type_)", "response": "A quick way to create a new class called name with attrs."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _patch_original_class(self):\n        cls = self._cls\n        base_names = self._base_names\n\n        # Clean class of attribute definitions (`attr.ib()`s).\n        if self._delete_attribs:\n            for name in self._attr_names:\n                if (\n                    name not in base_names\n                    and getattr(cls, name, None) is not None\n                ):\n                    try:\n                        delattr(cls, name)\n                    except AttributeError:\n                        # This can happen if a base class defines a class\n                        # variable and we want to set an attribute with the\n                        # same name by using only a type annotation.\n                        pass\n\n        # Attach our dunder methods.\n        for name, value in self._cls_dict.items():\n            setattr(cls, name, value)\n\n        # Attach __setstate__. This is necessary to clear the hash code\n        # cache on deserialization. See issue\n        # https://github.com/python-attrs/attrs/issues/482 .\n        # Note that this code only handles setstate for dict classes.\n        # For slotted classes, see similar code in _create_slots_class .\n        if self._cache_hash:\n            existing_set_state_method = getattr(cls, \"__setstate__\", None)\n            if existing_set_state_method:\n                raise NotImplementedError(\n                    \"Currently you cannot use hash caching if \"\n                    \"you specify your own __setstate__ method.\"\n                    \"See https://github.com/python-attrs/attrs/issues/494 .\"\n                )\n\n            def cache_hash_set_state(chss_self, _):\n                # clear hash code cache\n                setattr(chss_self, _hash_cache_field, None)\n\n            setattr(cls, \"__setstate__\", cache_hash_set_state)\n\n        return cls", "response": "Patch the original class with the new values."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds and return a new class with a __slots__ attribute.", "response": "def _create_slots_class(self):\n        \"\"\"\n        Build and return a new class with a `__slots__` attribute.\n        \"\"\"\n        base_names = self._base_names\n        cd = {\n            k: v\n            for k, v in iteritems(self._cls_dict)\n            if k not in tuple(self._attr_names) + (\"__dict__\", \"__weakref__\")\n        }\n\n        weakref_inherited = False\n\n        # Traverse the MRO to check for an existing __weakref__.\n        for base_cls in self._cls.__mro__[1:-1]:\n            if \"__weakref__\" in getattr(base_cls, \"__dict__\", ()):\n                weakref_inherited = True\n                break\n\n        names = self._attr_names\n        if (\n            self._weakref_slot\n            and \"__weakref__\" not in getattr(self._cls, \"__slots__\", ())\n            and \"__weakref__\" not in names\n            and not weakref_inherited\n        ):\n            names += (\"__weakref__\",)\n\n        # We only add the names of attributes that aren't inherited.\n        # Settings __slots__ to inherited attributes wastes memory.\n        slot_names = [name for name in names if name not in base_names]\n        if self._cache_hash:\n            slot_names.append(_hash_cache_field)\n        cd[\"__slots__\"] = tuple(slot_names)\n\n        qualname = getattr(self._cls, \"__qualname__\", None)\n        if qualname is not None:\n            cd[\"__qualname__\"] = qualname\n\n        # __weakref__ is not writable.\n        state_attr_names = tuple(\n            an for an in self._attr_names if an != \"__weakref__\"\n        )\n\n        def slots_getstate(self):\n            \"\"\"\n            Automatically created by attrs.\n            \"\"\"\n            return tuple(getattr(self, name) for name in state_attr_names)\n\n        hash_caching_enabled = self._cache_hash\n\n        def slots_setstate(self, state):\n            \"\"\"\n            Automatically created by attrs.\n            \"\"\"\n            __bound_setattr = _obj_setattr.__get__(self, Attribute)\n            for name, value in zip(state_attr_names, state):\n                __bound_setattr(name, value)\n            # Clearing the hash code cache on deserialization is needed\n            # because hash codes can change from run to run. See issue\n            # https://github.com/python-attrs/attrs/issues/482 .\n            # Note that this code only handles setstate for slotted classes.\n            # For dict classes, see similar code in _patch_original_class .\n            if hash_caching_enabled:\n                __bound_setattr(_hash_cache_field, None)\n\n        # slots and frozen require __getstate__/__setstate__ to work\n        cd[\"__getstate__\"] = slots_getstate\n        cd[\"__setstate__\"] = slots_setstate\n\n        # Create new class based on old class and our methods.\n        cls = type(self._cls)(self._cls.__name__, self._cls.__bases__, cd)\n\n        # The following is a fix for\n        # https://github.com/python-attrs/attrs/issues/102.  On Python 3,\n        # if a method mentions `__class__` or uses the no-arg super(), the\n        # compiler will bake a reference to the class in the method itself\n        # as `method.__closure__`.  Since we replace the class with a\n        # clone, we rewrite these references so it keeps working.\n        for item in cls.__dict__.values():\n            if isinstance(item, (classmethod, staticmethod)):\n                # Class- and staticmethods hide their functions inside.\n                # These might need to be rewritten as well.\n                closure_cells = getattr(item.__func__, \"__closure__\", None)\n            else:\n                closure_cells = getattr(item, \"__closure__\", None)\n\n            if not closure_cells:  # Catch None or the empty list.\n                continue\n            for cell in closure_cells:\n                if cell.cell_contents is self._cls:\n                    set_closure_cell(cell, cls)\n\n        return cls"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds __module__ and __qualname__ to a method if possible.", "response": "def _add_method_dunders(self, method):\n        \"\"\"\n        Add __module__ and __qualname__ to a *method* if possible.\n        \"\"\"\n        try:\n            method.__module__ = self._cls.__module__\n        except AttributeError:\n            pass\n\n        try:\n            method.__qualname__ = \".\".join(\n                (self._cls.__qualname__, method.__name__)\n            )\n        except AttributeError:\n            pass\n\n        return method"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a new object with the same attributes as self and the given changes.", "response": "def _assoc(self, **changes):\n        \"\"\"\n        Copy *self* and apply *changes*.\n        \"\"\"\n        new = copy.copy(self)\n\n        new._setattrs(changes.items())\n\n        return new"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _expand_args(command):\n\n    # Prepare arguments.\n    if isinstance(command, STR_TYPES):\n        if sys.version_info[0] == 2:\n            splitter = shlex.shlex(command.encode(\"utf-8\"))\n        elif sys.version_info[0] == 3:\n            splitter = shlex.shlex(command)\n        else:\n            splitter = shlex.shlex(command.encode(\"utf-8\"))\n        splitter.whitespace = \"|\"\n        splitter.whitespace_split = True\n        command = []\n\n        while True:\n            token = splitter.get_token()\n            if token:\n                command.append(token)\n            else:\n                break\n\n        command = list(map(shlex.split, command))\n\n    return command", "response": "Parses command strings and returns a Popen - ready list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef out(self):\n        if self.__out is not None:\n            return self.__out\n\n        if self._uses_subprocess:\n            self.__out = self.std_out.read()\n        else:\n            self.__out = self._pexpect_out\n\n        return self.__out", "response": "Return the output of the current process."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwait on the given pattern to appear in std_out.", "response": "def expect(self, pattern, timeout=-1):\n        \"\"\"Waits on the given pattern to appear in std_out\"\"\"\n\n        if self.blocking:\n            raise RuntimeError(\"expect can only be used on non-blocking commands.\")\n\n        try:\n            self.subprocess.expect(pattern=pattern, timeout=timeout)\n        except pexpect.EOF:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef block(self):\n        if self._uses_subprocess:\n            # consume stdout and stderr\n            if self.blocking:\n                try:\n                    stdout, stderr = self.subprocess.communicate()\n                    self.__out = stdout\n                    self.__err = stderr\n                except ValueError:\n                    pass  # Don't read from finished subprocesses.\n            else:\n                self.subprocess.stdin.close()\n                self.std_out.close()\n                self.std_err.close()\n                self.subprocess.wait()\n        else:\n            self.subprocess.sendeof()\n            try:\n                self.subprocess.wait()\n            finally:\n                if self.subprocess.proc.stdout:\n                    self.subprocess.proc.stdout.close()", "response": "Blocks until process is complete."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning the current command and passes its output to the next available entry point.", "response": "def pipe(self, command, timeout=None, cwd=None):\n        \"\"\"Runs the current command and passes its output to the next\n        given process.\n        \"\"\"\n        if not timeout:\n            timeout = self.timeout\n\n        if not self.was_run:\n            self.run(block=False, cwd=cwd)\n\n        data = self.out\n\n        if timeout:\n            c = Command(command, timeout)\n        else:\n            c = Command(command)\n\n        c.run(block=False, cwd=cwd)\n        if data:\n            c.send(data)\n        c.block()\n        return c"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_traceback(exc_info, source_hint=None):\n    exc_type, exc_value, tb = exc_info\n    if isinstance(exc_value, TemplateSyntaxError):\n        exc_info = translate_syntax_error(exc_value, source_hint)\n        initial_skip = 0\n    else:\n        initial_skip = 1\n    return translate_exception(exc_info, initial_skip)", "response": "Creates a processed traceback object from the exc_info."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrewriting a syntax error to please traceback systems.", "response": "def translate_syntax_error(error, source=None):\n    \"\"\"Rewrites a syntax error to please traceback systems.\"\"\"\n    error.source = source\n    error.translated = True\n    exc_info = (error.__class__, error, None)\n    filename = error.filename\n    if filename is None:\n        filename = '<unknown>'\n    return fake_exc_info(exc_info, filename, error.lineno)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef render_as_text(self, limit=None):\n        lines = traceback.format_exception(self.exc_type, self.exc_value,\n                                           self.frames[0], limit=limit)\n        return ''.join(lines).rstrip()", "response": "Return a string with the traceback."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a unicode string with the traceback as rendered HTML.", "response": "def render_as_html(self, full=False):\n        \"\"\"Return a unicode string with the traceback as rendered HTML.\"\"\"\n        from jinja2.debugrenderer import render_traceback\n        return u'%s\\n\\n<!--\\n%s\\n-->' % (\n            render_traceback(self, full=full),\n            self.render_as_text().decode('utf-8', 'replace')\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef resolve_command(self, ctx, args):\n        original_cmd_name = click.utils.make_str(args[0])\n\n        try:\n            return super(DYMMixin, self).resolve_command(ctx, args)\n        except click.exceptions.UsageError as error:\n            error_msg = str(error)\n            matches = difflib.get_close_matches(original_cmd_name,\n                                                self.list_commands(ctx), self.max_suggestions, self.cutoff)\n            if matches:\n                error_msg += '\\n\\nDid you mean one of these?\\n    %s' % '\\n    '.join(matches)  # pylint: disable=line-too-long\n\n            raise click.exceptions.UsageError(error_msg, error.ctx)", "response": "Override resolve_command method to add suggestions to the exception message."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nencoding into a cmd - executable string.", "response": "def cmdify(self, extra_args=None):\n        \"\"\"Encode into a cmd-executable string.\n\n        This re-implements CreateProcess's quoting logic to turn a list of\n        arguments into one single string for the shell to interpret.\n\n        * All double quotes are escaped with a backslash.\n        * Existing backslashes before a quote are doubled, so they are all\n          escaped properly.\n        * Backslashes elsewhere are left as-is; cmd will interpret them\n          literally.\n\n        The result is then quoted into a pair of double quotes to be grouped.\n\n        An argument is intentionally not quoted if it does not contain\n        whitespaces. This is done to be compatible with Windows built-in\n        commands that don't work well with quotes, e.g. everything with `echo`,\n        and DOS-style (forward slash) switches.\n\n        The intended use of this function is to pre-process an argument list\n        before passing it into ``subprocess.Popen(..., shell=True)``.\n\n        See also: https://docs.python.org/3/library/subprocess.html\n        \"\"\"\n        parts = list(self._parts)\n        if extra_args:\n            parts.extend(extra_args)\n        return \" \".join(\n            arg if not next(re.finditer(r'\\s', arg), None)\n            else '\"{0}\"'.format(re.sub(r'(\\\\*)\"', r'\\1\\1\\\\\"', arg))\n            for arg in parts\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef prepare_linked_requirement(\n        self,\n        req,  # type: InstallRequirement\n        session,  # type: PipSession\n        finder,  # type: PackageFinder\n        upgrade_allowed,  # type: bool\n        require_hashes  # type: bool\n    ):\n        # type: (...) -> DistAbstraction\n        \"\"\"Prepare a requirement that would be obtained from req.link\n        \"\"\"\n        # TODO: Breakup into smaller functions\n        if req.link and req.link.scheme == 'file':\n            path = url_to_path(req.link.url)\n            logger.info('Processing %s', display_path(path))\n        else:\n            logger.info('Collecting %s', req)\n\n        with indent_log():\n            # @@ if filesystem packages are not marked\n            # editable in a req, a non deterministic error\n            # occurs when the script attempts to unpack the\n            # build directory\n            req.ensure_has_source_dir(self.build_dir)\n            # If a checkout exists, it's unwise to keep going.  version\n            # inconsistencies are logged later, but do not fail the\n            # installation.\n            # FIXME: this won't upgrade when there's an existing\n            # package unpacked in `req.source_dir`\n            # package unpacked in `req.source_dir`\n            if os.path.exists(os.path.join(req.source_dir, 'setup.py')):\n                rmtree(req.source_dir)\n            req.populate_link(finder, upgrade_allowed, require_hashes)\n\n            # We can't hit this spot and have populate_link return None.\n            # req.satisfied_by is None here (because we're\n            # guarded) and upgrade has no impact except when satisfied_by\n            # is not None.\n            # Then inside find_requirement existing_applicable -> False\n            # If no new versions are found, DistributionNotFound is raised,\n            # otherwise a result is guaranteed.\n            assert req.link\n            link = req.link\n\n            # Now that we have the real link, we can tell what kind of\n            # requirements we have and raise some more informative errors\n            # than otherwise. (For example, we can raise VcsHashUnsupported\n            # for a VCS URL rather than HashMissing.)\n            if require_hashes:\n                # We could check these first 2 conditions inside\n                # unpack_url and save repetition of conditions, but then\n                # we would report less-useful error messages for\n                # unhashable requirements, complaining that there's no\n                # hash provided.\n                if is_vcs_url(link):\n                    raise VcsHashUnsupported()\n                elif is_file_url(link) and is_dir_url(link):\n                    raise DirectoryUrlHashUnsupported()\n                if not req.original_link and not req.is_pinned:\n                    # Unpinned packages are asking for trouble when a new\n                    # version is uploaded. This isn't a security check, but\n                    # it saves users a surprising hash mismatch in the\n                    # future.\n                    #\n                    # file:/// URLs aren't pinnable, so don't complain\n                    # about them not being pinned.\n                    raise HashUnpinned()\n\n            hashes = req.hashes(trust_internet=not require_hashes)\n            if require_hashes and not hashes:\n                # Known-good hashes are missing for this requirement, so\n                # shim it with a facade object that will provoke hash\n                # computation and then raise a HashMissing exception\n                # showing the user what the hash should be.\n                hashes = MissingHashes()\n\n            try:\n                download_dir = self.download_dir\n                # We always delete unpacked sdists after pip ran.\n                autodelete_unpacked = True\n                if req.link.is_wheel and self.wheel_download_dir:\n                    # when doing 'pip wheel` we download wheels to a\n                    # dedicated dir.\n                    download_dir = self.wheel_download_dir\n                if req.link.is_wheel:\n                    if download_dir:\n                        # When downloading, we only unpack wheels to get\n                        # metadata.\n                        autodelete_unpacked = True\n                    else:\n                        # When installing a wheel, we use the unpacked\n                        # wheel.\n                        autodelete_unpacked = False\n                unpack_url(\n                    req.link, req.source_dir,\n                    download_dir, autodelete_unpacked,\n                    session=session, hashes=hashes,\n                    progress_bar=self.progress_bar\n                )\n            except requests.HTTPError as exc:\n                logger.critical(\n                    'Could not install requirement %s because of error %s',\n                    req,\n                    exc,\n                )\n                raise InstallationError(\n                    'Could not install requirement %s because of HTTP '\n                    'error %s for URL %s' %\n                    (req, exc, req.link)\n                )\n            abstract_dist = make_abstract_dist(req)\n            with self.req_tracker.track(req):\n                abstract_dist.prep_for_dist(finder, self.build_isolation)\n            if self._download_should_save:\n                # Make a .zip of the source_dir we already created.\n                if req.link.scheme in vcs.all_schemes:\n                    req.archive(self.download_dir)\n        return abstract_dist", "response": "Prepare a linked version of a pkg - link."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef prepare_editable_requirement(\n        self,\n        req,  # type: InstallRequirement\n        require_hashes,  # type: bool\n        use_user_site,  # type: bool\n        finder  # type: PackageFinder\n    ):\n        # type: (...) -> DistAbstraction\n        \"\"\"Prepare an editable requirement\n        \"\"\"\n        assert req.editable, \"cannot prepare a non-editable req as editable\"\n\n        logger.info('Obtaining %s', req)\n\n        with indent_log():\n            if require_hashes:\n                raise InstallationError(\n                    'The editable requirement %s cannot be installed when '\n                    'requiring hashes, because there is no single file to '\n                    'hash.' % req\n                )\n            req.ensure_has_source_dir(self.src_dir)\n            req.update_editable(not self._download_should_save)\n\n            abstract_dist = make_abstract_dist(req)\n            with self.req_tracker.track(req):\n                abstract_dist.prep_for_dist(finder, self.build_isolation)\n\n            if self._download_should_save:\n                req.archive(self.download_dir)\n            req.check_if_exists(use_user_site)\n\n        return abstract_dist", "response": "Prepare an editable requirement."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprepare an already - installed requirement.", "response": "def prepare_installed_requirement(self, req, require_hashes, skip_reason):\n        # type: (InstallRequirement, bool, Optional[str]) -> DistAbstraction\n        \"\"\"Prepare an already-installed requirement\n        \"\"\"\n        assert req.satisfied_by, \"req should have been satisfied but isn't\"\n        assert skip_reason is not None, (\n            \"did not get skip reason skipped but req.satisfied_by \"\n            \"is set to %r\" % (req.satisfied_by,)\n        )\n        logger.info(\n            'Requirement %s: %s (%s)',\n            skip_reason, req, req.satisfied_by.version\n        )\n        with indent_log():\n            if require_hashes:\n                logger.debug(\n                    'Since it is already installed, we are trusting this '\n                    'package without checking its hash. To ensure a '\n                    'completely repeatable environment, install into an '\n                    'empty virtualenv.'\n                )\n            abstract_dist = Installed(req)\n\n        return abstract_dist"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef install_given_reqs(\n    to_install,  # type: List[InstallRequirement]\n    install_options,  # type: List[str]\n    global_options=(),  # type: Sequence[str]\n    *args, **kwargs\n):\n    # type: (...) -> List[InstallRequirement]\n    \"\"\"\n    Install everything in the given list.\n\n    (to be called after having downloaded and unpacked the packages)\n    \"\"\"\n\n    if to_install:\n        logger.info(\n            'Installing collected packages: %s',\n            ', '.join([req.name for req in to_install]),\n        )\n\n    with indent_log():\n        for requirement in to_install:\n            if requirement.conflicts_with:\n                logger.info(\n                    'Found existing installation: %s',\n                    requirement.conflicts_with,\n                )\n                with indent_log():\n                    uninstalled_pathset = requirement.uninstall(\n                        auto_confirm=True\n                    )\n            try:\n                requirement.install(\n                    install_options,\n                    global_options,\n                    *args,\n                    **kwargs\n                )\n            except Exception:\n                should_rollback = (\n                    requirement.conflicts_with and\n                    not requirement.install_succeeded\n                )\n                # if install did not succeed, rollback previous uninstall\n                if should_rollback:\n                    uninstalled_pathset.rollback()\n                raise\n            else:\n                should_commit = (\n                    requirement.conflicts_with and\n                    requirement.install_succeeded\n                )\n                if should_commit:\n                    uninstalled_pathset.commit()\n            requirement.remove_temporary_source()\n\n    return to_install", "response": "Installs all the given packages in the given list."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cprint(text, color=None, on_color=None, attrs=None, **kwargs):\n\n    print((colored(text, color, on_color, attrs)), **kwargs)", "response": "Print text to the console."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_provider(moduleOrReq):\n    if isinstance(moduleOrReq, Requirement):\n        return working_set.find(moduleOrReq) or require(str(moduleOrReq))[0]\n    try:\n        module = sys.modules[moduleOrReq]\n    except KeyError:\n        __import__(moduleOrReq)\n        module = sys.modules[moduleOrReq]\n    loader = getattr(module, '__loader__', None)\n    return _find_adapter(_provider_factories, loader)(module)", "response": "Return an IResourceProvider for the named module or requirement"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_build_platform():\n    from sysconfig import get_platform\n\n    plat = get_platform()\n    if sys.platform == \"darwin\" and not plat.startswith('macosx-'):\n        try:\n            version = _macosx_vers()\n            machine = os.uname()[4].replace(\" \", \"_\")\n            return \"macosx-%d.%d-%s\" % (\n                int(version[0]), int(version[1]),\n                _macosx_arch(machine),\n            )\n        except ValueError:\n            # if someone is running a non-Mac darwin system, this will fall\n            # through to the default implementation\n            pass\n    return plat", "response": "Return this platform s string for platform - specific distributions\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compatible_platforms(provided, required):\n    if provided is None or required is None or provided == required:\n        # easy case\n        return True\n\n    # Mac OS X special cases\n    reqMac = macosVersionString.match(required)\n    if reqMac:\n        provMac = macosVersionString.match(provided)\n\n        # is this a Mac package?\n        if not provMac:\n            # this is backwards compatibility for packages built before\n            # setuptools 0.6. All packages built after this point will\n            # use the new macosx designation.\n            provDarwin = darwinVersionString.match(provided)\n            if provDarwin:\n                dversion = int(provDarwin.group(1))\n                macosversion = \"%s.%s\" % (reqMac.group(1), reqMac.group(2))\n                if dversion == 7 and macosversion >= \"10.3\" or \\\n                        dversion == 8 and macosversion >= \"10.4\":\n                    return True\n            # egg isn't macosx or legacy darwin\n            return False\n\n        # are they the same major version and machine type?\n        if provMac.group(1) != reqMac.group(1) or \\\n                provMac.group(3) != reqMac.group(3):\n            return False\n\n        # is the required OS major update >= the provided one?\n        if int(provMac.group(2)) > int(reqMac.group(2)):\n            return False\n\n        return True\n\n    # XXX Linux and other platforms' special cases should go here\n    return False", "response": "Returns True if the provided platform run on the required platform."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlocate distribution dist_spec and run its script_name script", "response": "def run_script(dist_spec, script_name):\n    \"\"\"Locate distribution `dist_spec` and run its `script_name` script\"\"\"\n    ns = sys._getframe(1).f_globals\n    name = ns['__name__']\n    ns.clear()\n    ns['__name__'] = name\n    require(dist_spec)[0].run_script(script_name, ns)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_distribution(dist):\n    if isinstance(dist, six.string_types):\n        dist = Requirement.parse(dist)\n    if isinstance(dist, Requirement):\n        dist = get_provider(dist)\n    if not isinstance(dist, Distribution):\n        raise TypeError(\"Expected string, Requirement, or Distribution\", dist)\n    return dist", "response": "Return a current distribution object for a Requirement or string"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef safe_version(version):\n    try:\n        # normalize the version\n        return str(packaging.version.Version(version))\n    except packaging.version.InvalidVersion:\n        version = version.replace(' ', '.')\n        return re.sub('[^A-Za-z0-9.]+', '-', version)", "response": "Convert an arbitrary string to a standard version string"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef invalid_marker(text):\n    try:\n        evaluate_marker(text)\n    except SyntaxError as e:\n        e.filename = None\n        e.lineno = None\n        return e\n    return False", "response": "Validate text as a PEP 508 environment marker ; return an exception\n    if invalid or False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef evaluate_marker(text, extra=None):\n    try:\n        marker = packaging.markers.Marker(text)\n        return marker.evaluate()\n    except packaging.markers.InvalidMarker as e:\n        raise SyntaxError(e)", "response": "Evaluate a PEP 508 environment marker."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_distributions(path_item, only=False):\n    importer = get_importer(path_item)\n    finder = _find_adapter(_distribution_finders, importer)\n    return finder(importer, path_item, only)", "response": "Yields distributions accessible via path_item."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding eggs in zip files.", "response": "def find_eggs_in_zip(importer, path_item, only=False):\n    \"\"\"\n    Find eggs in zip files; possibly multiple nested eggs.\n    \"\"\"\n    if importer.archive.endswith('.whl'):\n        # wheels are not supported with this finder\n        # they don't have PKG-INFO metadata, and won't ever contain eggs\n        return\n    metadata = EggMetadata(importer)\n    if metadata.has_metadata('PKG-INFO'):\n        yield Distribution.from_filename(path_item, metadata=metadata)\n    if only:\n        # don't yield nested distros\n        return\n    for subitem in metadata.resource_listdir('/'):\n        if _is_egg_path(subitem):\n            subpath = os.path.join(path_item, subitem)\n            dists = find_eggs_in_zip(zipimport.zipimporter(subpath), subpath)\n            for dist in dists:\n                yield dist\n        elif subitem.lower().endswith('.dist-info'):\n            subpath = os.path.join(path_item, subitem)\n            submeta = EggMetadata(zipimport.zipimporter(subpath))\n            submeta.egg_info = subpath\n            yield Distribution.from_location(path_item, subitem, submeta)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _by_version_descending(names):\n    def _by_version(name):\n        \"\"\"\n        Parse each component of the filename\n        \"\"\"\n        name, ext = os.path.splitext(name)\n        parts = itertools.chain(name.split('-'), [ext])\n        return [packaging.version.parse(part) for part in parts]\n\n    return sorted(names, key=_by_version, reverse=True)", "response": "Given a list of filenames return them in descending order\n    by version number."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_on_path(importer, path_item, only=False):\n    path_item = _normalize_cached(path_item)\n\n    if _is_unpacked_egg(path_item):\n        yield Distribution.from_filename(\n            path_item, metadata=PathMetadata(\n                path_item, os.path.join(path_item, 'EGG-INFO')\n            )\n        )\n        return\n\n    entries = safe_listdir(path_item)\n\n    # for performance, before sorting by version,\n    # screen entries for only those that will yield\n    # distributions\n    filtered = (\n        entry\n        for entry in entries\n        if dist_factory(path_item, entry, only)\n    )\n\n    # scan for .egg and .egg-info in directory\n    path_item_entries = _by_version_descending(filtered)\n    for entry in path_item_entries:\n        fullpath = os.path.join(path_item, entry)\n        factory = dist_factory(path_item, entry, only)\n        for dist in factory(fullpath):\n            yield dist", "response": "Yields all distributions accessible on a sys. path directory."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a dist_factory for a path_item and entry", "response": "def dist_factory(path_item, entry, only):\n    \"\"\"\n    Return a dist_factory for a path_item and entry\n    \"\"\"\n    lower = entry.lower()\n    is_meta = any(map(lower.endswith, ('.egg-info', '.dist-info')))\n    return (\n        distributions_from_metadata\n        if is_meta else\n        find_distributions\n        if not only and _is_egg_path(entry) else\n        resolve_egg_link\n        if not only and lower.endswith('.egg-link') else\n        NoDists()\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef safe_listdir(path):\n    try:\n        return os.listdir(path)\n    except (PermissionError, NotADirectoryError):\n        pass\n    except OSError as e:\n        # Ignore the directory if does not exist, not a directory or\n        # permission denied\n        ignorable = (\n            e.errno in (errno.ENOTDIR, errno.EACCES, errno.ENOENT)\n            # Python 2 on Windows needs to be handled this way :(\n            or getattr(e, \"winerror\", None) == 267\n        )\n        if not ignorable:\n            raise\n    return ()", "response": "Attempt to list contents of path but suppress some exceptions."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nyielding non - empty lines from file at path", "response": "def non_empty_lines(path):\n    \"\"\"\n    Yield non-empty lines from file at path\n    \"\"\"\n    with open(path) as f:\n        for line in f:\n            line = line.strip()\n            if line:\n                yield line"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving a path to an. egg - link resolve distributions present in the referenced path.", "response": "def resolve_egg_link(path):\n    \"\"\"\n    Given a path to an .egg-link, resolve distributions\n    present in the referenced path.\n    \"\"\"\n    referenced_paths = non_empty_lines(path)\n    resolved_paths = (\n        os.path.join(os.path.dirname(path), ref)\n        for ref in referenced_paths\n    )\n    dist_groups = map(find_distributions, resolved_paths)\n    return next(dist_groups, ())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _handle_ns(packageName, path_item):\n\n    importer = get_importer(path_item)\n    if importer is None:\n        return None\n\n    # capture warnings due to #1111\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        loader = importer.find_module(packageName)\n\n    if loader is None:\n        return None\n    module = sys.modules.get(packageName)\n    if module is None:\n        module = sys.modules[packageName] = types.ModuleType(packageName)\n        module.__path__ = []\n        _set_parent_ns(packageName)\n    elif not hasattr(module, '__path__'):\n        raise TypeError(\"Not a package:\", packageName)\n    handler = _find_adapter(_namespace_handlers, importer)\n    subpath = handler(importer, path_item, packageName, module)\n    if subpath is not None:\n        path = module.__path__\n        path.append(subpath)\n        loader.load_module(packageName)\n        _rebuild_mod_path(path, packageName, module)\n    return subpath", "response": "Ensure that named package includes a subpath of path_item."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _rebuild_mod_path(orig_path, package_name, module):\n    sys_path = [_normalize_cached(p) for p in sys.path]\n\n    def safe_sys_path_index(entry):\n        \"\"\"\n        Workaround for #520 and #513.\n        \"\"\"\n        try:\n            return sys_path.index(entry)\n        except ValueError:\n            return float('inf')\n\n    def position_in_sys_path(path):\n        \"\"\"\n        Return the ordinal of the path based on its position in sys.path\n        \"\"\"\n        path_parts = path.split(os.sep)\n        module_parts = package_name.count('.') + 1\n        parts = path_parts[:-module_parts]\n        return safe_sys_path_index(_normalize_cached(os.sep.join(parts)))\n\n    new_path = sorted(orig_path, key=position_in_sys_path)\n    new_path = [_normalize_cached(p) for p in new_path]\n\n    if isinstance(module.__path__, list):\n        module.__path__[:] = new_path\n    else:\n        module.__path__ = new_path", "response": "Rebuild module. path based on sys. path."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeclares that package packageName is a namespace package.", "response": "def declare_namespace(packageName):\n    \"\"\"Declare that package 'packageName' is a namespace package\"\"\"\n\n    _imp.acquire_lock()\n    try:\n        if packageName in _namespace_packages:\n            return\n\n        path = sys.path\n        parent, _, _ = packageName.rpartition('.')\n\n        if parent:\n            declare_namespace(parent)\n            if parent not in _namespace_packages:\n                __import__(parent)\n            try:\n                path = sys.modules[parent].__path__\n            except AttributeError:\n                raise TypeError(\"Not a package:\", parent)\n\n        # Track what packages are namespaces, so when new path items are added,\n        # they can be updated\n        _namespace_packages.setdefault(parent or None, []).append(packageName)\n        _namespace_packages.setdefault(packageName, [])\n\n        for path_item in path:\n            # Ensure all the parent's path items are reflected in the child,\n            # if they apply\n            _handle_ns(packageName, path_item)\n\n    finally:\n        _imp.release_lock()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nensures that previously - declared namespace packages include path_item.", "response": "def fixup_namespace_packages(path_item, parent=None):\n    \"\"\"Ensure that previously-declared namespace packages include path_item\"\"\"\n    _imp.acquire_lock()\n    try:\n        for package in _namespace_packages.get(parent, ()):\n            subpath = _handle_ns(package, path_item)\n            if subpath:\n                fixup_namespace_packages(subpath, package)\n    finally:\n        _imp.release_lock()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing an ns - package subpath for a filesystem or zipfile importer", "response": "def file_ns_handler(importer, path_item, packageName, module):\n    \"\"\"Compute an ns-package subpath for a filesystem or zipfile importer\"\"\"\n\n    subpath = os.path.join(path_item, packageName.split('.')[-1])\n    normalized = _normalize_cached(subpath)\n    for item in module.__path__:\n        if _normalize_cached(item) == normalized:\n            break\n    else:\n        # Only return the path if it's not already there\n        return subpath"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef normalize_path(filename):\n    return os.path.normcase(os.path.realpath(os.path.normpath(_cygwin_patch(filename))))", "response": "Normalize a file name for comparison purposes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndetermine if given path appears to be an unpacked egg.", "response": "def _is_unpacked_egg(path):\n    \"\"\"\n    Determine if given path appears to be an unpacked egg.\n    \"\"\"\n    return (\n        _is_egg_path(path) and\n        os.path.isfile(os.path.join(path, 'EGG-INFO', 'PKG-INFO'))\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _version_from_file(lines):\n    def is_version_line(line):\n        return line.lower().startswith('version:')\n    version_lines = filter(is_version_line, lines)\n    line = next(iter(version_lines), '')\n    _, _, value = line.partition(':')\n    return safe_version(value.strip()) or None", "response": "Given an iterable of lines from a Metadata file return\n    the value of the Version field if present or None otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nyielding all requirements in a string.", "response": "def parse_requirements(strs):\n    \"\"\"Yield ``Requirement`` objects for each specification in `strs`\n\n    `strs` must be a string, or a (possibly-nested) iterable thereof.\n    \"\"\"\n    # create a steppable iterator, so we can handle \\-continuations\n    lines = iter(yield_lines(strs))\n\n    for line in lines:\n        # Drop comments -- a hash without a space may be in a URL.\n        if ' #' in line:\n            line = line[:line.find(' #')]\n        # If there is a line continuation, drop it, and append the next line.\n        if line.endswith('\\\\'):\n            line = line[:-2].strip()\n            try:\n                line += next(lines)\n            except StopIteration:\n                return\n        yield Requirement(line)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _find_adapter(registry, ob):\n    types = _always_object(inspect.getmro(getattr(ob, '__class__', type(ob))))\n    for t in types:\n        if t in registry:\n            return registry[t]", "response": "Return an adapter factory for ob from registry"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ensure_directory(path):\n    dirname = os.path.dirname(path)\n    py31compat.makedirs(dirname, exist_ok=True)", "response": "Ensure that the parent directory of path exists."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef split_sections(s):\n    section = None\n    content = []\n    for line in yield_lines(s):\n        if line.startswith(\"[\"):\n            if line.endswith(\"]\"):\n                if section or content:\n                    yield section, content\n                section = line[1:-1].strip()\n                content = []\n            else:\n                raise ValueError(\"Invalid section heading\", line)\n        else:\n            content.append(line)\n\n    # wrap up last segment\n    yield section, content", "response": "Split a string or iterable thereof into a list of sections and content."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef with_context(self, required_by):\n        if not required_by:\n            return self\n        args = self.args + (required_by,)\n        return ContextualVersionConflict(*args)", "response": "Returns a new version of self with the context required by."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _build_master(cls):\n        ws = cls()\n        try:\n            from __main__ import __requires__\n        except ImportError:\n            # The main program does not list any requirements\n            return ws\n\n        # ensure the requirements are met\n        try:\n            ws.require(__requires__)\n        except VersionConflict:\n            return cls._build_from_requirements(__requires__)\n\n        return ws", "response": "Build a master working set."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _build_from_requirements(cls, req_spec):\n        # try it without defaults already on sys.path\n        # by starting with an empty path\n        ws = cls([])\n        reqs = parse_requirements(req_spec)\n        dists = ws.resolve(reqs, Environment())\n        for dist in dists:\n            ws.add(dist)\n\n        # add any missing entries from sys.path\n        for entry in sys.path:\n            if entry not in ws.entries:\n                ws.add_entry(entry)\n\n        # then copy back to sys.path\n        sys.path[:] = ws.entries\n        return ws", "response": "Build a working set from a requirement spec."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a path item to the WorkingSet.", "response": "def add_entry(self, entry):\n        \"\"\"Add a path item to ``.entries``, finding any distributions on it\n\n        ``find_distributions(entry, True)`` is used to find distributions\n        corresponding to the path entry, and they are added.  `entry` is\n        always appended to ``.entries``, even if it is already present.\n        (This is because ``sys.path`` can contain the same value more than\n        once, and the ``.entries`` of the ``sys.path`` WorkingSet should always\n        equal ``sys.path``.)\n        \"\"\"\n        self.entry_keys.setdefault(entry, [])\n        self.entries.append(entry)\n        for dist in find_distributions(entry, True):\n            self.add(dist, entry, False)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nyield all entry points in group matching name.", "response": "def iter_entry_points(self, group, name=None):\n        \"\"\"Yield entry point objects from `group` matching `name`\n\n        If `name` is None, yields all entry points in `group` from all\n        distributions in the working set, otherwise only ones matching\n        both `group` and `name` are yielded (in distribution order).\n        \"\"\"\n        return (\n            entry\n            for dist in self\n            for entry in dist.get_entry_map(group).values()\n            if name is None or name == entry.name\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlocate distribution for requires and run script_name script", "response": "def run_script(self, requires, script_name):\n        \"\"\"Locate distribution for `requires` and run `script_name` script\"\"\"\n        ns = sys._getframe(1).f_globals\n        name = ns['__name__']\n        ns.clear()\n        ns['__name__'] = name\n        self.require(requires)[0].run_script(script_name, ns)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nensuring that the set of entries that are needed to be activated by the given set of requirements.", "response": "def require(self, *requirements):\n        \"\"\"Ensure that distributions matching `requirements` are activated\n\n        `requirements` must be a string or a (possibly-nested) sequence\n        thereof, specifying the distributions and versions required.  The\n        return value is a sequence of the distributions that needed to be\n        activated to fulfill the requirements; all relevant distributions are\n        included, even if they were already activated in this working set.\n        \"\"\"\n        needed = self.resolve(parse_requirements(requirements))\n\n        for dist in needed:\n            self.add(dist)\n\n        return needed"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsubscribes to all distributions in the cache.", "response": "def subscribe(self, callback, existing=True):\n        \"\"\"Invoke `callback` for all distributions\n\n        If `existing=True` (default),\n        call on all existing ones, as well.\n        \"\"\"\n        if callback in self.callbacks:\n            return\n        self.callbacks.append(callback)\n        if not existing:\n            return\n        for dist in self:\n            callback(dist)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef markers_pass(self, req, extras=None):\n        extra_evals = (\n            req.marker.evaluate({'extra': extra})\n            for extra in self.get(req, ()) + (extras or (None,))\n        )\n        return not req.marker or any(extra_evals)", "response": "Returns True if the req has a marker and fails otherwise return False."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nscan the system path for distributions usable in this environment and add them to the set of available modules.", "response": "def scan(self, search_path=None):\n        \"\"\"Scan `search_path` for distributions usable in this environment\n\n        Any distributions found are added to the environment.\n        `search_path` should be a sequence of ``sys.path`` items.  If not\n        supplied, ``sys.path`` is used.  Only distributions conforming to\n        the platform/python version defined at initialization are added.\n        \"\"\"\n        if search_path is None:\n            search_path = sys.path\n\n        for item in search_path:\n            for dist in find_distributions(item):\n                self.add(dist)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add(self, dist):\n        if self.can_add(dist) and dist.has_version():\n            dists = self._distmap.setdefault(dist.key, [])\n            if dist not in dists:\n                dists.append(dist)\n                dists.sort(key=operator.attrgetter('hashcmp'), reverse=True)", "response": "Add dist if we can add it and it has not already been added"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef best_match(\n            self, req, working_set, installer=None, replace_conflicting=False):\n        \"\"\"Find distribution best matching `req` and usable on `working_set`\n\n        This calls the ``find(req)`` method of the `working_set` to see if a\n        suitable distribution is already active.  (This may raise\n        ``VersionConflict`` if an unsuitable version of the project is already\n        active in the specified `working_set`.)  If a suitable distribution\n        isn't active, this method returns the newest distribution in the\n        environment that meets the ``Requirement`` in `req`.  If no suitable\n        distribution is found, and `installer` is supplied, then the result of\n        calling the environment's ``obtain(req, installer)`` method will be\n        returned.\n        \"\"\"\n        try:\n            dist = working_set.find(req)\n        except VersionConflict:\n            if not replace_conflicting:\n                raise\n            dist = None\n        if dist is not None:\n            return dist\n        for dist in self[req.key]:\n            if dist in req:\n                return dist\n        # try to download/install\n        return self.obtain(req, installer)", "response": "Find the best matching distribution that matches the given requirement and usable on the given working_set."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving an error message for problems extracting file(s", "response": "def extraction_error(self):\n        \"\"\"Give an error message for problems extracting file(s)\"\"\"\n\n        old_exc = sys.exc_info()[1]\n        cache_path = self.extraction_path or get_default_cache()\n\n        tmpl = textwrap.dedent(\"\"\"\n            Can't extract file(s) to egg cache\n\n            The following error occurred while trying to extract file(s)\n            to the Python egg cache:\n\n              {old_exc}\n\n            The Python egg cache directory is currently set to:\n\n              {cache_path}\n\n            Perhaps your account does not have write access to this directory?\n            You can change the cache directory by setting the PYTHON_EGG_CACHE\n            environment variable to point to an accessible directory.\n            \"\"\").lstrip()\n        err = ExtractionError(tmpl.format(**locals()))\n        err.manager = self\n        err.cache_path = cache_path\n        err.original_error = old_exc\n        raise err"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwarn user if the path is not allowed to extract a file.", "response": "def _warn_unsafe_extraction_path(path):\n        \"\"\"\n        If the default extraction path is overridden and set to an insecure\n        location, such as /tmp, it opens up an opportunity for an attacker to\n        replace an extracted file with an unauthorized payload. Warn the user\n        if a known insecure location is used.\n\n        See Distribute #375 for more details.\n        \"\"\"\n        if os.name == 'nt' and not path.startswith(os.environ['windir']):\n            # On Windows, permissions are generally restrictive by default\n            #  and temp directories are not writable by other users, so\n            #  bypass the warning.\n            return\n        mode = os.stat(path).st_mode\n        if mode & stat.S_IWOTH or mode & stat.S_IWGRP:\n            msg = (\n                \"%s is writable by group/others and vulnerable to attack \"\n                \"when \"\n                \"used with get_resource_filename. Consider a more secure \"\n                \"location (set with .set_extraction_path or the \"\n                \"PYTHON_EGG_CACHE environment variable).\" % path\n            )\n            warnings.warn(msg, UserWarning)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef postprocess(self, tempname, filename):\n\n        if os.name == 'posix':\n            # Make the resource executable\n            mode = ((os.stat(tempname).st_mode) | 0o555) & 0o7777\n            os.chmod(tempname, mode)", "response": "Perform any platform - specific postprocessing of tempname filename."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build(cls, path):\n        with zipfile.ZipFile(path) as zfile:\n            items = (\n                (\n                    name.replace('/', os.sep),\n                    zfile.getinfo(name),\n                )\n                for name in zfile.namelist()\n            )\n            return dict(items)", "response": "Build a dictionary similar to the zipimport directory\n        except instead of tuples store ZipInfo objects."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload a manifest at path or return a suitable manifest already loaded.", "response": "def load(self, path):\n        \"\"\"\n        Load a manifest at path or return a suitable manifest already loaded.\n        \"\"\"\n        path = os.path.normpath(path)\n        mtime = os.stat(path).st_mtime\n\n        if path not in self or self[path].mtime != mtime:\n            manifest = self.build(path)\n            self[path] = self.manifest_mod(manifest, mtime)\n\n        return self[path].manifest"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn True if the file_path is current for this zip_path", "response": "def _is_current(self, file_path, zip_path):\n        \"\"\"\n        Return True if the file_path is current for this zip_path\n        \"\"\"\n        timestamp, size = self._get_date_and_size(self.zipinfo[zip_path])\n        if not os.path.isfile(file_path):\n            return False\n        stat = os.stat(file_path)\n        if stat.st_size != size or stat.st_mtime != timestamp:\n            return False\n        # check that the contents match\n        zip_contents = self.loader.get_data(zip_path)\n        with open(file_path, 'rb') as f:\n            file_contents = f.read()\n        return zip_contents == file_contents"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload this EntryPoint into a new EntryPoint.", "response": "def load(self, require=True, *args, **kwargs):\n        \"\"\"\n        Require packages for this EntryPoint, then resolve it.\n        \"\"\"\n        if not require or args or kwargs:\n            warnings.warn(\n                \"Parameters to load are deprecated.  Call .resolve and \"\n                \".require separately.\",\n                PkgResourcesDeprecationWarning,\n                stacklevel=2,\n            )\n        if require:\n            self.require(*args, **kwargs)\n        return self.resolve()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nresolve the entry point from its module and attrs.", "response": "def resolve(self):\n        \"\"\"\n        Resolve the entry point from its module and attrs.\n        \"\"\"\n        module = __import__(self.module_name, fromlist=['__name__'], level=0)\n        try:\n            return functools.reduce(getattr, self.attrs, module)\n        except AttributeError as exc:\n            raise ImportError(str(exc))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse(cls, src, dist=None):\n        m = cls.pattern.match(src)\n        if not m:\n            msg = \"EntryPoint must be in 'name=module:attrs [extras]' format\"\n            raise ValueError(msg, src)\n        res = m.groupdict()\n        extras = cls._parse_extras(res['extras'])\n        attrs = res['attr'].split('.') if res['attr'] else ()\n        return cls(res['name'], res['module'], attrs, extras, dist)", "response": "Parse a single entry point from a string src."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_group(cls, group, lines, dist=None):\n        if not MODULE(group):\n            raise ValueError(\"Invalid group name\", group)\n        this = {}\n        for line in yield_lines(lines):\n            ep = cls.parse(line, dist)\n            if ep.name in this:\n                raise ValueError(\"Duplicate entry point\", group, ep.name)\n            this[ep.name] = ep\n        return this", "response": "Parse an entry point group"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses a map of entry point groups", "response": "def parse_map(cls, data, dist=None):\n        \"\"\"Parse a map of entry point groups\"\"\"\n        if isinstance(data, dict):\n            data = data.items()\n        else:\n            data = split_sections(data)\n        maps = {}\n        for group, lines in data:\n            if group is None:\n                if not lines:\n                    continue\n                raise ValueError(\"Entry points must be listed in groups\")\n            group = group.strip()\n            if group in maps:\n                raise ValueError(\"Duplicate group name\", group)\n            maps[group] = cls.parse_group(group, lines, dist)\n        return maps"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _filter_extras(dm):\n        for extra in list(filter(None, dm)):\n            new_extra = extra\n            reqs = dm.pop(extra)\n            new_extra, _, marker = extra.partition(':')\n            fails_marker = marker and (\n                invalid_marker(marker)\n                or not evaluate_marker(marker)\n            )\n            if fails_marker:\n                reqs = []\n            new_extra = safe_extra(new_extra) or None\n\n            dm.setdefault(new_extra, []).extend(reqs)\n        return dm", "response": "Given a mapping of extras to dependencies strip off any environment markers and filter out any dependencies\n            that are not matching the markers."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef requires(self, extras=()):\n        dm = self._dep_map\n        deps = []\n        deps.extend(dm.get(None, ()))\n        for ext in extras:\n            try:\n                deps.extend(dm[safe_extra(ext)])\n            except KeyError:\n                raise UnknownExtra(\n                    \"%s has no such extra feature %r\" % (self, ext)\n                )\n        return deps", "response": "Returns a list of Requirements needed for this distro if extras are used."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nensure distribution is importable on path ( default = sys. path", "response": "def activate(self, path=None, replace=False):\n        \"\"\"Ensure distribution is importable on `path` (default=sys.path)\"\"\"\n        if path is None:\n            path = sys.path\n        self.insert_on(path, replace=replace)\n        if path is sys.path:\n            fixup_namespace_packages(self.location)\n            for pkg in self._get_metadata('namespace_packages.txt'):\n                if pkg in sys.modules:\n                    declare_namespace(pkg)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef egg_name(self):\n        filename = \"%s-%s-py%s\" % (\n            to_filename(self.project_name), to_filename(self.version),\n            self.py_version or PY_MAJOR\n        )\n\n        if self.platform:\n            filename += '-' + self.platform\n        return filename", "response": "Return what this distribution s standard. egg filename should be"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef as_requirement(self):\n        if isinstance(self.parsed_version, packaging.version.Version):\n            spec = \"%s==%s\" % (self.project_name, self.parsed_version)\n        else:\n            spec = \"%s===%s\" % (self.project_name, self.parsed_version)\n\n        return Requirement.parse(spec)", "response": "Return a Requirement that matches this distribution exactly"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_entry_point(self, group, name):\n        ep = self.get_entry_info(group, name)\n        if ep is None:\n            raise ImportError(\"Entry point %r not found\" % ((group, name),))\n        return ep.load()", "response": "Return the name entry point of group. Raise ImportError if the entry point is not found."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the entry point map for group or the full entry map if group is None", "response": "def get_entry_map(self, group=None):\n        \"\"\"Return the entry point map for `group`, or the full entry map\"\"\"\n        try:\n            ep_map = self._ep_map\n        except AttributeError:\n            ep_map = self._ep_map = EntryPoint.parse_map(\n                self._get_metadata('entry_points.txt'), self\n            )\n        if group is not None:\n            return ep_map.get(group, {})\n        return ep_map"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef clone(self, **kw):\n        names = 'project_name version py_version platform location precedence'\n        for attr in names.split():\n            kw.setdefault(attr, getattr(self, attr, None))\n        kw.setdefault('metadata', self._provider)\n        return self.__class__(**kw)", "response": "Copy this distribution substituting in any changed keyword args"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreload the version number of the current object from the metadata file.", "response": "def _reload_version(self):\n        \"\"\"\n        Packages installed by distutils (e.g. numpy or scipy),\n        which uses an old safe_version, and so\n        their version numbers can get mangled when\n        converted to filenames (e.g., 1.11.0.dev0+2329eae to\n        1.11.0.dev0_2329eae). These distributions will not be\n        parsed properly\n        downstream by Distribution and safe_version, so\n        take an extra step and try to get the version number from\n        the metadata file itself instead of the filename.\n        \"\"\"\n        md_version = _version_from_file(self._get_metadata(self.PKG_INFO))\n        if md_version:\n            self._version = md_version\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexpanding agglutinated rules in a schema - definition.", "response": "def _expand_logical_shortcuts(cls, schema):\n        \"\"\" Expand agglutinated rules in a definition-schema.\n\n        :param schema: The schema-definition to expand.\n        :return: The expanded schema-definition.\n        \"\"\"\n        def is_of_rule(x):\n            return isinstance(x, _str_type) and \\\n                x.startswith(('allof_', 'anyof_', 'noneof_', 'oneof_'))\n\n        for field in schema:\n            for of_rule in (x for x in schema[field] if is_of_rule(x)):\n                operator, rule = of_rule.split('_')\n                schema[field].update({operator: []})\n                for value in schema[field][of_rule]:\n                    schema[field][operator].append({rule: value})\n                del schema[field][of_rule]\n        return schema"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _validate(self, schema):\n        if isinstance(schema, _str_type):\n            schema = self.validator.schema_registry.get(schema, schema)\n\n        if schema is None:\n            raise SchemaError(errors.SCHEMA_ERROR_MISSING)\n\n        schema = copy(schema)\n        for field in schema:\n            if isinstance(schema[field], _str_type):\n                schema[field] = rules_set_registry.get(schema[field],\n                                                       schema[field])\n\n        if not self.schema_validator(schema, normalize=False):\n            raise SchemaError(self.schema_validator.errors)", "response": "Validates a schema that defines rules against supported rules."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nvalidate a logical entry.", "response": "def _validate_logical(self, rule, field, value):\n        \"\"\" {'allowed': ('allof', 'anyof', 'noneof', 'oneof')} \"\"\"\n        if not isinstance(value, Sequence):\n            self._error(field, errors.BAD_TYPE)\n            return\n\n        validator = self._get_child_validator(\n            document_crumb=rule, allow_unknown=False,\n            schema=self.target_validator.validation_rules)\n\n        for constraints in value:\n            _hash = (mapping_hash({'turing': constraints}),\n                     mapping_hash(self.target_validator.types_mapping))\n            if _hash in self.target_validator._valid_schemas:\n                continue\n\n            validator(constraints, normalize=False)\n            if validator._errors:\n                self._error(validator._errors)\n            else:\n                self.target_validator._valid_schemas.add(_hash)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add(self, name, definition):\n        self._storage[name] = self._expand_definition(definition)", "response": "Add a definition to the registry. Existing definitions are not replaced silently."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef extend(self, definitions):\n        for name, definition in dict(definitions).items():\n            self.add(name, definition)", "response": "Add several definitions at once. Existing definitions are\n        replaced silently."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nexport the svn repository at the url to the destination location", "response": "def export(self, location):\n        \"\"\"Export the svn repository at the url to the destination location\"\"\"\n        url, rev_options = self.get_url_rev_options(self.url)\n\n        logger.info('Exporting svn repository %s to %s', url, location)\n        with indent_log():\n            if os.path.exists(location):\n                # Subversion doesn't like to check out over an existing\n                # directory --force fixes this, but was only added in svn 1.5\n                rmtree(location)\n            cmd_args = ['export'] + rev_options.to_args() + [url, location]\n            self.run_command(cmd_args, show_stdout=False)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\noverride to allow the auth information to be passed to svn via the URL.", "response": "def get_netloc_and_auth(self, netloc, scheme):\n        \"\"\"\n        This override allows the auth information to be passed to svn via the\n        --username and --password options instead of via the URL.\n        \"\"\"\n        if scheme == 'ssh':\n            # The --username and --password options can't be used for\n            # svn+ssh URLs, so keep the auth information in the URL.\n            return super(Subversion, self).get_netloc_and_auth(\n                netloc, scheme)\n\n        return split_auth_from_netloc(netloc)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cd(path):\n    if not path:\n        return\n    prev_cwd = Path.cwd().as_posix()\n    if isinstance(path, Path):\n        path = path.as_posix()\n    os.chdir(str(path))\n    try:\n        yield\n    finally:\n        os.chdir(prev_cwd)", "response": "Context manager to temporarily change working directories in a single file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef spinner(\n    spinner_name=None,\n    start_text=None,\n    handler_map=None,\n    nospin=False,\n    write_to_stdout=True,\n):\n    \"\"\"Get a spinner object or a dummy spinner to wrap a context.\n\n    :param str spinner_name: A spinner type e.g. \"dots\" or \"bouncingBar\" (default: {\"bouncingBar\"})\n    :param str start_text: Text to start off the spinner with (default: {None})\n    :param dict handler_map: Handler map for signals to be handled gracefully (default: {None})\n    :param bool nospin: If true, use the dummy spinner (default: {False})\n    :param bool write_to_stdout: Writes to stdout if true, otherwise writes to stderr (default: True)\n    :return: A spinner object which can be manipulated while alive\n    :rtype: :class:`~vistir.spin.VistirSpinner`\n\n    Raises:\n        RuntimeError -- Raised if the spinner extra is not installed\n    \"\"\"\n\n    from .spin import create_spinner\n\n    has_yaspin = None\n    try:\n        import yaspin\n    except ImportError:\n        has_yaspin = False\n        if not nospin:\n            raise RuntimeError(\n                \"Failed to import spinner! Reinstall vistir with command:\"\n                \" pip install --upgrade vistir[spinner]\"\n            )\n        else:\n            spinner_name = \"\"\n    else:\n        has_yaspin = True\n        spinner_name = \"\"\n    use_yaspin = (has_yaspin is False) or (nospin is True)\n    if has_yaspin is None or has_yaspin is True and not nospin:\n        use_yaspin = True\n    if start_text is None and use_yaspin is True:\n        start_text = \"Running...\"\n    with create_spinner(\n        spinner_name=spinner_name,\n        text=start_text,\n        handler_map=handler_map,\n        nospin=nospin,\n        use_yaspin=use_yaspin,\n        write_to_stdout=write_to_stdout,\n    ) as _spinner:\n        yield _spinner", "response": "Returns a spinner object or a dummy spinner object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef atomic_open_for_write(target, binary=False, newline=None, encoding=None):\n\n    mode = \"w+b\" if binary else \"w\"\n    f = NamedTemporaryFile(\n        dir=os.path.dirname(target),\n        prefix=\".__atomic-write\",\n        mode=mode,\n        encoding=encoding,\n        newline=newline,\n        delete=False,\n    )\n    # set permissions to 0644\n    os.chmod(f.name, stat.S_IWUSR | stat.S_IRUSR | stat.S_IRGRP | stat.S_IROTH)\n    try:\n        yield f\n    except BaseException:\n        f.close()\n        try:\n            os.remove(f.name)\n        except OSError:\n            pass\n        raise\n    else:\n        f.close()\n        try:\n            os.remove(target)  # This is needed on Windows.\n        except OSError:\n            pass\n        os.rename(f.name, target)", "response": "Atomically open target for writing."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nopening a file - like object for reading.", "response": "def open_file(link, session=None, stream=True):\n    \"\"\"\n    Open local or remote file for reading.\n\n    :type link: pip._internal.index.Link or str\n    :type session: requests.Session\n    :param bool stream: Try to stream if remote, default True\n    :raises ValueError: If link points to a local directory.\n    :return: a context manager to the opened file-like object\n    \"\"\"\n    if not isinstance(link, six.string_types):\n        try:\n            link = link.url_without_fragment\n        except AttributeError:\n            raise ValueError(\"Cannot parse url from unkown type: {0!r}\".format(link))\n\n    if not is_valid_url(link) and os.path.exists(link):\n        link = path_to_url(link)\n\n    if is_file_url(link):\n        # Local URL\n        local_path = url_to_path(link)\n        if os.path.isdir(local_path):\n            raise ValueError(\"Cannot open directory for read: {}\".format(link))\n        else:\n            with io.open(local_path, \"rb\") as local_file:\n                yield local_file\n    else:\n        # Remote URL\n        headers = {\"Accept-Encoding\": \"identity\"}\n        if not session:\n            from requests import Session\n\n            session = Session()\n        with session.get(link, headers=headers, stream=stream) as resp:\n            try:\n                raw = getattr(resp, \"raw\", None)\n                result = raw if raw else resp\n                yield result\n            finally:\n                if raw:\n                    conn = getattr(raw, \"_connection\")\n                    if conn is not None:\n                        conn.close()\n                result.close()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_nonblocking(self, size=1, timeout=None):\n\n        try:\n            s = os.read(self.child_fd, size)\n        except OSError as err:\n            if err.args[0] == errno.EIO:\n                # Linux-style EOF\n                self.flag_eof = True\n                raise EOF('End Of File (EOF). Exception style platform.')\n            raise\n        if s == b'':\n            # BSD-style EOF\n            self.flag_eof = True\n            raise EOF('End Of File (EOF). Empty string style platform.')\n\n        s = self._decoder.decode(s, final=False)\n        self._log(s, 'read')\n        return s", "response": "This reads data from the child file descriptor."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef expect_list(self, pattern_list, timeout=-1, searchwindowsize=-1,\n                    async_=False, **kw):\n        '''This takes a list of compiled regular expressions and returns the\n        index into the pattern_list that matched the child output. The list may\n        also contain EOF or TIMEOUT(which are not compiled regular\n        expressions). This method is similar to the expect() method except that\n        expect_list() does not recompile the pattern list on every call. This\n        may help if you are trying to optimize for speed, otherwise just use\n        the expect() method.  This is called by expect().\n\n\n        Like :meth:`expect`, passing ``async_=True`` will make this return an\n        asyncio coroutine.\n        '''\n        if timeout == -1:\n            timeout = self.timeout\n        if 'async' in kw:\n            async_ = kw.pop('async')\n        if kw:\n            raise TypeError(\"Unknown keyword arguments: {}\".format(kw))\n\n        exp = Expecter(self, searcher_re(pattern_list), searchwindowsize)\n        if async_:\n            from ._async import expect_async\n            return expect_async(exp, timeout)\n        else:\n            return exp.expect_loop(timeout)", "response": "This method takes a list of compiled regular expressions and returns the index into the pattern_list that matched the child output."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef expect_loop(self, searcher, timeout=-1, searchwindowsize=-1):\n        '''This is the common loop used inside expect. The 'searcher' should be\n        an instance of searcher_re or searcher_string, which describes how and\n        what to search for in the input.\n\n        See expect() for other arguments, return value and exceptions. '''\n\n        exp = Expecter(self, searcher, searchwindowsize)\n        return exp.expect_loop(timeout)", "response": "This is the common loop used inside expect."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef readline(self, size=-1):\n        '''This reads and returns one entire line. The newline at the end of\n        line is returned as part of the string, unless the file ends without a\n        newline. An empty string is returned if EOF is encountered immediately.\n        This looks for a newline as a CR/LF pair (\\\\r\\\\n) even on UNIX because\n        this is what the pseudotty device returns. So contrary to what you may\n        expect you will receive newlines as \\\\r\\\\n.\n\n        If the size argument is 0 then an empty string is returned. In all\n        other cases the size argument is ignored, which is not standard\n        behavior for a file-like object. '''\n\n        if size == 0:\n            return self.string_type()\n        # delimiter default is EOF\n        index = self.expect([self.crlf, self.delimiter])\n        if index == 0:\n            return self.before + self.crlf\n        else:\n            return self.before", "response": "This reads and returns one entire line from the file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the length hint of an object.", "response": "def _length_hint(obj):\n    \"\"\"Returns the length hint of an object.\"\"\"\n    try:\n        return len(obj)\n    except (AttributeError, TypeError):\n        try:\n            get_hint = type(obj).__length_hint__\n        except AttributeError:\n            return None\n        try:\n            hint = get_hint(obj)\n        except TypeError:\n            return None\n        if hint is NotImplemented or \\\n           not isinstance(hint, int_types) or \\\n           hint < 0:\n            return None\n        return hint"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npages through text by invoking a program on a temporary file.", "response": "def _tempfilepager(generator, cmd, color):\n    \"\"\"Page through text by invoking a program on a temporary file.\"\"\"\n    import tempfile\n    filename = tempfile.mktemp()\n    # TODO: This never terminates if the passed generator never terminates.\n    text = \"\".join(generator)\n    if not color:\n        text = strip_ansi(text)\n    encoding = get_best_encoding(sys.stdout)\n    with open_stream(filename, 'wb')[0] as f:\n        f.write(text.encode(encoding))\n    try:\n        os.system(cmd + ' \"' + filename + '\"')\n    finally:\n        os.unlink(filename)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generator(self):\n        if not self.entered:\n            raise RuntimeError('You need to use progress bars in a with block.')\n\n        if self.is_hidden:\n            for rv in self.iter:\n                yield rv\n        else:\n            for rv in self.iter:\n                self.current_item = rv\n                yield rv\n                self.update(1)\n            self.finish()\n            self.render_progress()", "response": "Returns a generator which yields the items added to the bar during the block."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bar(\n    it,\n    label=\"\",\n    width=32,\n    hide=None,\n    empty_char=BAR_EMPTY_CHAR,\n    filled_char=BAR_FILLED_CHAR,\n    expected_size=None,\n    every=1,\n):\n    \"\"\"Progress iterator. Wrap your iterables with it.\"\"\"\n    count = len(it) if expected_size is None else expected_size\n    with Bar(\n        label=label,\n        width=width,\n        hide=hide,\n        empty_char=BAR_EMPTY_CHAR,\n        filled_char=BAR_FILLED_CHAR,\n        expected_size=count,\n        every=every,\n    ) as bar:\n        for i, item in enumerate(it):\n            yield item\n\n            bar.show(i + 1)", "response": "Wrapper for Bar iterator. Wrap your iterables with it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprogressing iterator. Prints a dot for each item being iterated", "response": "def dots(it, label=\"\", hide=None, every=1):\n    \"\"\"Progress iterator. Prints a dot for each item being iterated\"\"\"\n    count = 0\n    if not hide:\n        STREAM.write(label)\n    for i, item in enumerate(it):\n        if not hide:\n            if i % every == 0:  # True every \"every\" updates\n                STREAM.write(DOTS_CHAR)\n                sys.stderr.flush()\n        count += 1\n        yield item\n\n    STREAM.write(\"\\n\")\n    STREAM.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse(version):\n    match = _REGEX.match(version)\n    if match is None:\n        raise ValueError('%s is not valid SemVer string' % version)\n\n    version_parts = match.groupdict()\n\n    version_parts['major'] = int(version_parts['major'])\n    version_parts['minor'] = int(version_parts['minor'])\n    version_parts['patch'] = int(version_parts['patch'])\n\n    return version_parts", "response": "Parse a SemVer string into major minor patch pre - release build parts."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_version_info(version):\n    parts = parse(version)\n    version_info = VersionInfo(\n            parts['major'], parts['minor'], parts['patch'],\n            parts['prerelease'], parts['build'])\n\n    return version_info", "response": "Parse a version string into a VersionInfo instance."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef compare(ver1, ver2):\n\n    v1, v2 = parse(ver1), parse(ver2)\n\n    return _compare_by_keys(v1, v2)", "response": "Compare two versions of the version string"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef match(version, match_expr):\n    prefix = match_expr[:2]\n    if prefix in ('>=', '<=', '==', '!='):\n        match_version = match_expr[2:]\n    elif prefix and prefix[0] in ('>', '<'):\n        prefix = prefix[0]\n        match_version = match_expr[1:]\n    else:\n        raise ValueError(\"match_expr parameter should be in format <op><ver>, \"\n                         \"where <op> is one of \"\n                         \"['<', '>', '==', '<=', '>=', '!=']. \"\n                         \"You provided: %r\" % match_expr)\n\n    possibilities_dict = {\n        '>': (1,),\n        '<': (-1,),\n        '==': (0,),\n        '!=': (-1, 1),\n        '>=': (0, 1),\n        '<=': (-1, 0)\n    }\n\n    possibilities = possibilities_dict[prefix]\n    cmp_res = compare(version, match_version)\n\n    return cmp_res in possibilities", "response": "Compare two versions through a comparison\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the greater version of two versions", "response": "def max_ver(ver1, ver2):\n    \"\"\"Returns the greater version of two versions\n\n    :param ver1: version string 1\n    :param ver2: version string 2\n    :return: the greater version of the two\n    :rtype: :class:`VersionInfo`\n\n    >>> import semver\n    >>> semver.max_ver(\"1.0.0\", \"2.0.0\")\n    '2.0.0'\n    \"\"\"\n    cmp_res = compare(ver1, ver2)\n    if cmp_res == 0 or cmp_res == 1:\n        return ver1\n    else:\n        return ver2"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef min_ver(ver1, ver2):\n    cmp_res = compare(ver1, ver2)\n    if cmp_res == 0 or cmp_res == -1:\n        return ver1\n    else:\n        return ver2", "response": "Returns the smaller version of two versions\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nformat a version according to the Semantic Versioning specification.", "response": "def format_version(major, minor, patch, prerelease=None, build=None):\n    \"\"\"Format a version according to the Semantic Versioning specification\n\n    :param str major: the required major part of a version\n    :param str minor: the required minor part of a version\n    :param str patch: the required patch part of a version\n    :param str prerelease: the optional prerelease part of a version\n    :param str build: the optional build part of a version\n    :return: the formatted string\n    :rtype: str\n\n    >>> import semver\n    >>> semver.format_version(3, 4, 5, 'pre.2', 'build.4')\n    '3.4.5-pre.2+build.4'\n    \"\"\"\n    version = \"%d.%d.%d\" % (major, minor, patch)\n    if prerelease is not None:\n        version = version + \"-%s\" % prerelease\n\n    if build is not None:\n        version = version + \"+%s\" % build\n\n    return version"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _make_eof_intr():\n    global _EOF, _INTR\n    if (_EOF is not None) and (_INTR is not None):\n        return\n\n    # inherit EOF and INTR definitions from controlling process.\n    try:\n        from termios import VEOF, VINTR\n        fd = None\n        for name in 'stdin', 'stdout':\n            stream = getattr(sys, '__%s__' % name, None)\n            if stream is None or not hasattr(stream, 'fileno'):\n                continue\n            try:\n                fd = stream.fileno()\n            except ValueError:\n                continue\n        if fd is None:\n            # no fd, raise ValueError to fallback on CEOF, CINTR\n            raise ValueError(\"No stream has a fileno\")\n        intr = ord(termios.tcgetattr(fd)[6][VINTR])\n        eof = ord(termios.tcgetattr(fd)[6][VEOF])\n    except (ImportError, OSError, IOError, ValueError, termios.error):\n        # unless the controlling process is also not a terminal,\n        # such as cron(1), or when stdin and stdout are both closed.\n        # Fall-back to using CEOF and CINTR. There\n        try:\n            from termios import CEOF, CINTR\n            (intr, eof) = (CINTR, CEOF)\n        except ImportError:\n            #                         ^C, ^D\n            (intr, eof) = (3, 4)\n    \n    _INTR = _byte(intr)\n    _EOF = _byte(eof)", "response": "Set constants _EOF and _INTR."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef spawn(\n            cls, argv, cwd=None, env=None, echo=True, preexec_fn=None,\n            dimensions=(24, 80)):\n        '''Start the given command in a child process in a pseudo terminal.\n\n        This does all the fork/exec type of stuff for a pty, and returns an\n        instance of PtyProcess.\n\n        If preexec_fn is supplied, it will be called with no arguments in the\n        child process before exec-ing the specified command.\n        It may, for instance, set signal handlers to SIG_DFL or SIG_IGN.\n\n        Dimensions of the psuedoterminal used for the subprocess can be\n        specified as a tuple (rows, cols), or the default (24, 80) will be used.\n        '''\n        # Note that it is difficult for this method to fail.\n        # You cannot detect if the child process cannot start.\n        # So the only way you can tell if the child process started\n        # or not is to try to read from the file descriptor. If you get\n        # EOF immediately then it means that the child is already dead.\n        # That may not necessarily be bad because you may have spawned a child\n        # that performs some task; creates no stdout output; and then dies.\n\n        if not isinstance(argv, (list, tuple)):\n            raise TypeError(\"Expected a list or tuple for argv, got %r\" % argv)\n\n        # Shallow copy of argv so we can modify it\n        argv = argv[:]\n        command = argv[0]\n\n        command_with_path = which(command)\n        if command_with_path is None:\n            raise FileNotFoundError('The command was not found or was not ' +\n                                    'executable: %s.' % command)\n        command = command_with_path\n        argv[0] = command\n\n        # [issue #119] To prevent the case where exec fails and the user is\n        # stuck interacting with a python child process instead of whatever\n        # was expected, we implement the solution from\n        # http://stackoverflow.com/a/3703179 to pass the exception to the\n        # parent process\n\n        # [issue #119] 1. Before forking, open a pipe in the parent process.\n        exec_err_pipe_read, exec_err_pipe_write = os.pipe()\n\n        if use_native_pty_fork:\n            pid, fd = pty.fork()\n        else:\n            # Use internal fork_pty, for Solaris\n            pid, fd = _fork_pty.fork_pty()\n\n        # Some platforms must call setwinsize() and setecho() from the\n        # child process, and others from the master process. We do both,\n        # allowing IOError for either.\n\n        if pid == CHILD:\n            # set window size\n            try:\n                _setwinsize(STDIN_FILENO, *dimensions)\n            except IOError as err:\n                if err.args[0] not in (errno.EINVAL, errno.ENOTTY):\n                    raise\n\n            # disable echo if spawn argument echo was unset\n            if not echo:\n                try:\n                    _setecho(STDIN_FILENO, False)\n                except (IOError, termios.error) as err:\n                    if err.args[0] not in (errno.EINVAL, errno.ENOTTY):\n                        raise\n\n            # [issue #119] 3. The child closes the reading end and sets the\n            # close-on-exec flag for the writing end.\n            os.close(exec_err_pipe_read)\n            fcntl.fcntl(exec_err_pipe_write, fcntl.F_SETFD, fcntl.FD_CLOEXEC)\n\n            # Do not allow child to inherit open file descriptors from parent,\n            # with the exception of the exec_err_pipe_write of the pipe\n            # Impose ceiling on max_fd: AIX bugfix for users with unlimited\n            # nofiles where resource.RLIMIT_NOFILE is 2^63-1 and os.closerange()\n            # occasionally raises out of range error\n            max_fd = min(1048576, resource.getrlimit(resource.RLIMIT_NOFILE)[0])\n            os.closerange(3, exec_err_pipe_write)\n            os.closerange(exec_err_pipe_write+1, max_fd)\n\n            if cwd is not None:\n                os.chdir(cwd)\n\n            if preexec_fn is not None:\n                try:\n                    preexec_fn()\n                except Exception as e:\n                    ename = type(e).__name__\n                    tosend = '{}:0:{}'.format(ename, str(e))\n                    if PY3:\n                        tosend = tosend.encode('utf-8')\n\n                    os.write(exec_err_pipe_write, tosend)\n                    os.close(exec_err_pipe_write)\n                    os._exit(1)\n\n            try:\n                if env is None:\n                    os.execv(command, argv)\n                else:\n                    os.execvpe(command, argv, env)\n            except OSError as err:\n                # [issue #119] 5. If exec fails, the child writes the error\n                # code back to the parent using the pipe, then exits.\n                tosend = 'OSError:{}:{}'.format(err.errno, str(err))\n                if PY3:\n                    tosend = tosend.encode('utf-8')\n                os.write(exec_err_pipe_write, tosend)\n                os.close(exec_err_pipe_write)\n                os._exit(os.EX_OSERR)\n\n        # Parent\n        inst = cls(pid, fd)\n        \n        # Set some informational attributes\n        inst.argv = argv\n        if env is not None:\n            inst.env = env\n        if cwd is not None:\n            inst.launch_dir = cwd\n\n        # [issue #119] 2. After forking, the parent closes the writing end\n        # of the pipe and reads from the reading end.\n        os.close(exec_err_pipe_write)\n        exec_err_data = os.read(exec_err_pipe_read, 4096)\n        os.close(exec_err_pipe_read)\n\n        # [issue #119] 6. The parent reads eof (a zero-length read) if the\n        # child successfully performed exec, since close-on-exec made\n        # successful exec close the writing end of the pipe. Or, if exec\n        # failed, the parent reads the error code and can proceed\n        # accordingly. Either way, the parent blocks until the child calls\n        # exec.\n        if len(exec_err_data) != 0:\n            try:\n                errclass, errno_s, errmsg = exec_err_data.split(b':', 2)\n                exctype = getattr(builtins, errclass.decode('ascii'), Exception)\n\n                exception = exctype(errmsg.decode('utf-8', 'replace'))\n                if exctype is OSError:\n                    exception.errno = int(errno_s)\n            except:\n                raise Exception('Subprocess failed, got bad error data: %r'\n                                    % exec_err_data)\n            else:\n                raise exception\n\n        try:\n            inst.setwinsize(*dimensions)\n        except IOError as err:\n            if err.args[0] not in (errno.EINVAL, errno.ENOTTY, errno.ENXIO):\n                raise\n\n        return inst", "response": "Start the given command in a pseudo terminal and return an instance of the appropriate class."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef close(self, force=True):\n        '''This closes the connection with the child application. Note that\n        calling close() more than once is valid. This emulates standard Python\n        behavior with files. Set force to True if you want to make sure that\n        the child is terminated (SIGKILL is sent if the child ignores SIGHUP\n        and SIGINT). '''\n        if not self.closed:\n            self.flush()\n            self.fileobj.close() # Closes the file descriptor\n            # Give kernel time to update process status.\n            time.sleep(self.delayafterclose)\n            if self.isalive():\n                if not self.terminate(force):\n                    raise PtyProcessError('Could not terminate the child.')\n            self.fd = -1\n            self.closed = True", "response": "This closes the connection with the child application."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getecho(self):\n        '''This returns the terminal echo mode. This returns True if echo is\n        on or False if echo is off. Child applications that are expecting you\n        to enter a password often set ECHO False. See waitnoecho().\n\n        Not supported on platforms where ``isatty()`` returns False.  '''\n\n        try:\n            attr = termios.tcgetattr(self.fd)\n        except termios.error as err:\n            errmsg = 'getecho() may not be called on this platform'\n            if err.args[0] == errno.EINVAL:\n                raise IOError(err.args[0], '%s: %s.' % (err.args[1], errmsg))\n            raise\n\n        self.echo = bool(attr[3] & termios.ECHO)\n        return self.echo", "response": "This returns the terminal echo mode. This returns True if echo is on False if echo is off."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read(self, size=1024):\n        try:\n            s = self.fileobj.read1(size)\n        except (OSError, IOError) as err:\n            if err.args[0] == errno.EIO:\n                # Linux-style EOF\n                self.flag_eof = True\n                raise EOFError('End Of File (EOF). Exception style platform.')\n            raise\n        if s == b'':\n            # BSD-style EOF (also appears to work on recent Solaris (OpenIndiana))\n            self.flag_eof = True\n            raise EOFError('End Of File (EOF). Empty string style platform.')\n\n        return s", "response": "Read and return at most size bytes from the pexpect terminal."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef readline(self):\n        try:\n            s = self.fileobj.readline()\n        except (OSError, IOError) as err:\n            if err.args[0] == errno.EIO:\n                # Linux-style EOF\n                self.flag_eof = True\n                raise EOFError('End Of File (EOF). Exception style platform.')\n            raise\n        if s == b'':\n            # BSD-style EOF (also appears to work on recent Solaris (OpenIndiana))\n            self.flag_eof = True\n            raise EOFError('End Of File (EOF). Empty string style platform.')\n\n        return s", "response": "Read one line from the pseudoterminal and return it as unicode. Raises EOFError if the pseudoterminal was closed."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write(self, s, flush=True):\n        return self._writeb(s, flush=flush)", "response": "Write a byte to the pseudoterminal."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef kill(self, sig):\n\n        # Same as os.kill, but the pid is given for you.\n        if self.isalive():\n            os.kill(self.pid, sig)", "response": "Send the given signal to the child application."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read(self, size=1024):\n        b = super(PtyProcessUnicode, self).read(size)\n        return self.decoder.decode(b, final=False)", "response": "Read at most size bytes from the pty return them as unicode."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef readline(self):\n        b = super(PtyProcessUnicode, self).readline()\n        return self.decoder.decode(b, final=False)", "response": "Read one line from the pseudoterminal and return it as unicode."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite the unicode string s to the pseudoterminal. Returns the number of bytes written.", "response": "def write(self, s):\n        \"\"\"Write the unicode string ``s`` to the pseudoterminal.\n\n        Returns the number of bytes written.\n        \"\"\"\n        b = s.encode(self.encoding)\n        return super(PtyProcessUnicode, self).write(b)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_requirement(cls, provider, requirement, parent):\n        candidates = provider.find_matches(requirement)\n        if not candidates:\n            raise NoVersionsAvailable(requirement, parent)\n        return cls(\n            candidates=candidates,\n            information=[RequirementInformation(requirement, parent)],\n        )", "response": "Build an instance from a requirement."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef merged_with(self, provider, requirement, parent):\n        infos = list(self.information)\n        infos.append(RequirementInformation(requirement, parent))\n        candidates = [\n            c for c in self.candidates\n            if provider.is_satisfied_by(requirement, c)\n        ]\n        if not candidates:\n            raise RequirementsConflicted(self)\n        return type(self)(candidates, infos)", "response": "Build a new instance from this and a new requirement."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npushing a new state into the history.", "response": "def _push_new_state(self):\n        \"\"\"Push a new state into history.\n\n        This new state will be used to hold resolution results of the next\n        coming round.\n        \"\"\"\n        try:\n            base = self._states[-1]\n        except IndexError:\n            graph = DirectedGraph()\n            graph.add(None)     # Sentinel as root dependencies' parent.\n            state = State(mapping={}, graph=graph)\n        else:\n            state = State(\n                mapping=base.mapping.copy(),\n                graph=base.graph.copy(),\n            )\n        self._states.append(state)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef resolve(self, requirements, max_rounds=20):\n        resolution = Resolution(self.provider, self.reporter)\n        resolution.resolve(requirements, max_rounds=max_rounds)\n        return resolution.state", "response": "Resolve a set of requirements and return a final resolution result."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving the extra operands from the list.", "response": "def _strip_extra(elements):\n    \"\"\"Remove the \"extra == ...\" operands from the list.\n\n    This is not a comprehensive implementation, but relies on an important\n    characteristic of metadata generation: The \"extra == ...\" operand is always\n    associated with an \"and\" operator. This means that we can simply remove the\n    operand and the \"and\" operator associated with it.\n    \"\"\"\n    extra_indexes = []\n    for i, element in enumerate(elements):\n        if isinstance(element, list):\n            cancelled = _strip_extra(element)\n            if cancelled:\n                extra_indexes.append(i)\n        elif isinstance(element, tuple) and element[0].value == \"extra\":\n            extra_indexes.append(i)\n    for i in reversed(extra_indexes):\n        del elements[i]\n        if i > 0 and elements[i - 1] == \"and\":\n            # Remove the \"and\" before it.\n            del elements[i - 1]\n        elif elements:\n            # This shouldn't ever happen, but is included for completeness.\n            # If there is not an \"and\" before this element, try to remove the\n            # operator after it.\n            del elements[0]\n    return (not elements)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbuilds a new marker without the extra ==... part.", "response": "def get_without_extra(marker):\n    \"\"\"Build a new marker without the `extra == ...` part.\n\n    The implementation relies very deep into packaging's internals, but I don't\n    have a better way now (except implementing the whole thing myself).\n\n    This could return `None` if the `extra == ...` part is the only one in the\n    input marker.\n    \"\"\"\n    # TODO: Why is this very deep in the internals? Why is a better solution\n    # implementing it yourself when someone is already maintaining a codebase\n    # for this? It's literally a grammar implementation that is required to\n    # meet the demands of a pep... -d\n    if not marker:\n        return None\n    marker = Marker(str(marker))\n    elements = marker._markers\n    _strip_extra(elements)\n    if elements:\n        return marker\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncollects extra ==... operands from a marker. Returns a list of str. Each str is a speficied extra in this marker.", "response": "def get_contained_extras(marker):\n    \"\"\"Collect \"extra == ...\" operands from a marker.\n\n    Returns a list of str. Each str is a speficied extra in this marker.\n    \"\"\"\n    if not marker:\n        return set()\n    marker = Marker(str(marker))\n    extras = set()\n    _markers_collect_extras(marker._markers, extras)\n    return extras"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef contains_extra(marker):\n    if not marker:\n        return False\n    marker = Marker(str(marker))\n    return _markers_contains_extra(marker._markers)", "response": "Check whehter a marker contains an extra ==... operand."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_errors(self):\n        result = []\n        while not self.errors.empty():  # pragma: no cover\n            try:\n                e = self.errors.get(False)\n                result.append(e)\n            except self.errors.Empty:\n                continue\n            self.errors.task_done()\n        return result", "response": "Return any errors which have occurred."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_project(self, name):\n        if self._cache is None:  # pragma: no cover\n            result = self._get_project(name)\n        elif name in self._cache:\n            result = self._cache[name]\n        else:\n            self.clear_errors()\n            result = self._get_project(name)\n            self._cache[name] = result\n        return result", "response": "Get a dictionary mapping available versions to Distribution\n        instances."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef score_url(self, url):\n        t = urlparse(url)\n        basename = posixpath.basename(t.path)\n        compatible = True\n        is_wheel = basename.endswith('.whl')\n        is_downloadable = basename.endswith(self.downloadable_extensions)\n        if is_wheel:\n            compatible = is_compatible(Wheel(basename), self.wheel_tags)\n        return (t.scheme == 'https', 'pypi.python.org' in t.netloc,\n                is_downloadable, is_wheel, compatible, basename)", "response": "Given an url give a score which can be used to choose preferred URLs\n        for a given project release."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchooses one of two URLs where both are candidates for distribution archives for the same version of a distribution and the archive name.", "response": "def prefer_url(self, url1, url2):\n        \"\"\"\n        Choose one of two URLs where both are candidates for distribution\n        archives for the same version of a distribution (for example,\n        .tar.gz vs. zip).\n\n        The current implementation favours https:// URLs over http://, archives\n        from PyPI over those from other locations, wheel compatibility (if a\n        wheel) and then the archive name.\n        \"\"\"\n        result = url2\n        if url1:\n            s1 = self.score_url(url1)\n            s2 = self.score_url(url2)\n            if s1 > s2:\n                result = url1\n            if result != url2:\n                logger.debug('Not replacing %r with %r', url1, url2)\n            else:\n                logger.debug('Replacing %r with %r', url1, url2)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a digest from a dictionary by looking at keys of the form algo_digest and algo_digest.", "response": "def _get_digest(self, info):\n        \"\"\"\n        Get a digest from a dictionary by looking at keys of the form\n        'algo_digest'.\n\n        Returns a 2-tuple (algo, digest) if found, else None. Currently\n        looks only for SHA256, then MD5.\n        \"\"\"\n        result = None\n        for algo in ('sha256', 'md5'):\n            key = '%s_digest' % algo\n            if key in info:\n                result = (algo, info[key])\n                break\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef links(self):\n        def clean(url):\n            \"Tidy up an URL.\"\n            scheme, netloc, path, params, query, frag = urlparse(url)\n            return urlunparse((scheme, netloc, quote(path),\n                               params, query, frag))\n\n        result = set()\n        for match in self._href.finditer(self.data):\n            d = match.groupdict('')\n            rel = (d['rel1'] or d['rel2'] or d['rel3'] or\n                   d['rel4'] or d['rel5'] or d['rel6'])\n            url = d['url1'] or d['url2'] or d['url3']\n            url = urljoin(self.base_url, url)\n            url = unescape(url)\n            url = self._clean_re.sub(lambda m: '%%%2x' % ord(m.group(0)), url)\n            result.add((url, rel))\n        # We sort the result, hoping to bring the most recent versions\n        # to the front\n        result = sorted(result, key=lambda t: t[0], reverse=True)\n        return result", "response": "Return the URLs of all the links on a page together with information\nAttributeNames."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates threads to fetch the resources from the server.", "response": "def _prepare_threads(self):\n        \"\"\"\n        Threads are created only when get_project is called, and terminate\n        before it returns. They are there primarily to parallelise I/O (i.e.\n        fetching web pages).\n        \"\"\"\n        self._threads = []\n        for i in range(self.num_workers):\n            t = threading.Thread(target=self._fetch)\n            t.setDaemon(True)\n            t.start()\n            self._threads.append(t)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _wait_threads(self):\n        # Note that you need two loops, since you can't say which\n        # thread will get each sentinel\n        for t in self._threads:\n            self._to_fetch.put(None)    # sentinel\n        for t in self._threads:\n            t.join()\n        self._threads = []", "response": "Wait for all the threads to terminate and then join them."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprocess a download for a specific version of a specific project.", "response": "def _process_download(self, url):\n        \"\"\"\n        See if an URL is a suitable download for a project.\n\n        If it is, register information in the result dictionary (for\n        _get_project) about the specific version it's for.\n\n        Note that the return value isn't actually used other than as a boolean\n        value.\n        \"\"\"\n        if self.platform_check and self._is_platform_dependent(url):\n            info = None\n        else:\n            info = self.convert_url_to_download_info(url, self.project_name)\n        logger.debug('process_download: %s -> %s', url, info)\n        if info:\n            with self._lock:    # needed because self.result is shared\n                self._update_version_data(self.result, info)\n        return info"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _should_queue(self, link, referrer, rel):\n        scheme, netloc, path, _, _, _ = urlparse(link)\n        if path.endswith(self.source_extensions + self.binary_extensions +\n                         self.excluded_extensions):\n            result = False\n        elif self.skip_externals and not link.startswith(self.base_url):\n            result = False\n        elif not referrer.startswith(self.base_url):\n            result = False\n        elif rel not in ('homepage', 'download'):\n            result = False\n        elif scheme not in ('http', 'https', 'ftp'):\n            result = False\n        elif self._is_platform_dependent(link):\n            result = False\n        else:\n            host = netloc.split(':', 1)[0]\n            if host.lower() == 'localhost':\n                result = False\n            else:\n                result = True\n        logger.debug('should_queue: %s (%s) from %s -> %s', link, rel,\n                     referrer, result)\n        return result", "response": "Determine whether a link URL from a referring page and with a particular rel attribute should be queued for scraping."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _fetch(self):\n        while True:\n            url = self._to_fetch.get()\n            try:\n                if url:\n                    page = self.get_page(url)\n                    if page is None:    # e.g. after an error\n                        continue\n                    for link, rel in page.links:\n                        if link not in self._seen:\n                            try:\n                                self._seen.add(link)\n                                if (not self._process_download(link) and\n                                    self._should_queue(link, url, rel)):\n                                    logger.debug('Queueing %s from %s', link, url)\n                                    self._to_fetch.put(link)\n                            except MetadataInvalidError:  # e.g. invalid versions\n                                pass\n            except Exception as e:  # pragma: no cover\n                self.errors.put(text_type(e))\n            finally:\n                # always do this, to avoid hangs :-)\n                self._to_fetch.task_done()\n            if not url:\n                #logger.debug('Sentinel seen, quitting.')\n                break", "response": "Fetch the URL to fetch from the work queue and process the links for download and further scraping."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets all the distribution names known to this locator.", "response": "def get_distribution_names(self):\n        \"\"\"\n        Return all the distribution names known to this locator.\n        \"\"\"\n        result = set()\n        page = self.get_page(self.base_url)\n        if not page:\n            raise DistlibException('Unable to get %s' % self.base_url)\n        for match in self._distname_re.finditer(page.data):\n            result.add(match.group(1))\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_distribution_names(self):\n        result = set()\n        for root, dirs, files in os.walk(self.base_dir):\n            for fn in files:\n                if self.should_include(fn, root):\n                    fn = os.path.join(root, fn)\n                    url = urlunparse(('file', '',\n                                      pathname2url(os.path.abspath(fn)),\n                                      '', '', ''))\n                    info = self.convert_url_to_download_info(url, None)\n                    if info:\n                        result.add(info['name'])\n            if not self.recursive:\n                break\n        return result", "response": "Returns all the distribution names known to this locator."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns all the distribution names known to this instance.", "response": "def get_distribution_names(self):\n        \"\"\"\n        Return all the distribution names known to this locator.\n        \"\"\"\n        result = set()\n        for locator in self.locators:\n            try:\n                result |= locator.get_distribution_names()\n            except NotImplementedError:\n                pass\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a distribution to the finder. This will update internal information about who provides what.", "response": "def add_distribution(self, dist):\n        \"\"\"\n        Add a distribution to the finder. This will update internal information\n        about who provides what.\n        :param dist: The distribution to add.\n        \"\"\"\n        logger.debug('adding distribution %s', dist)\n        name = dist.key\n        self.dists_by_name[name] = dist\n        self.dists[(name, dist.version)] = dist\n        for p in dist.provides:\n            name, version = parse_name_and_version(p)\n            logger.debug('Add to provided: %s, %s, %s', name, version, dist)\n            self.provided.setdefault(name, set()).add((version, dist))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves a distribution from the finder.", "response": "def remove_distribution(self, dist):\n        \"\"\"\n        Remove a distribution from the finder. This will update internal\n        information about who provides what.\n        :param dist: The distribution to remove.\n        \"\"\"\n        logger.debug('removing distribution %s', dist)\n        name = dist.key\n        del self.dists_by_name[name]\n        del self.dists[(name, dist.version)]\n        for p in dist.provides:\n            name, version = parse_name_and_version(p)\n            logger.debug('Remove from provided: %s, %s, %s', name, version, dist)\n            s = self.provided[name]\n            s.remove((version, dist))\n            if not s:\n                del self.provided[name]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_matcher(self, reqt):\n        try:\n            matcher = self.scheme.matcher(reqt)\n        except UnsupportedVersionError:  # pragma: no cover\n            # XXX compat-mode if cannot read the version\n            name = reqt.split()[0]\n            matcher = self.scheme.matcher(name)\n        return matcher", "response": "Get a version matcher for a requirement."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds the distributions which can fulfill a requirement.", "response": "def find_providers(self, reqt):\n        \"\"\"\n        Find the distributions which can fulfill a requirement.\n\n        :param reqt: The requirement.\n         :type reqt: str\n        :return: A set of distribution which can fulfill the requirement.\n        \"\"\"\n        matcher = self.get_matcher(reqt)\n        name = matcher.key   # case-insensitive\n        result = set()\n        provided = self.provided\n        if name in provided:\n            for version, provider in provided[name]:\n                try:\n                    match = matcher.match(version)\n                except UnsupportedVersionError:\n                    match = False\n\n                if match:\n                    result.add(provider)\n                    break\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntry to replace one provider with another.", "response": "def try_to_replace(self, provider, other, problems):\n        \"\"\"\n        Attempt to replace one provider with another. This is typically used\n        when resolving dependencies from multiple sources, e.g. A requires\n        (B >= 1.0) while C requires (B >= 1.1).\n\n        For successful replacement, ``provider`` must meet all the requirements\n        which ``other`` fulfills.\n\n        :param provider: The provider we are trying to replace with.\n        :param other: The provider we're trying to replace.\n        :param problems: If False is returned, this will contain what\n                         problems prevented replacement. This is currently\n                         a tuple of the literal string 'cantreplace',\n                         ``provider``, ``other``  and the set of requirements\n                         that ``provider`` couldn't fulfill.\n        :return: True if we can replace ``other`` with ``provider``, else\n                 False.\n        \"\"\"\n        rlist = self.reqts[other]\n        unmatched = set()\n        for s in rlist:\n            matcher = self.get_matcher(s)\n            if not matcher.match(provider.version):\n                unmatched.add(s)\n        if unmatched:\n            # can't replace other with provider\n            problems.add(('cantreplace', provider, other,\n                          frozenset(unmatched)))\n            result = False\n        else:\n            # can replace other with provider\n            self.remove_distribution(other)\n            del self.reqts[other]\n            for s in rlist:\n                self.reqts.setdefault(provider, set()).add(s)\n            self.add_distribution(provider)\n            result = True\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find(self, requirement, meta_extras=None, prereleases=False):\n\n        self.provided = {}\n        self.dists = {}\n        self.dists_by_name = {}\n        self.reqts = {}\n\n        meta_extras = set(meta_extras or [])\n        if ':*:' in meta_extras:\n            meta_extras.remove(':*:')\n            # :meta: and :run: are implicitly included\n            meta_extras |= set([':test:', ':build:', ':dev:'])\n\n        if isinstance(requirement, Distribution):\n            dist = odist = requirement\n            logger.debug('passed %s as requirement', odist)\n        else:\n            dist = odist = self.locator.locate(requirement,\n                                               prereleases=prereleases)\n            if dist is None:\n                raise DistlibException('Unable to locate %r' % requirement)\n            logger.debug('located %s', odist)\n        dist.requested = True\n        problems = set()\n        todo = set([dist])\n        install_dists = set([odist])\n        while todo:\n            dist = todo.pop()\n            name = dist.key     # case-insensitive\n            if name not in self.dists_by_name:\n                self.add_distribution(dist)\n            else:\n                #import pdb; pdb.set_trace()\n                other = self.dists_by_name[name]\n                if other != dist:\n                    self.try_to_replace(dist, other, problems)\n\n            ireqts = dist.run_requires | dist.meta_requires\n            sreqts = dist.build_requires\n            ereqts = set()\n            if meta_extras and dist in install_dists:\n                for key in ('test', 'build', 'dev'):\n                    e = ':%s:' % key\n                    if e in meta_extras:\n                        ereqts |= getattr(dist, '%s_requires' % key)\n            all_reqts = ireqts | sreqts | ereqts\n            for r in all_reqts:\n                providers = self.find_providers(r)\n                if not providers:\n                    logger.debug('No providers found for %r', r)\n                    provider = self.locator.locate(r, prereleases=prereleases)\n                    # If no provider is found and we didn't consider\n                    # prereleases, consider them now.\n                    if provider is None and not prereleases:\n                        provider = self.locator.locate(r, prereleases=True)\n                    if provider is None:\n                        logger.debug('Cannot satisfy %r', r)\n                        problems.add(('unsatisfied', r))\n                    else:\n                        n, v = provider.key, provider.version\n                        if (n, v) not in self.dists:\n                            todo.add(provider)\n                        providers.add(provider)\n                        if r in ireqts and dist in install_dists:\n                            install_dists.add(provider)\n                            logger.debug('Adding %s to install_dists',\n                                         provider.name_and_version)\n                for p in providers:\n                    name = p.key\n                    if name not in self.dists_by_name:\n                        self.reqts.setdefault(p, set()).add(r)\n                    else:\n                        other = self.dists_by_name[name]\n                        if other != p:\n                            # see if other can be replaced by p\n                            self.try_to_replace(p, other, problems)\n\n        dists = set(self.dists.values())\n        for dist in dists:\n            dist.build_time_dependency = dist not in install_dists\n            if dist.build_time_dependency:\n                logger.debug('%s is a build-time dependency only.',\n                             dist.name_and_version)\n        logger.debug('find done for %s', odist)\n        return dists, problems", "response": "Find a distribution and all distributions that depend on a given requirement."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning in a thread to move output from a pipe to a queue.", "response": "def _read_incoming(self):\n        \"\"\"Run in a thread to move output from a pipe to a queue.\"\"\"\n        fileno = self.proc.stdout.fileno()\n        while 1:\n            buf = b''\n            try:\n                buf = os.read(fileno, 1024)\n            except OSError as e:\n                self._log(e, 'read')\n\n            if not buf:\n                # This indicates we have reached EOF\n                self._read_queue.put(None)\n                return\n\n            self._read_queue.put(buf)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef send(self, s):\n        '''Send data to the subprocess' stdin.\n\n        Returns the number of bytes written.\n        '''\n        s = self._coerce_send_string(s)\n        self._log(s, 'send')\n\n        b = self._encoder.encode(s, final=False)\n        if PY3:\n            return self.proc.stdin.write(b)\n        else:\n            # On Python 2, .write() returns None, so we return the length of\n            # bytes written ourselves. This assumes they all got written.\n            self.proc.stdin.write(b)\n            return len(b)", "response": "Send data to the subprocess stdin. Returns the number of bytes written."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sendline(self, s=''):\n        '''Wraps send(), sending string ``s`` to child process, with os.linesep\n        automatically appended. Returns number of bytes written. '''\n\n        n = self.send(s)\n        return n + self.send(self.linesep)", "response": "Wraps send to child process with os. linesep automatically appended. Returns number of bytes written."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef wait(self):\n        '''Wait for the subprocess to finish.\n\n        Returns the exit code.\n        '''\n        status = self.proc.wait()\n        if status >= 0:\n            self.exitstatus = status\n            self.signalstatus = None\n        else:\n            self.exitstatus = None\n            self.signalstatus = -status\n        self.terminated = True\n        return status", "response": "Wait for the subprocess to finish. Returns the exit code."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsend a Unix signal to the subprocess.", "response": "def kill(self, sig):\n        '''Sends a Unix signal to the subprocess.\n\n        Use constants from the :mod:`signal` module to specify which signal.\n        '''\n        if sys.platform == 'win32':\n            if sig in [signal.SIGINT, signal.CTRL_C_EVENT]:\n                sig = signal.CTRL_C_EVENT\n            elif sig in [signal.SIGBREAK, signal.CTRL_BREAK_EVENT]:\n                sig = signal.CTRL_BREAK_EVENT\n            else:\n                sig = signal.SIGTERM\n\n        os.kill(self.proc.pid, sig)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef mkdir_p(*args, **kwargs):\n    try:\n        return os.mkdir(*args, **kwargs)\n    except OSError as exc:\n        if exc.errno != errno.EEXIST:\n            raise", "response": "Like mkdir but does not raise an exception if the the\nCTYPE directory already exists."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nattaches a regular expression pattern matcher to a custom type converter function.", "response": "def with_pattern(pattern, regex_group_count=None):\n    \"\"\"Attach a regular expression pattern matcher to a custom type converter\n    function.\n\n    This annotates the type converter with the :attr:`pattern` attribute.\n\n    EXAMPLE:\n        >>> import parse\n        >>> @parse.with_pattern(r\"\\d+\")\n        ... def parse_number(text):\n        ...     return int(text)\n\n    is equivalent to:\n\n        >>> def parse_number(text):\n        ...     return int(text)\n        >>> parse_number.pattern = r\"\\d+\"\n\n    :param pattern: regular expression pattern (as text)\n    :param regex_group_count: Indicates how many regex-groups are in pattern.\n    :return: wrapped function\n    \"\"\"\n    def decorator(func):\n        func.pattern = pattern\n        func.regex_group_count = regex_group_count\n        return func\n    return decorator"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a string to an integer.", "response": "def int_convert(base):\n    '''Convert a string to an integer.\n\n    The string may start with a sign.\n\n    It may be of a base other than 10.\n\n    If may start with a base indicator, 0#nnnn, which we assume should\n    override the specified base.\n\n    It may also have other non-numeric characters that we can ignore.\n    '''\n    CHARS = '0123456789abcdefghijklmnopqrstuvwxyz'\n\n    def f(string, match, base=base):\n        if string[0] == '-':\n            sign = -1\n        else:\n            sign = 1\n\n        if string[0] == '0' and len(string) > 2:\n            if string[1] in 'bB':\n                base = 2\n            elif string[1] in 'oO':\n                base = 8\n            elif string[1] in 'xX':\n                base = 16\n            else:\n                # just go with the base specifed\n                pass\n\n        chars = CHARS[:base]\n        string = re.sub('[^%s]' % chars, '', string.lower())\n        return sign * int(string, base)\n    return f"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting the incoming string containing some date / time info into a a datetime instance.", "response": "def date_convert(string, match, ymd=None, mdy=None, dmy=None,\n        d_m_y=None, hms=None, am=None, tz=None, mm=None, dd=None):\n    '''Convert the incoming string containing some date / time info into a\n    datetime instance.\n    '''\n    groups = match.groups()\n    time_only = False\n    if mm and dd:\n        y=datetime.today().year\n        m=groups[mm]\n        d=groups[dd]\n    elif ymd is not None:\n        y, m, d = re.split(r'[-/\\s]', groups[ymd])\n    elif mdy is not None:\n        m, d, y = re.split(r'[-/\\s]', groups[mdy])\n    elif dmy is not None:\n        d, m, y = re.split(r'[-/\\s]', groups[dmy])\n    elif d_m_y is not None:\n        d, m, y = d_m_y\n        d = groups[d]\n        m = groups[m]\n        y = groups[y]\n    else:\n        time_only = True\n\n    H = M = S = u = 0\n    if hms is not None and groups[hms]:\n        t = groups[hms].split(':')\n        if len(t) == 2:\n            H, M = t\n        else:\n            H, M, S = t\n            if '.' in S:\n                S, u = S.split('.')\n                u = int(float('.' + u) * 1000000)\n            S = int(S)\n        H = int(H)\n        M = int(M)\n\n    if am is not None:\n        am = groups[am]\n        if am:\n            am = am.strip()\n        if am == 'AM' and H == 12:\n            # correction for \"12\" hour functioning as \"0\" hour: 12:15 AM = 00:15 by 24 hr clock\n            H -= 12\n        elif am == 'PM' and H == 12:\n            # no correction needed: 12PM is midday, 12:00 by 24 hour clock\n            pass\n        elif am == 'PM':\n            H += 12\n\n    if tz is not None:\n        tz = groups[tz]\n    if tz == 'Z':\n        tz = FixedTzOffset(0, 'UTC')\n    elif tz:\n        tz = tz.strip()\n        if tz.isupper():\n            # TODO use the awesome python TZ module?\n            pass\n        else:\n            sign = tz[0]\n            if ':' in tz:\n                tzh, tzm = tz[1:].split(':')\n            elif len(tz) == 4:  # 'snnn'\n                tzh, tzm = tz[1], tz[2:4]\n            else:\n                tzh, tzm = tz[1:3], tz[3:5]\n            offset = int(tzm) + int(tzh) * 60\n            if sign == '-':\n                offset = -offset\n            tz = FixedTzOffset(offset, tz)\n\n    if time_only:\n        d = time(H, M, S, u, tzinfo=tz)\n    else:\n        y = int(y)\n        if m.isdigit():\n            m = int(m)\n        else:\n            m = MONTHS_MAP[m]\n        d = int(d)\n        d = datetime(y, m, d, H, M, S, u, tzinfo=tz)\n\n    return d"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef extract_format(format, extra_types):\n    '''Pull apart the format [[fill]align][0][width][.precision][type]\n    '''\n    fill = align = None\n    if format[0] in '<>=^':\n        align = format[0]\n        format = format[1:]\n    elif len(format) > 1 and format[1] in '<>=^':\n        fill = format[0]\n        align = format[1]\n        format = format[2:]\n\n    zero = False\n    if format and format[0] == '0':\n        zero = True\n        format = format[1:]\n\n    width = ''\n    while format:\n        if not format[0].isdigit():\n            break\n        width += format[0]\n        format = format[1:]\n\n    if format.startswith('.'):\n        # Precision isn't needed but we need to capture it so that\n        # the ValueError isn't raised.\n        format = format[1:]  # drop the '.'\n        precision = ''\n        while format:\n            if not format[0].isdigit():\n                break\n            precision += format[0]\n            format = format[1:]\n\n    # the rest is the type, if present\n    type = format\n    if type and type not in ALLOWED_TYPES and type not in extra_types:\n        raise ValueError('format spec %r not recognised' % type)\n\n    return locals()", "response": "Pull apart the format and return the dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing a string using the given format.", "response": "def parse(format, string, extra_types=None, evaluate_result=True, case_sensitive=False):\n    '''Using \"format\" attempt to pull values from \"string\".\n\n    The format must match the string contents exactly. If the value\n    you're looking for is instead just a part of the string use\n    search().\n\n    If ``evaluate_result`` is True the return value will be an Result instance with two attributes:\n\n     .fixed - tuple of fixed-position values from the string\n     .named - dict of named values from the string\n\n    If ``evaluate_result`` is False the return value will be a Match instance with one method:\n\n     .evaluate_result() - This will return a Result instance like you would get\n                          with ``evaluate_result`` set to True\n\n    The default behaviour is to match strings case insensitively. You may match with\n    case by specifying case_sensitive=True.\n\n    If the format is invalid a ValueError will be raised.\n\n    See the module documentation for the use of \"extra_types\".\n\n    In the case there is no match parse() will return None.\n    '''\n    p = Parser(format, extra_types=extra_types, case_sensitive=case_sensitive)\n    return p.parse(string, evaluate_result=evaluate_result)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef search(format, string, pos=0, endpos=None, extra_types=None, evaluate_result=True,\n        case_sensitive=False):\n    '''Search \"string\" for the first occurrence of \"format\".\n\n    The format may occur anywhere within the string. If\n    instead you wish for the format to exactly match the string\n    use parse().\n\n    Optionally start the search at \"pos\" character index and limit the search\n    to a maximum index of endpos - equivalent to search(string[:endpos]).\n\n    If ``evaluate_result`` is True the return value will be an Result instance with two attributes:\n\n     .fixed - tuple of fixed-position values from the string\n     .named - dict of named values from the string\n\n    If ``evaluate_result`` is False the return value will be a Match instance with one method:\n\n     .evaluate_result() - This will return a Result instance like you would get\n                          with ``evaluate_result`` set to True\n\n    The default behaviour is to match strings case insensitively. You may match with\n    case by specifying case_sensitive=True.\n\n    If the format is invalid a ValueError will be raised.\n\n    See the module documentation for the use of \"extra_types\".\n\n    In the case there is no match parse() will return None.\n    '''\n    p = Parser(format, extra_types=extra_types, case_sensitive=case_sensitive)\n    return p.search(string, pos, endpos, evaluate_result=evaluate_result)", "response": "Search string for the first occurrence of format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmatching my format to the string exactly. Return a Result or Match instance or None.", "response": "def parse(self, string, evaluate_result=True):\n        '''Match my format to the string exactly.\n\n        Return a Result or Match instance or None if there's no match.\n        '''\n        m = self._match_re.match(string)\n        if m is None:\n            return None\n\n        if evaluate_result:\n            return self.evaluate_result(m)\n        else:\n            return Match(self, m)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsearching the string for my format.", "response": "def search(self, string, pos=0, endpos=None, evaluate_result=True):\n        '''Search the string for my format.\n\n        Optionally start the search at \"pos\" character index and limit the\n        search to a maximum index of endpos - equivalent to\n        search(string[:endpos]).\n\n        If the ``evaluate_result`` argument is set to ``False`` a\n        Match instance is returned instead of the actual Result instance.\n\n        Return either a Result instance or None if there's no match.\n        '''\n        if endpos is None:\n            endpos = len(string)\n        m = self._search_re.search(string, pos, endpos)\n        if m is None:\n            return None\n\n        if evaluate_result:\n            return self.evaluate_result(m)\n        else:\n            return Match(self, m)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef findall(self, string, pos=0, endpos=None, extra_types=None, evaluate_result=True):\n        '''Search \"string\" for all occurrences of \"format\".\n\n        Optionally start the search at \"pos\" character index and limit the\n        search to a maximum index of endpos - equivalent to\n        search(string[:endpos]).\n\n        Returns an iterator that holds Result or Match instances for each format match\n        found.\n        '''\n        if endpos is None:\n            endpos = len(string)\n        return ResultIterator(self, string, pos, endpos, evaluate_result=evaluate_result)", "response": "Search string for all occurrences of format."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nevaluate the result of a regex match object m.", "response": "def evaluate_result(self, m):\n        '''Generate a Result instance for the given regex match object'''\n        # ok, figure the fixed fields we've pulled out and type convert them\n        fixed_fields = list(m.groups())\n        for n in self._fixed_fields:\n            if n in self._type_conversions:\n                fixed_fields[n] = self._type_conversions[n](fixed_fields[n], m)\n        fixed_fields = tuple(fixed_fields[n] for n in self._fixed_fields)\n\n        # grab the named fields, converting where requested\n        groupdict = m.groupdict()\n        named_fields = {}\n        name_map = {}\n        for k in self._named_fields:\n            korig = self._group_to_name_map[k]\n            name_map[korig] = k\n            if k in self._type_conversions:\n                value = self._type_conversions[k](groupdict[k], m)\n            else:\n                value = groupdict[k]\n\n            named_fields[korig] = value\n\n        # now figure the match spans\n        spans = dict((n, m.span(name_map[n])) for n in named_fields)\n        spans.update((i, m.span(n + 1))\n            for i, n in enumerate(self._fixed_fields))\n\n        # and that's our result\n        return Result(fixed_fields, self._expand_named_fields(named_fields), spans)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef install(\n    ctx,\n    state,\n    **kwargs\n):\n    \"\"\"Installs provided packages and adds them to Pipfile, or (if no packages are given), installs all packages from Pipfile.\"\"\"\n    from ..core import do_install\n\n    retcode = do_install(\n        dev=state.installstate.dev,\n        three=state.three,\n        python=state.python,\n        pypi_mirror=state.pypi_mirror,\n        system=state.system,\n        lock=not state.installstate.skip_lock,\n        ignore_pipfile=state.installstate.ignore_pipfile,\n        skip_lock=state.installstate.skip_lock,\n        requirements=state.installstate.requirementstxt,\n        sequential=state.installstate.sequential,\n        pre=state.installstate.pre,\n        code=state.installstate.code,\n        deploy=state.installstate.deploy,\n        keep_outdated=state.installstate.keep_outdated,\n        selective_upgrade=state.installstate.selective_upgrade,\n        index_url=state.index,\n        extra_index_url=state.extra_index_urls,\n        packages=state.installstate.packages,\n        editable_packages=state.installstate.editables,\n    )\n    if retcode:\n        ctx.abort()", "response": "Installs provided packages and adds them to Pipfile."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef shell(\n    state,\n    fancy=False,\n    shell_args=None,\n    anyway=False,\n):\n    \"\"\"Spawns a shell within the virtualenv.\"\"\"\n    from ..core import load_dot_env, do_shell\n\n    # Prevent user from activating nested environments.\n    if \"PIPENV_ACTIVE\" in os.environ:\n        # If PIPENV_ACTIVE is set, VIRTUAL_ENV should always be set too.\n        venv_name = os.environ.get(\"VIRTUAL_ENV\", \"UNKNOWN_VIRTUAL_ENVIRONMENT\")\n        if not anyway:\n            echo(\n                \"{0} {1} {2}\\nNo action taken to avoid nested environments.\".format(\n                    crayons.normal(\"Shell for\"),\n                    crayons.green(venv_name, bold=True),\n                    crayons.normal(\"already activated.\", bold=True),\n                ),\n                err=True,\n            )\n            sys.exit(1)\n    # Load .env file.\n    load_dot_env()\n    # Use fancy mode for Windows.\n    if os.name == \"nt\":\n        fancy = True\n    do_shell(\n        three=state.three,\n        python=state.python,\n        fancy=fancy,\n        shell_args=shell_args,\n        pypi_mirror=state.pypi_mirror,\n    )", "response": "Spawns a shell within the virtualenv."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run(state, command, args):\n    from ..core import do_run\n    do_run(\n        command=command, args=args, three=state.three, python=state.python, pypi_mirror=state.pypi_mirror\n    )", "response": "Spawns a command installed into the virtualenv."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking for security vulnerabilities and against PEP 508 markers provided in Pipfile.", "response": "def check(\n    state,\n    unused=False,\n    style=False,\n    ignore=None,\n    args=None,\n    **kwargs\n):\n    \"\"\"Checks for security vulnerabilities and against PEP 508 markers provided in Pipfile.\"\"\"\n    from ..core import do_check\n\n    do_check(\n        three=state.three,\n        python=state.python,\n        system=state.system,\n        unused=unused,\n        ignore=ignore,\n        args=args,\n        pypi_mirror=state.pypi_mirror,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update(\n    ctx,\n    state,\n    bare=False,\n    dry_run=None,\n    outdated=False,\n    **kwargs\n):\n    \"\"\"Runs lock, then sync.\"\"\"\n    from ..core import (\n        ensure_project,\n        do_outdated,\n        do_lock,\n        do_sync,\n        project,\n    )\n\n    ensure_project(three=state.three, python=state.python, warn=True, pypi_mirror=state.pypi_mirror)\n    if not outdated:\n        outdated = bool(dry_run)\n    if outdated:\n        do_outdated(pypi_mirror=state.pypi_mirror)\n    packages = [p for p in state.installstate.packages if p]\n    editable = [p for p in state.installstate.editables if p]\n    if not packages:\n        echo(\n            \"{0} {1} {2} {3}{4}\".format(\n                crayons.white(\"Running\", bold=True),\n                crayons.red(\"$ pipenv lock\", bold=True),\n                crayons.white(\"then\", bold=True),\n                crayons.red(\"$ pipenv sync\", bold=True),\n                crayons.white(\".\", bold=True),\n            )\n        )\n    else:\n        for package in packages + editable:\n            if package not in project.all_packages:\n                echo(\n                    \"{0}: {1} was not found in your Pipfile! Aborting.\"\n                    \"\".format(\n                        crayons.red(\"Warning\", bold=True),\n                        crayons.green(package, bold=True),\n                    ),\n                    err=True,\n                )\n                ctx.abort()\n\n    do_lock(\n        clear=state.clear,\n        pre=state.installstate.pre,\n        keep_outdated=state.installstate.keep_outdated,\n        pypi_mirror=state.pypi_mirror,\n    )\n    do_sync(\n        ctx=ctx,\n        dev=state.installstate.dev,\n        three=state.three,\n        python=state.python,\n        bare=bare,\n        dont_upgrade=not state.installstate.keep_outdated,\n        user=False,\n        clear=state.clear,\n        unused=False,\n        sequential=state.installstate.sequential,\n        pypi_mirror=state.pypi_mirror,\n    )", "response": "Runs lock then sync."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndisplaying currently - installed dependency graph information.", "response": "def graph(bare=False, json=False, json_tree=False, reverse=False):\n    \"\"\"Displays currently-installed dependency graph information.\"\"\"\n    from ..core import do_graph\n\n    do_graph(bare=bare, json=json, json_tree=json_tree, reverse=reverse)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run_open(state, module, *args, **kwargs):\n    from ..core import which, ensure_project, inline_activate_virtual_environment\n\n    # Ensure that virtualenv is available.\n    ensure_project(\n        three=state.three, python=state.python,\n        validate=False, pypi_mirror=state.pypi_mirror,\n    )\n    c = delegator.run(\n        '{0} -c \"import {1}; print({1}.__file__);\"'.format(which(\"python\"), module)\n    )\n    try:\n        assert c.return_code == 0\n    except AssertionError:\n        echo(crayons.red(\"Module not found!\"))\n        sys.exit(1)\n    if \"__init__.py\" in c.out:\n        p = os.path.dirname(c.out.strip().rstrip(\"cdo\"))\n    else:\n        p = c.out.strip().rstrip(\"cdo\")\n    echo(crayons.normal(\"Opening {0!r} in your EDITOR.\".format(p), bold=True))\n    inline_activate_virtual_environment()\n    edit(filename=p)\n    return 0", "response": "View a given module in your editor."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sync(\n    ctx,\n    state,\n    bare=False,\n    user=False,\n    unused=False,\n    **kwargs\n):\n    \"\"\"Installs all packages specified in Pipfile.lock.\"\"\"\n    from ..core import do_sync\n\n    retcode = do_sync(\n        ctx=ctx,\n        dev=state.installstate.dev,\n        three=state.three,\n        python=state.python,\n        bare=bare,\n        dont_upgrade=(not state.installstate.keep_outdated),\n        user=user,\n        clear=state.clear,\n        unused=unused,\n        sequential=state.installstate.sequential,\n        pypi_mirror=state.pypi_mirror,\n    )\n    if retcode:\n        ctx.abort()", "response": "Installs all packages specified in Pipfile. lock."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef consumeNumberEntity(self, isHex):\n\n        allowed = digits\n        radix = 10\n        if isHex:\n            allowed = hexDigits\n            radix = 16\n\n        charStack = []\n\n        # Consume all the characters that are in range while making sure we\n        # don't hit an EOF.\n        c = self.stream.char()\n        while c in allowed and c is not EOF:\n            charStack.append(c)\n            c = self.stream.char()\n\n        # Convert the set of characters consumed to an int.\n        charAsInt = int(\"\".join(charStack), radix)\n\n        # Certain characters get replaced with others\n        if charAsInt in replacementCharacters:\n            char = replacementCharacters[charAsInt]\n            self.tokenQueue.append({\"type\": tokenTypes[\"ParseError\"], \"data\":\n                                    \"illegal-codepoint-for-numeric-entity\",\n                                    \"datavars\": {\"charAsInt\": charAsInt}})\n        elif ((0xD800 <= charAsInt <= 0xDFFF) or\n              (charAsInt > 0x10FFFF)):\n            char = \"\\uFFFD\"\n            self.tokenQueue.append({\"type\": tokenTypes[\"ParseError\"], \"data\":\n                                    \"illegal-codepoint-for-numeric-entity\",\n                                    \"datavars\": {\"charAsInt\": charAsInt}})\n        else:\n            # Should speed up this check somehow (e.g. move the set to a constant)\n            if ((0x0001 <= charAsInt <= 0x0008) or\n                (0x000E <= charAsInt <= 0x001F) or\n                (0x007F <= charAsInt <= 0x009F) or\n                (0xFDD0 <= charAsInt <= 0xFDEF) or\n                charAsInt in frozenset([0x000B, 0xFFFE, 0xFFFF, 0x1FFFE,\n                                        0x1FFFF, 0x2FFFE, 0x2FFFF, 0x3FFFE,\n                                        0x3FFFF, 0x4FFFE, 0x4FFFF, 0x5FFFE,\n                                        0x5FFFF, 0x6FFFE, 0x6FFFF, 0x7FFFE,\n                                        0x7FFFF, 0x8FFFE, 0x8FFFF, 0x9FFFE,\n                                        0x9FFFF, 0xAFFFE, 0xAFFFF, 0xBFFFE,\n                                        0xBFFFF, 0xCFFFE, 0xCFFFF, 0xDFFFE,\n                                        0xDFFFF, 0xEFFFE, 0xEFFFF, 0xFFFFE,\n                                        0xFFFFF, 0x10FFFE, 0x10FFFF])):\n                self.tokenQueue.append({\"type\": tokenTypes[\"ParseError\"],\n                                        \"data\":\n                                        \"illegal-codepoint-for-numeric-entity\",\n                                        \"datavars\": {\"charAsInt\": charAsInt}})\n            try:\n                # Try/except needed as UCS-2 Python builds' unichar only works\n                # within the BMP.\n                char = chr(charAsInt)\n            except ValueError:\n                v = charAsInt - 0x10000\n                char = chr(0xD800 | (v >> 10)) + chr(0xDC00 | (v & 0x3FF))\n\n        # Discard the ; if present. Otherwise, put it back on the queue and\n        # invoke parseError on parser.\n        if c != \";\":\n            self.tokenQueue.append({\"type\": tokenTypes[\"ParseError\"], \"data\":\n                                    \"numeric-entity-without-semicolon\"})\n            self.stream.unget(c)\n\n        return char", "response": "This function returns either U + FFFD or the character based on the ISO - 10646 numeric entity. It returns U + FFFD or the character based on the ISO - 10646 decimal or hexadecimal representation. It discards \";\" if present."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef emitCurrentToken(self):\n        token = self.currentToken\n        # Add token to the queue to be yielded\n        if (token[\"type\"] in tagTokenTypes):\n            token[\"name\"] = token[\"name\"].translate(asciiUpper2Lower)\n            if token[\"type\"] == tokenTypes[\"EndTag\"]:\n                if token[\"data\"]:\n                    self.tokenQueue.append({\"type\": tokenTypes[\"ParseError\"],\n                                            \"data\": \"attributes-in-end-tag\"})\n                if token[\"selfClosing\"]:\n                    self.tokenQueue.append({\"type\": tokenTypes[\"ParseError\"],\n                                            \"data\": \"self-closing-flag-on-end-tag\"})\n        self.tokenQueue.append(token)\n        self.state = self.dataState", "response": "This method is a generic handler for emitting the tags. It also sets the state to data because that s what s needed after a token has been emitted. It also adds the current token to the queue to be yielded\n           ."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_package_set_from_installed(**kwargs):\n    # type: (**Any) -> Tuple[PackageSet, bool]\n    \"\"\"Converts a list of distributions into a PackageSet.\n    \"\"\"\n    # Default to using all packages installed on the system\n    if kwargs == {}:\n        kwargs = {\"local_only\": False, \"skip\": ()}\n\n    package_set = {}\n    problems = False\n    for dist in get_installed_distributions(**kwargs):\n        name = canonicalize_name(dist.project_name)\n        try:\n            package_set[name] = PackageDetails(dist.version, dist.requires())\n        except RequirementParseError as e:\n            # Don't crash on broken metadata\n            logging.warning(\"Error parsing requirements for %s: %s\", name, e)\n            problems = True\n    return package_set, problems", "response": "Converts a list of distributions into a PackageSet."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if a package set is consistent with another package set.", "response": "def check_package_set(package_set, should_ignore=None):\n    # type: (PackageSet, Optional[Callable[[str], bool]]) -> CheckResult\n    \"\"\"Check if a package set is consistent\n\n    If should_ignore is passed, it should be a callable that takes a\n    package name and returns a boolean.\n    \"\"\"\n    if should_ignore is None:\n        def should_ignore(name):\n            return False\n\n    missing = dict()\n    conflicting = dict()\n\n    for package_name in package_set:\n        # Info about dependencies of package_name\n        missing_deps = set()  # type: Set[Missing]\n        conflicting_deps = set()  # type: Set[Conflicting]\n\n        if should_ignore(package_name):\n            continue\n\n        for req in package_set[package_name].requires:\n            name = canonicalize_name(req.project_name)  # type: str\n\n            # Check if it's missing\n            if name not in package_set:\n                missed = True\n                if req.marker is not None:\n                    missed = req.marker.evaluate()\n                if missed:\n                    missing_deps.add((name, req))\n                continue\n\n            # Check if there's a conflict\n            version = package_set[name].version  # type: str\n            if not req.specifier.contains(version, prereleases=True):\n                conflicting_deps.add((name, version, req))\n\n        if missing_deps:\n            missing[package_name] = sorted(missing_deps, key=str)\n        if conflicting_deps:\n            conflicting[package_name] = sorted(conflicting_deps, key=str)\n\n    return missing, conflicting"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _simulate_installation_of(to_install, package_set):\n    # type: (List[InstallRequirement], PackageSet) -> Set[str]\n    \"\"\"Computes the version of packages after installing to_install.\n    \"\"\"\n\n    # Keep track of packages that were installed\n    installed = set()\n\n    # Modify it as installing requirement_set would (assuming no errors)\n    for inst_req in to_install:\n        dist = make_abstract_dist(inst_req).dist()\n        name = canonicalize_name(dist.key)\n        package_set[name] = PackageDetails(dist.version, dist.requires())\n\n        installed.add(name)\n\n    return installed", "response": "Simulates the version of packages after installing to_install."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef filter_international_words(buf):\n        filtered = bytearray()\n\n        # This regex expression filters out only words that have at-least one\n        # international character. The word may include one marker character at\n        # the end.\n        words = re.findall(b'[a-zA-Z]*[\\x80-\\xFF]+[a-zA-Z]*[^a-zA-Z\\x80-\\xFF]?',\n                           buf)\n\n        for word in words:\n            filtered.extend(word[:-1])\n\n            # If the last character in the word is a marker, replace it with a\n            # space as markers shouldn't affect our analysis (they are used\n            # similarly across all languages and may thus have similar\n            # frequencies).\n            last_char = word[-1:]\n            if not last_char.isalpha() and last_char < b'\\x80':\n                last_char = b' '\n            filtered.extend(last_char)\n\n        return filtered", "response": "This function filters out the international characters in the input buffer that are not part of the language."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef filter_with_english_letters(buf):\n        filtered = bytearray()\n        in_tag = False\n        prev = 0\n\n        for curr in range(len(buf)):\n            # Slice here to get bytes instead of an int with Python 3\n            buf_char = buf[curr:curr + 1]\n            # Check if we're coming out of or entering an HTML tag\n            if buf_char == b'>':\n                in_tag = False\n            elif buf_char == b'<':\n                in_tag = True\n\n            # If current character is not extended-ASCII and not alphabetic...\n            if buf_char < b'\\x80' and not buf_char.isalpha():\n                # ...and we're not in a tag\n                if curr > prev and not in_tag:\n                    # Keep everything after last non-extended-ASCII,\n                    # non-alphabetic character\n                    filtered.extend(buf[prev:curr])\n                    # Output a space to delimit stretch we kept\n                    filtered.extend(b' ')\n                prev = curr + 1\n\n        # If we're not in a tag...\n        if not in_tag:\n            # Keep everything after last non-extended-ASCII, non-alphabetic\n            # character\n            filtered.extend(buf[prev:])\n\n        return filtered", "response": "Filter out all occurrences of English - specific characters in the buffer."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef join_parsed_parts(self, drv, root, parts, drv2, root2, parts2):\n        if root2:\n            if not drv2 and drv:\n                return drv, root2, [drv + root2] + parts2[1:]\n        elif drv2:\n            if drv2 == drv or self.casefold(drv2) == self.casefold(drv):\n                # Same drive => second path is relative to the first\n                return drv, root, parts + parts2[1:]\n        else:\n            # Second path is non-anchored (common case)\n            return drv, root, parts + parts2\n        return drv2, root2, parts2", "response": "Join the two paths represented by the respective\n            and return a new tuple."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef select_from(self, parent_path):\n        path_cls = type(parent_path)\n        is_dir = path_cls.is_dir\n        exists = path_cls.exists\n        scandir = parent_path._accessor.scandir\n        if not is_dir(parent_path):\n            return iter([])\n        return self._select_from(parent_path, is_dir, exists, scandir)", "response": "Iterate over all child paths of parent_path matched by this\n        selector. This can contain parent_path itself."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the string representation of the path with forward ( / ) slashes.", "response": "def as_posix(self):\n        \"\"\"Return the string representation of the path with forward (/)\n        slashes.\"\"\"\n        f = self._flavour\n        return str(self).replace(f.sep, '/')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef name(self):\n        parts = self._parts\n        if len(parts) == (1 if (self._drv or self._root) else 0):\n            return ''\n        return parts[-1]", "response": "The name of the current path component."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef stem(self):\n        name = self.name\n        i = name.rfind('.')\n        if 0 < i < len(name) - 1:\n            return name[:i]\n        else:\n            return name", "response": "The final path component minus its last suffix."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a new path with the file name changed.", "response": "def with_name(self, name):\n        \"\"\"Return a new path with the file name changed.\"\"\"\n        if not self.name:\n            raise ValueError(\"%r has an empty name\" % (self,))\n        drv, root, parts = self._flavour.parse_parts((name,))\n        if (not name or name[-1] in [self._flavour.sep, self._flavour.altsep]\n                or drv or root or len(parts) != 1):\n            raise ValueError(\"Invalid name %r\" % (name))\n        return self._from_parsed_parts(self._drv, self._root,\n                                       self._parts[:-1] + [name])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a new path with the file suffix changed or added.", "response": "def with_suffix(self, suffix):\n        \"\"\"Return a new path with the file suffix changed (or added, if\n        none).\n        \"\"\"\n        # XXX if suffix is None, should the current suffix be removed?\n        f = self._flavour\n        if f.sep in suffix or f.altsep and f.altsep in suffix:\n            raise ValueError(\"Invalid suffix %r\" % (suffix))\n        if suffix and not suffix.startswith('.') or suffix == '.':\n            raise ValueError(\"Invalid suffix %r\" % (suffix))\n        name = self.name\n        if not name:\n            raise ValueError(\"%r has an empty name\" % (self,))\n        old_suffix = self.suffix\n        if not old_suffix:\n            name = name + suffix\n        else:\n            name = name[:-len(old_suffix)] + suffix\n        return self._from_parsed_parts(self._drv, self._root,\n                                       self._parts[:-1] + [name])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parent(self):\n        drv = self._drv\n        root = self._root\n        parts = self._parts\n        if len(parts) == 1 and (drv or root):\n            return self\n        return self._from_parsed_parts(drv, root, parts[:-1])", "response": "The logical parent of the path."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntrues if the path is absolute.", "response": "def is_absolute(self):\n        \"\"\"True if the path is absolute (has both a root and, if applicable,\n        a drive).\"\"\"\n        if not self._root:\n            return False\n        return not self._flavour.has_drv or bool(self._drv)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning True if this path matches the given pattern.", "response": "def match(self, path_pattern):\n        \"\"\"\n        Return True if this path matches the given pattern.\n        \"\"\"\n        cf = self._flavour.casefold\n        path_pattern = cf(path_pattern)\n        drv, root, pat_parts = self._flavour.parse_parts((path_pattern,))\n        if not pat_parts:\n            raise ValueError(\"empty pattern\")\n        if drv and drv != cf(self._drv):\n            return False\n        if root and root != cf(self._root):\n            return False\n        parts = self._cparts\n        if drv or root:\n            if len(pat_parts) != len(parts):\n                return False\n            pat_parts = pat_parts[1:]\n        elif len(pat_parts) > len(parts):\n            return False\n        for part, pat in zip(reversed(parts), reversed(pat_parts)):\n            if not fnmatch.fnmatchcase(part, pat):\n                return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _raw_open(self, flags, mode=0o777):\n        if self._closed:\n            self._raise_closed()\n        return self._accessor.open(self, flags, mode)", "response": "Open the file pointed by this path and return a file descriptor."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning whether this file is the same or not as another file.", "response": "def samefile(self, other_path):\n        \"\"\"Return whether other_path is the same or not as this file\n        (as returned by os.path.samefile()).\n        \"\"\"\n        if hasattr(os.path, \"samestat\"):\n            st = self.stat()\n            try:\n                other_st = other_path.stat()\n            except AttributeError:\n                other_st = os.stat(other_path)\n            return os.path.samestat(st, other_st)\n        else:\n            filename1 = six.text_type(self)\n            filename2 = six.text_type(other_path)\n            st1 = _win32_get_unique_path_id(filename1)\n            st2 = _win32_get_unique_path_id(filename2)\n            return st1 == st2"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\niterates over the files in this directory. Does not yield any result for the special paths. and..", "response": "def iterdir(self):\n        \"\"\"Iterate over the files in this directory.  Does not yield any\n        result for the special paths '.' and '..'.\n        \"\"\"\n        if self._closed:\n            self._raise_closed()\n        for name in self._accessor.listdir(self):\n            if name in ('.', '..'):\n                # Yielding a path object for these makes little sense\n                continue\n            yield self._make_child_relpath(name)\n            if self._closed:\n                self._raise_closed()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates this file with the given access mode if it doesn t exist.", "response": "def touch(self, mode=0o666, exist_ok=True):\n        \"\"\"\n        Create this file with the given access mode, if it doesn't exist.\n        \"\"\"\n        if self._closed:\n            self._raise_closed()\n        if exist_ok:\n            # First try to bump modification time\n            # Implementation note: GNU touch uses the UTIME_NOW option of\n            # the utimensat() / futimens() functions.\n            try:\n                self._accessor.utime(self, None)\n            except OSError:\n                # Avoid exception chaining\n                pass\n            else:\n                return\n        flags = os.O_CREAT | os.O_WRONLY\n        if not exist_ok:\n            flags |= os.O_EXCL\n        fd = self._raw_open(flags, mode)\n        os.close(fd)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new directory at this given path.", "response": "def mkdir(self, mode=0o777, parents=False, exist_ok=False):\n        \"\"\"\n        Create a new directory at this given path.\n        \"\"\"\n        if self._closed:\n            self._raise_closed()\n\n        def _try_func():\n            self._accessor.mkdir(self, mode)\n\n        def _exc_func(exc):\n            if not parents or self.parent == self:\n                raise exc\n            self.parent.mkdir(parents=True, exist_ok=True)\n            self.mkdir(mode, parents=False, exist_ok=exist_ok)\n\n        try:\n            _try_except_filenotfounderror(_try_func, _exc_func)\n        except OSError:\n            if not exist_ok or not self.is_dir():\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef chmod(self, mode):\n        if self._closed:\n            self._raise_closed()\n        self._accessor.chmod(self, mode)", "response": "Change the permissions of the path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef lchmod(self, mode):\n        if self._closed:\n            self._raise_closed()\n        self._accessor.lchmod(self, mode)", "response": "Like os. lchmod except that the permissions of the symlink is changed rather than its target s permissions."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves this file or link.", "response": "def unlink(self):\n        \"\"\"\n        Remove this file or link.\n        If the path is a directory, use rmdir() instead.\n        \"\"\"\n        if self._closed:\n            self._raise_closed()\n        self._accessor.unlink(self)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rmdir(self):\n        if self._closed:\n            self._raise_closed()\n        self._accessor.rmdir(self)", "response": "Remove this directory.  The directory must be empty."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef lstat(self):\n        if self._closed:\n            self._raise_closed()\n        return self._accessor.lstat(self)", "response": "Like os. lstat except that the path points to a symlink and the symlink s status information is returned rather than its target s status information."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrenaming this path to the given path.", "response": "def rename(self, target):\n        \"\"\"\n        Rename this path to the given path.\n        \"\"\"\n        if self._closed:\n            self._raise_closed()\n        self._accessor.rename(self, target)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrename this path to the given path clobbering the existing one if it exists.", "response": "def replace(self, target):\n        \"\"\"\n        Rename this path to the given path, clobbering the existing\n        destination if it exists.\n        \"\"\"\n        if sys.version_info < (3, 3):\n            raise NotImplementedError(\"replace() is only available \"\n                                      \"with Python 3.3 and later\")\n        if self._closed:\n            self._raise_closed()\n        self._accessor.replace(self, target)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef symlink_to(self, target, target_is_directory=False):\n        if self._closed:\n            self._raise_closed()\n        self._accessor.symlink(target, self, target_is_directory)", "response": "Make this path a symlink pointing to the given path."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef exists(self):\n        try:\n            self.stat()\n        except OSError as e:\n            if e.errno not in (ENOENT, ENOTDIR):\n                raise\n            return False\n        return True", "response": "Returns True if the path exists."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_dir(self):\n        try:\n            return S_ISDIR(self.stat().st_mode)\n        except OSError as e:\n            if e.errno not in (ENOENT, ENOTDIR):\n                raise\n            # Path doesn't exist or is a broken symlink\n            # (see https://bitbucket.org/pitrou/pathlib/issue/12/)\n            return False", "response": "Returns True if the path is a directory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning True if this path is a regular file.", "response": "def is_file(self):\n        \"\"\"\n        Whether this path is a regular file (also True for symlinks pointing\n        to regular files).\n        \"\"\"\n        try:\n            return S_ISREG(self.stat().st_mode)\n        except OSError as e:\n            if e.errno not in (ENOENT, ENOTDIR):\n                raise\n            # Path doesn't exist or is a broken symlink\n            # (see https://bitbucket.org/pitrou/pathlib/issue/12/)\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning True if this path is a FIFO.", "response": "def is_fifo(self):\n        \"\"\"\n        Whether this path is a FIFO.\n        \"\"\"\n        try:\n            return S_ISFIFO(self.stat().st_mode)\n        except OSError as e:\n            if e.errno not in (ENOENT, ENOTDIR):\n                raise\n            # Path doesn't exist or is a broken symlink\n            # (see https://bitbucket.org/pitrou/pathlib/issue/12/)\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns True if this path is a socket.", "response": "def is_socket(self):\n        \"\"\"\n        Whether this path is a socket.\n        \"\"\"\n        try:\n            return S_ISSOCK(self.stat().st_mode)\n        except OSError as e:\n            if e.errno not in (ENOENT, ENOTDIR):\n                raise\n            # Path doesn't exist or is a broken symlink\n            # (see https://bitbucket.org/pitrou/pathlib/issue/12/)\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef expanduser(self):\n        if (not (self._drv or self._root)\n                and self._parts and self._parts[0][:1] == '~'):\n            homedir = self._flavour.gethomedir(self._parts[0][1:])\n            return self._from_parts([homedir] + self._parts[1:])\n\n        return self", "response": "Return a new path with expanded ~ and ~user constructs\n           "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef lookupEncoding(encoding):\n    if isinstance(encoding, binary_type):\n        try:\n            encoding = encoding.decode(\"ascii\")\n        except UnicodeDecodeError:\n            return None\n\n    if encoding is not None:\n        try:\n            return webencodings.lookup(encoding)\n        except AttributeError:\n            return None\n    else:\n        return None", "response": "Return the python codec name corresponding to an encoding or None if the encoding is not a valid encoding."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nproduces a file object from source. source can be either a file object local filename or a string.", "response": "def openStream(self, source):\n        \"\"\"Produces a file object from source.\n\n        source can be either a file object, local filename or a string.\n\n        \"\"\"\n        # Already a file object\n        if hasattr(source, 'read'):\n            stream = source\n        else:\n            stream = StringIO(source)\n\n        return stream"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the line and column of the current position in the stream.", "response": "def position(self):\n        \"\"\"Returns (line, col) of the current position in the stream.\"\"\"\n        line, col = self._position(self.chunkOffset)\n        return (line + 1, col)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread one character from the input stream or queue if available. Return EOF when EOF is reached.", "response": "def char(self):\n        \"\"\" Read one character from the stream or queue if available. Return\n            EOF when EOF is reached.\n        \"\"\"\n        # Read a new chunk from the input stream if necessary\n        if self.chunkOffset >= self.chunkSize:\n            if not self.readChunk():\n                return EOF\n\n        chunkOffset = self.chunkOffset\n        char = self.chunk[chunkOffset]\n        self.chunkOffset = chunkOffset + 1\n\n        return char"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef charsUntil(self, characters, opposite=False):\n\n        # Use a cache of regexps to find the required characters\n        try:\n            chars = charsUntilRegEx[(characters, opposite)]\n        except KeyError:\n            if __debug__:\n                for c in characters:\n                    assert(ord(c) < 128)\n            regex = \"\".join([\"\\\\x%02x\" % ord(c) for c in characters])\n            if not opposite:\n                regex = \"^%s\" % regex\n            chars = charsUntilRegEx[(characters, opposite)] = re.compile(\"[%s]+\" % regex)\n\n        rv = []\n\n        while True:\n            # Find the longest matching prefix\n            m = chars.match(self.chunk, self.chunkOffset)\n            if m is None:\n                # If nothing matched, and it wasn't because we ran out of chunk,\n                # then stop\n                if self.chunkOffset != self.chunkSize:\n                    break\n            else:\n                end = m.end()\n                # If not the whole chunk matched, return everything\n                # up to the part that didn't match\n                if end != self.chunkSize:\n                    rv.append(self.chunk[self.chunkOffset:end])\n                    self.chunkOffset = end\n                    break\n            # If the whole remainder of the chunk matched,\n            # use it all and read the next chunk\n            rv.append(self.chunk[self.chunkOffset:])\n            if not self.readChunk():\n                # Reached EOF\n                break\n\n        r = \"\".join(rv)\n        return r", "response": "Returns a string of characters from the stream up to but not\n        including any character in characters or EOF."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nproducing a file object from source. source can be either a file object local filename or a string.", "response": "def openStream(self, source):\n        \"\"\"Produces a file object from source.\n\n        source can be either a file object, local filename or a string.\n\n        \"\"\"\n        # Already a file object\n        if hasattr(source, 'read'):\n            stream = source\n        else:\n            stream = BytesIO(source)\n\n        try:\n            stream.seek(stream.tell())\n        except:  # pylint:disable=bare-except\n            stream = BufferedStream(stream)\n\n        return stream"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreport the encoding declared by the meta element", "response": "def detectEncodingMeta(self):\n        \"\"\"Report the encoding declared by the meta element\n        \"\"\"\n        buffer = self.rawStream.read(self.numBytesMeta)\n        assert isinstance(buffer, bytes)\n        parser = EncodingParser(buffer)\n        self.rawStream.seek(0)\n        encoding = parser.getEncoding()\n\n        if encoding is not None and encoding.name in (\"utf-16be\", \"utf-16le\"):\n            encoding = lookupEncoding(\"utf-8\")\n\n        return encoding"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nskipping past a list of characters", "response": "def skip(self, chars=spaceCharactersBytes):\n        \"\"\"Skip past a list of characters\"\"\"\n        p = self.position               # use property for the error-checking\n        while p < len(self):\n            c = self[p:p + 1]\n            if c not in chars:\n                self._position = p\n                return c\n            p += 1\n        self._position = p\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if the bytes in the bytes are in the beginning of the string False otherwise.", "response": "def matchBytes(self, bytes):\n        \"\"\"Look for a sequence of bytes at the start of a string. If the bytes\n        are found return True and advance the position to the byte after the\n        match. Otherwise return False and leave the position alone\"\"\"\n        p = self.position\n        data = self[p:p + len(bytes)]\n        rv = data.startswith(bytes)\n        if rv:\n            self.position += len(bytes)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef jumpTo(self, bytes):\n        newPosition = self[self.position:].find(bytes)\n        if newPosition > -1:\n            # XXX: This is ugly, but I can't see a nicer way to fix this.\n            if self._position == -1:\n                self._position = 0\n            self._position += (newPosition + len(bytes) - 1)\n            return True\n        else:\n            raise StopIteration", "response": "Look for the next sequence of bytes matching a given sequence. If the sequence is found advance the position to the last byte of the match. If the sequence is not found raise StopIteration."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a name value pair for the next attribute in the stream or None if no next attribute is found.", "response": "def getAttribute(self):\n        \"\"\"Return a name,value pair for the next attribute in the stream,\n        if one is found, or None\"\"\"\n        data = self.data\n        # Step 1 (skip chars)\n        c = data.skip(spaceCharactersBytes | frozenset([b\"/\"]))\n        assert c is None or len(c) == 1\n        # Step 2\n        if c in (b\">\", None):\n            return None\n        # Step 3\n        attrName = []\n        attrValue = []\n        # Step 4 attribute name\n        while True:\n            if c == b\"=\" and attrName:\n                break\n            elif c in spaceCharactersBytes:\n                # Step 6!\n                c = data.skip()\n                break\n            elif c in (b\"/\", b\">\"):\n                return b\"\".join(attrName), b\"\"\n            elif c in asciiUppercaseBytes:\n                attrName.append(c.lower())\n            elif c is None:\n                return None\n            else:\n                attrName.append(c)\n            # Step 5\n            c = next(data)\n        # Step 7\n        if c != b\"=\":\n            data.previous()\n            return b\"\".join(attrName), b\"\"\n        # Step 8\n        next(data)\n        # Step 9\n        c = data.skip()\n        # Step 10\n        if c in (b\"'\", b'\"'):\n            # 10.1\n            quoteChar = c\n            while True:\n                # 10.2\n                c = next(data)\n                # 10.3\n                if c == quoteChar:\n                    next(data)\n                    return b\"\".join(attrName), b\"\".join(attrValue)\n                # 10.4\n                elif c in asciiUppercaseBytes:\n                    attrValue.append(c.lower())\n                # 10.5\n                else:\n                    attrValue.append(c)\n        elif c == b\">\":\n            return b\"\".join(attrName), b\"\"\n        elif c in asciiUppercaseBytes:\n            attrValue.append(c.lower())\n        elif c is None:\n            return None\n        else:\n            attrValue.append(c)\n        # Step 11\n        while True:\n            c = next(data)\n            if c in spacesAngleBrackets:\n                return b\"\".join(attrName), b\"\".join(attrValue)\n            elif c in asciiUppercaseBytes:\n                attrValue.append(c.lower())\n            elif c is None:\n                return None\n            else:\n                attrValue.append(c)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds and load the build backend", "response": "def _build_backend():\n    \"\"\"Find and load the build backend\"\"\"\n    ep = os.environ['PEP517_BUILD_BACKEND']\n    mod_path, _, obj_path = ep.partition(':')\n    try:\n        obj = import_module(mod_path)\n    except ImportError:\n        raise BackendUnavailable\n    if obj_path:\n        for path_part in obj_path.split('.'):\n            obj = getattr(obj, path_part)\n    return obj"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninvoke the optional get_requires_for_build_wheel hook on the base object.", "response": "def get_requires_for_build_wheel(config_settings):\n    \"\"\"Invoke the optional get_requires_for_build_wheel hook\n\n    Returns [] if the hook is not defined.\n    \"\"\"\n    backend = _build_backend()\n    try:\n        hook = backend.get_requires_for_build_wheel\n    except AttributeError:\n        return []\n    else:\n        return hook(config_settings)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef prepare_metadata_for_build_wheel(metadata_directory, config_settings):\n    backend = _build_backend()\n    try:\n        hook = backend.prepare_metadata_for_build_wheel\n    except AttributeError:\n        return _get_wheel_metadata_from_wheel(backend, metadata_directory,\n                                              config_settings)\n    else:\n        return hook(metadata_directory, config_settings)", "response": "Invoke optional prepare_metadata_for_build_wheel method by building a wheel if it isn t defined."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nidentifying the. dist - info folder inside a wheel ZipFile.", "response": "def _dist_info_files(whl_zip):\n    \"\"\"Identify the .dist-info folder inside a wheel ZipFile.\"\"\"\n    res = []\n    for path in whl_zip.namelist():\n        m = re.match(r'[^/\\\\]+-[^/\\\\]+\\.dist-info/', path)\n        if m:\n            res.append(path)\n    if res:\n        return res\n    raise Exception(\"No .dist-info folder found in wheel\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_wheel_metadata_from_wheel(\n        backend, metadata_directory, config_settings):\n    \"\"\"Build a wheel and extract the metadata from it.\n\n    Fallback for when the build backend does not\n    define the 'get_wheel_metadata' hook.\n    \"\"\"\n    from zipfile import ZipFile\n    whl_basename = backend.build_wheel(metadata_directory, config_settings)\n    with open(os.path.join(metadata_directory, WHEEL_BUILT_MARKER), 'wb'):\n        pass  # Touch marker file\n\n    whl_file = os.path.join(metadata_directory, whl_basename)\n    with ZipFile(whl_file) as zipf:\n        dist_info = _dist_info_files(zipf)\n        zipf.extractall(path=metadata_directory, members=dist_info)\n    return dist_info[0].split('/')[0]", "response": "Build a wheel and extract the metadata from it."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds a wheel that is already built in the metadata directory.", "response": "def _find_already_built_wheel(metadata_directory):\n    \"\"\"Check for a wheel already built during the get_wheel_metadata hook.\n    \"\"\"\n    if not metadata_directory:\n        return None\n    metadata_parent = os.path.dirname(metadata_directory)\n    if not os.path.isfile(pjoin(metadata_parent, WHEEL_BUILT_MARKER)):\n        return None\n\n    whl_files = glob(os.path.join(metadata_parent, '*.whl'))\n    if not whl_files:\n        print('Found wheel built marker, but no .whl files')\n        return None\n    if len(whl_files) > 1:\n        print('Found multiple .whl files; unspecified behaviour. '\n              'Will call build_wheel.')\n        return None\n\n    # Exactly one .whl file\n    return whl_files[0]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbuilding a wheel from a directory.", "response": "def build_wheel(wheel_directory, config_settings, metadata_directory=None):\n    \"\"\"Invoke the mandatory build_wheel hook.\n\n    If a wheel was already built in the\n    prepare_metadata_for_build_wheel fallback, this\n    will copy it rather than rebuilding the wheel.\n    \"\"\"\n    prebuilt_whl = _find_already_built_wheel(metadata_directory)\n    if prebuilt_whl:\n        shutil.copy2(prebuilt_whl, wheel_directory)\n        return os.path.basename(prebuilt_whl)\n\n    return _build_backend().build_wheel(wheel_directory, config_settings,\n                                        metadata_directory)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_requires_for_build_sdist(config_settings):\n    backend = _build_backend()\n    try:\n        hook = backend.get_requires_for_build_sdist\n    except AttributeError:\n        return []\n    else:\n        return hook(config_settings)", "response": "Invoke the optional get_requires_for_build_wheel hook on the base object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nappend a new item to the table.", "response": "def append(self, key, _item):  # type: (Union[Key, str], Any) -> Table\n        \"\"\"\n        Appends a (key, item) to the table.\n        \"\"\"\n        if not isinstance(_item, Item):\n            _item = item(_item)\n\n        self._value.append(key, _item)\n\n        if isinstance(key, Key):\n            key = key.key\n\n        if key is not None:\n            super(Table, self).__setitem__(key, _item)\n\n        m = re.match(\"(?s)^[^ ]*([ ]+).*$\", self._trivia.indent)\n        if not m:\n            return self\n\n        indent = m.group(1)\n\n        if not isinstance(_item, Whitespace):\n            m = re.match(\"(?s)^([^ ]*)(.*)$\", _item.trivia.indent)\n            if not m:\n                _item.trivia.indent = indent\n            else:\n                _item.trivia.indent = m.group(1) + indent + m.group(2)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef append(self, key, _item):  # type: (Union[Key, str], Any) -> InlineTable\n        if not isinstance(_item, Item):\n            _item = item(_item)\n\n        if not isinstance(_item, (Whitespace, Comment)):\n            if not _item.trivia.indent and len(self._value) > 0:\n                _item.trivia.indent = \" \"\n            if _item.trivia.comment:\n                _item.trivia.comment = \"\"\n\n        self._value.append(key, _item)\n\n        if isinstance(key, Key):\n            key = key.key\n\n        if key is not None:\n            super(InlineTable, self).__setitem__(key, _item)\n\n        return self", "response": "Appends a new item to the table."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating a wrapper with the given object.", "response": "def update_wrapper(wrapper,\n                   wrapped,\n                   assigned = functools.WRAPPER_ASSIGNMENTS,\n                   updated = functools.WRAPPER_UPDATES):\n    \"\"\"\n    Patch two bugs in functools.update_wrapper.\n    \"\"\"\n    # workaround for http://bugs.python.org/issue3445\n    assigned = tuple(attr for attr in assigned if hasattr(wrapped, attr))\n    wrapper = functools.update_wrapper(wrapper, wrapped, assigned, updated)\n    # workaround for https://bugs.python.org/issue17482\n    wrapper.__wrapped__ = wrapped\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef first(iterable, default=None, key=None):\n    if key is None:\n        for el in iterable:\n            if el:\n                return el\n    else:\n        for el in iterable:\n            if key(el):\n                return el\n\n    return default", "response": "Return first element of iterable that evaluates true else return default."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntry to find the process tree via the output of ps.", "response": "def get_process_mapping():\n    \"\"\"Try to look up the process tree via the output of `ps`.\n    \"\"\"\n    try:\n        output = subprocess.check_output([\n            'ps', '-ww', '-o', 'pid=', '-o', 'ppid=', '-o', 'args=',\n        ])\n    except OSError as e:    # Python 2-compatible FileNotFoundError.\n        if e.errno != errno.ENOENT:\n            raise\n        raise PsNotAvailable('ps not found')\n    except subprocess.CalledProcessError as e:\n        # `ps` can return 1 if the process list is completely empty.\n        # (sarugaku/shellingham#15)\n        if not e.output.strip():\n            return {}\n        raise\n    if not isinstance(output, str):\n        encoding = sys.getfilesystemencoding() or sys.getdefaultencoding()\n        output = output.decode(encoding)\n    processes = {}\n    for line in output.split('\\n'):\n        try:\n            pid, ppid, args = line.strip().split(None, 2)\n            # XXX: This is not right, but we are really out of options.\n            # ps does not offer a sane way to decode the argument display,\n            # and this is \"Good Enough\" for obtaining shell names. Hopefully\n            # people don't name their shell with a space, or have something\n            # like \"/usr/bin/xonsh is uber\". (sarugaku/shellingham#14)\n            args = tuple(a.strip() for a in args.split(' '))\n        except ValueError:\n            continue\n        processes[pid] = Process(args=args, pid=pid, ppid=ppid)\n    return processes"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef visit_Name(self, node, store_as_param=False, **kwargs):\n        if store_as_param or node.ctx == 'param':\n            self.symbols.declare_parameter(node.name)\n        elif node.ctx == 'store':\n            self.symbols.store(node.name)\n        elif node.ctx == 'load':\n            self.symbols.load(node.name)", "response": "All assignments to names go through this function."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef visit_Assign(self, node, **kwargs):\n        self.visit(node.node, **kwargs)\n        self.visit(node.target, **kwargs)", "response": "Visit assignments in the correct order."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmoves into a function for testability. Moved into a function for testability.", "response": "def make_set_closure_cell():\n    \"\"\"\n    Moved into a function for testability.\n    \"\"\"\n    if PYPY:  # pragma: no cover\n\n        def set_closure_cell(cell, value):\n            cell.__setstate__((value,))\n\n    else:\n        try:\n            ctypes = import_ctypes()\n\n            set_closure_cell = ctypes.pythonapi.PyCell_Set\n            set_closure_cell.argtypes = (ctypes.py_object, ctypes.py_object)\n            set_closure_cell.restype = ctypes.c_int\n        except Exception:\n            # We try best effort to set the cell, but sometimes it's not\n            # possible.  For example on Jython or on GAE.\n            set_closure_cell = just_warn\n    return set_closure_cell"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef close(self, force=True):\n        '''This closes the connection with the child application. Note that\n        calling close() more than once is valid. This emulates standard Python\n        behavior with files. Set force to True if you want to make sure that\n        the child is terminated (SIGKILL is sent if the child ignores SIGHUP\n        and SIGINT). '''\n\n        self.flush()\n        with _wrap_ptyprocess_err():\n            # PtyProcessError may be raised if it is not possible to terminate\n            # the child.\n            self.ptyproc.close(force=force)\n        self.isalive()  # Update exit status from ptyproc\n        self.child_fd = -1\n        self.closed = True", "response": "This closes the connection with the child application."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading at most size characters from the child application.", "response": "def read_nonblocking(self, size=1, timeout=-1):\n        '''This reads at most size characters from the child application. It\n        includes a timeout. If the read does not complete within the timeout\n        period then a TIMEOUT exception is raised. If the end of file is read\n        then an EOF exception will be raised.  If a logfile is specified, a\n        copy is written to that log.\n\n        If timeout is None then the read may block indefinitely.\n        If timeout is -1 then the self.timeout value is used. If timeout is 0\n        then the child is polled and if there is no data immediately ready\n        then this will raise a TIMEOUT exception.\n\n        The timeout refers only to the amount of time to read at least one\n        character. This is not affected by the 'size' parameter, so if you call\n        read_nonblocking(size=100, timeout=30) and only one character is\n        available right away then one character will be returned immediately.\n        It will not wait for 30 seconds for another 99 characters to come in.\n\n        This is a wrapper around os.read(). It uses select.select() to\n        implement the timeout. '''\n\n        if self.closed:\n            raise ValueError('I/O operation on closed file.')\n\n        if timeout == -1:\n            timeout = self.timeout\n\n        # Note that some systems such as Solaris do not give an EOF when\n        # the child dies. In fact, you can still try to read\n        # from the child_fd -- it will block forever or until TIMEOUT.\n        # For this case, I test isalive() before doing any reading.\n        # If isalive() is false, then I pretend that this is the same as EOF.\n        if not self.isalive():\n            # timeout of 0 means \"poll\"\n            if self.use_poll:\n                r = poll_ignore_interrupts([self.child_fd], timeout)\n            else:\n                r, w, e = select_ignore_interrupts([self.child_fd], [], [], 0)\n            if not r:\n                self.flag_eof = True\n                raise EOF('End Of File (EOF). Braindead platform.')\n        elif self.__irix_hack:\n            # Irix takes a long time before it realizes a child was terminated.\n            # FIXME So does this mean Irix systems are forced to always have\n            # FIXME a 2 second delay when calling read_nonblocking? That sucks.\n            if self.use_poll:\n                r = poll_ignore_interrupts([self.child_fd], timeout)\n            else:\n                r, w, e = select_ignore_interrupts([self.child_fd], [], [], 2)\n            if not r and not self.isalive():\n                self.flag_eof = True\n                raise EOF('End Of File (EOF). Slow platform.')\n        if self.use_poll:\n            r = poll_ignore_interrupts([self.child_fd], timeout)\n        else:\n            r, w, e = select_ignore_interrupts(\n                [self.child_fd], [], [], timeout\n            )\n\n        if not r:\n            if not self.isalive():\n                # Some platforms, such as Irix, will claim that their\n                # processes are alive; timeout on the select; and\n                # then finally admit that they are not alive.\n                self.flag_eof = True\n                raise EOF('End of File (EOF). Very slow platform.')\n            else:\n                raise TIMEOUT('Timeout exceeded.')\n\n        if self.child_fd in r:\n            return super(spawn, self).read_nonblocking(size)\n\n        raise ExceptionPexpect('Reached an unexpected state.')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend a string s to the child process returning the number of bytes written.", "response": "def send(self, s):\n        '''Sends string ``s`` to the child process, returning the number of\n        bytes written. If a logfile is specified, a copy is written to that\n        log.\n\n        The default terminal input mode is canonical processing unless set\n        otherwise by the child process. This allows backspace and other line\n        processing to be performed prior to transmitting to the receiving\n        program. As this is buffered, there is a limited size of such buffer.\n\n        On Linux systems, this is 4096 (defined by N_TTY_BUF_SIZE). All\n        other systems honor the POSIX.1 definition PC_MAX_CANON -- 1024\n        on OSX, 256 on OpenSolaris, and 1920 on FreeBSD.\n\n        This value may be discovered using fpathconf(3)::\n\n            >>> from os import fpathconf\n            >>> print(fpathconf(0, 'PC_MAX_CANON'))\n            256\n\n        On such a system, only 256 bytes may be received per line. Any\n        subsequent bytes received will be discarded. BEL (``'\\a'``) is then\n        sent to output if IMAXBEL (termios.h) is set by the tty driver.\n        This is usually enabled by default.  Linux does not honor this as\n        an option -- it behaves as though it is always set on.\n\n        Canonical input processing may be disabled altogether by executing\n        a shell, then stty(1), before executing the final program::\n\n            >>> bash = pexpect.spawn('/bin/bash', echo=False)\n            >>> bash.sendline('stty -icanon')\n            >>> bash.sendline('base64')\n            >>> bash.sendline('x' * 5000)\n        '''\n\n        if self.delaybeforesend is not None:\n            time.sleep(self.delaybeforesend)\n\n        s = self._coerce_send_string(s)\n        self._log(s, 'send')\n\n        b = self._encoder.encode(s, final=False)\n        return os.write(self.child_fd, b)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrap send to child process with with automatically appended. Returns number of bytes written.", "response": "def sendline(self, s=''):\n        '''Wraps send(), sending string ``s`` to child process, with\n        ``os.linesep`` automatically appended. Returns number of bytes\n        written.  Only a limited number of bytes may be sent for each\n        line in the default terminal mode, see docstring of :meth:`send`.\n        '''\n        s = self._coerce_send_string(s)\n        return self.send(s + self.linesep)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _log_control(self, s):\n        if self.encoding is not None:\n            s = s.decode(self.encoding, 'replace')\n        self._log(s, 'send')", "response": "Write control characters to the appropriate log files"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sendcontrol(self, char):\n        '''Helper method that wraps send() with mnemonic access for sending control\n        character to the child (such as Ctrl-C or Ctrl-D).  For example, to send\n        Ctrl-G (ASCII 7, bell, '\\a')::\n\n            child.sendcontrol('g')\n\n        See also, sendintr() and sendeof().\n        '''\n        n, byte = self.ptyproc.sendcontrol(char)\n        self._log_control(byte)\n        return n", "response": "This method wraps send with mnemonic access for sending control characters to the child."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sendintr(self):\n        '''This sends a SIGINT to the child. It does not require\n        the SIGINT to be the first character on a line. '''\n\n        n, byte = self.ptyproc.sendintr()\n        self._log_control(byte)", "response": "This method sends a SIGINT to the child. It does not require that the SIGINT is the first character on a line."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef interact(self, escape_character=chr(29),\n            input_filter=None, output_filter=None):\n\n        '''This gives control of the child process to the interactive user (the\n        human at the keyboard). Keystrokes are sent to the child process, and\n        the stdout and stderr output of the child process is printed. This\n        simply echos the child stdout and child stderr to the real stdout and\n        it echos the real stdin to the child stdin. When the user types the\n        escape_character this method will return None. The escape_character\n        will not be transmitted.  The default for escape_character is\n        entered as ``Ctrl - ]``, the very same as BSD telnet. To prevent\n        escaping, escape_character may be set to None.\n\n        If a logfile is specified, then the data sent and received from the\n        child process in interact mode is duplicated to the given log.\n\n        You may pass in optional input and output filter functions. These\n        functions should take a string and return a string. The output_filter\n        will be passed all the output from the child process. The input_filter\n        will be passed all the keyboard input from the user. The input_filter\n        is run BEFORE the check for the escape_character.\n\n        Note that if you change the window size of the parent the SIGWINCH\n        signal will not be passed through to the child. If you want the child\n        window size to change when the parent's window size changes then do\n        something like the following example::\n\n            import pexpect, struct, fcntl, termios, signal, sys\n            def sigwinch_passthrough (sig, data):\n                s = struct.pack(\"HHHH\", 0, 0, 0, 0)\n                a = struct.unpack('hhhh', fcntl.ioctl(sys.stdout.fileno(),\n                    termios.TIOCGWINSZ , s))\n                if not p.closed:\n                    p.setwinsize(a[0],a[1])\n\n            # Note this 'p' is global and used in sigwinch_passthrough.\n            p = pexpect.spawn('/bin/bash')\n            signal.signal(signal.SIGWINCH, sigwinch_passthrough)\n            p.interact()\n        '''\n\n        # Flush the buffer.\n        self.write_to_stdout(self.buffer)\n        self.stdout.flush()\n        self._buffer = self.buffer_type()\n        mode = tty.tcgetattr(self.STDIN_FILENO)\n        tty.setraw(self.STDIN_FILENO)\n        if escape_character is not None and PY3:\n            escape_character = escape_character.encode('latin-1')\n        try:\n            self.__interact_copy(escape_character, input_filter, output_filter)\n        finally:\n            tty.tcsetattr(self.STDIN_FILENO, tty.TCSAFLUSH, mode)", "response": "This method is used to interact with the child process."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __interact_writen(self, fd, data):\n        '''This is used by the interact() method.\n        '''\n\n        while data != b'' and self.isalive():\n            n = os.write(fd, data)\n            data = data[n:]", "response": "This is used by the interact method."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nturns a list of extras into a string", "response": "def extras_to_string(extras):\n    # type: (Iterable[S]) -> S\n    \"\"\"Turn a list of extras into a string\"\"\"\n    if isinstance(extras, six.string_types):\n        if extras.startswith(\"[\"):\n            return extras\n        else:\n            extras = [extras]\n    if not extras:\n        return \"\"\n    return \"[{0}]\".format(\",\".join(sorted(set(extras))))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_extras(extras_str):\n    # type: (AnyStr) -> List[AnyStr]\n    \"\"\"\n    Turn a string of extras into a parsed extras list\n    \"\"\"\n\n    from pkg_resources import Requirement\n\n    extras = Requirement.parse(\"fakepkg{0}\".format(extras_to_string(extras_str))).extras\n    return sorted(dedup([extra.lower() for extra in extras]))", "response": "Turn a string of extras into a parsed extras list"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef specs_to_string(specs):\n    # type: (List[Union[STRING_TYPE, Specifier]]) -> AnyStr\n    \"\"\"\n    Turn a list of specifier tuples into a string\n    \"\"\"\n\n    if specs:\n        if isinstance(specs, six.string_types):\n            return specs\n        try:\n            extras = \",\".join([\"\".join(spec) for spec in specs])\n        except TypeError:\n            extras = \",\".join([\"\".join(spec._spec) for spec in specs])  # type: ignore\n        return extras\n    return \"\"", "response": "Converts a list of specifier tuples into a string."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a direct URL to a URL that is compatible with pip - 508.", "response": "def convert_direct_url_to_url(direct_url):\n    # type: (AnyStr) -> AnyStr\n    \"\"\"\n    Given a direct url as defined by *PEP 508*, convert to a :class:`~pip_shims.shims.Link`\n    compatible URL by moving the name and extras into an **egg_fragment**.\n\n    :param str direct_url: A pep-508 compliant direct url.\n    :return: A reformatted URL for use with Link objects and :class:`~pip_shims.shims.InstallRequirement` objects.\n    :rtype: AnyStr\n    \"\"\"\n    direct_match = DIRECT_URL_RE.match(direct_url)  # type: Optional[Match]\n    if direct_match is None:\n        url_match = URL_RE.match(direct_url)\n        if url_match or is_valid_url(direct_url):\n            return direct_url\n    match_dict = (\n        {}\n    )  # type: Dict[STRING_TYPE, Union[Tuple[STRING_TYPE, ...], STRING_TYPE]]\n    if direct_match is not None:\n        match_dict = direct_match.groupdict()  # type: ignore\n    if not match_dict:\n        raise ValueError(\n            \"Failed converting value to normal URL, is it a direct URL? {0!r}\".format(\n                direct_url\n            )\n        )\n    url_segments = [match_dict.get(s) for s in (\"scheme\", \"host\", \"path\", \"pathsep\")]\n    url = \"\"  # type: STRING_TYPE\n    url = \"\".join([s for s in url_segments if s is not None])  # type: ignore\n    new_url = build_vcs_uri(\n        None,\n        url,\n        ref=match_dict.get(\"ref\"),\n        name=match_dict.get(\"name\"),\n        extras=match_dict.get(\"extras\"),\n        subdirectory=match_dict.get(\"subdirectory\"),\n    )\n    return new_url"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef convert_url_to_direct_url(url, name=None):\n    # type: (AnyStr, Optional[AnyStr]) -> AnyStr\n    \"\"\"\n    Given a :class:`~pip_shims.shims.Link` compatible URL, convert to a direct url as\n    defined by *PEP 508* by extracting the name and extras from the **egg_fragment**.\n\n    :param AnyStr url: A :class:`~pip_shims.shims.InstallRequirement` compliant URL.\n    :param Optiona[AnyStr] name: A name to use in case the supplied URL doesn't provide one.\n    :return: A pep-508 compliant direct url.\n    :rtype: AnyStr\n\n    :raises ValueError: Raised when the URL can't be parsed or a name can't be found.\n    :raises TypeError: When a non-string input is provided.\n    \"\"\"\n    if not isinstance(url, six.string_types):\n        raise TypeError(\n            \"Expected a string to convert to a direct url, got {0!r}\".format(url)\n        )\n    direct_match = DIRECT_URL_RE.match(url)\n    if direct_match:\n        return url\n    url_match = URL_RE.match(url)\n    if url_match is None or not url_match.groupdict():\n        raise ValueError(\"Failed parse a valid URL from {0!r}\".format(url))\n    match_dict = url_match.groupdict()\n    url_segments = [match_dict.get(s) for s in (\"scheme\", \"host\", \"path\", \"pathsep\")]\n    name = match_dict.get(\"name\", name)\n    extras = match_dict.get(\"extras\")\n    new_url = \"\"\n    if extras and not name:\n        url_segments.append(extras)\n    elif extras and name:\n        new_url = \"{0}{1}@ \".format(name, extras)\n    else:\n        if name is not None:\n            new_url = \"{0}@ \".format(name)\n        else:\n            raise ValueError(\n                \"Failed to construct direct url: \"\n                \"No name could be parsed from {0!r}\".format(url)\n            )\n    if match_dict.get(\"ref\"):\n        url_segments.append(\"@{0}\".format(match_dict.get(\"ref\")))\n    url = \"\".join([s for s in url if s is not None])\n    url = \"{0}{1}\".format(new_url, url)\n    return url", "response": "Converts a URL to a direct url."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef strip_extras_markers_from_requirement(req):\n    # type: (TRequirement) -> TRequirement\n    \"\"\"\n    Given a :class:`~packaging.requirements.Requirement` instance with markers defining\n    *extra == 'name'*, strip out the extras from the markers and return the cleaned\n    requirement\n\n    :param PackagingRequirement req: A packaging requirement to clean\n    :return: A cleaned requirement\n    :rtype: PackagingRequirement\n    \"\"\"\n    if req is None:\n        raise TypeError(\"Must pass in a valid requirement, received {0!r}\".format(req))\n    if getattr(req, \"marker\", None) is not None:\n        marker = req.marker  # type: TMarker\n        marker._markers = _strip_extras_markers(marker._markers)\n        if not marker._markers:\n            req.marker = None\n        else:\n            req.marker = marker\n    return req", "response": "Given a PackagingRequirement object with markers defining\n    extra == name * and extra == name * return the cleaned\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_pyproject(path):\n    # type: (Union[STRING_TYPE, Path]) -> Optional[Tuple[List[STRING_TYPE], STRING_TYPE]]\n    \"\"\"\n    Given a base path, look for the corresponding ``pyproject.toml`` file and return its\n    build_requires and build_backend.\n\n    :param AnyStr path: The root path of the project, should be a directory (will be truncated)\n    :return: A 2 tuple of build requirements and the build backend\n    :rtype: Optional[Tuple[List[AnyStr], AnyStr]]\n    \"\"\"\n\n    if not path:\n        return\n    from vistir.compat import Path\n\n    if not isinstance(path, Path):\n        path = Path(path)\n    if not path.is_dir():\n        path = path.parent\n    pp_toml = path.joinpath(\"pyproject.toml\")\n    setup_py = path.joinpath(\"setup.py\")\n    if not pp_toml.exists():\n        if not setup_py.exists():\n            return None\n        requires = [\"setuptools>=40.8\", \"wheel\"]\n        backend = get_default_pyproject_backend()\n    else:\n        pyproject_data = {}\n        with io.open(pp_toml.as_posix(), encoding=\"utf-8\") as fh:\n            pyproject_data = tomlkit.loads(fh.read())\n        build_system = pyproject_data.get(\"build-system\", None)\n        if build_system is None:\n            if setup_py.exists():\n                requires = [\"setuptools>=40.8\", \"wheel\"]\n                backend = get_default_pyproject_backend()\n            else:\n                requires = [\"setuptools>=40.8\", \"wheel\"]\n                backend = get_default_pyproject_backend()\n            build_system = {\"requires\": requires, \"build-backend\": backend}\n            pyproject_data[\"build_system\"] = build_system\n        else:\n            requires = build_system.get(\"requires\", [\"setuptools>=40.8\", \"wheel\"])\n            backend = build_system.get(\"build-backend\", get_default_pyproject_backend())\n    return requires, backend", "response": "Returns the base path of the pyproject. toml file and the corresponding build requirements and build backend."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef split_markers_from_line(line):\n    # type: (AnyStr) -> Tuple[AnyStr, Optional[AnyStr]]\n    \"\"\"Split markers from a dependency\"\"\"\n    if not any(line.startswith(uri_prefix) for uri_prefix in SCHEME_LIST):\n        marker_sep = \";\"\n    else:\n        marker_sep = \"; \"\n    markers = None\n    if marker_sep in line:\n        line, markers = line.split(marker_sep, 1)\n        markers = markers.strip() if markers else None\n    return line, markers", "response": "Split markers from a dependency line"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef split_vcs_method_from_uri(uri):\n    # type: (AnyStr) -> Tuple[Optional[STRING_TYPE], STRING_TYPE]\n    \"\"\"Split a vcs+uri formatted uri into (vcs, uri)\"\"\"\n    vcs_start = \"{0}+\"\n    vcs = None  # type: Optional[STRING_TYPE]\n    vcs = first([vcs for vcs in VCS_LIST if uri.startswith(vcs_start.format(vcs))])\n    if vcs:\n        vcs, uri = uri.split(\"+\", 1)\n    return vcs, uri", "response": "Split a vcs + uri formatted uri into a vcs + uri formatted uri."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef split_ref_from_uri(uri):\n    # type: (AnyStr) -> Tuple[AnyStr, Optional[AnyStr]]\n    \"\"\"\n    Given a path or URI, check for a ref and split it from the path if it is present,\n    returning a tuple of the original input and the ref or None.\n\n    :param AnyStr uri: The path or URI to split\n    :returns: A 2-tuple of the path or URI and the ref\n    :rtype: Tuple[AnyStr, Optional[AnyStr]]\n    \"\"\"\n    if not isinstance(uri, six.string_types):\n        raise TypeError(\"Expected a string, received {0!r}\".format(uri))\n    parsed = urllib_parse.urlparse(uri)\n    path = parsed.path\n    ref = None\n    if \"@\" in path:\n        path, _, ref = path.rpartition(\"@\")\n    parsed = parsed._replace(path=path)\n    return (urllib_parse.urlunparse(parsed), ref)", "response": "Splits a ref from a path or URI into a path or URI and returns a tuple of the original input and the ref or None."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef key_from_ireq(ireq):\n    if ireq.req is None and ireq.link is not None:\n        return str(ireq.link)\n    else:\n        return key_from_req(ireq.req)", "response": "Get a standardized key for an InstallRequirement."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef key_from_req(req):\n    if hasattr(req, \"key\"):\n        # from pkg_resources, such as installed dists for pip-sync\n        key = req.key\n    else:\n        # from packaging, such as install requirements from requirements.txt\n        key = req.name\n\n    key = key.replace(\"_\", \"-\").lower()\n    return key", "response": "Get an all - lowercase version of the requirement s name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _requirement_to_str_lowercase_name(requirement):\n\n    parts = [requirement.name.lower()]\n\n    if requirement.extras:\n        parts.append(\"[{0}]\".format(\",\".join(sorted(requirement.extras))))\n\n    if requirement.specifier:\n        parts.append(str(requirement.specifier))\n\n    if requirement.url:\n        parts.append(\"@ {0}\".format(requirement.url))\n\n    if requirement.marker:\n        parts.append(\"; {0}\".format(requirement.marker))\n\n    return \"\".join(parts)", "response": "Converts a packaging. requirements. Requirement with a lowercase name."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef format_requirement(ireq):\n\n    if ireq.editable:\n        line = \"-e {}\".format(ireq.link)\n    else:\n        line = _requirement_to_str_lowercase_name(ireq.req)\n\n    if str(ireq.req.marker) != str(ireq.markers):\n        if not ireq.req.marker:\n            line = \"{}; {}\".format(line, ireq.markers)\n        else:\n            name, markers = line.split(\";\", 1)\n            markers = markers.strip()\n            line = \"{}; ({}) and ({})\".format(name, markers, ireq.markers)\n\n    return line", "response": "Formats a InstallRequirements object to a pretty printing line."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef format_specifier(ireq):\n    # TODO: Ideally, this is carried over to the pip library itself\n    specs = ireq.specifier._specs if ireq.req is not None else []\n    specs = sorted(specs, key=lambda x: x._spec[1])\n    return \",\".join(str(s) for s in specs) or \"<any>\"", "response": "Generic formatter for pretty printing the specifier part of a node - pip installation."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert an InstallRequirement into a tuple.", "response": "def as_tuple(ireq):\n    \"\"\"\n    Pulls out the (name: str, version:str, extras:(str)) tuple from the pinned InstallRequirement.\n    \"\"\"\n\n    if not is_pinned_requirement(ireq):\n        raise TypeError(\"Expected a pinned InstallRequirement, got {}\".format(ireq))\n\n    name = key_from_req(ireq.req)\n    version = first(ireq.specifier._specs)._spec[1]\n    extras = tuple(sorted(ireq.extras))\n    return name, version, extras"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef full_groupby(iterable, key=None):\n\n    return groupby(sorted(iterable, key=key), key=key)", "response": "Like groupby but sorts the input on the group key first."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef lookup_table(values, key=None, keyval=None, unique=False, use_lists=False):\n\n    if keyval is None:\n        if key is None:\n            keyval = lambda v: v\n        else:\n            keyval = lambda v: (key(v), v)\n\n    if unique:\n        return dict(keyval(v) for v in values)\n\n    lut = {}\n    for value in values:\n        k, v = keyval(value)\n        try:\n            s = lut[k]\n        except KeyError:\n            if use_lists:\n                s = lut[k] = list()\n            else:\n                s = lut[k] = set()\n        if use_lists:\n            s.append(v)\n        else:\n            s.add(v)\n    return dict(lut)", "response": "Builds a dict - based lookup table for the given list of values."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate an InstallRequirement object from the supplied metadata.", "response": "def make_install_requirement(name, version, extras, markers, constraint=False):\n    \"\"\"\n    Generates an :class:`~pip._internal.req.req_install.InstallRequirement`.\n\n    Create an InstallRequirement from the supplied metadata.\n\n    :param name: The requirement's name.\n    :type name: str\n    :param version: The requirement version (must be pinned).\n    :type version: str.\n    :param extras: The desired extras.\n    :type extras: list[str]\n    :param markers: The desired markers, without a preceding semicolon.\n    :type markers: str\n    :param constraint: Whether to flag the requirement as a constraint, defaults to False.\n    :param constraint: bool, optional\n    :return: A generated InstallRequirement\n    :rtype: :class:`~pip._internal.req.req_install.InstallRequirement`\n    \"\"\"\n\n    # If no extras are specified, the extras string is blank\n    from pip_shims.shims import install_req_from_line\n\n    extras_string = \"\"\n    if extras:\n        # Sort extras for stability\n        extras_string = \"[{}]\".format(\",\".join(sorted(extras)))\n\n    if not markers:\n        return install_req_from_line(\n            str(\"{}{}=={}\".format(name, extras_string, version)), constraint=constraint\n        )\n    else:\n        return install_req_from_line(\n            str(\"{}{}=={}; {}\".format(name, extras_string, version, str(markers))),\n            constraint=constraint,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a cleaned list of all the candidates with valid specifiers in the requires_python attributes.", "response": "def clean_requires_python(candidates):\n    \"\"\"Get a cleaned list of all the candidates with valid specifiers in the `requires_python` attributes.\"\"\"\n    all_candidates = []\n    sys_version = \".\".join(map(str, sys.version_info[:3]))\n    from packaging.version import parse as parse_version\n\n    py_version = parse_version(os.environ.get(\"PIP_PYTHON_VERSION\", sys_version))\n    for c in candidates:\n        from_location = attrgetter(\"location.requires_python\")\n        requires_python = getattr(c, \"requires_python\", from_location(c))\n        if requires_python:\n            # Old specifications had people setting this to single digits\n            # which is effectively the same as '>=digit,<digit+1'\n            if requires_python.isdigit():\n                requires_python = \">={0},<{1}\".format(\n                    requires_python, int(requires_python) + 1\n                )\n            try:\n                specifierset = SpecifierSet(requires_python)\n            except InvalidSpecifier:\n                continue\n            else:\n                if not specifierset.contains(py_version):\n                    continue\n        all_candidates.append(c)\n    return all_candidates"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_name_variants(pkg):\n    # type: (STRING_TYPE) -> Set[STRING_TYPE]\n    \"\"\"\n    Given a packager name, get the variants of its name for both the canonicalized\n    and \"safe\" forms.\n\n    :param AnyStr pkg: The package to lookup\n    :returns: A list of names.\n    :rtype: Set\n    \"\"\"\n\n    if not isinstance(pkg, six.string_types):\n        raise TypeError(\"must provide a string to derive package names\")\n    from pkg_resources import safe_name\n    from packaging.utils import canonicalize_name\n\n    pkg = pkg.lower()\n    names = {safe_name(pkg), canonicalize_name(pkg), pkg.replace(\"-\", \"_\")}\n    return names", "response": "Given a packager name get the variants of its name for both the canonicalized\n    and safe forms."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndetect the best version depending on the fields used.", "response": "def _best_version(fields):\n    \"\"\"Detect the best version depending on the fields used.\"\"\"\n    def _has_marker(keys, markers):\n        for marker in markers:\n            if marker in keys:\n                return True\n        return False\n\n    keys = []\n    for key, value in fields.items():\n        if value in ([], 'UNKNOWN', None):\n            continue\n        keys.append(key)\n\n    possible_versions = ['1.0', '1.1', '1.2', '1.3', '2.0', '2.1']\n\n    # first let's try to see if a field is not part of one of the version\n    for key in keys:\n        if key not in _241_FIELDS and '1.0' in possible_versions:\n            possible_versions.remove('1.0')\n            logger.debug('Removed 1.0 due to %s', key)\n        if key not in _314_FIELDS and '1.1' in possible_versions:\n            possible_versions.remove('1.1')\n            logger.debug('Removed 1.1 due to %s', key)\n        if key not in _345_FIELDS and '1.2' in possible_versions:\n            possible_versions.remove('1.2')\n            logger.debug('Removed 1.2 due to %s', key)\n        if key not in _566_FIELDS and '1.3' in possible_versions:\n            possible_versions.remove('1.3')\n            logger.debug('Removed 1.3 due to %s', key)\n        if key not in _566_FIELDS and '2.1' in possible_versions:\n            if key != 'Description':  # In 2.1, description allowed after headers\n                possible_versions.remove('2.1')\n                logger.debug('Removed 2.1 due to %s', key)\n        if key not in _426_FIELDS and '2.0' in possible_versions:\n            possible_versions.remove('2.0')\n            logger.debug('Removed 2.0 due to %s', key)\n\n    # possible_version contains qualified versions\n    if len(possible_versions) == 1:\n        return possible_versions[0]   # found !\n    elif len(possible_versions) == 0:\n        logger.debug('Out of options - unknown metadata set: %s', fields)\n        raise MetadataConflictError('Unknown metadata set')\n\n    # let's see if one unique marker is found\n    is_1_1 = '1.1' in possible_versions and _has_marker(keys, _314_MARKERS)\n    is_1_2 = '1.2' in possible_versions and _has_marker(keys, _345_MARKERS)\n    is_2_1 = '2.1' in possible_versions and _has_marker(keys, _566_MARKERS)\n    is_2_0 = '2.0' in possible_versions and _has_marker(keys, _426_MARKERS)\n    if int(is_1_1) + int(is_1_2) + int(is_2_1) + int(is_2_0) > 1:\n        raise MetadataConflictError('You used incompatible 1.1/1.2/2.0/2.1 fields')\n\n    # we have the choice, 1.0, or 1.2, or 2.0\n    #   - 1.0 has a broken Summary field but works with all tools\n    #   - 1.1 is to avoid\n    #   - 1.2 fixes Summary but has little adoption\n    #   - 2.0 adds more features and is very new\n    if not is_1_1 and not is_1_2 and not is_2_1 and not is_2_0:\n        # we couldn't find any specific marker\n        if PKG_INFO_PREFERRED_VERSION in possible_versions:\n            return PKG_INFO_PREFERRED_VERSION\n    if is_1_1:\n        return '1.1'\n    if is_1_2:\n        return '1.2'\n    if is_2_1:\n        return '2.1'\n\n    return '2.0'"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_name_and_version(name, version, for_filename=False):\n    if for_filename:\n        # For both name and version any runs of non-alphanumeric or '.'\n        # characters are replaced with a single '-'.  Additionally any\n        # spaces in the version string become '.'\n        name = _FILESAFE.sub('-', name)\n        version = _FILESAFE.sub('-', version.replace(' ', '.'))\n    return '%s-%s' % (name, version)", "response": "Return the distribution name with version."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads the metadata values from a file path.", "response": "def read(self, filepath):\n        \"\"\"Read the metadata values from a file path.\"\"\"\n        fp = codecs.open(filepath, 'r', encoding='utf-8')\n        try:\n            self.read_file(fp)\n        finally:\n            fp.close()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write(self, filepath, skip_unknown=False):\n        fp = codecs.open(filepath, 'w', encoding='utf-8')\n        try:\n            self.write_file(fp, skip_unknown)\n        finally:\n            fp.close()", "response": "Write the metadata fields to filepath."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write_file(self, fileobject, skip_unknown=False):\n        self.set_metadata_version()\n\n        for field in _version2fieldlist(self['Metadata-Version']):\n            values = self.get(field)\n            if skip_unknown and values in ('UNKNOWN', [], ['UNKNOWN']):\n                continue\n            if field in _ELEMENTSFIELD:\n                self._write_field(fileobject, field, ','.join(values))\n                continue\n            if field not in _LISTFIELDS:\n                if field == 'Description':\n                    if self.metadata_version in ('1.0', '1.1'):\n                        values = values.replace('\\n', '\\n        ')\n                    else:\n                        values = values.replace('\\n', '\\n       |')\n                values = [values]\n\n            if field in _LISTTUPLEFIELDS:\n                values = [','.join(value) for value in values]\n\n            for value in values:\n                self._write_field(fileobject, field, value)", "response": "Write the PKG - INFO format data to a file object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets metadata values from the given iterable other and kwargs.", "response": "def update(self, other=None, **kwargs):\n        \"\"\"Set metadata values from the given iterable `other` and kwargs.\n\n        Behavior is like `dict.update`: If `other` has a ``keys`` method,\n        they are looped over and ``self[key]`` is assigned ``other[key]``.\n        Else, ``other`` is an iterable of ``(key, value)`` iterables.\n\n        Keys that don't match a metadata field or that have an empty value are\n        dropped.\n        \"\"\"\n        def _set(key, value):\n            if key in _ATTR2FIELD and value:\n                self.set(self._convert_name(key), value)\n\n        if not other:\n            # other is None or empty container\n            pass\n        elif hasattr(other, 'keys'):\n            for k in other.keys():\n                _set(k, other[k])\n        else:\n            for k, v in other:\n                _set(k, v)\n\n        if kwargs:\n            for k, v in kwargs.items():\n                _set(k, v)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set(self, name, value):\n        name = self._convert_name(name)\n\n        if ((name in _ELEMENTSFIELD or name == 'Platform') and\n            not isinstance(value, (list, tuple))):\n            if isinstance(value, string_types):\n                value = [v.strip() for v in value.split(',')]\n            else:\n                value = []\n        elif (name in _LISTFIELDS and\n              not isinstance(value, (list, tuple))):\n            if isinstance(value, string_types):\n                value = [value]\n            else:\n                value = []\n\n        if logger.isEnabledFor(logging.WARNING):\n            project_name = self['Name']\n\n            scheme = get_scheme(self.scheme)\n            if name in _PREDICATE_FIELDS and value is not None:\n                for v in value:\n                    # check that the values are valid\n                    if not scheme.is_valid_matcher(v.split(';')[0]):\n                        logger.warning(\n                            \"'%s': '%s' is not valid (field '%s')\",\n                            project_name, v, name)\n            # FIXME this rejects UNKNOWN, is that right?\n            elif name in _VERSIONS_FIELDS and value is not None:\n                if not scheme.is_valid_constraint_list(value):\n                    logger.warning(\"'%s': '%s' is not a valid version (field '%s')\",\n                                   project_name, value, name)\n            elif name in _VERSION_FIELDS and value is not None:\n                if not scheme.is_valid_version(value):\n                    logger.warning(\"'%s': '%s' is not a valid version (field '%s')\",\n                                   project_name, value, name)\n\n        if name in _UNICODEFIELDS:\n            if name == 'Description':\n                value = self._remove_line_prefix(value)\n\n        self._fields[name] = value", "response": "Set a metadata field."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a metadata field.", "response": "def get(self, name, default=_MISSING):\n        \"\"\"Get a metadata field.\"\"\"\n        name = self._convert_name(name)\n        if name not in self._fields:\n            if default is _MISSING:\n                default = self._default_value(name)\n            return default\n        if name in _UNICODEFIELDS:\n            value = self._fields[name]\n            return value\n        elif name in _LISTFIELDS:\n            value = self._fields[name]\n            if value is None:\n                return []\n            res = []\n            for val in value:\n                if name not in _LISTTUPLEFIELDS:\n                    res.append(val)\n                else:\n                    # That's for Project-URL\n                    res.append((val[0], val[1]))\n            return res\n\n        elif name in _ELEMENTSFIELD:\n            value = self._fields[name]\n            if isinstance(value, string_types):\n                return value.split(',')\n        return self._fields[name]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef todict(self, skip_missing=False):\n        self.set_metadata_version()\n\n        mapping_1_0 = (\n            ('metadata_version', 'Metadata-Version'),\n            ('name', 'Name'),\n            ('version', 'Version'),\n            ('summary', 'Summary'),\n            ('home_page', 'Home-page'),\n            ('author', 'Author'),\n            ('author_email', 'Author-email'),\n            ('license', 'License'),\n            ('description', 'Description'),\n            ('keywords', 'Keywords'),\n            ('platform', 'Platform'),\n            ('classifiers', 'Classifier'),\n            ('download_url', 'Download-URL'),\n        )\n\n        data = {}\n        for key, field_name in mapping_1_0:\n            if not skip_missing or field_name in self._fields:\n                data[key] = self[field_name]\n\n        if self['Metadata-Version'] == '1.2':\n            mapping_1_2 = (\n                ('requires_dist', 'Requires-Dist'),\n                ('requires_python', 'Requires-Python'),\n                ('requires_external', 'Requires-External'),\n                ('provides_dist', 'Provides-Dist'),\n                ('obsoletes_dist', 'Obsoletes-Dist'),\n                ('project_url', 'Project-URL'),\n                ('maintainer', 'Maintainer'),\n                ('maintainer_email', 'Maintainer-email'),\n            )\n            for key, field_name in mapping_1_2:\n                if not skip_missing or field_name in self._fields:\n                    if key != 'project_url':\n                        data[key] = self[field_name]\n                    else:\n                        data[key] = [','.join(u) for u in self[field_name]]\n\n        elif self['Metadata-Version'] == '1.1':\n            mapping_1_1 = (\n                ('provides', 'Provides'),\n                ('requires', 'Requires'),\n                ('obsoletes', 'Obsoletes'),\n            )\n            for key, field_name in mapping_1_1:\n                if not skip_missing or field_name in self._fields:\n                    data[key] = self[field_name]\n\n        return data", "response": "Return the fields as a dict."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbasing method to get dependencies given a set of sometimes - wanted components and environment context.", "response": "def get_requirements(self, reqts, extras=None, env=None):\n        \"\"\"\n        Base method to get dependencies, given a set of extras\n        to satisfy and an optional environment context.\n        :param reqts: A list of sometimes-wanted dependencies,\n                      perhaps dependent on extras and environment.\n        :param extras: A list of optional components being requested.\n        :param env: An optional environment for marker evaluation.\n        \"\"\"\n        if self._legacy:\n            result = reqts\n        else:\n            result = []\n            extras = get_extras(extras or [], self.extras)\n            for d in reqts:\n                if 'extra' not in d and 'environment' not in d:\n                    # unconditional\n                    include = True\n                else:\n                    if 'extra' not in d:\n                        # Not extra-dependent - only environment-dependent\n                        include = True\n                    else:\n                        include = d.get('extra') in extras\n                    if include:\n                        # Not excluded because of extras, check environment\n                        marker = d.get('environment')\n                        if marker:\n                            include = interpret(marker, env)\n                if include:\n                    result.extend(d['requires'])\n            for key in ('build', 'dev', 'test'):\n                e = ':%s:' % key\n                if e in extras:\n                    extras.remove(e)\n                    # A recursive call, but it should terminate since 'test'\n                    # has been removed from the extras\n                    reqts = self._data.get('%s_requires' % key, [])\n                    result.extend(self.get_requirements(reqts, extras=extras,\n                                                        env=env))\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove_move(name):\n    try:\n        delattr(_MovedItems, name)\n    except AttributeError:\n        try:\n            del moves.__dict__[name]\n        except KeyError:\n            raise AttributeError(\"no such move, %r\" % (name,))", "response": "Remove item from six. moves."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ensure_binary(s, encoding='utf-8', errors='strict'):\n    if isinstance(s, text_type):\n        return s.encode(encoding, errors)\n    elif isinstance(s, binary_type):\n        return s\n    else:\n        raise TypeError(\"not expecting type '%s'\" % type(s))", "response": "Coerce **s** to six. binary_type.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncoercing *s* to str.", "response": "def ensure_str(s, encoding='utf-8', errors='strict'):\n    \"\"\"Coerce *s* to `str`.\n\n    For Python 2:\n      - `unicode` -> encoded to `str`\n      - `str` -> `str`\n\n    For Python 3:\n      - `str` -> `str`\n      - `bytes` -> decoded to `str`\n    \"\"\"\n    if not isinstance(s, (text_type, binary_type)):\n        raise TypeError(\"not expecting type '%s'\" % type(s))\n    if PY2 and isinstance(s, text_type):\n        s = s.encode(encoding, errors)\n    elif PY3 and isinstance(s, binary_type):\n        s = s.decode(encoding, errors)\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ensure_text(s, encoding='utf-8', errors='strict'):\n    if isinstance(s, binary_type):\n        return s.decode(encoding, errors)\n    elif isinstance(s, text_type):\n        return s\n    else:\n        raise TypeError(\"not expecting type '%s'\" % type(s))", "response": "Coerce *s* to text_type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_requirements(\n    filename,  # type: str\n    finder=None,  # type: Optional[PackageFinder]\n    comes_from=None,  # type: Optional[str]\n    options=None,  # type: Optional[optparse.Values]\n    session=None,  # type: Optional[PipSession]\n    constraint=False,  # type: bool\n    wheel_cache=None,  # type: Optional[WheelCache]\n    use_pep517=None  # type: Optional[bool]\n):\n    # type: (...) -> Iterator[InstallRequirement]\n    \"\"\"Parse a requirements file and yield InstallRequirement instances.\n\n    :param filename:    Path or url of requirements file.\n    :param finder:      Instance of pip.index.PackageFinder.\n    :param comes_from:  Origin description of requirements.\n    :param options:     cli options.\n    :param session:     Instance of pip.download.PipSession.\n    :param constraint:  If true, parsing a constraint file rather than\n        requirements file.\n    :param wheel_cache: Instance of pip.wheel.WheelCache\n    :param use_pep517:  Value of the --use-pep517 option.\n    \"\"\"\n    if session is None:\n        raise TypeError(\n            \"parse_requirements() missing 1 required keyword argument: \"\n            \"'session'\"\n        )\n\n    _, content = get_file_content(\n        filename, comes_from=comes_from, session=session\n    )\n\n    lines_enum = preprocess(content, options)\n\n    for line_number, line in lines_enum:\n        req_iter = process_line(line, filename, line_number, finder,\n                                comes_from, options, session, wheel_cache,\n                                use_pep517=use_pep517, constraint=constraint)\n        for req in req_iter:\n            yield req", "response": "Parse a requirements file and yield InstallRequirement instances."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsplitting filter and join lines and return a line iterator", "response": "def preprocess(content, options):\n    # type: (Text, Optional[optparse.Values]) -> ReqFileLines\n    \"\"\"Split, filter, and join lines, and return a line iterator\n\n    :param content: the content of the requirements file\n    :param options: cli options\n    \"\"\"\n    lines_enum = enumerate(content.splitlines(), start=1)  # type: ReqFileLines\n    lines_enum = join_lines(lines_enum)\n    lines_enum = ignore_comments(lines_enum)\n    lines_enum = skip_regex(lines_enum, options)\n    lines_enum = expand_env_variables(lines_enum)\n    return lines_enum"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprocesses a single line of a single - line set of requirements.", "response": "def process_line(\n    line,  # type: Text\n    filename,  # type: str\n    line_number,  # type: int\n    finder=None,  # type: Optional[PackageFinder]\n    comes_from=None,  # type: Optional[str]\n    options=None,  # type: Optional[optparse.Values]\n    session=None,  # type: Optional[PipSession]\n    wheel_cache=None,  # type: Optional[WheelCache]\n    use_pep517=None,  # type: Optional[bool]\n    constraint=False  # type: bool\n):\n    # type: (...) -> Iterator[InstallRequirement]\n    \"\"\"Process a single requirements line; This can result in creating/yielding\n    requirements, or updating the finder.\n\n    For lines that contain requirements, the only options that have an effect\n    are from SUPPORTED_OPTIONS_REQ, and they are scoped to the\n    requirement. Other options from SUPPORTED_OPTIONS may be present, but are\n    ignored.\n\n    For lines that do not contain requirements, the only options that have an\n    effect are from SUPPORTED_OPTIONS. Options from SUPPORTED_OPTIONS_REQ may\n    be present, but are ignored. These lines may contain multiple options\n    (although our docs imply only one is supported), and all our parsed and\n    affect the finder.\n\n    :param constraint: If True, parsing a constraints file.\n    :param options: OptionParser options that we may update\n    \"\"\"\n    parser = build_parser(line)\n    defaults = parser.get_default_values()\n    defaults.index_url = None\n    if finder:\n        defaults.format_control = finder.format_control\n    args_str, options_str = break_args_options(line)\n    # Prior to 2.7.3, shlex cannot deal with unicode entries\n    if sys.version_info < (2, 7, 3):\n        # https://github.com/python/mypy/issues/1174\n        options_str = options_str.encode('utf8')  # type: ignore\n    # https://github.com/python/mypy/issues/1174\n    opts, _ = parser.parse_args(\n        shlex.split(options_str), defaults)  # type: ignore\n\n    # preserve for the nested code path\n    line_comes_from = '%s %s (line %s)' % (\n        '-c' if constraint else '-r', filename, line_number,\n    )\n\n    # yield a line requirement\n    if args_str:\n        isolated = options.isolated_mode if options else False\n        if options:\n            cmdoptions.check_install_build_global(options, opts)\n        # get the options that apply to requirements\n        req_options = {}\n        for dest in SUPPORTED_OPTIONS_REQ_DEST:\n            if dest in opts.__dict__ and opts.__dict__[dest]:\n                req_options[dest] = opts.__dict__[dest]\n        yield install_req_from_line(\n            args_str, line_comes_from, constraint=constraint,\n            use_pep517=use_pep517,\n            isolated=isolated, options=req_options, wheel_cache=wheel_cache\n        )\n\n    # yield an editable requirement\n    elif opts.editables:\n        isolated = options.isolated_mode if options else False\n        yield install_req_from_editable(\n            opts.editables[0], comes_from=line_comes_from,\n            use_pep517=use_pep517,\n            constraint=constraint, isolated=isolated, wheel_cache=wheel_cache\n        )\n\n    # parse a nested requirements file\n    elif opts.requirements or opts.constraints:\n        if opts.requirements:\n            req_path = opts.requirements[0]\n            nested_constraint = False\n        else:\n            req_path = opts.constraints[0]\n            nested_constraint = True\n        # original file is over http\n        if SCHEME_RE.search(filename):\n            # do a url join so relative paths work\n            req_path = urllib_parse.urljoin(filename, req_path)\n        # original file and nested file are paths\n        elif not SCHEME_RE.search(req_path):\n            # do a join so relative paths work\n            req_path = os.path.join(os.path.dirname(filename), req_path)\n        # TODO: Why not use `comes_from='-r {} (line {})'` here as well?\n        parsed_reqs = parse_requirements(\n            req_path, finder, comes_from, options, session,\n            constraint=nested_constraint, wheel_cache=wheel_cache\n        )\n        for req in parsed_reqs:\n            yield req\n\n    # percolate hash-checking option upward\n    elif opts.require_hashes:\n        options.require_hashes = opts.require_hashes\n\n    # set finder options\n    elif finder:\n        if opts.index_url:\n            finder.index_urls = [opts.index_url]\n        if opts.no_index is True:\n            finder.index_urls = []\n        if opts.extra_index_urls:\n            finder.index_urls.extend(opts.extra_index_urls)\n        if opts.find_links:\n            # FIXME: it would be nice to keep track of the source\n            # of the find_links: support a find-links local path\n            # relative to a requirements file.\n            value = opts.find_links[0]\n            req_dir = os.path.dirname(os.path.abspath(filename))\n            relative_to_reqs_file = os.path.join(req_dir, value)\n            if os.path.exists(relative_to_reqs_file):\n                value = relative_to_reqs_file\n            finder.find_links.append(value)\n        if opts.pre:\n            finder.allow_all_prereleases = True\n        if opts.trusted_hosts:\n            finder.secure_origins.extend(\n                (\"*\", host, \"*\") for host in opts.trusted_hosts)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef break_args_options(line):\n    # type: (Text) -> Tuple[str, Text]\n    \"\"\"Break up the line into an args and options string.  We only want to shlex\n    (and then optparse) the options, not the args.  args can contain markers\n    which are corrupted by shlex.\n    \"\"\"\n    tokens = line.split(' ')\n    args = []\n    options = tokens[:]\n    for token in tokens:\n        if token.startswith('-') or token.startswith('--'):\n            break\n        else:\n            args.append(token)\n            options.pop(0)\n    return ' '.join(args), ' '.join(options)", "response": "Break up the line into an args and options string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build_parser(line):\n    # type: (Text) -> optparse.OptionParser\n    \"\"\"\n    Return a parser for parsing requirement lines\n    \"\"\"\n    parser = optparse.OptionParser(add_help_option=False)\n\n    option_factories = SUPPORTED_OPTIONS + SUPPORTED_OPTIONS_REQ\n    for option_factory in option_factories:\n        option = option_factory()\n        parser.add_option(option)\n\n    # By default optparse sys.exits on parsing errors. We want to wrap\n    # that in our own exception.\n    def parser_exit(self, msg):\n        # add offending line\n        msg = 'Invalid requirement: %s\\n%s' % (line, msg)\n        raise RequirementsFileParseError(msg)\n    # NOTE: mypy disallows assigning to a method\n    #       https://github.com/python/mypy/issues/2427\n    parser.exit = parser_exit  # type: ignore\n\n    return parser", "response": "Build a optparse. OptionParser for parsing a requirement line."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nyield a list of lines from the lines_enum.", "response": "def join_lines(lines_enum):\n    # type: (ReqFileLines) -> ReqFileLines\n    \"\"\"Joins a line ending in '\\' with the previous line (except when following\n    comments).  The joined line takes on the index of the first line.\n    \"\"\"\n    primary_line_number = None\n    new_line = []  # type: List[Text]\n    for line_number, line in lines_enum:\n        if not line.endswith('\\\\') or COMMENT_RE.match(line):\n            if COMMENT_RE.match(line):\n                # this ensures comments are always matched later\n                line = ' ' + line\n            if new_line:\n                new_line.append(line)\n                yield primary_line_number, ''.join(new_line)\n                new_line = []\n            else:\n                yield line_number, line\n        else:\n            if not new_line:\n                primary_line_number = line_number\n            new_line.append(line.strip('\\\\'))\n\n    # last line contains \\\n    if new_line:\n        yield primary_line_number, ''.join(new_line)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstripping comments and filter empty lines.", "response": "def ignore_comments(lines_enum):\n    # type: (ReqFileLines) -> ReqFileLines\n    \"\"\"\n    Strips comments and filter empty lines.\n    \"\"\"\n    for line_number, line in lines_enum:\n        line = COMMENT_RE.sub('', line)\n        line = line.strip()\n        if line:\n            yield line_number, line"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nskipping lines that match the skip_requirements_regex pattern pattern", "response": "def skip_regex(lines_enum, options):\n    # type: (ReqFileLines, Optional[optparse.Values]) -> ReqFileLines\n    \"\"\"\n    Skip lines that match '--skip-requirements-regex' pattern\n\n    Note: the regex pattern is only built once\n    \"\"\"\n    skip_regex = options.skip_requirements_regex if options else None\n    if skip_regex:\n        pattern = re.compile(skip_regex)\n        lines_enum = filterfalse(lambda e: pattern.search(e[1]), lines_enum)\n    return lines_enum"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef expand_env_variables(lines_enum):\n    # type: (ReqFileLines) -> ReqFileLines\n    \"\"\"Replace all environment variables that can be retrieved via `os.getenv`.\n\n    The only allowed format for environment variables defined in the\n    requirement file is `${MY_VARIABLE_1}` to ensure two things:\n\n    1. Strings that contain a `$` aren't accidentally (partially) expanded.\n    2. Ensure consistency across platforms for requirement files.\n\n    These points are the result of a discusssion on the `github pull\n    request #3514 <https://github.com/pypa/pip/pull/3514>`_.\n\n    Valid characters in variable names follow the `POSIX standard\n    <http://pubs.opengroup.org/onlinepubs/9699919799/>`_ and are limited\n    to uppercase letter, digits and the `_` (underscore).\n    \"\"\"\n    for line_number, line in lines_enum:\n        for env_var, var_name in ENV_VAR_RE.findall(line):\n            value = os.getenv(var_name)\n            if not value:\n                continue\n\n            line = line.replace(env_var, value)\n\n        yield line_number, line", "response": "Expand all environment variables that can be retrieved via os. getenv."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef finish(self):\n        super(InterruptibleMixin, self).finish()\n        signal(SIGINT, self.original_handler)", "response": "Restore the original SIGINT handler after finishing."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef handle_sigint(self, signum, frame):\n        self.finish()\n        self.original_handler(signum, frame)", "response": "Call self.finish() before delegating to the original SIGINT handler.\n\n        This handler should only be in place while the progress display is\n        active."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef iter_fields(self, exclude=None, only=None):\n        for name in self.fields:\n            if (exclude is only is None) or \\\n               (exclude is not None and name not in exclude) or \\\n               (only is not None and name in only):\n                try:\n                    yield name, getattr(self, name)\n                except AttributeError:\n                    pass", "response": "This method iterates over all the fields that are defined and yields tuples of key value pairs."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef iter_child_nodes(self, exclude=None, only=None):\n        for field, item in self.iter_fields(exclude, only):\n            if isinstance(item, list):\n                for n in item:\n                    if isinstance(n, Node):\n                        yield n\n            elif isinstance(item, Node):\n                yield item", "response": "This iterates over all direct child nodes of the node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_all(self, node_type):\n        for child in self.iter_child_nodes():\n            if isinstance(child, node_type):\n                yield child\n            for result in child.find_all(node_type):\n                yield result", "response": "Find all the nodes of a given type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_ctx(self, ctx):\n        todo = deque([self])\n        while todo:\n            node = todo.popleft()\n            if 'ctx' in node.fields:\n                node.ctx = ctx\n            todo.extend(node.iter_child_nodes())\n        return self", "response": "Reset the context of a node and all child nodes."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the line numbers of the node and all children.", "response": "def set_lineno(self, lineno, override=False):\n        \"\"\"Set the line numbers of the node and children.\"\"\"\n        todo = deque([self])\n        while todo:\n            node = todo.popleft()\n            if 'lineno' in node.attributes:\n                if node.lineno is None or override:\n                    node.lineno = lineno\n            todo.extend(node.iter_child_nodes())\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_environment(self, environment):\n        todo = deque([self])\n        while todo:\n            node = todo.popleft()\n            node.environment = environment\n            todo.extend(node.iter_child_nodes())\n        return self", "response": "Set the environment for all nodes."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a const object if the value is representable as constant value otherwise it will raise Impossible exception.", "response": "def from_untrusted(cls, value, lineno=None, environment=None):\n        \"\"\"Return a const object if the value is representable as\n        constant value in the generated code, otherwise it will raise\n        an `Impossible` exception.\n        \"\"\"\n        from .compiler import has_safe_repr\n        if not has_safe_repr(value):\n            raise Impossible()\n        return cls(value, lineno=lineno, environment=environment)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding a wheel from a source directory using PEP 517 hooks.", "response": "def build_wheel(source_dir, wheel_dir, config_settings=None):\n    \"\"\"Build a wheel from a source directory using PEP 517 hooks.\n\n    :param str source_dir: Source directory containing pyproject.toml\n    :param str wheel_dir: Target directory to create wheel in\n    :param dict config_settings: Options to pass to build backend\n\n    This is a blocking function which will run pip in a subprocess to install\n    build requirements.\n    \"\"\"\n    if config_settings is None:\n        config_settings = {}\n    requires, backend = _load_pyproject(source_dir)\n    hooks = Pep517HookCaller(source_dir, backend)\n\n    with BuildEnvironment() as env:\n        env.pip_install(requires)\n        reqs = hooks.get_requires_for_build_wheel(config_settings)\n        env.pip_install(reqs)\n        return hooks.build_wheel(wheel_dir, config_settings)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds an sdist from a source directory using PEP 517 hooks.", "response": "def build_sdist(source_dir, sdist_dir, config_settings=None):\n    \"\"\"Build an sdist from a source directory using PEP 517 hooks.\n\n    :param str source_dir: Source directory containing pyproject.toml\n    :param str sdist_dir: Target directory to place sdist in\n    :param dict config_settings: Options to pass to build backend\n\n    This is a blocking function which will run pip in a subprocess to install\n    build requirements.\n    \"\"\"\n    if config_settings is None:\n        config_settings = {}\n    requires, backend = _load_pyproject(source_dir)\n    hooks = Pep517HookCaller(source_dir, backend)\n\n    with BuildEnvironment() as env:\n        env.pip_install(requires)\n        reqs = hooks.get_requires_for_build_sdist(config_settings)\n        env.pip_install(reqs)\n        return hooks.build_sdist(sdist_dir, config_settings)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pip_install(self, reqs):\n        if not reqs:\n            return\n        log.info('Calling pip to install %s', reqs)\n        check_call([\n            sys.executable, '-m', 'pip', 'install', '--ignore-installed',\n            '--prefix', self.path] + list(reqs))", "response": "Install dependencies into this env by calling pip."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmoves all the children of the current node to newParent.", "response": "def reparentChildren(self, newParent):\n        \"\"\"Move all the children of the current node to newParent.\n        This is needed so that trees that don't store text as nodes move the\n        text in the correct way\n\n        :arg newParent: the node to move all this node's children to\n\n        \"\"\"\n        # XXX - should this method be made more general?\n        for child in self.childNodes:\n            newParent.appendChild(child)\n        self.childNodes = []"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef elementInActiveFormattingElements(self, name):\n\n        for item in self.activeFormattingElements[::-1]:\n            # Check for Marker first because if it's a Marker it doesn't have a\n            # name attribute.\n            if item == Marker:\n                break\n            elif item.name == name:\n                return item\n        return False", "response": "Check if an element exists between the end of the activeFormattingElements and the last marker. If it does return it else return false."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates an element but don t insert it anywhere", "response": "def createElement(self, token):\n        \"\"\"Create an element but don't insert it anywhere\"\"\"\n        name = token[\"name\"]\n        namespace = token.get(\"namespace\", self.defaultNamespace)\n        element = self.elementClass(name, namespace)\n        element.attributes = token[\"data\"]\n        return element"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _setInsertFromTable(self, value):\n        self._insertFromTable = value\n        if value:\n            self.insertElement = self.insertElementTable\n        else:\n            self.insertElement = self.insertElementNormal", "response": "Switch the function used to insert an element from the misnested table one to the misnested table one and back again"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating an element and insert it into the tree.", "response": "def insertElementTable(self, token):\n        \"\"\"Create an element and insert it into the tree\"\"\"\n        element = self.createElement(token)\n        if self.openElements[-1].name not in tableInsertModeElements:\n            return self.insertElementNormal(token)\n        else:\n            # We should be in the InTable mode. This means we want to do\n            # special magic element rearranging\n            parent, insertBefore = self.getTableMisnestedNodePosition()\n            if insertBefore is None:\n                parent.appendChild(element)\n            else:\n                parent.insertBefore(element, insertBefore)\n            self.openElements.append(element)\n        return element"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getTableMisnestedNodePosition(self):\n        # The foster parent element is the one which comes before the most\n        # recently opened table element\n        # XXX - this is really inelegant\n        lastTable = None\n        fosterParent = None\n        insertBefore = None\n        for elm in self.openElements[::-1]:\n            if elm.name == \"table\":\n                lastTable = elm\n                break\n        if lastTable:\n            # XXX - we should really check that this parent is actually a\n            # node here\n            if lastTable.parent:\n                fosterParent = lastTable.parent\n                insertBefore = lastTable\n            else:\n                fosterParent = self.openElements[\n                    self.openElements.index(lastTable) - 1]\n        else:\n            fosterParent = self.openElements[0]\n        return fosterParent, insertBefore", "response": "Get the foster parent element and sibling to insert before the most recent opened table element or None when inserting a misnested table node"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getFragment(self):\n        # assert self.innerHTML\n        fragment = self.fragmentClass()\n        self.openElements[0].reparentChildren(fragment)\n        return fragment", "response": "Return the final fragment"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nevaluating a marker. Return the boolean from evaluating the given marker against the environment. environment is an optional argument to override all or part of the determined environment. The environment is determined from the current Python process.", "response": "def evaluate(self, environment=None):\n        \"\"\"Evaluate a marker.\n\n        Return the boolean from evaluating the given marker against the\n        environment. environment is an optional argument to override all or\n        part of the determined environment.\n\n        The environment is determined from the current Python process.\n        \"\"\"\n        current_environment = default_environment()\n        if environment is not None:\n            current_environment.update(environment)\n\n        return _evaluate_markers(self._markers, current_environment)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _allow_all_wheels():\n    original_wheel_supported = Wheel.supported\n    original_support_index_min = Wheel.support_index_min\n\n    Wheel.supported = _wheel_supported\n    Wheel.support_index_min = _wheel_support_index_min\n    yield\n    Wheel.supported = original_wheel_supported\n    Wheel.support_index_min = original_support_index_min", "response": "Monkey patch pip. Wheel to allow all wheels"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create(self):\n        if self.path is not None:\n            logger.debug(\n                \"Skipped creation of temporary directory: {}\".format(self.path)\n            )\n            return\n        # We realpath here because some systems have their default tmpdir\n        # symlinked to another directory.  This tends to confuse build\n        # scripts, so we canonicalize the path by traversing potential\n        # symlinks here.\n        self.path = os.path.realpath(\n            tempfile.mkdtemp(prefix=\"pip-{}-\".format(self.kind))\n        )\n        self._register_finalizer()\n        logger.debug(\"Created temporary directory: {}\".format(self.path))", "response": "Create a temporary directory and store its path in self. path\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cleanup(self):\n        if getattr(self._finalizer, \"detach\", None) and self._finalizer.detach():\n            if os.path.exists(self.path):\n                try:\n                    rmtree(self.path)\n                except OSError:\n                    pass\n                else:\n                    self.path = None", "response": "Remove the temporary directory created and reset state\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _generate_names(cls, name):\n        for i in range(1, len(name)):\n            for candidate in itertools.combinations_with_replacement(\n                    cls.LEADING_CHARS, i - 1):\n                new_name = '~' + ''.join(candidate) + name[i:]\n                if new_name != name:\n                    yield new_name\n\n        # If we make it this far, we will have to make a longer name\n        for i in range(len(cls.LEADING_CHARS)):\n            for candidate in itertools.combinations_with_replacement(\n                    cls.LEADING_CHARS, i):\n                new_name = '~' + ''.join(candidate) + name\n                if new_name != name:\n                    yield new_name", "response": "Generates a series of temporary names."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndetect the encoding of the given byte string.", "response": "def detect(byte_str):\n    \"\"\"\n    Detect the encoding of the given byte string.\n\n    :param byte_str:     The byte sequence to examine.\n    :type byte_str:      ``bytes`` or ``bytearray``\n    \"\"\"\n    if not isinstance(byte_str, bytearray):\n        if not isinstance(byte_str, bytes):\n            raise TypeError('Expected object of type bytes or bytearray, got: '\n                            '{0}'.format(type(byte_str)))\n        else:\n            byte_str = bytearray(byte_str)\n    detector = UniversalDetector()\n    detector.feed(byte_str)\n    return detector.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef unescape(self):\n        from ._constants import HTML_ENTITIES\n\n        def handle_match(m):\n            name = m.group(1)\n            if name in HTML_ENTITIES:\n                return unichr(HTML_ENTITIES[name])\n            try:\n                if name[:2] in (\"#x\", \"#X\"):\n                    return unichr(int(name[2:], 16))\n                elif name.startswith(\"#\"):\n                    return unichr(int(name[1:]))\n            except ValueError:\n                pass\n            # Don't modify unexpected input.\n            return m.group()\n\n        return _entity_re.sub(handle_match, text_type(self))", "response": "Convert escaped markup back into a text string."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nescaping a string. Calls cls. escape and ensures that the class is returned.", "response": "def escape(cls, s):\n        \"\"\"Escape a string. Calls :func:`escape` and ensures that for\n        subclasses the correct type is returned.\n        \"\"\"\n        rv = escape(s)\n        if rv.__class__ is not cls:\n            return cls(rv)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npopulating the internal link attribute with the link from the package finder.", "response": "def populate_link(self, finder, upgrade, require_hashes):\n        # type: (PackageFinder, bool, bool) -> None\n        \"\"\"Ensure that if a link can be found for this, that it is found.\n\n        Note that self.link may still be None - if Upgrade is False and the\n        requirement is already installed.\n\n        If require_hashes is True, don't use the wheel cache, because cached\n        wheels, always built locally, have different hashes than the files\n        downloaded from the index server and thus throw false hash mismatches.\n        Furthermore, cached wheels at present have undeterministic contents due\n        to file modification times.\n        \"\"\"\n        if self.link is None:\n            self.link = finder.find_requirement(self, upgrade)\n        if self._wheel_cache is not None and not require_hashes:\n            old_link = self.link\n            self.link = self._wheel_cache.get(self.link, self.name)\n            if old_link != self.link:\n                logger.debug('Using cached wheel link: %s', self.link)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_pinned(self):\n        # type: () -> bool\n        \"\"\"Return whether I am pinned to an exact version.\n\n        For example, some-package==1.2 is pinned; some-package>1.2 is not.\n        \"\"\"\n        specifiers = self.specifier\n        return (len(specifiers) == 1 and\n                next(iter(specifiers)).operator in {'==', '==='})", "response": "Return whether I am pinned to an exact version."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a hash - comparer that considers my option - and URL - based hashes to be known - good.", "response": "def hashes(self, trust_internet=True):\n        # type: (bool) -> Hashes\n        \"\"\"Return a hash-comparer that considers my option- and URL-based\n        hashes to be known-good.\n\n        Hashes in URLs--ones embedded in the requirements file, not ones\n        downloaded from an index server--are almost peers with ones from\n        flags. They satisfy --require-hashes (whether it was implicitly or\n        explicitly activated) but do not activate it. md5 and sha224 are not\n        allowed in flags, which should nudge people toward good algos. We\n        always OR all hashes together, even ones from URLs.\n\n        :param trust_internet: Whether to trust URL-based (#md5=...) hashes\n            downloaded from the internet, as by populate_link()\n\n        \"\"\"\n        good_hashes = self.options.get('hashes', {}).copy()\n        link = self.link if trust_internet else self.original_link\n        if link and link.hash:\n            good_hashes.setdefault(link.hash_name, []).append(link.hash)\n        return Hashes(good_hashes)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmoving the build directory to the correct location.", "response": "def _correct_build_location(self):\n        # type: () -> None\n        \"\"\"Move self._temp_build_dir to self._ideal_build_dir/self.req.name\n\n        For some requirements (e.g. a path to a directory), the name of the\n        package is not available until we run egg_info, so the build_location\n        will return a temporary directory and store the _ideal_build_dir.\n\n        This is only called by self.run_egg_info to fix the temporary build\n        directory.\n        \"\"\"\n        if self.source_dir is not None:\n            return\n        assert self.req is not None\n        assert self._temp_build_dir.path\n        assert (self._ideal_build_dir is not None and\n                self._ideal_build_dir.path)  # type: ignore\n        old_location = self._temp_build_dir.path\n        self._temp_build_dir.path = None\n\n        new_location = self.build_location(self._ideal_build_dir)\n        if os.path.exists(new_location):\n            raise InstallationError(\n                'A package already exists in %s; please remove it to continue'\n                % display_path(new_location))\n        logger.debug(\n            'Moving package %s from %s to new location %s',\n            self, display_path(old_location), display_path(new_location),\n        )\n        shutil.move(old_location, new_location)\n        self._temp_build_dir.path = new_location\n        self._ideal_build_dir = None\n        self.source_dir = os.path.normpath(os.path.abspath(new_location))\n        self._egg_info_path = None\n\n        # Correct the metadata directory, if it exists\n        if self.metadata_directory:\n            old_meta = self.metadata_directory\n            rel = os.path.relpath(old_meta, start=old_location)\n            new_meta = os.path.join(new_location, rel)\n            new_meta = os.path.normpath(os.path.abspath(new_meta))\n            self.metadata_directory = new_meta"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remove_temporary_source(self):\n        # type: () -> None\n        \"\"\"Remove the source files from this requirement, if they are marked\n        for deletion\"\"\"\n        if self.source_dir and os.path.exists(\n                os.path.join(self.source_dir, PIP_DELETE_MARKER_FILENAME)):\n            logger.debug('Removing source in %s', self.source_dir)\n            rmtree(self.source_dir)\n        self.source_dir = None\n        self._temp_build_dir.cleanup()\n        self.build_env.cleanup()", "response": "Remove the temporary source files from this requirement."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_pyproject_toml(self):\n        # type: () -> None\n        \"\"\"Load the pyproject.toml file.\n\n        After calling this routine, all of the attributes related to PEP 517\n        processing for this requirement have been set. In particular, the\n        use_pep517 attribute can be used to determine whether we should\n        follow the PEP 517 or legacy (setup.py) code path.\n        \"\"\"\n        pep517_data = load_pyproject_toml(\n            self.use_pep517,\n            self.pyproject_toml,\n            self.setup_py,\n            str(self)\n        )\n\n        if pep517_data is None:\n            self.use_pep517 = False\n        else:\n            self.use_pep517 = True\n            requires, backend, check = pep517_data\n            self.requirements_to_check = check\n            self.pyproject_requires = requires\n            self.pep517_backend = Pep517HookCaller(self.setup_py_dir, backend)\n\n            # Use a custom function to call subprocesses\n            self.spin_message = \"\"\n\n            def runner(cmd, cwd=None, extra_environ=None):\n                with open_spinner(self.spin_message) as spinner:\n                    call_subprocess(\n                        cmd,\n                        cwd=cwd,\n                        extra_environ=extra_environ,\n                        show_stdout=False,\n                        spinner=spinner\n                    )\n                self.spin_message = \"\"\n\n            self.pep517_backend._subprocess_runner = runner", "response": "Load the pyproject. toml file and set the attributes related to this requirement."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef prepare_metadata(self):\n        # type: () -> None\n        \"\"\"Ensure that project metadata is available.\n\n        Under PEP 517, call the backend hook to prepare the metadata.\n        Under legacy processing, call setup.py egg-info.\n        \"\"\"\n        assert self.source_dir\n\n        with indent_log():\n            if self.use_pep517:\n                self.prepare_pep517_metadata()\n            else:\n                self.run_egg_info()\n\n        if not self.req:\n            if isinstance(parse_version(self.metadata[\"Version\"]), Version):\n                op = \"==\"\n            else:\n                op = \"===\"\n            self.req = Requirement(\n                \"\".join([\n                    self.metadata[\"Name\"],\n                    op,\n                    self.metadata[\"Version\"],\n                ])\n            )\n            self._correct_build_location()\n        else:\n            metadata_name = canonicalize_name(self.metadata[\"Name\"])\n            if canonicalize_name(self.req.name) != metadata_name:\n                logger.warning(\n                    'Generating metadata for package %s '\n                    'produced metadata for project name %s. Fix your '\n                    '#egg=%s fragments.',\n                    self.name, metadata_name, self.name\n                )\n                self.req = Requirement(metadata_name)", "response": "Prepares the project metadata for the current version of the project."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_dist(self):\n        # type: () -> Distribution\n        \"\"\"Return a pkg_resources.Distribution for this requirement\"\"\"\n        if self.metadata_directory:\n            base_dir, distinfo = os.path.split(self.metadata_directory)\n            metadata = pkg_resources.PathMetadata(\n                base_dir, self.metadata_directory\n            )\n            dist_name = os.path.splitext(distinfo)[0]\n            typ = pkg_resources.DistInfoDistribution\n        else:\n            egg_info = self.egg_info_path.rstrip(os.path.sep)\n            base_dir = os.path.dirname(egg_info)\n            metadata = pkg_resources.PathMetadata(base_dir, egg_info)\n            dist_name = os.path.splitext(os.path.basename(egg_info))[0]\n            # https://github.com/python/mypy/issues/1174\n            typ = pkg_resources.Distribution  # type: ignore\n\n        return typ(\n            base_dir,\n            project_name=dist_name,\n            metadata=metadata,\n        )", "response": "Return a pkg_resources. Distribution for this requirement"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\npopulate a requirement set with the contents of the command line arguments.", "response": "def populate_requirement_set(requirement_set,  # type: RequirementSet\n                                 args,             # type: List[str]\n                                 options,          # type: Values\n                                 finder,           # type: PackageFinder\n                                 session,          # type: PipSession\n                                 name,             # type: str\n                                 wheel_cache       # type: Optional[WheelCache]\n                                 ):\n        # type: (...) -> None\n        \"\"\"\n        Marshal cmd line args into a requirement set.\n        \"\"\"\n        # NOTE: As a side-effect, options.require_hashes and\n        #       requirement_set.require_hashes may be updated\n\n        for filename in options.constraints:\n            for req_to_add in parse_requirements(\n                    filename,\n                    constraint=True, finder=finder, options=options,\n                    session=session, wheel_cache=wheel_cache):\n                req_to_add.is_direct = True\n                requirement_set.add_requirement(req_to_add)\n\n        for req in args:\n            req_to_add = install_req_from_line(\n                req, None, isolated=options.isolated_mode,\n                use_pep517=options.use_pep517,\n                wheel_cache=wheel_cache\n            )\n            req_to_add.is_direct = True\n            requirement_set.add_requirement(req_to_add)\n\n        for req in options.editables:\n            req_to_add = install_req_from_editable(\n                req,\n                isolated=options.isolated_mode,\n                use_pep517=options.use_pep517,\n                wheel_cache=wheel_cache\n            )\n            req_to_add.is_direct = True\n            requirement_set.add_requirement(req_to_add)\n\n        for filename in options.requirements:\n            for req_to_add in parse_requirements(\n                    filename,\n                    finder=finder, options=options, session=session,\n                    wheel_cache=wheel_cache,\n                    use_pep517=options.use_pep517):\n                req_to_add.is_direct = True\n                requirement_set.add_requirement(req_to_add)\n        # If --require-hashes was a line in a requirements file, tell\n        # RequirementSet about it:\n        requirement_set.require_hashes = options.require_hashes\n\n        if not (args or options.editables or options.requirements):\n            opts = {'name': name}\n            if options.find_links:\n                raise CommandError(\n                    'You must give at least one requirement to %(name)s '\n                    '(maybe you meant \"pip %(name)s %(links)s\"?)' %\n                    dict(opts, links=' '.join(options.find_links)))\n            else:\n                raise CommandError(\n                    'You must give at least one requirement to %(name)s '\n                    '(see \"pip help %(name)s\")' % opts)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbuilds a package finder appropriate to this requirement command.", "response": "def _build_package_finder(\n        self,\n        options,               # type: Values\n        session,               # type: PipSession\n        platform=None,         # type: Optional[str]\n        python_versions=None,  # type: Optional[List[str]]\n        abi=None,              # type: Optional[str]\n        implementation=None    # type: Optional[str]\n    ):\n        # type: (...) -> PackageFinder\n        \"\"\"\n        Create a package finder appropriate to this requirement command.\n        \"\"\"\n        index_urls = [options.index_url] + options.extra_index_urls\n        if options.no_index:\n            logger.debug(\n                'Ignoring indexes: %s',\n                ','.join(redact_password_from_url(url) for url in index_urls),\n            )\n            index_urls = []\n\n        return PackageFinder(\n            find_links=options.find_links,\n            format_control=options.format_control,\n            index_urls=index_urls,\n            trusted_hosts=options.trusted_hosts,\n            allow_all_prereleases=options.pre,\n            session=session,\n            platform=platform,\n            versions=python_versions,\n            abi=abi,\n            implementation=implementation,\n            prefer_binary=options.prefer_binary,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef intranges_from_list(list_):\n\n    sorted_list = sorted(list_)\n    ranges = []\n    last_write = -1\n    for i in range(len(sorted_list)):\n        if i+1 < len(sorted_list):\n            if sorted_list[i] == sorted_list[i+1]-1:\n                continue\n        current_range = sorted_list[last_write+1:i+1]\n        ranges.append(_encode_range(current_range[0], current_range[-1] + 1))\n        last_write = i\n\n    return tuple(ranges)", "response": "Represent a list of integers as a sequence of ranges."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndetermines if int_ falls into one of the ranges in ranges.", "response": "def intranges_contain(int_, ranges):\n    \"\"\"Determine if `int_` falls into one of the ranges in `ranges`.\"\"\"\n    tuple_ = _encode_range(int_, 0)\n    pos = bisect.bisect_left(ranges, tuple_)\n    # we could be immediately ahead of a tuple (start, end)\n    # with start < int_ <= end\n    if pos > 0:\n        left, right = _decode_range(ranges[pos-1])\n        if left <= int_ < right:\n            return True\n    # or we could be immediately behind a tuple (int_, end)\n    if pos < len(ranges):\n        left, _ = _decode_range(ranges[pos])\n        if left == int_:\n            return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the hash digest of a file.", "response": "def _hash_of_file(path, algorithm):\n    \"\"\"Return the hash digest of a file.\"\"\"\n    with open(path, 'rb') as archive:\n        hash = hashlib.new(algorithm)\n        for chunk in read_chunks(archive):\n            hash.update(chunk)\n    return hash.hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning information about the OS distribution that is compatible with Python s Linux distribution.", "response": "def linux_distribution(self, full_distribution_name=True):\n        \"\"\"\n        Return information about the OS distribution that is compatible\n        with Python's :func:`platform.linux_distribution`, supporting a subset\n        of its parameters.\n\n        For details, see :func:`distro.linux_distribution`.\n        \"\"\"\n        return (\n            self.name() if full_distribution_name else self.id(),\n            self.version(),\n            self.codename()\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef id(self):\n        def normalize(distro_id, table):\n            distro_id = distro_id.lower().replace(' ', '_')\n            return table.get(distro_id, distro_id)\n\n        distro_id = self.os_release_attr('id')\n        if distro_id:\n            return normalize(distro_id, NORMALIZED_OS_ID)\n\n        distro_id = self.lsb_release_attr('distributor_id')\n        if distro_id:\n            return normalize(distro_id, NORMALIZED_LSB_ID)\n\n        distro_id = self.distro_release_attr('id')\n        if distro_id:\n            return normalize(distro_id, NORMALIZED_DISTRO_ID)\n\n        distro_id = self.uname_attr('id')\n        if distro_id:\n            return normalize(distro_id, NORMALIZED_DISTRO_ID)\n\n        return ''", "response": "Return the distro ID of the OS distribution as a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef name(self, pretty=False):\n        name = self.os_release_attr('name') \\\n            or self.lsb_release_attr('distributor_id') \\\n            or self.distro_release_attr('name') \\\n            or self.uname_attr('name')\n        if pretty:\n            name = self.os_release_attr('pretty_name') \\\n                or self.lsb_release_attr('description')\n            if not name:\n                name = self.distro_release_attr('name') \\\n                       or self.uname_attr('name')\n                version = self.version(pretty=True)\n                if version:\n                    name = name + ' ' + version\n        return name or ''", "response": "Return the name of the OS distribution as a string."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the version of the OS distribution as a string.", "response": "def version(self, pretty=False, best=False):\n        \"\"\"\n        Return the version of the OS distribution, as a string.\n\n        For details, see :func:`distro.version`.\n        \"\"\"\n        versions = [\n            self.os_release_attr('version_id'),\n            self.lsb_release_attr('release'),\n            self.distro_release_attr('version_id'),\n            self._parse_distro_release_content(\n                self.os_release_attr('pretty_name')).get('version_id', ''),\n            self._parse_distro_release_content(\n                self.lsb_release_attr('description')).get('version_id', ''),\n            self.uname_attr('release')\n        ]\n        version = ''\n        if best:\n            # This algorithm uses the last version in priority order that has\n            # the best precision. If the versions are not in conflict, that\n            # does not matter; otherwise, using the last one instead of the\n            # first one might be considered a surprise.\n            for v in versions:\n                if v.count(\".\") > version.count(\".\") or version == '':\n                    version = v\n        else:\n            for v in versions:\n                if v != '':\n                    version = v\n                    break\n        if pretty and version and self.codename():\n            version = u'{0} ({1})'.format(version, self.codename())\n        return version"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the version of the OS distribution as a tuple of version numbers.", "response": "def version_parts(self, best=False):\n        \"\"\"\n        Return the version of the OS distribution, as a tuple of version\n        numbers.\n\n        For details, see :func:`distro.version_parts`.\n        \"\"\"\n        version_str = self.version(best=best)\n        if version_str:\n            version_regex = re.compile(r'(\\d+)\\.?(\\d+)?\\.?(\\d+)?')\n            matches = version_regex.match(version_str)\n            if matches:\n                major, minor, build_number = matches.groups()\n                return major, minor or '', build_number or ''\n        return '', '', ''"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn certain machine - readable information about the OS distribution.", "response": "def info(self, pretty=False, best=False):\n        \"\"\"\n        Return certain machine-readable information about the OS\n        distribution.\n\n        For details, see :func:`distro.info`.\n        \"\"\"\n        return dict(\n            id=self.id(),\n            version=self.version(pretty, best),\n            version_parts=dict(\n                major=self.major_version(best),\n                minor=self.minor_version(best),\n                build_number=self.build_number(best)\n            ),\n            like=self.like(),\n            codename=self.codename(),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _os_release_info(self):\n        if os.path.isfile(self.os_release_file):\n            with open(self.os_release_file) as release_file:\n                return self._parse_os_release_content(release_file)\n        return {}", "response": "Get the information items from the specified os - release file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the information items from the lsb_release command output.", "response": "def _lsb_release_info(self):\n        \"\"\"\n        Get the information items from the lsb_release command output.\n\n        Returns:\n            A dictionary containing all information items.\n        \"\"\"\n        if not self.include_lsb:\n            return {}\n        with open(os.devnull, 'w') as devnull:\n            try:\n                cmd = ('lsb_release', '-a')\n                stdout = subprocess.check_output(cmd, stderr=devnull)\n            except OSError:  # Command not found\n                return {}\n        content = stdout.decode(sys.getfilesystemencoding()).splitlines()\n        return self._parse_lsb_release_content(content)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _parse_lsb_release_content(lines):\n        props = {}\n        for line in lines:\n            kv = line.strip('\\n').split(':', 1)\n            if len(kv) != 2:\n                # Ignore lines without colon.\n                continue\n            k, v = kv\n            props.update({k.replace(' ', '_').lower(): v.strip()})\n        return props", "response": "Parse the output of the lsb_release command."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the information items from the specified distro release file.", "response": "def _distro_release_info(self):\n        \"\"\"\n        Get the information items from the specified distro release file.\n\n        Returns:\n            A dictionary containing all information items.\n        \"\"\"\n        if self.distro_release_file:\n            # If it was specified, we use it and parse what we can, even if\n            # its file name or content does not match the expected pattern.\n            distro_info = self._parse_distro_release_file(\n                self.distro_release_file)\n            basename = os.path.basename(self.distro_release_file)\n            # The file name pattern for user-specified distro release files\n            # is somewhat more tolerant (compared to when searching for the\n            # file), because we want to use what was specified as best as\n            # possible.\n            match = _DISTRO_RELEASE_BASENAME_PATTERN.match(basename)\n            if match:\n                distro_info['id'] = match.group(1)\n            return distro_info\n        else:\n            try:\n                basenames = os.listdir(_UNIXCONFDIR)\n                # We sort for repeatability in cases where there are multiple\n                # distro specific files; e.g. CentOS, Oracle, Enterprise all\n                # containing `redhat-release` on top of their own.\n                basenames.sort()\n            except OSError:\n                # This may occur when /etc is not readable but we can't be\n                # sure about the *-release files. Check common entries of\n                # /etc for information. If they turn out to not be there the\n                # error is handled in `_parse_distro_release_file()`.\n                basenames = ['SuSE-release',\n                             'arch-release',\n                             'base-release',\n                             'centos-release',\n                             'fedora-release',\n                             'gentoo-release',\n                             'mageia-release',\n                             'mandrake-release',\n                             'mandriva-release',\n                             'mandrivalinux-release',\n                             'manjaro-release',\n                             'oracle-release',\n                             'redhat-release',\n                             'sl-release',\n                             'slackware-version']\n            for basename in basenames:\n                if basename in _DISTRO_RELEASE_IGNORE_BASENAMES:\n                    continue\n                match = _DISTRO_RELEASE_BASENAME_PATTERN.match(basename)\n                if match:\n                    filepath = os.path.join(_UNIXCONFDIR, basename)\n                    distro_info = self._parse_distro_release_file(filepath)\n                    if 'name' in distro_info:\n                        # The name is always present if the pattern matches\n                        self.distro_release_file = filepath\n                        distro_info['id'] = match.group(1)\n                        return distro_info\n            return {}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing a distro release file and return a dictionary containing all information items.", "response": "def _parse_distro_release_file(self, filepath):\n        \"\"\"\n        Parse a distro release file.\n\n        Parameters:\n\n        * filepath: Path name of the distro release file.\n\n        Returns:\n            A dictionary containing all information items.\n        \"\"\"\n        try:\n            with open(filepath) as fp:\n                # Only parse the first line. For instance, on SLES there\n                # are multiple lines. We don't want them...\n                return self._parse_distro_release_content(fp.readline())\n        except (OSError, IOError):\n            # Ignore not being able to read a specific, seemingly version\n            # related file.\n            # See https://github.com/nir0s/distro/issues/162\n            return {}"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse a line from a distro release file.", "response": "def _parse_distro_release_content(line):\n        \"\"\"\n        Parse a line from a distro release file.\n\n        Parameters:\n        * line: Line from the distro release file. Must be a unicode string\n                or a UTF-8 encoded byte string.\n\n        Returns:\n            A dictionary containing all information items.\n        \"\"\"\n        if isinstance(line, bytes):\n            line = line.decode('utf-8')\n        matches = _DISTRO_RELEASE_CONTENT_REVERSED_PATTERN.match(\n            line.strip()[::-1])\n        distro_info = {}\n        if matches:\n            # regexp ensures non-None\n            distro_info['name'] = matches.group(3)[::-1]\n            if matches.group(2):\n                distro_info['version_id'] = matches.group(2)[::-1]\n            if matches.group(1):\n                distro_info['codename'] = matches.group(1)[::-1]\n        elif line:\n            distro_info['name'] = line.strip()\n        return distro_info"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the dependency tree for the package root_key and return the collection of all its dependencies.", "response": "def dependency_tree(installed_keys, root_key):\n    \"\"\"\n    Calculate the dependency tree for the package `root_key` and return\n    a collection of all its dependencies.  Uses a DFS traversal algorithm.\n\n    `installed_keys` should be a {key: requirement} mapping, e.g.\n        {'django': from_line('django==1.8')}\n    `root_key` should be the key to return the dependency tree for.\n    \"\"\"\n    dependencies = set()\n    queue = collections.deque()\n\n    if root_key in installed_keys:\n        dep = installed_keys[root_key]\n        queue.append(dep)\n\n    while queue:\n        v = queue.popleft()\n        key = key_from_req(v)\n        if key in dependencies:\n            continue\n\n        dependencies.add(key)\n\n        for dep_specifier in v.requires():\n            dep_name = key_from_req(dep_specifier)\n            if dep_name in installed_keys:\n                dep = installed_keys[dep_name]\n\n                if dep_specifier.specifier.contains(dep.version):\n                    queue.append(dep)\n\n    return dependencies"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_dists_to_ignore(installed):\n    installed_keys = {key_from_req(r): r for r in installed}\n    return list(flat_map(lambda req: dependency_tree(installed_keys, req), PACKAGES_TO_IGNORE))", "response": "Returns a collection of package names to ignore when performing pip - sync."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate which packages should be installed or uninstalled given a set of compiled requirements and a list of currently installed modules.", "response": "def diff(compiled_requirements, installed_dists):\n    \"\"\"\n    Calculate which packages should be installed or uninstalled, given a set\n    of compiled requirements and a list of currently installed modules.\n    \"\"\"\n    requirements_lut = {r.link or key_from_req(r.req): r for r in compiled_requirements}\n\n    satisfied = set()  # holds keys\n    to_install = set()  # holds InstallRequirement objects\n    to_uninstall = set()  # holds keys\n\n    pkgs_to_ignore = get_dists_to_ignore(installed_dists)\n    for dist in installed_dists:\n        key = key_from_req(dist)\n        if key not in requirements_lut or not requirements_lut[key].match_markers():\n            to_uninstall.add(key)\n        elif requirements_lut[key].specifier.contains(dist.version):\n            satisfied.add(key)\n\n    for key, requirement in requirements_lut.items():\n        if key not in satisfied and requirement.match_markers():\n            to_install.add(requirement)\n\n    # Make sure to not uninstall any packages that should be ignored\n    to_uninstall -= set(pkgs_to_ignore)\n\n    return (to_install, to_uninstall)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsynchronizing the given set of modules.", "response": "def sync(to_install, to_uninstall, verbose=False, dry_run=False, install_flags=None):\n    \"\"\"\n    Install and uninstalls the given sets of modules.\n    \"\"\"\n    if not to_uninstall and not to_install:\n        click.echo(\"Everything up-to-date\")\n\n    pip_flags = []\n    if not verbose:\n        pip_flags += ['-q']\n\n    if to_uninstall:\n        if dry_run:\n            click.echo(\"Would uninstall:\")\n            for pkg in to_uninstall:\n                click.echo(\"  {}\".format(pkg))\n        else:\n            check_call([sys.executable, '-m', 'pip', 'uninstall', '-y'] + pip_flags + sorted(to_uninstall))\n\n    if to_install:\n        if install_flags is None:\n            install_flags = []\n        if dry_run:\n            click.echo(\"Would install:\")\n            for ireq in to_install:\n                click.echo(\"  {}\".format(format_requirement(ireq)))\n        else:\n            # prepare requirement lines\n            req_lines = []\n            for ireq in sorted(to_install, key=key_from_ireq):\n                ireq_hashes = get_hashes_from_ireq(ireq)\n                req_lines.append(format_requirement(ireq, hashes=ireq_hashes))\n\n            # save requirement lines to a temporary file\n            tmp_req_file = tempfile.NamedTemporaryFile(mode='wt', delete=False)\n            tmp_req_file.write('\\n'.join(req_lines))\n            tmp_req_file.close()\n\n            try:\n                check_call(\n                    [sys.executable, '-m', 'pip', 'install', '-r', tmp_req_file.name] + pip_flags + install_flags\n                )\n            finally:\n                os.unlink(tmp_req_file.name)\n\n    return 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a new spontaneous environment.", "response": "def get_spontaneous_environment(*args):\n    \"\"\"Return a new spontaneous environment.  A spontaneous environment is an\n    unnamed and unaccessible (in theory) environment that is used for\n    templates generated from a string and not from the file system.\n    \"\"\"\n    try:\n        env = _spontaneous_environments.get(args)\n    except TypeError:\n        return Environment(*args)\n    if env is not None:\n        return env\n    _spontaneous_environments[args] = env = Environment(*args)\n    env.shared = True\n    return env"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _environment_sanity_check(environment):\n    assert issubclass(environment.undefined, Undefined), 'undefined must ' \\\n        'be a subclass of undefined because filters depend on it.'\n    assert environment.block_start_string != \\\n        environment.variable_start_string != \\\n        environment.comment_start_string, 'block, variable and comment ' \\\n        'start strings must be different'\n    assert environment.newline_sequence in ('\\r', '\\r\\n', '\\n'), \\\n        'newline_sequence set to unknown line ending string.'\n    return environment", "response": "Perform a sanity check on the environment."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new environment that shares all the data with the current environment.", "response": "def overlay(self, block_start_string=missing, block_end_string=missing,\n                variable_start_string=missing, variable_end_string=missing,\n                comment_start_string=missing, comment_end_string=missing,\n                line_statement_prefix=missing, line_comment_prefix=missing,\n                trim_blocks=missing, lstrip_blocks=missing,\n                extensions=missing, optimized=missing,\n                undefined=missing, finalize=missing, autoescape=missing,\n                loader=missing, cache_size=missing, auto_reload=missing,\n                bytecode_cache=missing):\n        \"\"\"Create a new overlay environment that shares all the data with the\n        current environment except for cache and the overridden attributes.\n        Extensions cannot be removed for an overlayed environment.  An overlayed\n        environment automatically gets all the extensions of the environment it\n        is linked to plus optional extra extensions.\n\n        Creating overlays should happen after the initial environment was set\n        up completely.  Not all attributes are truly linked, some are just\n        copied over so modifications on the original environment may not shine\n        through.\n        \"\"\"\n        args = dict(locals())\n        del args['self'], args['cache_size'], args['extensions']\n\n        rv = object.__new__(self.__class__)\n        rv.__dict__.update(self.__dict__)\n        rv.overlayed = True\n        rv.linked_to = self\n\n        for key, value in iteritems(args):\n            if value is not missing:\n                setattr(rv, key, value)\n\n        if cache_size is not missing:\n            rv.cache = create_cache(cache_size)\n        else:\n            rv.cache = copy_cache(self.cache)\n\n        rv.extensions = {}\n        for key, value in iteritems(self.extensions):\n            rv.extensions[key] = value.bind(rv)\n        if extensions is not missing:\n            rv.extensions.update(load_extensions(rv, extensions))\n\n        return _environment_sanity_check(rv)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\niterating over the extensions by priority.", "response": "def iter_extensions(self):\n        \"\"\"Iterates over the extensions by priority.\"\"\"\n        return iter(sorted(self.extensions.values(),\n                           key=lambda x: x.priority))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting an item or attribute of an object but prefer the attribute.", "response": "def getattr(self, obj, attribute):\n        \"\"\"Get an item or attribute of an object but prefer the attribute.\n        Unlike :meth:`getitem` the attribute *must* be a bytestring.\n        \"\"\"\n        try:\n            return getattr(obj, attribute)\n        except AttributeError:\n            pass\n        try:\n            return obj[attribute]\n        except (TypeError, LookupError, AttributeError):\n            return self.undefined(obj=obj, name=attribute)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef call_filter(self, name, value, args=None, kwargs=None,\n                    context=None, eval_ctx=None):\n        \"\"\"Invokes a filter on a value the same way the compiler does it.\n\n        Note that on Python 3 this might return a coroutine in case the\n        filter is running from an environment in async mode and the filter\n        supports async execution.  It's your responsibility to await this\n        if needed.\n\n        .. versionadded:: 2.7\n        \"\"\"\n        func = self.filters.get(name)\n        if func is None:\n            fail_for_missing_callable('no filter named %r', name)\n        args = [value] + list(args or ())\n        if getattr(func, 'contextfilter', False):\n            if context is None:\n                raise TemplateRuntimeError('Attempted to invoke context '\n                                           'filter without context')\n            args.insert(0, context)\n        elif getattr(func, 'evalcontextfilter', False):\n            if eval_ctx is None:\n                if context is not None:\n                    eval_ctx = context.eval_ctx\n                else:\n                    eval_ctx = EvalContext(self)\n            args.insert(0, eval_ctx)\n        elif getattr(func, 'environmentfilter', False):\n            args.insert(0, self)\n        return func(*args, **(kwargs or {}))", "response": "Invokes a filter on a value."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the sourcecode and return the abstract syntax tree.", "response": "def parse(self, source, name=None, filename=None):\n        \"\"\"Parse the sourcecode and return the abstract syntax tree.  This\n        tree of nodes is used by the compiler to convert the template into\n        executable source- or bytecode.  This is useful for debugging or to\n        extract information from templates.\n\n        If you are :ref:`developing Jinja2 extensions <writing-extensions>`\n        this gives you a good overview of the node tree generated.\n        \"\"\"\n        try:\n            return self._parse(source, name, filename)\n        except TemplateSyntaxError:\n            exc_info = sys.exc_info()\n        self.handle_exception(exc_info, source_hint=source)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _parse(self, source, name, filename):\n        return Parser(self, source, name, encode_filename(filename)).parse()", "response": "Internal parsing function used by parse and compile."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef lex(self, source, name=None, filename=None):\n        source = text_type(source)\n        try:\n            return self.lexer.tokeniter(source, name, filename)\n        except TemplateSyntaxError:\n            exc_info = sys.exc_info()\n        self.handle_exception(exc_info, source_hint=source)", "response": "Lex the given sourcecode and return a generator that yields tokens as tuples in the form of lineno token_type value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _tokenize(self, source, name, filename=None, state=None):\n        source = self.preprocess(source, name, filename)\n        stream = self.lexer.tokenize(source, name, filename, state)\n        for ext in self.iter_extensions():\n            stream = ext.filter_stream(stream)\n            if not isinstance(stream, TokenStream):\n                stream = TokenStream(stream, name, filename)\n        return stream", "response": "Returns a TokenStream object for the current language."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef compile(self, source, name=None, filename=None, raw=False,\n                defer_init=False):\n        \"\"\"Compile a node or template source code.  The `name` parameter is\n        the load name of the template after it was joined using\n        :meth:`join_path` if necessary, not the filename on the file system.\n        the `filename` parameter is the estimated filename of the template on\n        the file system.  If the template came from a database or memory this\n        can be omitted.\n\n        The return value of this method is a python code object.  If the `raw`\n        parameter is `True` the return value will be a string with python\n        code equivalent to the bytecode returned otherwise.  This method is\n        mainly used internally.\n\n        `defer_init` is use internally to aid the module code generator.  This\n        causes the generated code to be able to import without the global\n        environment variable to be set.\n\n        .. versionadded:: 2.4\n           `defer_init` parameter added.\n        \"\"\"\n        source_hint = None\n        try:\n            if isinstance(source, string_types):\n                source_hint = source\n                source = self._parse(source, name, filename)\n            source = self._generate(source, name, filename,\n                                    defer_init=defer_init)\n            if raw:\n                return source\n            if filename is None:\n                filename = '<template>'\n            else:\n                filename = encode_filename(filename)\n            return self._compile(source, filename)\n        except TemplateSyntaxError:\n            exc_info = sys.exc_info()\n        self.handle_exception(exc_info, source_hint=source_hint)", "response": "Compile a node or template source code."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_template(self, name, parent=None, globals=None):\n        if isinstance(name, Template):\n            return name\n        if parent is not None:\n            name = self.join_path(name, parent)\n        return self._load_template(name, self.make_globals(globals))", "response": "Load a template from the loader."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntry to load a template from the database.", "response": "def select_template(self, names, parent=None, globals=None):\n        \"\"\"Works like :meth:`get_template` but tries a number of templates\n        before it fails.  If it cannot find any of the templates, it will\n        raise a :exc:`TemplatesNotFound` exception.\n\n        .. versionadded:: 2.3\n\n        .. versionchanged:: 2.4\n           If `names` contains a :class:`Template` object it is returned\n           from the function unchanged.\n        \"\"\"\n        if not names:\n            raise TemplatesNotFound(message=u'Tried to select from an empty list '\n                                            u'of templates.')\n        globals = self.make_globals(globals)\n        for name in names:\n            if isinstance(name, Template):\n                return name\n            if parent is not None:\n                name = self.join_path(name, parent)\n            try:\n                return self._load_template(name, globals)\n            except TemplateNotFound:\n                pass\n        raise TemplatesNotFound(names)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_or_select_template(self, template_name_or_list,\n                               parent=None, globals=None):\n        \"\"\"Does a typecheck and dispatches to :meth:`select_template`\n        if an iterable of template names is given, otherwise to\n        :meth:`get_template`.\n\n        .. versionadded:: 2.3\n        \"\"\"\n        if isinstance(template_name_or_list, string_types):\n            return self.get_template(template_name_or_list, parent, globals)\n        elif isinstance(template_name_or_list, Template):\n            return template_name_or_list\n        return self.select_template(template_name_or_list, parent, globals)", "response": "Does a typecheck and dispatches to get_template or select_template."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_string(self, source, globals=None, template_class=None):\n        globals = self.make_globals(globals)\n        cls = template_class or self.template_class\n        return cls.from_code(self, self.compile(source), globals, None)", "response": "Load a template from a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_globals(self, d):\n        if not d:\n            return self.globals\n        return dict(self.globals, **d)", "response": "Return a dict for the globals."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_module_dict(cls, environment, module_dict, globals):\n        return cls._from_namespace(environment, module_dict, globals)", "response": "Creates a new object from a module dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef new_context(self, vars=None, shared=False, locals=None):\n        return new_context(self.environment, self.name, self.blocks,\n                           vars, shared, self.globals, locals)", "response": "Create a new context for this template."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_module(self, vars=None, shared=False, locals=None):\n        return TemplateModule(self, self.new_context(vars, shared, locals))", "response": "This method creates a new module with the given template variables and locals."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_corresponding_lineno(self, lineno):\n        for template_line, code_line in reversed(self.debug_info):\n            if code_line <= lineno:\n                return template_line\n        return 1", "response": "Return the source line number of a line number in the the\n            generated bytecode as they are not in sync."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_parsed_url(url):\n    # type: (S) -> Url\n    \"\"\"\n    This is a stand-in function for `urllib3.util.parse_url`\n\n    The orignal function doesn't handle special characters very well, this simply splits\n    out the authentication section, creates the parsed url, then puts the authentication\n    section back in, bypassing validation.\n\n    :return: The new, parsed URL object\n    :rtype: :class:`~urllib3.util.url.Url`\n    \"\"\"\n\n    try:\n        parsed = urllib3_parse(url)\n    except ValueError:\n        scheme, _, url = url.partition(\"://\")\n        auth, _, url = url.rpartition(\"@\")\n        url = \"{scheme}://{url}\".format(scheme=scheme, url=url)\n        parsed = urllib3_parse(url)._replace(auth=auth)\n    return parsed", "response": "This function is a stand - in function for urllib3. util. parse_url."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef remove_password_from_url(url):\n    # type: (S) -> S\n    \"\"\"\n    Given a url, remove the password and insert 4 dashes\n\n    :param url: The url to replace the authentication in\n    :type url: S\n    :return: The new URL without authentication\n    :rtype: S\n    \"\"\"\n\n    parsed = _get_parsed_url(url)\n    if parsed.auth:\n        auth, _, _ = parsed.auth.partition(\":\")\n        return parsed._replace(auth=\"{auth}:----\".format(auth=auth)).url\n    return parsed.url", "response": "Given a url remove the password and insert 4 dashes in\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nensure that the environment is good for unicode on Python 3.", "response": "def _verify_python3_env():\n    \"\"\"Ensures that the environment is good for unicode on Python 3.\"\"\"\n    if PY2:\n        return\n    try:\n        import locale\n        fs_enc = codecs.lookup(locale.getpreferredencoding()).name\n    except Exception:\n        fs_enc = 'ascii'\n    if fs_enc != 'ascii':\n        return\n\n    extra = ''\n    if os.name == 'posix':\n        import subprocess\n        try:\n            rv = subprocess.Popen(['locale', '-a'], stdout=subprocess.PIPE,\n                                  stderr=subprocess.PIPE).communicate()[0]\n        except OSError:\n            rv = b''\n        good_locales = set()\n        has_c_utf8 = False\n\n        # Make sure we're operating on text here.\n        if isinstance(rv, bytes):\n            rv = rv.decode('ascii', 'replace')\n\n        for line in rv.splitlines():\n            locale = line.strip()\n            if locale.lower().endswith(('.utf-8', '.utf8')):\n                good_locales.add(locale)\n                if locale.lower() in ('c.utf8', 'c.utf-8'):\n                    has_c_utf8 = True\n\n        extra += '\\n\\n'\n        if not good_locales:\n            extra += (\n                'Additional information: on this system no suitable UTF-8\\n'\n                'locales were discovered.  This most likely requires resolving\\n'\n                'by reconfiguring the locale system.'\n            )\n        elif has_c_utf8:\n            extra += (\n                'This system supports the C.UTF-8 locale which is recommended.\\n'\n                'You might be able to resolve your issue by exporting the\\n'\n                'following environment variables:\\n\\n'\n                '    export LC_ALL=C.UTF-8\\n'\n                '    export LANG=C.UTF-8'\n            )\n        else:\n            extra += (\n                'This system lists a couple of UTF-8 supporting locales that\\n'\n                'you can pick from.  The following suitable locales were\\n'\n                'discovered: %s'\n            ) % ', '.join(sorted(good_locales))\n\n        bad_locale = None\n        for locale in os.environ.get('LC_ALL'), os.environ.get('LANG'):\n            if locale and locale.lower().endswith(('.utf-8', '.utf8')):\n                bad_locale = locale\n            if locale is not None:\n                break\n        if bad_locale is not None:\n            extra += (\n                '\\n\\nClick discovered that you exported a UTF-8 locale\\n'\n                'but the locale system could not pick up from it because\\n'\n                'it does not exist.  The exported locale is \"%s\" but it\\n'\n                'is not supported'\n            ) % bad_locale\n\n    raise RuntimeError(\n        'Click will abort further execution because Python 3 was'\n        ' configured to use ASCII as encoding for the environment.'\n        ' Consult https://click.palletsprojects.com/en/7.x/python3/ for'\n        ' mitigation steps.' + extra\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rmtree_errorhandler(func, path, exc_info):\n    # if file type currently read only\n    if os.stat(path).st_mode & stat.S_IREAD:\n        # convert to read/write\n        os.chmod(path, stat.S_IWRITE)\n        # use the original function to repeat the operation\n        func(path)\n        return\n    else:\n        raise", "response": "This function is used to remove the directories in the current directory."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngiving the display value for a given path making it relative to cwd if possible.", "response": "def display_path(path):\n    # type: (Union[str, Text]) -> str\n    \"\"\"Gives the display value for a given path, making it relative to cwd\n    if possible.\"\"\"\n    path = os.path.normcase(os.path.abspath(path))\n    if sys.version_info[0] == 2:\n        path = path.decode(sys.getfilesystemencoding(), 'replace')\n        path = path.encode(sys.getdefaultencoding(), 'replace')\n    if path.startswith(os.getcwd() + os.path.sep):\n        path = '.' + path[len(os.getcwd()):]\n    return path"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_installable_dir(path):\n    # type: (str) -> bool\n    \"\"\"Is path is a directory containing setup.py or pyproject.toml?\n    \"\"\"\n    if not os.path.isdir(path):\n        return False\n    setup_py = os.path.join(path, 'setup.py')\n    if os.path.isfile(setup_py):\n        return True\n    pyproject_toml = os.path.join(path, 'pyproject.toml')\n    if os.path.isfile(pyproject_toml):\n        return True\n    return False", "response": "Checks if a directory is a directory containing setup. py or pyproject. toml."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning true if the page appears to be the svn index page of an svn repository otherwise returns False.", "response": "def is_svn_page(html):\n    # type: (Union[str, Text]) -> Optional[Match[Union[str, Text]]]\n    \"\"\"\n    Returns true if the page appears to be the index page of an svn repository\n    \"\"\"\n    return (re.search(r'<title>[^<]*Revision \\d+:', html) and\n            re.search(r'Powered by (?:<a[^>]*?>)?Subversion', html, re.I))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nyield pieces of data from a file - like object until EOF.", "response": "def read_chunks(file, size=io.DEFAULT_BUFFER_SIZE):\n    \"\"\"Yield pieces of data from a file-like object until EOF.\"\"\"\n    while True:\n        chunk = file.read(size)\n        if not chunk:\n            break\n        yield chunk"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef normalize_path(path, resolve_symlinks=True):\n    # type: (str, bool) -> str\n    \"\"\"\n    Convert a path to its canonical, case-normalized, absolute version.\n\n    \"\"\"\n    path = expanduser(path)\n    if resolve_symlinks:\n        path = os.path.realpath(path)\n    else:\n        path = os.path.abspath(path)\n    return os.path.normcase(path)", "response": "Converts a path to its canonical case - normalized absolute version."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlike os. path. splitext but take off. tar too", "response": "def splitext(path):\n    # type: (str) -> Tuple[str, str]\n    \"\"\"Like os.path.splitext, but take off .tar too\"\"\"\n    base, ext = posixpath.splitext(path)\n    if base.lower().endswith('.tar'):\n        ext = base[-4:] + ext\n        base = base[:-4]\n    return base, ext"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlike os. renames but handles renaming across devices.", "response": "def renames(old, new):\n    # type: (str, str) -> None\n    \"\"\"Like os.renames(), but handles renaming across devices.\"\"\"\n    # Implementation borrowed from os.renames().\n    head, tail = os.path.split(new)\n    if head and tail and not os.path.exists(head):\n        os.makedirs(head)\n\n    shutil.move(old, new)\n\n    head, tail = os.path.split(old)\n    if head and tail:\n        try:\n            os.removedirs(head)\n        except OSError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn True if path is within sys. prefix.", "response": "def is_local(path):\n    # type: (str) -> bool\n    \"\"\"\n    Return True if path is within sys.prefix, if we're running in a virtualenv.\n\n    If we're not in a virtualenv, all paths are considered \"local.\"\n\n    \"\"\"\n    if not running_under_virtualenv():\n        return True\n    return normalize_path(path).startswith(normalize_path(sys.prefix))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn True if given Distribution is an editable install.", "response": "def dist_is_editable(dist):\n    # type: (Distribution) -> bool\n    \"\"\"\n    Return True if given Distribution is an editable install.\n    \"\"\"\n    for path_item in sys.path:\n        egg_link = os.path.join(path_item, dist.project_name + '.egg-link')\n        if os.path.isfile(egg_link):\n            return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_installed_distributions(local_only=True,\n                                skip=stdlib_pkgs,\n                                include_editables=True,\n                                editables_only=False,\n                                user_only=False):\n    # type: (bool, Container[str], bool, bool, bool) -> List[Distribution]\n    \"\"\"\n    Return a list of installed Distribution objects.\n\n    If ``local_only`` is True (default), only return installations\n    local to the current virtualenv, if in a virtualenv.\n\n    ``skip`` argument is an iterable of lower-case project names to\n    ignore; defaults to stdlib_pkgs\n\n    If ``include_editables`` is False, don't report editables.\n\n    If ``editables_only`` is True , only report editables.\n\n    If ``user_only`` is True , only report installations in the user\n    site directory.\n\n    \"\"\"\n    if local_only:\n        local_test = dist_is_local\n    else:\n        def local_test(d):\n            return True\n\n    if include_editables:\n        def editable_test(d):\n            return True\n    else:\n        def editable_test(d):\n            return not dist_is_editable(d)\n\n    if editables_only:\n        def editables_only_test(d):\n            return dist_is_editable(d)\n    else:\n        def editables_only_test(d):\n            return True\n\n    if user_only:\n        user_test = dist_in_usersite\n    else:\n        def user_test(d):\n            return True\n\n    # because of pkg_resources vendoring, mypy cannot find stub in typeshed\n    return [d for d in pkg_resources.working_set  # type: ignore\n            if local_test(d) and\n            d.key not in skip and\n            editable_test(d) and\n            editables_only_test(d) and\n            user_test(d)\n            ]", "response": "Returns a list of installed distributions."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef egg_link_path(dist):\n    # type: (Distribution) -> Optional[str]\n    \"\"\"\n    Return the path for the .egg-link file if it exists, otherwise, None.\n\n    There's 3 scenarios:\n    1) not in a virtualenv\n       try to find in site.USER_SITE, then site_packages\n    2) in a no-global virtualenv\n       try to find in site_packages\n    3) in a yes-global virtualenv\n       try to find in site_packages, then site.USER_SITE\n       (don't look in global location)\n\n    For #1 and #3, there could be odd cases, where there's an egg-link in 2\n    locations.\n\n    This method will just return the first one found.\n    \"\"\"\n    sites = []\n    if running_under_virtualenv():\n        if virtualenv_no_global():\n            sites.append(site_packages)\n        else:\n            sites.append(site_packages)\n            if user_site:\n                sites.append(user_site)\n    else:\n        if user_site:\n            sites.append(user_site)\n        sites.append(site_packages)\n\n    for site in sites:\n        egglink = os.path.join(site, dist.project_name) + '.egg-link'\n        if os.path.isfile(egglink):\n            return egglink\n    return None", "response": "Returns the path to the. egg - link file if it exists otherwise None."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nunzips the file with path filename to the destination location.", "response": "def unzip_file(filename, location, flatten=True):\n    # type: (str, str, bool) -> None\n    \"\"\"\n    Unzip the file (with path `filename`) to the destination `location`.  All\n    files are written based on system defaults and umask (i.e. permissions are\n    not preserved), except that regular file members with any execute\n    permissions (user, group, or world) have \"chmod +x\" applied after being\n    written. Note that for windows, any execute changes using os.chmod are\n    no-ops per the python docs.\n    \"\"\"\n    ensure_dir(location)\n    zipfp = open(filename, 'rb')\n    try:\n        zip = zipfile.ZipFile(zipfp, allowZip64=True)\n        leading = has_leading_dir(zip.namelist()) and flatten\n        for info in zip.infolist():\n            name = info.filename\n            fn = name\n            if leading:\n                fn = split_leading_dir(name)[1]\n            fn = os.path.join(location, fn)\n            dir = os.path.dirname(fn)\n            if fn.endswith('/') or fn.endswith('\\\\'):\n                # A directory\n                ensure_dir(fn)\n            else:\n                ensure_dir(dir)\n                # Don't use read() to avoid allocating an arbitrarily large\n                # chunk of memory for the file's content\n                fp = zip.open(name)\n                try:\n                    with open(fn, 'wb') as destfp:\n                        shutil.copyfileobj(fp, destfp)\n                finally:\n                    fp.close()\n                    mode = info.external_attr >> 16\n                    # if mode and regular file and any execute permissions for\n                    # user/group/world?\n                    if mode and stat.S_ISREG(mode) and mode & 0o111:\n                        # make dest file have execute for user/group/world\n                        # (chmod +x) no-op on windows per python docs\n                        os.chmod(fn, (0o777 - current_umask() | 0o111))\n    finally:\n        zipfp.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef call_subprocess(\n    cmd,  # type: List[str]\n    show_stdout=True,  # type: bool\n    cwd=None,  # type: Optional[str]\n    on_returncode='raise',  # type: str\n    extra_ok_returncodes=None,  # type: Optional[Iterable[int]]\n    command_desc=None,  # type: Optional[str]\n    extra_environ=None,  # type: Optional[Mapping[str, Any]]\n    unset_environ=None,  # type: Optional[Iterable[str]]\n    spinner=None  # type: Optional[SpinnerInterface]\n):\n    # type: (...) -> Optional[Text]\n    \"\"\"\n    Args:\n      extra_ok_returncodes: an iterable of integer return codes that are\n        acceptable, in addition to 0. Defaults to None, which means [].\n      unset_environ: an iterable of environment variable names to unset\n        prior to calling subprocess.Popen().\n    \"\"\"\n    if extra_ok_returncodes is None:\n        extra_ok_returncodes = []\n    if unset_environ is None:\n        unset_environ = []\n    # This function's handling of subprocess output is confusing and I\n    # previously broke it terribly, so as penance I will write a long comment\n    # explaining things.\n    #\n    # The obvious thing that affects output is the show_stdout=\n    # kwarg. show_stdout=True means, let the subprocess write directly to our\n    # stdout. Even though it is nominally the default, it is almost never used\n    # inside pip (and should not be used in new code without a very good\n    # reason); as of 2016-02-22 it is only used in a few places inside the VCS\n    # wrapper code. Ideally we should get rid of it entirely, because it\n    # creates a lot of complexity here for a rarely used feature.\n    #\n    # Most places in pip set show_stdout=False. What this means is:\n    # - We connect the child stdout to a pipe, which we read.\n    # - By default, we hide the output but show a spinner -- unless the\n    #   subprocess exits with an error, in which case we show the output.\n    # - If the --verbose option was passed (= loglevel is DEBUG), then we show\n    #   the output unconditionally. (But in this case we don't want to show\n    #   the output a second time if it turns out that there was an error.)\n    #\n    # stderr is always merged with stdout (even if show_stdout=True).\n    if show_stdout:\n        stdout = None\n    else:\n        stdout = subprocess.PIPE\n    if command_desc is None:\n        cmd_parts = []\n        for part in cmd:\n            if ' ' in part or '\\n' in part or '\"' in part or \"'\" in part:\n                part = '\"%s\"' % part.replace('\"', '\\\\\"')\n            cmd_parts.append(part)\n        command_desc = ' '.join(cmd_parts)\n    logger.debug(\"Running command %s\", command_desc)\n    env = os.environ.copy()\n    if extra_environ:\n        env.update(extra_environ)\n    for name in unset_environ:\n        env.pop(name, None)\n    try:\n        proc = subprocess.Popen(\n            cmd, stderr=subprocess.STDOUT, stdin=subprocess.PIPE,\n            stdout=stdout, cwd=cwd, env=env,\n        )\n        proc.stdin.close()\n    except Exception as exc:\n        logger.critical(\n            \"Error %s while executing command %s\", exc, command_desc,\n        )\n        raise\n    all_output = []\n    if stdout is not None:\n        while True:\n            line = console_to_str(proc.stdout.readline())\n            if not line:\n                break\n            line = line.rstrip()\n            all_output.append(line + '\\n')\n            if logger.getEffectiveLevel() <= std_logging.DEBUG:\n                # Show the line immediately\n                logger.debug(line)\n            else:\n                # Update the spinner\n                if spinner is not None:\n                    spinner.spin()\n    try:\n        proc.wait()\n    finally:\n        if proc.stdout:\n            proc.stdout.close()\n    if spinner is not None:\n        if proc.returncode:\n            spinner.finish(\"error\")\n        else:\n            spinner.finish(\"done\")\n    if proc.returncode and proc.returncode not in extra_ok_returncodes:\n        if on_returncode == 'raise':\n            if (logger.getEffectiveLevel() > std_logging.DEBUG and\n                    not show_stdout):\n                logger.info(\n                    'Complete output from command %s:', command_desc,\n                )\n                logger.info(\n                    ''.join(all_output) +\n                    '\\n----------------------------------------'\n                )\n            raise InstallationError(\n                'Command \"%s\" failed with error code %s in %s'\n                % (command_desc, proc.returncode, cwd))\n        elif on_returncode == 'warn':\n            logger.warning(\n                'Command \"%s\" had error code %s in %s',\n                command_desc, proc.returncode, cwd,\n            )\n        elif on_returncode == 'ignore':\n            pass\n        else:\n            raise ValueError('Invalid value: on_returncode=%s' %\n                             repr(on_returncode))\n    if not show_stdout:\n        return ''.join(all_output)\n    return None", "response": "Calls a subprocess and returns the output of the child process."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread the contents of filename.", "response": "def read_text_file(filename):\n    # type: (str) -> str\n    \"\"\"Return the contents of *filename*.\n\n    Try to decode the file contents with utf-8, the preferred system encoding\n    (e.g., cp1252 on some Windows machines), and latin1, in that order.\n    Decoding a byte string with latin1 will never raise an error. In the worst\n    case, the returned string will contain some garbage characters.\n\n    \"\"\"\n    with open(filename, 'rb') as fp:\n        data = fp.read()\n\n    encodings = ['utf-8', locale.getpreferredencoding(False), 'latin1']\n    for enc in encodings:\n        try:\n            # https://github.com/python/mypy/issues/1174\n            data = data.decode(enc)  # type: ignore\n        except UnicodeDecodeError:\n            continue\n        break\n\n    assert not isinstance(data, bytes)  # Latin1 should have worked.\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a context manager that temporarily replaces the sys stream stream_name with a StringIO.", "response": "def captured_output(stream_name):\n    \"\"\"Return a context manager used by captured_stdout/stdin/stderr\n    that temporarily replaces the sys stream *stream_name* with a StringIO.\n\n    Taken from Lib/support/__init__.py in the CPython repo.\n    \"\"\"\n    orig_stdout = getattr(sys, stream_name)\n    setattr(sys, stream_name, StreamWrapper.from_stream(orig_stdout))\n    try:\n        yield getattr(sys, stream_name)\n    finally:\n        setattr(sys, stream_name, orig_stdout)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_installed_version(dist_name, working_set=None):\n    # Create a requirement that we'll look for inside of setuptools.\n    req = pkg_resources.Requirement.parse(dist_name)\n\n    if working_set is None:\n        # We want to avoid having this cached, so we need to construct a new\n        # working set each time.\n        working_set = pkg_resources.WorkingSet()\n\n    # Get the installed distribution from our working set\n    dist = working_set.find(req)\n\n    # Check to see if we got an installed distribution or not, if we did\n    # we want to return it's version.\n    return dist.version if dist else None", "response": "Get the installed version of dist_name avoiding pkg_resources cache"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_vcs_requirement_url(repo_url, rev, project_name, subdir=None):\n    egg_project_name = pkg_resources.to_filename(project_name)\n    req = '{}@{}#egg={}'.format(repo_url, rev, egg_project_name)\n    if subdir:\n        req += '&subdirectory={}'.format(subdir)\n\n    return req", "response": "Returns the URL for a VCS requirement."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef split_auth_from_netloc(netloc):\n    if '@' not in netloc:\n        return netloc, (None, None)\n\n    # Split from the right because that's how urllib.parse.urlsplit()\n    # behaves if more than one @ is present (which can be checked using\n    # the password attribute of urlsplit()'s return value).\n    auth, netloc = netloc.rsplit('@', 1)\n    if ':' in auth:\n        # Split from the left because that's how urllib.parse.urlsplit()\n        # behaves if more than one : is present (which again can be checked\n        # using the password attribute of the return value)\n        user_pass = auth.split(':', 1)\n    else:\n        user_pass = auth, None\n\n    user_pass = tuple(\n        None if x is None else urllib_unquote(x) for x in user_pass\n    )\n\n    return netloc, user_pass", "response": "Parse out and remove the auth information from a netloc."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreplaces the password in a netloc with \"****\", if it exists. For example, \"user:pass@example.com\" returns \"user:****@example.com\".", "response": "def redact_netloc(netloc):\n    # type: (str) -> str\n    \"\"\"\n    Replace the password in a netloc with \"****\", if it exists.\n\n    For example, \"user:pass@example.com\" returns \"user:****@example.com\".\n    \"\"\"\n    netloc, (user, password) = split_auth_from_netloc(netloc)\n    if user is None:\n        return netloc\n    password = '' if password is None else ':****'\n    return '{user}{password}@{netloc}'.format(user=urllib_parse.quote(user),\n                                              password=password,\n                                              netloc=netloc)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef protect_pip_from_modification_on_windows(modifying_pip):\n    pip_names = [\n        \"pip.exe\",\n        \"pip{}.exe\".format(sys.version_info[0]),\n        \"pip{}.{}.exe\".format(*sys.version_info[:2])\n    ]\n\n    # See https://github.com/pypa/pip/issues/1299 for more discussion\n    should_show_use_python_msg = (\n        modifying_pip and\n        WINDOWS and\n        os.path.basename(sys.argv[0]) in pip_names\n    )\n\n    if should_show_use_python_msg:\n        new_command = [\n            sys.executable, \"-m\", \"pip\"\n        ] + sys.argv[1:]\n        raise CommandError(\n            'To modify pip, please run the following command:\\n{}'\n            .format(\" \".join(new_command))\n        )", "response": "Protect pip. exe from modification on Windows\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_requires_python(requires_python):\n    # type: (Optional[str]) -> bool\n    \"\"\"\n    Check if the python version in use match the `requires_python` specifier.\n\n    Returns `True` if the version of python in use matches the requirement.\n    Returns `False` if the version of python in use does not matches the\n    requirement.\n\n    Raises an InvalidSpecifier if `requires_python` have an invalid format.\n    \"\"\"\n    if requires_python is None:\n        # The package provides no information\n        return True\n    requires_python_specifier = specifiers.SpecifierSet(requires_python)\n\n    # We only use major.minor.micro\n    python_version = version.parse('{0}.{1}.{2}'.format(*sys.version_info[:3]))\n    return python_version in requires_python_specifier", "response": "Checks if the python version in use matches the requirements of the base package."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef init(complete_options=False, match_incomplete=None):\n    global _initialized\n    if not _initialized:\n        _patch()\n        completion_configuration.complete_options = complete_options\n        if match_incomplete is not None:\n            completion_configuration.match_incomplete = match_incomplete\n        _initialized = True", "response": "Initialize the enhanced click completion\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef transform_hits(hits):\n    packages = OrderedDict()\n    for hit in hits:\n        name = hit['name']\n        summary = hit['summary']\n        version = hit['version']\n\n        if name not in packages.keys():\n            packages[name] = {\n                'name': name,\n                'summary': summary,\n                'versions': [version],\n            }\n        else:\n            packages[name]['versions'].append(version)\n\n            # if this is the highest version, replace summary and score\n            if version == highest_version(packages[name]['versions']):\n                packages[name]['summary'] = summary\n\n    return list(packages.values())", "response": "Transform a list of hits into a list of packages with the list of versions stored inline."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding install_req as a requirement to the set of unnamed dependencies.", "response": "def add_requirement(\n        self,\n        install_req,  # type: InstallRequirement\n        parent_req_name=None,  # type: Optional[str]\n        extras_requested=None  # type: Optional[Iterable[str]]\n    ):\n        # type: (...) -> Tuple[List[InstallRequirement], Optional[InstallRequirement]]  # noqa: E501\n        \"\"\"Add install_req as a requirement to install.\n\n        :param parent_req_name: The name of the requirement that needed this\n            added. The name is used because when multiple unnamed requirements\n            resolve to the same name, we could otherwise end up with dependency\n            links that point outside the Requirements set. parent_req must\n            already be added. Note that None implies that this is a user\n            supplied requirement, vs an inferred one.\n        :param extras_requested: an iterable of extras used to evaluate the\n            environment markers.\n        :return: Additional requirements to scan. That is either [] if\n            the requirement is not applicable, or [install_req] if the\n            requirement is applicable and has just been added.\n        \"\"\"\n        name = install_req.name\n\n        # If the markers do not match, ignore this requirement.\n        if not install_req.match_markers(extras_requested):\n            logger.info(\n                \"Ignoring %s: markers '%s' don't match your environment\",\n                name, install_req.markers,\n            )\n            return [], None\n\n        # If the wheel is not supported, raise an error.\n        # Should check this after filtering out based on environment markers to\n        # allow specifying different wheels based on the environment/OS, in a\n        # single requirements file.\n        if install_req.link and install_req.link.is_wheel:\n            wheel = Wheel(install_req.link.filename)\n            if self.check_supported_wheels and not wheel.supported():\n                raise InstallationError(\n                    \"%s is not a supported wheel on this platform.\" %\n                    wheel.filename\n                )\n\n        # This next bit is really a sanity check.\n        assert install_req.is_direct == (parent_req_name is None), (\n            \"a direct req shouldn't have a parent and also, \"\n            \"a non direct req should have a parent\"\n        )\n\n        # Unnamed requirements are scanned again and the requirement won't be\n        # added as a dependency until after scanning.\n        if not name:\n            # url or path requirement w/o an egg fragment\n            self.unnamed_requirements.append(install_req)\n            return [install_req], None\n\n        try:\n            existing_req = self.get_requirement(name)\n        except KeyError:\n            existing_req = None\n\n        has_conflicting_requirement = (\n            parent_req_name is None and\n            existing_req and\n            not existing_req.constraint and\n            existing_req.extras == install_req.extras and\n            existing_req.req.specifier != install_req.req.specifier\n        )\n        if has_conflicting_requirement:\n            raise InstallationError(\n                \"Double requirement given: %s (already in %s, name=%r)\"\n                % (install_req, existing_req, name)\n            )\n\n        # When no existing requirement exists, add the requirement as a\n        # dependency and it will be scanned again after.\n        if not existing_req:\n            self.requirements[name] = install_req\n            # FIXME: what about other normalizations?  E.g., _ vs. -?\n            if name.lower() != name:\n                self.requirement_aliases[name.lower()] = name\n            # We'd want to rescan this requirements later\n            return [install_req], install_req\n\n        # Assume there's no need to scan, and that we've already\n        # encountered this for scanning.\n        if install_req.constraint or not existing_req.constraint:\n            return [], existing_req\n\n        does_not_satisfy_constraint = (\n            install_req.link and\n            not (\n                existing_req.link and\n                install_req.link.path == existing_req.link.path\n            )\n        )\n        if does_not_satisfy_constraint:\n            self.reqs_to_cleanup.append(install_req)\n            raise InstallationError(\n                \"Could not satisfy constraints for '%s': \"\n                \"installation from path or url cannot be \"\n                \"constrained to a version\" % name,\n            )\n        # If we're now installing a constraint, mark the existing\n        # object for real installation.\n        existing_req.constraint = False\n        existing_req.extras = tuple(sorted(\n            set(existing_req.extras) | set(install_req.extras)\n        ))\n        logger.debug(\n            \"Setting %s extras to: %s\",\n            existing_req, existing_req.extras,\n        )\n        # Return the existing requirement for addition to the parent and\n        # scanning again.\n        return [existing_req], existing_req"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nresolving what operations need to be done", "response": "def resolve(self, requirement_set):\n        # type: (RequirementSet) -> None\n        \"\"\"Resolve what operations need to be done\n\n        As a side-effect of this method, the packages (and their dependencies)\n        are downloaded, unpacked and prepared for installation. This\n        preparation is done by ``pip.operations.prepare``.\n\n        Once PyPI has static dependency metadata available, it would be\n        possible to move the preparation to become a step separated from\n        dependency resolution.\n        \"\"\"\n        # make the wheelhouse\n        if self.preparer.wheel_download_dir:\n            ensure_dir(self.preparer.wheel_download_dir)\n\n        # If any top-level requirement has a hash specified, enter\n        # hash-checking mode, which requires hashes from all.\n        root_reqs = (\n            requirement_set.unnamed_requirements +\n            list(requirement_set.requirements.values())\n        )\n        self.require_hashes = (\n            requirement_set.require_hashes or\n            any(req.has_hash_options for req in root_reqs)\n        )\n\n        # Display where finder is looking for packages\n        locations = self.finder.get_formatted_locations()\n        if locations:\n            logger.info(locations)\n\n        # Actually prepare the files, and collect any exceptions. Most hash\n        # exceptions cannot be checked ahead of time, because\n        # req.populate_link() needs to be called before we can make decisions\n        # based on link type.\n        discovered_reqs = []  # type: List[InstallRequirement]\n        hash_errors = HashErrors()\n        for req in chain(root_reqs, discovered_reqs):\n            try:\n                discovered_reqs.extend(\n                    self._resolve_one(requirement_set, req)\n                )\n            except HashError as exc:\n                exc.req = req\n                hash_errors.append(exc)\n\n        if hash_errors:\n            raise hash_errors"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _set_req_to_reinstall(self, req):\n        # type: (InstallRequirement) -> None\n        \"\"\"\n        Set a requirement to be installed.\n        \"\"\"\n        # Don't uninstall the conflict if doing a user install and the\n        # conflict is not a user install.\n        if not self.use_user_site or dist_in_usersite(req.satisfied_by):\n            req.conflicts_with = req.satisfied_by\n        req.satisfied_by = None", "response": "Sets a requirement to be installed and uninstalls the conflict if needed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if the install requirement should be skipped.", "response": "def _check_skip_installed(self, req_to_install):\n        # type: (InstallRequirement) -> Optional[str]\n        \"\"\"Check if req_to_install should be skipped.\n\n        This will check if the req is installed, and whether we should upgrade\n        or reinstall it, taking into account all the relevant user options.\n\n        After calling this req_to_install will only have satisfied_by set to\n        None if the req_to_install is to be upgraded/reinstalled etc. Any\n        other value will be a dist recording the current thing installed that\n        satisfies the requirement.\n\n        Note that for vcs urls and the like we can't assess skipping in this\n        routine - we simply identify that we need to pull the thing down,\n        then later on it is pulled down and introspected to assess upgrade/\n        reinstalls etc.\n\n        :return: A text reason for why it was skipped, or None.\n        \"\"\"\n        if self.ignore_installed:\n            return None\n\n        req_to_install.check_if_exists(self.use_user_site)\n        if not req_to_install.satisfied_by:\n            return None\n\n        if self.force_reinstall:\n            self._set_req_to_reinstall(req_to_install)\n            return None\n\n        if not self._is_upgrade_allowed(req_to_install):\n            if self.upgrade_strategy == \"only-if-needed\":\n                return 'already satisfied, skipping upgrade'\n            return 'already satisfied'\n\n        # Check for the possibility of an upgrade.  For link-based\n        # requirements we have to pull the tree down and inspect to assess\n        # the version #, so it's handled way down.\n        if not req_to_install.link:\n            try:\n                self.finder.find_requirement(req_to_install, upgrade=True)\n            except BestVersionAlreadyInstalled:\n                # Then the best version is installed.\n                return 'already up-to-date'\n            except DistributionNotFound:\n                # No distribution found, so we squash the error.  It will\n                # be raised later when we re-try later to do the install.\n                # Why don't we just raise here?\n                pass\n\n        self._set_req_to_reinstall(req_to_install)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_abstract_dist_for(self, req):\n        # type: (InstallRequirement) -> DistAbstraction\n        \"\"\"Takes a InstallRequirement and returns a single AbstractDist \\\n        representing a prepared variant of the same.\n        \"\"\"\n        assert self.require_hashes is not None, (\n            \"require_hashes should have been set in Resolver.resolve()\"\n        )\n\n        if req.editable:\n            return self.preparer.prepare_editable_requirement(\n                req, self.require_hashes, self.use_user_site, self.finder,\n            )\n\n        # satisfied_by is only evaluated by calling _check_skip_installed,\n        # so it must be None here.\n        assert req.satisfied_by is None\n        skip_reason = self._check_skip_installed(req)\n\n        if req.satisfied_by:\n            return self.preparer.prepare_installed_requirement(\n                req, self.require_hashes, skip_reason\n            )\n\n        upgrade_allowed = self._is_upgrade_allowed(req)\n        abstract_dist = self.preparer.prepare_linked_requirement(\n            req, self.session, self.finder, upgrade_allowed,\n            self.require_hashes\n        )\n\n        # NOTE\n        # The following portion is for determining if a certain package is\n        # going to be re-installed/upgraded or not and reporting to the user.\n        # This should probably get cleaned up in a future refactor.\n\n        # req.req is only avail after unpack for URL\n        # pkgs repeat check_if_exists to uninstall-on-upgrade\n        # (#14)\n        if not self.ignore_installed:\n            req.check_if_exists(self.use_user_site)\n\n        if req.satisfied_by:\n            should_modify = (\n                self.upgrade_strategy != \"to-satisfy-only\" or\n                self.force_reinstall or\n                self.ignore_installed or\n                req.link.scheme == 'file'\n            )\n            if should_modify:\n                self._set_req_to_reinstall(req)\n            else:\n                logger.info(\n                    'Requirement already satisfied (use --upgrade to upgrade):'\n                    ' %s', req,\n                )\n\n        return abstract_dist", "response": "Takes a InstallRequirement and returns a single AbstractDist representing a prepared variant of the same."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprepares a single requirement file and return a list of InstallRequirements to also install.", "response": "def _resolve_one(\n        self,\n        requirement_set,  # type: RequirementSet\n        req_to_install,  # type: InstallRequirement\n        ignore_requires_python=False  # type: bool\n    ):\n        # type: (...) -> List[InstallRequirement]\n        \"\"\"Prepare a single requirements file.\n\n        :return: A list of additional InstallRequirements to also install.\n        \"\"\"\n        # Tell user what we are doing for this requirement:\n        # obtain (editable), skipping, processing (local url), collecting\n        # (remote url or package name)\n        if req_to_install.constraint or req_to_install.prepared:\n            return []\n\n        req_to_install.prepared = True\n\n        # register tmp src for cleanup in case something goes wrong\n        requirement_set.reqs_to_cleanup.append(req_to_install)\n\n        abstract_dist = self._get_abstract_dist_for(req_to_install)\n\n        # Parse and return dependencies\n        dist = abstract_dist.dist()\n        try:\n            check_dist_requires_python(dist)\n        except UnsupportedPythonVersion as err:\n            if self.ignore_requires_python or ignore_requires_python or self.ignore_compatibility:\n                logger.warning(err.args[0])\n            else:\n                raise\n\n        # A huge hack, by Kenneth Reitz.\n        try:\n            self.requires_python = check_dist_requires_python(dist, absorb=False)\n        except TypeError:\n            self.requires_python = None\n\n\n        more_reqs = []  # type: List[InstallRequirement]\n\n        def add_req(subreq, extras_requested):\n            sub_install_req = install_req_from_req_string(\n                str(subreq),\n                req_to_install,\n                isolated=self.isolated,\n                wheel_cache=self.wheel_cache,\n                use_pep517=self.use_pep517\n            )\n            parent_req_name = req_to_install.name\n            to_scan_again, add_to_parent = requirement_set.add_requirement(\n                sub_install_req,\n                parent_req_name=parent_req_name,\n                extras_requested=extras_requested,\n            )\n            if parent_req_name and add_to_parent:\n                self._discovered_dependencies[parent_req_name].append(\n                    add_to_parent\n                )\n            more_reqs.extend(to_scan_again)\n\n        with indent_log():\n            # We add req_to_install before its dependencies, so that we\n            # can refer to it when adding dependencies.\n            if not requirement_set.has_requirement(req_to_install.name):\n                available_requested = sorted(\n                    set(dist.extras) & set(req_to_install.extras)\n                )\n                # 'unnamed' requirements will get added here\n                req_to_install.is_direct = True\n                requirement_set.add_requirement(\n                    req_to_install, parent_req_name=None,\n                    extras_requested=available_requested,\n                )\n\n            if not self.ignore_dependencies:\n                if req_to_install.extras:\n                    logger.debug(\n                        \"Installing extra requirements: %r\",\n                        ','.join(req_to_install.extras),\n                    )\n                missing_requested = sorted(\n                    set(req_to_install.extras) - set(dist.extras)\n                )\n                for missing in missing_requested:\n                    logger.warning(\n                        '%s does not provide the extra \\'%s\\'',\n                        dist, missing\n                    )\n\n                available_requested = sorted(\n                    set(dist.extras) & set(req_to_install.extras)\n                )\n                for subreq in dist.requires(available_requested):\n                    add_req(subreq, extras_requested=available_requested)\n\n                # Hack for deep-resolving extras.\n                for available in available_requested:\n                    if hasattr(dist, '_DistInfoDistribution__dep_map'):\n                        for req in dist._DistInfoDistribution__dep_map[available]:\n                            req = InstallRequirement(\n                                req,\n                                req_to_install,\n                                isolated=self.isolated,\n                                wheel_cache=self.wheel_cache,\n                                use_pep517=None\n                            )\n\n                            more_reqs.append(req)\n\n            if not req_to_install.editable and not req_to_install.satisfied_by:\n                # XXX: --no-install leads this to report 'Successfully\n                # downloaded' for only non-editable reqs, even though we took\n                # action on them.\n                requirement_set.successfully_downloaded.append(req_to_install)\n\n        return more_reqs"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate the installation order for the given requirement set.", "response": "def get_installation_order(self, req_set):\n        # type: (RequirementSet) -> List[InstallRequirement]\n        \"\"\"Create the installation order.\n\n        The installation order is topological - requirements are installed\n        before the requiring thing. We break cycles at an arbitrary point,\n        and make no other guarantees.\n        \"\"\"\n        # The current implementation, which we may change at any point\n        # installs the user specified things in the order given, except when\n        # dependencies must come earlier to achieve topological order.\n        order = []\n        ordered_reqs = set()  # type: Set[InstallRequirement]\n\n        def schedule(req):\n            if req.satisfied_by or req in ordered_reqs:\n                return\n            if req.constraint:\n                return\n            ordered_reqs.add(req)\n            for dep in self._discovered_dependencies[req.name]:\n                schedule(dep)\n            order.append(req)\n\n        for install_req in req_set.requirements.values():\n            schedule(install_req)\n        return order"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nescapes & and < and > in a string of data.", "response": "def _xml_escape(data):\n    \"\"\"Escape &, <, >, \", ', etc. in a string of data.\"\"\"\n\n    # ampersand must be replaced first\n    from_symbols = '&><\"\\''\n    to_symbols = ('&'+s+';' for s in \"amp gt lt quot apos\".split())\n    for from_,to_ in zip(from_symbols, to_symbols):\n        data = data.replace(from_, to_)\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef line( loc, strg ):\n    lastCR = strg.rfind(\"\\n\", 0, loc)\n    nextCR = strg.find(\"\\n\", loc)\n    if nextCR >= 0:\n        return strg[lastCR+1:nextCR]\n    else:\n        return strg[lastCR+1:]", "response": "Returns the line of text containing loc within a string."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef traceParseAction(f):\n    f = _trim_arity(f)\n    def z(*paArgs):\n        thisFunc = f.__name__\n        s,l,t = paArgs[-3:]\n        if len(paArgs)>3:\n            thisFunc = paArgs[0].__class__.__name__ + '.' + thisFunc\n        sys.stderr.write( \">>entering %s(line: '%s', %d, %r)\\n\" % (thisFunc,line(l,s),l,t) )\n        try:\n            ret = f(*paArgs)\n        except Exception as exc:\n            sys.stderr.write( \"<<leaving %s (exception: %s)\\n\" % (thisFunc,exc) )\n            raise\n        sys.stderr.write( \"<<leaving %s (ret: %r)\\n\" % (thisFunc,ret) )\n        return ret\n    try:\n        z.__name__ = f.__name__\n    except AttributeError:\n        pass\n    return z", "response": "Decorator for debugging parse actions."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delimitedList( expr, delim=\",\", combine=False ):\n    dlName = _ustr(expr)+\" [\"+_ustr(delim)+\" \"+_ustr(expr)+\"]...\"\n    if combine:\n        return Combine( expr + ZeroOrMore( delim + expr ) ).setName(dlName)\n    else:\n        return ( expr + ZeroOrMore( Suppress( delim ) + expr ) ).setName(dlName)", "response": "Helper to define a delimited list of expressions."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef locatedExpr(expr):\n    locator = Empty().setParseAction(lambda s,l,t: l)\n    return Group(locator(\"locn_start\") + expr(\"value\") + locator.copy().leaveWhitespace()(\"locn_end\"))", "response": "Helper to decorate a returned token with its starting and ending locations in the input string."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef srange(s):\n    _expanded = lambda p: p if not isinstance(p,ParseResults) else ''.join(unichr(c) for c in range(ord(p[0]),ord(p[1])+1))\n    try:\n        return \"\".join(_expanded(part) for part in _reBracketExpr.parseString(s).body)\n    except Exception:\n        return \"\"", "response": "r Helper function to easily define string ranges for use in Word\n    construction."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef matchOnlyAtCol(n):\n    def verifyCol(strg,locn,toks):\n        if col(locn,strg) != n:\n            raise ParseException(strg,locn,\"matched token not at column %d\" % n)\n    return verifyCol", "response": "Returns a parse action that checks that the input text contains only tokens at a specific column."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef tokenMap(func, *args):\n    def pa(s,l,t):\n        return [func(tokn, *args) for tokn in t]\n\n    try:\n        func_name = getattr(func, '__name__',\n                            getattr(func, '__class__').__name__)\n    except Exception:\n        func_name = str(func)\n    pa.__name__ = func_name\n\n    return pa", "response": "This function maps a function to all the elements of a ParseResults list."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a nested expression that can be used to construct a nested list of items within a nested list.", "response": "def nestedExpr(opener=\"(\", closer=\")\", content=None, ignoreExpr=quotedString.copy()):\n    \"\"\"Helper method for defining nested lists enclosed in opening and\n    closing delimiters (\"(\" and \")\" are the default).\n\n    Parameters:\n     - opener - opening character for a nested list\n       (default= ``\"(\"``); can also be a pyparsing expression\n     - closer - closing character for a nested list\n       (default= ``\")\"``); can also be a pyparsing expression\n     - content - expression for items within the nested lists\n       (default= ``None``)\n     - ignoreExpr - expression for ignoring opening and closing\n       delimiters (default= :class:`quotedString`)\n\n    If an expression is not provided for the content argument, the\n    nested expression will capture all whitespace-delimited content\n    between delimiters as a list of separate values.\n\n    Use the ``ignoreExpr`` argument to define expressions that may\n    contain opening or closing characters that should not be treated as\n    opening or closing characters for nesting, such as quotedString or\n    a comment expression.  Specify multiple expressions using an\n    :class:`Or` or :class:`MatchFirst`. The default is\n    :class:`quotedString`, but if no expressions are to be ignored, then\n    pass ``None`` for this argument.\n\n    Example::\n\n        data_type = oneOf(\"void int short long char float double\")\n        decl_data_type = Combine(data_type + Optional(Word('*')))\n        ident = Word(alphas+'_', alphanums+'_')\n        number = pyparsing_common.number\n        arg = Group(decl_data_type + ident)\n        LPAR,RPAR = map(Suppress, \"()\")\n\n        code_body = nestedExpr('{', '}', ignoreExpr=(quotedString | cStyleComment))\n\n        c_function = (decl_data_type(\"type\")\n                      + ident(\"name\")\n                      + LPAR + Optional(delimitedList(arg), [])(\"args\") + RPAR\n                      + code_body(\"body\"))\n        c_function.ignore(cStyleComment)\n\n        source_code = '''\n            int is_odd(int x) {\n                return (x%2);\n            }\n\n            int dec_to_hex(char hchar) {\n                if (hchar >= '0' && hchar <= '9') {\n                    return (ord(hchar)-ord('0'));\n                } else {\n                    return (10+ord(hchar)-ord('A'));\n                }\n            }\n        '''\n        for func in c_function.searchString(source_code):\n            print(\"%(name)s (%(type)s) args: %(args)s\" % func)\n\n\n    prints::\n\n        is_odd (int) args: [['int', 'x']]\n        dec_to_hex (int) args: [['char', 'hchar']]\n    \"\"\"\n    if opener == closer:\n        raise ValueError(\"opening and closing strings cannot be the same\")\n    if content is None:\n        if isinstance(opener,basestring) and isinstance(closer,basestring):\n            if len(opener) == 1 and len(closer)==1:\n                if ignoreExpr is not None:\n                    content = (Combine(OneOrMore(~ignoreExpr +\n                                    CharsNotIn(opener+closer+ParserElement.DEFAULT_WHITE_CHARS,exact=1))\n                                ).setParseAction(lambda t:t[0].strip()))\n                else:\n                    content = (empty.copy()+CharsNotIn(opener+closer+ParserElement.DEFAULT_WHITE_CHARS\n                                ).setParseAction(lambda t:t[0].strip()))\n            else:\n                if ignoreExpr is not None:\n                    content = (Combine(OneOrMore(~ignoreExpr +\n                                    ~Literal(opener) + ~Literal(closer) +\n                                    CharsNotIn(ParserElement.DEFAULT_WHITE_CHARS,exact=1))\n                                ).setParseAction(lambda t:t[0].strip()))\n                else:\n                    content = (Combine(OneOrMore(~Literal(opener) + ~Literal(closer) +\n                                    CharsNotIn(ParserElement.DEFAULT_WHITE_CHARS,exact=1))\n                                ).setParseAction(lambda t:t[0].strip()))\n        else:\n            raise ValueError(\"opening and closing arguments must be strings if no content expression is given\")\n    ret = Forward()\n    if ignoreExpr is not None:\n        ret <<= Group( Suppress(opener) + ZeroOrMore( ignoreExpr | ret | content ) + Suppress(closer) )\n    else:\n        ret <<= Group( Suppress(opener) + ZeroOrMore( ret | content )  + Suppress(closer) )\n    ret.setName('nested %s%s expression' % (opener,closer))\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef markInputline( self, markerString = \">!<\" ):\n        line_str = self.line\n        line_column = self.column - 1\n        if markerString:\n            line_str = \"\".join((line_str[:line_column],\n                                markerString, line_str[line_column:]))\n        return line_str.strip()", "response": "Extracts the exception line from the input string and marks the exception line with a special symbol."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving and returns an item from the list of results.", "response": "def pop( self, *args, **kwargs):\n        \"\"\"\n        Removes and returns item at specified index (default= ``last``).\n        Supports both ``list`` and ``dict`` semantics for ``pop()``. If\n        passed no argument or an integer argument, it will use ``list``\n        semantics and pop tokens from the list of parsed tokens. If passed\n        a non-integer argument (most likely a string), it will use ``dict``\n        semantics and pop the corresponding value from any defined results\n        names. A second default return value argument is supported, just as in\n        ``dict.pop()``.\n\n        Example::\n\n            def remove_first(tokens):\n                tokens.pop(0)\n            print(OneOrMore(Word(nums)).parseString(\"0 123 321\")) # -> ['0', '123', '321']\n            print(OneOrMore(Word(nums)).addParseAction(remove_first).parseString(\"0 123 321\")) # -> ['123', '321']\n\n            label = Word(alphas)\n            patt = label(\"LABEL\") + OneOrMore(Word(nums))\n            print(patt.parseString(\"AAB 123 321\").dump())\n\n            # Use pop() in a parse action to remove named result (note that corresponding value is not\n            # removed from list form of results)\n            def remove_LABEL(tokens):\n                tokens.pop(\"LABEL\")\n                return tokens\n            patt.addParseAction(remove_LABEL)\n            print(patt.parseString(\"AAB 123 321\").dump())\n\n        prints::\n\n            ['AAB', '123', '321']\n            - LABEL: AAB\n\n            ['AAB', '123', '321']\n        \"\"\"\n        if not args:\n            args = [-1]\n        for k,v in kwargs.items():\n            if k == 'default':\n                args = (args[0], v)\n            else:\n                raise TypeError(\"pop() got an unexpected keyword argument '%s'\" % k)\n        if (isinstance(args[0], int) or\n                        len(args) == 1 or\n                        args[0] in self):\n            index = args[0]\n            ret = self[index]\n            del self[index]\n            return ret\n        else:\n            defaultvalue = args[1]\n            return defaultvalue"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds sequence of elements to end of ParseResults list of elements.", "response": "def extend( self, itemseq ):\n        \"\"\"\n        Add sequence of elements to end of ParseResults list of elements.\n\n        Example::\n\n            patt = OneOrMore(Word(alphas))\n\n            # use a parse action to append the reverse of the matched strings, to make a palindrome\n            def make_palindrome(tokens):\n                tokens.extend(reversed([t[::-1] for t in tokens]))\n                return ''.join(tokens)\n            print(patt.addParseAction(make_palindrome).parseString(\"lskdj sdlkjf lksd\")) # -> 'lskdjsdlkjflksddsklfjkldsjdksl'\n        \"\"\"\n        if isinstance(itemseq, ParseResults):\n            self += itemseq\n        else:\n            self.__toklist.extend(itemseq)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the parse results as a nested list of matching tokens all converted to strings.", "response": "def asList( self ):\n        \"\"\"\n        Returns the parse results as a nested list of matching tokens, all converted to strings.\n\n        Example::\n\n            patt = OneOrMore(Word(alphas))\n            result = patt.parseString(\"sldkj lsdkj sldkj\")\n            # even though the result prints in string-like form, it is actually a pyparsing ParseResults\n            print(type(result), result) # -> <class 'pyparsing.ParseResults'> ['sldkj', 'lsdkj', 'sldkj']\n\n            # Use asList() to create an actual list\n            result_list = result.asList()\n            print(type(result_list), result_list) # -> <class 'list'> ['sldkj', 'lsdkj', 'sldkj']\n        \"\"\"\n        return [res.asList() if isinstance(res,ParseResults) else res for res in self.__toklist]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the named parse results as a nested dictionary.", "response": "def asDict( self ):\n        \"\"\"\n        Returns the named parse results as a nested dictionary.\n\n        Example::\n\n            integer = Word(nums)\n            date_str = integer(\"year\") + '/' + integer(\"month\") + '/' + integer(\"day\")\n\n            result = date_str.parseString('12/31/1999')\n            print(type(result), repr(result)) # -> <class 'pyparsing.ParseResults'> (['12', '/', '31', '/', '1999'], {'day': [('1999', 4)], 'year': [('12', 0)], 'month': [('31', 2)]})\n\n            result_dict = result.asDict()\n            print(type(result_dict), repr(result_dict)) # -> <class 'dict'> {'day': '1999', 'year': '12', 'month': '31'}\n\n            # even though a ParseResults supports dict-like access, sometime you just need to have a dict\n            import json\n            print(json.dumps(result)) # -> Exception: TypeError: ... is not JSON serializable\n            print(json.dumps(result.asDict())) # -> {\"month\": \"31\", \"day\": \"1999\", \"year\": \"12\"}\n        \"\"\"\n        if PY_3:\n            item_fn = self.items\n        else:\n            item_fn = self.iteritems\n\n        def toItem(obj):\n            if isinstance(obj, ParseResults):\n                if obj.haskeys():\n                    return obj.asDict()\n                else:\n                    return [toItem(v) for v in obj]\n            else:\n                return obj\n\n        return dict((k,toItem(v)) for k,v in item_fn())"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dump(self, indent='', depth=0, full=True):\n        out = []\n        NL = '\\n'\n        out.append( indent+_ustr(self.asList()) )\n        if full:\n            if self.haskeys():\n                items = sorted((str(k), v) for k,v in self.items())\n                for k,v in items:\n                    if out:\n                        out.append(NL)\n                    out.append( \"%s%s- %s: \" % (indent,('  '*depth), k) )\n                    if isinstance(v,ParseResults):\n                        if v:\n                            out.append( v.dump(indent,depth+1) )\n                        else:\n                            out.append(_ustr(v))\n                    else:\n                        out.append(repr(v))\n            elif any(isinstance(vv,ParseResults) for vv in self):\n                v = self\n                for i,vv in enumerate(v):\n                    if isinstance(vv,ParseResults):\n                        out.append(\"\\n%s%s[%d]:\\n%s%s%s\" % (indent,('  '*(depth)),i,indent,('  '*(depth+1)),vv.dump(indent,depth+1) ))\n                    else:\n                        out.append(\"\\n%s%s[%d]:\\n%s%s%s\" % (indent,('  '*(depth)),i,indent,('  '*(depth+1)),_ustr(vv)))\n\n        return \"\".join(out)", "response": "Diagnostic method for listing out the contents of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmaking a copy of this ParserElement.", "response": "def copy( self ):\n        \"\"\"\n        Make a copy of this :class:`ParserElement`.  Useful for defining\n        different parse actions for the same parsing pattern, using copies of\n        the original parse element.\n\n        Example::\n\n            integer = Word(nums).setParseAction(lambda toks: int(toks[0]))\n            integerK = integer.copy().addParseAction(lambda toks: toks[0]*1024) + Suppress(\"K\")\n            integerM = integer.copy().addParseAction(lambda toks: toks[0]*1024*1024) + Suppress(\"M\")\n\n            print(OneOrMore(integerK | integerM | integer).parseString(\"5K 100 640K 256M\"))\n\n        prints::\n\n            [5120, 100, 655360, 268435456]\n\n        Equivalent form of ``expr.copy()`` is just ``expr()``::\n\n            integerM = integer().addParseAction(lambda toks: toks[0]*1024*1024) + Suppress(\"M\")\n        \"\"\"\n        cpy = copy.copy( self )\n        cpy.parseAction = self.parseAction[:]\n        cpy.ignoreExprs = self.ignoreExprs[:]\n        if self.copyDefaultWhiteChars:\n            cpy.whiteChars = ParserElement.DEFAULT_WHITE_CHARS\n        return cpy"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef setName( self, name ):\n        self.name = name\n        self.errmsg = \"Expected \" + self.name\n        if hasattr(self,\"exception\"):\n            self.exception.msg = self.errmsg\n        return self", "response": "Define name for this expression makes debugging and exception messages clearer."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the name for referencing matching tokens as a nested attribute .", "response": "def setResultsName( self, name, listAllMatches=False ):\n        \"\"\"\n        Define name for referencing matching tokens as a nested attribute\n        of the returned parse results.\n        NOTE: this returns a *copy* of the original :class:`ParserElement` object;\n        this is so that the client can define a basic element, such as an\n        integer, and reference it in multiple places with different names.\n\n        You can also set results names using the abbreviated syntax,\n        ``expr(\"name\")`` in place of ``expr.setResultsName(\"name\")``\n        - see :class:`__call__`.\n\n        Example::\n\n            date_str = (integer.setResultsName(\"year\") + '/'\n                        + integer.setResultsName(\"month\") + '/'\n                        + integer.setResultsName(\"day\"))\n\n            # equivalent form:\n            date_str = integer(\"year\") + '/' + integer(\"month\") + '/' + integer(\"day\")\n        \"\"\"\n        newself = self.copy()\n        if name.endswith(\"*\"):\n            name = name[:-1]\n            listAllMatches=True\n        newself.resultsName = name\n        newself.modalResults = not listAllMatches\n        return newself"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a boolean predicate function to expression s list of parse actions.", "response": "def addCondition(self, *fns, **kwargs):\n        \"\"\"Add a boolean predicate function to expression's list of parse actions. See\n        :class:`setParseAction` for function call signatures. Unlike ``setParseAction``,\n        functions passed to ``addCondition`` need to return boolean success/fail of the condition.\n\n        Optional keyword arguments:\n        - message = define a custom message to be used in the raised exception\n        - fatal   = if True, will raise ParseFatalException to stop parsing immediately; otherwise will raise ParseException\n\n        Example::\n\n            integer = Word(nums).setParseAction(lambda toks: int(toks[0]))\n            year_int = integer.copy()\n            year_int.addCondition(lambda toks: toks[0] >= 2000, message=\"Only support years 2000 and later\")\n            date_str = year_int + '/' + integer + '/' + integer\n\n            result = date_str.parseString(\"1999/12/31\")  # -> Exception: Only support years 2000 and later (at char 0), (line:1, col:1)\n        \"\"\"\n        msg = kwargs.get(\"message\", \"failed user-defined condition\")\n        exc_type = ParseFatalException if kwargs.get(\"fatal\", False) else ParseException\n        for fn in fns:\n            fn = _trim_arity(fn)\n            def pa(s,l,t):\n                if not bool(fn(s,l,t)):\n                    raise exc_type(s,l,msg)\n            self.parseAction.append(pa)\n        self.callDuringTry = self.callDuringTry or kwargs.get(\"callDuringTry\", False)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nenables packrat parsing for the current language.", "response": "def enablePackrat(cache_size_limit=128):\n        \"\"\"Enables \"packrat\" parsing, which adds memoizing to the parsing logic.\n           Repeated parse attempts at the same string location (which happens\n           often in many complex grammars) can immediately return a cached value,\n           instead of re-executing parsing/validating code.  Memoizing is done of\n           both valid results and parsing exceptions.\n\n           Parameters:\n\n           - cache_size_limit - (default= ``128``) - if an integer value is provided\n             will limit the size of the packrat cache; if None is passed, then\n             the cache size will be unbounded; if 0 is passed, the cache will\n             be effectively disabled.\n\n           This speedup may break existing programs that use parse actions that\n           have side-effects.  For this reason, packrat parsing is disabled when\n           you first import pyparsing.  To activate the packrat feature, your\n           program must call the class method :class:`ParserElement.enablePackrat`.\n           For best results, call ``enablePackrat()`` immediately after\n           importing pyparsing.\n\n           Example::\n\n               import pyparsing\n               pyparsing.ParserElement.enablePackrat()\n        \"\"\"\n        if not ParserElement._packratEnabled:\n            ParserElement._packratEnabled = True\n            if cache_size_limit is None:\n                ParserElement.packrat_cache = ParserElement._UnboundedCache()\n            else:\n                ParserElement.packrat_cache = ParserElement._FifoCache(cache_size_limit)\n            ParserElement._parse = ParserElement._parseCache"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parseString( self, instring, parseAll=False ):\n        ParserElement.resetCache()\n        if not self.streamlined:\n            self.streamline()\n            #~ self.saveAsList = True\n        for e in self.ignoreExprs:\n            e.streamline()\n        if not self.keepTabs:\n            instring = instring.expandtabs()\n        try:\n            loc, tokens = self._parse( instring, 0 )\n            if parseAll:\n                loc = self.preParse( instring, loc )\n                se = Empty() + StringEnd()\n                se._parse( instring, loc )\n        except ParseBaseException as exc:\n            if ParserElement.verbose_stacktrace:\n                raise\n            else:\n                # catch and re-raise exception from here, clears out pyparsing internal stack trace\n                raise exc\n        else:\n            return tokens", "response": "Execute the parse expression with the given string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef split(self, instring, maxsplit=_MAX_INT, includeSeparators=False):\n        splits = 0\n        last = 0\n        for t,s,e in self.scanString(instring, maxMatches=maxsplit):\n            yield instring[last:s]\n            if includeSeparators:\n                yield t[0]\n            last = e\n        yield instring[last:]", "response": "Generator method to split a string into individual entries."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the default whitespace chars for the current locale.", "response": "def setWhitespaceChars( self, chars ):\n        \"\"\"\n        Overrides the default whitespace chars\n        \"\"\"\n        self.skipWhitespace = True\n        self.whiteChars = chars\n        self.copyDefaultWhiteChars = False\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a new expression to be ignored.", "response": "def ignore( self, other ):\n        \"\"\"\n        Define expression to be ignored (e.g., comments) while doing pattern\n        matching; may be called repeatedly, to define multiple comment or other\n        ignorable patterns.\n\n        Example::\n\n            patt = OneOrMore(Word(alphas))\n            patt.parseString('ablaj /* comment */ lskjd') # -> ['ablaj']\n\n            patt.ignore(cStyleComment)\n            patt.parseString('ablaj /* comment */ lskjd') # -> ['ablaj', 'lskjd']\n        \"\"\"\n        if isinstance(other, basestring):\n            other = Suppress(other)\n\n        if isinstance( other, Suppress ):\n            if other not in self.ignoreExprs:\n                self.ignoreExprs.append(other)\n        else:\n            self.ignoreExprs.append( Suppress( other.copy() ) )\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setDebugActions( self, startAction, successAction, exceptionAction ):\n        self.debugActions = (startAction or _defaultStartDebugAction,\n                             successAction or _defaultSuccessDebugAction,\n                             exceptionAction or _defaultExceptionDebugAction)\n        self.debug = True\n        return self", "response": "Enable display of debugging messages while doing pattern matching."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setDebug( self, flag=True ):\n        if flag:\n            self.setDebugActions( _defaultStartDebugAction, _defaultSuccessDebugAction, _defaultExceptionDebugAction )\n        else:\n            self.debug = False\n        return self", "response": "Set debug flag for the current locale."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parseFile( self, file_or_filename, parseAll=False ):\n        try:\n            file_contents = file_or_filename.read()\n        except AttributeError:\n            with open(file_or_filename, \"r\") as f:\n                file_contents = f.read()\n        try:\n            return self.parseString(file_contents, parseAll)\n        except ParseBaseException as exc:\n            if ParserElement.verbose_stacktrace:\n                raise\n            else:\n                # catch and re-raise exception from here, clears out pyparsing internal stack trace\n                raise exc", "response": "Parse the contents of a file or filename and return a dictionary of the parsed data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sub(self, repl):\n        if self.asGroupList:\n            warnings.warn(\"cannot use sub() with Regex(asGroupList=True)\",\n                           SyntaxWarning, stacklevel=2)\n            raise SyntaxError()\n\n        if self.asMatch and callable(repl):\n            warnings.warn(\"cannot use sub() with a callable with Regex(asMatch=True)\",\n                           SyntaxWarning, stacklevel=2)\n            raise SyntaxError()\n\n        if self.asMatch:\n            def pa(tokens):\n                return tokens[0].expand(repl)\n        else:\n            def pa(tokens):\n                return self.re.sub(repl, tokens[0])\n        return self.addParseAction(pa)", "response": "Return a new instance of the parse action that will parse the string with the sub - expression repl."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nextends leaveWhitespace defined in base class and invokes leaveWhitespace on all contained expressions.", "response": "def leaveWhitespace( self ):\n        \"\"\"Extends ``leaveWhitespace`` defined in base class, and also invokes ``leaveWhitespace`` on\n           all contained expressions.\"\"\"\n        self.skipWhitespace = False\n        self.exprs = [ e.copy() for e in self.exprs ]\n        for e in self.exprs:\n            e.leaveWhitespace()\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a function that converts parsed date string to Python date object", "response": "def convertToDate(fmt=\"%Y-%m-%d\"):\n        \"\"\"\n        Helper to create a parse action for converting parsed date string to Python datetime.date\n\n        Params -\n         - fmt - format to be passed to datetime.strptime (default= ``\"%Y-%m-%d\"``)\n\n        Example::\n\n            date_expr = pyparsing_common.iso8601_date.copy()\n            date_expr.setParseAction(pyparsing_common.convertToDate())\n            print(date_expr.parseString(\"1999-12-31\"))\n\n        prints::\n\n            [datetime.date(1999, 12, 31)]\n        \"\"\"\n        def cvt_fn(s,l,t):\n            try:\n                return datetime.strptime(t[0], fmt).date()\n            except ValueError as ve:\n                raise ParseException(s, l, str(ve))\n        return cvt_fn"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef convertToDatetime(fmt=\"%Y-%m-%dT%H:%M:%S.%f\"):\n        def cvt_fn(s,l,t):\n            try:\n                return datetime.strptime(t[0], fmt)\n            except ValueError as ve:\n                raise ParseException(s, l, str(ve))\n        return cvt_fn", "response": "Helper function to convert parsed\n            datetime string to Python datetime. datetime object"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _match_vcs_scheme(url):\n    # type: (str) -> Optional[str]\n    \"\"\"Look for VCS schemes in the URL.\n\n    Returns the matched VCS scheme, or None if there's no match.\n    \"\"\"\n    from pipenv.patched.notpip._internal.vcs import VcsSupport\n    for scheme in VcsSupport.schemes:\n        if url.lower().startswith(scheme) and url[len(scheme)] in '+:':\n            return scheme\n    return None", "response": "Returns the VCS scheme that matches the given URL."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _is_url_like_archive(url):\n    # type: (str) -> bool\n    \"\"\"Return whether the URL looks like an archive.\n    \"\"\"\n    filename = Link(url).filename\n    for bad_ext in ARCHIVE_EXTENSIONS:\n        if filename.endswith(bad_ext):\n            return True\n    return False", "response": "Return whether the URL looks like an archive."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _ensure_html_header(response):\n    # type: (Response) -> None\n    \"\"\"Check the Content-Type header to ensure the response contains HTML.\n\n    Raises `_NotHTML` if the content type is not text/html.\n    \"\"\"\n    content_type = response.headers.get(\"Content-Type\", \"\")\n    if not content_type.lower().startswith(\"text/html\"):\n        raise _NotHTML(content_type, response.request.method)", "response": "Checks the Content - Type header to ensure the response contains HTML."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsends a HEAD request to the URL and ensure that the response contains HTML.", "response": "def _ensure_html_response(url, session):\n    # type: (str, PipSession) -> None\n    \"\"\"Send a HEAD request to the URL, and ensure the response contains HTML.\n\n    Raises `_NotHTTP` if the URL is not available for a HEAD request, or\n    `_NotHTML` if the content type is not text/html.\n    \"\"\"\n    scheme, netloc, path, query, fragment = urllib_parse.urlsplit(url)\n    if scheme not in {'http', 'https'}:\n        raise _NotHTTP()\n\n    resp = session.head(url, allow_redirects=True)\n    resp.raise_for_status()\n\n    _ensure_html_header(resp)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets an HTML page with GET and return the response.", "response": "def _get_html_response(url, session):\n    # type: (str, PipSession) -> Response\n    \"\"\"Access an HTML page with GET, and return the response.\n\n    This consists of three parts:\n\n    1. If the URL looks suspiciously like an archive, send a HEAD first to\n       check the Content-Type is HTML, to avoid downloading a large file.\n       Raise `_NotHTTP` if the content type cannot be determined, or\n       `_NotHTML` if it is not HTML.\n    2. Actually perform the request. Raise HTTP exceptions on network failures.\n    3. Check the Content-Type header to make sure we got HTML, and raise\n       `_NotHTML` otherwise.\n    \"\"\"\n    if _is_url_like_archive(url):\n        _ensure_html_response(url, session=session)\n\n    logger.debug('Getting page %s', url)\n\n    resp = session.get(\n        url,\n        headers={\n            \"Accept\": \"text/html\",\n            # We don't want to blindly returned cached data for\n            # /simple/, because authors generally expecting that\n            # twine upload && pip install will function, but if\n            # they've done a pip install in the last ~10 minutes\n            # it won't. Thus by setting this to zero we will not\n            # blindly use any cached data, however the benefit of\n            # using max-age=0 instead of no-cache, is that we will\n            # still support conditional requests, so we will still\n            # minimize traffic sent in cases where the page hasn't\n            # changed at all, we will just always incur the round\n            # trip for the conditional GET now instead of only\n            # once per 10 minutes.\n            # For more information, please see pypa/pip#5670.\n            \"Cache-Control\": \"max-age=0\",\n        },\n    )\n    resp.raise_for_status()\n\n    # The check for archives above only works if the url ends with\n    # something that looks like an archive. However that is not a\n    # requirement of an url. Unless we issue a HEAD request on every\n    # url we cannot know ahead of time for sure if something is HTML\n    # or not. However we can check after we've downloaded it.\n    _ensure_html_header(resp)\n\n    return resp"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _find_name_version_sep(egg_info, canonical_name):\n    # type: (str, str) -> int\n    \"\"\"Find the separator's index based on the package's canonical name.\n\n    `egg_info` must be an egg info string for the given package, and\n    `canonical_name` must be the package's canonical name.\n\n    This function is needed since the canonicalized name does not necessarily\n    have the same length as the egg info's name part. An example::\n\n    >>> egg_info = 'foo__bar-1.0'\n    >>> canonical_name = 'foo-bar'\n    >>> _find_name_version_sep(egg_info, canonical_name)\n    8\n    \"\"\"\n    # Project name and version must be separated by one single dash. Find all\n    # occurrences of dashes; if the string in front of it matches the canonical\n    # name, this is the one separating the name and version parts.\n    for i, c in enumerate(egg_info):\n        if c != \"-\":\n            continue\n        if canonicalize_name(egg_info[:i]) == canonical_name:\n            return i\n    raise ValueError(\"{} does not match {}\".format(egg_info, canonical_name))", "response": "Find the separator s index based on the package s canonical name."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _egg_info_matches(egg_info, canonical_name):\n    # type: (str, str) -> Optional[str]\n    \"\"\"Pull the version part out of a string.\n\n    :param egg_info: The string to parse. E.g. foo-2.1\n    :param canonical_name: The canonicalized name of the package this\n        belongs to.\n    \"\"\"\n    try:\n        version_start = _find_name_version_sep(egg_info, canonical_name) + 1\n    except ValueError:\n        return None\n    version = egg_info[version_start:]\n    if not version:\n        return None\n    return version", "response": "Return the version part of a string that matches the canonicalized name of the package."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _determine_base_url(document, page_url):\n    for base in document.findall(\".//base\"):\n        href = base.get(\"href\")\n        if href is not None:\n            return href\n    return page_url", "response": "Determine the base URL of the HTML document."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndetermining if we have any encoding information in our headers.", "response": "def _get_encoding_from_headers(headers):\n    \"\"\"Determine if we have any encoding information in our headers.\n    \"\"\"\n    if headers and \"Content-Type\" in headers:\n        content_type, params = cgi.parse_header(headers[\"Content-Type\"])\n        if \"charset\" in params:\n            return params['charset']\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfunction used to generate a candidate sorting key for a given installation candidate.", "response": "def _candidate_sort_key(self, candidate, ignore_compatibility=True):\n        # type: (InstallationCandidate, bool) -> CandidateSortingKey\n        \"\"\"\n        Function used to generate link sort key for link tuples.\n        The greater the return value, the more preferred it is.\n        If not finding wheels, then sorted by version only.\n        If finding wheels, then the sort order is by version, then:\n          1. existing installs\n          2. wheels ordered via Wheel.support_index_min(self.valid_tags)\n          3. source archives\n        If prefer_binary was set, then all wheels are sorted above sources.\n        Note: it was considered to embed this logic into the Link\n              comparison operators, but then different sdist links\n              with the same version, would have to be considered equal\n        \"\"\"\n        support_num = len(self.valid_tags)\n        build_tag = tuple()  # type: BuildTag\n        binary_preference = 0\n        if candidate.location.is_wheel:\n            # can raise InvalidWheelFilename\n            wheel = Wheel(candidate.location.filename)\n            if not wheel.supported(self.valid_tags) and not ignore_compatibility:\n                raise UnsupportedWheel(\n                    \"%s is not a supported wheel for this platform. It \"\n                    \"can't be sorted.\" % wheel.filename\n                )\n            if self.prefer_binary:\n                binary_preference = 1\n            tags = self.valid_tags if not ignore_compatibility else None\n            try:\n                pri = -(wheel.support_index_min(tags=tags))\n            except TypeError:\n                pri = -(support_num)\n            if wheel.build_tag is not None:\n                match = re.match(r'^(\\d+)(.*)$', wheel.build_tag)\n                build_tag_groups = match.groups()\n                build_tag = (int(build_tag_groups[0]), build_tag_groups[1])\n        else:  # sdist\n            pri = -(support_num)\n        return (binary_preference, candidate.version, build_tag, pri)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_index_urls_locations(self, project_name):\n        # type: (str) -> List[str]\n        \"\"\"Returns the locations found via self.index_urls\n\n        Checks the url_name on the main (first in the list) index and\n        use this url_name to produce all locations\n        \"\"\"\n\n        def mkurl_pypi_url(url):\n            loc = posixpath.join(\n                url,\n                urllib_parse.quote(canonicalize_name(project_name)))\n            # For maximum compatibility with easy_install, ensure the path\n            # ends in a trailing slash.  Although this isn't in the spec\n            # (and PyPI can handle it without the slash) some other index\n            # implementations might break if they relied on easy_install's\n            # behavior.\n            if not loc.endswith('/'):\n                loc = loc + '/'\n            return loc\n\n        return [mkurl_pypi_url(url) for url in self.index_urls]", "response": "Returns the locations found via self. index_urls\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds all available InstallationCandidate for project_name.", "response": "def find_all_candidates(self, project_name):\n        # type: (str) -> List[Optional[InstallationCandidate]]\n        \"\"\"Find all available InstallationCandidate for project_name\n\n        This checks index_urls and find_links.\n        All versions found are returned as an InstallationCandidate list.\n\n        See _link_package_versions for details on which files are accepted\n        \"\"\"\n        index_locations = self._get_index_urls_locations(project_name)\n        index_file_loc, index_url_loc = self._sort_locations(index_locations)\n        fl_file_loc, fl_url_loc = self._sort_locations(\n            self.find_links, expand_dir=True,\n        )\n\n        file_locations = (Link(url) for url in itertools.chain(\n            index_file_loc, fl_file_loc,\n        ))\n\n        # We trust every url that the user has given us whether it was given\n        #   via --index-url or --find-links.\n        # We want to filter out any thing which does not have a secure origin.\n        url_locations = [\n            link for link in itertools.chain(\n                (Link(url) for url in index_url_loc),\n                (Link(url) for url in fl_url_loc),\n            )\n            if self._validate_secure_origin(logger, link)\n        ]\n\n        logger.debug('%d location(s) to search for versions of %s:',\n                     len(url_locations), project_name)\n\n        for location in url_locations:\n            logger.debug('* %s', location)\n\n        canonical_name = canonicalize_name(project_name)\n        formats = self.format_control.get_allowed_formats(canonical_name)\n        search = Search(project_name, canonical_name, formats)\n        find_links_versions = self._package_versions(\n            # We trust every directly linked archive in find_links\n            (Link(url, '-f') for url in self.find_links),\n            search\n        )\n\n        page_versions = []\n        for page in self._get_pages(url_locations, project_name):\n            try:\n                logger.debug('Analyzing links from page %s', page.url)\n            except AttributeError:\n                continue\n            with indent_log():\n                page_versions.extend(\n                    self._package_versions(page.iter_links(), search)\n                )\n\n        file_versions = self._package_versions(file_locations, search)\n        if file_versions:\n            file_versions.sort(reverse=True)\n            logger.debug(\n                'Local files found: %s',\n                ', '.join([\n                    url_to_path(candidate.location.url)\n                    for candidate in file_versions\n                ])\n            )\n\n        # This is an intentional priority ordering\n        return file_versions + find_links_versions + page_versions"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_requirement(self, req, upgrade, ignore_compatibility=False):\n        # type: (InstallRequirement, bool, bool) -> Optional[Link]\n        \"\"\"Try to find a Link matching req\n\n        Expects req, an InstallRequirement and upgrade, a boolean\n        Returns a Link if found,\n        Raises DistributionNotFound or BestVersionAlreadyInstalled otherwise\n        \"\"\"\n        all_candidates = self.find_all_candidates(req.name)\n\n        # Filter out anything which doesn't match our specifier\n        compatible_versions = set(\n            req.specifier.filter(\n                # We turn the version object into a str here because otherwise\n                # when we're debundled but setuptools isn't, Python will see\n                # packaging.version.Version and\n                # pkg_resources._vendor.packaging.version.Version as different\n                # types. This way we'll use a str as a common data interchange\n                # format. If we stop using the pkg_resources provided specifier\n                # and start using our own, we can drop the cast to str().\n                [str(c.version) for c in all_candidates],\n                prereleases=(\n                    self.allow_all_prereleases\n                    if self.allow_all_prereleases else None\n                ),\n            )\n        )\n        applicable_candidates = [\n            # Again, converting to str to deal with debundling.\n            c for c in all_candidates if str(c.version) in compatible_versions\n        ]\n\n        if applicable_candidates:\n            best_candidate = max(applicable_candidates,\n                                 key=self._candidate_sort_key)\n        else:\n            best_candidate = None\n\n        if req.satisfied_by is not None:\n            installed_version = parse_version(req.satisfied_by.version)\n        else:\n            installed_version = None\n\n        if installed_version is None and best_candidate is None:\n            logger.critical(\n                'Could not find a version that satisfies the requirement %s '\n                '(from versions: %s)',\n                req,\n                ', '.join(\n                    sorted(\n                        {str(c.version) for c in all_candidates},\n                        key=parse_version,\n                    )\n                )\n            )\n\n            raise DistributionNotFound(\n                'No matching distribution found for %s' % req\n            )\n\n        best_installed = False\n        if installed_version and (\n                best_candidate is None or\n                best_candidate.version <= installed_version):\n            best_installed = True\n\n        if not upgrade and installed_version is not None:\n            if best_installed:\n                logger.debug(\n                    'Existing installed version (%s) is most up-to-date and '\n                    'satisfies requirement',\n                    installed_version,\n                )\n            else:\n                logger.debug(\n                    'Existing installed version (%s) satisfies requirement '\n                    '(most up-to-date version is %s)',\n                    installed_version,\n                    best_candidate.version,\n                )\n            return None\n\n        if best_installed:\n            # We have an existing version, and its the best version\n            logger.debug(\n                'Installed version (%s) is most up-to-date (past versions: '\n                '%s)',\n                installed_version,\n                ', '.join(sorted(compatible_versions, key=parse_version)) or\n                \"none\",\n            )\n            raise BestVersionAlreadyInstalled\n\n        logger.debug(\n            'Using version %s (newest of versions: %s)',\n            best_candidate.version,\n            ', '.join(sorted(compatible_versions, key=parse_version))\n        )\n        return best_candidate.location", "response": "Try to find a Link matching req."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_pages(self, locations, project_name):\n        # type: (Iterable[Link], str) -> Iterable[HTMLPage]\n        \"\"\"\n        Yields (page, page_url) from the given locations, skipping\n        locations that have errors.\n        \"\"\"\n        seen = set()  # type: Set[Link]\n        for location in locations:\n            if location in seen:\n                continue\n            seen.add(location)\n\n            page = _get_html_page(location, session=self.session)\n            if page is None:\n                continue\n\n            yield page", "response": "Yields HTML pages from the given locations skipping locations that have errors."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning an InstallationCandidate object for the given link and search.", "response": "def _link_package_versions(self, link, search, ignore_compatibility=True):\n        # type: (Link, Search, bool) -> Optional[InstallationCandidate]\n        \"\"\"Return an InstallationCandidate or None\"\"\"\n        version = None\n        if link.egg_fragment:\n            egg_info = link.egg_fragment\n            ext = link.ext\n        else:\n            egg_info, ext = link.splitext()\n            if not ext:\n                self._log_skipped_link(link, 'not a file')\n                return None\n            if ext not in SUPPORTED_EXTENSIONS:\n                self._log_skipped_link(\n                    link, 'unsupported archive format: %s' % ext,\n                )\n                return None\n            if \"binary\" not in search.formats and ext == WHEEL_EXTENSION and not ignore_compatibility:\n                self._log_skipped_link(\n                    link, 'No binaries permitted for %s' % search.supplied,\n                )\n                return None\n            if \"macosx10\" in link.path and ext == '.zip' and not ignore_compatibility:\n                self._log_skipped_link(link, 'macosx10 one')\n                return None\n            if ext == WHEEL_EXTENSION:\n                try:\n                    wheel = Wheel(link.filename)\n                except InvalidWheelFilename:\n                    self._log_skipped_link(link, 'invalid wheel filename')\n                    return None\n                if canonicalize_name(wheel.name) != search.canonical:\n                    self._log_skipped_link(\n                        link, 'wrong project name (not %s)' % search.supplied)\n                    return None\n\n                if not wheel.supported(self.valid_tags) and not ignore_compatibility:\n                    self._log_skipped_link(\n                        link, 'it is not compatible with this Python')\n                    return None\n\n                version = wheel.version\n\n        # This should be up by the search.ok_binary check, but see issue 2700.\n        if \"source\" not in search.formats and ext != WHEEL_EXTENSION:\n            self._log_skipped_link(\n                link, 'No sources permitted for %s' % search.supplied,\n            )\n            return None\n\n        if not version:\n            version = _egg_info_matches(egg_info, search.canonical)\n        if not version:\n            self._log_skipped_link(\n                link, 'Missing project version for %s' % search.supplied)\n            return None\n\n        match = self._py_version_re.search(version)\n        if match:\n            version = version[:match.start()]\n            py_version = match.group(1)\n            if py_version != sys.version[:3]:\n                self._log_skipped_link(\n                    link, 'Python version is incorrect')\n                return None\n        try:\n            support_this_python = check_requires_python(link.requires_python)\n        except specifiers.InvalidSpecifier:\n            logger.debug(\"Package %s has an invalid Requires-Python entry: %s\",\n                         link.filename, link.requires_python)\n            support_this_python = True\n\n        if not support_this_python and not ignore_compatibility:\n            logger.debug(\"The package %s is incompatible with the python \"\n                         \"version in use. Acceptable python versions are: %s\",\n                         link, link.requires_python)\n            return None\n        logger.debug('Found link %s, version: %s', link, version)\n\n        return InstallationCandidate(search.supplied, version, link, link.requires_python)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef iter_links(self):\n        # type: () -> Iterable[Link]\n        \"\"\"Yields all links in the page\"\"\"\n        document = html5lib.parse(\n            self.content,\n            transport_encoding=_get_encoding_from_headers(self.headers),\n            namespaceHTMLElements=False,\n        )\n        base_url = _determine_base_url(document, self.url)\n        for anchor in document.findall(\".//a\"):\n            if anchor.get(\"href\"):\n                href = anchor.get(\"href\")\n                url = _clean_link(urllib_parse.urljoin(base_url, href))\n                pyrequire = anchor.get('data-requires-python')\n                pyrequire = unescape(pyrequire) if pyrequire else None\n                yield Link(url, self.url, requires_python=pyrequire)", "response": "Yields all links in the page."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run(command, timeout=30, withexitstatus=False, events=None,\n        extra_args=None, logfile=None, cwd=None, env=None, **kwargs):\n\n    '''\n    This function runs the given command; waits for it to finish; then\n    returns all output as a string. STDERR is included in output. If the full\n    path to the command is not given then the path is searched.\n\n    Note that lines are terminated by CR/LF (\\\\r\\\\n) combination even on\n    UNIX-like systems because this is the standard for pseudottys. If you set\n    'withexitstatus' to true, then run will return a tuple of (command_output,\n    exitstatus). If 'withexitstatus' is false then this returns just\n    command_output.\n\n    The run() function can often be used instead of creating a spawn instance.\n    For example, the following code uses spawn::\n\n        from pexpect import *\n        child = spawn('scp foo user@example.com:.')\n        child.expect('(?i)password')\n        child.sendline(mypassword)\n\n    The previous code can be replace with the following::\n\n        from pexpect import *\n        run('scp foo user@example.com:.', events={'(?i)password': mypassword})\n\n    **Examples**\n\n    Start the apache daemon on the local machine::\n\n        from pexpect import *\n        run(\"/usr/local/apache/bin/apachectl start\")\n\n    Check in a file using SVN::\n\n        from pexpect import *\n        run(\"svn ci -m 'automatic commit' my_file.py\")\n\n    Run a command and capture exit status::\n\n        from pexpect import *\n        (command_output, exitstatus) = run('ls -l /bin', withexitstatus=1)\n\n    The following will run SSH and execute 'ls -l' on the remote machine. The\n    password 'secret' will be sent if the '(?i)password' pattern is ever seen::\n\n        run(\"ssh username@machine.example.com 'ls -l'\",\n            events={'(?i)password':'secret\\\\n'})\n\n    This will start mencoder to rip a video from DVD. This will also display\n    progress ticks every 5 seconds as it runs. For example::\n\n        from pexpect import *\n        def print_ticks(d):\n            print d['event_count'],\n        run(\"mencoder dvd://1 -o video.avi -oac copy -ovc copy\",\n            events={TIMEOUT:print_ticks}, timeout=5)\n\n    The 'events' argument should be either a dictionary or a tuple list that\n    contains patterns and responses. Whenever one of the patterns is seen\n    in the command output, run() will send the associated response string.\n    So, run() in the above example can be also written as:\n    \n        run(\"mencoder dvd://1 -o video.avi -oac copy -ovc copy\",\n            events=[(TIMEOUT,print_ticks)], timeout=5)\n\n    Use a tuple list for events if the command output requires a delicate\n    control over what pattern should be matched, since the tuple list is passed\n    to pexpect() as its pattern list, with the order of patterns preserved.\n\n    Note that you should put newlines in your string if Enter is necessary.\n\n    Like the example above, the responses may also contain a callback, either\n    a function or method.  It should accept a dictionary value as an argument.\n    The dictionary contains all the locals from the run() function, so you can\n    access the child spawn object or any other variable defined in run()\n    (event_count, child, and extra_args are the most useful). A callback may\n    return True to stop the current run process.  Otherwise run() continues\n    until the next event. A callback may also return a string which will be\n    sent to the child. 'extra_args' is not used by directly run(). It provides\n    a way to pass data to a callback function through run() through the locals\n    dictionary passed to a callback.\n\n    Like :class:`spawn`, passing *encoding* will make it work with unicode\n    instead of bytes. You can pass *codec_errors* to control how errors in\n    encoding and decoding are handled.\n    '''\n    if timeout == -1:\n        child = spawn(command, maxread=2000, logfile=logfile, cwd=cwd, env=env,\n                        **kwargs)\n    else:\n        child = spawn(command, timeout=timeout, maxread=2000, logfile=logfile,\n                cwd=cwd, env=env, **kwargs)\n    if isinstance(events, list):\n        patterns= [x for x,y in events]\n        responses = [y for x,y in events]\n    elif isinstance(events, dict):\n        patterns = list(events.keys())\n        responses = list(events.values())\n    else:\n        # This assumes EOF or TIMEOUT will eventually cause run to terminate.\n        patterns = None\n        responses = None\n    child_result_list = []\n    event_count = 0\n    while True:\n        try:\n            index = child.expect(patterns)\n            if isinstance(child.after, child.allowed_string_types):\n                child_result_list.append(child.before + child.after)\n            else:\n                # child.after may have been a TIMEOUT or EOF,\n                # which we don't want appended to the list.\n                child_result_list.append(child.before)\n            if isinstance(responses[index], child.allowed_string_types):\n                child.send(responses[index])\n            elif (isinstance(responses[index], types.FunctionType) or\n                  isinstance(responses[index], types.MethodType)):\n                callback_result = responses[index](locals())\n                sys.stdout.flush()\n                if isinstance(callback_result, child.allowed_string_types):\n                    child.send(callback_result)\n                elif callback_result:\n                    break\n            else:\n                raise TypeError(\"parameter `event' at index {index} must be \"\n                                \"a string, method, or function: {value!r}\"\n                                .format(index=index, value=responses[index]))\n            event_count = event_count + 1\n        except TIMEOUT:\n            child_result_list.append(child.before)\n            break\n        except EOF:\n            child_result_list.append(child.before)\n            break\n    child_result = child.string_type().join(child_result_list)\n    if withexitstatus:\n        child.close()\n        return (child_result, child.exitstatus)\n    else:\n        return child_result", "response": "Runs a command and returns the output of the command."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns a command in a sequence of threads.", "response": "def runu(command, timeout=30, withexitstatus=False, events=None,\n        extra_args=None, logfile=None, cwd=None, env=None, **kwargs):\n    \"\"\"Deprecated: pass encoding to run() instead.\n    \"\"\"\n    kwargs.setdefault('encoding', 'utf-8')\n    return run(command, timeout=timeout, withexitstatus=withexitstatus,\n                events=events, extra_args=extra_args, logfile=logfile, cwd=cwd,\n                env=env, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntry to look up the process tree via the output of ps.", "response": "def get_process_mapping():\n    \"\"\"Try to look up the process tree via the output of `ps`.\n    \"\"\"\n    output = subprocess.check_output([\n        'ps', '-ww', '-o', 'pid=', '-o', 'ppid=', '-o', 'args=',\n    ])\n    if not isinstance(output, str):\n        output = output.decode(sys.stdout.encoding)\n    processes = {}\n    for line in output.split('\\n'):\n        try:\n            pid, ppid, args = line.strip().split(None, 2)\n        except ValueError:\n            continue\n        processes[pid] = Process(\n            args=tuple(shlex.split(args)), pid=pid, ppid=ppid,\n        )\n    return processes"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbuild the path URL to use.", "response": "def path_url(self):\n        \"\"\"Build the path URL to use.\"\"\"\n\n        url = []\n\n        p = urlsplit(self.url)\n\n        path = p.path\n        if not path:\n            path = '/'\n\n        url.append(path)\n\n        query = p.query\n        if query:\n            url.append('?')\n            url.append(query)\n\n        return ''.join(url)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _encode_params(data):\n\n        if isinstance(data, (str, bytes)):\n            return data\n        elif hasattr(data, 'read'):\n            return data\n        elif hasattr(data, '__iter__'):\n            result = []\n            for k, vs in to_key_val_list(data):\n                if isinstance(vs, basestring) or not hasattr(vs, '__iter__'):\n                    vs = [vs]\n                for v in vs:\n                    if v is not None:\n                        result.append(\n                            (k.encode('utf-8') if isinstance(k, str) else k,\n                             v.encode('utf-8') if isinstance(v, str) else v))\n            return urlencode(result, doseq=True)\n        else:\n            return data", "response": "Encode parameters in a piece of data."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds the body for a multipart / form - data request.", "response": "def _encode_files(files, data):\n        \"\"\"Build the body for a multipart/form-data request.\n\n        Will successfully encode files when passed as a dict or a list of\n        tuples. Order is retained if data is a list of tuples but arbitrary\n        if parameters are supplied as a dict.\n        The tuples may be 2-tuples (filename, fileobj), 3-tuples (filename, fileobj, contentype)\n        or 4-tuples (filename, fileobj, contentype, custom_headers).\n        \"\"\"\n        if (not files):\n            raise ValueError(\"Files must be provided.\")\n        elif isinstance(data, basestring):\n            raise ValueError(\"Data must not be a string.\")\n\n        new_fields = []\n        fields = to_key_val_list(data or {})\n        files = to_key_val_list(files or {})\n\n        for field, val in fields:\n            if isinstance(val, basestring) or not hasattr(val, '__iter__'):\n                val = [val]\n            for v in val:\n                if v is not None:\n                    # Don't call str() on bytestrings: in Py3 it all goes wrong.\n                    if not isinstance(v, bytes):\n                        v = str(v)\n\n                    new_fields.append(\n                        (field.decode('utf-8') if isinstance(field, bytes) else field,\n                         v.encode('utf-8') if isinstance(v, str) else v))\n\n        for (k, v) in files:\n            # support for explicit filename\n            ft = None\n            fh = None\n            if isinstance(v, (tuple, list)):\n                if len(v) == 2:\n                    fn, fp = v\n                elif len(v) == 3:\n                    fn, fp, ft = v\n                else:\n                    fn, fp, ft, fh = v\n            else:\n                fn = guess_filename(v) or k\n                fp = v\n\n            if isinstance(fp, (str, bytes, bytearray)):\n                fdata = fp\n            elif hasattr(fp, 'read'):\n                fdata = fp.read()\n            elif fp is None:\n                continue\n            else:\n                fdata = fp\n\n            rf = RequestField(name=k, data=fdata, filename=fn, headers=fh)\n            rf.make_multipart(content_type=ft)\n            new_fields.append(rf)\n\n        body, content_type = encode_multipart_formdata(new_fields)\n\n        return body, content_type"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef register_hook(self, event, hook):\n\n        if event not in self.hooks:\n            raise ValueError('Unsupported event specified, with event name \"%s\"' % (event))\n\n        if isinstance(hook, Callable):\n            self.hooks[event].append(hook)\n        elif hasattr(hook, '__iter__'):\n            self.hooks[event].extend(h for h in hook if isinstance(h, Callable))", "response": "Properly register a hook."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef deregister_hook(self, event, hook):\n\n        try:\n            self.hooks[event].remove(hook)\n            return True\n        except ValueError:\n            return False", "response": "Deregister a previously registered hook. Returns True if the hook existed False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npreparing the entire request with the given parameters.", "response": "def prepare(self,\n            method=None, url=None, headers=None, files=None, data=None,\n            params=None, auth=None, cookies=None, hooks=None, json=None):\n        \"\"\"Prepares the entire request with the given parameters.\"\"\"\n\n        self.prepare_method(method)\n        self.prepare_url(url, params)\n        self.prepare_headers(headers)\n        self.prepare_cookies(cookies)\n        self.prepare_body(data, files, json)\n        self.prepare_auth(auth, url)\n\n        # Note that prepare_auth must be last to enable authentication schemes\n        # such as OAuth to work on a fully prepared request.\n\n        # This MUST go after prepare_auth. Authenticators could add a hook\n        self.prepare_hooks(hooks)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef prepare_body(self, data, files, json=None):\n\n        # Check if file, fo, generator, iterator.\n        # If not, run through normal process.\n\n        # Nottin' on you.\n        body = None\n        content_type = None\n\n        if not data and json is not None:\n            # urllib3 requires a bytes-like body. Python 2's json.dumps\n            # provides this natively, but Python 3 gives a Unicode string.\n            content_type = 'application/json'\n            body = complexjson.dumps(json)\n            if not isinstance(body, bytes):\n                body = body.encode('utf-8')\n\n        is_stream = all([\n            hasattr(data, '__iter__'),\n            not isinstance(data, (basestring, list, tuple, Mapping))\n        ])\n\n        try:\n            length = super_len(data)\n        except (TypeError, AttributeError, UnsupportedOperation):\n            length = None\n\n        if is_stream:\n            body = data\n\n            if getattr(body, 'tell', None) is not None:\n                # Record the current file position before reading.\n                # This will allow us to rewind a file in the event\n                # of a redirect.\n                try:\n                    self._body_position = body.tell()\n                except (IOError, OSError):\n                    # This differentiates from None, allowing us to catch\n                    # a failed `tell()` later when trying to rewind the body\n                    self._body_position = object()\n\n            if files:\n                raise NotImplementedError('Streamed bodies and files are mutually exclusive.')\n\n            if length:\n                self.headers['Content-Length'] = builtin_str(length)\n            else:\n                self.headers['Transfer-Encoding'] = 'chunked'\n        else:\n            # Multi-part file uploads.\n            if files:\n                (body, content_type) = self._encode_files(files, data)\n            else:\n                if data:\n                    body = self._encode_params(data)\n                    if isinstance(data, basestring) or hasattr(data, 'read'):\n                        content_type = None\n                    else:\n                        content_type = 'application/x-www-form-urlencoded'\n\n            self.prepare_content_length(body)\n\n            # Add content-type if it wasn't explicitly provided.\n            if content_type and ('content-type' not in self.headers):\n                self.headers['Content-Type'] = content_type\n\n        self.body = body", "response": "Prepares the HTTP body data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef prepare_content_length(self, body):\n        if body is not None:\n            length = super_len(body)\n            if length:\n                # If length exists, set it. Otherwise, we fallback\n                # to Transfer-Encoding: chunked.\n                self.headers['Content-Length'] = builtin_str(length)\n        elif self.method not in ('GET', 'HEAD') and self.headers.get('Content-Length') is None:\n            # Set Content-Length to 0 for methods that can have a body\n            # but don't provide one. (i.e. not GET or HEAD)\n            self.headers['Content-Length'] = '0'", "response": "Prepare Content - Length header based on request method and body."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npreparing the given HTTP auth data.", "response": "def prepare_auth(self, auth, url=''):\n        \"\"\"Prepares the given HTTP auth data.\"\"\"\n\n        # If no Auth is explicitly provided, extract it from the URL first.\n        if auth is None:\n            url_auth = get_auth_from_url(self.url)\n            auth = url_auth if any(url_auth) else None\n\n        if auth:\n            if isinstance(auth, tuple) and len(auth) == 2:\n                # special-case basic HTTP auth\n                auth = HTTPBasicAuth(*auth)\n\n            # Allow auth to make its changes.\n            r = auth(self)\n\n            # Update self to reflect the auth changes.\n            self.__dict__.update(r.__dict__)\n\n            # Recompute Content-Length\n            self.prepare_content_length(self.body)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprepares the given HTTP cookie data.", "response": "def prepare_cookies(self, cookies):\n        \"\"\"Prepares the given HTTP cookie data.\n\n        This function eventually generates a ``Cookie`` header from the\n        given cookies using cookielib. Due to cookielib's design, the header\n        will not be regenerated if it already exists, meaning this function\n        can only be called once for the life of the\n        :class:`PreparedRequest <PreparedRequest>` object. Any subsequent calls\n        to ``prepare_cookies`` will have no actual effect, unless the \"Cookie\"\n        header is removed beforehand.\n        \"\"\"\n        if isinstance(cookies, cookielib.CookieJar):\n            self._cookies = cookies\n        else:\n            self._cookies = cookiejar_from_dict(cookies)\n\n        cookie_header = get_cookie_header(self._cookies, self)\n        if cookie_header is not None:\n            self.headers['Cookie'] = cookie_header"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprepares the given hooks.", "response": "def prepare_hooks(self, hooks):\n        \"\"\"Prepares the given hooks.\"\"\"\n        # hooks can be passed as None to the prepare method and to this\n        # method. To prevent iterating over None, simply use an empty list\n        # if hooks is False-y\n        hooks = hooks or []\n        for event in hooks:\n            self.register_hook(event, hooks[event])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntruing if this Response is one of the permanent versions of redirect.", "response": "def is_permanent_redirect(self):\n        \"\"\"True if this Response one of the permanent versions of redirect.\"\"\"\n        return ('location' in self.headers and self.status_code in (codes.moved_permanently, codes.permanent_redirect))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef text(self):\n\n        # Try charset from content-type\n        content = None\n        encoding = self.encoding\n\n        if not self.content:\n            return str('')\n\n        # Fallback to auto-detected encoding.\n        if self.encoding is None:\n            encoding = self.apparent_encoding\n\n        # Decode unicode from given encoding.\n        try:\n            content = str(self.content, encoding, errors='replace')\n        except (LookupError, TypeError):\n            # A LookupError is raised if the encoding was not found which could\n            # indicate a misspelling or similar mistake.\n            #\n            # A TypeError can be raised if encoding is None\n            #\n            # So we try blindly encoding.\n            content = str(self.content, errors='replace')\n\n        return content", "response": "Return the content of the response in unicode."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nraise HTTPError if one occurred.", "response": "def raise_for_status(self):\n        \"\"\"Raises stored :class:`HTTPError`, if one occurred.\"\"\"\n\n        http_error_msg = ''\n        if isinstance(self.reason, bytes):\n            # We attempt to decode utf-8 first because some servers\n            # choose to localize their reason strings. If the string\n            # isn't utf-8, we fall back to iso-8859-1 for all other\n            # encodings. (See PR #3538)\n            try:\n                reason = self.reason.decode('utf-8')\n            except UnicodeDecodeError:\n                reason = self.reason.decode('iso-8859-1')\n        else:\n            reason = self.reason\n\n        if 400 <= self.status_code < 500:\n            http_error_msg = u'%s Client Error: %s for url: %s' % (self.status_code, reason, self.url)\n\n        elif 500 <= self.status_code < 600:\n            http_error_msg = u'%s Server Error: %s for url: %s' % (self.status_code, reason, self.url)\n\n        if http_error_msg:\n            raise HTTPError(http_error_msg, response=self)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nclose the connection to the pool.", "response": "def close(self):\n        \"\"\"Releases the connection back to the pool. Once this method has been\n        called the underlying ``raw`` object must not be accessed again.\n\n        *Note: Should not normally need to be called explicitly.*\n        \"\"\"\n        if not self._content_consumed:\n            self.raw.close()\n\n        release_conn = getattr(self.raw, 'release_conn', None)\n        if release_conn is not None:\n            release_conn()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a message for an EnvironmentError.", "response": "def create_env_error_message(error, show_traceback, using_user_site):\n    \"\"\"Format an error message for an EnvironmentError\n\n    It may occur anytime during the execution of the install command.\n    \"\"\"\n    parts = []\n\n    # Mention the error if we are not going to show a traceback\n    parts.append(\"Could not install packages due to an EnvironmentError\")\n    if not show_traceback:\n        parts.append(\": \")\n        parts.append(str(error))\n    else:\n        parts.append(\".\")\n\n    # Spilt the error indication from a helper message (if any)\n    parts[-1] += \"\\n\"\n\n    # Suggest useful actions to the user:\n    #  (1) using user site-packages or (2) verifying the permissions\n    if error.errno == errno.EACCES:\n        user_option_part = \"Consider using the `--user` option\"\n        permissions_part = \"Check the permissions\"\n\n        if not using_user_site:\n            parts.extend([\n                user_option_part, \" or \",\n                permissions_part.lower(),\n            ])\n        else:\n            parts.append(permissions_part)\n        parts.append(\".\\n\")\n\n    return \"\".join(parts).strip() + \"\\n\""}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprocess IPv6 addresses and return the host string.", "response": "def _ipv6_host(host, scheme):\n    \"\"\"\n    Process IPv6 address literals\n    \"\"\"\n\n    # httplib doesn't like it when we include brackets in IPv6 addresses\n    # Specifically, if we include brackets but also pass the port then\n    # httplib crazily doubles up the square brackets on the Host header.\n    # Instead, we need to make sure we never pass ``None`` as the port.\n    # However, for backward compatibility reasons we can't actually\n    # *assert* that.  See http://bugs.python.org/issue28539\n    #\n    # Also if an IPv6 address literal has a zone identifier, the\n    # percent sign might be URIencoded, convert it back into ASCII\n    if host.startswith('[') and host.endswith(']'):\n        host = host.replace('%25', '%').strip('[]')\n    if scheme in NORMALIZABLE_SCHEMES:\n        host = host.lower()\n    return host"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_conn(self, timeout=None):\n        conn = None\n        try:\n            conn = self.pool.get(block=self.block, timeout=timeout)\n\n        except AttributeError:  # self.pool is None\n            raise ClosedPoolError(self, \"Pool is closed.\")\n\n        except queue.Empty:\n            if self.block:\n                raise EmptyPoolError(self,\n                                     \"Pool reached maximum size and no more \"\n                                     \"connections are allowed.\")\n            pass  # Oh well, we'll create a new connection then\n\n        # If this is a persistent connection, check if it got disconnected\n        if conn and is_connection_dropped(conn):\n            log.debug(\"Resetting dropped connection: %s\", self.host)\n            conn.close()\n            if getattr(conn, 'auto_open', 1) == 0:\n                # This is a proxied connection that has been mutated by\n                # httplib._tunnel() and cannot be reused (since it would\n                # attempt to bypass the proxy)\n                conn = None\n\n        return conn or self._new_conn()", "response": "Get a connection from the pool. Will return a pooled connection if one is available and raise an exception if it is not."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _raise_timeout(self, err, url, timeout_value):\n\n        if isinstance(err, SocketTimeout):\n            raise ReadTimeoutError(self, url, \"Read timed out. (read timeout=%s)\" % timeout_value)\n\n        # See the above comment about EAGAIN in Python 3. In Python 2 we have\n        # to specifically catch it and throw the timeout error\n        if hasattr(err, 'errno') and err.errno in _blocking_errnos:\n            raise ReadTimeoutError(self, url, \"Read timed out. (read timeout=%s)\" % timeout_value)\n\n        # Catch possible read timeouts thrown as SSL errors. If not the\n        # case, rethrow the original. We need to do this because of:\n        # http://bugs.python.org/issue10272\n        if 'timed out' in str(err) or 'did not complete (read)' in str(err):  # Python < 2.7.4\n            raise ReadTimeoutError(self, url, \"Read timed out. (read timeout=%s)\" % timeout_value)", "response": "Raises a ReadTimeoutError or pass."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a connection from the pool and perform an HTTP request. This is the lowest level call for making a request, so you'll need to specify all the raw details. .. note:: More commonly, it's appropriate to use a convenience method provided by :class:`.RequestMethods`, such as :meth:`request`. .. note:: `release_conn` will only behave as expected if `preload_content=False` because we want to make `preload_content=False` the default behaviour someday soon without breaking backwards compatibility. :param method: HTTP request method (such as GET, POST, PUT, etc.) :param body: Data to send in the request body (useful for creating POST requests, see HTTPConnectionPool.post_url for more convenience). :param headers: Dictionary of custom headers to send, such as User-Agent, If-None-Match, etc. If None, pool headers are used. If provided, these headers completely replace any pool-specific headers. :param retries: Configure the number of retries to allow before raising a :class:`~urllib3.exceptions.MaxRetryError` exception. Pass ``None`` to retry until you receive a response. Pass a :class:`~urllib3.util.retry.Retry` object for fine-grained control over different types of retries. Pass an integer number to retry connection errors that many times, but no other types of errors. Pass zero to never retry. If ``False``, then retries are disabled and any exception is raised immediately. Also, instead of raising a MaxRetryError on redirects, the redirect response will be returned. :type retries: :class:`~urllib3.util.retry.Retry`, False, or an int. :param redirect: If True, automatically handle redirects (status codes 301, 302, 303, 307, 308). Each redirect counts as a retry. Disabling retries will disable redirect, too. :param assert_same_host: If ``True``, will make sure that the host of the pool requests is consistent else will raise HostChangedError. When False, you can use the pool on an HTTP proxy and request foreign hosts. :param timeout: If specified, overrides the default timeout for this one request. It may be a float (in seconds) or an instance of :class:`urllib3.util.Timeout`. :param pool_timeout: If set and the pool is set to block=True, then this method will block for ``pool_timeout`` seconds and raise EmptyPoolError if no connection is available within the time period. :param release_conn: If False, then the urlopen call will not release the connection back into the pool once a response is received (but will release if you read the entire contents of the response such as when `preload_content=True`). This is useful if you're not preloading the response's content immediately. You will need to call ``r.release_conn()`` on the response ``r`` to return the connection back into the pool. If None, it takes the value of ``response_kw.get('preload_content', True)``. :param chunked: If True, urllib3 will send the body using chunked transfer encoding. Otherwise, urllib3 will send the body using the standard content-length form. Defaults to False. :param int body_pos: Position to seek to in file-like body in the event of a retry or redirect. Typically this won't need to be set because urllib3 will auto-populate the value when needed. :param \\\\**response_kw: Additional parameters are passed to :meth:`urllib3.response.HTTPResponse.from_httplib`", "response": "def urlopen(self, method, url, body=None, headers=None, retries=None,\n                redirect=True, assert_same_host=True, timeout=_Default,\n                pool_timeout=None, release_conn=None, chunked=False,\n                body_pos=None, **response_kw):\n        \"\"\"\n        Get a connection from the pool and perform an HTTP request. This is the\n        lowest level call for making a request, so you'll need to specify all\n        the raw details.\n\n        .. note::\n\n           More commonly, it's appropriate to use a convenience method provided\n           by :class:`.RequestMethods`, such as :meth:`request`.\n\n        .. note::\n\n           `release_conn` will only behave as expected if\n           `preload_content=False` because we want to make\n           `preload_content=False` the default behaviour someday soon without\n           breaking backwards compatibility.\n\n        :param method:\n            HTTP request method (such as GET, POST, PUT, etc.)\n\n        :param body:\n            Data to send in the request body (useful for creating\n            POST requests, see HTTPConnectionPool.post_url for\n            more convenience).\n\n        :param headers:\n            Dictionary of custom headers to send, such as User-Agent,\n            If-None-Match, etc. If None, pool headers are used. If provided,\n            these headers completely replace any pool-specific headers.\n\n        :param retries:\n            Configure the number of retries to allow before raising a\n            :class:`~urllib3.exceptions.MaxRetryError` exception.\n\n            Pass ``None`` to retry until you receive a response. Pass a\n            :class:`~urllib3.util.retry.Retry` object for fine-grained control\n            over different types of retries.\n            Pass an integer number to retry connection errors that many times,\n            but no other types of errors. Pass zero to never retry.\n\n            If ``False``, then retries are disabled and any exception is raised\n            immediately. Also, instead of raising a MaxRetryError on redirects,\n            the redirect response will be returned.\n\n        :type retries: :class:`~urllib3.util.retry.Retry`, False, or an int.\n\n        :param redirect:\n            If True, automatically handle redirects (status codes 301, 302,\n            303, 307, 308). Each redirect counts as a retry. Disabling retries\n            will disable redirect, too.\n\n        :param assert_same_host:\n            If ``True``, will make sure that the host of the pool requests is\n            consistent else will raise HostChangedError. When False, you can\n            use the pool on an HTTP proxy and request foreign hosts.\n\n        :param timeout:\n            If specified, overrides the default timeout for this one\n            request. It may be a float (in seconds) or an instance of\n            :class:`urllib3.util.Timeout`.\n\n        :param pool_timeout:\n            If set and the pool is set to block=True, then this method will\n            block for ``pool_timeout`` seconds and raise EmptyPoolError if no\n            connection is available within the time period.\n\n        :param release_conn:\n            If False, then the urlopen call will not release the connection\n            back into the pool once a response is received (but will release if\n            you read the entire contents of the response such as when\n            `preload_content=True`). This is useful if you're not preloading\n            the response's content immediately. You will need to call\n            ``r.release_conn()`` on the response ``r`` to return the connection\n            back into the pool. If None, it takes the value of\n            ``response_kw.get('preload_content', True)``.\n\n        :param chunked:\n            If True, urllib3 will send the body using chunked transfer\n            encoding. Otherwise, urllib3 will send the body using the standard\n            content-length form. Defaults to False.\n\n        :param int body_pos:\n            Position to seek to in file-like body in the event of a retry or\n            redirect. Typically this won't need to be set because urllib3 will\n            auto-populate the value when needed.\n\n        :param \\\\**response_kw:\n            Additional parameters are passed to\n            :meth:`urllib3.response.HTTPResponse.from_httplib`\n        \"\"\"\n        if headers is None:\n            headers = self.headers\n\n        if not isinstance(retries, Retry):\n            retries = Retry.from_int(retries, redirect=redirect, default=self.retries)\n\n        if release_conn is None:\n            release_conn = response_kw.get('preload_content', True)\n\n        # Check host\n        if assert_same_host and not self.is_same_host(url):\n            raise HostChangedError(self, url, retries)\n\n        conn = None\n\n        # Track whether `conn` needs to be released before\n        # returning/raising/recursing. Update this variable if necessary, and\n        # leave `release_conn` constant throughout the function. That way, if\n        # the function recurses, the original value of `release_conn` will be\n        # passed down into the recursive call, and its value will be respected.\n        #\n        # See issue #651 [1] for details.\n        #\n        # [1] <https://github.com/shazow/urllib3/issues/651>\n        release_this_conn = release_conn\n\n        # Merge the proxy headers. Only do this in HTTP. We have to copy the\n        # headers dict so we can safely change it without those changes being\n        # reflected in anyone else's copy.\n        if self.scheme == 'http':\n            headers = headers.copy()\n            headers.update(self.proxy_headers)\n\n        # Must keep the exception bound to a separate variable or else Python 3\n        # complains about UnboundLocalError.\n        err = None\n\n        # Keep track of whether we cleanly exited the except block. This\n        # ensures we do proper cleanup in finally.\n        clean_exit = False\n\n        # Rewind body position, if needed. Record current position\n        # for future rewinds in the event of a redirect/retry.\n        body_pos = set_file_position(body, body_pos)\n\n        try:\n            # Request a connection from the queue.\n            timeout_obj = self._get_timeout(timeout)\n            conn = self._get_conn(timeout=pool_timeout)\n\n            conn.timeout = timeout_obj.connect_timeout\n\n            is_new_proxy_conn = self.proxy is not None and not getattr(conn, 'sock', None)\n            if is_new_proxy_conn:\n                self._prepare_proxy(conn)\n\n            # Make the request on the httplib connection object.\n            httplib_response = self._make_request(conn, method, url,\n                                                  timeout=timeout_obj,\n                                                  body=body, headers=headers,\n                                                  chunked=chunked)\n\n            # If we're going to release the connection in ``finally:``, then\n            # the response doesn't need to know about the connection. Otherwise\n            # it will also try to release it and we'll have a double-release\n            # mess.\n            response_conn = conn if not release_conn else None\n\n            # Pass method to Response for length checking\n            response_kw['request_method'] = method\n\n            # Import httplib's response into our own wrapper object\n            response = self.ResponseCls.from_httplib(httplib_response,\n                                                     pool=self,\n                                                     connection=response_conn,\n                                                     retries=retries,\n                                                     **response_kw)\n\n            # Everything went great!\n            clean_exit = True\n\n        except queue.Empty:\n            # Timed out by queue.\n            raise EmptyPoolError(self, \"No pool connections are available.\")\n\n        except (TimeoutError, HTTPException, SocketError, ProtocolError,\n                BaseSSLError, SSLError, CertificateError) as e:\n            # Discard the connection for these exceptions. It will be\n            # replaced during the next _get_conn() call.\n            clean_exit = False\n            if isinstance(e, (BaseSSLError, CertificateError)):\n                e = SSLError(e)\n            elif isinstance(e, (SocketError, NewConnectionError)) and self.proxy:\n                e = ProxyError('Cannot connect to proxy.', e)\n            elif isinstance(e, (SocketError, HTTPException)):\n                e = ProtocolError('Connection aborted.', e)\n\n            retries = retries.increment(method, url, error=e, _pool=self,\n                                        _stacktrace=sys.exc_info()[2])\n            retries.sleep()\n\n            # Keep track of the error for the retry warning.\n            err = e\n\n        finally:\n            if not clean_exit:\n                # We hit some kind of exception, handled or otherwise. We need\n                # to throw the connection away unless explicitly told not to.\n                # Close the connection, set the variable to None, and make sure\n                # we put the None back in the pool to avoid leaking it.\n                conn = conn and conn.close()\n                release_this_conn = True\n\n            if release_this_conn:\n                # Put the connection back to be reused. If the connection is\n                # expired then it will be None, which will get replaced with a\n                # fresh connection during _get_conn.\n                self._put_conn(conn)\n\n        if not conn:\n            # Try again\n            log.warning(\"Retrying (%r) after connection \"\n                        \"broken by '%r': %s\", retries, err, url)\n            return self.urlopen(method, url, body, headers, retries,\n                                redirect, assert_same_host,\n                                timeout=timeout, pool_timeout=pool_timeout,\n                                release_conn=release_conn, body_pos=body_pos,\n                                **response_kw)\n\n        def drain_and_release_conn(response):\n            try:\n                # discard any remaining response body, the connection will be\n                # released back to the pool once the entire response is read\n                response.read()\n            except (TimeoutError, HTTPException, SocketError, ProtocolError,\n                    BaseSSLError, SSLError) as e:\n                pass\n\n        # Handle redirect?\n        redirect_location = redirect and response.get_redirect_location()\n        if redirect_location:\n            if response.status == 303:\n                method = 'GET'\n\n            try:\n                retries = retries.increment(method, url, response=response, _pool=self)\n            except MaxRetryError:\n                if retries.raise_on_redirect:\n                    # Drain and release the connection for this response, since\n                    # we're not returning it to be released manually.\n                    drain_and_release_conn(response)\n                    raise\n                return response\n\n            # drain and return the connection to the pool before recursing\n            drain_and_release_conn(response)\n\n            retries.sleep_for_retry(response)\n            log.debug(\"Redirecting %s -> %s\", url, redirect_location)\n            return self.urlopen(\n                method, redirect_location, body, headers,\n                retries=retries, redirect=redirect,\n                assert_same_host=assert_same_host,\n                timeout=timeout, pool_timeout=pool_timeout,\n                release_conn=release_conn, body_pos=body_pos,\n                **response_kw)\n\n        # Check if we should retry the HTTP response.\n        has_retry_after = bool(response.getheader('Retry-After'))\n        if retries.is_retry(method, response.status, has_retry_after):\n            try:\n                retries = retries.increment(method, url, response=response, _pool=self)\n            except MaxRetryError:\n                if retries.raise_on_status:\n                    # Drain and release the connection for this response, since\n                    # we're not returning it to be released manually.\n                    drain_and_release_conn(response)\n                    raise\n                return response\n\n            # drain and return the connection to the pool before recursing\n            drain_and_release_conn(response)\n\n            retries.sleep(response)\n            log.debug(\"Retry: %s\", url)\n            return self.urlopen(\n                method, url, body, headers,\n                retries=retries, redirect=redirect,\n                assert_same_host=assert_same_host,\n                timeout=timeout, pool_timeout=pool_timeout,\n                release_conn=release_conn,\n                body_pos=body_pos, **response_kw)\n\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npreparing the connection for ssl_wrap_socket.", "response": "def _prepare_conn(self, conn):\n        \"\"\"\n        Prepare the ``connection`` for :meth:`urllib3.util.ssl_wrap_socket`\n        and establish the tunnel if proxy is used.\n        \"\"\"\n\n        if isinstance(conn, VerifiedHTTPSConnection):\n            conn.set_cert(key_file=self.key_file,\n                          cert_file=self.cert_file,\n                          cert_reqs=self.cert_reqs,\n                          ca_certs=self.ca_certs,\n                          ca_cert_dir=self.ca_cert_dir,\n                          assert_hostname=self.assert_hostname,\n                          assert_fingerprint=self.assert_fingerprint)\n            conn.ssl_version = self.ssl_version\n        return conn"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nestablishing tunnel connection early because otherwise httplib would improperly set Host header to proxy s IP and port.", "response": "def _prepare_proxy(self, conn):\n        \"\"\"\n        Establish tunnel connection early, because otherwise httplib\n        would improperly set Host: header to proxy's IP:port.\n        \"\"\"\n        conn.set_tunnel(self._proxy_host, self.port, self.proxy_headers)\n        conn.connect()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reload_system_path(self):\n        # type: () -> None\n        \"\"\"\n        Rebuilds the base system path and all of the contained finders within it.\n\n        This will re-apply any changes to the environment or any version changes on the system.\n        \"\"\"\n\n        if self._system_path is not None:\n            self._system_path.clear_caches()\n        self._system_path = None\n        six.moves.reload_module(pyfinder_path)\n        self._system_path = self.create_system_path()", "response": "Rebuilds the base system path and all of the contained finders within it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a dict containing the python version of the current version.", "response": "def find_python_version(\n        self, major=None, minor=None, patch=None, pre=None, dev=None, arch=None, name=None\n    ):\n        # type: (Optional[Union[str, int]], Optional[int], Optional[int], Optional[bool], Optional[bool], Optional[str], Optional[str]) -> PathEntry\n        \"\"\"\n        Find the python version which corresponds most closely to the version requested.\n\n        :param Union[str, int] major: The major version to look for, or the full version, or the name of the target version.\n        :param Optional[int] minor: The minor version. If provided, disables string-based lookups from the major version field.\n        :param Optional[int] patch: The patch version.\n        :param Optional[bool] pre: If provided, specifies whether to search pre-releases.\n        :param Optional[bool] dev: If provided, whether to search dev-releases.\n        :param Optional[str] arch: If provided, which architecture to search.\n        :param Optional[str] name: *Name* of the target python, e.g. ``anaconda3-5.3.0``\n        :return: A new *PathEntry* pointer at a matching python version, if one can be located.\n        :rtype: :class:`pythonfinder.models.path.PathEntry`\n        \"\"\"\n\n        from .models import PythonVersion\n\n        minor = int(minor) if minor is not None else minor\n        patch = int(patch) if patch is not None else patch\n\n        version_dict = {\n            \"minor\": minor,\n            \"patch\": patch,\n        }  # type: Dict[str, Union[str, int, Any]]\n\n        if (\n            isinstance(major, six.string_types)\n            and pre is None\n            and minor is None\n            and dev is None\n            and patch is None\n        ):\n            if arch is None and \"-\" in major and major[0].isdigit():\n                orig_string = \"{0!s}\".format(major)\n                major, _, arch = major.rpartition(\"-\")\n                if arch.startswith(\"x\"):\n                    arch = arch.lstrip(\"x\")\n                if arch.lower().endswith(\"bit\"):\n                    arch = arch.lower().replace(\"bit\", \"\")\n                if not (arch.isdigit() and (int(arch) & int(arch) - 1) == 0):\n                    major = orig_string\n                    arch = None\n                else:\n                    arch = \"{0}bit\".format(arch)\n                try:\n                    version_dict = PythonVersion.parse(major)\n                except (ValueError, InvalidPythonVersion):\n                    if name is None:\n                        name = \"{0!s}\".format(major)\n                        major = None\n                    version_dict = {}\n            elif major[0].isalpha():\n                name = \"%s\" % major\n                major = None\n            else:\n                if \".\" in major and all(part.isdigit() for part in major.split(\".\")[:2]):\n                    match = version_re.match(major)\n                    version_dict = match.groupdict()\n                    version_dict[\"is_prerelease\"] = bool(\n                        version_dict.get(\"prerel\", False)\n                    )\n                    version_dict[\"is_devrelease\"] = bool(version_dict.get(\"dev\", False))\n                else:\n                    version_dict = {\n                        \"major\": major,\n                        \"minor\": minor,\n                        \"patch\": patch,\n                        \"pre\": pre,\n                        \"dev\": dev,\n                        \"arch\": arch,\n                    }\n            if version_dict.get(\"minor\") is not None:\n                minor = int(version_dict[\"minor\"])\n            if version_dict.get(\"patch\") is not None:\n                patch = int(version_dict[\"patch\"])\n            if version_dict.get(\"major\") is not None:\n                major = int(version_dict[\"major\"])\n            _pre = version_dict.get(\"is_prerelease\", pre)\n            pre = bool(_pre) if _pre is not None else pre\n            _dev = version_dict.get(\"is_devrelease\", dev)\n            dev = bool(_dev) if _dev is not None else dev\n            arch = (\n                version_dict.get(\"architecture\", None) if arch is None else arch\n            )  # type: ignore\n        if os.name == \"nt\" and self.windows_finder is not None:\n            match = self.windows_finder.find_python_version(\n                major=major,\n                minor=minor,\n                patch=patch,\n                pre=pre,\n                dev=dev,\n                arch=arch,\n                name=name,\n            )\n            if match:\n                return match\n        return self.system_path.find_python_version(\n            major=major, minor=minor, patch=patch, pre=pre, dev=dev, arch=arch, name=name\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_shell(pid=None, max_depth=6):\n    pid = str(pid or os.getpid())\n    mapping = _get_process_mapping()\n    login_shell = os.environ.get('SHELL', '')\n    for _ in range(max_depth):\n        try:\n            proc = mapping[pid]\n        except KeyError:\n            break\n        name = os.path.basename(proc.args[0]).lower()\n        if name in SHELL_NAMES:\n            return (name, proc.args[0])\n        elif proc.args[0].startswith('-'):\n            # This is the login shell. Use the SHELL environ if possible\n            # because it provides better information.\n            if login_shell:\n                name = login_shell.lower()\n            else:\n                name = proc.args[0][1:].lower()\n            return (os.path.basename(name), name)\n        pid = proc.ppid     # Go up one level.\n    return None", "response": "Get the shell that the supplied pid or os. getpid is running in."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmaking the given class un - picklable.", "response": "def _make_class_unpicklable(cls):\n    \"\"\"Make the given class un-picklable.\"\"\"\n    def _break_on_call_reduce(self, protocol=None):\n        raise TypeError('%r cannot be pickled' % self)\n    cls.__reduce_ex__ = _break_on_call_reduce\n    cls.__module__ = '<unknown>'"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert all constants from source to new Enum subclass.", "response": "def _convert(cls, name, module, filter, source=None):\n    \"\"\"\n    Create a new Enum subclass that replaces a collection of global constants\n    \"\"\"\n    # convert all constants from source (or module) that pass filter() to\n    # a new Enum called name, and export the enum and its members back to\n    # module;\n    # also, replace the __reduce_ex__ method so unpickling works in\n    # previous Python versions\n    module_globals = vars(_sys.modules[module])\n    if source:\n        source = vars(source)\n    else:\n        source = module_globals\n    members = dict((name, value) for name, value in source.items() if filter(name))\n    cls = cls(name, members, module=module)\n    cls.__reduce_ex__ = _reduce_ex_by_name\n    module_globals.update(cls.__members__)\n    module_globals[name] = cls\n    return cls"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unique(enumeration):\n    duplicates = []\n    for name, member in enumeration.__members__.items():\n        if name != member.name:\n            duplicates.append((name, member.name))\n    if duplicates:\n        duplicate_names = ', '.join(\n                [\"%s -> %s\" % (alias, name) for (alias, name) in duplicates]\n                )\n        raise ValueError('duplicate names found in %r: %s' %\n                (enumeration, duplicate_names)\n                )\n    return enumeration", "response": "Class decorator that ensures only unique members exist in an enumeration."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_mixins_(bases):\n        if not bases or Enum is None:\n            return object, Enum\n\n\n        # double check that we are not subclassing a class with existing\n        # enumeration members; while we're at it, see if any other data\n        # type has been mixed in so we can use the correct __new__\n        member_type = first_enum = None\n        for base in bases:\n            if  (base is not Enum and\n                    issubclass(base, Enum) and\n                    base._member_names_):\n                raise TypeError(\"Cannot extend enumerations\")\n        # base is now the last base in bases\n        if not issubclass(base, Enum):\n            raise TypeError(\"new enumerations must be created as \"\n                    \"`ClassName([mixin_type,] enum_type)`\")\n\n        # get correct mix-in type (either mix-in type of Enum subclass, or\n        # first base if last base is Enum)\n        if not issubclass(bases[0], Enum):\n            member_type = bases[0]     # first data type\n            first_enum = bases[-1]  # enum type\n        else:\n            for base in bases[0].__mro__:\n                # most common: (IntEnum, int, Enum, object)\n                # possible:    (<Enum 'AutoIntEnum'>, <Enum 'IntEnum'>,\n                #               <class 'int'>, <Enum 'Enum'>,\n                #               <class 'object'>)\n                if issubclass(base, Enum):\n                    if first_enum is None:\n                        first_enum = base\n                else:\n                    if member_type is None:\n                        member_type = base\n\n        return member_type, first_enum", "response": "Returns the type for creating enum members and the first inherited enumeration class."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a Basic Auth string.", "response": "def _basic_auth_str(username, password):\n    \"\"\"Returns a Basic Auth string.\"\"\"\n\n    # \"I want us to put a big-ol' comment on top of it that\n    # says that this behaviour is dumb but we need to preserve\n    # it because people are relying on it.\"\n    #    - Lukasa\n    #\n    # These are here solely to maintain backwards compatibility\n    # for things like ints. This will be removed in 3.0.0.\n    if not isinstance(username, basestring):\n        warnings.warn(\n            \"Non-string usernames will no longer be supported in Requests \"\n            \"3.0.0. Please convert the object you've passed in ({!r}) to \"\n            \"a string or bytes object in the near future to avoid \"\n            \"problems.\".format(username),\n            category=DeprecationWarning,\n        )\n        username = str(username)\n\n    if not isinstance(password, basestring):\n        warnings.warn(\n            \"Non-string passwords will no longer be supported in Requests \"\n            \"3.0.0. Please convert the object you've passed in ({!r}) to \"\n            \"a string or bytes object in the near future to avoid \"\n            \"problems.\".format(password),\n            category=DeprecationWarning,\n        )\n        password = str(password)\n    # -- End Removal --\n\n    if isinstance(username, str):\n        username = username.encode('latin1')\n\n    if isinstance(password, str):\n        password = password.encode('latin1')\n\n    authstr = 'Basic ' + to_native_string(\n        b64encode(b':'.join((username, password))).strip()\n    )\n\n    return authstr"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef format_for_columns(pkgs, options):\n    running_outdated = options.outdated\n    # Adjust the header for the `pip list --outdated` case.\n    if running_outdated:\n        header = [\"Package\", \"Version\", \"Latest\", \"Type\"]\n    else:\n        header = [\"Package\", \"Version\"]\n\n    data = []\n    if options.verbose >= 1 or any(dist_is_editable(x) for x in pkgs):\n        header.append(\"Location\")\n    if options.verbose >= 1:\n        header.append(\"Installer\")\n\n    for proj in pkgs:\n        # if we're working on the 'outdated' list, separate out the\n        # latest_version and type\n        row = [proj.project_name, proj.version]\n\n        if running_outdated:\n            row.append(proj.latest_version)\n            row.append(proj.latest_filetype)\n\n        if options.verbose >= 1 or dist_is_editable(proj):\n            row.append(proj.location)\n        if options.verbose >= 1:\n            row.append(get_installer(proj))\n\n        data.append(row)\n\n    return data, header", "response": "Convert the package data into something usable\n    by output_package_listing_columns."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _build_package_finder(self, options, index_urls, session):\n        return PackageFinder(\n            find_links=options.find_links,\n            index_urls=index_urls,\n            allow_all_prereleases=options.pre,\n            trusted_hosts=options.trusted_hosts,\n            session=session,\n        )", "response": "Build a package finder appropriate to this list command."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _build_shebang(self, executable, post_interp):\n        if os.name != 'posix':\n            simple_shebang = True\n        else:\n            # Add 3 for '#!' prefix and newline suffix.\n            shebang_length = len(executable) + len(post_interp) + 3\n            if sys.platform == 'darwin':\n                max_shebang_length = 512\n            else:\n                max_shebang_length = 127\n            simple_shebang = ((b' ' not in executable) and\n                              (shebang_length <= max_shebang_length))\n\n        if simple_shebang:\n            result = b'#!' + executable + post_interp + b'\\n'\n        else:\n            result = b'#!/bin/sh\\n'\n            result += b\"'''exec' \" + executable + post_interp + b' \"$0\" \"$@\"\\n'\n            result += b\"' '''\"\n        return result", "response": "Build a shebang line for the entry - point of the process."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmaking a script from a specific entry specification.", "response": "def make(self, specification, options=None):\n        \"\"\"\n        Make a script.\n\n        :param specification: The specification, which is either a valid export\n                              entry specification (to make a script from a\n                              callable) or a filename (to make a script by\n                              copying from a source location).\n        :param options: A dictionary of options controlling script generation.\n        :return: A list of all absolute pathnames written to.\n        \"\"\"\n        filenames = []\n        entry = get_export_entry(specification)\n        if entry is None:\n            self._copy_script(specification, filenames)\n        else:\n            self._make_script(entry, filenames, options=options)\n        return filenames"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_multiple(self, specifications, options=None):\n        filenames = []\n        for specification in specifications:\n            filenames.extend(self.make(specification, options))\n        return filenames", "response": "Takes a list of specifications and makes scripts from them and returns a list of all absolute pathnames written to them."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a generator that yields all files under a directory and optionally a list of ignored patterns.", "response": "def iter_find_files(directory, patterns, ignored=None):\n    \"\"\"Returns a generator that yields file paths under a *directory*,\n    matching *patterns* using `glob`_ syntax (e.g., ``*.txt``). Also\n    supports *ignored* patterns.\n\n    Args:\n        directory (str): Path that serves as the root of the\n            search. Yielded paths will include this as a prefix.\n        patterns (str or list): A single pattern or list of\n            glob-formatted patterns to find under *directory*.\n        ignored (str or list): A single pattern or list of\n            glob-formatted patterns to ignore.\n\n    For example, finding Python files in the directory of this module:\n\n    >>> files = set(iter_find_files(os.path.dirname(__file__), '*.py'))\n\n    Or, Python files while ignoring emacs lockfiles:\n\n    >>> filenames = iter_find_files('.', '*.py', ignored='.#*')\n\n    .. _glob: https://en.wikipedia.org/wiki/Glob_%28programming%29\n\n    \"\"\"\n    if isinstance(patterns, basestring):\n        patterns = [patterns]\n    pats_re = re.compile('|'.join([fnmatch.translate(p) for p in patterns]))\n\n    if not ignored:\n        ignored = []\n    elif isinstance(ignored, basestring):\n        ignored = [ignored]\n    ign_re = re.compile('|'.join([fnmatch.translate(p) for p in ignored]))\n    for root, dirs, files in os.walk(directory):\n        for basename in files:\n            if pats_re.match(basename):\n                if ignored and ign_re.match(basename):\n                    continue\n                filename = os.path.join(root, basename)\n                yield filename\n    return"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a FilePerms object from an integer.", "response": "def from_int(cls, i):\n        \"\"\"Create a :class:`FilePerms` object from an integer.\n\n        >>> FilePerms.from_int(0o644)  # note the leading zero-oh for octal\n        FilePerms(user='rw', group='r', other='r')\n        \"\"\"\n        i &= FULL_PERMS\n        key = ('', 'x', 'w', 'xw', 'r', 'rx', 'rw', 'rwx')\n        parts = []\n        while i:\n            parts.append(key[i & _SINGLE_FULL_PERM])\n            i >>= 3\n        parts.reverse()\n        return cls(*parts)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_path(cls, path):\n        stat_res = os.stat(path)\n        return cls.from_int(stat.S_IMODE(stat_res.st_mode))", "response": "Make a new FilePerms object based on the permissions of the file or directory at path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setup(self):\n        if os.path.lexists(self.dest_path):\n            if not self.overwrite:\n                raise OSError(errno.EEXIST,\n                              'Overwrite disabled and file already exists',\n                              self.dest_path)\n        if self.overwrite_part and os.path.lexists(self.part_path):\n            os.unlink(self.part_path)\n        self._open_part_file()\n        return", "response": "Called on context manager entry."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading a pipfile from a given path.", "response": "def load(pipfile_path=None, inject_env=True):\n    \"\"\"Loads a pipfile from a given path.\n    If none is provided, one will try to be found.\n    \"\"\"\n\n    if pipfile_path is None:\n        pipfile_path = Pipfile.find()\n\n    return Pipfile.load(filename=pipfile_path, inject_env=inject_env)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the path of a Pipfile in parent directories.", "response": "def find(max_depth=3):\n        \"\"\"Returns the path of a Pipfile in parent directories.\"\"\"\n        i = 0\n        for c, d, f in walk_up(os.getcwd()):\n            i += 1\n\n            if i < max_depth:\n                if 'Pipfile':\n                    p = os.path.join(c, 'Pipfile')\n                    if os.path.isfile(p):\n                        return p\n        raise RuntimeError('No Pipfile found!')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load(klass, filename, inject_env=True):\n        p = PipfileParser(filename=filename)\n        pipfile = klass(filename=filename)\n        pipfile.data = p.parse(inject_env=inject_env)\n        return pipfile", "response": "Load a Pipfile from a given filename."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the SHA256 of the pipfile s data.", "response": "def hash(self):\n        \"\"\"Returns the SHA256 of the pipfile's data.\"\"\"\n        content = json.dumps(self.data, sort_keys=True, separators=(\",\", \":\"))\n        return hashlib.sha256(content.encode(\"utf8\")).hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a JSON representation of the Pipfile.", "response": "def lock(self):\n        \"\"\"Returns a JSON representation of the Pipfile.\"\"\"\n        data = self.data\n        data['_meta']['hash'] = {\"sha256\": self.hash}\n        data['_meta']['pipfile-spec'] = 6\n        return json.dumps(data, indent=4, separators=(',', ': '))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nassert PEP 508 specifiers.", "response": "def assert_requirements(self):\n        \"\"\"\"Asserts PEP 508 specifiers.\"\"\"\n\n        # Support for 508's implementation_version.\n        if hasattr(sys, 'implementation'):\n            implementation_version = format_full_version(sys.implementation.version)\n        else:\n            implementation_version = \"0\"\n\n        # Default to cpython for 2.7.\n        if hasattr(sys, 'implementation'):\n            implementation_name = sys.implementation.name\n        else:\n            implementation_name = 'cpython'\n\n        lookup = {\n            'os_name': os.name,\n            'sys_platform': sys.platform,\n            'platform_machine': platform.machine(),\n            'platform_python_implementation': platform.python_implementation(),\n            'platform_release': platform.release(),\n            'platform_system': platform.system(),\n            'platform_version': platform.version(),\n            'python_version': platform.python_version()[:3],\n            'python_full_version': platform.python_version(),\n            'implementation_name': implementation_name,\n            'implementation_version': implementation_version\n        }\n\n        # Assert each specified requirement.\n        for marker, specifier in self.data['_meta']['requires'].items():\n\n            if marker in lookup:\n                try:\n                    assert lookup[marker] == specifier\n                except AssertionError:\n                    raise AssertionError('Specifier {!r} does not match {!r}.'.format(marker, specifier))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef copyfileobj(fsrc, fdst, length=16*1024):\n    while 1:\n        buf = fsrc.read(length)\n        if not buf:\n            break\n        fdst.write(buf)", "response": "copy data from file - like object fsrc to file - like object fdst"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncopying data from src to dst", "response": "def copyfile(src, dst):\n    \"\"\"Copy data from src to dst\"\"\"\n    if _samefile(src, dst):\n        raise Error(\"`%s` and `%s` are the same file\" % (src, dst))\n\n    for fn in [src, dst]:\n        try:\n            st = os.stat(fn)\n        except OSError:\n            # File most likely does not exist\n            pass\n        else:\n            # XXX What about other special files? (sockets, devices...)\n            if stat.S_ISFIFO(st.st_mode):\n                raise SpecialFileError(\"`%s` is a named pipe\" % fn)\n\n    with open(src, 'rb') as fsrc:\n        with open(dst, 'wb') as fdst:\n            copyfileobj(fsrc, fdst)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef copymode(src, dst):\n    if hasattr(os, 'chmod'):\n        st = os.stat(src)\n        mode = stat.S_IMODE(st.st_mode)\n        os.chmod(dst, mode)", "response": "Copy mode bits from src to dst"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef copystat(src, dst):\n    st = os.stat(src)\n    mode = stat.S_IMODE(st.st_mode)\n    if hasattr(os, 'utime'):\n        os.utime(dst, (st.st_atime, st.st_mtime))\n    if hasattr(os, 'chmod'):\n        os.chmod(dst, mode)\n    if hasattr(os, 'chflags') and hasattr(st, 'st_flags'):\n        try:\n            os.chflags(dst, st.st_flags)\n        except OSError as why:\n            if (not hasattr(errno, 'EOPNOTSUPP') or\n                why.errno != errno.EOPNOTSUPP):\n                raise", "response": "Copy all stat info from src to dst"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncopying data and mode bits from src to dst.", "response": "def copy(src, dst):\n    \"\"\"Copy data and mode bits (\"cp src dst\").\n\n    The destination may be a directory.\n\n    \"\"\"\n    if os.path.isdir(dst):\n        dst = os.path.join(dst, os.path.basename(src))\n    copyfile(src, dst)\n    copymode(src, dst)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncopying data and all stat info from src to dst.", "response": "def copy2(src, dst):\n    \"\"\"Copy data and all stat info (\"cp -p src dst\").\n\n    The destination may be a directory.\n\n    \"\"\"\n    if os.path.isdir(dst):\n        dst = os.path.join(dst, os.path.basename(src))\n    copyfile(src, dst)\n    copystat(src, dst)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rmtree(path, ignore_errors=False, onerror=None):\n    if ignore_errors:\n        def onerror(*args):\n            pass\n    elif onerror is None:\n        def onerror(*args):\n            raise\n    try:\n        if os.path.islink(path):\n            # symlinks to directories are forbidden, see bug #1669\n            raise OSError(\"Cannot call rmtree on a symbolic link\")\n    except OSError:\n        onerror(os.path.islink, path, sys.exc_info())\n        # can't continue even if onerror hook returns\n        return\n    names = []\n    try:\n        names = os.listdir(path)\n    except os.error:\n        onerror(os.listdir, path, sys.exc_info())\n    for name in names:\n        fullname = os.path.join(path, name)\n        try:\n            mode = os.lstat(fullname).st_mode\n        except os.error:\n            mode = 0\n        if stat.S_ISDIR(mode):\n            rmtree(fullname, ignore_errors, onerror)\n        else:\n            try:\n                os.remove(fullname)\n            except os.error:\n                onerror(os.remove, fullname, sys.exc_info())\n    try:\n        os.rmdir(path)\n    except os.error:\n        onerror(os.rmdir, path, sys.exc_info())", "response": "Recursively delete a directory tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_gid(name):\n    if getgrnam is None or name is None:\n        return None\n    try:\n        result = getgrnam(name)\n    except KeyError:\n        result = None\n    if result is not None:\n        return result[2]\n    return None", "response": "Returns a gid given a group name."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_uid(name):\n    if getpwnam is None or name is None:\n        return None\n    try:\n        result = getpwnam(name)\n    except KeyError:\n        result = None\n    if result is not None:\n        return result[2]\n    return None", "response": "Returns an uid given a user name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a tarball file from all the files under base_dir.", "response": "def _make_tarball(base_name, base_dir, compress=\"gzip\", verbose=0, dry_run=0,\n                  owner=None, group=None, logger=None):\n    \"\"\"Create a (possibly compressed) tar file from all the files under\n    'base_dir'.\n\n    'compress' must be \"gzip\" (the default), \"bzip2\", or None.\n\n    'owner' and 'group' can be used to define an owner and a group for the\n    archive that is being built. If not provided, the current owner and group\n    will be used.\n\n    The output tar file will be named 'base_name' +  \".tar\", possibly plus\n    the appropriate compression extension (\".gz\", or \".bz2\").\n\n    Returns the output filename.\n    \"\"\"\n    tar_compression = {'gzip': 'gz', None: ''}\n    compress_ext = {'gzip': '.gz'}\n\n    if _BZ2_SUPPORTED:\n        tar_compression['bzip2'] = 'bz2'\n        compress_ext['bzip2'] = '.bz2'\n\n    # flags for compression program, each element of list will be an argument\n    if compress is not None and compress not in compress_ext:\n        raise ValueError(\"bad value for 'compress', or compression format not \"\n                         \"supported : {0}\".format(compress))\n\n    archive_name = base_name + '.tar' + compress_ext.get(compress, '')\n    archive_dir = os.path.dirname(archive_name)\n\n    if not os.path.exists(archive_dir):\n        if logger is not None:\n            logger.info(\"creating %s\", archive_dir)\n        if not dry_run:\n            os.makedirs(archive_dir)\n\n    # creating the tarball\n    if logger is not None:\n        logger.info('Creating tar archive')\n\n    uid = _get_uid(owner)\n    gid = _get_gid(group)\n\n    def _set_uid_gid(tarinfo):\n        if gid is not None:\n            tarinfo.gid = gid\n            tarinfo.gname = group\n        if uid is not None:\n            tarinfo.uid = uid\n            tarinfo.uname = owner\n        return tarinfo\n\n    if not dry_run:\n        tar = tarfile.open(archive_name, 'w|%s' % tar_compression[compress])\n        try:\n            tar.add(base_dir, filter=_set_uid_gid)\n        finally:\n            tar.close()\n\n    return archive_name"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _make_zipfile(base_name, base_dir, verbose=0, dry_run=0, logger=None):\n    zip_filename = base_name + \".zip\"\n    archive_dir = os.path.dirname(base_name)\n\n    if not os.path.exists(archive_dir):\n        if logger is not None:\n            logger.info(\"creating %s\", archive_dir)\n        if not dry_run:\n            os.makedirs(archive_dir)\n\n    # If zipfile module is not available, try spawning an external 'zip'\n    # command.\n    try:\n        import zipfile\n    except ImportError:\n        zipfile = None\n\n    if zipfile is None:\n        _call_external_zip(base_dir, zip_filename, verbose, dry_run)\n    else:\n        if logger is not None:\n            logger.info(\"creating '%s' and adding '%s' to it\",\n                        zip_filename, base_dir)\n\n        if not dry_run:\n            zip = zipfile.ZipFile(zip_filename, \"w\",\n                                  compression=zipfile.ZIP_DEFLATED)\n\n            for dirpath, dirnames, filenames in os.walk(base_dir):\n                for name in filenames:\n                    path = os.path.normpath(os.path.join(dirpath, name))\n                    if os.path.isfile(path):\n                        zip.write(path, path)\n                        if logger is not None:\n                            logger.info(\"adding '%s'\", path)\n            zip.close()\n\n    return zip_filename", "response": "Create a zip file from all the files under base_dir."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of supported formats for archiving and unarchiving.", "response": "def get_archive_formats():\n    \"\"\"Returns a list of supported formats for archiving and unarchiving.\n\n    Each element of the returned sequence is a tuple (name, description)\n    \"\"\"\n    formats = [(name, registry[2]) for name, registry in\n               _ARCHIVE_FORMATS.items()]\n    formats.sort()\n    return formats"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef register_archive_format(name, function, extra_args=None, description=''):\n    if extra_args is None:\n        extra_args = []\n    if not isinstance(function, collections.Callable):\n        raise TypeError('The %s object is not callable' % function)\n    if not isinstance(extra_args, (tuple, list)):\n        raise TypeError('extra_args needs to be a sequence')\n    for element in extra_args:\n        if not isinstance(element, (tuple, list)) or len(element) !=2:\n            raise TypeError('extra_args elements are : (arg_name, value)')\n\n    _ARCHIVE_FORMATS[name] = (function, extra_args, description)", "response": "Registers a function that will be called when archive files are generated."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_archive(base_name, format, root_dir=None, base_dir=None, verbose=0,\n                 dry_run=0, owner=None, group=None, logger=None):\n    \"\"\"Create an archive file (eg. zip or tar).\n\n    'base_name' is the name of the file to create, minus any format-specific\n    extension; 'format' is the archive format: one of \"zip\", \"tar\", \"bztar\"\n    or \"gztar\".\n\n    'root_dir' is a directory that will be the root directory of the\n    archive; ie. we typically chdir into 'root_dir' before creating the\n    archive.  'base_dir' is the directory where we start archiving from;\n    ie. 'base_dir' will be the common prefix of all files and\n    directories in the archive.  'root_dir' and 'base_dir' both default\n    to the current directory.  Returns the name of the archive file.\n\n    'owner' and 'group' are used when creating a tar archive. By default,\n    uses the current owner and group.\n    \"\"\"\n    save_cwd = os.getcwd()\n    if root_dir is not None:\n        if logger is not None:\n            logger.debug(\"changing into '%s'\", root_dir)\n        base_name = os.path.abspath(base_name)\n        if not dry_run:\n            os.chdir(root_dir)\n\n    if base_dir is None:\n        base_dir = os.curdir\n\n    kwargs = {'dry_run': dry_run, 'logger': logger}\n\n    try:\n        format_info = _ARCHIVE_FORMATS[format]\n    except KeyError:\n        raise ValueError(\"unknown archive format '%s'\" % format)\n\n    func = format_info[0]\n    for arg, val in format_info[1]:\n        kwargs[arg] = val\n\n    if format != 'zip':\n        kwargs['owner'] = owner\n        kwargs['group'] = group\n\n    try:\n        filename = func(base_name, base_dir, **kwargs)\n    finally:\n        if root_dir is not None:\n            if logger is not None:\n                logger.debug(\"changing back to '%s'\", save_cwd)\n            os.chdir(save_cwd)\n\n    return filename", "response": "Create an archive file from a base name and a format."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_unpack_formats():\n    formats = [(name, info[0], info[3]) for name, info in\n               _UNPACK_FORMATS.items()]\n    formats.sort()\n    return formats", "response": "Returns a list of supported formats for unpacking."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _check_unpack_options(extensions, function, extra_args):\n    # first make sure no other unpacker is registered for this extension\n    existing_extensions = {}\n    for name, info in _UNPACK_FORMATS.items():\n        for ext in info[0]:\n            existing_extensions[ext] = name\n\n    for extension in extensions:\n        if extension in existing_extensions:\n            msg = '%s is already registered for \"%s\"'\n            raise RegistryError(msg % (extension,\n                                       existing_extensions[extension]))\n\n    if not isinstance(function, collections.Callable):\n        raise TypeError('The registered function must be a callable')", "response": "Checks what gets registered as an unpacker."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nregister an unpack format.", "response": "def register_unpack_format(name, extensions, function, extra_args=None,\n                           description=''):\n    \"\"\"Registers an unpack format.\n\n    `name` is the name of the format. `extensions` is a list of extensions\n    corresponding to the format.\n\n    `function` is the callable that will be\n    used to unpack archives. The callable will receive archives to unpack.\n    If it's unable to handle an archive, it needs to raise a ReadError\n    exception.\n\n    If provided, `extra_args` is a sequence of\n    (name, value) tuples that will be passed as arguments to the callable.\n    description can be provided to describe the format, and will be returned\n    by the get_unpack_formats() function.\n    \"\"\"\n    if extra_args is None:\n        extra_args = []\n    _check_unpack_options(extensions, function, extra_args)\n    _UNPACK_FORMATS[name] = extensions, function, extra_args, description"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _ensure_directory(path):\n    dirname = os.path.dirname(path)\n    if not os.path.isdir(dirname):\n        os.makedirs(dirname)", "response": "Ensure that the parent directory of path exists"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _unpack_zipfile(filename, extract_dir):\n    try:\n        import zipfile\n    except ImportError:\n        raise ReadError('zlib not supported, cannot unpack this archive.')\n\n    if not zipfile.is_zipfile(filename):\n        raise ReadError(\"%s is not a zip file\" % filename)\n\n    zip = zipfile.ZipFile(filename)\n    try:\n        for info in zip.infolist():\n            name = info.filename\n\n            # don't extract absolute paths or ones with .. in them\n            if name.startswith('/') or '..' in name:\n                continue\n\n            target = os.path.join(extract_dir, *name.split('/'))\n            if not target:\n                continue\n\n            _ensure_directory(target)\n            if not name.endswith('/'):\n                # file\n                data = zip.read(info.filename)\n                f = open(target, 'wb')\n                try:\n                    f.write(data)\n                finally:\n                    f.close()\n                    del data\n    finally:\n        zip.close()", "response": "Unpack a zip file to extract_dir"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nunpack tar file to extract_dir", "response": "def _unpack_tarfile(filename, extract_dir):\n    \"\"\"Unpack tar/tar.gz/tar.bz2 `filename` to `extract_dir`\n    \"\"\"\n    try:\n        tarobj = tarfile.open(filename)\n    except tarfile.TarError:\n        raise ReadError(\n            \"%s is not a compressed or uncompressed tar file\" % filename)\n    try:\n        tarobj.extractall(extract_dir)\n    finally:\n        tarobj.close()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef unpack_archive(filename, extract_dir=None, format=None):\n    if extract_dir is None:\n        extract_dir = os.getcwd()\n\n    if format is not None:\n        try:\n            format_info = _UNPACK_FORMATS[format]\n        except KeyError:\n            raise ValueError(\"Unknown unpack format '{0}'\".format(format))\n\n        func = format_info[1]\n        func(filename, extract_dir, **dict(format_info[2]))\n    else:\n        # we need to look at the registered unpackers supported extensions\n        format = _find_unpack_format(filename)\n        if format is None:\n            raise ReadError(\"Unknown archive format '{0}'\".format(filename))\n\n        func = _UNPACK_FORMATS[format][1]\n        kwargs = dict(_UNPACK_FORMATS[format][2])\n        func(filename, extract_dir, **kwargs)", "response": "Unpack an archive into a new directory and return the archive object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing an HTML fragment into a tree", "response": "def parseFragment(doc, container=\"div\", treebuilder=\"etree\", namespaceHTMLElements=True, **kwargs):\n    \"\"\"Parse an HTML fragment as a string or file-like object into a tree\n\n    :arg doc: the fragment to parse as a string or file-like object\n\n    :arg container: the container context to parse the fragment in\n\n    :arg treebuilder: the treebuilder to use when parsing\n\n    :arg namespaceHTMLElements: whether or not to namespace HTML elements\n\n    :returns: parsed tree\n\n    Example:\n\n    >>> from html5lib.html5libparser import parseFragment\n    >>> parseFragment('<b>this is a fragment</b>')\n    <Element u'DOCUMENT_FRAGMENT' at 0x7feac484b090>\n\n    \"\"\"\n    tb = treebuilders.getTreeBuilder(treebuilder)\n    p = HTMLParser(tb, namespaceHTMLElements=namespaceHTMLElements)\n    return p.parseFragment(doc, container=container, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse(self, stream, *args, **kwargs):\n        self._parse(stream, False, None, *args, **kwargs)\n        return self.tree.getDocument()", "response": "Parse a HTML document into a well - formed tree."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing a HTML fragment into a well - formed tree fragment", "response": "def parseFragment(self, stream, *args, **kwargs):\n        \"\"\"Parse a HTML fragment into a well-formed tree fragment\n\n        :arg container: name of the element we're setting the innerHTML\n            property if set to None, default to 'div'\n\n        :arg stream: a file-like object or string containing the HTML to be parsed\n\n            The optional encoding parameter must be a string that indicates\n            the encoding.  If specified, that encoding will be used,\n            regardless of any BOM or later declaration (such as in a meta\n            element)\n\n        :arg scripting: treat noscript elements as if JavaScript was turned on\n\n        :returns: parsed tree\n\n        Example:\n\n        >>> from html5lib.html5libparser import HTMLParser\n        >>> parser = HTMLParser()\n        >>> parser.parseFragment('<b>this is a fragment</b>')\n        <Element u'DOCUMENT_FRAGMENT' at 0x7feac484b090>\n\n        \"\"\"\n        self._parse(stream, True, *args, **kwargs)\n        return self.tree.getFragment()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef construct_tree(index):\n    return dict((p, [ReqPackage(r, index.get(r.key))\n                     for r in p.requires()])\n                for p in index.values())", "response": "Construct the tree representation of the pkgs from the index."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsorting the dict representation of the tree by package name.", "response": "def sorted_tree(tree):\n    \"\"\"Sorts the dict representation of the tree\n\n    The root packages as well as the intermediate packages are sorted\n    in the alphabetical order of the package names.\n\n    :param dict tree: the pkg dependency tree obtained by calling\n                     `construct_tree` function\n    :returns: sorted tree\n    :rtype: collections.OrderedDict\n\n    \"\"\"\n    return OrderedDict(sorted([(k, sorted(v, key=attrgetter('key')))\n                               for k, v in tree.items()],\n                              key=lambda kv: kv[0].key))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_tree_root(tree, key):\n    result = [p for p in tree.keys() if p.key == key]\n    assert len(result) in [0, 1]\n    return None if len(result) == 0 else result[0]", "response": "Find a root node in a tree by its key"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reverse_tree(tree):\n    rtree = defaultdict(list)\n    child_keys = set(c.key for c in flatten(tree.values()))\n    for k, vs in tree.items():\n        for v in vs:\n            node = find_tree_root(rtree, v.key) or v\n            rtree[node].append(k.as_required_by(v))\n        if k.key not in child_keys:\n            rtree[k.as_requirement()] = []\n    return rtree", "response": "Reverse the dependency tree."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef guess_version(pkg_key, default='?'):\n    try:\n        m = import_module(pkg_key)\n    except ImportError:\n        return default\n    else:\n        return getattr(m, '__version__', default)", "response": "Guess the version of a pkg when pip doesn t provide it\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a dict tree to string representation of the tree.", "response": "def render_tree(tree, list_all=True, show_only=None, frozen=False, exclude=None):\n    \"\"\"Convert tree to string representation\n\n    :param dict tree: the package tree\n    :param bool list_all: whether to list all the pgks at the root\n                          level or only those that are the\n                          sub-dependencies\n    :param set show_only: set of select packages to be shown in the\n                          output. This is optional arg, default: None.\n    :param bool frozen: whether or not show the names of the pkgs in\n                        the output that's favourable to pip --freeze\n    :param set exclude: set of select packages to be excluded from the\n                          output. This is optional arg, default: None.\n    :returns: string representation of the tree\n    :rtype: str\n\n    \"\"\"\n    tree = sorted_tree(tree)\n    branch_keys = set(r.key for r in flatten(tree.values()))\n    nodes = tree.keys()\n    use_bullets = not frozen\n\n    key_tree = dict((k.key, v) for k, v in tree.items())\n    get_children = lambda n: key_tree.get(n.key, [])\n\n    if show_only:\n        nodes = [p for p in nodes\n                 if p.key in show_only or p.project_name in show_only]\n    elif not list_all:\n        nodes = [p for p in nodes if p.key not in branch_keys]\n\n    def aux(node, parent=None, indent=0, chain=None):\n        if exclude and (node.key in exclude or node.project_name in exclude):\n            return []\n        if chain is None:\n            chain = [node.project_name]\n        node_str = node.render(parent, frozen)\n        if parent:\n            prefix = ' '*indent + ('- ' if use_bullets else '')\n            node_str = prefix + node_str\n        result = [node_str]\n        children = [aux(c, node, indent=indent+2,\n                        chain=chain+[c.project_name])\n                    for c in get_children(node)\n                    if c.project_name not in chain]\n        result += list(flatten(children))\n        return result\n\n    lines = flatten([aux(p) for p in nodes])\n    return '\\n'.join(lines)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef render_json(tree, indent):\n    return json.dumps([{'package': k.as_dict(),\n                        'dependencies': [v.as_dict() for v in vs]}\n                       for k, vs in tree.items()],\n                      indent=indent)", "response": "Converts the tree into a flat json representation."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef render_json_tree(tree, indent):\n    tree = sorted_tree(tree)\n    branch_keys = set(r.key for r in flatten(tree.values()))\n    nodes = [p for p in tree.keys() if p.key not in branch_keys]\n    key_tree = dict((k.key, v) for k, v in tree.items())\n    get_children = lambda n: key_tree.get(n.key, [])\n\n    def aux(node, parent=None, chain=None):\n        if chain is None:\n            chain = [node.project_name]\n\n        d = node.as_dict()\n        if parent:\n            d['required_version'] = node.version_spec if node.version_spec else 'Any'\n        else:\n            d['required_version'] = d['installed_version']\n\n        d['dependencies'] = [\n            aux(c, parent=node, chain=chain+[c.project_name])\n            for c in get_children(node)\n            if c.project_name not in chain\n        ]\n\n        return d\n\n    return json.dumps([aux(p) for p in nodes], indent=indent)", "response": "Converts the tree into a nested json representation."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndumps the dependency graph as a GraphViz graph.", "response": "def dump_graphviz(tree, output_format='dot'):\n    \"\"\"Output dependency graph as one of the supported GraphViz output formats.\n\n    :param dict tree: dependency graph\n    :param string output_format: output format\n    :returns: representation of tree in the specified output format\n    :rtype: str or binary representation depending on the output format\n\n    \"\"\"\n    try:\n        from graphviz import backend, Digraph\n    except ImportError:\n        print('graphviz is not available, but necessary for the output '\n              'option. Please install it.', file=sys.stderr)\n        sys.exit(1)\n\n    if output_format not in backend.FORMATS:\n        print('{0} is not a supported output format.'.format(output_format),\n              file=sys.stderr)\n        print('Supported formats are: {0}'.format(\n            ', '.join(sorted(backend.FORMATS))), file=sys.stderr)\n        sys.exit(1)\n\n    graph = Digraph(format=output_format)\n    for package, deps in tree.items():\n        project_name = package.project_name\n        label = '{0}\\n{1}'.format(project_name, package.version)\n        graph.node(project_name, label=label)\n        for dep in deps:\n            label = dep.version_spec\n            if not label:\n                label = 'any'\n            graph.edge(project_name, dep.project_name, label=label)\n\n    # Allow output of dot format, even if GraphViz isn't installed.\n    if output_format == 'dot':\n        return graph.source\n\n    # As it's unknown if the selected output format is binary or not, try to\n    # decode it as UTF8 and only print it out in binary if that's not possible.\n    try:\n        return graph.pipe().decode('utf-8')\n    except UnicodeDecodeError:\n        return graph.pipe()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndumps the data generated by GraphViz to stdout.", "response": "def print_graphviz(dump_output):\n    \"\"\"Dump the data generated by GraphViz to stdout.\n\n    :param dump_output: The output from dump_graphviz\n    \"\"\"\n    if hasattr(dump_output, 'encode'):\n        print(dump_output)\n    else:\n        with os.fdopen(sys.stdout.fileno(), 'wb') as bytestream:\n            bytestream.write(dump_output)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef conflicting_deps(tree):\n    conflicting = defaultdict(list)\n    for p, rs in tree.items():\n        for req in rs:\n            if req.is_conflicting():\n                conflicting[p].append(req)\n    return conflicting", "response": "Returns dependencies which are not present or conflict with the\n    requirements of other packages."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns list of tuples representing cyclic dependencies of pkg_resources. Distribution instances.", "response": "def cyclic_deps(tree):\n    \"\"\"Return cyclic dependencies as list of tuples\n\n    :param list pkgs: pkg_resources.Distribution instances\n    :param dict pkg_index: mapping of pkgs with their respective keys\n    :returns: list of tuples representing cyclic dependencies\n    :rtype: generator\n\n    \"\"\"\n    key_tree = dict((k.key, v) for k, v in tree.items())\n    get_children = lambda n: key_tree.get(n.key, [])\n    cyclic = []\n    for p, rs in tree.items():\n        for req in rs:\n            if p.key in map(attrgetter('key'), get_children(req)):\n                cyclic.append((p, req, p))\n    return cyclic"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning True if installed version conflicts with required version", "response": "def is_conflicting(self):\n        \"\"\"If installed version conflicts with required version\"\"\"\n        # unknown installed version is also considered conflicting\n        if self.installed_version == self.UNKNOWN_VERSION:\n            return True\n        ver_spec = (self.version_spec if self.version_spec else '')\n        req_version_str = '{0}{1}'.format(self.project_name, ver_spec)\n        req_obj = pkg_resources.Requirement.parse(req_version_str)\n        return self.installed_version not in req_obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_against_chunks(self, chunks):\n        # type: (Iterator[bytes]) -> None\n        \"\"\"Check good hashes against ones built from iterable of chunks of\n        data.\n\n        Raise HashMismatch if none match.\n\n        \"\"\"\n        gots = {}\n        for hash_name in iterkeys(self._allowed):\n            try:\n                gots[hash_name] = hashlib.new(hash_name)\n            except (ValueError, TypeError):\n                raise InstallationError('Unknown hash name: %s' % hash_name)\n\n        for chunk in chunks:\n            for hash in itervalues(gots):\n                hash.update(chunk)\n\n        for hash_name, got in iteritems(gots):\n            if got.hexdigest() in self._allowed[hash_name]:\n                return\n        self._raise(gots)", "response": "Check good hashes against ones built from iterable of chunks of\n        data. Raise HashMismatch if none match."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves nodes from an errorpath relatively to the basepaths of self.", "response": "def _drop_nodes_from_errorpaths(self, _errors, dp_items, sp_items):\n        \"\"\" Removes nodes by index from an errorpath, relatively to the\n            basepaths of self.\n\n        :param errors: A list of :class:`errors.ValidationError` instances.\n        :param dp_items: A list of integers, pointing at the nodes to drop from\n                         the :attr:`document_path`.\n        :param sp_items: Alike ``dp_items``, but for :attr:`schema_path`.\n        \"\"\"\n        dp_basedepth = len(self.document_path)\n        sp_basedepth = len(self.schema_path)\n        for error in _errors:\n            for i in sorted(dp_items, reverse=True):\n                error.document_path = \\\n                    drop_item_from_tuple(error.document_path, dp_basedepth + i)\n            for i in sorted(sp_items, reverse=True):\n                error.schema_path = \\\n                    drop_item_from_tuple(error.schema_path, sp_basedepth + i)\n            if error.child_errors:\n                self._drop_nodes_from_errorpaths(error.child_errors,\n                                                 dp_items, sp_items)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsearches for a field in the current document and returns its name and value.", "response": "def _lookup_field(self, path):\n        \"\"\" Searches for a field as defined by path. This method is used by the\n            ``dependency`` evaluation logic.\n\n        :param path: Path elements are separated by a ``.``. A leading ``^``\n                     indicates that the path relates to the document root,\n                     otherwise it relates to the currently evaluated document,\n                     which is possibly a subdocument.\n                     The sequence ``^^`` at the start will be interpreted as a\n                     literal ``^``.\n        :type path: :class:`str`\n        :returns: Either the found field name and its value or :obj:`None` for\n                  both.\n        :rtype: A two-value :class:`tuple`.\n        \"\"\"\n        if path.startswith('^'):\n            path = path[1:]\n            context = self.document if path.startswith('^') \\\n                else self.root_document\n        else:\n            context = self.document\n\n        parts = path.split('.')\n        for part in parts:\n            if part not in context:\n                return None, None\n            context = context.get(part)\n\n        return parts[-1], context"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef types(cls):\n        redundant_types = \\\n            set(cls.types_mapping) & set(cls._types_from_methods)\n        if redundant_types:\n            warn(\"These types are defined both with a method and in the\"\n                 \"'types_mapping' property of this validator: %s\"\n                 % redundant_types)\n\n        return tuple(cls.types_mapping) + cls._types_from_methods", "response": "A tuple of strings that can be used for the type rule."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _drop_remaining_rules(self, *rules):\n        if rules:\n            for rule in rules:\n                try:\n                    self._remaining_rules.remove(rule)\n                except ValueError:\n                    pass\n        else:\n            self._remaining_rules = []", "response": "Drops the remaining rules from the queue of the currently processed field."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the document normalized according to the specified rules AttributeNames schema.", "response": "def normalized(self, document, schema=None, always_return_document=False):\n        \"\"\" Returns the document normalized according to the specified rules\n        of a schema.\n\n        :param document: The document to normalize.\n        :type document: any :term:`mapping`\n        :param schema: The validation schema. Defaults to :obj:`None`. If not\n                       provided here, the schema must have been provided at\n                       class instantiation.\n        :type schema: any :term:`mapping`\n        :param always_return_document: Return the document, even if an error\n                                       occurred. Defaults to: ``False``.\n        :type always_return_document: :class:`bool`\n        :return: A normalized copy of the provided mapping or :obj:`None` if an\n                 error occurred during normalization.\n        \"\"\"\n        self.__init_processing(document, schema)\n        self.__normalize_mapping(self.document, self.schema)\n        self.error_handler.end(self)\n        if self._errors and not always_return_document:\n            return None\n        else:\n            return self.document"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _normalize_coerce(self, mapping, schema):\n\n        error = errors.COERCION_FAILED\n        for field in mapping:\n            if field in schema and 'coerce' in schema[field]:\n                mapping[field] = self.__normalize_coerce(\n                    schema[field]['coerce'], field, mapping[field],\n                    schema[field].get('nullable', False), error)\n            elif isinstance(self.allow_unknown, Mapping) and \\\n                    'coerce' in self.allow_unknown:\n                mapping[field] = self.__normalize_coerce(\n                    self.allow_unknown['coerce'], field, mapping[field],\n                    self.allow_unknown.get('nullable', False), error)", "response": "Normalizes the coerce field in the mapping."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _normalize_purge_unknown(mapping, schema):\n        for field in tuple(mapping):\n            if field not in schema:\n                del mapping[field]\n        return mapping", "response": "Remove unknown fields from a mapping."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nnormalizes rename_handler field in the schema.", "response": "def _normalize_rename_handler(self, mapping, schema, field):\n        \"\"\" {'oneof': [\n                {'type': 'callable'},\n                {'type': 'list',\n                 'schema': {'oneof': [{'type': 'callable'},\n                                      {'type': 'string'}]}},\n                {'type': 'string'}\n                ]} \"\"\"\n        if 'rename_handler' not in schema[field]:\n            return\n        new_name = self.__normalize_coerce(\n            schema[field]['rename_handler'], field, field,\n            False, errors.RENAMING_FAILED)\n        if new_name != field:\n            mapping[new_name] = mapping[field]\n            del mapping[field]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nnormalizes the default_setter of the resource.", "response": "def _normalize_default_setter(self, mapping, schema, field):\n        \"\"\" {'oneof': [\n                {'type': 'callable'},\n                {'type': 'string'}\n                ]} \"\"\"\n        if 'default_setter' in schema[field]:\n            setter = schema[field]['default_setter']\n            if isinstance(setter, _str_type):\n                setter = self.__get_rule_handler('normalize_default_setter',\n                                                 setter)\n            mapping[field] = setter(mapping)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef validate(self, document, schema=None, update=False, normalize=True):\n        self.update = update\n        self._unrequired_by_excludes = set()\n\n        self.__init_processing(document, schema)\n        if normalize:\n            self.__normalize_mapping(self.document, self.schema)\n\n        for field in self.document:\n            if self.ignore_none_values and self.document[field] is None:\n                continue\n            definitions = self.schema.get(field)\n            if definitions is not None:\n                self.__validate_definitions(definitions, field)\n            else:\n                self.__validate_unknown_fields(field)\n\n        if not self.update:\n            self.__validate_required_fields(self.document)\n\n        self.error_handler.end(self)\n\n        return not bool(self._errors)", "response": "Validates a document against a validation - schema."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef validated(self, *args, **kwargs):\n        always_return_document = kwargs.pop('always_return_document', False)\n        self.validate(*args, **kwargs)\n        if self._errors and not always_return_document:\n            return None\n        else:\n            return self.document", "response": "Wrapper around ~cerberus. Validator. validate that returns\n            the normalized and validated document or None if validation fails."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nvalidate that the value is in the allowed_values list.", "response": "def _validate_allowed(self, allowed_values, field, value):\n        \"\"\" {'type': 'list'} \"\"\"\n        if isinstance(value, Iterable) and not isinstance(value, _str_type):\n            unallowed = set(value) - set(allowed_values)\n            if unallowed:\n                self._error(field, errors.UNALLOWED_VALUES, list(unallowed))\n        else:\n            if value not in allowed_values:\n                self._error(field, errors.UNALLOWED_VALUE, value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nvalidates that the value is empty.", "response": "def _validate_empty(self, empty, field, value):\n        \"\"\" {'type': 'boolean'} \"\"\"\n        if isinstance(value, Iterable) and len(value) == 0:\n            self._drop_remaining_rules(\n                'allowed', 'forbidden', 'items', 'minlength', 'maxlength',\n                'regex', 'validator')\n            if not empty:\n                self._error(field, errors.EMPTY_NOT_ALLOWED)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _validate_excludes(self, excludes, field, value):\n        if isinstance(excludes, Hashable):\n            excludes = [excludes]\n\n        # Save required field to be checked latter\n        if 'required' in self.schema[field] and self.schema[field]['required']:\n            self._unrequired_by_excludes.add(field)\n        for exclude in excludes:\n            if (exclude in self.schema and\n                'required' in self.schema[exclude] and\n                    self.schema[exclude]['required']):\n\n                self._unrequired_by_excludes.add(exclude)\n\n        if [True for key in excludes if key in self.document]:\n            # Wrap each field in `excludes` list between quotes\n            exclusion_str = ', '.join(\"'{0}'\"\n                                      .format(word) for word in excludes)\n            self._error(field, errors.EXCLUDES_FIELD, exclusion_str)", "response": "{'type': ('hashable', 'list'),\n             'schema': {'type': 'hashable'}}"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nvalidate that the value is in forbidden_values.", "response": "def _validate_forbidden(self, forbidden_values, field, value):\n        \"\"\" {'type': 'list'} \"\"\"\n        if isinstance(value, _str_type):\n            if value in forbidden_values:\n                self._error(field, errors.FORBIDDEN_VALUE, value)\n        elif isinstance(value, Sequence):\n            forbidden = set(value) & set(forbidden_values)\n            if forbidden:\n                self._error(field, errors.FORBIDDEN_VALUES, list(forbidden))\n        elif isinstance(value, int):\n            if value in forbidden_values:\n                self._error(field, errors.FORBIDDEN_VALUE, value)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nvalidate the value against all the logical rules of the resource.", "response": "def __validate_logical(self, operator, definitions, field, value):\n        \"\"\" Validates value against all definitions and logs errors according\n            to the operator. \"\"\"\n        valid_counter = 0\n        _errors = errors.ErrorList()\n\n        for i, definition in enumerate(definitions):\n            schema = {field: definition.copy()}\n            for rule in ('allow_unknown', 'type'):\n                if rule not in schema[field] and rule in self.schema[field]:\n                    schema[field][rule] = self.schema[field][rule]\n            if 'allow_unknown' not in schema[field]:\n                schema[field]['allow_unknown'] = self.allow_unknown\n\n            validator = self._get_child_validator(\n                schema_crumb=(field, operator, i),\n                schema=schema, allow_unknown=True)\n            if validator(self.document, update=self.update, normalize=False):\n                valid_counter += 1\n            else:\n                self._drop_nodes_from_errorpaths(validator._errors, [], [3])\n                _errors.extend(validator._errors)\n\n        return valid_counter, _errors"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nvalidates that the value is an any of the class entries in the list.", "response": "def _validate_anyof(self, definitions, field, value):\n        \"\"\" {'type': 'list', 'logical': 'anyof'} \"\"\"\n        valids, _errors = \\\n            self.__validate_logical('anyof', definitions, field, value)\n        if valids < 1:\n            self._error(field, errors.ANYOF, _errors,\n                        valids, len(definitions))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _validate_allof(self, definitions, field, value):\n        valids, _errors = \\\n            self.__validate_logical('allof', definitions, field, value)\n        if valids < len(definitions):\n            self._error(field, errors.ALLOF, _errors,\n                        valids, len(definitions))", "response": "Validate the allof field."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nvalidating that the value is a none of the class.", "response": "def _validate_noneof(self, definitions, field, value):\n        \"\"\" {'type': 'list', 'logical': 'noneof'} \"\"\"\n        valids, _errors = \\\n            self.__validate_logical('noneof', definitions, field, value)\n        if valids > 0:\n            self._error(field, errors.NONEOF, _errors,\n                        valids, len(definitions))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _validate_oneof(self, definitions, field, value):\n        valids, _errors = \\\n            self.__validate_logical('oneof', definitions, field, value)\n        if valids != 1:\n            self._error(field, errors.ONEOF, _errors,\n                        valids, len(definitions))", "response": "Validate that the value is a one of the class."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nvalidate that the value is greater than max_value.", "response": "def _validate_max(self, max_value, field, value):\n        \"\"\" {'nullable': False } \"\"\"\n        try:\n            if value > max_value:\n                self._error(field, errors.MAX_VALUE)\n        except TypeError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _validate_min(self, min_value, field, value):\n        try:\n            if value < min_value:\n                self._error(field, errors.MIN_VALUE)\n        except TypeError:\n            pass", "response": "Validate that the value of a user - defined attribute is less than the given value."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _validate_maxlength(self, max_length, field, value):\n        if isinstance(value, Iterable) and len(value) > max_length:\n            self._error(field, errors.MAX_LENGTH, len(value))", "response": "Validate that the length of value is greater than max_length."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _validate_minlength(self, min_length, field, value):\n        if isinstance(value, Iterable) and len(value) < min_length:\n            self._error(field, errors.MIN_LENGTH, len(value))", "response": "Validate that the length of value is less than min_length."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nvalidate that the keyschema of the resource is valid.", "response": "def _validate_keyschema(self, schema, field, value):\n        \"\"\" {'type': ['dict', 'string'], 'validator': 'bulk_schema',\n            'forbidden': ['rename', 'rename_handler']} \"\"\"\n        if isinstance(value, Mapping):\n            validator = self._get_child_validator(\n                document_crumb=field,\n                schema_crumb=(field, 'keyschema'),\n                schema=dict(((k, schema) for k in value.keys())))\n            if not validator(dict(((k, k) for k in value.keys())),\n                             normalize=False):\n                self._drop_nodes_from_errorpaths(validator._errors,\n                                                 [], [2, 4])\n                self._error(field, errors.KEYSCHEMA, validator._errors)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nvalidate that the field is not readonly.", "response": "def _validate_readonly(self, readonly, field, value):\n        \"\"\" {'type': 'boolean'} \"\"\"\n        if readonly:\n            if not self._is_normalized:\n                self._error(field, errors.READONLY_FIELD)\n            # If the document was normalized (and therefore already been\n            # checked for readonly fields), we still have to return True\n            # if an error was filed.\n            has_error = errors.READONLY_FIELD in \\\n                self.document_error_tree.fetch_errors_from(\n                    self.document_path + (field,))\n            if self._is_normalized and has_error:\n                self._drop_remaining_rules()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _validate_regex(self, pattern, field, value):\n        if not isinstance(value, _str_type):\n            return\n        if not pattern.endswith('$'):\n            pattern += '$'\n        re_obj = re.compile(pattern)\n        if not re_obj.match(value):\n            self._error(field, errors.REGEX_MISMATCH)", "response": "Validate that the value is a valid regular expression."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __validate_required_fields(self, document):\n        try:\n            required = set(field for field, definition in self.schema.items()\n                           if self._resolve_rules_set(definition).\n                           get('required') is True)\n        except AttributeError:\n            if self.is_child and self.schema_path[-1] == 'schema':\n                raise _SchemaRuleTypeError\n            else:\n                raise\n        required -= self._unrequired_by_excludes\n        missing = required - set(field for field in document\n                                 if document.get(field) is not None or\n                                 not self.ignore_none_values)\n\n        for field in missing:\n            self._error(field, errors.REQUIRED_FIELD)\n\n        # At least on field from self._unrequired_by_excludes should be\n        # present in document\n        if self._unrequired_by_excludes:\n            fields = set(field for field in document\n                         if document.get(field) is not None)\n            if self._unrequired_by_excludes.isdisjoint(fields):\n                for field in self._unrequired_by_excludes - fields:\n                    self._error(field, errors.REQUIRED_FIELD)", "response": "Validates that required fields are present in the document."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nvalidate that the value is a valid resource ID.", "response": "def _validate_schema(self, schema, field, value):\n        \"\"\" {'type': ['dict', 'string'],\n             'anyof': [{'validator': 'schema'},\n                       {'validator': 'bulk_schema'}]} \"\"\"\n        if schema is None:\n            return\n\n        if isinstance(value, Sequence) and not isinstance(value, _str_type):\n            self.__validate_schema_sequence(field, schema, value)\n        elif isinstance(value, Mapping):\n            self.__validate_schema_mapping(field, schema, value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nvalidate the type of the object.", "response": "def _validate_type(self, data_type, field, value):\n        \"\"\" {'type': ['string', 'list'],\n             'validator': 'type'} \"\"\"\n        if not data_type:\n            return\n\n        types = (data_type,) if isinstance(data_type, _str_type) else data_type\n\n        for _type in types:\n            # TODO remove this block on next major release\n            # this implementation still supports custom type validation methods\n            type_definition = self.types_mapping.get(_type)\n            if type_definition is not None:\n                matched = isinstance(value, type_definition.included_types) \\\n                    and not isinstance(value, type_definition.excluded_types)\n            else:\n                type_handler = self.__get_rule_handler('validate_type', _type)\n                matched = type_handler(value)\n            if matched:\n                return\n\n            # TODO uncomment this block on next major release\n            #      when _validate_type_* methods were deprecated:\n            # type_definition = self.types_mapping[_type]\n            # if isinstance(value, type_definition.included_types) \\\n            #         and not isinstance(value, type_definition.excluded_types):  # noqa 501\n            #     return\n\n        self._error(field, errors.BAD_TYPE)\n        self._drop_remaining_rules()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nvalidates the value of a resource field.", "response": "def _validate_validator(self, validator, field, value):\n        \"\"\" {'oneof': [\n                {'type': 'callable'},\n                {'type': 'list',\n                 'schema': {'oneof': [{'type': 'callable'},\n                                      {'type': 'string'}]}},\n                {'type': 'string'}\n                ]} \"\"\"\n        if isinstance(validator, _str_type):\n            validator = self.__get_rule_handler('validator', validator)\n            validator(field, value)\n        elif isinstance(validator, Iterable):\n            for v in validator:\n                self._validate_validator(v, field, value)\n        else:\n            validator(field, value, self._error)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _validate_valueschema(self, schema, field, value):\n        schema_crumb = (field, 'valueschema')\n        if isinstance(value, Mapping):\n            validator = self._get_child_validator(\n                document_crumb=field, schema_crumb=schema_crumb,\n                schema=dict((k, schema) for k in value))\n            validator(value, update=self.update, normalize=False)\n            if validator._errors:\n                self._drop_nodes_from_errorpaths(validator._errors, [], [2])\n                self._error(field, errors.VALUESCHEMA, validator._errors)", "response": "Validate that the value is a valid value for the given schema."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a callable that looks up the given attribute from a passed object with the rules of the environment.", "response": "def make_attrgetter(environment, attribute, postprocess=None):\n    \"\"\"Returns a callable that looks up the given attribute from a\n    passed object with the rules of the environment.  Dots are allowed\n    to access attributes of attributes.  Integer parts in paths are\n    looked up as integers.\n    \"\"\"\n    if attribute is None:\n        attribute = []\n    elif isinstance(attribute, string_types):\n        attribute = [int(x) if x.isdigit() else x for x in attribute.split('.')]\n    else:\n        attribute = [attribute]\n\n    def attrgetter(item):\n        for part in attribute:\n            item = environment.getitem(item, part)\n\n        if postprocess is not None:\n            item = postprocess(item)\n\n        return item\n\n    return attrgetter"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nenforce HTML escaping. This will probably double escape variables.", "response": "def do_forceescape(value):\n    \"\"\"Enforce HTML escaping.  This will probably double escape variables.\"\"\"\n    if hasattr(value, '__html__'):\n        value = value.__html__()\n    return escape(text_type(value))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef do_urlencode(value):\n    itemiter = None\n    if isinstance(value, dict):\n        itemiter = iteritems(value)\n    elif not isinstance(value, string_types):\n        try:\n            itemiter = iter(value)\n        except TypeError:\n            pass\n    if itemiter is None:\n        return unicode_urlencode(value)\n    return u'&'.join(unicode_urlencode(k) + '=' +\n                     unicode_urlencode(v, for_qs=True)\n                     for k, v in itemiter)", "response": "Escape strings for use in URLs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a titlecased version of the value. I. e. words will start with uppercase letters all remaining characters are lowercase.", "response": "def do_title(s):\n    \"\"\"Return a titlecased version of the value. I.e. words will start with\n    uppercase letters, all remaining characters are lowercase.\n    \"\"\"\n    return ''.join(\n        [item[0].upper() + item[1:].lower()\n         for item in _word_beginning_split_re.split(soft_unicode(s))\n         if item])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsorts a dict and yield key value pairs.", "response": "def do_dictsort(value, case_sensitive=False, by='key', reverse=False):\n    \"\"\"Sort a dict and yield (key, value) pairs. Because python dicts are\n    unsorted you may want to use this function to order them by either\n    key or value:\n\n    .. sourcecode:: jinja\n\n        {% for item in mydict|dictsort %}\n            sort the dict by key, case insensitive\n\n        {% for item in mydict|dictsort(reverse=true) %}\n            sort the dict by key, case insensitive, reverse order\n\n        {% for item in mydict|dictsort(true) %}\n            sort the dict by key, case sensitive\n\n        {% for item in mydict|dictsort(false, 'value') %}\n            sort the dict by value, case insensitive\n    \"\"\"\n    if by == 'key':\n        pos = 0\n    elif by == 'value':\n        pos = 1\n    else:\n        raise FilterArgumentError(\n            'You can only sort by either \"key\" or \"value\"'\n        )\n\n    def sort_func(item):\n        value = item[pos]\n\n        if not case_sensitive:\n            value = ignore_case(value)\n\n        return value\n\n    return sorted(value.items(), key=sort_func, reverse=reverse)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsorting an iterable. Per default it sorts ascending, if you pass it true as first argument it will reverse the sorting. If the iterable is made of strings the third parameter can be used to control the case sensitiveness of the comparison which is disabled by default. .. sourcecode:: jinja {% for item in iterable|sort %} ... {% endfor %} It is also possible to sort by an attribute (for example to sort by the date of an object) by specifying the `attribute` parameter: .. sourcecode:: jinja {% for item in iterable|sort(attribute='date') %} ... {% endfor %} .. versionchanged:: 2.6 The `attribute` parameter was added.", "response": "def do_sort(\n    environment, value, reverse=False, case_sensitive=False, attribute=None\n):\n    \"\"\"Sort an iterable.  Per default it sorts ascending, if you pass it\n    true as first argument it will reverse the sorting.\n\n    If the iterable is made of strings the third parameter can be used to\n    control the case sensitiveness of the comparison which is disabled by\n    default.\n\n    .. sourcecode:: jinja\n\n        {% for item in iterable|sort %}\n            ...\n        {% endfor %}\n\n    It is also possible to sort by an attribute (for example to sort\n    by the date of an object) by specifying the `attribute` parameter:\n\n    .. sourcecode:: jinja\n\n        {% for item in iterable|sort(attribute='date') %}\n            ...\n        {% endfor %}\n\n    .. versionchanged:: 2.6\n       The `attribute` parameter was added.\n    \"\"\"\n    key_func = make_attrgetter(\n        environment, attribute,\n        postprocess=ignore_case if not case_sensitive else None\n    )\n    return sorted(value, key=key_func, reverse=reverse)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef do_unique(environment, value, case_sensitive=False, attribute=None):\n    getter = make_attrgetter(\n        environment, attribute,\n        postprocess=ignore_case if not case_sensitive else None\n    )\n    seen = set()\n\n    for item in value:\n        key = getter(item)\n\n        if key not in seen:\n            seen.add(key)\n            yield item", "response": "Yields unique items from the given iterable."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef do_min(environment, value, case_sensitive=False, attribute=None):\n    return _min_or_max(environment, value, min, case_sensitive, attribute)", "response": "Return the smallest item from the sequence."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef do_max(environment, value, case_sensitive=False, attribute=None):\n    return _min_or_max(environment, value, max, case_sensitive, attribute)", "response": "Return the largest item from the sequence."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a string which is the concatenation of the strings in the sequence.", "response": "def do_join(eval_ctx, value, d=u'', attribute=None):\n    \"\"\"Return a string which is the concatenation of the strings in the\n    sequence. The separator between elements is an empty string per\n    default, you can define it with the optional parameter:\n\n    .. sourcecode:: jinja\n\n        {{ [1, 2, 3]|join('|') }}\n            -> 1|2|3\n\n        {{ [1, 2, 3]|join }}\n            -> 123\n\n    It is also possible to join certain attributes of an object:\n\n    .. sourcecode:: jinja\n\n        {{ users|join(', ', attribute='username') }}\n\n    .. versionadded:: 2.6\n       The `attribute` parameter was added.\n    \"\"\"\n    if attribute is not None:\n        value = imap(make_attrgetter(eval_ctx.environment, attribute), value)\n\n    # no automatic escaping?  joining is a lot eaiser then\n    if not eval_ctx.autoescape:\n        return text_type(d).join(imap(text_type, value))\n\n    # if the delimiter doesn't have an html representation we check\n    # if any of the items has.  If yes we do a coercion to Markup\n    if not hasattr(d, '__html__'):\n        value = list(value)\n        do_escape = False\n        for idx, item in enumerate(value):\n            if hasattr(item, '__html__'):\n                do_escape = True\n            else:\n                value[idx] = text_type(item)\n        if do_escape:\n            d = escape(d)\n        else:\n            d = text_type(d)\n        return d.join(value)\n\n    # no html involved, to normal joining\n    return soft_unicode(d).join(imap(soft_unicode, value))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the last item of a sequence.", "response": "def do_last(environment, seq):\n    \"\"\"Return the last item of a sequence.\"\"\"\n    try:\n        return next(iter(reversed(seq)))\n    except StopIteration:\n        return environment.undefined('No last item, sequence was empty.')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a random item from the sequence.", "response": "def do_random(context, seq):\n    \"\"\"Return a random item from the sequence.\"\"\"\n    try:\n        return random.choice(seq)\n    except IndexError:\n        return context.environment.undefined('No random item, sequence was empty.')"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nformats the value like a human - readable file size.", "response": "def do_filesizeformat(value, binary=False):\n    \"\"\"Format the value like a 'human-readable' file size (i.e. 13 kB,\n    4.1 MB, 102 Bytes, etc).  Per default decimal prefixes are used (Mega,\n    Giga, etc.), if the second parameter is set to `True` the binary\n    prefixes are used (Mebi, Gibi).\n    \"\"\"\n    bytes = float(value)\n    base = binary and 1024 or 1000\n    prefixes = [\n        (binary and 'KiB' or 'kB'),\n        (binary and 'MiB' or 'MB'),\n        (binary and 'GiB' or 'GB'),\n        (binary and 'TiB' or 'TB'),\n        (binary and 'PiB' or 'PB'),\n        (binary and 'EiB' or 'EB'),\n        (binary and 'ZiB' or 'ZB'),\n        (binary and 'YiB' or 'YB')\n    ]\n    if bytes == 1:\n        return '1 Byte'\n    elif bytes < base:\n        return '%d Bytes' % bytes\n    else:\n        for i, prefix in enumerate(prefixes):\n            unit = base ** (i + 2)\n            if bytes < unit:\n                return '%.1f %s' % ((base * bytes / unit), prefix)\n        return '%.1f %s' % ((base * bytes / unit), prefix)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a URL into clickable links.", "response": "def do_urlize(eval_ctx, value, trim_url_limit=None, nofollow=False,\n              target=None, rel=None):\n    \"\"\"Converts URLs in plain text into clickable links.\n\n    If you pass the filter an additional integer it will shorten the urls\n    to that number. Also a third argument exists that makes the urls\n    \"nofollow\":\n\n    .. sourcecode:: jinja\n\n        {{ mytext|urlize(40, true) }}\n            links are shortened to 40 chars and defined with rel=\"nofollow\"\n\n    If *target* is specified, the ``target`` attribute will be added to the\n    ``<a>`` tag:\n\n    .. sourcecode:: jinja\n\n       {{ mytext|urlize(40, target='_blank') }}\n\n    .. versionchanged:: 2.8+\n       The *target* parameter was added.\n    \"\"\"\n    policies = eval_ctx.environment.policies\n    rel = set((rel or '').split() or [])\n    if nofollow:\n        rel.add('nofollow')\n    rel.update((policies['urlize.rel'] or '').split())\n    if target is None:\n        target = policies['urlize.target']\n    rel = ' '.join(sorted(rel)) or None\n    rv = urlize(value, trim_url_limit, rel=rel, target=target)\n    if eval_ctx.autoescape:\n        rv = Markup(rv)\n    return rv"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef do_indent(\n    s, width=4, first=False, blank=False, indentfirst=None\n):\n    \"\"\"Return a copy of the string with each line indented by 4 spaces. The\n    first line and blank lines are not indented by default.\n\n    :param width: Number of spaces to indent by.\n    :param first: Don't skip indenting the first line.\n    :param blank: Don't skip indenting empty lines.\n\n    .. versionchanged:: 2.10\n        Blank lines are not indented by default.\n\n        Rename the ``indentfirst`` argument to ``first``.\n    \"\"\"\n    if indentfirst is not None:\n        warnings.warn(DeprecationWarning(\n            'The \"indentfirst\" argument is renamed to \"first\".'\n        ), stacklevel=2)\n        first = indentfirst\n\n    s += u'\\n'  # this quirk is necessary for splitlines method\n    indention = u' ' * width\n\n    if blank:\n        rv = (u'\\n' + indention).join(s.splitlines())\n    else:\n        lines = s.splitlines()\n        rv = lines.pop(0)\n\n        if lines:\n            rv += u'\\n' + u'\\n'.join(\n                indention + line if line else line for line in lines\n            )\n\n    if first:\n        rv = indention + rv\n\n    return rv", "response": "Return a copy of the string with each line indented by 4 spaces."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef do_truncate(env, s, length=255, killwords=False, end='...', leeway=None):\n    if leeway is None:\n        leeway = env.policies['truncate.leeway']\n    assert length >= len(end), 'expected length >= %s, got %s' % (len(end), length)\n    assert leeway >= 0, 'expected leeway >= 0, got %s' % leeway\n    if len(s) <= length + leeway:\n        return s\n    if killwords:\n        return s[:length - len(end)] + end\n    result = s[:length - len(end)].rsplit(' ', 1)[0]\n    return result + end", "response": "Return a truncated copy of the string s."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef do_wordwrap(environment, s, width=79, break_long_words=True,\n                wrapstring=None):\n    \"\"\"\n    Return a copy of the string passed to the filter wrapped after\n    ``79`` characters.  You can override this default using the first\n    parameter.  If you set the second parameter to `false` Jinja will not\n    split words apart if they are longer than `width`. By default, the newlines\n    will be the default newlines for the environment, but this can be changed\n    using the wrapstring keyword argument.\n\n    .. versionadded:: 2.7\n       Added support for the `wrapstring` parameter.\n    \"\"\"\n    if not wrapstring:\n        wrapstring = environment.newline_sequence\n    import textwrap\n    return wrapstring.join(textwrap.wrap(s, width=width, expand_tabs=False,\n                                   replace_whitespace=False,\n                                   break_long_words=break_long_words))", "response": "Returns a copy of the string passed to the filter wrapped after the specified width characters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef do_int(value, default=0, base=10):\n    try:\n        if isinstance(value, string_types):\n            return int(value, base)\n        return int(value)\n    except (TypeError, ValueError):\n        # this quirk is necessary so that \"42.23\"|int gives 42.\n        try:\n            return int(float(value))\n        except (TypeError, ValueError):\n            return default", "response": "Convert the value into an integer."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef do_format(value, *args, **kwargs):\n    if args and kwargs:\n        raise FilterArgumentError('can\\'t handle positional and keyword '\n                                  'arguments at the same time')\n    return soft_unicode(value) % (kwargs or args)", "response": "Apply python string formatting on an object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstripping SGML and XML tags and replace adjacent whitespace by one space.", "response": "def do_striptags(value):\n    \"\"\"Strip SGML/XML tags and replace adjacent whitespace by one space.\n    \"\"\"\n    if hasattr(value, '__html__'):\n        value = value.__html__()\n    return Markup(text_type(value)).striptags()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nslicing an iterator and return a list of lists containing those items.", "response": "def do_slice(value, slices, fill_with=None):\n    \"\"\"Slice an iterator and return a list of lists containing\n    those items. Useful if you want to create a div containing\n    three ul tags that represent columns:\n\n    .. sourcecode:: html+jinja\n\n        <div class=\"columwrapper\">\n          {%- for column in items|slice(3) %}\n            <ul class=\"column-{{ loop.index }}\">\n            {%- for item in column %}\n              <li>{{ item }}</li>\n            {%- endfor %}\n            </ul>\n          {%- endfor %}\n        </div>\n\n    If you pass it a second argument it's used to fill missing\n    values on the last iteration.\n    \"\"\"\n    seq = list(value)\n    length = len(seq)\n    items_per_slice = length // slices\n    slices_with_extra = length % slices\n    offset = 0\n    for slice_number in range(slices):\n        start = offset + slice_number * items_per_slice\n        if slice_number < slices_with_extra:\n            offset += 1\n        end = offset + (slice_number + 1) * items_per_slice\n        tmp = seq[start:end]\n        if fill_with is not None and slice_number >= slices_with_extra:\n            tmp.append(fill_with)\n        yield tmp"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef do_batch(value, linecount, fill_with=None):\n    tmp = []\n    for item in value:\n        if len(tmp) == linecount:\n            yield tmp\n            tmp = []\n        tmp.append(item)\n    if tmp:\n        if fill_with is not None and len(tmp) < linecount:\n            tmp += [fill_with] * (linecount - len(tmp))\n        yield tmp", "response": "A filter that batches items into a list of lists with the given number of items."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef do_round(value, precision=0, method='common'):\n    if not method in ('common', 'ceil', 'floor'):\n        raise FilterArgumentError('method must be common, ceil or floor')\n    if method == 'common':\n        return round(value, precision)\n    func = getattr(math, method)\n    return func(value * (10 ** precision)) / (10 ** precision)", "response": "Round the number to a given precision."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef do_groupby(environment, value, attribute):\n    expr = make_attrgetter(environment, attribute)\n    return [_GroupTuple(key, list(values)) for key, values\n            in groupby(sorted(value, key=expr), expr)]", "response": "Group a sequence of objects by a common attribute."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef do_sum(environment, iterable, attribute=None, start=0):\n    if attribute is not None:\n        iterable = imap(make_attrgetter(environment, attribute), iterable)\n    return sum(iterable, start)", "response": "Returns the sum of a sequence of numbers plus the value of parameter\nWorkItem start."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef do_reverse(value):\n    if isinstance(value, string_types):\n        return value[::-1]\n    try:\n        return reversed(value)\n    except TypeError:\n        try:\n            rv = list(value)\n            rv.reverse()\n            return rv\n        except TypeError:\n            raise FilterArgumentError('argument must be iterable')", "response": "Reverse the object or return an iterator that iterates over it the other\n    way round."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting an attribute of an object.", "response": "def do_attr(environment, obj, name):\n    \"\"\"Get an attribute of an object.  ``foo|attr(\"bar\")`` works like\n    ``foo.bar`` just that always an attribute is returned and items are not\n    looked up.\n\n    See :ref:`Notes on subscriptions <notes-on-subscriptions>` for more details.\n    \"\"\"\n    try:\n        name = str(name)\n    except UnicodeError:\n        pass\n    else:\n        try:\n            value = getattr(obj, name)\n        except AttributeError:\n            pass\n        else:\n            if environment.sandboxed and not \\\n               environment.is_safe_attribute(obj, name, value):\n                return environment.unsafe_undefined(obj, name)\n            return value\n    return environment.undefined(obj=obj, name=name)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\napply a filter on a sequence of objects or looks up an attribute.", "response": "def do_map(*args, **kwargs):\n    \"\"\"Applies a filter on a sequence of objects or looks up an attribute.\n    This is useful when dealing with lists of objects but you are really\n    only interested in a certain value of it.\n\n    The basic usage is mapping on an attribute.  Imagine you have a list\n    of users but you are only interested in a list of usernames:\n\n    .. sourcecode:: jinja\n\n        Users on this page: {{ users|map(attribute='username')|join(', ') }}\n\n    Alternatively you can let it invoke a filter by passing the name of the\n    filter and the arguments afterwards.  A good example would be applying a\n    text conversion filter on a sequence:\n\n    .. sourcecode:: jinja\n\n        Users on this page: {{ titles|map('lower')|join(', ') }}\n\n    .. versionadded:: 2.7\n    \"\"\"\n    seq, func = prepare_map(args, kwargs)\n    if seq:\n        for item in seq:\n            yield func(item)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndumps a structure to JSON so that it s safe to use in HTML contexts.", "response": "def do_tojson(eval_ctx, value, indent=None):\n    \"\"\"Dumps a structure to JSON so that it's safe to use in ``<script>``\n    tags.  It accepts the same arguments and returns a JSON string.  Note that\n    this is available in templates through the ``|tojson`` filter which will\n    also mark the result as safe.  Due to how this function escapes certain\n    characters this is safe even if used outside of ``<script>`` tags.\n\n    The following characters are escaped in strings:\n\n    -   ``<``\n    -   ``>``\n    -   ``&``\n    -   ``'``\n\n    This makes it safe to embed such strings in any place in HTML with the\n    notable exception of double quoted attributes.  In that case single\n    quote your attributes or HTML escape it in addition.\n\n    The indent parameter can be used to enable pretty printing.  Set it to\n    the number of spaces that the structures should be indented with.\n\n    Note that this filter is for use in HTML contexts only.\n\n    .. versionadded:: 2.9\n    \"\"\"\n    policies = eval_ctx.environment.policies\n    dumper = policies['json.dumps_function']\n    options = policies['json.dumps_kwargs']\n    if indent is not None:\n        options = dict(options)\n        options['indent'] = indent\n    return htmlsafe_json_dumps(value, dumper=dumper, **options)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef autocomplete():\n    # Don't complete if user hasn't sourced bash_completion file.\n    if 'PIP_AUTO_COMPLETE' not in os.environ:\n        return\n    cwords = os.environ['COMP_WORDS'].split()[1:]\n    cword = int(os.environ['COMP_CWORD'])\n    try:\n        current = cwords[cword - 1]\n    except IndexError:\n        current = ''\n\n    subcommands = [cmd for cmd, summary in get_summaries()]\n    options = []\n    # subcommand\n    try:\n        subcommand_name = [w for w in cwords if w in subcommands][0]\n    except IndexError:\n        subcommand_name = None\n\n    parser = create_main_parser()\n    # subcommand options\n    if subcommand_name:\n        # special case: 'help' subcommand has no options\n        if subcommand_name == 'help':\n            sys.exit(1)\n        # special case: list locally installed dists for show and uninstall\n        should_list_installed = (\n            subcommand_name in ['show', 'uninstall'] and\n            not current.startswith('-')\n        )\n        if should_list_installed:\n            installed = []\n            lc = current.lower()\n            for dist in get_installed_distributions(local_only=True):\n                if dist.key.startswith(lc) and dist.key not in cwords[1:]:\n                    installed.append(dist.key)\n            # if there are no dists installed, fall back to option completion\n            if installed:\n                for dist in installed:\n                    print(dist)\n                sys.exit(1)\n\n        subcommand = commands_dict[subcommand_name]()\n\n        for opt in subcommand.parser.option_list_all:\n            if opt.help != optparse.SUPPRESS_HELP:\n                for opt_str in opt._long_opts + opt._short_opts:\n                    options.append((opt_str, opt.nargs))\n\n        # filter out previously specified options from available options\n        prev_opts = [x.split('=')[0] for x in cwords[1:cword - 1]]\n        options = [(x, v) for (x, v) in options if x not in prev_opts]\n        # filter options by current input\n        options = [(k, v) for k, v in options if k.startswith(current)]\n        # get completion type given cwords and available subcommand options\n        completion_type = get_path_completion_type(\n            cwords, cword, subcommand.parser.option_list_all,\n        )\n        # get completion files and directories if ``completion_type`` is\n        # ``<file>``, ``<dir>`` or ``<path>``\n        if completion_type:\n            options = auto_complete_paths(current, completion_type)\n            options = ((opt, 0) for opt in options)\n        for option in options:\n            opt_label = option[0]\n            # append '=' to options which require args\n            if option[1] and option[0][:2] == \"--\":\n                opt_label += '='\n            print(opt_label)\n    else:\n        # show main parser options only when necessary\n\n        opts = [i.option_list for i in parser.option_groups]\n        opts.append(parser.option_list)\n        opts = (o for it in opts for o in it)\n        if current.startswith('-'):\n            for opt in opts:\n                if opt.help != optparse.SUPPRESS_HELP:\n                    subcommands += opt._long_opts + opt._short_opts\n        else:\n            # get completion type given cwords and all available options\n            completion_type = get_path_completion_type(cwords, cword, opts)\n            if completion_type:\n                subcommands = auto_complete_paths(current, completion_type)\n\n        print(' '.join([x for x in subcommands if x.startswith(current)]))\n    sys.exit(1)", "response": "Entry Point for completion of main and subcommand options."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_path_completion_type(cwords, cword, opts):\n    if cword < 2 or not cwords[cword - 2].startswith('-'):\n        return\n    for opt in opts:\n        if opt.help == optparse.SUPPRESS_HELP:\n            continue\n        for o in str(opt).split('/'):\n            if cwords[cword - 2].split('=')[0] == o:\n                if not opt.metavar or any(\n                        x in ('path', 'file', 'dir')\n                        for x in opt.metavar.split('/')):\n                    return opt.metavar", "response": "Get the type of path completion."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a generator of all possible paths to complete the current word.", "response": "def auto_complete_paths(current, completion_type):\n    \"\"\"If ``completion_type`` is ``file`` or ``path``, list all regular files\n    and directories starting with ``current``; otherwise only list directories\n    starting with ``current``.\n\n    :param current: The word to be completed\n    :param completion_type: path completion type(`file`, `path` or `dir`)i\n    :return: A generator of regular files and/or directories\n    \"\"\"\n    directory, filename = os.path.split(current)\n    current_path = os.path.abspath(directory)\n    # Don't complete paths if they can't be accessed\n    if not os.access(current_path, os.R_OK):\n        return\n    filename = os.path.normcase(filename)\n    # list all files that start with ``filename``\n    file_list = (x for x in os.listdir(current_path)\n                 if os.path.normcase(x).startswith(filename))\n    for f in file_list:\n        opt = os.path.join(current_path, f)\n        comp_file = os.path.normcase(os.path.join(directory, f))\n        # complete regular files when there is not ``<dir>`` after option\n        # complete directories when there is ``<file>``, ``<path>`` or\n        # ``<dir>``after option\n        if completion_type != 'dir' and os.path.isfile(opt):\n            yield comp_file\n        elif os.path.isdir(opt):\n            yield os.path.join(comp_file, '')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _build_wheel_modern(ireq, output_dir, finder, wheel_cache, kwargs):\n    kwargs.update({\"progress_bar\": \"off\", \"build_isolation\": False})\n    with pip_shims.RequirementTracker() as req_tracker:\n        if req_tracker:\n            kwargs[\"req_tracker\"] = req_tracker\n        preparer = pip_shims.RequirementPreparer(**kwargs)\n        builder = pip_shims.WheelBuilder(finder, preparer, wheel_cache)\n        return builder._build_one(ireq, output_dir)", "response": "Build a modern wheel."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_python_version(path):\n    # type: (str) -> str\n    \"\"\"Get python version string using subprocess from a given path.\"\"\"\n    version_cmd = [path, \"-c\", \"import sys; print(sys.version.split()[0])\"]\n    try:\n        c = vistir.misc.run(\n            version_cmd,\n            block=True,\n            nospin=True,\n            return_object=True,\n            combine_stderr=False,\n            write_to_stdout=False,\n        )\n    except OSError:\n        raise InvalidPythonVersion(\"%s is not a valid python path\" % path)\n    if not c.out:\n        raise InvalidPythonVersion(\"%s is not a valid python path\" % path)\n    return c.out.strip()", "response": "Get python version string using subprocess from a given path."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn whether a given path is a known executable from known executable extensions or has the executable bit toggled.", "response": "def path_is_known_executable(path):\n    # type: (vistir.compat.Path) -> bool\n    \"\"\"\n    Returns whether a given path is a known executable from known executable extensions\n    or has the executable bit toggled.\n\n    :param path: The path to the target executable.\n    :type path: :class:`~vistir.compat.Path`\n    :return: True if the path has chmod +x, or is a readable, known executable extension.\n    :rtype: bool\n    \"\"\"\n\n    return (\n        path_is_executable(path)\n        or os.access(str(path), os.R_OK)\n        and path.suffix in KNOWN_EXTS\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndetermine whether the provided filename looks like a possible name of python.", "response": "def looks_like_python(name):\n    # type: (str) -> bool\n    \"\"\"\n    Determine whether the supplied filename looks like a possible name of python.\n\n    :param str name: The name of the provided file.\n    :return: Whether the provided name looks like python.\n    :rtype: bool\n    \"\"\"\n\n    if not any(name.lower().startswith(py_name) for py_name in PYTHON_IMPLEMENTATIONS):\n        return False\n    match = RE_MATCHER.match(name)\n    if match:\n        return any(fnmatch(name, rule) for rule in MATCH_RULES)\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ensure_path(path):\n    # type: (Union[vistir.compat.Path, str]) -> vistir.compat.Path\n    \"\"\"\n    Given a path (either a string or a Path object), expand variables and return a Path object.\n\n    :param path: A string or a :class:`~pathlib.Path` object.\n    :type path: str or :class:`~pathlib.Path`\n    :return: A fully expanded Path object.\n    :rtype: :class:`~pathlib.Path`\n    \"\"\"\n\n    if isinstance(path, vistir.compat.Path):\n        return path\n    path = vistir.compat.Path(os.path.expandvars(path))\n    return path.absolute()", "response": "Ensures that a path is fully expanded."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef filter_pythons(path):\n    # type: (Union[str, vistir.compat.Path]) -> Iterable\n    \"\"\"Return all valid pythons in a given path\"\"\"\n    if not isinstance(path, vistir.compat.Path):\n        path = vistir.compat.Path(str(path))\n    if not path.is_dir():\n        return path if path_is_python(path) else None\n    return filter(path_is_python, path.iterdir())", "response": "Return all valid pythons in a given path"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting parts of path that must be os. path. joined with cache_dir", "response": "def _get_cache_path_parts(self, link):\n        # type: (Link) -> List[str]\n        \"\"\"Get parts of part that must be os.path.joined with cache_dir\n        \"\"\"\n\n        # We want to generate an url to use as our cache key, we don't want to\n        # just re-use the URL because it might have other items in the fragment\n        # and we don't care about those.\n        key_parts = [link.url_without_fragment]\n        if link.hash_name is not None and link.hash is not None:\n            key_parts.append(\"=\".join([link.hash_name, link.hash]))\n        key_url = \"#\".join(key_parts)\n\n        # Encode our key url with sha224, we'll use this because it has similar\n        # security properties to sha256, but with a shorter total output (and\n        # thus less secure). However the differences don't make a lot of\n        # difference for our use case here.\n        hashed = hashlib.sha224(key_url.encode()).hexdigest()\n\n        # We want to nest the directories some to prevent having a ton of top\n        # level directories where we might run out of sub directories on some\n        # FS.\n        parts = [hashed[:2], hashed[2:4], hashed[4:6], hashed[6:]]\n\n        return parts"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_path_for_link(self, link):\n        # type: (Link) -> str\n        \"\"\"Return a directory to store cached wheels for link\n\n        Because there are M wheels for any one sdist, we provide a directory\n        to cache them in, and then consult that directory when looking up\n        cache hits.\n\n        We only insert things into the cache if they have plausible version\n        numbers, so that we don't contaminate the cache with things that were\n        not unique. E.g. ./package might have dozens of installs done for it\n        and build a version of 0.0...and if we built and cached a wheel, we'd\n        end up using the same wheel even if the source has been edited.\n\n        :param link: The link of the sdist for which this will cache wheels.\n        \"\"\"\n        parts = self._get_cache_path_parts(link)\n\n        # Store wheels within the root cache_dir\n        return os.path.join(self.cache_dir, \"wheels\", *parts)", "response": "Return a directory to store wheels for a link."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndetermining if this is an artifact or not.", "response": "def is_artifact(self):\n        # type: () -> bool\n        \"\"\"\n        Determines if this points to an actual artifact (e.g. a tarball) or if\n        it points to an \"abstract\" thing like a path or a VCS location.\n        \"\"\"\n        from pipenv.patched.notpip._internal.vcs import vcs\n\n        if self.scheme in vcs.all_schemes:\n            return False\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves the dependencies for the given requirement from the dependency cache.", "response": "def _get_dependencies_from_cache(ireq):\n    \"\"\"Retrieves dependencies for the requirement from the dependency cache.\n    \"\"\"\n    if os.environ.get(\"PASSA_IGNORE_LOCAL_CACHE\"):\n        return\n    if ireq.editable:\n        return\n    try:\n        deps = DEPENDENCY_CACHE[ireq]\n        pyrq = REQUIRES_PYTHON_CACHE[ireq]\n    except KeyError:\n        return\n\n    # Preserving sanity: Run through the cache and make sure every entry if\n    # valid. If this fails, something is wrong with the cache. Drop it.\n    try:\n        packaging.specifiers.SpecifierSet(pyrq)\n        ireq_name = packaging.utils.canonicalize_name(ireq.name)\n        if any(_is_cache_broken(line, ireq_name) for line in deps):\n            broken = True\n        else:\n            broken = False\n    except Exception:\n        broken = True\n\n    if broken:\n        print(\"dropping broken cache for {0}\".format(ireq.name))\n        del DEPENDENCY_CACHE[ireq]\n        del REQUIRES_PYTHON_CACHE[ireq]\n        return\n\n    return deps, pyrq"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving the dependencies for the install requirement from the JSON API.", "response": "def _get_dependencies_from_json(ireq, sources):\n    \"\"\"Retrieves dependencies for the install requirement from the JSON API.\n\n    :param ireq: A single InstallRequirement\n    :type ireq: :class:`~pip._internal.req.req_install.InstallRequirement`\n    :return: A set of dependency lines for generating new InstallRequirements.\n    :rtype: set(str) or None\n    \"\"\"\n    if os.environ.get(\"PASSA_IGNORE_JSON_API\"):\n        return\n\n    # It is technically possible to parse extras out of the JSON API's\n    # requirement format, but it is such a chore let's just use the simple API.\n    if ireq.extras:\n        return\n\n    try:\n        version = get_pinned_version(ireq)\n    except ValueError:\n        return\n\n    url_prefixes = [\n        proc_url[:-7]   # Strip \"/simple\".\n        for proc_url in (\n            raw_url.rstrip(\"/\")\n            for raw_url in (source.get(\"url\", \"\") for source in sources)\n        )\n        if proc_url.endswith(\"/simple\")\n    ]\n\n    session = requests.session()\n\n    for prefix in url_prefixes:\n        url = \"{prefix}/pypi/{name}/{version}/json\".format(\n            prefix=prefix,\n            name=packaging.utils.canonicalize_name(ireq.name),\n            version=version,\n        )\n        try:\n            dependencies = _get_dependencies_from_json_url(url, session)\n            if dependencies is not None:\n                return dependencies\n        except Exception as e:\n            print(\"unable to read dependencies via {0} ({1})\".format(url, e))\n    session.close()\n    return"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _read_requirements(metadata, extras):\n    extras = extras or ()\n    requirements = []\n    for entry in metadata.run_requires:\n        if isinstance(entry, six.text_type):\n            entry = {\"requires\": [entry]}\n            extra = None\n        else:\n            extra = entry.get(\"extra\")\n        if extra is not None and extra not in extras:\n            continue\n        for line in entry.get(\"requires\", []):\n            r = requirementslib.Requirement.from_line(line)\n            if r.markers:\n                contained = get_contained_extras(r.markers)\n                if (contained and not any(e in contained for e in extras)):\n                    continue\n                marker = get_without_extra(r.markers)\n                r.markers = str(marker) if marker else None\n                line = r.as_line(include_hashes=False)\n            requirements.append(line)\n    return requirements", "response": "Read wheel metadata to know what it depends on."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread wheel metadata to know the value of Requires - Python.", "response": "def _read_requires_python(metadata):\n    \"\"\"Read wheel metadata to know the value of Requires-Python.\n\n    This is surprisingly poorly supported in Distlib. This function tries\n    several ways to get this information:\n\n    * Metadata 2.0: metadata.dictionary.get(\"requires_python\") is not None\n    * Metadata 2.1: metadata._legacy.get(\"Requires-Python\") is not None\n    * Metadata 1.2: metadata._legacy.get(\"Requires-Python\") != \"UNKNOWN\"\n    \"\"\"\n    # TODO: Support more metadata formats.\n    value = metadata.dictionary.get(\"requires_python\")\n    if value is not None:\n        return value\n    if metadata._legacy:\n        value = metadata._legacy.get(\"Requires-Python\")\n        if value is not None and value != \"UNKNOWN\":\n            return value\n    return \"\""}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve the dependencies for the given pip - style requirement.", "response": "def _get_dependencies_from_pip(ireq, sources):\n    \"\"\"Retrieves dependencies for the requirement from pipenv.patched.notpip internals.\n\n    The current strategy is to try the followings in order, returning the\n    first successful result.\n\n    1. Try to build a wheel out of the ireq, and read metadata out of it.\n    2. Read metadata out of the egg-info directory if it is present.\n    \"\"\"\n    extras = ireq.extras or ()\n    try:\n        wheel = build_wheel(ireq, sources)\n    except WheelBuildError:\n        # XXX: This depends on a side effect of `build_wheel`. This block is\n        # reached when it fails to build an sdist, where the sdist would have\n        # been downloaded, extracted into `ireq.source_dir`, and partially\n        # built (hopefully containing .egg-info).\n        metadata = read_sdist_metadata(ireq)\n        if not metadata:\n            raise\n    else:\n        metadata = wheel.metadata\n    requirements = _read_requirements(metadata, extras)\n    requires_python = _read_requires_python(metadata)\n    return requirements, requires_python"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_dependencies(requirement, sources):\n    getters = [\n        _get_dependencies_from_cache,\n        _cached(_get_dependencies_from_json, sources=sources),\n        _cached(_get_dependencies_from_pip, sources=sources),\n    ]\n    ireq = requirement.as_ireq()\n    last_exc = None\n    for getter in getters:\n        try:\n            result = getter(ireq)\n        except Exception as e:\n            last_exc = sys.exc_info()\n            continue\n        if result is not None:\n            deps, pyreq = result\n            reqs = [requirementslib.Requirement.from_line(d) for d in deps]\n            return reqs, pyreq\n    if last_exc:\n        six.reraise(*last_exc)\n    raise RuntimeError(\"failed to get dependencies for {}\".format(\n        requirement.as_line(),\n    ))", "response": "Get all dependencies for a given install requirement."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrendering the headers for this request field.", "response": "def render_headers(self):\n        \"\"\"\n        Renders the headers for this request field.\n        \"\"\"\n        lines = []\n\n        sort_keys = ['Content-Disposition', 'Content-Type', 'Content-Location']\n        for sort_key in sort_keys:\n            if self.headers.get(sort_key, False):\n                lines.append('%s: %s' % (sort_key, self.headers[sort_key]))\n\n        for header_name, header_value in self.headers.items():\n            if header_name not in sort_keys:\n                if header_value:\n                    lines.append('%s: %s' % (header_name, header_value))\n\n        lines.append('\\r\\n')\n        return '\\r\\n'.join(lines)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_multipart(self, content_disposition=None, content_type=None,\n                       content_location=None):\n        \"\"\"\n        Makes this request field into a multipart request field.\n\n        This method overrides \"Content-Disposition\", \"Content-Type\" and\n        \"Content-Location\" headers to the request parameter.\n\n        :param content_type:\n            The 'Content-Type' of the request body.\n        :param content_location:\n            The 'Content-Location' of the request body.\n\n        \"\"\"\n        self.headers['Content-Disposition'] = content_disposition or 'form-data'\n        self.headers['Content-Disposition'] += '; '.join([\n            '', self._render_parts(\n                (('name', self._name), ('filename', self._filename))\n            )\n        ])\n        self.headers['Content-Type'] = content_type\n        self.headers['Content-Location'] = content_location", "response": "Makes this request field into a multipart request field."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef emptyTag(self, namespace, name, attrs, hasChildren=False):\n        yield {\"type\": \"EmptyTag\", \"name\": name,\n               \"namespace\": namespace,\n               \"data\": attrs}\n        if hasChildren:\n            yield self.error(\"Void element has children\")", "response": "Generates an EmptyTag token"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates SpaceCharacters and Characters tokens from the text.", "response": "def text(self, data):\n        \"\"\"Generates SpaceCharacters and Characters tokens\n\n        Depending on what's in the data, this generates one or more\n        ``SpaceCharacters`` and ``Characters`` tokens.\n\n        For example:\n\n            >>> from html5lib.treewalkers.base import TreeWalker\n            >>> # Give it an empty tree just so it instantiates\n            >>> walker = TreeWalker([])\n            >>> list(walker.text(''))\n            []\n            >>> list(walker.text('  '))\n            [{u'data': '  ', u'type': u'SpaceCharacters'}]\n            >>> list(walker.text(' abc '))  # doctest: +NORMALIZE_WHITESPACE\n            [{u'data': ' ', u'type': u'SpaceCharacters'},\n            {u'data': u'abc', u'type': u'Characters'},\n            {u'data': u' ', u'type': u'SpaceCharacters'}]\n\n        :arg data: the text data\n\n        :returns: one or more ``SpaceCharacters`` and ``Characters`` tokens\n\n        \"\"\"\n        data = data\n        middle = data.lstrip(spaceCharacters)\n        left = data[:len(data) - len(middle)]\n        if left:\n            yield {\"type\": \"SpaceCharacters\", \"data\": left}\n        data = middle\n        middle = data.rstrip(spaceCharacters)\n        right = data[len(middle):]\n        if middle:\n            yield {\"type\": \"Characters\", \"data\": middle}\n        if right:\n            yield {\"type\": \"SpaceCharacters\", \"data\": right}"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the script to activate a virtualenv.", "response": "def _get_activate_script(cmd, venv):\n    \"\"\"Returns the string to activate a virtualenv.\n\n    This is POSIX-only at the moment since the compat (pexpect-based) shell\n    does not work elsewhere anyway.\n    \"\"\"\n    # Suffix and source command for other shells.\n    # Support for fish shell.\n    if \"fish\" in cmd:\n        suffix = \".fish\"\n        command = \"source\"\n    # Support for csh shell.\n    elif \"csh\" in cmd:\n        suffix = \".csh\"\n        command = \"source\"\n    else:\n        suffix = \"\"\n        command = \".\"\n    # Escape any spaces located within the virtualenv path to allow\n    # for proper activation.\n    venv_location = str(venv).replace(\" \", r\"\\ \")\n    # The leading space can make history cleaner in some shells.\n    return \" {2} {0}/bin/activate{1}\".format(venv_location, suffix, command)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbuilds and return a filename from the various components.", "response": "def filename(self):\n        \"\"\"\n        Build and return a filename from the various components.\n        \"\"\"\n        if self.buildver:\n            buildver = '-' + self.buildver\n        else:\n            buildver = ''\n        pyver = '.'.join(self.pyver)\n        abi = '.'.join(self.abi)\n        arch = '.'.join(self.arch)\n        # replace - with _ as a local version separator\n        version = self.version.replace('-', '_')\n        return '%s-%s%s-%s-%s-%s.whl' % (self.name, version, buildver,\n                                         pyver, abi, arch)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds a wheel from files in specified paths and use any specified tags when determining the name of the wheel.", "response": "def build(self, paths, tags=None, wheel_version=None):\n        \"\"\"\n        Build a wheel from files in specified paths, and use any specified tags\n        when determining the name of the wheel.\n        \"\"\"\n        if tags is None:\n            tags = {}\n\n        libkey = list(filter(lambda o: o in paths, ('purelib', 'platlib')))[0]\n        if libkey == 'platlib':\n            is_pure = 'false'\n            default_pyver = [IMPVER]\n            default_abi = [ABI]\n            default_arch = [ARCH]\n        else:\n            is_pure = 'true'\n            default_pyver = [PYVER]\n            default_abi = ['none']\n            default_arch = ['any']\n\n        self.pyver = tags.get('pyver', default_pyver)\n        self.abi = tags.get('abi', default_abi)\n        self.arch = tags.get('arch', default_arch)\n\n        libdir = paths[libkey]\n\n        name_ver = '%s-%s' % (self.name, self.version)\n        data_dir = '%s.data' % name_ver\n        info_dir = '%s.dist-info' % name_ver\n\n        archive_paths = []\n\n        # First, stuff which is not in site-packages\n        for key in ('data', 'headers', 'scripts'):\n            if key not in paths:\n                continue\n            path = paths[key]\n            if os.path.isdir(path):\n                for root, dirs, files in os.walk(path):\n                    for fn in files:\n                        p = fsdecode(os.path.join(root, fn))\n                        rp = os.path.relpath(p, path)\n                        ap = to_posix(os.path.join(data_dir, key, rp))\n                        archive_paths.append((ap, p))\n                        if key == 'scripts' and not p.endswith('.exe'):\n                            with open(p, 'rb') as f:\n                                data = f.read()\n                            data = self.process_shebang(data)\n                            with open(p, 'wb') as f:\n                                f.write(data)\n\n        # Now, stuff which is in site-packages, other than the\n        # distinfo stuff.\n        path = libdir\n        distinfo = None\n        for root, dirs, files in os.walk(path):\n            if root == path:\n                # At the top level only, save distinfo for later\n                # and skip it for now\n                for i, dn in enumerate(dirs):\n                    dn = fsdecode(dn)\n                    if dn.endswith('.dist-info'):\n                        distinfo = os.path.join(root, dn)\n                        del dirs[i]\n                        break\n                assert distinfo, '.dist-info directory expected, not found'\n\n            for fn in files:\n                # comment out next suite to leave .pyc files in\n                if fsdecode(fn).endswith(('.pyc', '.pyo')):\n                    continue\n                p = os.path.join(root, fn)\n                rp = to_posix(os.path.relpath(p, path))\n                archive_paths.append((rp, p))\n\n        # Now distinfo. Assumed to be flat, i.e. os.listdir is enough.\n        files = os.listdir(distinfo)\n        for fn in files:\n            if fn not in ('RECORD', 'INSTALLER', 'SHARED', 'WHEEL'):\n                p = fsdecode(os.path.join(distinfo, fn))\n                ap = to_posix(os.path.join(info_dir, fn))\n                archive_paths.append((ap, p))\n\n        wheel_metadata = [\n            'Wheel-Version: %d.%d' % (wheel_version or self.wheel_version),\n            'Generator: distlib %s' % __version__,\n            'Root-Is-Purelib: %s' % is_pure,\n        ]\n        for pyver, abi, arch in self.tags:\n            wheel_metadata.append('Tag: %s-%s-%s' % (pyver, abi, arch))\n        p = os.path.join(distinfo, 'WHEEL')\n        with open(p, 'w') as f:\n            f.write('\\n'.join(wheel_metadata))\n        ap = to_posix(os.path.join(info_dir, 'WHEEL'))\n        archive_paths.append((ap, p))\n\n        # Now, at last, RECORD.\n        # Paths in here are archive paths - nothing else makes sense.\n        self.write_records((distinfo, info_dir), libdir, archive_paths)\n        # Now, ready to build the zip file\n        pathname = os.path.join(self.dirname, self.filename)\n        self.build_zip(pathname, archive_paths)\n        return pathname"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninstall a wheel to the specified paths.", "response": "def install(self, paths, maker, **kwargs):\n        \"\"\"\n        Install a wheel to the specified paths. If kwarg ``warner`` is\n        specified, it should be a callable, which will be called with two\n        tuples indicating the wheel version of this software and the wheel\n        version in the file, if there is a discrepancy in the versions.\n        This can be used to issue any warnings to raise any exceptions.\n        If kwarg ``lib_only`` is True, only the purelib/platlib files are\n        installed, and the headers, scripts, data and dist-info metadata are\n        not written. If kwarg ``bytecode_hashed_invalidation`` is True, written\n        bytecode will try to use file-hash based invalidation (PEP-552) on\n        supported interpreter versions (CPython 2.7+).\n\n        The return value is a :class:`InstalledDistribution` instance unless\n        ``options.lib_only`` is True, in which case the return value is ``None``.\n        \"\"\"\n\n        dry_run = maker.dry_run\n        warner = kwargs.get('warner')\n        lib_only = kwargs.get('lib_only', False)\n        bc_hashed_invalidation = kwargs.get('bytecode_hashed_invalidation', False)\n\n        pathname = os.path.join(self.dirname, self.filename)\n        name_ver = '%s-%s' % (self.name, self.version)\n        data_dir = '%s.data' % name_ver\n        info_dir = '%s.dist-info' % name_ver\n\n        metadata_name = posixpath.join(info_dir, METADATA_FILENAME)\n        wheel_metadata_name = posixpath.join(info_dir, 'WHEEL')\n        record_name = posixpath.join(info_dir, 'RECORD')\n\n        wrapper = codecs.getreader('utf-8')\n\n        with ZipFile(pathname, 'r') as zf:\n            with zf.open(wheel_metadata_name) as bwf:\n                wf = wrapper(bwf)\n                message = message_from_file(wf)\n            wv = message['Wheel-Version'].split('.', 1)\n            file_version = tuple([int(i) for i in wv])\n            if (file_version != self.wheel_version) and warner:\n                warner(self.wheel_version, file_version)\n\n            if message['Root-Is-Purelib'] == 'true':\n                libdir = paths['purelib']\n            else:\n                libdir = paths['platlib']\n\n            records = {}\n            with zf.open(record_name) as bf:\n                with CSVReader(stream=bf) as reader:\n                    for row in reader:\n                        p = row[0]\n                        records[p] = row\n\n            data_pfx = posixpath.join(data_dir, '')\n            info_pfx = posixpath.join(info_dir, '')\n            script_pfx = posixpath.join(data_dir, 'scripts', '')\n\n            # make a new instance rather than a copy of maker's,\n            # as we mutate it\n            fileop = FileOperator(dry_run=dry_run)\n            fileop.record = True    # so we can rollback if needed\n\n            bc = not sys.dont_write_bytecode    # Double negatives. Lovely!\n\n            outfiles = []   # for RECORD writing\n\n            # for script copying/shebang processing\n            workdir = tempfile.mkdtemp()\n            # set target dir later\n            # we default add_launchers to False, as the\n            # Python Launcher should be used instead\n            maker.source_dir = workdir\n            maker.target_dir = None\n            try:\n                for zinfo in zf.infolist():\n                    arcname = zinfo.filename\n                    if isinstance(arcname, text_type):\n                        u_arcname = arcname\n                    else:\n                        u_arcname = arcname.decode('utf-8')\n                    # The signature file won't be in RECORD,\n                    # and we  don't currently don't do anything with it\n                    if u_arcname.endswith('/RECORD.jws'):\n                        continue\n                    row = records[u_arcname]\n                    if row[2] and str(zinfo.file_size) != row[2]:\n                        raise DistlibException('size mismatch for '\n                                               '%s' % u_arcname)\n                    if row[1]:\n                        kind, value = row[1].split('=', 1)\n                        with zf.open(arcname) as bf:\n                            data = bf.read()\n                        _, digest = self.get_hash(data, kind)\n                        if digest != value:\n                            raise DistlibException('digest mismatch for '\n                                                   '%s' % arcname)\n\n                    if lib_only and u_arcname.startswith((info_pfx, data_pfx)):\n                        logger.debug('lib_only: skipping %s', u_arcname)\n                        continue\n                    is_script = (u_arcname.startswith(script_pfx)\n                                 and not u_arcname.endswith('.exe'))\n\n                    if u_arcname.startswith(data_pfx):\n                        _, where, rp = u_arcname.split('/', 2)\n                        outfile = os.path.join(paths[where], convert_path(rp))\n                    else:\n                        # meant for site-packages.\n                        if u_arcname in (wheel_metadata_name, record_name):\n                            continue\n                        outfile = os.path.join(libdir, convert_path(u_arcname))\n                    if not is_script:\n                        with zf.open(arcname) as bf:\n                            fileop.copy_stream(bf, outfile)\n                        outfiles.append(outfile)\n                        # Double check the digest of the written file\n                        if not dry_run and row[1]:\n                            with open(outfile, 'rb') as bf:\n                                data = bf.read()\n                                _, newdigest = self.get_hash(data, kind)\n                                if newdigest != digest:\n                                    raise DistlibException('digest mismatch '\n                                                           'on write for '\n                                                           '%s' % outfile)\n                        if bc and outfile.endswith('.py'):\n                            try:\n                                pyc = fileop.byte_compile(outfile,\n                                                          hashed_invalidation=bc_hashed_invalidation)\n                                outfiles.append(pyc)\n                            except Exception:\n                                # Don't give up if byte-compilation fails,\n                                # but log it and perhaps warn the user\n                                logger.warning('Byte-compilation failed',\n                                               exc_info=True)\n                    else:\n                        fn = os.path.basename(convert_path(arcname))\n                        workname = os.path.join(workdir, fn)\n                        with zf.open(arcname) as bf:\n                            fileop.copy_stream(bf, workname)\n\n                        dn, fn = os.path.split(outfile)\n                        maker.target_dir = dn\n                        filenames = maker.make(fn)\n                        fileop.set_executable_mode(filenames)\n                        outfiles.extend(filenames)\n\n                if lib_only:\n                    logger.debug('lib_only: returning None')\n                    dist = None\n                else:\n                    # Generate scripts\n\n                    # Try to get pydist.json so we can see if there are\n                    # any commands to generate. If this fails (e.g. because\n                    # of a legacy wheel), log a warning but don't give up.\n                    commands = None\n                    file_version = self.info['Wheel-Version']\n                    if file_version == '1.0':\n                        # Use legacy info\n                        ep = posixpath.join(info_dir, 'entry_points.txt')\n                        try:\n                            with zf.open(ep) as bwf:\n                                epdata = read_exports(bwf)\n                            commands = {}\n                            for key in ('console', 'gui'):\n                                k = '%s_scripts' % key\n                                if k in epdata:\n                                    commands['wrap_%s' % key] = d = {}\n                                    for v in epdata[k].values():\n                                        s = '%s:%s' % (v.prefix, v.suffix)\n                                        if v.flags:\n                                            s += ' %s' % v.flags\n                                        d[v.name] = s\n                        except Exception:\n                            logger.warning('Unable to read legacy script '\n                                           'metadata, so cannot generate '\n                                           'scripts')\n                    else:\n                        try:\n                            with zf.open(metadata_name) as bwf:\n                                wf = wrapper(bwf)\n                                commands = json.load(wf).get('extensions')\n                                if commands:\n                                    commands = commands.get('python.commands')\n                        except Exception:\n                            logger.warning('Unable to read JSON metadata, so '\n                                           'cannot generate scripts')\n                    if commands:\n                        console_scripts = commands.get('wrap_console', {})\n                        gui_scripts = commands.get('wrap_gui', {})\n                        if console_scripts or gui_scripts:\n                            script_dir = paths.get('scripts', '')\n                            if not os.path.isdir(script_dir):\n                                raise ValueError('Valid script path not '\n                                                 'specified')\n                            maker.target_dir = script_dir\n                            for k, v in console_scripts.items():\n                                script = '%s = %s' % (k, v)\n                                filenames = maker.make(script)\n                                fileop.set_executable_mode(filenames)\n\n                            if gui_scripts:\n                                options = {'gui': True }\n                                for k, v in gui_scripts.items():\n                                    script = '%s = %s' % (k, v)\n                                    filenames = maker.make(script, options)\n                                    fileop.set_executable_mode(filenames)\n\n                    p = os.path.join(libdir, info_dir)\n                    dist = InstalledDistribution(p)\n\n                    # Write SHARED\n                    paths = dict(paths)     # don't change passed in dict\n                    del paths['purelib']\n                    del paths['platlib']\n                    paths['lib'] = libdir\n                    p = dist.write_shared_locations(paths, dry_run)\n                    if p:\n                        outfiles.append(p)\n\n                    # Write RECORD\n                    dist.write_installed_files(outfiles, paths['prefix'],\n                                               dry_run)\n                return dist\n            except Exception:  # pragma: no cover\n                logger.exception('installation failed.')\n                fileop.rollback()\n                raise\n            finally:\n                shutil.rmtree(workdir)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update(self, modifier, dest_dir=None, **kwargs):\n\n        def get_version(path_map, info_dir):\n            version = path = None\n            key = '%s/%s' % (info_dir, METADATA_FILENAME)\n            if key not in path_map:\n                key = '%s/PKG-INFO' % info_dir\n            if key in path_map:\n                path = path_map[key]\n                version = Metadata(path=path).version\n            return version, path\n\n        def update_version(version, path):\n            updated = None\n            try:\n                v = NormalizedVersion(version)\n                i = version.find('-')\n                if i < 0:\n                    updated = '%s+1' % version\n                else:\n                    parts = [int(s) for s in version[i + 1:].split('.')]\n                    parts[-1] += 1\n                    updated = '%s+%s' % (version[:i],\n                                         '.'.join(str(i) for i in parts))\n            except UnsupportedVersionError:\n                logger.debug('Cannot update non-compliant (PEP-440) '\n                             'version %r', version)\n            if updated:\n                md = Metadata(path=path)\n                md.version = updated\n                legacy = not path.endswith(METADATA_FILENAME)\n                md.write(path=path, legacy=legacy)\n                logger.debug('Version updated from %r to %r', version,\n                             updated)\n\n        pathname = os.path.join(self.dirname, self.filename)\n        name_ver = '%s-%s' % (self.name, self.version)\n        info_dir = '%s.dist-info' % name_ver\n        record_name = posixpath.join(info_dir, 'RECORD')\n        with tempdir() as workdir:\n            with ZipFile(pathname, 'r') as zf:\n                path_map = {}\n                for zinfo in zf.infolist():\n                    arcname = zinfo.filename\n                    if isinstance(arcname, text_type):\n                        u_arcname = arcname\n                    else:\n                        u_arcname = arcname.decode('utf-8')\n                    if u_arcname == record_name:\n                        continue\n                    if '..' in u_arcname:\n                        raise DistlibException('invalid entry in '\n                                               'wheel: %r' % u_arcname)\n                    zf.extract(zinfo, workdir)\n                    path = os.path.join(workdir, convert_path(u_arcname))\n                    path_map[u_arcname] = path\n\n            # Remember the version.\n            original_version, _ = get_version(path_map, info_dir)\n            # Files extracted. Call the modifier.\n            modified = modifier(path_map, **kwargs)\n            if modified:\n                # Something changed - need to build a new wheel.\n                current_version, path = get_version(path_map, info_dir)\n                if current_version and (current_version == original_version):\n                    # Add or update local version to signify changes.\n                    update_version(current_version, path)\n                # Decide where the new wheel goes.\n                if dest_dir is None:\n                    fd, newpath = tempfile.mkstemp(suffix='.whl',\n                                                   prefix='wheel-update-',\n                                                   dir=workdir)\n                    os.close(fd)\n                else:\n                    if not os.path.isdir(dest_dir):\n                        raise DistlibException('Not a directory: %r' % dest_dir)\n                    newpath = os.path.join(dest_dir, self.filename)\n                archive_paths = list(path_map.items())\n                distinfo = os.path.join(workdir, info_dir)\n                info = distinfo, info_dir\n                self.write_records(info, workdir, archive_paths)\n                self.build_zip(newpath, archive_paths)\n                if dest_dir is None:\n                    shutil.copyfile(newpath, pathname)\n        return modified", "response": "This method updates the contents of a wheel in a generic way."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloads .env file into sys.environ.", "response": "def load_dot_env():\n    \"\"\"Loads .env file into sys.environ.\"\"\"\n    if not environments.PIPENV_DONT_LOAD_ENV:\n        # If the project doesn't exist yet, check current directory for a .env file\n        project_directory = project.project_directory or \".\"\n        dotenv_file = environments.PIPENV_DOTENV_LOCATION or os.sep.join(\n            [project_directory, \".env\"]\n        )\n\n        if os.path.isfile(dotenv_file):\n            click.echo(\n                crayons.normal(fix_utf8(\"Loading .env environment variables\u2026\"), bold=True),\n                err=True,\n            )\n        else:\n            if environments.PIPENV_DOTENV_LOCATION:\n                click.echo(\n                    \"{0}: file {1}={2} does not exist!!\\n{3}\".format(\n                        crayons.red(\"Warning\", bold=True),\n                        crayons.normal(\"PIPENV_DOTENV_LOCATION\", bold=True),\n                        crayons.normal(environments.PIPENV_DOTENV_LOCATION, bold=True),\n                        crayons.red(\"Not loading environment variables.\", bold=True),\n                    ),\n                    err=True,\n                )\n        dotenv.load_dotenv(dotenv_file, override=True)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_to_path(p):\n    if p not in os.environ[\"PATH\"]:\n        os.environ[\"PATH\"] = \"{0}{1}{2}\".format(p, os.pathsep, os.environ[\"PATH\"])", "response": "Adds a given path to the PATH."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cleanup_virtualenv(bare=True):\n    if not bare:\n        click.echo(crayons.red(\"Environment creation aborted.\"))\n    try:\n        # Delete the virtualenv.\n        vistir.path.rmtree(project.virtualenv_location)\n    except OSError as e:\n        click.echo(\n            \"{0} An error occurred while removing {1}!\".format(\n                crayons.red(\"Error: \", bold=True),\n                crayons.green(project.virtualenv_location),\n            ),\n            err=True,\n        )\n        click.echo(crayons.blue(e), err=True)", "response": "Removes the virtualenv directory from the system."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ensure_pipfile(validate=True, skip_requirements=False, system=False):\n    from .environments import PIPENV_VIRTUALENV\n\n    # Assert Pipfile exists.\n    python = which(\"python\") if not (USING_DEFAULT_PYTHON or system) else None\n    if project.pipfile_is_empty:\n        # Show an error message and exit if system is passed and no pipfile exists\n        if system and not PIPENV_VIRTUALENV:\n            raise exceptions.PipenvOptionsError(\n                \"--system\",\n                \"--system is intended to be used for pre-existing Pipfile \"\n                \"installation, not installation of specific packages. Aborting.\"\n            )\n        # If there's a requirements file, but no Pipfile\u2026\n        if project.requirements_exists and not skip_requirements:\n            click.echo(\n                crayons.normal(\n                    fix_utf8(\"requirements.txt found, instead of Pipfile! Converting\u2026\"),\n                    bold=True,\n                )\n            )\n            # Create a Pipfile\u2026\n            project.create_pipfile(python=python)\n            with create_spinner(\"Importing requirements...\") as sp:\n                # Import requirements.txt.\n                try:\n                    import_requirements()\n                except Exception:\n                    sp.fail(environments.PIPENV_SPINNER_FAIL_TEXT.format(\"Failed...\"))\n                else:\n                    sp.ok(environments.PIPENV_SPINNER_OK_TEXT.format(\"Success!\"))\n            # Warn the user of side-effects.\n            click.echo(\n                u\"{0}: Your {1} now contains pinned versions, if your {2} did. \\n\"\n                \"We recommend updating your {1} to specify the {3} version, instead.\"\n                \"\".format(\n                    crayons.red(\"Warning\", bold=True),\n                    crayons.normal(\"Pipfile\", bold=True),\n                    crayons.normal(\"requirements.txt\", bold=True),\n                    crayons.normal('\"*\"', bold=True),\n                )\n            )\n        else:\n            click.echo(\n                crayons.normal(fix_utf8(\"Creating a Pipfile for this project\u2026\"), bold=True),\n                err=True,\n            )\n            # Create the pipfile if it doesn't exist.\n            project.create_pipfile(python=python)\n    # Validate the Pipfile's contents.\n    if validate and project.virtualenv_exists and not PIPENV_SKIP_VALIDATION:\n        # Ensure that Pipfile is using proper casing.\n        p = project.parsed_pipfile\n        changed = project.ensure_proper_casing()\n        # Write changes out to disk.\n        if changed:\n            click.echo(\n                crayons.normal(u\"Fixing package names in Pipfile\u2026\", bold=True), err=True\n            )\n            project.write_toml(p)", "response": "Ensures that the Pipfile exists and creates a Pipfile for the project."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds a Python installation from a given line.", "response": "def find_a_system_python(line):\n    \"\"\"Find a Python installation from a given line.\n\n    This tries to parse the line in various of ways:\n\n    * Looks like an absolute path? Use it directly.\n    * Looks like a py.exe call? Use py.exe to get the executable.\n    * Starts with \"py\" something? Looks like a python command. Try to find it\n      in PATH, and use it directly.\n    * Search for \"python\" and \"pythonX.Y\" executables in PATH to find a match.\n    * Nothing fits, return None.\n    \"\"\"\n\n    from .vendor.pythonfinder import Finder\n    finder = Finder(system=False, global_search=True)\n    if not line:\n        return next(iter(finder.find_all_python_versions()), None)\n    # Use the windows finder executable\n    if (line.startswith(\"py \") or line.startswith(\"py.exe \")) and os.name == \"nt\":\n        line = line.split(\" \", 1)[1].lstrip(\"-\")\n    python_entry = find_python(finder, line)\n    return python_entry"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ensure_virtualenv(three=None, python=None, site_packages=False, pypi_mirror=None):\n    from .environments import PIPENV_USE_SYSTEM\n\n    def abort():\n        sys.exit(1)\n\n    global USING_DEFAULT_PYTHON\n    if not project.virtualenv_exists:\n        try:\n            # Ensure environment variables are set properly.\n            ensure_environment()\n            # Ensure Python is available.\n            python = ensure_python(three=three, python=python)\n            if python is not None and not isinstance(python, six.string_types):\n                python = python.path.as_posix()\n            # Create the virtualenv.\n            # Abort if --system (or running in a virtualenv).\n            if PIPENV_USE_SYSTEM:\n                click.echo(\n                    crayons.red(\n                        \"You are attempting to re\u2013create a virtualenv that \"\n                        \"Pipenv did not create. Aborting.\"\n                    )\n                )\n                sys.exit(1)\n            do_create_virtualenv(\n                python=python, site_packages=site_packages, pypi_mirror=pypi_mirror\n            )\n        except KeyboardInterrupt:\n            # If interrupted, cleanup the virtualenv.\n            cleanup_virtualenv(bare=False)\n            sys.exit(1)\n    # If --three, --two, or --python were passed\u2026\n    elif (python) or (three is not None) or (site_packages is not False):\n        USING_DEFAULT_PYTHON = False\n        # Ensure python is installed before deleting existing virtual env\n        python = ensure_python(three=three, python=python)\n        if python is not None and not isinstance(python, six.string_types):\n            python = python.path.as_posix()\n\n        click.echo(crayons.red(\"Virtualenv already exists!\"), err=True)\n        # If VIRTUAL_ENV is set, there is a possibility that we are\n        # going to remove the active virtualenv that the user cares\n        # about, so confirm first.\n        if \"VIRTUAL_ENV\" in os.environ:\n            if not (\n                PIPENV_YES or click.confirm(\"Remove existing virtualenv?\", default=True)\n            ):\n                abort()\n        click.echo(\n            crayons.normal(fix_utf8(\"Removing existing virtualenv\u2026\"), bold=True), err=True\n        )\n        # Remove the virtualenv.\n        cleanup_virtualenv(bare=True)\n        # Call this function again.\n        ensure_virtualenv(\n            three=three,\n            python=python,\n            site_packages=site_packages,\n            pypi_mirror=pypi_mirror,\n        )", "response": "Creates a virtualenv if one does not exist."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ensure_project(\n    three=None,\n    python=None,\n    validate=True,\n    system=False,\n    warn=True,\n    site_packages=False,\n    deploy=False,\n    skip_requirements=False,\n    pypi_mirror=None,\n    clear=False,\n):\n    \"\"\"Ensures both Pipfile and virtualenv exist for the project.\"\"\"\n    from .environments import PIPENV_USE_SYSTEM\n\n    # Clear the caches, if appropriate.\n    if clear:\n        print(\"clearing\")\n        sys.exit(1)\n\n    # Automatically use an activated virtualenv.\n    if PIPENV_USE_SYSTEM:\n        system = True\n    if not project.pipfile_exists and deploy:\n        raise exceptions.PipfileNotFound\n    # Fail if working under /\n    if not project.name:\n        click.echo(\n            \"{0}: Pipenv is not intended to work under the root directory, \"\n            \"please choose another path.\".format(crayons.red(\"ERROR\")),\n            err=True\n        )\n        sys.exit(1)\n    # Skip virtualenv creation when --system was used.\n    if not system:\n        ensure_virtualenv(\n            three=three,\n            python=python,\n            site_packages=site_packages,\n            pypi_mirror=pypi_mirror,\n        )\n        if warn:\n            # Warn users if they are using the wrong version of Python.\n            if project.required_python_version:\n                path_to_python = which(\"python\") or which(\"py\")\n                if path_to_python and project.required_python_version not in (\n                    python_version(path_to_python) or \"\"\n                ):\n                    click.echo(\n                        \"{0}: Your Pipfile requires {1} {2}, \"\n                        \"but you are using {3} ({4}).\".format(\n                            crayons.red(\"Warning\", bold=True),\n                            crayons.normal(\"python_version\", bold=True),\n                            crayons.blue(project.required_python_version),\n                            crayons.blue(python_version(path_to_python)),\n                            crayons.green(shorten_path(path_to_python)),\n                        ),\n                        err=True,\n                    )\n                    click.echo(\n                        \"  {0} and rebuilding the virtual environment \"\n                        \"may resolve the issue.\".format(crayons.green(\"$ pipenv --rm\")),\n                        err=True,\n                    )\n                    if not deploy:\n                        click.echo(\n                            \"  {0} will surely fail.\"\n                            \"\".format(crayons.red(\"$ pipenv check\")),\n                            err=True,\n                        )\n                    else:\n                        raise exceptions.DeployException\n    # Ensure the Pipfile exists.\n    ensure_pipfile(\n        validate=validate, skip_requirements=skip_requirements, system=system\n    )", "response": "Ensures both Pipfile and virtualenv exist for the project."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a visually shorter representation of a given system path.", "response": "def shorten_path(location, bold=False):\n    \"\"\"Returns a visually shorter representation of a given system path.\"\"\"\n    original = location\n    short = os.sep.join(\n        [s[0] if len(s) > (len(\"2long4\")) else s for s in location.split(os.sep)]\n    )\n    short = short.split(os.sep)\n    short[-1] = original.split(os.sep)[-1]\n    if bold:\n        short[-1] = str(crayons.normal(short[-1], bold=True))\n    return os.sep.join(short)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef do_where(virtualenv=False, bare=True):\n    if not virtualenv:\n        if not project.pipfile_exists:\n            click.echo(\n                \"No Pipfile present at project home. Consider running \"\n                \"{0} first to automatically generate a Pipfile for you.\"\n                \"\".format(crayons.green(\"`pipenv install`\")),\n                err=True,\n            )\n            return\n        location = project.pipfile_location\n        # Shorten the virtual display of the path to the virtualenv.\n        if not bare:\n            location = shorten_path(location)\n            click.echo(\n                \"Pipfile found at {0}.\\n  Considering this to be the project home.\"\n                \"\".format(crayons.green(location)),\n                err=True,\n            )\n        else:\n            click.echo(project.project_directory)\n    else:\n        location = project.virtualenv_location\n        if not bare:\n            click.echo(\n                \"Virtualenv location: {0}\".format(crayons.green(location)), err=True\n            )\n        else:\n            click.echo(location)", "response": "Executes the where functionality."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexecutes the install functionality.", "response": "def do_install_dependencies(\n    dev=False,\n    only=False,\n    bare=False,\n    requirements=False,\n    allow_global=False,\n    ignore_hashes=False,\n    skip_lock=False,\n    concurrent=True,\n    requirements_dir=None,\n    pypi_mirror=False,\n):\n    \"\"\"\"\n    Executes the install functionality.\n\n    If requirements is True, simply spits out a requirements format to stdout.\n    \"\"\"\n\n    from six.moves import queue\n    if requirements:\n        bare = True\n    blocking = not concurrent\n    # Load the lockfile if it exists, or if only is being used (e.g. lock is being used).\n    if skip_lock or only or not project.lockfile_exists:\n        if not bare:\n            click.echo(\n                crayons.normal(fix_utf8(\"Installing dependencies from Pipfile\u2026\"), bold=True)\n            )\n            # skip_lock should completely bypass the lockfile (broken in 4dac1676)\n            lockfile = project.get_or_create_lockfile(from_pipfile=True)\n    else:\n        lockfile = project.get_or_create_lockfile()\n        if not bare:\n            click.echo(\n                crayons.normal(\n                    fix_utf8(\"Installing dependencies from Pipfile.lock ({0})\u2026\".format(\n                        lockfile[\"_meta\"].get(\"hash\", {}).get(\"sha256\")[-6:]\n                    )),\n                    bold=True,\n                )\n            )\n    # Allow pip to resolve dependencies when in skip-lock mode.\n    no_deps = not skip_lock\n    deps_list = list(lockfile.get_requirements(dev=dev, only=requirements))\n    if requirements:\n        index_args = prepare_pip_source_args(project.sources)\n        index_args = \" \".join(index_args).replace(\" -\", \"\\n-\")\n        deps = [\n            req.as_line(sources=False, include_hashes=False) for req in deps_list\n        ]\n        # Output only default dependencies\n        click.echo(index_args)\n        click.echo(\n            \"\\n\".join(sorted(deps))\n        )\n        sys.exit(0)\n\n    procs = queue.Queue(maxsize=PIPENV_MAX_SUBPROCESS)\n    failed_deps_queue = queue.Queue()\n    if skip_lock:\n        ignore_hashes = True\n\n    install_kwargs = {\n        \"no_deps\": no_deps, \"ignore_hashes\": ignore_hashes, \"allow_global\": allow_global,\n        \"blocking\": blocking, \"pypi_mirror\": pypi_mirror\n    }\n    if concurrent:\n        install_kwargs[\"nprocs\"] = PIPENV_MAX_SUBPROCESS\n    else:\n        install_kwargs[\"nprocs\"] = 1\n\n    # with project.environment.activated():\n    batch_install(\n        deps_list, procs, failed_deps_queue, requirements_dir, **install_kwargs\n    )\n\n    if not procs.empty():\n        _cleanup_procs(procs, concurrent, failed_deps_queue)\n\n    # Iterate over the hopefully-poorly-packaged dependencies\u2026\n    if not failed_deps_queue.empty():\n        click.echo(\n            crayons.normal(fix_utf8(\"Installing initially failed dependencies\u2026\"), bold=True)\n        )\n        retry_list = []\n        while not failed_deps_queue.empty():\n            failed_dep = failed_deps_queue.get()\n            retry_list.append(failed_dep)\n        install_kwargs.update({\n            \"nprocs\": 1,\n            \"retry\": False,\n            \"blocking\": True,\n        })\n        batch_install(\n            retry_list, procs, failed_deps_queue, requirements_dir, **install_kwargs\n        )\n    if not procs.empty():\n        _cleanup_procs(procs, False, failed_deps_queue, retry=False)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef do_create_virtualenv(python=None, site_packages=False, pypi_mirror=None):\n\n    click.echo(\n        crayons.normal(fix_utf8(\"Creating a virtualenv for this project\u2026\"), bold=True), err=True\n    )\n    click.echo(\n        u\"Pipfile: {0}\".format(crayons.red(project.pipfile_location, bold=True)),\n        err=True,\n    )\n\n    # Default to using sys.executable, if Python wasn't provided.\n    if not python:\n        python = sys.executable\n    click.echo(\n        u\"{0} {1} {3} {2}\".format(\n            crayons.normal(\"Using\", bold=True),\n            crayons.red(python, bold=True),\n            crayons.normal(fix_utf8(\"to create virtualenv\u2026\"), bold=True),\n            crayons.green(\"({0})\".format(python_version(python))),\n        ),\n        err=True,\n    )\n\n    cmd = [\n        vistir.compat.Path(sys.executable).absolute().as_posix(),\n        \"-m\",\n        \"virtualenv\",\n        \"--prompt=({0}) \".format(project.name),\n        \"--python={0}\".format(python),\n        project.get_location_for_virtualenv(),\n    ]\n\n    # Pass site-packages flag to virtualenv, if desired\u2026\n    if site_packages:\n        click.echo(\n            crayons.normal(fix_utf8(\"Making site-packages available\u2026\"), bold=True), err=True\n        )\n        cmd.append(\"--system-site-packages\")\n\n    if pypi_mirror:\n        pip_config = {\"PIP_INDEX_URL\": vistir.misc.fs_str(pypi_mirror)}\n    else:\n        pip_config = {}\n\n    # Actually create the virtualenv.\n    nospin = environments.PIPENV_NOSPIN\n    with create_spinner(\"Creating virtual environment...\") as sp:\n        c = vistir.misc.run(\n            cmd, verbose=False, return_object=True, write_to_stdout=False,\n            combine_stderr=False, block=True, nospin=True, env=pip_config,\n        )\n        click.echo(crayons.blue(\"{0}\".format(c.out)), err=True)\n        if c.returncode != 0:\n            sp.fail(environments.PIPENV_SPINNER_FAIL_TEXT.format(\"Failed creating virtual environment\"))\n            error = c.err if environments.is_verbose() else exceptions.prettify_exc(c.err)\n            raise exceptions.VirtualenvCreationException(\n                extra=[crayons.red(\"{0}\".format(error)),]\n            )\n        else:\n\n            sp.green.ok(environments.PIPENV_SPINNER_OK_TEXT.format(u\"Successfully created virtual environment!\"))\n\n    # Associate project directory with the environment.\n    # This mimics Pew's \"setproject\".\n    project_file_name = os.path.join(project.virtualenv_location, \".project\")\n    with open(project_file_name, \"w\") as f:\n        f.write(vistir.misc.fs_str(project.project_directory))\n    from .environment import Environment\n    sources = project.pipfile_sources\n    project._environment = Environment(\n        prefix=project.get_location_for_virtualenv(),\n        is_venv=True,\n        sources=sources,\n        pipfile=project.parsed_pipfile,\n        project=project\n    )\n    project._environment.add_dist(\"pipenv\")\n    # Say where the virtualenv is.\n    do_where(virtualenv=True, bare=False)", "response": "Create a new virtual environment for this project."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef do_lock(\n    ctx=None,\n    system=False,\n    clear=False,\n    pre=False,\n    keep_outdated=False,\n    write=True,\n    pypi_mirror=None,\n):\n    \"\"\"Executes the freeze functionality.\"\"\"\n\n    cached_lockfile = {}\n    if not pre:\n        pre = project.settings.get(\"allow_prereleases\")\n    if keep_outdated:\n        if not project.lockfile_exists:\n            raise exceptions.PipenvOptionsError(\n                \"--keep-outdated\", ctx=ctx,\n                message=\"Pipfile.lock must exist to use --keep-outdated!\"\n            )\n        cached_lockfile = project.lockfile_content\n    # Create the lockfile.\n    lockfile = project._lockfile\n    # Cleanup lockfile.\n    for section in (\"default\", \"develop\"):\n        for k, v in lockfile[section].copy().items():\n            if not hasattr(v, \"keys\"):\n                del lockfile[section][k]\n    # Ensure that develop inherits from default.\n    dev_packages = project.dev_packages.copy()\n    dev_packages = overwrite_dev(project.packages, dev_packages)\n    # Resolve dev-package dependencies, with pip-tools.\n    for is_dev in [True, False]:\n        pipfile_section = \"dev-packages\" if is_dev else \"packages\"\n        lockfile_section = \"develop\" if is_dev else \"default\"\n        if project.pipfile_exists:\n            packages = project.parsed_pipfile.get(pipfile_section, {})\n        else:\n            packages = getattr(project, pipfile_section.replace(\"-\", \"_\"))\n\n        if write:\n            # Alert the user of progress.\n            click.echo(\n                u\"{0} {1} {2}\".format(\n                    crayons.normal(u\"Locking\"),\n                    crayons.red(u\"[{0}]\".format(pipfile_section.replace(\"_\", \"-\"))),\n                    crayons.normal(fix_utf8(\"dependencies\u2026\")),\n                ),\n                err=True,\n            )\n\n        # Mutates the lockfile\n        venv_resolve_deps(\n            packages,\n            which=which,\n            project=project,\n            dev=is_dev,\n            clear=clear,\n            pre=pre,\n            allow_global=system,\n            pypi_mirror=pypi_mirror,\n            pipfile=packages,\n            lockfile=lockfile,\n            keep_outdated=keep_outdated\n        )\n\n    # Support for --keep-outdated\u2026\n    if keep_outdated:\n        from pipenv.vendor.packaging.utils import canonicalize_name\n        for section_name, section in (\n            (\"default\", project.packages),\n            (\"develop\", project.dev_packages),\n        ):\n            for package_specified in section.keys():\n                if not is_pinned(section[package_specified]):\n                    canonical_name = canonicalize_name(package_specified)\n                    if canonical_name in cached_lockfile[section_name]:\n                        lockfile[section_name][canonical_name] = cached_lockfile[\n                            section_name\n                        ][canonical_name].copy()\n            for key in [\"default\", \"develop\"]:\n                packages = set(cached_lockfile[key].keys())\n                new_lockfile = set(lockfile[key].keys())\n                missing = packages - new_lockfile\n                for missing_pkg in missing:\n                    lockfile[key][missing_pkg] = cached_lockfile[key][missing_pkg].copy()\n    # Overwrite any develop packages with default packages.\n    lockfile[\"develop\"].update(overwrite_dev(lockfile.get(\"default\", {}), lockfile[\"develop\"]))\n    if write:\n        project.write_lockfile(lockfile)\n        click.echo(\n            \"{0}\".format(\n                crayons.normal(\n                    \"Updated Pipfile.lock ({0})!\".format(\n                        lockfile[\"_meta\"].get(\"hash\", {}).get(\"sha256\")[-6:]\n                    ),\n                    bold=True,\n                )\n            ),\n            err=True,\n        )\n    else:\n        return lockfile", "response": "Executes the freeze functionality."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef do_purge(bare=False, downloads=False, allow_global=False):\n\n    if downloads:\n        if not bare:\n            click.echo(crayons.normal(fix_utf8(\"Clearing out downloads directory\u2026\"), bold=True))\n        vistir.path.rmtree(project.download_location)\n        return\n\n    # Remove comments from the output, if any.\n    installed = set([\n        pep423_name(pkg.project_name) for pkg in project.environment.get_installed_packages()\n    ])\n    bad_pkgs = set([pep423_name(pkg) for pkg in BAD_PACKAGES])\n    # Remove setuptools, pip, etc from targets for removal\n    to_remove = installed - bad_pkgs\n\n    # Skip purging if there is no packages which needs to be removed\n    if not to_remove:\n        if not bare:\n            click.echo(\"Found 0 installed package, skip purging.\")\n            click.echo(crayons.green(\"Environment now purged and fresh!\"))\n        return installed\n\n    if not bare:\n        click.echo(\n            fix_utf8(\"Found {0} installed package(s), purging\u2026\".format(len(to_remove)))\n        )\n\n    command = \"{0} uninstall {1} -y\".format(\n        escape_grouped_arguments(which_pip(allow_global=allow_global)),\n        \" \".join(to_remove),\n    )\n    if environments.is_verbose():\n        click.echo(\"$ {0}\".format(command))\n    c = delegator.run(command)\n    if c.return_code != 0:\n        raise exceptions.UninstallError(installed, command, c.out + c.err, c.return_code)\n    if not bare:\n        click.echo(crayons.blue(c.out))\n        click.echo(crayons.green(\"Environment now purged and fresh!\"))\n    return installed", "response": "Executes the purge functionality."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef do_init(\n    dev=False,\n    requirements=False,\n    allow_global=False,\n    ignore_pipfile=False,\n    skip_lock=False,\n    system=False,\n    concurrent=True,\n    deploy=False,\n    pre=False,\n    keep_outdated=False,\n    requirements_dir=None,\n    pypi_mirror=None,\n):\n    \"\"\"Executes the init functionality.\"\"\"\n    from .environments import (\n        PIPENV_VIRTUALENV, PIPENV_DEFAULT_PYTHON_VERSION, PIPENV_PYTHON, PIPENV_USE_SYSTEM\n    )\n    python = None\n    if PIPENV_PYTHON is not None:\n        python = PIPENV_PYTHON\n    elif PIPENV_DEFAULT_PYTHON_VERSION is not None:\n        python = PIPENV_DEFAULT_PYTHON_VERSION\n\n    if not system and not PIPENV_USE_SYSTEM:\n        if not project.virtualenv_exists:\n            try:\n                do_create_virtualenv(python=python, three=None, pypi_mirror=pypi_mirror)\n            except KeyboardInterrupt:\n                cleanup_virtualenv(bare=False)\n                sys.exit(1)\n    # Ensure the Pipfile exists.\n    if not deploy:\n        ensure_pipfile(system=system)\n    if not requirements_dir:\n        requirements_dir = vistir.path.create_tracked_tempdir(\n            suffix=\"-requirements\", prefix=\"pipenv-\"\n        )\n    # Write out the lockfile if it doesn't exist, but not if the Pipfile is being ignored\n    if (project.lockfile_exists and not ignore_pipfile) and not skip_lock:\n        old_hash = project.get_lockfile_hash()\n        new_hash = project.calculate_pipfile_hash()\n        if new_hash != old_hash:\n            if deploy:\n                click.echo(\n                    crayons.red(\n                        \"Your Pipfile.lock ({0}) is out of date. Expected: ({1}).\".format(\n                            old_hash[-6:], new_hash[-6:]\n                        )\n                    )\n                )\n                raise exceptions.DeployException\n                sys.exit(1)\n            elif (system or allow_global) and not (PIPENV_VIRTUALENV):\n                click.echo(\n                    crayons.red(fix_utf8(\n                        \"Pipfile.lock ({0}) out of date, but installation \"\n                        \"uses {1}\u2026 re-building lockfile must happen in \"\n                        \"isolation. Please rebuild lockfile in a virtualenv. \"\n                        \"Continuing anyway\u2026\".format(\n                            crayons.white(old_hash[-6:]), crayons.white(\"--system\")\n                        )),\n                        bold=True,\n                    ),\n                    err=True,\n                )\n            else:\n                if old_hash:\n                    msg = fix_utf8(\"Pipfile.lock ({0}) out of date, updating to ({1})\u2026\")\n                else:\n                    msg = fix_utf8(\"Pipfile.lock is corrupted, replaced with ({1})\u2026\")\n                click.echo(\n                    crayons.red(msg.format(old_hash[-6:], new_hash[-6:]), bold=True),\n                    err=True,\n                )\n                do_lock(\n                    system=system,\n                    pre=pre,\n                    keep_outdated=keep_outdated,\n                    write=True,\n                    pypi_mirror=pypi_mirror,\n                )\n    # Write out the lockfile if it doesn't exist.\n    if not project.lockfile_exists and not skip_lock:\n        # Unless we're in a virtualenv not managed by pipenv, abort if we're\n        # using the system's python.\n        if (system or allow_global) and not (PIPENV_VIRTUALENV):\n            raise exceptions.PipenvOptionsError(\n                \"--system\",\n                \"--system is intended to be used for Pipfile installation, \"\n                \"not installation of specific packages. Aborting.\\n\"\n                \"See also: --deploy flag.\"\n            )\n        else:\n            click.echo(\n                crayons.normal(fix_utf8(\"Pipfile.lock not found, creating\u2026\"), bold=True),\n                err=True,\n            )\n            do_lock(\n                system=system,\n                pre=pre,\n                keep_outdated=keep_outdated,\n                write=True,\n                pypi_mirror=pypi_mirror,\n            )\n    do_install_dependencies(\n        dev=dev,\n        requirements=requirements,\n        allow_global=allow_global,\n        skip_lock=skip_lock,\n        concurrent=concurrent,\n        requirements_dir=requirements_dir,\n        pypi_mirror=pypi_mirror,\n    )\n\n    # Hint the user what to do to activate the virtualenv.\n    if not allow_global and not deploy and \"PIPENV_ACTIVE\" not in os.environ:\n        click.echo(\n            \"To activate this project's virtualenv, run {0}.\\n\"\n            \"Alternatively, run a command \"\n            \"inside the virtualenv with {1}.\".format(\n                crayons.red(\"pipenv shell\"), crayons.red(\"pipenv run\")\n            )\n        )", "response": "Executes the init functionality."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the location of virtualenv - installed pip.", "response": "def which_pip(allow_global=False):\n    \"\"\"Returns the location of virtualenv-installed pip.\"\"\"\n\n    location = None\n    if \"VIRTUAL_ENV\" in os.environ:\n        location = os.environ[\"VIRTUAL_ENV\"]\n    if allow_global:\n        if location:\n            pip = which(\"pip\", location=location)\n            if pip:\n                return pip\n\n        for p in (\"pip\", \"pip3\", \"pip2\"):\n            where = system_which(p)\n            if where:\n                return where\n\n    pip = which(\"pip\")\n    if not pip:\n        pip = fallback_which(\"pip\", allow_global=allow_global, location=location)\n    return pip"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef system_which(command, mult=False):\n    _which = \"which -a\" if not os.name == \"nt\" else \"where\"\n    os.environ = {\n        vistir.compat.fs_str(k): vistir.compat.fs_str(val)\n        for k, val in os.environ.items()\n    }\n    result = None\n    try:\n        c = delegator.run(\"{0} {1}\".format(_which, command))\n        try:\n            # Which Not found\u2026\n            if c.return_code == 127:\n                click.echo(\n                    \"{}: the {} system utility is required for Pipenv to find Python installations properly.\"\n                    \"\\n  Please install it.\".format(\n                        crayons.red(\"Warning\", bold=True), crayons.red(_which)\n                    ),\n                    err=True,\n                )\n            assert c.return_code == 0\n        except AssertionError:\n            result = fallback_which(command, allow_global=True)\n    except TypeError:\n        if not result:\n            result = fallback_which(command, allow_global=True)\n    else:\n        if not result:\n            result = next(iter([c.out, c.err]), \"\").split(\"\\n\")\n            result = next(iter(result)) if not mult else result\n            return result\n        if not result:\n            result = fallback_which(command, allow_global=True)\n    result = [result] if mult else result\n    return result", "response": "Emulates the system s which. Returns None if not found."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nformat the help string for pipenv", "response": "def format_help(help):\n    \"\"\"Formats the help string.\"\"\"\n    help = help.replace(\"Options:\", str(crayons.normal(\"Options:\", bold=True)))\n    help = help.replace(\n        \"Usage: pipenv\", str(\"Usage: {0}\".format(crayons.normal(\"pipenv\", bold=True)))\n    )\n    help = help.replace(\"  check\", str(crayons.red(\"  check\", bold=True)))\n    help = help.replace(\"  clean\", str(crayons.red(\"  clean\", bold=True)))\n    help = help.replace(\"  graph\", str(crayons.red(\"  graph\", bold=True)))\n    help = help.replace(\"  install\", str(crayons.magenta(\"  install\", bold=True)))\n    help = help.replace(\"  lock\", str(crayons.green(\"  lock\", bold=True)))\n    help = help.replace(\"  open\", str(crayons.red(\"  open\", bold=True)))\n    help = help.replace(\"  run\", str(crayons.yellow(\"  run\", bold=True)))\n    help = help.replace(\"  shell\", str(crayons.yellow(\"  shell\", bold=True)))\n    help = help.replace(\"  sync\", str(crayons.green(\"  sync\", bold=True)))\n    help = help.replace(\"  uninstall\", str(crayons.magenta(\"  uninstall\", bold=True)))\n    help = help.replace(\"  update\", str(crayons.green(\"  update\", bold=True)))\n    additional_help = \"\"\"\nUsage Examples:\n   Create a new project using Python 3.7, specifically:\n   $ {1}\n\n   Remove project virtualenv (inferred from current directory):\n   $ {9}\n\n   Install all dependencies for a project (including dev):\n   $ {2}\n\n   Create a lockfile containing pre-releases:\n   $ {6}\n\n   Show a graph of your installed dependencies:\n   $ {4}\n\n   Check your installed dependencies for security vulnerabilities:\n   $ {7}\n\n   Install a local setup.py into your virtual environment/Pipfile:\n   $ {5}\n\n   Use a lower-level pip command:\n   $ {8}\n\nCommands:\"\"\".format(\n        crayons.red(\"pipenv --three\"),\n        crayons.red(\"pipenv --python 3.7\"),\n        crayons.red(\"pipenv install --dev\"),\n        crayons.red(\"pipenv lock\"),\n        crayons.red(\"pipenv graph\"),\n        crayons.red(\"pipenv install -e .\"),\n        crayons.red(\"pipenv lock --pre\"),\n        crayons.red(\"pipenv check\"),\n        crayons.red(\"pipenv run pip freeze\"),\n        crayons.red(\"pipenv --rm\"),\n    )\n    help = help.replace(\"Commands:\", additional_help)\n    return help"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ensure_lockfile(keep_outdated=False, pypi_mirror=None):\n    if not keep_outdated:\n        keep_outdated = project.settings.get(\"keep_outdated\")\n    # Write out the lockfile if it doesn't exist, but not if the Pipfile is being ignored\n    if project.lockfile_exists:\n        old_hash = project.get_lockfile_hash()\n        new_hash = project.calculate_pipfile_hash()\n        if new_hash != old_hash:\n            click.echo(\n                crayons.red(\n                    fix_utf8(\"Pipfile.lock ({0}) out of date, updating to ({1})\u2026\".format(\n                        old_hash[-6:], new_hash[-6:]\n                    )),\n                    bold=True,\n                ),\n                err=True,\n            )\n            do_lock(keep_outdated=keep_outdated, pypi_mirror=pypi_mirror)\n    else:\n        do_lock(keep_outdated=keep_outdated, pypi_mirror=pypi_mirror)", "response": "Ensures that the lockfile is up - to - date."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _inline_activate_venv():\n    components = []\n    for name in (\"bin\", \"Scripts\"):\n        bindir = os.path.join(project.virtualenv_location, name)\n        if os.path.exists(bindir):\n            components.append(bindir)\n    if \"PATH\" in os.environ:\n        components.append(os.environ[\"PATH\"])\n    os.environ[\"PATH\"] = os.pathsep.join(components)", "response": "Inlined activate_this. py in the current virtualenv."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef do_run(command, args, three=None, python=False, pypi_mirror=None):\n    from .cmdparse import ScriptEmptyError\n\n    # Ensure that virtualenv is available.\n    ensure_project(\n        three=three, python=python, validate=False, pypi_mirror=pypi_mirror,\n    )\n\n    load_dot_env()\n\n    previous_pip_shims_module = os.environ.pop(\"PIP_SHIMS_BASE_MODULE\", None)\n\n    # Activate virtualenv under the current interpreter's environment\n    inline_activate_virtual_environment()\n\n    # Set an environment variable, so we know we're in the environment.\n    # Only set PIPENV_ACTIVE after finishing reading virtualenv_location\n    # such as in inline_activate_virtual_environment\n    # otherwise its value will be changed\n    previous_pipenv_active_value = os.environ.get(\"PIPENV_ACTIVE\")\n    os.environ[\"PIPENV_ACTIVE\"] = vistir.misc.fs_str(\"1\")\n\n    try:\n        script = project.build_script(command, args)\n        cmd_string = ' '.join([script.command] + script.args)\n        if environments.is_verbose():\n            click.echo(crayons.normal(\"$ {0}\".format(cmd_string)), err=True)\n    except ScriptEmptyError:\n        click.echo(\"Can't run script {0!r}-it's empty?\", err=True)\n    run_args = [script]\n    run_kwargs = {}\n    if os.name == \"nt\":\n        run_fn = do_run_nt\n    else:\n        run_fn = do_run_posix\n        run_kwargs = {\"command\": command}\n    try:\n        run_fn(*run_args, **run_kwargs)\n    finally:\n        os.environ.pop(\"PIPENV_ACTIVE\", None)\n        if previous_pipenv_active_value is not None:\n            os.environ[\"PIPENV_ACTIVE\"] = previous_pipenv_active_value\n        if previous_pip_shims_module is not None:\n            os.environ[\"PIP_SHIMS_BASE_MODULE\"] = previous_pip_shims_module", "response": "Run a command in the current virtualenv."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _iter_process():\n    # TODO: Process32{First,Next} does not return full executable path, only\n    # the name. To get the full path, Module32{First,Next} is needed, but that\n    # does not contain parent process information. We probably need to call\n    # BOTH to build the correct process tree.\n    h_process = windll.kernel32.CreateToolhelp32Snapshot(\n        2,  # dwFlags=TH32CS_SNAPPROCESS (include all processes).\n        0,  # th32ProcessID=0 (the current process).\n    )\n    if h_process == INVALID_HANDLE_VALUE:\n        raise WinError()\n    pe = PROCESSENTRY32()\n    pe.dwSize = sizeof(PROCESSENTRY32)\n    success = windll.kernel32.Process32First(h_process, byref(pe))\n    while True:\n        if not success:\n            errcode = windll.kernel32.GetLastError()\n            if errcode == ERROR_NO_MORE_FILES:\n                # No more processes to iterate through, we're done here.\n                return\n            elif errcode == ERROR_INSUFFICIENT_BUFFER:\n                # This is likely because the file path is longer than the\n                # Windows limit. Just ignore it, it's likely not what we're\n                # looking for. We can fix this when it actually matters. (#8)\n                continue\n            raise WinError()\n\n        # The executable name would be encoded with the current code page if\n        # we're in ANSI mode (usually). Try to decode it into str/unicode,\n        # replacing invalid characters to be safe (not thoeratically necessary,\n        # I think). Note that we need to use 'mbcs' instead of encoding\n        # settings from sys because this is from the Windows API, not Python\n        # internals (which those settings reflect). (pypa/pipenv#3382)\n        executable = pe.szExeFile\n        if isinstance(executable, bytes):\n            executable = executable.decode('mbcs', 'replace')\n\n        info = {'executable': executable}\n        if pe.th32ParentProcessID:\n            info['parent_pid'] = pe.th32ParentProcessID\n        yield pe.th32ProcessID, info\n        success = windll.kernel32.Process32Next(h_process, byref(pe))", "response": "Iterate through processes yielding process ID and properties of each."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the shell that the supplied pid or os. getpid() is running in.", "response": "def get_shell(pid=None, max_depth=6):\n    \"\"\"Get the shell that the supplied pid or os.getpid() is running in.\n    \"\"\"\n    if not pid:\n        pid = os.getpid()\n    processes = dict(_iter_process())\n\n    def check_parent(pid, lvl=0):\n        ppid = processes[pid].get('parent_pid')\n        shell_name = _get_executable(processes.get(ppid))\n        if shell_name in SHELL_NAMES:\n            return (shell_name, processes[ppid]['executable'])\n        if lvl >= max_depth:\n            return None\n        return check_parent(ppid, lvl=lvl + 1)\n\n    shell_name = _get_executable(processes.get(pid))\n    if shell_name in SHELL_NAMES:\n        return (shell_name, processes[pid]['executable'])\n    try:\n        return check_parent(pid)\n    except KeyError:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef auto_decode(data):\n    # type: (bytes) -> Text\n    \"\"\"Check a bytes string for a BOM to correctly detect the encoding\n\n    Fallback to locale.getpreferredencoding(False) like open() on Python3\"\"\"\n    for bom, encoding in BOMS:\n        if data.startswith(bom):\n            return data[len(bom):].decode(encoding)\n    # Lets check the first two lines as in PEP263\n    for line in data.split(b'\\n')[:2]:\n        if line[0:1] == b'#' and ENCODING_RE.search(line):\n            encoding = ENCODING_RE.search(line).groups()[0].decode('ascii')\n            return data.decode(encoding)\n    return data.decode(\n        locale.getpreferredencoding(False) or sys.getdefaultencoding(),\n    )", "response": "Try to auto - decode the bytes string for a BOM to correctly detect the encoding\nTaxonomy"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef resolve_color_default(color=None):\n    if color is not None:\n        return color\n    ctx = get_current_context(silent=True)\n    if ctx is not None:\n        return ctx.color", "response": "Internal helper to get the default value of the color flag."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a file or list of files as toml and returns a dictionary of the class of the current object.", "response": "def load(f, _dict=dict, decoder=None):\n    \"\"\"Parses named file or files as toml and returns a dictionary\n\n    Args:\n        f: Path to the file to open, array of files to read into single dict\n           or a file descriptor\n        _dict: (optional) Specifies the class of the returned toml dictionary\n\n    Returns:\n        Parsed toml file represented as a dictionary\n\n    Raises:\n        TypeError -- When f is invalid type\n        TomlDecodeError: Error while decoding toml\n        IOError / FileNotFoundError -- When an array with no valid (existing)\n        (Python 2 / Python 3)          file paths is passed\n    \"\"\"\n\n    if _ispath(f):\n        with io.open(_getpath(f), encoding='utf-8') as ffile:\n            return loads(ffile.read(), _dict, decoder)\n    elif isinstance(f, list):\n        from os import path as op\n        from warnings import warn\n        if not [path for path in f if op.exists(path)]:\n            error_msg = \"Load expects a list to contain filenames only.\"\n            error_msg += linesep\n            error_msg += (\"The list needs to contain the path of at least one \"\n                          \"existing file.\")\n            raise FNFError(error_msg)\n        if decoder is None:\n            decoder = TomlDecoder()\n        d = decoder.get_empty_table()\n        for l in f:\n            if op.exists(l):\n                d.update(load(l, _dict, decoder))\n            else:\n                warn(\"Non-existent filename in list with at least one valid \"\n                     \"filename\")\n        return d\n    else:\n        try:\n            return loads(f.read(), _dict, decoder)\n        except AttributeError:\n            raise TypeError(\"You can only load a file descriptor, filename or \"\n                            \"list\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing a string as toml into a dictionary of the class of the current object.", "response": "def loads(s, _dict=dict, decoder=None):\n    \"\"\"Parses string as toml\n\n    Args:\n        s: String to be parsed\n        _dict: (optional) Specifies the class of the returned toml dictionary\n\n    Returns:\n        Parsed toml file represented as a dictionary\n\n    Raises:\n        TypeError: When a non-string is passed\n        TomlDecodeError: Error while decoding toml\n    \"\"\"\n\n    implicitgroups = []\n    if decoder is None:\n        decoder = TomlDecoder(_dict)\n    retval = decoder.get_empty_table()\n    currentlevel = retval\n    if not isinstance(s, basestring):\n        raise TypeError(\"Expecting something like a string\")\n\n    if not isinstance(s, unicode):\n        s = s.decode('utf8')\n\n    original = s\n    sl = list(s)\n    openarr = 0\n    openstring = False\n    openstrchar = \"\"\n    multilinestr = False\n    arrayoftables = False\n    beginline = True\n    keygroup = False\n    dottedkey = False\n    keyname = 0\n    for i, item in enumerate(sl):\n        if item == '\\r' and sl[i + 1] == '\\n':\n            sl[i] = ' '\n            continue\n        if keyname:\n            if item == '\\n':\n                raise TomlDecodeError(\"Key name found without value.\"\n                                      \" Reached end of line.\", original, i)\n            if openstring:\n                if item == openstrchar:\n                    keyname = 2\n                    openstring = False\n                    openstrchar = \"\"\n                continue\n            elif keyname == 1:\n                if item.isspace():\n                    keyname = 2\n                    continue\n                elif item == '.':\n                    dottedkey = True\n                    continue\n                elif item.isalnum() or item == '_' or item == '-':\n                    continue\n                elif (dottedkey and sl[i - 1] == '.' and\n                      (item == '\"' or item == \"'\")):\n                    openstring = True\n                    openstrchar = item\n                    continue\n            elif keyname == 2:\n                if item.isspace():\n                    if dottedkey:\n                        nextitem = sl[i + 1]\n                        if not nextitem.isspace() and nextitem != '.':\n                            keyname = 1\n                    continue\n                if item == '.':\n                    dottedkey = True\n                    nextitem = sl[i + 1]\n                    if not nextitem.isspace() and nextitem != '.':\n                        keyname = 1\n                    continue\n            if item == '=':\n                keyname = 0\n                dottedkey = False\n            else:\n                raise TomlDecodeError(\"Found invalid character in key name: '\" +\n                                      item + \"'. Try quoting the key name.\",\n                                      original, i)\n        if item == \"'\" and openstrchar != '\"':\n            k = 1\n            try:\n                while sl[i - k] == \"'\":\n                    k += 1\n                    if k == 3:\n                        break\n            except IndexError:\n                pass\n            if k == 3:\n                multilinestr = not multilinestr\n                openstring = multilinestr\n            else:\n                openstring = not openstring\n            if openstring:\n                openstrchar = \"'\"\n            else:\n                openstrchar = \"\"\n        if item == '\"' and openstrchar != \"'\":\n            oddbackslash = False\n            k = 1\n            tripquote = False\n            try:\n                while sl[i - k] == '\"':\n                    k += 1\n                    if k == 3:\n                        tripquote = True\n                        break\n                if k == 1 or (k == 3 and tripquote):\n                    while sl[i - k] == '\\\\':\n                        oddbackslash = not oddbackslash\n                        k += 1\n            except IndexError:\n                pass\n            if not oddbackslash:\n                if tripquote:\n                    multilinestr = not multilinestr\n                    openstring = multilinestr\n                else:\n                    openstring = not openstring\n            if openstring:\n                openstrchar = '\"'\n            else:\n                openstrchar = \"\"\n        if item == '#' and (not openstring and not keygroup and\n                            not arrayoftables):\n            j = i\n            try:\n                while sl[j] != '\\n':\n                    sl[j] = ' '\n                    j += 1\n            except IndexError:\n                break\n        if item == '[' and (not openstring and not keygroup and\n                            not arrayoftables):\n            if beginline:\n                if len(sl) > i + 1 and sl[i + 1] == '[':\n                    arrayoftables = True\n                else:\n                    keygroup = True\n            else:\n                openarr += 1\n        if item == ']' and not openstring:\n            if keygroup:\n                keygroup = False\n            elif arrayoftables:\n                if sl[i - 1] == ']':\n                    arrayoftables = False\n            else:\n                openarr -= 1\n        if item == '\\n':\n            if openstring or multilinestr:\n                if not multilinestr:\n                    raise TomlDecodeError(\"Unbalanced quotes\", original, i)\n                if ((sl[i - 1] == \"'\" or sl[i - 1] == '\"') and (\n                        sl[i - 2] == sl[i - 1])):\n                    sl[i] = sl[i - 1]\n                    if sl[i - 3] == sl[i - 1]:\n                        sl[i - 3] = ' '\n            elif openarr:\n                sl[i] = ' '\n            else:\n                beginline = True\n        elif beginline and sl[i] != ' ' and sl[i] != '\\t':\n            beginline = False\n            if not keygroup and not arrayoftables:\n                if sl[i] == '=':\n                    raise TomlDecodeError(\"Found empty keyname. \", original, i)\n                keyname = 1\n    s = ''.join(sl)\n    s = s.split('\\n')\n    multikey = None\n    multilinestr = \"\"\n    multibackslash = False\n    pos = 0\n    for idx, line in enumerate(s):\n        if idx > 0:\n            pos += len(s[idx - 1]) + 1\n        if not multilinestr or multibackslash or '\\n' not in multilinestr:\n            line = line.strip()\n        if line == \"\" and (not multikey or multibackslash):\n            continue\n        if multikey:\n            if multibackslash:\n                multilinestr += line\n            else:\n                multilinestr += line\n            multibackslash = False\n            if len(line) > 2 and (line[-1] == multilinestr[0] and\n                                  line[-2] == multilinestr[0] and\n                                  line[-3] == multilinestr[0]):\n                try:\n                    value, vtype = decoder.load_value(multilinestr)\n                except ValueError as err:\n                    raise TomlDecodeError(str(err), original, pos)\n                currentlevel[multikey] = value\n                multikey = None\n                multilinestr = \"\"\n            else:\n                k = len(multilinestr) - 1\n                while k > -1 and multilinestr[k] == '\\\\':\n                    multibackslash = not multibackslash\n                    k -= 1\n                if multibackslash:\n                    multilinestr = multilinestr[:-1]\n                else:\n                    multilinestr += \"\\n\"\n            continue\n        if line[0] == '[':\n            arrayoftables = False\n            if len(line) == 1:\n                raise TomlDecodeError(\"Opening key group bracket on line by \"\n                                      \"itself.\", original, pos)\n            if line[1] == '[':\n                arrayoftables = True\n                line = line[2:]\n                splitstr = ']]'\n            else:\n                line = line[1:]\n                splitstr = ']'\n            i = 1\n            quotesplits = decoder._get_split_on_quotes(line)\n            quoted = False\n            for quotesplit in quotesplits:\n                if not quoted and splitstr in quotesplit:\n                    break\n                i += quotesplit.count(splitstr)\n                quoted = not quoted\n            line = line.split(splitstr, i)\n            if len(line) < i + 1 or line[-1].strip() != \"\":\n                raise TomlDecodeError(\"Key group not on a line by itself.\",\n                                      original, pos)\n            groups = splitstr.join(line[:-1]).split('.')\n            i = 0\n            while i < len(groups):\n                groups[i] = groups[i].strip()\n                if len(groups[i]) > 0 and (groups[i][0] == '\"' or\n                                           groups[i][0] == \"'\"):\n                    groupstr = groups[i]\n                    j = i + 1\n                    while not groupstr[0] == groupstr[-1]:\n                        j += 1\n                        if j > len(groups) + 2:\n                            raise TomlDecodeError(\"Invalid group name '\" +\n                                                  groupstr + \"' Something \" +\n                                                  \"went wrong.\", original, pos)\n                        groupstr = '.'.join(groups[i:j]).strip()\n                    groups[i] = groupstr[1:-1]\n                    groups[i + 1:j] = []\n                else:\n                    if not _groupname_re.match(groups[i]):\n                        raise TomlDecodeError(\"Invalid group name '\" +\n                                              groups[i] + \"'. Try quoting it.\",\n                                              original, pos)\n                i += 1\n            currentlevel = retval\n            for i in _range(len(groups)):\n                group = groups[i]\n                if group == \"\":\n                    raise TomlDecodeError(\"Can't have a keygroup with an empty \"\n                                          \"name\", original, pos)\n                try:\n                    currentlevel[group]\n                    if i == len(groups) - 1:\n                        if group in implicitgroups:\n                            implicitgroups.remove(group)\n                            if arrayoftables:\n                                raise TomlDecodeError(\"An implicitly defined \"\n                                                      \"table can't be an array\",\n                                                      original, pos)\n                        elif arrayoftables:\n                            currentlevel[group].append(decoder.get_empty_table()\n                                                       )\n                        else:\n                            raise TomlDecodeError(\"What? \" + group +\n                                                  \" already exists?\" +\n                                                  str(currentlevel),\n                                                  original, pos)\n                except TypeError:\n                    currentlevel = currentlevel[-1]\n                    if group not in currentlevel:\n                        currentlevel[group] = decoder.get_empty_table()\n                        if i == len(groups) - 1 and arrayoftables:\n                            currentlevel[group] = [decoder.get_empty_table()]\n                except KeyError:\n                    if i != len(groups) - 1:\n                        implicitgroups.append(group)\n                    currentlevel[group] = decoder.get_empty_table()\n                    if i == len(groups) - 1 and arrayoftables:\n                        currentlevel[group] = [decoder.get_empty_table()]\n                currentlevel = currentlevel[group]\n                if arrayoftables:\n                    try:\n                        currentlevel = currentlevel[-1]\n                    except KeyError:\n                        pass\n        elif line[0] == \"{\":\n            if line[-1] != \"}\":\n                raise TomlDecodeError(\"Line breaks are not allowed in inline\"\n                                      \"objects\", original, pos)\n            try:\n                decoder.load_inline_object(line, currentlevel, multikey,\n                                           multibackslash)\n            except ValueError as err:\n                raise TomlDecodeError(str(err), original, pos)\n        elif \"=\" in line:\n            try:\n                ret = decoder.load_line(line, currentlevel, multikey,\n                                        multibackslash)\n            except ValueError as err:\n                raise TomlDecodeError(str(err), original, pos)\n            if ret is not None:\n                multikey, multilinestr, multibackslash = ret\n    return retval"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngiving a logger object this returns a new undefined class that will log certain failures and printing.", "response": "def make_logging_undefined(logger=None, base=None):\n    \"\"\"Given a logger object this returns a new undefined class that will\n    log certain failures.  It will log iterations and printing.  If no\n    logger is given a default logger is created.\n\n    Example::\n\n        logger = logging.getLogger(__name__)\n        LoggingUndefined = make_logging_undefined(\n            logger=logger,\n            base=Undefined\n        )\n\n    .. versionadded:: 2.8\n\n    :param logger: the logger to use.  If not provided, a default logger\n                   is created.\n    :param base: the base class to add logging functionality to.  This\n                 defaults to :class:`Undefined`.\n    \"\"\"\n    if logger is None:\n        import logging\n        logger = logging.getLogger(__name__)\n        logger.addHandler(logging.StreamHandler(sys.stderr))\n    if base is None:\n        base = Undefined\n\n    def _log_message(undef):\n        if undef._undefined_hint is None:\n            if undef._undefined_obj is missing:\n                hint = '%s is undefined' % undef._undefined_name\n            elif not isinstance(undef._undefined_name, string_types):\n                hint = '%s has no element %s' % (\n                    object_type_repr(undef._undefined_obj),\n                    undef._undefined_name)\n            else:\n                hint = '%s has no attribute %s' % (\n                    object_type_repr(undef._undefined_obj),\n                    undef._undefined_name)\n        else:\n            hint = undef._undefined_hint\n        logger.warning('Template variable warning: %s', hint)\n\n    class LoggingUndefined(base):\n\n        def _fail_with_undefined_error(self, *args, **kwargs):\n            try:\n                return base._fail_with_undefined_error(self, *args, **kwargs)\n            except self._undefined_exception as e:\n                logger.error('Template variable error: %s', str(e))\n                raise e\n\n        def __str__(self):\n            rv = base.__str__(self)\n            _log_message(self)\n            return rv\n\n        def __iter__(self):\n            rv = base.__iter__(self)\n            _log_message(self)\n            return rv\n\n        if PY2:\n            def __nonzero__(self):\n                rv = base.__nonzero__(self)\n                _log_message(self)\n                return rv\n\n            def __unicode__(self):\n                rv = base.__unicode__(self)\n                _log_message(self)\n                return rv\n        else:\n            def __bool__(self):\n                rv = base.__bool__(self)\n                _log_message(self)\n                return rv\n\n    return LoggingUndefined"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrendering a parent block.", "response": "def super(self, name, current):\n        \"\"\"Render a parent block.\"\"\"\n        try:\n            blocks = self.blocks[name]\n            index = blocks.index(current) + 1\n            blocks[index]\n        except LookupError:\n            return self.environment.undefined('there is no parent block '\n                                              'called %r.' % name,\n                                              name='super')\n        return BlockReference(name, self, blocks, index)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef resolve(self, key):\n        if self._legacy_resolve_mode:\n            rv = resolve_or_missing(self, key)\n        else:\n            rv = self.resolve_or_missing(key)\n        if rv is missing:\n            return self.environment.undefined(name=key)\n        return rv", "response": "Looks up a variable like __getitem__ or get but returns an analyse."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nresolve a variable like resolve but returns the special missing value if it cannot be found.", "response": "def resolve_or_missing(self, key):\n        \"\"\"Resolves a variable like :meth:`resolve` but returns the\n        special `missing` value if it cannot be found.\n        \"\"\"\n        if self._legacy_resolve_mode:\n            rv = self.resolve(key)\n            if isinstance(rv, Undefined):\n                rv = missing\n            return rv\n        return resolve_or_missing(self, key)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a new dict with the exported variables.", "response": "def get_exported(self):\n        \"\"\"Get a new dict with the exported variables.\"\"\"\n        return dict((k, self.vars[k]) for k in self.exported_vars)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_all(self):\n        if not self.vars:\n            return self.parent\n        if not self.parent:\n            return self.vars\n        return dict(self.parent, **self.vars)", "response": "Return the complete context as dict including the exported\n        variables."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the super block.", "response": "def super(self):\n        \"\"\"Super the block.\"\"\"\n        if self._depth + 1 >= len(self._stack):\n            return self._context.environment. \\\n                undefined('there is no parent block called %r.' %\n                          self.name, name='super')\n        return BlockReference(self.name, self._context, self._stack,\n                              self._depth + 1)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cycle(self, *args):\n        if not args:\n            raise TypeError('no items for cycling given')\n        return args[self.index0 % len(args)]", "response": "Cycles among the arguments with the current loop index."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck whether the value has changed since the last call.", "response": "def changed(self, *value):\n        \"\"\"Checks whether the value has changed since the last call.\"\"\"\n        if self._last_checked_value != value:\n            self._last_checked_value = value\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef main():\n\n    '''This is where the example starts and the FSM state transitions are\n    defined. Note that states are strings (such as 'INIT'). This is not\n    necessary, but it makes the example easier to read. '''\n\n    f = FSM ('INIT', [])\n    f.set_default_transition (Error, 'INIT')\n    f.add_transition_any  ('INIT', None, 'INIT')\n    f.add_transition      ('=',               'INIT',            DoEqual,          'INIT')\n    f.add_transition_list (string.digits,     'INIT',            BeginBuildNumber, 'BUILDING_NUMBER')\n    f.add_transition_list (string.digits,     'BUILDING_NUMBER', BuildNumber,      'BUILDING_NUMBER')\n    f.add_transition_list (string.whitespace, 'BUILDING_NUMBER', EndBuildNumber,   'INIT')\n    f.add_transition_list ('+-*/',            'INIT',            DoOperator,       'INIT')\n\n    print()\n    print('Enter an RPN Expression.')\n    print('Numbers may be integers. Operators are * / + -')\n    print('Use the = sign to evaluate and print the expression.')\n    print('For example: ')\n    print('    167 3 2 2 * * * 1 - =')\n    inputstr = (input if PY3 else raw_input)('> ')  # analysis:ignore\n    f.process_list(inputstr)", "response": "This is the example starts and the FSM state transitions are not\n   . It is where the example starts and the FSM state transitions are not\n    defined."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_transition_list (self, list_input_symbols, state, action=None, next_state=None):\n\n        '''This adds the same transition for a list of input symbols.\n        You can pass a list or a string. Note that it is handy to use\n        string.digits, string.whitespace, string.letters, etc. to add\n        transitions that match character classes.\n\n        The action may be set to None in which case the process() method will\n        ignore the action and only set the next_state. The next_state may be\n        set to None in which case the current state will be unchanged. '''\n\n        if next_state is None:\n            next_state = state\n        for input_symbol in list_input_symbols:\n            self.add_transition (input_symbol, state, action, next_state)", "response": "This method adds the same transition for a list of input symbols."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef process (self, input_symbol):\n\n        '''This is the main method that you call to process input. This may\n        cause the FSM to change state and call an action. This method calls\n        get_transition() to find the action and next_state associated with the\n        input_symbol and current_state. If the action is None then the action\n        is not called and only the current state is changed. This method\n        processes one complete input symbol. You can process a list of symbols\n        (or a string) by calling process_list(). '''\n\n        self.input_symbol = input_symbol\n        (self.action, self.next_state) = self.get_transition (self.input_symbol, self.current_state)\n        if self.action is not None:\n            self.action (self)\n        self.current_state = self.next_state\n        self.next_state = None", "response": "This method is called by the FSM when input symbol is processed. This method is called by the FSM when input symbol is processed."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntakes an IP string or integer and return an object of the correct type.", "response": "def ip_address(address):\n    \"\"\"Take an IP string/int and return an object of the correct type.\n\n    Args:\n        address: A string or integer, the IP address.  Either IPv4 or\n          IPv6 addresses may be supplied; integers less than 2**32 will\n          be considered to be IPv4 by default.\n\n    Returns:\n        An IPv4Address or IPv6Address object.\n\n    Raises:\n        ValueError: if the *address* passed isn't either a v4 or a v6\n          address\n\n    \"\"\"\n    try:\n        return IPv4Address(address)\n    except (AddressValueError, NetmaskValueError):\n        pass\n\n    try:\n        return IPv6Address(address)\n    except (AddressValueError, NetmaskValueError):\n        pass\n\n    if isinstance(address, bytes):\n        raise AddressValueError(\n            '%r does not appear to be an IPv4 or IPv6 address. '\n            'Did you pass in a bytes (str in Python 2) instead of'\n            ' a unicode object?' % address)\n\n    raise ValueError('%r does not appear to be an IPv4 or IPv6 address' %\n                     address)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntaking an IP string or integer and return an object of the correct type.", "response": "def ip_interface(address):\n    \"\"\"Take an IP string/int and return an object of the correct type.\n\n    Args:\n        address: A string or integer, the IP address.  Either IPv4 or\n          IPv6 addresses may be supplied; integers less than 2**32 will\n          be considered to be IPv4 by default.\n\n    Returns:\n        An IPv4Interface or IPv6Interface object.\n\n    Raises:\n        ValueError: if the string passed isn't either a v4 or a v6\n          address.\n\n    Notes:\n        The IPv?Interface classes describe an Address on a particular\n        Network, so they're basically a combination of both the Address\n        and Network classes.\n\n    \"\"\"\n    try:\n        return IPv4Interface(address)\n    except (AddressValueError, NetmaskValueError):\n        pass\n\n    try:\n        return IPv6Interface(address)\n    except (AddressValueError, NetmaskValueError):\n        pass\n\n    raise ValueError('%r does not appear to be an IPv4 or IPv6 interface' %\n                     address)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _split_optional_netmask(address):\n    addr = _compat_str(address).split('/')\n    if len(addr) > 2:\n        raise AddressValueError(\"Only one '/' permitted in %r\" % address)\n    return addr", "response": "Helper to split the netmask and raise AddressValueError if needed"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind a sequence of sorted deduplicated IPv#Address objects.", "response": "def _find_address_range(addresses):\n    \"\"\"Find a sequence of sorted deduplicated IPv#Address.\n\n    Args:\n        addresses: a list of IPv#Address objects.\n\n    Yields:\n        A tuple containing the first and last IP addresses in the sequence.\n\n    \"\"\"\n    it = iter(addresses)\n    first = last = next(it)\n    for ip in it:\n        if ip._ip != last._ip + 1:\n            yield first, last\n            first = ip\n        last = ip\n    yield first, last"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _count_righthand_zero_bits(number, bits):\n    if number == 0:\n        return bits\n    return min(bits, _compat_bit_length(~number & (number - 1)))", "response": "Count the number of zero bits on the right hand side of the number."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _collapse_addresses_internal(addresses):\n    # First merge\n    to_merge = list(addresses)\n    subnets = {}\n    while to_merge:\n        net = to_merge.pop()\n        supernet = net.supernet()\n        existing = subnets.get(supernet)\n        if existing is None:\n            subnets[supernet] = net\n        elif existing != net:\n            # Merge consecutive subnets\n            del subnets[supernet]\n            to_merge.append(supernet)\n    # Then iterate over resulting networks, skipping subsumed subnets\n    last = None\n    for net in sorted(subnets.values()):\n        if last is not None:\n            # Since they are sorted,\n            # last.network_address <= net.network_address is a given.\n            if last.broadcast_address >= net.broadcast_address:\n                continue\n        yield net\n        last = net", "response": "This function iterates through the addresses and merges them into one single set of netblocks."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncollapse a list of IPv4Network or IPv6Network objects into a single list of IPv4Network or IPv6Network objects.", "response": "def collapse_addresses(addresses):\n    \"\"\"Collapse a list of IP objects.\n\n    Example:\n        collapse_addresses([IPv4Network('192.0.2.0/25'),\n                            IPv4Network('192.0.2.128/25')]) ->\n                           [IPv4Network('192.0.2.0/24')]\n\n    Args:\n        addresses: An iterator of IPv4Network or IPv6Network objects.\n\n    Returns:\n        An iterator of the collapsed IPv(4|6)Network objects.\n\n    Raises:\n        TypeError: If passed a list of mixed version objects.\n\n    \"\"\"\n    addrs = []\n    ips = []\n    nets = []\n\n    # split IP addresses and networks\n    for ip in addresses:\n        if isinstance(ip, _BaseAddress):\n            if ips and ips[-1]._version != ip._version:\n                raise TypeError(\"%s and %s are not of the same version\" % (\n                                ip, ips[-1]))\n            ips.append(ip)\n        elif ip._prefixlen == ip._max_prefixlen:\n            if ips and ips[-1]._version != ip._version:\n                raise TypeError(\"%s and %s are not of the same version\" % (\n                                ip, ips[-1]))\n            try:\n                ips.append(ip.ip)\n            except AttributeError:\n                ips.append(ip.network_address)\n        else:\n            if nets and nets[-1]._version != ip._version:\n                raise TypeError(\"%s and %s are not of the same version\" % (\n                                ip, nets[-1]))\n            nets.append(ip)\n\n    # sort and dedup\n    ips = sorted(set(ips))\n\n    # find consecutive address ranges in the sorted sequence and summarize them\n    if ips:\n        for first, last in _find_address_range(ips):\n            addrs.extend(summarize_address_range(first, last))\n\n    return _collapse_addresses_internal(addrs + nets)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a key suitable for sorting between networks and addresses.", "response": "def get_mixed_type_key(obj):\n    \"\"\"Return a key suitable for sorting between networks and addresses.\n\n    Address and Network objects are not sortable by default; they're\n    fundamentally different so the expression\n\n        IPv4Address('192.0.2.0') <= IPv4Network('192.0.2.0/24')\n\n    doesn't make any sense.  There are some times however, where you may wish\n    to have ipaddress sort these for you anyway. If you need to do this, you\n    can use this function as the key= argument to sorted().\n\n    Args:\n      obj: either a Network or Address object.\n    Returns:\n      appropriate key.\n\n    \"\"\"\n    if isinstance(obj, _BaseNetwork):\n        return obj._get_networks_key()\n    elif isinstance(obj, _BaseAddress):\n        return obj._get_address_key()\n    return NotImplemented"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the prefix length from the bitwise netmask.", "response": "def _prefix_from_ip_int(cls, ip_int):\n        \"\"\"Return prefix length from the bitwise netmask.\n\n        Args:\n            ip_int: An integer, the netmask in expanded bitwise format\n\n        Returns:\n            An integer, the prefix length.\n\n        Raises:\n            ValueError: If the input intermingles zeroes & ones\n        \"\"\"\n        trailing_zeroes = _count_righthand_zero_bits(ip_int,\n                                                     cls._max_prefixlen)\n        prefixlen = cls._max_prefixlen - trailing_zeroes\n        leading_ones = ip_int >> trailing_zeroes\n        all_ones = (1 << prefixlen) - 1\n        if leading_ones != all_ones:\n            byteslen = cls._max_prefixlen // 8\n            details = _compat_to_bytes(ip_int, byteslen, 'big')\n            msg = 'Netmask pattern %r mixes zeroes & ones'\n            raise ValueError(msg % details)\n        return prefixlen"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _prefix_from_prefix_string(cls, prefixlen_str):\n        # int allows a leading +/- as well as surrounding whitespace,\n        # so we ensure that isn't the case\n        if not _BaseV4._DECIMAL_DIGITS.issuperset(prefixlen_str):\n            cls._report_invalid_netmask(prefixlen_str)\n        try:\n            prefixlen = int(prefixlen_str)\n        except ValueError:\n            cls._report_invalid_netmask(prefixlen_str)\n        if not (0 <= prefixlen <= cls._max_prefixlen):\n            cls._report_invalid_netmask(prefixlen_str)\n        return prefixlen", "response": "Convert a numeric string to a prefix length."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nturns a netmask or hostmask string into a prefix length.", "response": "def _prefix_from_ip_string(cls, ip_str):\n        \"\"\"Turn a netmask/hostmask string into a prefix length\n\n        Args:\n            ip_str: The netmask/hostmask to be converted\n\n        Returns:\n            An integer, the prefix length.\n\n        Raises:\n            NetmaskValueError: If the input is not a valid netmask/hostmask\n        \"\"\"\n        # Parse the netmask/hostmask like an IP address.\n        try:\n            ip_int = cls._ip_int_from_string(ip_str)\n        except AddressValueError:\n            cls._report_invalid_netmask(ip_str)\n\n        # Try matching a netmask (this would be /1*0*/ as a bitwise regexp).\n        # Note that the two ambiguous cases (all-ones and all-zeroes) are\n        # treated as netmasks.\n        try:\n            return cls._prefix_from_ip_int(ip_int)\n        except ValueError:\n            pass\n\n        # Invert the bits, and try matching a /0+1+/ hostmask instead.\n        ip_int ^= cls._ALL_ONES\n        try:\n            return cls._prefix_from_ip_int(ip_int)\n        except ValueError:\n            cls._report_invalid_netmask(ip_str)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntelling if self is partly contained in other.", "response": "def overlaps(self, other):\n        \"\"\"Tell if self is partly contained in other.\"\"\"\n        return self.network_address in other or (\n            self.broadcast_address in other or (\n                other.network_address in self or (\n                    other.broadcast_address in self)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef address_exclude(self, other):\n        if not self._version == other._version:\n            raise TypeError(\"%s and %s are not of the same version\" % (\n                            self, other))\n\n        if not isinstance(other, _BaseNetwork):\n            raise TypeError(\"%s is not a network object\" % other)\n\n        if not other.subnet_of(self):\n            raise ValueError('%s not contained in %s' % (other, self))\n        if other == self:\n            return\n\n        # Make sure we're comparing the network of other.\n        other = other.__class__('%s/%s' % (other.network_address,\n                                           other.prefixlen))\n\n        s1, s2 = self.subnets()\n        while s1 != other and s2 != other:\n            if other.subnet_of(s1):\n                yield s2\n                s1, s2 = s1.subnets()\n            elif other.subnet_of(s2):\n                yield s1\n                s1, s2 = s2.subnets()\n            else:\n                # If we got here, there's a bug somewhere.\n                raise AssertionError('Error performing exclusion: '\n                                     's1: %s s2: %s other: %s' %\n                                     (s1, s2, other))\n        if s1 == other:\n            yield s2\n        elif s2 == other:\n            yield s1\n        else:\n            # If we got here, there's a bug somewhere.\n            raise AssertionError('Error performing exclusion: '\n                                 's1: %s s2: %s other: %s' %\n                                 (s1, s2, other))", "response": "Returns an iterator of the IPv4Network objects which are not contained in self and other."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compare_networks(self, other):\n        # does this need to raise a ValueError?\n        if self._version != other._version:\n            raise TypeError('%s and %s are not of the same type' % (\n                            self, other))\n        # self._version == other._version below here:\n        if self.network_address < other.network_address:\n            return -1\n        if self.network_address > other.network_address:\n            return 1\n        # self.network_address == other.network_address below here:\n        if self.netmask < other.netmask:\n            return -1\n        if self.netmask > other.netmask:\n            return 1\n        return 0", "response": "This function compares two IP objects to see if they are the same."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef subnets(self, prefixlen_diff=1, new_prefix=None):\n        if self._prefixlen == self._max_prefixlen:\n            yield self\n            return\n\n        if new_prefix is not None:\n            if new_prefix < self._prefixlen:\n                raise ValueError('new prefix must be longer')\n            if prefixlen_diff != 1:\n                raise ValueError('cannot set prefixlen_diff and new_prefix')\n            prefixlen_diff = new_prefix - self._prefixlen\n\n        if prefixlen_diff < 0:\n            raise ValueError('prefix length diff must be > 0')\n        new_prefixlen = self._prefixlen + prefixlen_diff\n\n        if new_prefixlen > self._max_prefixlen:\n            raise ValueError(\n                'prefix length diff %d is invalid for netblock %s' % (\n                    new_prefixlen, self))\n\n        start = int(self.network_address)\n        end = int(self.broadcast_address) + 1\n        step = (int(self.hostmask) + 1) >> prefixlen_diff\n        for new_addr in _compat_range(start, end, step):\n            current = self.__class__((new_addr, new_prefixlen))\n            yield current", "response": "Returns an iterator over the subnets of the current netblock."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef supernet(self, prefixlen_diff=1, new_prefix=None):\n        if self._prefixlen == 0:\n            return self\n\n        if new_prefix is not None:\n            if new_prefix > self._prefixlen:\n                raise ValueError('new prefix must be shorter')\n            if prefixlen_diff != 1:\n                raise ValueError('cannot set prefixlen_diff and new_prefix')\n            prefixlen_diff = self._prefixlen - new_prefix\n\n        new_prefixlen = self.prefixlen - prefixlen_diff\n        if new_prefixlen < 0:\n            raise ValueError(\n                'current prefixlen is %d, cannot have a prefixlen_diff of %d' %\n                (self.prefixlen, prefixlen_diff))\n        return self.__class__((\n            int(self.network_address) & (int(self.netmask) << prefixlen_diff),\n            new_prefixlen))", "response": "Returns a new IPv4 network object containing the current network."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmake a netmask and prefix_len tuple from the given argument.", "response": "def _make_netmask(cls, arg):\n        \"\"\"Make a (netmask, prefix_len) tuple from the given argument.\n\n        Argument can be:\n        - an integer (the prefix length)\n        - a string representing the prefix length (e.g. \"24\")\n        - a string representing the prefix netmask (e.g. \"255.255.255.0\")\n        \"\"\"\n        if arg not in cls._netmask_cache:\n            if isinstance(arg, _compat_int_types):\n                prefixlen = arg\n            else:\n                try:\n                    # Check for a netmask in prefix length form\n                    prefixlen = cls._prefix_from_prefix_string(arg)\n                except NetmaskValueError:\n                    # Check for a netmask or hostmask in dotted-quad form.\n                    # This may raise NetmaskValueError.\n                    prefixlen = cls._prefix_from_ip_string(arg)\n            netmask = IPv4Address(cls._ip_int_from_prefix(prefixlen))\n            cls._netmask_cache[arg] = netmask, prefixlen\n        return cls._netmask_cache[arg]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nturning the given string into an integer for comparison.", "response": "def _ip_int_from_string(cls, ip_str):\n        \"\"\"Turn the given IP string into an integer for comparison.\n\n        Args:\n            ip_str: A string, the IP ip_str.\n\n        Returns:\n            The IP ip_str as an integer.\n\n        Raises:\n            AddressValueError: if ip_str isn't a valid IPv4 Address.\n\n        \"\"\"\n        if not ip_str:\n            raise AddressValueError('Address cannot be empty')\n\n        octets = ip_str.split('.')\n        if len(octets) != 4:\n            raise AddressValueError(\"Expected 4 octets in %r\" % ip_str)\n\n        try:\n            return _compat_int_from_byte_vals(\n                map(cls._parse_octet, octets), 'big')\n        except ValueError as exc:\n            raise AddressValueError(\"%s in %r\" % (exc, ip_str))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nturn a 32 - bit integer into a dotted decimal notation.", "response": "def _string_from_ip_int(cls, ip_int):\n        \"\"\"Turns a 32-bit integer into dotted decimal notation.\n\n        Args:\n            ip_int: An integer, the IP address.\n\n        Returns:\n            The IP address as a string in dotted decimal notation.\n\n        \"\"\"\n        return '.'.join(_compat_str(struct.unpack(b'!B', b)[0]\n                                    if isinstance(b, bytes)\n                                    else b)\n                        for b in _compat_to_bytes(ip_int, 4, 'big'))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _is_hostmask(self, ip_str):\n        bits = ip_str.split('.')\n        try:\n            parts = [x for x in map(int, bits) if x in self._valid_mask_octets]\n        except ValueError:\n            return False\n        if len(parts) != len(bits):\n            return False\n        if parts[0] < parts[-1]:\n            return True\n        return False", "response": "Test if the string is a hostmask."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_global(self):\n        return (not (self.network_address in IPv4Network('100.64.0.0/10') and\n                self.broadcast_address in IPv4Network('100.64.0.0/10')) and\n                not self.is_private)", "response": "Test if this address is allocated for public networks."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _make_netmask(cls, arg):\n        if arg not in cls._netmask_cache:\n            if isinstance(arg, _compat_int_types):\n                prefixlen = arg\n            else:\n                prefixlen = cls._prefix_from_prefix_string(arg)\n            netmask = IPv6Address(cls._ip_int_from_prefix(prefixlen))\n            cls._netmask_cache[arg] = netmask, prefixlen\n        return cls._netmask_cache[arg]", "response": "Make a netmask tuple from the given argument."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nturns an IPv6 address into an integer.", "response": "def _ip_int_from_string(cls, ip_str):\n        \"\"\"Turn an IPv6 ip_str into an integer.\n\n        Args:\n            ip_str: A string, the IPv6 ip_str.\n\n        Returns:\n            An int, the IPv6 address\n\n        Raises:\n            AddressValueError: if ip_str isn't a valid IPv6 Address.\n\n        \"\"\"\n        if not ip_str:\n            raise AddressValueError('Address cannot be empty')\n\n        parts = ip_str.split(':')\n\n        # An IPv6 address needs at least 2 colons (3 parts).\n        _min_parts = 3\n        if len(parts) < _min_parts:\n            msg = \"At least %d parts expected in %r\" % (_min_parts, ip_str)\n            raise AddressValueError(msg)\n\n        # If the address has an IPv4-style suffix, convert it to hexadecimal.\n        if '.' in parts[-1]:\n            try:\n                ipv4_int = IPv4Address(parts.pop())._ip\n            except AddressValueError as exc:\n                raise AddressValueError(\"%s in %r\" % (exc, ip_str))\n            parts.append('%x' % ((ipv4_int >> 16) & 0xFFFF))\n            parts.append('%x' % (ipv4_int & 0xFFFF))\n\n        # An IPv6 address can't have more than 8 colons (9 parts).\n        # The extra colon comes from using the \"::\" notation for a single\n        # leading or trailing zero part.\n        _max_parts = cls._HEXTET_COUNT + 1\n        if len(parts) > _max_parts:\n            msg = \"At most %d colons permitted in %r\" % (\n                _max_parts - 1, ip_str)\n            raise AddressValueError(msg)\n\n        # Disregarding the endpoints, find '::' with nothing in between.\n        # This indicates that a run of zeroes has been skipped.\n        skip_index = None\n        for i in _compat_range(1, len(parts) - 1):\n            if not parts[i]:\n                if skip_index is not None:\n                    # Can't have more than one '::'\n                    msg = \"At most one '::' permitted in %r\" % ip_str\n                    raise AddressValueError(msg)\n                skip_index = i\n\n        # parts_hi is the number of parts to copy from above/before the '::'\n        # parts_lo is the number of parts to copy from below/after the '::'\n        if skip_index is not None:\n            # If we found a '::', then check if it also covers the endpoints.\n            parts_hi = skip_index\n            parts_lo = len(parts) - skip_index - 1\n            if not parts[0]:\n                parts_hi -= 1\n                if parts_hi:\n                    msg = \"Leading ':' only permitted as part of '::' in %r\"\n                    raise AddressValueError(msg % ip_str)  # ^: requires ^::\n            if not parts[-1]:\n                parts_lo -= 1\n                if parts_lo:\n                    msg = \"Trailing ':' only permitted as part of '::' in %r\"\n                    raise AddressValueError(msg % ip_str)  # :$ requires ::$\n            parts_skipped = cls._HEXTET_COUNT - (parts_hi + parts_lo)\n            if parts_skipped < 1:\n                msg = \"Expected at most %d other parts with '::' in %r\"\n                raise AddressValueError(msg % (cls._HEXTET_COUNT - 1, ip_str))\n        else:\n            # Otherwise, allocate the entire address to parts_hi.  The\n            # endpoints could still be empty, but _parse_hextet() will check\n            # for that.\n            if len(parts) != cls._HEXTET_COUNT:\n                msg = \"Exactly %d parts expected without '::' in %r\"\n                raise AddressValueError(msg % (cls._HEXTET_COUNT, ip_str))\n            if not parts[0]:\n                msg = \"Leading ':' only permitted as part of '::' in %r\"\n                raise AddressValueError(msg % ip_str)  # ^: requires ^::\n            if not parts[-1]:\n                msg = \"Trailing ':' only permitted as part of '::' in %r\"\n                raise AddressValueError(msg % ip_str)  # :$ requires ::$\n            parts_hi = len(parts)\n            parts_lo = 0\n            parts_skipped = 0\n\n        try:\n            # Now, parse the hextets into a 128-bit integer.\n            ip_int = 0\n            for i in range(parts_hi):\n                ip_int <<= 16\n                ip_int |= cls._parse_hextet(parts[i])\n            ip_int <<= 16 * parts_skipped\n            for i in range(-parts_lo, 0):\n                ip_int <<= 16\n                ip_int |= cls._parse_hextet(parts[i])\n            return ip_int\n        except ValueError as exc:\n            raise AddressValueError(\"%s in %r\" % (exc, ip_str))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _parse_hextet(cls, hextet_str):\n        # Whitelist the characters, since int() allows a lot of bizarre stuff.\n        if not cls._HEX_DIGITS.issuperset(hextet_str):\n            raise ValueError(\"Only hex digits permitted in %r\" % hextet_str)\n        # We do the length check second, since the invalid character error\n        # is likely to be more informative for the user\n        if len(hextet_str) > 4:\n            msg = \"At most 4 characters permitted in %r\"\n            raise ValueError(msg % hextet_str)\n        # Length check means we can skip checking the integer value\n        return int(hextet_str, 16)", "response": "Convert an IPv6 hextet string into an integer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompress a list of hextets.", "response": "def _compress_hextets(cls, hextets):\n        \"\"\"Compresses a list of hextets.\n\n        Compresses a list of strings, replacing the longest continuous\n        sequence of \"0\" in the list with \"\" and adding empty strings at\n        the beginning or at the end of the string such that subsequently\n        calling \":\".join(hextets) will produce the compressed version of\n        the IPv6 address.\n\n        Args:\n            hextets: A list of strings, the hextets to compress.\n\n        Returns:\n            A list of strings.\n\n        \"\"\"\n        best_doublecolon_start = -1\n        best_doublecolon_len = 0\n        doublecolon_start = -1\n        doublecolon_len = 0\n        for index, hextet in enumerate(hextets):\n            if hextet == '0':\n                doublecolon_len += 1\n                if doublecolon_start == -1:\n                    # Start of a sequence of zeros.\n                    doublecolon_start = index\n                if doublecolon_len > best_doublecolon_len:\n                    # This is the longest sequence of zeros so far.\n                    best_doublecolon_len = doublecolon_len\n                    best_doublecolon_start = doublecolon_start\n            else:\n                doublecolon_len = 0\n                doublecolon_start = -1\n\n        if best_doublecolon_len > 1:\n            best_doublecolon_end = (best_doublecolon_start +\n                                    best_doublecolon_len)\n            # For zeros at the end of the address.\n            if best_doublecolon_end == len(hextets):\n                hextets += ['']\n            hextets[best_doublecolon_start:best_doublecolon_end] = ['']\n            # For zeros at the beginning of the address.\n            if best_doublecolon_start == 0:\n                hextets = [''] + hextets\n\n        return hextets"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef teredo(self):\n        if (self._ip >> 96) != 0x20010000:\n            return None\n        return (IPv4Address((self._ip >> 64) & 0xFFFFFFFF),\n                IPv4Address(~self._ip & 0xFFFFFFFF))", "response": "Tuple of embedded teredo IPs."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef serialize(input, tree=\"etree\", encoding=None, **serializer_opts):\n    # XXX: Should we cache this?\n    walker = treewalkers.getTreeWalker(tree)\n    s = HTMLSerializer(**serializer_opts)\n    return s.render(walker(input), encoding)", "response": "Serializes the input token stream using the specified treewalker"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef render(self, treewalker, encoding=None):\n        if encoding:\n            return b\"\".join(list(self.serialize(treewalker, encoding)))\n        else:\n            return \"\".join(list(self.serialize(treewalker)))", "response": "Serializes the stream from the treewalker into a string"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the current branch of the current branch.", "response": "def get_current_branch(self, location):\n        \"\"\"\n        Return the current branch, or None if HEAD isn't at a branch\n        (e.g. detached HEAD).\n        \"\"\"\n        # git-symbolic-ref exits with empty stdout if \"HEAD\" is a detached\n        # HEAD rather than a symbolic ref.  In addition, the -q causes the\n        # command to exit with status code 1 instead of 128 in this case\n        # and to suppress the message to stderr.\n        args = ['symbolic-ref', '-q', 'HEAD']\n        output = self.run_command(\n            args, extra_ok_returncodes=(1, ), show_stdout=False, cwd=location,\n        )\n        ref = output.strip()\n\n        if ref.startswith('refs/heads/'):\n            return ref[len('refs/heads/'):]\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the SHA of the revision with the given name.", "response": "def get_revision_sha(self, dest, rev):\n        \"\"\"\n        Return (sha_or_none, is_branch), where sha_or_none is a commit hash\n        if the revision names a remote branch or tag, otherwise None.\n\n        Args:\n          dest: the repository directory.\n          rev: the revision name.\n        \"\"\"\n        # Pass rev to pre-filter the list.\n        output = self.run_command(['show-ref', rev], cwd=dest,\n                                  show_stdout=False, on_returncode='ignore')\n        refs = {}\n        for line in output.strip().splitlines():\n            try:\n                sha, ref = line.split()\n            except ValueError:\n                # Include the offending line to simplify troubleshooting if\n                # this error ever occurs.\n                raise ValueError('unexpected show-ref line: {!r}'.format(line))\n\n            refs[ref] = sha\n\n        branch_ref = 'refs/remotes/origin/{}'.format(rev)\n        tag_ref = 'refs/tags/{}'.format(rev)\n\n        sha = refs.get(branch_ref)\n        if sha is not None:\n            return (sha, True)\n\n        sha = refs.get(tag_ref)\n\n        return (sha, False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef resolve_revision(self, dest, url, rev_options):\n        rev = rev_options.arg_rev\n        sha, is_branch = self.get_revision_sha(dest, rev)\n\n        if sha is not None:\n            rev_options = rev_options.make_new(sha)\n            rev_options.branch_name = rev if is_branch else None\n\n            return rev_options\n\n        # Do not show a warning for the common case of something that has\n        # the form of a Git commit hash.\n        if not looks_like_hash(rev):\n            logger.warning(\n                \"Did not find branch or tag '%s', assuming revision or ref.\",\n                rev,\n            )\n\n        if not rev.startswith('refs/'):\n            return rev_options\n\n        # If it looks like a ref, we have to fetch it explicitly.\n        self.run_command(\n            ['fetch', '-q', url] + rev_options.to_args(),\n            cwd=dest,\n        )\n        # Change the revision to the SHA of the ref we fetched\n        sha = self.get_revision(dest, rev='FETCH_HEAD')\n        rev_options = rev_options.make_new(sha)\n\n        return rev_options", "response": "Resolve a revision to a new RevOptions object with the SHA1 of the branch tag or ref if found."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if the current commit hash equals the given name.", "response": "def is_commit_id_equal(self, dest, name):\n        \"\"\"\n        Return whether the current commit hash equals the given name.\n\n        Args:\n          dest: the repository directory.\n          name: a string name.\n        \"\"\"\n        if not name:\n            # Then avoid an unnecessary subprocess call.\n            return False\n\n        return self.get_revision(dest) == name"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_remote_url(cls, location):\n        # We need to pass 1 for extra_ok_returncodes since the command\n        # exits with return code 1 if there are no matching lines.\n        stdout = cls.run_command(\n            ['config', '--get-regexp', r'remote\\..*\\.url'],\n            extra_ok_returncodes=(1, ), show_stdout=False, cwd=location,\n        )\n        remotes = stdout.splitlines()\n        try:\n            found_remote = remotes[0]\n        except IndexError:\n            raise RemoteNotFoundError\n\n        for remote in remotes:\n            if remote.startswith('remote.origin.url '):\n                found_remote = remote\n                break\n        url = found_remote.split(' ')[1]\n        return url.strip()", "response": "Get the URL of the first remote encountered."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the relative path of setup. py to the git repo root.", "response": "def _get_subdirectory(cls, location):\n        \"\"\"Return the relative path of setup.py to the git repo root.\"\"\"\n        # find the repo root\n        git_dir = cls.run_command(['rev-parse', '--git-dir'],\n                                  show_stdout=False, cwd=location).strip()\n        if not os.path.isabs(git_dir):\n            git_dir = os.path.join(location, git_dir)\n        root_dir = os.path.join(git_dir, '..')\n        # find setup.py\n        orig_location = location\n        while not os.path.exists(os.path.join(location, 'setup.py')):\n            last_location = location\n            location = os.path.dirname(location)\n            if location == last_location:\n                # We've traversed up to the root of the filesystem without\n                # finding setup.py\n                logger.warning(\n                    \"Could not find setup.py for directory %s (tried all \"\n                    \"parent directories)\",\n                    orig_location,\n                )\n                return None\n        # relative path of setup.py to repo root\n        if samefile(root_dir, location):\n            return None\n        return os.path.relpath(location, root_dir)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_url_rev_and_auth(self, url):\n        if '://' not in url:\n            assert 'file:' not in url\n            url = url.replace('git+', 'git+ssh://')\n            url, rev, user_pass = super(Git, self).get_url_rev_and_auth(url)\n            url = url.replace('ssh://', '')\n        else:\n            url, rev, user_pass = super(Git, self).get_url_rev_and_auth(url)\n\n        return url, rev, user_pass", "response": "Returns the url and rev and auth for the given URL."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _normalize_name(name):\n    # type: (str) -> str\n    \"\"\"Make a name consistent regardless of source (environment or file)\n    \"\"\"\n    name = name.lower().replace('_', '-')\n    if name.startswith('--'):\n        name = name[2:]  # only prefer long opts\n    return name", "response": "Make a name consistent regardless of source."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a value from the configuration.", "response": "def get_value(self, key):\n        # type: (str) -> Any\n        \"\"\"Get a value from the configuration.\n        \"\"\"\n        try:\n            return self._dictionary[key]\n        except KeyError:\n            raise ConfigurationError(\"No such key - {}\".format(key))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmodify a value in the configuration.", "response": "def set_value(self, key, value):\n        # type: (str, Any) -> None\n        \"\"\"Modify a value in the configuration.\n        \"\"\"\n        self._ensure_have_load_only()\n\n        fname, parser = self._get_parser_to_modify()\n\n        if parser is not None:\n            section, name = _disassemble_key(key)\n\n            # Modify the parser and the configuration\n            if not parser.has_section(section):\n                parser.add_section(section)\n            parser.set(section, name, value)\n\n        self._config[self.load_only][key] = value\n        self._mark_as_modified(fname, parser)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsaving the currentin - memory state.", "response": "def save(self):\n        # type: () -> None\n        \"\"\"Save the currentin-memory state.\n        \"\"\"\n        self._ensure_have_load_only()\n\n        for fname, parser in self._modified_parsers:\n            logger.info(\"Writing to %s\", fname)\n\n            # Ensure directory exists.\n            ensure_dir(os.path.dirname(fname))\n\n            with open(fname, \"w\") as f:\n                parser.write(f)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _load_config_files(self):\n        # type: () -> None\n        \"\"\"Loads configuration from configuration files\n        \"\"\"\n        config_files = dict(self._iter_config_files())\n        if config_files[kinds.ENV][0:1] == [os.devnull]:\n            logger.debug(\n                \"Skipping loading configuration files due to \"\n                \"environment's PIP_CONFIG_FILE being os.devnull\"\n            )\n            return\n\n        for variant, files in config_files.items():\n            for fname in files:\n                # If there's specific variant set in `load_only`, load only\n                # that variant, not the others.\n                if self.load_only is not None and variant != self.load_only:\n                    logger.debug(\n                        \"Skipping file '%s' (variant: %s)\", fname, variant\n                    )\n                    continue\n\n                parser = self._load_file(variant, fname)\n\n                # Keeping track of the parsers used\n                self._parsers[variant].append((fname, parser))", "response": "Loads configuration from configuration files."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _load_environment_vars(self):\n        # type: () -> None\n        \"\"\"Loads configuration from environment variables\n        \"\"\"\n        self._config[kinds.ENV_VAR].update(\n            self._normalized_keys(\":env:\", self._get_environ_vars())\n        )", "response": "Loads configuration from environment variables."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _normalized_keys(self, section, items):\n        # type: (str, Iterable[Tuple[str, Any]]) -> Dict[str, Any]\n        \"\"\"Normalizes items to construct a dictionary with normalized keys.\n\n        This routine is where the names become keys and are made the same\n        regardless of source - configuration files or environment.\n        \"\"\"\n        normalized = {}\n        for name, val in items:\n            key = section + \".\" + _normalize_name(name)\n            normalized[key] = val\n        return normalized", "response": "Normalizes items to construct a dictionary with normalized keys."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a generator with all environmental vars with prefix PIP_", "response": "def _get_environ_vars(self):\n        # type: () -> Iterable[Tuple[str, str]]\n        \"\"\"Returns a generator with all environmental vars with prefix PIP_\"\"\"\n        for key, val in os.environ.items():\n            should_be_yielded = (\n                key.startswith(\"PIP_\") and\n                key[4:].lower() not in self._ignore_env_names\n            )\n            if should_be_yielded:\n                yield key[4:].lower(), val"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nyield the variant and configuration files associated with a specific locale.", "response": "def _iter_config_files(self):\n        # type: () -> Iterable[Tuple[Kind, List[str]]]\n        \"\"\"Yields variant and configuration files associated with it.\n\n        This should be treated like items of a dictionary.\n        \"\"\"\n        # SMELL: Move the conditions out of this function\n\n        # environment variables have the lowest priority\n        config_file = os.environ.get('PIP_CONFIG_FILE', None)\n        if config_file is not None:\n            yield kinds.ENV, [config_file]\n        else:\n            yield kinds.ENV, []\n\n        # at the base we have any global configuration\n        yield kinds.GLOBAL, list(site_config_files)\n\n        # per-user configuration next\n        should_load_user_config = not self.isolated and not (\n            config_file and os.path.exists(config_file)\n        )\n        if should_load_user_config:\n            # The legacy config file is overridden by the new config file\n            yield kinds.USER, [legacy_config_file, new_config_file]\n\n        # finally virtualenv configuration first trumping others\n        if running_under_virtualenv():\n            yield kinds.VENV, [venv_config_file]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nencode into a cmd - executable string.", "response": "def cmdify(self):\n        \"\"\"Encode into a cmd-executable string.\n\n        This re-implements CreateProcess's quoting logic to turn a list of\n        arguments into one single string for the shell to interpret.\n\n        * All double quotes are escaped with a backslash.\n        * Existing backslashes before a quote are doubled, so they are all\n          escaped properly.\n        * Backslashes elsewhere are left as-is; cmd will interpret them\n          literally.\n\n        The result is then quoted into a pair of double quotes to be grouped.\n\n        An argument is intentionally not quoted if it does not contain\n        foul characters. This is done to be compatible with Windows built-in\n        commands that don't work well with quotes, e.g. everything with `echo`,\n        and DOS-style (forward slash) switches.\n\n        Foul characters include:\n\n        * Whitespaces.\n        * Carets (^). (pypa/pipenv#3307)\n        * Parentheses in the command. (pypa/pipenv#3168)\n\n        Carets introduce a difficult situation since they are essentially\n        \"lossy\" when parsed. Consider this in cmd.exe::\n\n            > echo \"foo^bar\"\n            \"foo^bar\"\n            > echo foo^^bar\n            foo^bar\n\n        The two commands produce different results, but are both parsed by the\n        shell as `foo^bar`, and there's essentially no sensible way to tell\n        what was actually passed in. This implementation assumes the quoted\n        variation (the first) since it is easier to implement, and arguably\n        the more common case.\n\n        The intended use of this function is to pre-process an argument list\n        before passing it into ``subprocess.Popen(..., shell=True)``.\n\n        See also: https://docs.python.org/3/library/subprocess.html#converting-argument-sequence\n        \"\"\"\n        return \" \".join(itertools.chain(\n            [_quote_if_contains(self.command, r'[\\s^()]')],\n            (_quote_if_contains(arg, r'[\\s^]') for arg in self.args),\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a case-normalized absolute variable-expanded path. :param str path: The non-normalized path :return: A normalized, expanded, case-normalized path :rtype: str", "response": "def normalize_path(path):\n    # type: (AnyStr) -> AnyStr\n    \"\"\"\n    Return a case-normalized absolute variable-expanded path.\n\n    :param str path: The non-normalized path\n    :return: A normalized, expanded, case-normalized path\n    :rtype: str\n    \"\"\"\n\n    return os.path.normpath(\n        os.path.normcase(\n            os.path.abspath(os.path.expandvars(os.path.expanduser(str(path))))\n        )\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting the supplied local path to a file uri.", "response": "def path_to_url(path):\n    # type: (str) -> Text\n    \"\"\"Convert the supplied local path to a file uri.\n\n    :param str path: A string pointing to or representing a local path\n    :return: A `file://` uri for the same location\n    :rtype: str\n\n    >>> path_to_url(\"/home/user/code/myrepo/myfile.zip\")\n    'file:///home/user/code/myrepo/myfile.zip'\n    \"\"\"\n    from .misc import to_text, to_bytes\n\n    if not path:\n        return path\n    path = to_bytes(path, encoding=\"utf-8\")\n    normalized_path = to_text(normalize_drive(os.path.abspath(path)), encoding=\"utf-8\")\n    return to_text(Path(normalized_path).as_uri(), encoding=\"utf-8\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a valid file url to a local filesystem path", "response": "def url_to_path(url):\n    # type: (str) -> ByteString\n    \"\"\"\n    Convert a valid file url to a local filesystem path\n\n    Follows logic taken from pip's equivalent function\n    \"\"\"\n    from .misc import to_bytes\n\n    assert is_file_url(url), \"Only file: urls can be converted to local paths\"\n    _, netloc, path, _, _ = urllib_parse.urlsplit(url)\n    # Netlocs are UNC paths\n    if netloc:\n        netloc = \"\\\\\\\\\" + netloc\n\n    path = urllib_request.url2pathname(netloc + path)\n    return to_bytes(path, encoding=\"utf-8\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_valid_url(url):\n    from .misc import to_text\n\n    if not url:\n        return url\n    pieces = urllib_parse.urlparse(to_text(url))\n    return all([pieces.scheme, pieces.netloc])", "response": "Checks if a given string is a valid url"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning true if the given url is a file url", "response": "def is_file_url(url):\n    \"\"\"Returns true if the given url is a file url\"\"\"\n    from .misc import to_text\n\n    if not url:\n        return False\n    if not isinstance(url, six.string_types):\n        try:\n            url = getattr(url, \"url\")\n        except AttributeError:\n            raise ValueError(\"Cannot parse url from unknown type: {0!r}\".format(url))\n    url = to_text(url, encoding=\"utf-8\")\n    return urllib_parse.urlparse(url.lower()).scheme == \"file\""}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_readonly_path(fn):\n\n    fn = fs_encode(fn)\n    if os.path.exists(fn):\n        file_stat = os.stat(fn).st_mode\n        return not bool(file_stat & stat.S_IWRITE) or not os.access(fn, os.W_OK)\n    return False", "response": "Check if a provided path exists and is readonly."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef mkdir_p(newdir, mode=0o777):\n    # http://code.activestate.com/recipes/82465-a-friendly-mkdir/\n\n    newdir = fs_encode(newdir)\n    if os.path.exists(newdir):\n        if not os.path.isdir(newdir):\n            raise OSError(\n                \"a file with the same name as the desired dir, '{0}', already exists.\".format(\n                    fs_decode(newdir)\n                )\n            )\n    else:\n        head, tail = os.path.split(newdir)\n        # Make sure the tail doesn't point to the asame place as the head\n        curdir = fs_encode(\".\")\n        tail_and_head_match = (\n            os.path.relpath(tail, start=os.path.basename(head)) == curdir\n        )\n        if tail and not tail_and_head_match and not os.path.isdir(newdir):\n            target = os.path.join(head, tail)\n            if os.path.exists(target) and os.path.isfile(target):\n                raise OSError(\n                    \"A file with the same name as the desired dir, '{0}', already exists.\".format(\n                        fs_decode(newdir)\n                    )\n                )\n            os.makedirs(os.path.join(head, tail), mode)", "response": "Recursively creates the target directory and all of its parents if they do not already exist. Fails silently if they do not exist."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ensure_mkdir_p(mode=0o777):\n\n    def decorator(f):\n        @functools.wraps(f)\n        def decorated(*args, **kwargs):\n            path = f(*args, **kwargs)\n            mkdir_p(path, mode=mode)\n            return path\n\n        return decorated\n\n    return decorator", "response": "Decorator to ensure mkdir_p is called to the function s return value."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_tracked_tempdir(*args, **kwargs):\n\n    tempdir = TemporaryDirectory(*args, **kwargs)\n    TRACKED_TEMPORARY_DIRECTORIES.append(tempdir)\n    atexit.register(tempdir.cleanup)\n    warnings.simplefilter(\"ignore\", ResourceWarning)\n    return tempdir.name", "response": "Create a tracked temporary directory."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the read - write bit for the current user on the target path. Fail silently Set read - write permissions for the current user on the target path.", "response": "def set_write_bit(fn):\n    # type: (str) -> None\n    \"\"\"\n    Set read-write permissions for the current user on the target path.  Fail silently\n    if the path doesn't exist.\n\n    :param str fn: The target filename or path\n    :return: None\n    \"\"\"\n\n    fn = fs_encode(fn)\n    if not os.path.exists(fn):\n        return\n    file_stat = os.stat(fn).st_mode\n    os.chmod(fn, file_stat | stat.S_IRWXU | stat.S_IRWXG | stat.S_IRWXO)\n    if not os.path.isdir(fn):\n        for path in [fn, os.path.dirname(fn)]:\n            try:\n                os.chflags(path, 0)\n            except AttributeError:\n                pass\n        return None\n    for root, dirs, files in os.walk(fn, topdown=False):\n        for dir_ in [os.path.join(root, d) for d in dirs]:\n            set_write_bit(dir_)\n        for file_ in [os.path.join(root, f) for f in files]:\n            set_write_bit(file_)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretry with backoff up to 1 second to delete files from a directory.", "response": "def _wait_for_files(path):\n    \"\"\"\n    Retry with backoff up to 1 second to delete files from a directory.\n\n    :param str path: The path to crawl to delete files from\n    :return: A list of remaining paths or None\n    :rtype: Optional[List[str]]\n    \"\"\"\n    timeout = 0.001\n    remaining = []\n    while timeout < 1.0:\n        remaining = []\n        if os.path.isdir(path):\n            L = os.listdir(path)\n            for target in L:\n                _remaining = _wait_for_files(target)\n                if _remaining:\n                    remaining.extend(_remaining)\n            continue\n        try:\n            os.unlink(path)\n        except FileNotFoundError as e:\n            if e.errno == errno.ENOENT:\n                return\n        except (OSError, IOError, PermissionError):\n            time.sleep(timeout)\n            timeout *= 2\n            remaining.append(path)\n        else:\n            return\n    return remaining"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef handle_remove_readonly(func, path, exc):\n    # Check for read-only attribute\n    from .compat import ResourceWarning, FileNotFoundError, PermissionError\n\n    PERM_ERRORS = (errno.EACCES, errno.EPERM, errno.ENOENT)\n    default_warning_message = \"Unable to remove file due to permissions restriction: {!r}\"\n    # split the initial exception out into its type, exception, and traceback\n    exc_type, exc_exception, exc_tb = exc\n    if is_readonly_path(path):\n        # Apply write permission and call original function\n        set_write_bit(path)\n        try:\n            func(path)\n        except (OSError, IOError, FileNotFoundError, PermissionError) as e:\n            if e.errno == errno.ENOENT:\n                return\n            elif e.errno in PERM_ERRORS:\n                remaining = None\n                if os.path.isdir(path):\n                    remaining =_wait_for_files(path)\n                if remaining:\n                    warnings.warn(default_warning_message.format(path), ResourceWarning)\n                return\n            raise\n\n    if exc_exception.errno in PERM_ERRORS:\n        set_write_bit(path)\n        remaining = _wait_for_files(path)\n        try:\n            func(path)\n        except (OSError, IOError, FileNotFoundError, PermissionError) as e:\n            if e.errno in PERM_ERRORS:\n                warnings.warn(default_warning_message.format(path), ResourceWarning)\n                pass\n            elif e.errno == errno.ENOENT:  # File already gone\n                pass\n            else:\n                raise\n        else:\n            return\n    elif exc_exception.errno == errno.ENOENT:\n        pass\n    else:\n        raise exc_exception", "response": "This function handles the error handling for shutil. rmtree."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_for_unc_path(path):\n    if (\n        os.name == \"nt\"\n        and len(path.drive) > 2\n        and not path.drive[0].isalpha()\n        and path.drive[1] != \":\"\n    ):\n        return True\n    else:\n        return False", "response": "Checks to see if a pathlib Path object is a unc path or not"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a vague relative path to be relative.", "response": "def get_converted_relative_path(path, relative_to=None):\n    \"\"\"Convert `path` to be relative.\n\n    Given a vague relative path, return the path relative to the given\n    location.\n\n    :param str path: The location of a target path\n    :param str relative_to: The starting path to build against, optional\n    :returns: A relative posix-style path with a leading `./`\n\n    This performs additional conversion to ensure the result is of POSIX form,\n    and starts with `./`, or is precisely `.`.\n\n    >>> os.chdir('/home/user/code/myrepo/myfolder')\n    >>> vistir.path.get_converted_relative_path('/home/user/code/file.zip')\n    './../../file.zip'\n    >>> vistir.path.get_converted_relative_path('/home/user/code/myrepo/myfolder/mysubfolder')\n    './mysubfolder'\n    >>> vistir.path.get_converted_relative_path('/home/user/code/myrepo/myfolder')\n    '.'\n    \"\"\"\n    from .misc import to_text, to_bytes  # noqa\n\n    if not relative_to:\n        relative_to = os.getcwdu() if six.PY2 else os.getcwd()\n    if six.PY2:\n        path = to_bytes(path, encoding=\"utf-8\")\n    else:\n        path = to_text(path, encoding=\"utf-8\")\n    relative_to = to_text(relative_to, encoding=\"utf-8\")\n    start_path = Path(relative_to)\n    try:\n        start = start_path.resolve()\n    except OSError:\n        start = start_path.absolute()\n\n    # check if there is a drive letter or mount point\n    # if it is a mountpoint use the original absolute path\n    # instead of the unc path\n    if check_for_unc_path(start):\n        start = start_path.absolute()\n\n    path = start.joinpath(path).relative_to(start)\n\n    # check and see if the path that was passed into the function is a UNC path\n    # and raise value error if it is not.\n    if check_for_unc_path(path):\n        raise ValueError(\"The path argument does not currently accept UNC paths\")\n\n    relpath_s = to_text(posixpath.normpath(path.as_posix()))\n    if not (relpath_s == \".\" or relpath_s.startswith(\"./\")):\n        relpath_s = posixpath.join(\".\", relpath_s)\n    return relpath_s"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_fp_closed(obj):\n\n    try:\n        # Check `isclosed()` first, in case Python3 doesn't set `closed`.\n        # GH Issue #928\n        return obj.isclosed()\n    except AttributeError:\n        pass\n\n    try:\n        # Check via the official file-like-object way.\n        return obj.closed\n    except AttributeError:\n        pass\n\n    try:\n        # Check if the object is a container for another file-like object that\n        # gets released on exhaustion (e.g. HTTPResponse).\n        return obj.fp is None\n    except AttributeError:\n        pass\n\n    raise ValueError(\"Unable to determine whether fp is closed.\")", "response": "Checks whether a given file - like object is closed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef assert_header_parsing(headers):\n\n    # This will fail silently if we pass in the wrong kind of parameter.\n    # To make debugging easier add an explicit check.\n    if not isinstance(headers, httplib.HTTPMessage):\n        raise TypeError('expected httplib.Message, got {0}.'.format(\n            type(headers)))\n\n    defects = getattr(headers, 'defects', None)\n    get_payload = getattr(headers, 'get_payload', None)\n\n    unparsed_data = None\n    if get_payload:\n        # get_payload is actually email.message.Message.get_payload;\n        # we're only interested in the result if it's not a multipart message\n        if not headers.is_multipart():\n            payload = get_payload()\n\n            if isinstance(payload, (bytes, str)):\n                unparsed_data = payload\n\n    if defects or unparsed_data:\n        raise HeaderParsingError(defects=defects, unparsed_data=unparsed_data)", "response": "Assert that all headers have been successfully parsed."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_response_to_head(response):\n    # FIXME: Can we do this somehow without accessing private httplib _method?\n    method = response._method\n    if isinstance(method, int):  # Platform-specific: Appengine\n        return method == 3\n    return method.upper() == 'HEAD'", "response": "Checks whether the response is to a HEAD - request."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef try_read_prompt(self, timeout_multiplier):\n        '''This facilitates using communication timeouts to perform\n        synchronization as quickly as possible, while supporting high latency\n        connections with a tunable worst case performance. Fast connections\n        should be read almost immediately. Worst case performance for this\n        method is timeout_multiplier * 3 seconds.\n        '''\n\n        # maximum time allowed to read the first response\n        first_char_timeout = timeout_multiplier * 0.5\n\n        # maximum time allowed between subsequent characters\n        inter_char_timeout = timeout_multiplier * 0.1\n\n        # maximum time for reading the entire prompt\n        total_timeout = timeout_multiplier * 3.0\n\n        prompt = self.string_type()\n        begin = time.time()\n        expired = 0.0\n        timeout = first_char_timeout\n\n        while expired < total_timeout:\n            try:\n                prompt += self.read_nonblocking(size=1, timeout=timeout)\n                expired = time.time() - begin # updated total time expired\n                timeout = inter_char_timeout\n            except TIMEOUT:\n                break\n\n        return prompt", "response": "Try to read a prompt from the remote server."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sync_original_prompt (self, sync_multiplier=1.0):\n        '''This attempts to find the prompt. Basically, press enter and record\n        the response; press enter again and record the response; if the two\n        responses are similar then assume we are at the original prompt.\n        This can be a slow function. Worst case with the default sync_multiplier\n        can take 12 seconds. Low latency connections are more likely to fail\n        with a low sync_multiplier. Best case sync time gets worse with a\n        high sync multiplier (500 ms with default). '''\n        \n        # All of these timing pace values are magic.\n        # I came up with these based on what seemed reliable for\n        # connecting to a heavily loaded machine I have.\n        self.sendline()\n        time.sleep(0.1)\n\n        try:\n            # Clear the buffer before getting the prompt.\n            self.try_read_prompt(sync_multiplier)\n        except TIMEOUT:\n            pass\n\n        self.sendline()\n        x = self.try_read_prompt(sync_multiplier)\n\n        self.sendline()\n        a = self.try_read_prompt(sync_multiplier)\n\n        self.sendline()\n        b = self.try_read_prompt(sync_multiplier)\n\n        ld = self.levenshtein_distance(a,b)\n        len_a = len(a)\n        if len_a == 0:\n            return False\n        if float(ld)/len_a < 0.4:\n            return True\n        return False", "response": "This function attempts to find the prompt and returns True if it is at the original prompt False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsends exit to the remote shell.", "response": "def logout (self):\n        '''Sends exit to the remote shell.\n\n        If there are stopped jobs then this automatically sends exit twice.\n        '''\n        self.sendline(\"exit\")\n        index = self.expect([EOF, \"(?i)there are stopped jobs\"])\n        if index==1:\n            self.sendline(\"exit\")\n            self.expect(EOF)\n        self.close()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmatch the next shell prompt.", "response": "def prompt(self, timeout=-1):\n        '''Match the next shell prompt.\n\n        This is little more than a short-cut to the :meth:`~pexpect.spawn.expect`\n        method. Note that if you called :meth:`login` with\n        ``auto_prompt_reset=False``, then before calling :meth:`prompt` you must\n        set the :attr:`PROMPT` attribute to a regex that it will use for\n        matching the prompt.\n\n        Calling :meth:`prompt` will erase the contents of the :attr:`before`\n        attribute even if no prompt is ever matched. If timeout is not given or\n        it is set to -1 then self.timeout is used.\n\n        :return: True if the shell prompt was matched, False if the timeout was\n                 reached.\n        '''\n\n        if timeout == -1:\n            timeout = self.timeout\n        i = self.expect([self.PROMPT, TIMEOUT], timeout=timeout)\n        if i==1:\n            return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_unique_prompt(self):\n        '''This sets the remote prompt to something more unique than ``#`` or ``$``.\n        This makes it easier for the :meth:`prompt` method to match the shell prompt\n        unambiguously. This method is called automatically by the :meth:`login`\n        method, but you may want to call it manually if you somehow reset the\n        shell prompt. For example, if you 'su' to a different user then you\n        will need to manually reset the prompt. This sends shell commands to\n        the remote host to set the prompt, so this assumes the remote host is\n        ready to receive commands.\n\n        Alternatively, you may use your own prompt pattern. In this case you\n        should call :meth:`login` with ``auto_prompt_reset=False``; then set the\n        :attr:`PROMPT` attribute to a regular expression. After that, the\n        :meth:`prompt` method will try to match your prompt pattern.\n        '''\n\n        self.sendline(\"unset PROMPT_COMMAND\")\n        self.sendline(self.PROMPT_SET_SH) # sh-style\n        i = self.expect ([TIMEOUT, self.PROMPT], timeout=10)\n        if i == 0: # csh-style\n            self.sendline(self.PROMPT_SET_CSH)\n            i = self.expect([TIMEOUT, self.PROMPT], timeout=10)\n            if i == 0:\n                return False\n        return True", "response": "This method sets the remote prompt to something more unique than the shell prompt\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a string representing the user agent.", "response": "def user_agent():\n    \"\"\"\n    Return a string representing the user agent.\n    \"\"\"\n    data = {\n        \"installer\": {\"name\": \"pip\", \"version\": pipenv.patched.notpip.__version__},\n        \"python\": platform.python_version(),\n        \"implementation\": {\n            \"name\": platform.python_implementation(),\n        },\n    }\n\n    if data[\"implementation\"][\"name\"] == 'CPython':\n        data[\"implementation\"][\"version\"] = platform.python_version()\n    elif data[\"implementation\"][\"name\"] == 'PyPy':\n        if sys.pypy_version_info.releaselevel == 'final':\n            pypy_version_info = sys.pypy_version_info[:3]\n        else:\n            pypy_version_info = sys.pypy_version_info\n        data[\"implementation\"][\"version\"] = \".\".join(\n            [str(x) for x in pypy_version_info]\n        )\n    elif data[\"implementation\"][\"name\"] == 'Jython':\n        # Complete Guess\n        data[\"implementation\"][\"version\"] = platform.python_version()\n    elif data[\"implementation\"][\"name\"] == 'IronPython':\n        # Complete Guess\n        data[\"implementation\"][\"version\"] = platform.python_version()\n\n    if sys.platform.startswith(\"linux\"):\n        from pipenv.patched.notpip._vendor import distro\n        distro_infos = dict(filter(\n            lambda x: x[1],\n            zip([\"name\", \"version\", \"id\"], distro.linux_distribution()),\n        ))\n        libc = dict(filter(\n            lambda x: x[1],\n            zip([\"lib\", \"version\"], libc_ver()),\n        ))\n        if libc:\n            distro_infos[\"libc\"] = libc\n        if distro_infos:\n            data[\"distro\"] = distro_infos\n\n    if sys.platform.startswith(\"darwin\") and platform.mac_ver()[0]:\n        data[\"distro\"] = {\"name\": \"macOS\", \"version\": platform.mac_ver()[0]}\n\n    if platform.system():\n        data.setdefault(\"system\", {})[\"name\"] = platform.system()\n\n    if platform.release():\n        data.setdefault(\"system\", {})[\"release\"] = platform.release()\n\n    if platform.machine():\n        data[\"cpu\"] = platform.machine()\n\n    if HAS_TLS:\n        data[\"openssl_version\"] = ssl.OPENSSL_VERSION\n\n    setuptools_version = get_installed_version(\"setuptools\")\n    if setuptools_version is not None:\n        data[\"setuptools_version\"] = setuptools_version\n\n    return \"{data[installer][name]}/{data[installer][version]} {json}\".format(\n        data=data,\n        json=json.dumps(data, separators=(\",\", \":\"), sort_keys=True),\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a file URL to a path.", "response": "def url_to_path(url):\n    # type: (str) -> str\n    \"\"\"\n    Convert a file: URL to a path.\n    \"\"\"\n    assert url.startswith('file:'), (\n        \"You can only turn file: urls into filenames (not %r)\" % url)\n\n    _, netloc, path, _, _ = urllib_parse.urlsplit(url)\n\n    # if we have a UNC path, prepend UNC share notation\n    if netloc:\n        netloc = '\\\\\\\\' + netloc\n\n    path = urllib_request.url2pathname(netloc + path)\n    return path"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef path_to_url(path):\n    # type: (Union[str, Text]) -> str\n    \"\"\"\n    Convert a path to a file: URL.  The path will be made absolute and have\n    quoted path parts.\n    \"\"\"\n    path = os.path.normpath(os.path.abspath(path))\n    url = urllib_parse.urljoin('file:', urllib_request.pathname2url(path))\n    return url", "response": "Converts a path to a file URL."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn True if name is a considered as an archive file.", "response": "def is_archive_file(name):\n    # type: (str) -> bool\n    \"\"\"Return True if `name` is a considered as an archive file.\"\"\"\n    ext = splitext(name)[1].lower()\n    if ext in ARCHIVE_EXTENSIONS:\n        return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_dir_url(link):\n    # type: (Link) -> bool\n    \"\"\"Return whether a file:// Link points to a directory.\n\n    ``link`` must not have any other scheme but file://. Call is_file_url()\n    first.\n\n    \"\"\"\n    link_path = url_to_path(link.url_without_fragment)\n    return os.path.isdir(link_path)", "response": "Return whether a file:// Link points to a directory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nunpack a link into the build dir location.", "response": "def unpack_file_url(\n    link,  # type: Link\n    location,  # type: str\n    download_dir=None,  # type: Optional[str]\n    hashes=None  # type: Optional[Hashes]\n):\n    # type: (...) -> None\n    \"\"\"Unpack link into location.\n\n    If download_dir is provided and link points to a file, make a copy\n    of the link file inside download_dir.\n    \"\"\"\n    link_path = url_to_path(link.url_without_fragment)\n\n    # If it's a url to a local directory\n    if is_dir_url(link):\n        if os.path.isdir(location):\n            rmtree(location)\n        shutil.copytree(link_path, location, symlinks=True)\n        if download_dir:\n            logger.info('Link is a directory, ignoring download_dir')\n        return\n\n    # If --require-hashes is off, `hashes` is either empty, the\n    # link's embedded hash, or MissingHashes; it is required to\n    # match. If --require-hashes is on, we are satisfied by any\n    # hash in `hashes` matching: a URL-based or an option-based\n    # one; no internet-sourced hash will be in `hashes`.\n    if hashes:\n        hashes.check_against_path(link_path)\n\n    # If a download dir is specified, is the file already there and valid?\n    already_downloaded_path = None\n    if download_dir:\n        already_downloaded_path = _check_download_dir(link,\n                                                      download_dir,\n                                                      hashes)\n\n    if already_downloaded_path:\n        from_path = already_downloaded_path\n    else:\n        from_path = link_path\n\n    content_type = mimetypes.guess_type(from_path)[0]\n\n    # unpack the archive to the build dir location. even when only downloading\n    # archives, they have to be unpacked to parse dependencies\n    unpack_file(from_path, location, content_type, link)\n\n    # a download dir is specified and not already downloaded\n    if download_dir and not already_downloaded_path:\n        _copy_file(from_path, download_dir, link)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _copy_dist_from_dir(link_path, location):\n\n    # Note: This is currently VERY SLOW if you have a lot of data in the\n    # directory, because it copies everything with `shutil.copytree`.\n    # What it should really do is build an sdist and install that.\n    # See https://github.com/pypa/pip/issues/2195\n\n    if os.path.isdir(location):\n        rmtree(location)\n\n    # build an sdist\n    setup_py = 'setup.py'\n    sdist_args = [sys.executable]\n    sdist_args.append('-c')\n    sdist_args.append(SETUPTOOLS_SHIM % setup_py)\n    sdist_args.append('sdist')\n    sdist_args += ['--dist-dir', location]\n    logger.info('Running setup.py sdist for %s', link_path)\n\n    with indent_log():\n        call_subprocess(sdist_args, cwd=link_path, show_stdout=False)\n\n    # unpack sdist into `location`\n    sdist = os.path.join(location, os.listdir(location)[0])\n    logger.info('Unpacking sdist %s into %s', sdist, location)\n    unpack_file(sdist, location, content_type=None, link=None)", "response": "Copy distribution files in link_path to location."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef unpack_url(\n    link,  # type: Optional[Link]\n    location,  # type: Optional[str]\n    download_dir=None,  # type: Optional[str]\n    only_download=False,  # type: bool\n    session=None,  # type: Optional[PipSession]\n    hashes=None,  # type: Optional[Hashes]\n    progress_bar=\"on\"  # type: str\n):\n    # type: (...) -> None\n    \"\"\"Unpack link.\n       If link is a VCS link:\n         if only_download, export into download_dir and ignore location\n          else unpack into location\n       for other types of link:\n         - unpack into location\n         - if download_dir, copy the file into download_dir\n         - if only_download, mark location for deletion\n\n    :param hashes: A Hashes object, one of whose embedded hashes must match,\n        or HashMismatch will be raised. If the Hashes is empty, no matches are\n        required, and unhashable types of requirements (like VCS ones, which\n        would ordinarily raise HashUnsupported) are allowed.\n    \"\"\"\n    # non-editable vcs urls\n    if is_vcs_url(link):\n        unpack_vcs_link(link, location)\n\n    # file urls\n    elif is_file_url(link):\n        unpack_file_url(link, location, download_dir, hashes=hashes)\n\n    # http urls\n    else:\n        if session is None:\n            session = PipSession()\n\n        unpack_http_url(\n            link,\n            location,\n            download_dir,\n            session,\n            hashes=hashes,\n            progress_bar=progress_bar\n        )\n    if only_download:\n        write_delete_marker_file(location)", "response": "Unpacks a URL into a new object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck download_dir for previously downloaded file with correct hash and returns its path.", "response": "def _check_download_dir(link, download_dir, hashes):\n    # type: (Link, str, Hashes) -> Optional[str]\n    \"\"\" Check download_dir for previously downloaded file with correct hash\n        If a correct file is found return its path else None\n    \"\"\"\n    download_path = os.path.join(download_dir, link.filename)\n    if os.path.exists(download_path):\n        # If already downloaded, does its hash match?\n        logger.info('File was already downloaded %s', download_path)\n        if hashes:\n            try:\n                hashes.check_against_path(download_path)\n            except HashMismatch:\n                logger.warning(\n                    'Previously-downloaded file %s has bad hash. '\n                    'Re-downloading.',\n                    download_path\n                )\n                os.unlink(download_path)\n                return None\n        return download_path\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndefaults key normalization function for the connection pool key.", "response": "def _default_key_normalizer(key_class, request_context):\n    \"\"\"\n    Create a pool key out of a request context dictionary.\n\n    According to RFC 3986, both the scheme and host are case-insensitive.\n    Therefore, this function normalizes both before constructing the pool\n    key for an HTTPS request. If you wish to change this behaviour, provide\n    alternate callables to ``key_fn_by_scheme``.\n\n    :param key_class:\n        The class to use when constructing the key. This should be a namedtuple\n        with the ``scheme`` and ``host`` keys at a minimum.\n    :type  key_class: namedtuple\n    :param request_context:\n        A dictionary-like object that contain the context for a request.\n    :type  request_context: dict\n\n    :return: A namedtuple that can be used as a connection pool key.\n    :rtype:  PoolKey\n    \"\"\"\n    # Since we mutate the dictionary, make a copy first\n    context = request_context.copy()\n    context['scheme'] = context['scheme'].lower()\n    context['host'] = context['host'].lower()\n\n    # These are both dictionaries and need to be transformed into frozensets\n    for key in ('headers', '_proxy_headers', '_socks_options'):\n        if key in context and context[key] is not None:\n            context[key] = frozenset(context[key].items())\n\n    # The socket_options key may be a list and needs to be transformed into a\n    # tuple.\n    socket_opts = context.get('socket_options')\n    if socket_opts is not None:\n        context['socket_options'] = tuple(socket_opts)\n\n    # Map the kwargs to the names in the namedtuple - this is necessary since\n    # namedtuples can't have fields starting with '_'.\n    for key in list(context.keys()):\n        context['key_' + key] = context.pop(key)\n\n    # Default to ``None`` for keys missing from the context\n    for field in key_class._fields:\n        if field not in context:\n            context[field] = None\n\n    return key_class(**context)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new connection pool based on scheme host and port and request_context.", "response": "def _new_pool(self, scheme, host, port, request_context=None):\n        \"\"\"\n        Create a new :class:`ConnectionPool` based on host, port, scheme, and\n        any additional pool keyword arguments.\n\n        If ``request_context`` is provided, it is provided as keyword arguments\n        to the pool class used. This method is used to actually create the\n        connection pools handed out by :meth:`connection_from_url` and\n        companion methods. It is intended to be overridden for customization.\n        \"\"\"\n        pool_cls = self.pool_classes_by_scheme[scheme]\n        if request_context is None:\n            request_context = self.connection_pool_kw.copy()\n\n        # Although the context has everything necessary to create the pool,\n        # this function has historically only used the scheme, host, and port\n        # in the positional args. When an API change is acceptable these can\n        # be removed.\n        for key in ('scheme', 'host', 'port'):\n            request_context.pop(key, None)\n\n        if scheme == 'http':\n            for kw in SSL_KEYWORDS:\n                request_context.pop(kw, None)\n\n        return pool_cls(host, port, **request_context)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef connection_from_host(self, host, port=None, scheme='http', pool_kwargs=None):\n\n        if not host:\n            raise LocationValueError(\"No host specified.\")\n\n        request_context = self._merge_pool_kwargs(pool_kwargs)\n        request_context['scheme'] = scheme or 'http'\n        if not port:\n            port = port_by_scheme.get(request_context['scheme'].lower(), 80)\n        request_context['port'] = port\n        request_context['host'] = host\n\n        return self.connection_from_context(request_context)", "response": "Get a connection pool based on the host and port."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a ConnectionPool based on the request context.", "response": "def connection_from_context(self, request_context):\n        \"\"\"\n        Get a :class:`ConnectionPool` based on the request context.\n\n        ``request_context`` must at least contain the ``scheme`` key and its\n        value must be a key in ``key_fn_by_scheme`` instance variable.\n        \"\"\"\n        scheme = request_context['scheme'].lower()\n        pool_key_constructor = self.key_fn_by_scheme[scheme]\n        pool_key = pool_key_constructor(request_context)\n\n        return self.connection_from_pool_key(pool_key, request_context=request_context)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a ConnectionPool based on the provided pool key.", "response": "def connection_from_pool_key(self, pool_key, request_context=None):\n        \"\"\"\n        Get a :class:`ConnectionPool` based on the provided pool key.\n\n        ``pool_key`` should be a namedtuple that only contains immutable\n        objects. At a minimum it must have the ``scheme``, ``host``, and\n        ``port`` fields.\n        \"\"\"\n        with self.pools.lock:\n            # If the scheme, host, or port doesn't match existing open\n            # connections, open a new ConnectionPool.\n            pool = self.pools.get(pool_key)\n            if pool:\n                return pool\n\n            # Make a fresh ConnectionPool of the desired type\n            scheme = request_context['scheme']\n            host = request_context['host']\n            port = request_context['port']\n            pool = self._new_pool(scheme, host, port, request_context=request_context)\n            self.pools[pool_key] = pool\n\n        return pool"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _merge_pool_kwargs(self, override):\n        base_pool_kwargs = self.connection_pool_kw.copy()\n        if override:\n            for key, value in override.items():\n                if value is None:\n                    try:\n                        del base_pool_kwargs[key]\n                    except KeyError:\n                        pass\n                else:\n                    base_pool_kwargs[key] = value\n        return base_pool_kwargs", "response": "Merge a dictionary of override values for self. connection_pool_kw."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef urlopen(self, method, url, redirect=True, **kw):\n        u = parse_url(url)\n        conn = self.connection_from_host(u.host, port=u.port, scheme=u.scheme)\n\n        kw['assert_same_host'] = False\n        kw['redirect'] = False\n\n        if 'headers' not in kw:\n            kw['headers'] = self.headers.copy()\n\n        if self.proxy is not None and u.scheme == \"http\":\n            response = conn.urlopen(method, url, **kw)\n        else:\n            response = conn.urlopen(method, u.request_uri, **kw)\n\n        redirect_location = redirect and response.get_redirect_location()\n        if not redirect_location:\n            return response\n\n        # Support relative URLs for redirecting.\n        redirect_location = urljoin(url, redirect_location)\n\n        # RFC 7231, Section 6.4.4\n        if response.status == 303:\n            method = 'GET'\n\n        retries = kw.get('retries')\n        if not isinstance(retries, Retry):\n            retries = Retry.from_int(retries, redirect=redirect)\n\n        # Strip headers marked as unsafe to forward to the redirected location.\n        # Check remove_headers_on_redirect to avoid a potential network call within\n        # conn.is_same_host() which may use socket.gethostbyname() in the future.\n        if (retries.remove_headers_on_redirect\n                and not conn.is_same_host(redirect_location)):\n            for header in retries.remove_headers_on_redirect:\n                kw['headers'].pop(header, None)\n\n        try:\n            retries = retries.increment(method, url, response=response, _pool=conn)\n        except MaxRetryError:\n            if retries.raise_on_redirect:\n                raise\n            return response\n\n        kw['retries'] = retries\n        kw['redirect'] = redirect\n\n        log.info(\"Redirecting %s -> %s\", url, redirect_location)\n        return self.urlopen(method, redirect_location, **kw)", "response": "A method that returns a response object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _set_proxy_headers(self, url, headers=None):\n        headers_ = {'Accept': '*/*'}\n\n        netloc = parse_url(url).netloc\n        if netloc:\n            headers_['Host'] = netloc\n\n        if headers:\n            headers_.update(headers)\n        return headers_", "response": "Sets the headers needed by proxies."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef urlopen(self, method, url, redirect=True, **kw):\n        \"Same as HTTP(S)ConnectionPool.urlopen, ``url`` must be absolute.\"\n        u = parse_url(url)\n\n        if u.scheme == \"http\":\n            # For proxied HTTPS requests, httplib sets the necessary headers\n            # on the CONNECT to the proxy. For HTTP, we'll definitely\n            # need to set 'Host' at the very least.\n            headers = kw.get('headers', self.headers)\n            kw['headers'] = self._set_proxy_headers(url, headers)\n\n        return super(ProxyManager, self).urlopen(method, url, redirect=redirect, **kw)", "response": "Same as HTTP ( S )ConnectionPool. urlopen url must be absolute."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a set of all variables that are not declared in the context at runtime.", "response": "def find_undeclared_variables(ast):\n    \"\"\"Returns a set of all variables in the AST that will be looked up from\n    the context at runtime.  Because at compile time it's not known which\n    variables will be used depending on the path the execution takes at\n    runtime, all variables are returned.\n\n    >>> from jinja2 import Environment, meta\n    >>> env = Environment()\n    >>> ast = env.parse('{% set foo = 42 %}{{ bar + foo }}')\n    >>> meta.find_undeclared_variables(ast) == set(['bar'])\n    True\n\n    .. admonition:: Implementation\n\n       Internally the code generator is used for finding undeclared variables.\n       This is good to know because the code generator might raise a\n       :exc:`TemplateAssertionError` during compilation and as a matter of\n       fact this function can currently raise that exception as well.\n    \"\"\"\n    codegen = TrackingCodeGenerator(ast.environment)\n    codegen.visit(ast)\n    return codegen.undeclared_identifiers"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind all the referenced templates from the AST.", "response": "def find_referenced_templates(ast):\n    \"\"\"Finds all the referenced templates from the AST.  This will return an\n    iterator over all the hardcoded template extensions, inclusions and\n    imports.  If dynamic inheritance or inclusion is used, `None` will be\n    yielded.\n\n    >>> from jinja2 import Environment, meta\n    >>> env = Environment()\n    >>> ast = env.parse('{% extends \"layout.html\" %}{% include helper %}')\n    >>> list(meta.find_referenced_templates(ast))\n    ['layout.html', None]\n\n    This function is useful for dependency tracking.  For example if you want\n    to rebuild parts of the website after a layout template has changed.\n    \"\"\"\n    for node in ast.find_all((nodes.Extends, nodes.FromImport, nodes.Import,\n                              nodes.Include)):\n        if not isinstance(node.template, nodes.Const):\n            # a tuple with some non consts in there\n            if isinstance(node.template, (nodes.Tuple, nodes.List)):\n                for template_name in node.template.items:\n                    # something const, only yield the strings and ignore\n                    # non-string consts that really just make no sense\n                    if isinstance(template_name, nodes.Const):\n                        if isinstance(template_name.value, string_types):\n                            yield template_name.value\n                    # something dynamic in there\n                    else:\n                        yield None\n            # something dynamic we don't know about here\n            else:\n                yield None\n            continue\n        # constant is a basestring, direct template name\n        if isinstance(node.template.value, string_types):\n            yield node.template.value\n        # a tuple or list (latter *should* not happen) made of consts,\n        # yield the consts that are strings.  We could warn here for\n        # non string values\n        elif isinstance(node, nodes.Include) and \\\n             isinstance(node.template.value, (tuple, list)):\n            for template_name in node.template.value:\n                if isinstance(template_name, string_types):\n                    yield template_name\n        # something else we don't care about, we could warn here\n        else:\n            yield None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef enter_frame(self, frame):\n        CodeGenerator.enter_frame(self, frame)\n        for _, (action, param) in iteritems(frame.symbols.loads):\n            if action == 'resolve':\n                self.undeclared_identifiers.add(param)", "response": "Remember all undeclared identifiers."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a marker string and return a dictionary containing the keys op lhs and rhs for non - terminals in the marker expression grammar.", "response": "def parse_marker(marker_string):\n    \"\"\"\n    Parse a marker string and return a dictionary containing a marker expression.\n\n    The dictionary will contain keys \"op\", \"lhs\" and \"rhs\" for non-terminals in\n    the expression grammar, or strings. A string contained in quotes is to be\n    interpreted as a literal string, and a string not contained in quotes is a\n    variable (such as os_name).\n    \"\"\"\n    def marker_var(remaining):\n        # either identifier, or literal string\n        m = IDENTIFIER.match(remaining)\n        if m:\n            result = m.groups()[0]\n            remaining = remaining[m.end():]\n        elif not remaining:\n            raise SyntaxError('unexpected end of input')\n        else:\n            q = remaining[0]\n            if q not in '\\'\"':\n                raise SyntaxError('invalid expression: %s' % remaining)\n            oq = '\\'\"'.replace(q, '')\n            remaining = remaining[1:]\n            parts = [q]\n            while remaining:\n                # either a string chunk, or oq, or q to terminate\n                if remaining[0] == q:\n                    break\n                elif remaining[0] == oq:\n                    parts.append(oq)\n                    remaining = remaining[1:]\n                else:\n                    m = STRING_CHUNK.match(remaining)\n                    if not m:\n                        raise SyntaxError('error in string literal: %s' % remaining)\n                    parts.append(m.groups()[0])\n                    remaining = remaining[m.end():]\n            else:\n                s = ''.join(parts)\n                raise SyntaxError('unterminated string: %s' % s)\n            parts.append(q)\n            result = ''.join(parts)\n            remaining = remaining[1:].lstrip() # skip past closing quote\n        return result, remaining\n\n    def marker_expr(remaining):\n        if remaining and remaining[0] == '(':\n            result, remaining = marker(remaining[1:].lstrip())\n            if remaining[0] != ')':\n                raise SyntaxError('unterminated parenthesis: %s' % remaining)\n            remaining = remaining[1:].lstrip()\n        else:\n            lhs, remaining = marker_var(remaining)\n            while remaining:\n                m = MARKER_OP.match(remaining)\n                if not m:\n                    break\n                op = m.groups()[0]\n                remaining = remaining[m.end():]\n                rhs, remaining = marker_var(remaining)\n                lhs = {'op': op, 'lhs': lhs, 'rhs': rhs}\n            result = lhs\n        return result, remaining\n\n    def marker_and(remaining):\n        lhs, remaining = marker_expr(remaining)\n        while remaining:\n            m = AND.match(remaining)\n            if not m:\n                break\n            remaining = remaining[m.end():]\n            rhs, remaining = marker_expr(remaining)\n            lhs = {'op': 'and', 'lhs': lhs, 'rhs': rhs}\n        return lhs, remaining\n\n    def marker(remaining):\n        lhs, remaining = marker_and(remaining)\n        while remaining:\n            m = OR.match(remaining)\n            if not m:\n                break\n            remaining = remaining[m.end():]\n            rhs, remaining = marker_and(remaining)\n            lhs = {'op': 'or', 'lhs': lhs, 'rhs': rhs}\n        return lhs, remaining\n\n    return marker(marker_string)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a string containing a Container requirement.", "response": "def parse_requirement(req):\n    \"\"\"\n    Parse a requirement passed in as a string. Return a Container\n    whose attributes contain the various parts of the requirement.\n    \"\"\"\n    remaining = req.strip()\n    if not remaining or remaining.startswith('#'):\n        return None\n    m = IDENTIFIER.match(remaining)\n    if not m:\n        raise SyntaxError('name expected: %s' % remaining)\n    distname = m.groups()[0]\n    remaining = remaining[m.end():]\n    extras = mark_expr = versions = uri = None\n    if remaining and remaining[0] == '[':\n        i = remaining.find(']', 1)\n        if i < 0:\n            raise SyntaxError('unterminated extra: %s' % remaining)\n        s = remaining[1:i]\n        remaining = remaining[i + 1:].lstrip()\n        extras = []\n        while s:\n            m = IDENTIFIER.match(s)\n            if not m:\n                raise SyntaxError('malformed extra: %s' % s)\n            extras.append(m.groups()[0])\n            s = s[m.end():]\n            if not s:\n                break\n            if s[0] != ',':\n                raise SyntaxError('comma expected in extras: %s' % s)\n            s = s[1:].lstrip()\n        if not extras:\n            extras = None\n    if remaining:\n        if remaining[0] == '@':\n            # it's a URI\n            remaining = remaining[1:].lstrip()\n            m = NON_SPACE.match(remaining)\n            if not m:\n                raise SyntaxError('invalid URI: %s' % remaining)\n            uri = m.groups()[0]\n            t = urlparse(uri)\n            # there are issues with Python and URL parsing, so this test\n            # is a bit crude. See bpo-20271, bpo-23505. Python doesn't\n            # always parse invalid URLs correctly - it should raise\n            # exceptions for malformed URLs\n            if not (t.scheme and t.netloc):\n                raise SyntaxError('Invalid URL: %s' % uri)\n            remaining = remaining[m.end():].lstrip()\n        else:\n\n            def get_versions(ver_remaining):\n                \"\"\"\n                Return a list of operator, version tuples if any are\n                specified, else None.\n                \"\"\"\n                m = COMPARE_OP.match(ver_remaining)\n                versions = None\n                if m:\n                    versions = []\n                    while True:\n                        op = m.groups()[0]\n                        ver_remaining = ver_remaining[m.end():]\n                        m = VERSION_IDENTIFIER.match(ver_remaining)\n                        if not m:\n                            raise SyntaxError('invalid version: %s' % ver_remaining)\n                        v = m.groups()[0]\n                        versions.append((op, v))\n                        ver_remaining = ver_remaining[m.end():]\n                        if not ver_remaining or ver_remaining[0] != ',':\n                            break\n                        ver_remaining = ver_remaining[1:].lstrip()\n                        m = COMPARE_OP.match(ver_remaining)\n                        if not m:\n                            raise SyntaxError('invalid constraint: %s' % ver_remaining)\n                    if not versions:\n                        versions = None\n                return versions, ver_remaining\n\n            if remaining[0] != '(':\n                versions, remaining = get_versions(remaining)\n            else:\n                i = remaining.find(')', 1)\n                if i < 0:\n                    raise SyntaxError('unterminated parenthesis: %s' % remaining)\n                s = remaining[1:i]\n                remaining = remaining[i + 1:].lstrip()\n                # As a special diversion from PEP 508, allow a version number\n                # a.b.c in parentheses as a synonym for ~= a.b.c (because this\n                # is allowed in earlier PEPs)\n                if COMPARE_OP.match(s):\n                    versions, _ = get_versions(s)\n                else:\n                    m = VERSION_IDENTIFIER.match(s)\n                    if not m:\n                        raise SyntaxError('invalid constraint: %s' % s)\n                    v = m.groups()[0]\n                    s = s[m.end():].lstrip()\n                    if s:\n                        raise SyntaxError('invalid constraint: %s' % s)\n                    versions = [('~=', v)]\n\n    if remaining:\n        if remaining[0] != ';':\n            raise SyntaxError('invalid requirement: %s' % remaining)\n        remaining = remaining[1:].lstrip()\n\n        mark_expr, remaining = parse_marker(remaining)\n\n    if remaining and remaining[0] != '#':\n        raise SyntaxError('unexpected trailing data: %s' % remaining)\n\n    if not versions:\n        rs = distname\n    else:\n        rs = '%s %s' % (distname, ', '.join(['%s %s' % con for con in versions]))\n    return Container(name=distname, extras=extras, constraints=versions,\n                     marker=mark_expr, url=uri, requirement=rs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a path to a name that will work on the native filesystem.", "response": "def convert_path(pathname):\n    \"\"\"Return 'pathname' as a name that will work on the native filesystem.\n\n    The path is split on '/' and put back together again using the current\n    directory separator.  Needed because filenames in the setup script are\n    always supplied in Unix style, and have to be converted to the local\n    convention before we can actually use them in the filesystem.  Raises\n    ValueError on non-Unix-ish systems if 'pathname' either starts or\n    ends with a slash.\n    \"\"\"\n    if os.sep == '/':\n        return pathname\n    if not pathname:\n        return pathname\n    if pathname[0] == '/':\n        raise ValueError(\"path '%s' cannot be absolute\" % pathname)\n    if pathname[-1] == '/':\n        raise ValueError(\"path '%s' cannot end with '/'\" % pathname)\n\n    paths = pathname.split('/')\n    while os.curdir in paths:\n        paths.remove(os.curdir)\n    if not paths:\n        return os.curdir\n    return os.path.join(*paths)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_cache_base(suffix=None):\n    if suffix is None:\n        suffix = '.distlib'\n    if os.name == 'nt' and 'LOCALAPPDATA' in os.environ:\n        result = os.path.expandvars('$localappdata')\n    else:\n        # Assume posix, or old Windows\n        result = os.path.expanduser('~')\n    # we use 'isdir' instead of 'exists', because we want to\n    # fail if there's a file with that name\n    if os.path.isdir(result):\n        usable = os.access(result, os.W_OK)\n        if not usable:\n            logger.warning('Directory exists but is not writable: %s', result)\n    else:\n        try:\n            os.makedirs(result)\n            usable = True\n        except OSError:\n            logger.warning('Unable to create %s', result, exc_info=True)\n            usable = False\n    if not usable:\n        result = tempfile.mkdtemp()\n        logger.warning('Default location unusable, using %s', result)\n    return os.path.join(result, suffix)", "response": "Return the default base location for distlib caches."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting an absolute path to a directory name for use in a cache.", "response": "def path_to_cache_dir(path):\n    \"\"\"\n    Convert an absolute path to a directory name for use in a cache.\n\n    The algorithm used is:\n\n    #. On Windows, any ``':'`` in the drive is replaced with ``'---'``.\n    #. Any occurrence of ``os.sep`` is replaced with ``'--'``.\n    #. ``'.cache'`` is appended.\n    \"\"\"\n    d, p = os.path.splitdrive(os.path.abspath(path))\n    if d:\n        d = d.replace(':', '---')\n    p = p.replace(os.sep, '--')\n    return d + p + '.cache'"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsplits a filename into name version and python version.", "response": "def split_filename(filename, project_name=None):\n    \"\"\"\n    Extract name, version, python version from a filename (no extension)\n\n    Return name, version, pyver or None\n    \"\"\"\n    result = None\n    pyver = None\n    filename = unquote(filename).replace(' ', '-')\n    m = PYTHON_VERSION.search(filename)\n    if m:\n        pyver = m.group(1)\n        filename = filename[:m.start()]\n    if project_name and len(filename) > len(project_name) + 1:\n        m = re.match(re.escape(project_name) + r'\\b', filename)\n        if m:\n            n = m.end()\n            result = filename[:n], filename[n + 1:], pyver\n    if result is None:\n        m = PROJECT_NAME_AND_VERSION.match(filename)\n        if m:\n            result = m.group(1), m.group(3), pyver\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_name_and_version(p):\n    m = NAME_VERSION_RE.match(p)\n    if not m:\n        raise DistlibException('Ill-formed name/version string: \\'%s\\'' % p)\n    d = m.groupdict()\n    return d['name'].strip().lower(), d['ver']", "response": "A utility method used to get name and version from a string."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nzip a directory tree into a BytesIO object", "response": "def zip_dir(directory):\n    \"\"\"zip a directory tree into a BytesIO object\"\"\"\n    result = io.BytesIO()\n    dlen = len(directory)\n    with ZipFile(result, \"w\") as zf:\n        for root, dirs, files in os.walk(directory):\n            for name in files:\n                full = os.path.join(root, name)\n                rel = root[dlen:]\n                dest = os.path.join(rel, name)\n                zf.write(full, dest)\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nextends globbing function that supports ** and { opt1 opt2 opt3.", "response": "def iglob(path_glob):\n    \"\"\"Extended globbing function that supports ** and {opt1,opt2,opt3}.\"\"\"\n    if _CHECK_RECURSIVE_GLOB.search(path_glob):\n        msg = \"\"\"invalid glob %r: recursive glob \"**\" must be used alone\"\"\"\n        raise ValueError(msg % path_glob)\n    if _CHECK_MISMATCH_SET.search(path_glob):\n        msg = \"\"\"invalid glob %r: mismatching set marker '{' or '}'\"\"\"\n        raise ValueError(msg % path_glob)\n    return _iglob(path_glob)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntell if the target is newer than the source.", "response": "def newer(self, source, target):\n        \"\"\"Tell if the target is newer than the source.\n\n        Returns true if 'source' exists and is more recently modified than\n        'target', or if 'source' exists and 'target' doesn't.\n\n        Returns false if both exist and 'target' is the same age or younger\n        than 'source'. Raise PackagingFileError if 'source' does not exist.\n\n        Note that this test is not very accurate: files created in the same\n        second will have the same \"age\".\n        \"\"\"\n        if not os.path.exists(source):\n            raise DistlibException(\"file '%r' does not exist\" %\n                                   os.path.abspath(source))\n        if not os.path.exists(target):\n            return True\n\n        return os.stat(source).st_mtime > os.stat(target).st_mtime"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncopy a file to a new location.", "response": "def copy_file(self, infile, outfile, check=True):\n        \"\"\"Copy a file respecting dry-run and force flags.\n        \"\"\"\n        self.ensure_dir(os.path.dirname(outfile))\n        logger.info('Copying %s to %s', infile, outfile)\n        if not self.dry_run:\n            msg = None\n            if check:\n                if os.path.islink(outfile):\n                    msg = '%s is a symlink' % outfile\n                elif os.path.exists(outfile) and not os.path.isfile(outfile):\n                    msg = '%s is a non-regular file' % outfile\n            if msg:\n                raise ValueError(msg + ' which would be overwritten')\n            shutil.copyfile(infile, outfile)\n        self.record_as_written(outfile)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncommitting recorded changes turn off recording return the current set of files written and dirs created.", "response": "def commit(self):\n        \"\"\"\n        Commit recorded changes, turn off recording, return\n        changes.\n        \"\"\"\n        assert self.record\n        result = self.files_written, self.dirs_created\n        self._init_record()\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add(self, event, subscriber, append=True):\n        subs = self._subscribers\n        if event not in subs:\n            subs[event] = deque([subscriber])\n        else:\n            sq = subs[event]\n            if append:\n                sq.append(subscriber)\n            else:\n                sq.appendleft(subscriber)", "response": "Adds a subscriber to the list of subscribers for an event."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef remove(self, event, subscriber):\n        subs = self._subscribers\n        if event not in subs:\n            raise ValueError('No subscribers: %r' % event)\n        subs[event].remove(subscriber)", "response": "Removes a subscriber for an event."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef publish(self, event, *args, **kwargs):\n        result = []\n        for subscriber in self.get_subscribers(event):\n            try:\n                value = subscriber(event, *args, **kwargs)\n            except Exception:\n                logger.exception('Exception during event publication')\n                value = None\n            result.append(value)\n        logger.debug('publish %s: args = %s, kwargs = %s, result = %s',\n                     event, args, kwargs, result)\n        return result", "response": "Publish an event and return a list of values returned by its\n                     subscribers."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndefaulting converter for the inc:// protocol.", "response": "def inc_convert(self, value):\n        \"\"\"Default converter for the inc:// protocol.\"\"\"\n        if not os.path.isabs(value):\n            value = os.path.join(self.base, value)\n        with codecs.open(value, 'r', encoding='utf-8') as f:\n            result = json.load(f)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading lines from a subprocess output stream and pass to a progress callable or write progress information to sys. stderr.", "response": "def reader(self, stream, context):\n        \"\"\"\n        Read lines from a subprocess' output stream and either pass to a progress\n        callable (if specified) or write progress information to sys.stderr.\n        \"\"\"\n        progress = self.progress\n        verbose = self.verbose\n        while True:\n            s = stream.readline()\n            if not s:\n                break\n            if progress is not None:\n                progress(s, context)\n            else:\n                if not verbose:\n                    sys.stderr.write('.')\n                else:\n                    sys.stderr.write(s.decode('utf-8'))\n                sys.stderr.flush()\n        stream.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get(pypi_server):\n    response = requests.get(pypi_server)\n    if response.status_code >= 300:\n        raise HTTPError(status_code=response.status_code,\n                        reason=response.reason)\n    if hasattr(response.content, 'decode'):\n        tree = xml.etree.ElementTree.fromstring(response.content.decode())\n    else:\n        tree = xml.etree.ElementTree.fromstring(response.content)\n    channel = tree.find('channel')\n    return channel.findall('item')", "response": "Query the PyPI RSS feed and return a list\n    of XML items."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconstructing a request to the PyPI server and returns a list of new Available packages.", "response": "def newest_packages(\n        pypi_server=\"https://pypi.python.org/pypi?%3Aaction=packages_rss\"):\n    \"\"\"\n    Constructs a request to the PyPI server and returns a list of\n    :class:`yarg.parse.Package`.\n\n    :param pypi_server: (option) URL to the PyPI server.\n\n        >>> import yarg\n        >>> yarg.newest_packages()\n        [<Package yarg>, <Package gray>, <Package ragy>]\n    \"\"\"\n    items = _get(pypi_server)\n    i = []\n    for item in items:\n        i_dict = {'name': item[0].text.split()[0],\n                  'url': item[1].text,\n                  'description': item[3].text,\n                  'date': item[4].text}\n        i.append(Package(i_dict))\n    return i"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nproduce a mapping of identifier to requirement from the section.", "response": "def _get_requirements(model, section_name):\n    \"\"\"Produce a mapping of identifier: requirement from the section.\n    \"\"\"\n    if not model:\n        return {}\n    return {identify_requirment(r): r for r in (\n        requirementslib.Requirement.from_pipfile(name, package._data)\n        for name, package in model.get(section_name, {}).items()\n    )}"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nproduce a mapping containing all candidates derived from identifiers.", "response": "def _collect_derived_entries(state, traces, identifiers):\n    \"\"\"Produce a mapping containing all candidates derived from `identifiers`.\n\n    `identifiers` should provide a collection of requirement identifications\n    from a section (i.e. `packages` or `dev-packages`). This function uses\n    `trace` to filter out candidates in the state that are present because of\n    an entry in that collection.\n    \"\"\"\n    identifiers = set(identifiers)\n    if not identifiers:\n        return {}\n\n    entries = {}\n    extras = {}\n    for identifier, requirement in state.mapping.items():\n        routes = {trace[1] for trace in traces[identifier] if len(trace) > 1}\n        if identifier not in identifiers and not (identifiers & routes):\n            continue\n        name = requirement.normalized_name\n        if requirement.extras:\n            # Aggregate extras from multiple routes so we can produce their\n            # union in the lock file. (sarugaku/passa#24)\n            try:\n                extras[name].extend(requirement.extras)\n            except KeyError:\n                extras[name] = list(requirement.extras)\n        entries[name] = next(iter(requirement.as_pipfile().values()))\n    for name, ext in extras.items():\n        entries[name][\"extras\"] = ext\n\n    return entries"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef lock(self):\n        provider = self.get_provider()\n        reporter = self.get_reporter()\n        resolver = resolvelib.Resolver(provider, reporter)\n\n        with vistir.cd(self.project.root):\n            state = resolver.resolve(self.requirements)\n\n        traces = trace_graph(state.graph)\n\n        hash_cache = HashCache()\n        for r in state.mapping.values():\n            if not r.hashes:\n                r.hashes = get_hashes(hash_cache, r)\n\n        set_metadata(\n            state.mapping, traces,\n            provider.fetched_dependencies,\n            provider.collected_requires_pythons,\n        )\n\n        lockfile = plette.Lockfile.with_meta_from(self.project.pipfile)\n        lockfile[\"default\"] = _collect_derived_entries(\n            state, traces, self.default_requirements,\n        )\n        lockfile[\"develop\"] = _collect_derived_entries(\n            state, traces, self.develop_requirements,\n        )\n        self.project.lockfile = lockfile", "response": "Lock the specified requirements into concrete candidates."}
