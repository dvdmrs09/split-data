{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndownload and display a text file from the internet.", "response": "async def jsk_curl(self, ctx: commands.Context, url: str):\n        \"\"\"\n        Download and display a text file from the internet.\n\n        This command is similar to jsk cat, but accepts a URL.\n        \"\"\"\n\n        # remove embed maskers if present\n        url = url.lstrip(\"<\").rstrip(\">\")\n\n        async with ReplResponseReactor(ctx.message):\n            async with aiohttp.ClientSession() as session:\n                async with session.get(url) as response:\n                    data = await response.read()\n                    hints = (\n                        response.content_type,\n                        url\n                    )\n                    code = response.status\n\n            if not data:\n                return await ctx.send(f\"HTTP response was empty (status code {code}).\")\n\n            try:\n                paginator = WrappedFilePaginator(io.BytesIO(data), language_hints=hints, max_size=1985)\n            except UnicodeDecodeError:\n                return await ctx.send(f\"Couldn't determine the encoding of the response. (status code {code})\")\n            except ValueError as exc:\n                return await ctx.send(f\"Couldn't read response (status code {code}), {exc}\")\n\n            interface = PaginatorInterface(ctx.bot, paginator, owner=ctx.author)\n            await interface.send_to(ctx)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndisplay the source code for a command.", "response": "async def jsk_source(self, ctx: commands.Context, *, command_name: str):\n        \"\"\"\n        Displays the source code for a command.\n        \"\"\"\n\n        command = self.bot.get_command(command_name)\n        if not command:\n            return await ctx.send(f\"Couldn't find command `{command_name}`.\")\n\n        try:\n            source_lines, _ = inspect.getsourcelines(command.callback)\n        except (TypeError, OSError):\n            return await ctx.send(f\"Was unable to retrieve the source for `{command}` for some reason.\")\n\n        # getsourcelines for some reason returns WITH line endings\n        source_lines = ''.join(source_lines).split('\\n')\n\n        paginator = WrappedPaginator(prefix='```py', suffix='```', max_size=1985)\n        for line in source_lines:\n            paginator.add_line(line)\n\n        interface = PaginatorInterface(ctx.bot, paginator, owner=ctx.author)\n        await interface.send_to(ctx)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nturning variable retention for REPL on or off.", "response": "async def jsk_retain(self, ctx: commands.Context, *, toggle: bool = None):\n        \"\"\"\n        Turn variable retention for REPL on or off.\n\n        Provide no argument for current status.\n        \"\"\"\n\n        if toggle is None:\n            if self.retain:\n                return await ctx.send(\"Variable retention is set to ON.\")\n\n            return await ctx.send(\"Variable retention is set to OFF.\")\n\n        if toggle:\n            if self.retain:\n                return await ctx.send(\"Variable retention is already set to ON.\")\n\n            self.retain = True\n            self._scope = Scope()\n            return await ctx.send(\"Variable retention is ON. Future REPL sessions will retain their scope.\")\n\n        if not self.retain:\n            return await ctx.send(\"Variable retention is already set to OFF.\")\n\n        self.retain = False\n        return await ctx.send(\"Variable retention is OFF. Future REPL sessions will dispose their scope when done.\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndirect evaluation of Python code.", "response": "async def jsk_python(self, ctx: commands.Context, *, argument: CodeblockConverter):\n        \"\"\"\n        Direct evaluation of Python code.\n        \"\"\"\n\n        arg_dict = get_var_dict_from_ctx(ctx, SCOPE_PREFIX)\n        arg_dict[\"_\"] = self.last_result\n\n        scope = self.scope\n\n        try:\n            async with ReplResponseReactor(ctx.message):\n                with self.submit(ctx):\n                    async for result in AsyncCodeExecutor(argument.content, scope, arg_dict=arg_dict):\n                        if result is None:\n                            continue\n\n                        self.last_result = result\n\n                        if isinstance(result, discord.File):\n                            await ctx.send(file=result)\n                        elif isinstance(result, discord.Embed):\n                            await ctx.send(embed=result)\n                        elif isinstance(result, PaginatorInterface):\n                            await result.send_to(ctx)\n                        else:\n                            if not isinstance(result, str):\n                                # repr all non-strings\n                                result = repr(result)\n\n                            if len(result) > 2000:\n                                # inconsistency here, results get wrapped in codeblocks when they are too large\n                                #  but don't if they're not. probably not that bad, but noting for later review\n                                paginator = WrappedPaginator(prefix='```py', suffix='```', max_size=1985)\n\n                                paginator.add_line(result)\n\n                                interface = PaginatorInterface(ctx.bot, paginator, owner=ctx.author)\n                                await interface.send_to(ctx)\n                            else:\n                                if result.strip() == '':\n                                    result = \"\\u200b\"\n\n                                await ctx.send(result.replace(self.bot.http.token, \"[token omitted]\"))\n        finally:\n            scope.clear_intersection(arg_dict)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nevaluate Python code with inspect information.", "response": "async def jsk_python_inspect(self, ctx: commands.Context, *, argument: CodeblockConverter):\n        \"\"\"\n        Evaluation of Python code with inspect information.\n        \"\"\"\n\n        arg_dict = get_var_dict_from_ctx(ctx, SCOPE_PREFIX)\n        arg_dict[\"_\"] = self.last_result\n\n        scope = self.scope\n\n        try:\n            async with ReplResponseReactor(ctx.message):\n                with self.submit(ctx):\n                    async for result in AsyncCodeExecutor(argument.content, scope, arg_dict=arg_dict):\n                        self.last_result = result\n\n                        header = repr(result).replace(\"``\", \"`\\u200b`\").replace(self.bot.http.token, \"[token omitted]\")\n\n                        if len(header) > 485:\n                            header = header[0:482] + \"...\"\n\n                        paginator = WrappedPaginator(prefix=f\"```prolog\\n=== {header} ===\\n\", max_size=1985)\n\n                        for name, res in all_inspections(result):\n                            paginator.add_line(f\"{name:16.16} :: {res}\")\n\n                        interface = PaginatorInterface(ctx.bot, paginator, owner=ctx.author)\n                        await interface.send_to(ctx)\n        finally:\n            scope.clear_intersection(arg_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexecute statements in the system shell.", "response": "async def jsk_shell(self, ctx: commands.Context, *, argument: CodeblockConverter):\n        \"\"\"\n        Executes statements in the system shell.\n\n        This uses the bash shell. Execution can be cancelled by closing the paginator.\n        \"\"\"\n\n        async with ReplResponseReactor(ctx.message):\n            with self.submit(ctx):\n                paginator = WrappedPaginator(prefix=\"```sh\", max_size=1985)\n                paginator.add_line(f\"$ {argument.content}\\n\")\n\n                interface = PaginatorInterface(ctx.bot, paginator, owner=ctx.author)\n                self.bot.loop.create_task(interface.send_to(ctx))\n\n                with ShellReader(argument.content) as reader:\n                    async for line in reader:\n                        if interface.closed:\n                            return\n                        await interface.add_line(line)\n\n                await interface.add_line(f\"\\n[status] Return code {reader.close_code}\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the current voice client s current state.", "response": "async def jsk_voice(self, ctx: commands.Context):\n        \"\"\"\n        Voice-related commands.\n\n        If invoked without subcommand, relays current voice state.\n        \"\"\"\n\n        # if using a subcommand, short out\n        if ctx.invoked_subcommand is not None and ctx.invoked_subcommand is not self.jsk_voice:\n            return\n\n        # give info about the current voice client if there is one\n        voice = ctx.guild.voice_client\n\n        if not voice or not voice.is_connected():\n            return await ctx.send(\"Not connected.\")\n\n        await ctx.send(f\"Connected to {voice.channel.name}, \"\n                       f\"{'paused' if voice.is_paused() else 'playing' if voice.is_playing() else 'idle'}.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def jsk_vc_join(self, ctx: commands.Context, *,\n                          destination: typing.Union[discord.VoiceChannel, discord.Member] = None):\n        \"\"\"\n        Joins a voice channel, or moves to it if already connected.\n\n        Passing a voice channel uses that voice channel.\n        Passing a member will use that member's current voice channel.\n        Passing nothing will use the author's voice channel.\n        \"\"\"\n\n        destination = destination or ctx.author\n\n        if isinstance(destination, discord.Member):\n            if destination.voice and destination.voice.channel:\n                destination = destination.voice.channel\n            else:\n                return await ctx.send(\"Member has no voice channel.\")\n\n        voice = ctx.guild.voice_client\n\n        if voice:\n            await voice.move_to(destination)\n        else:\n            await destination.connect(reconnect=True)\n\n        await ctx.send(f\"Connected to {destination.name}.\")", "response": "Joins a voice channel or moves to it if already connected."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndisconnecting from the voice channel if there is one.", "response": "async def jsk_vc_disconnect(self, ctx: commands.Context):\n        \"\"\"\n        Disconnects from the voice channel in this guild, if there is one.\n        \"\"\"\n\n        voice = ctx.guild.voice_client\n\n        await voice.disconnect()\n        await ctx.send(f\"Disconnected from {voice.channel.name}.\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstop running an audio source.", "response": "async def jsk_vc_stop(self, ctx: commands.Context):\n        \"\"\"\n        Stops running an audio source, if there is one.\n        \"\"\"\n\n        voice = ctx.guild.voice_client\n\n        voice.stop()\n        await ctx.send(f\"Stopped playing audio in {voice.channel.name}.\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npauses a running audio source.", "response": "async def jsk_vc_pause(self, ctx: commands.Context):\n        \"\"\"\n        Pauses a running audio source, if there is one.\n        \"\"\"\n\n        voice = ctx.guild.voice_client\n\n        if voice.is_paused():\n            return await ctx.send(\"Audio is already paused.\")\n\n        voice.pause()\n        await ctx.send(f\"Paused audio in {voice.channel.name}.\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nresume a running audio source.", "response": "async def jsk_vc_resume(self, ctx: commands.Context):\n        \"\"\"\n        Resumes a running audio source, if there is one.\n        \"\"\"\n\n        voice = ctx.guild.voice_client\n\n        if not voice.is_paused():\n            return await ctx.send(\"Audio is not paused.\")\n\n        voice.resume()\n        await ctx.send(f\"Resumed audio in {voice.channel.name}.\")"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def jsk_vc_volume(self, ctx: commands.Context, *, percentage: float):\n\n        volume = max(0.0, min(1.0, percentage / 100))\n\n        source = ctx.guild.voice_client.source\n\n        if not isinstance(source, discord.PCMVolumeTransformer):\n            return await ctx.send(\"This source doesn't support adjusting volume or \"\n                                  \"the interface to do so is not exposed.\")\n\n        source.volume = volume\n\n        await ctx.send(f\"Volume set to {volume * 100:.2f}%\")", "response": "Adjusts the volume of an audio source if it is supported."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nplaying audio direct from a URI.", "response": "async def jsk_vc_play(self, ctx: commands.Context, *, uri: str):\n        \"\"\"\n        Plays audio direct from a URI.\n\n        Can be either a local file or an audio resource on the internet.\n        \"\"\"\n\n        voice = ctx.guild.voice_client\n\n        if voice.is_playing():\n            voice.stop()\n\n        # remove embed maskers if present\n        uri = uri.lstrip(\"<\").rstrip(\">\")\n\n        voice.play(discord.PCMVolumeTransformer(discord.FFmpegPCMAudio(uri)))\n        await ctx.send(f\"Playing in {voice.channel.name}.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nplays audio from youtube_dl - compatible sources.", "response": "async def jsk_vc_youtube_dl(self, ctx: commands.Context, *, url: str):\n        \"\"\"\n        Plays audio from youtube_dl-compatible sources.\n        \"\"\"\n\n        if not youtube_dl:\n            return await ctx.send(\"youtube_dl is not installed.\")\n\n        voice = ctx.guild.voice_client\n\n        if voice.is_playing():\n            voice.stop()\n\n        # remove embed maskers if present\n        url = url.lstrip(\"<\").rstrip(\">\")\n\n        voice.play(discord.PCMVolumeTransformer(BasicYouTubeDLSource(url)))\n        await ctx.send(f\"Playing in {voice.channel.name}.\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntry to work out the highlight. js language of a given file name or an empty string if none match. Returns an empty string if no match.", "response": "def get_language(query: str) -> str:\n    \"\"\"Tries to work out the highlight.js language of a given file name or\n    shebang. Returns an empty string if none match.\n    \"\"\"\n    query = query.lower()\n    for language in LANGUAGES:\n        if query.endswith(language):\n            return language\n    return ''"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntry to find bot extensions in a directory.", "response": "def find_extensions_in(path: typing.Union[str, pathlib.Path]) -> list:\n    \"\"\"\n    Tries to find things that look like bot extensions in a directory.\n    \"\"\"\n\n    if not isinstance(path, pathlib.Path):\n        path = pathlib.Path(path)\n\n    if not path.is_dir():\n        return []\n\n    extension_names = []\n\n    # Find extensions directly in this folder\n    for subpath in path.glob('*.py'):\n        parts = subpath.with_suffix('').parts\n        if parts[0] == '.':\n            parts = parts[1:]\n\n        extension_names.append('.'.join(parts))\n\n    # Find extensions as subfolder modules\n    for subpath in path.glob('*/__init__.py'):\n        parts = subpath.parent.parts\n        if parts[0] == '.':\n            parts = parts[1:]\n\n        extension_names.append('.'.join(parts))\n\n    return extension_names"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntrying to resolve extension queries into a list of extension names.", "response": "def resolve_extensions(bot: commands.Bot, name: str) -> list:\n    \"\"\"\n    Tries to resolve extension queries into a list of extension names.\n    \"\"\"\n\n    if name.endswith('.*'):\n        module_parts = name[:-2].split('.')\n        path = pathlib.Path(module_parts.pop(0))\n\n        for part in module_parts:\n            path = path / part\n\n        return find_extensions_in(path)\n\n    if name == '~':\n        return list(bot.extensions.keys())\n\n    return [name]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef package_version(package_name: str) -> typing.Optional[str]:\n\n    try:\n        return pkg_resources.get_distribution(package_name).version\n    except (pkg_resources.DistributionNotFound, AttributeError):\n        return None", "response": "Returns package version as a string or None if package_name is not found."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def vc_check(ctx: commands.Context):  # pylint: disable=unused-argument\n\n    if not discord.voice_client.has_nacl:\n        raise commands.CheckFailure(\"voice cannot be used because PyNaCl is not loaded\")\n\n    if not discord.opus.is_loaded():\n        raise commands.CheckFailure(\"voice cannot be used because libopus is not loaded\")\n\n    return True", "response": "Check whether VC is available in this bot."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck whether we are connected to VC in this guild.", "response": "async def connected_check(ctx: commands.Context):\n    \"\"\"\n    Check whether we are connected to VC in this guild.\n    \"\"\"\n\n    voice = ctx.guild.voice_client\n\n    if not voice or not voice.is_connected():\n        raise commands.CheckFailure(\"Not connected to VC in this guild\")\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def playing_check(ctx: commands.Context):\n\n    if await connected_check(ctx) and not ctx.guild.voice_client.is_playing():\n        raise commands.CheckFailure(\"The voice client in this guild is not playing anything.\")\n\n    return True", "response": "Checks whether we are playing audio in VC in this guild."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef safe_format(format_string, *args, **kwargs):\n    try:\n        if not args and not kwargs:\n            return format_string\n        else:\n            return format_string.format(*args, **kwargs)\n\n    # catch encoding errors and transform everything into utf-8 string\n    # before logging:\n    except (UnicodeEncodeError, UnicodeDecodeError):\n        format_string = to_utf8(format_string)\n        args = [to_utf8(p) for p in args]\n        kwargs = {k: to_utf8(v) for k, v in six.iteritems(kwargs)}\n        return format_string.format(*args, **kwargs)\n\n    # ignore other errors\n    except:\n        return u''", "response": "Helper function to format a string with any combination of bytestrings and unicode strings without raising exceptions\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_unicode(str_or_unicode, precise=False):\n    if not isinstance(str_or_unicode, six.text_type):\n        encoding = quick_detect_encoding(str_or_unicode) if precise else 'utf-8'\n        return six.text_type(str_or_unicode, encoding, 'replace')\n    return str_or_unicode", "response": "Safely returns a unicode version of a given string"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef detect_encoding(string):\n    assert isinstance(string, bytes)\n    try:\n        detected = chardet.detect(string)\n        if detected:\n            return detected.get('encoding') or 'utf-8'\n    except Exception as e:\n        pass\n    return 'utf-8'", "response": "Detects the encoding of the passed string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntrying to detect the encoding of the passed string.", "response": "def quick_detect_encoding(string):\n    \"\"\"\n    Tries to detect the encoding of the passed string.\n\n    Uses cchardet. Fallbacks to detect_encoding.\n    \"\"\"\n    assert isinstance(string, bytes)\n    try:\n        detected = cchardet.detect(string)\n        if detected:\n            return detected.get('encoding') or detect_encoding(string)\n    except Exception as e:\n        pass\n    return detect_encoding(string)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef html_fromstring(s):\n    if isinstance(s, six.text_type):\n        s = s.encode('utf8')\n    try:\n        if html_too_big(s):\n            return None\n\n        return html5parser.fromstring(s, parser=_html5lib_parser())\n    except Exception:\n        pass", "response": "Parse html tree from string. Return None if the string can t be parsed."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing html tree from string. Return None if the string can t be parsed.", "response": "def html_document_fromstring(s):\n    \"\"\"Parse html tree from string. Return None if the string can't be parsed.\n    \"\"\"\n    if isinstance(s, six.text_type):\n        s = s.encode('utf8')\n    try:\n        if html_too_big(s):\n            return None\n\n        return html5parser.document_fromstring(s, parser=_html5lib_parser())\n    except Exception:\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef features(sender=''):\n    '''Returns a list of signature features.'''\n    return [\n        # This one isn't from paper.\n        # Meant to match companies names, sender's names, address.\n        many_capitalized_words,\n        # This one is not from paper.\n        # Line is too long.\n        # This one is less aggressive than `Line is too short`\n        lambda line: 1 if len(line) > TOO_LONG_SIGNATURE_LINE else 0,\n        # Line contains email pattern.\n        binary_regex_search(RE_EMAIL),\n        # Line contains url.\n        binary_regex_search(RE_URL),\n        # Line contains phone number pattern.\n        binary_regex_search(RE_RELAX_PHONE),\n        # Line matches the regular expression \"^[\\s]*---*[\\s]*$\".\n        binary_regex_match(RE_SEPARATOR),\n        # Line has a sequence of 10 or more special characters.\n        binary_regex_search(RE_SPECIAL_CHARS),\n        # Line contains any typical signature words.\n        binary_regex_search(RE_SIGNATURE_WORDS),\n        # Line contains a pattern like Vitor R. Carvalho or William W. Cohen.\n        binary_regex_search(RE_NAME),\n        # Percentage of punctuation symbols in the line is larger than 50%\n        lambda line: 1 if punctuation_percent(line) > 50 else 0,\n        # Percentage of punctuation symbols in the line is larger than 90%\n        lambda line: 1 if punctuation_percent(line) > 90 else 0,\n        contains_sender_names(sender)\n        ]", "response": "Returns a list of signature features."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\napply features to message body lines. Returns list of lists. Each of the lists corresponds to the message body line.", "response": "def apply_features(body, features):\n    '''Applies features to message body lines.\n\n    Returns list of lists. Each of the lists corresponds to the body line\n    and is constituted by the numbers of features occurrences (0 or 1).\n    E.g. if element j of list i equals 1 this means that\n    feature j occurred in line i (counting from the last line of the body).\n    '''\n    # collect all non empty lines\n    lines = [line for line in body.splitlines() if line.strip()]\n\n    # take the last SIGNATURE_MAX_LINES\n    last_lines = lines[-SIGNATURE_MAX_LINES:]\n\n    # apply features, fallback to zeros\n    return ([[f(line) for f in features] for line in last_lines] or\n            [[0 for f in features]])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build_pattern(body, features):\n    '''Converts body into a pattern i.e. a point in the features space.\n\n    Applies features to the body lines and sums up the results.\n    Elements of the pattern indicate how many times a certain feature occurred\n    in the last lines of the body.\n    '''\n    line_patterns = apply_features(body, features)\n    return reduce(lambda x, y: [i + j for i, j in zip(x, y)], line_patterns)", "response": "Converts a body into a pattern i. e. a point in the features space."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef extract_signature(msg_body):\n    '''\n    Analyzes message for a presence of signature block (by common patterns)\n    and returns tuple with two elements: message text without signature block\n    and the signature itself.\n\n    >>> extract_signature('Hey man! How r u?\\n\\n--\\nRegards,\\nRoman')\n    ('Hey man! How r u?', '--\\nRegards,\\nRoman')\n\n    >>> extract_signature('Hey man!')\n    ('Hey man!', None)\n    '''\n    try:\n        # identify line delimiter first\n        delimiter = get_delimiter(msg_body)\n\n        # make an assumption\n        stripped_body = msg_body.strip()\n        phone_signature = None\n\n        # strip off phone signature\n        phone_signature = RE_PHONE_SIGNATURE.search(msg_body)\n        if phone_signature:\n            stripped_body = stripped_body[:phone_signature.start()]\n            phone_signature = phone_signature.group()\n\n        # decide on signature candidate\n        lines = stripped_body.splitlines()\n        candidate = get_signature_candidate(lines)\n        candidate = delimiter.join(candidate)\n\n        # try to extract signature\n        signature = RE_SIGNATURE.search(candidate)\n        if not signature:\n            return (stripped_body.strip(), phone_signature)\n        else:\n            signature = signature.group()\n            # when we splitlines() and then join them\n            # we can lose a new line at the end\n            # we did it when identifying a candidate\n            # so we had to do it for stripped_body now\n            stripped_body = delimiter.join(lines)\n            stripped_body = stripped_body[:-len(signature)]\n\n            if phone_signature:\n                signature = delimiter.join([signature, phone_signature])\n\n            return (stripped_body.strip(),\n                    signature.strip())\n    except Exception:\n        log.exception('ERROR extracting signature')\n        return (msg_body, None)", "response": "Extract the signature from the message body."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns lines that could hold signature", "response": "def get_signature_candidate(lines):\n    \"\"\"Return lines that could hold signature\n\n    The lines should:\n\n    * be among last SIGNATURE_MAX_LINES non-empty lines.\n    * not include first line\n    * be shorter than TOO_LONG_SIGNATURE_LINE\n    * not include more than one line that starts with dashes\n    \"\"\"\n    # non empty lines indexes\n    non_empty = [i for i, line in enumerate(lines) if line.strip()]\n\n    # if message is empty or just one line then there is no signature\n    if len(non_empty) <= 1:\n        return []\n\n    # we don't expect signature to start at the 1st line\n    candidate = non_empty[1:]\n    # signature shouldn't be longer then SIGNATURE_MAX_LINES\n    candidate = candidate[-SIGNATURE_MAX_LINES:]\n\n    markers = _mark_candidate_indexes(lines, candidate)\n    candidate = _process_marked_candidate_indexes(candidate, markers)\n\n    # get actual lines for the candidate instead of indexes\n    if candidate:\n        candidate = lines[candidate[0]:]\n        return candidate\n\n    return []"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmarks candidate indexes with markers", "response": "def _mark_candidate_indexes(lines, candidate):\n    \"\"\"Mark candidate indexes with markers\n\n    Markers:\n\n    * c - line that could be a signature line\n    * l - long line\n    * d - line that starts with dashes but has other chars as well\n\n    >>> _mark_candidate_lines(['Some text', '', '-', 'Bob'], [0, 2, 3])\n    'cdc'\n    \"\"\"\n    # at first consider everything to be potential signature lines\n    markers = list('c' * len(candidate))\n\n    # mark lines starting from bottom up\n    for i, line_idx in reversed(list(enumerate(candidate))):\n        if len(lines[line_idx].strip()) > TOO_LONG_SIGNATURE_LINE:\n            markers[i] = 'l'\n        else:\n            line = lines[line_idx].strip()\n            if line.startswith('-') and line.strip(\"-\"):\n                markers[i] = 'd'\n\n    return \"\".join(markers)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprocessing marked candidate indexes.", "response": "def _process_marked_candidate_indexes(candidate, markers):\n    \"\"\"\n    Run regexes against candidate's marked indexes to strip\n    signature candidate.\n\n    >>> _process_marked_candidate_indexes([9, 12, 14, 15, 17], 'clddc')\n    [15, 17]\n    \"\"\"\n    match = RE_SIGNATURE_CANDIDATE.match(markers[::-1])\n    return candidate[-match.end('candidate'):] if match else []"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a function to search sender s name or it s part.", "response": "def contains_sender_names(sender):\n    '''Returns a functions to search sender\\'s name or it\\'s part.\n\n    >>> feature = contains_sender_names(\"Sergey N.  Obukhov <xxx@example.com>\")\n    >>> feature(\"Sergey Obukhov\")\n    1\n    >>> feature(\"BR, Sergey N.\")\n    1\n    >>> feature(\"Sergey\")\n    1\n    >>> contains_sender_names(\"<serobnic@mail.ru>\")(\"Serobnic\")\n    1\n    >>> contains_sender_names(\"<serobnic@mail.ru>\")(\"serobnic\")\n    1\n    '''\n    names = '( |$)|'.join(flatten_list([[e, e.capitalize()]\n                                        for e in extract_names(sender)]))\n    names = names or sender\n    if names != '':\n        return binary_regex_search(re.compile(names))\n    return lambda s: 0"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef extract_names(sender):\n    sender = to_unicode(sender, precise=True)\n    # Remove non-alphabetical characters\n    sender = \"\".join([char if char.isalpha() else ' ' for char in sender])\n    # Remove too short words and words from \"black\" list i.e.\n    # words like `ru`, `gmail`, `com`, `org`, etc.\n    sender = [word for word in sender.split() if len(word) > 1 and\n              not word in BAD_SENDER_NAMES]\n    # Remove duplicates\n    names = list(set(sender))\n    return names", "response": "Tries to extract names from sender s names from From header."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef categories_percent(s, categories):\n    '''Returns category characters percent.\n\n    >>> categories_percent(\"qqq ggg hhh\", [\"Po\"])\n    0.0\n    >>> categories_percent(\"q,w.\", [\"Po\"])\n    50.0\n    >>> categories_percent(\"qqq ggg hhh\", [\"Nd\"])\n    0.0\n    >>> categories_percent(\"q5\", [\"Nd\"])\n    50.0\n    >>> categories_percent(\"s.s,5s\", [\"Po\", \"Nd\"])\n    50.0\n    '''\n    count = 0\n    s = to_unicode(s, precise=True)\n    for c in s:\n        if unicodedata.category(c) in categories:\n            count += 1\n    return 100 * float(count) / len(s) if len(s) else 0", "response": "Returns category characters percent."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns capitalized words percent.", "response": "def capitalized_words_percent(s):\n    '''Returns capitalized words percent.'''\n    s = to_unicode(s, precise=True)\n    words = re.split('\\s', s)\n    words = [w for w in words if w.strip()]\n    words = [w for w in words if len(w) > 2]    \n    capitalized_words_counter = 0\n    valid_words_counter = 0\n    for word in words:\n        if not INVALID_WORD_START.match(word):\n            valid_words_counter += 1\n            if word[0].isupper() and not word[1].isupper():\n                capitalized_words_counter += 1\n    if valid_words_counter > 0 and len(words) > 1:\n        return 100 * float(capitalized_words_counter) / valid_words_counter\n\n    return 0"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if the body has signature. Returns True or False.", "response": "def has_signature(body, sender):\n    '''Checks if the body has signature. Returns True or False.'''\n    non_empty = [line for line in body.splitlines() if line.strip()]\n    candidate = non_empty[-SIGNATURE_MAX_LINES:]\n    upvotes = 0\n    for line in candidate:\n        # we check lines for sender's name, phone, email and url,\n        # those signature lines don't take more then 27 lines\n        if len(line.strip()) > 27:\n            continue\n        elif contains_sender_names(sender)(line):\n            return True\n        elif (binary_regex_search(RE_RELAX_PHONE)(line) +\n              binary_regex_search(RE_EMAIL)(line) +\n              binary_regex_search(RE_URL)(line) == 1):\n            upvotes += 1\n    if upvotes > 1:\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove initial spaces in each line before marking message lines.", "response": "def remove_initial_spaces_and_mark_message_lines(lines):\n    \"\"\"\n    Removes the initial spaces in each line before marking message lines.\n\n    This ensures headers can be identified if they are indented with spaces.\n    \"\"\"\n    i = 0\n    while i < len(lines):\n        lines[i] = lines[i].lstrip(' ')\n        i += 1\n    return mark_message_lines(lines)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmarks message lines with markers to distinguish quotation lines.", "response": "def mark_message_lines(lines):\n    \"\"\"Mark message lines with markers to distinguish quotation lines.\n\n    Markers:\n\n    * e - empty line\n    * m - line that starts with quotation marker '>'\n    * s - splitter line\n    * t - presumably lines from the last message in the conversation\n\n    >>> mark_message_lines(['answer', 'From: foo@bar.com', '', '> question'])\n    'tsem'\n    \"\"\"\n    markers = ['e' for _ in lines]\n    i = 0\n    while i < len(lines):\n        if not lines[i].strip():\n            markers[i] = 'e'  # empty line\n        elif QUOT_PATTERN.match(lines[i]):\n            markers[i] = 'm'  # line with quotation marker\n        elif RE_FWD.match(lines[i]):\n            markers[i] = 'f'  # ---- Forwarded message ----\n        else:\n            # in case splitter is spread across several lines\n            splitter = is_splitter('\\n'.join(lines[i:i + SPLITTER_MAX_LINES]))\n\n            if splitter:\n                # append as many splitter markers as lines in splitter\n                splitter_lines = splitter.group().splitlines()\n                for j in range(len(splitter_lines)):\n                    markers[i + j] = 's'\n\n                # skip splitter lines\n                i += len(splitter_lines) - 1\n            else:\n                # probably the line from the last message in the conversation\n                markers[i] = 't'\n        i += 1\n\n    return ''.join(markers)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprocesses the lines in the message s marked lines and return only the last message lines.", "response": "def process_marked_lines(lines, markers, return_flags=[False, -1, -1]):\n    \"\"\"Run regexes against message's marked lines to strip quotations.\n\n    Return only last message lines.\n    >>> mark_message_lines(['Hello', 'From: foo@bar.com', '', '> Hi', 'tsem'])\n    ['Hello']\n\n    Also returns return_flags.\n    return_flags = [were_lines_deleted, first_deleted_line,\n                    last_deleted_line]\n    \"\"\"\n    markers = ''.join(markers)\n    # if there are no splitter there should be no markers\n    if 's' not in markers and not re.search('(me*){3}', markers):\n        markers = markers.replace('m', 't')\n\n    if re.match('[te]*f', markers):\n        return_flags[:] = [False, -1, -1]\n        return lines\n\n    # inlined reply\n    # use lookbehind assertions to find overlapping entries e.g. for 'mtmtm'\n    # both 't' entries should be found\n    for inline_reply in re.finditer('(?<=m)e*(t[te]*)m', markers):\n        # long links could break sequence of quotation lines but they shouldn't\n        # be considered an inline reply\n        links = (\n            RE_PARENTHESIS_LINK.search(lines[inline_reply.start() - 1]) or\n            RE_PARENTHESIS_LINK.match(lines[inline_reply.start()].strip()))\n        if not links:\n            return_flags[:] = [False, -1, -1]\n            return lines\n\n    # cut out text lines coming after splitter if there are no markers there\n    quotation = re.search('(se*)+((t|f)+e*)+', markers)\n    if quotation:\n        return_flags[:] = [True, quotation.start(), len(lines)]\n        return lines[:quotation.start()]\n\n    # handle the case with markers\n    quotation = (RE_QUOTATION.search(markers) or\n                 RE_EMPTY_QUOTATION.search(markers))\n\n    if quotation:\n        return_flags[:] = True, quotation.start(1), quotation.end(1)\n        return lines[:quotation.start(1)] + lines[quotation.end(1):]\n\n    return_flags[:] = [False, -1, -1]\n    return lines"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npreparing msg_body for being stripped.", "response": "def preprocess(msg_body, delimiter, content_type='text/plain'):\n    \"\"\"Prepares msg_body for being stripped.\n\n    Replaces link brackets so that they couldn't be taken for quotation marker.\n    Splits line in two if splitter pattern preceded by some text on the same\n    line (done only for 'On <date> <person> wrote:' pattern).\n\n    Converts msg_body into a unicode.\n    \"\"\"\n    msg_body = _replace_link_brackets(msg_body)\n\n    msg_body = _wrap_splitter_with_newline(msg_body, delimiter, content_type)\n\n    return msg_body"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _replace_link_brackets(msg_body):\n    if isinstance(msg_body, bytes):\n        msg_body = msg_body.decode('utf8')\n\n    def link_wrapper(link):\n        newline_index = msg_body[:link.start()].rfind(\"\\n\")\n        if msg_body[newline_index + 1] == \">\":\n            return link.group()\n        else:\n            return \"@@%s@@\" % link.group(1)\n\n    msg_body = re.sub(RE_LINK, link_wrapper, msg_body)\n    return msg_body", "response": "Normalize links i. e. replace < > wrapping the link with some symbols\n    so that > closing the link can be mistakenly taken for quotation\n    marker."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrap a splitter with a new line.", "response": "def _wrap_splitter_with_newline(msg_body, delimiter, content_type='text/plain'):\n    \"\"\"\n    Splits line in two if splitter pattern preceded by some text on the same\n    line (done only for 'On <date> <person> wrote:' pattern.\n    \"\"\"\n    def splitter_wrapper(splitter):\n        \"\"\"Wraps splitter with new line\"\"\"\n        if splitter.start() and msg_body[splitter.start() - 1] != '\\n':\n            return '%s%s' % (delimiter, splitter.group())\n        else:\n            return splitter.group()\n\n    if content_type == 'text/plain':\n        msg_body = re.sub(RE_ON_DATE_SMB_WROTE, splitter_wrapper, msg_body)\n\n    return msg_body"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nextracts a non quoted message from a plain text.", "response": "def extract_from_plain(msg_body):\n    \"\"\"Extracts a non quoted message from provided plain text.\"\"\"\n    stripped_text = msg_body\n\n    delimiter = get_delimiter(msg_body)\n    msg_body = preprocess(msg_body, delimiter)\n    # don't process too long messages\n    lines = msg_body.splitlines()[:MAX_LINES_COUNT]\n    markers = mark_message_lines(lines)\n    lines = process_marked_lines(lines, markers)\n\n    # concatenate lines, change links back, strip and return\n    msg_body = delimiter.join(lines)\n    msg_body = postprocess(msg_body)\n    return msg_body"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nextract not quoted message from provided html message body.", "response": "def extract_from_html(msg_body):\n    \"\"\"\n    Extract not quoted message from provided html message body\n    using tags and plain text algorithm.\n\n    Cut out the 'blockquote', 'gmail_quote' tags.\n    Cut Microsoft quotations.\n\n    Then use plain text algorithm to cut out splitter or\n    leftover quotation.\n    This works by adding checkpoint text to all html tags,\n    then converting html to text,\n    then extracting quotations from text,\n    then checking deleted checkpoints,\n    then deleting necessary tags.\n\n    Returns a unicode string.\n    \"\"\"\n    if isinstance(msg_body, six.text_type):\n        msg_body = msg_body.encode('utf8')\n    elif not isinstance(msg_body, bytes):\n        msg_body = msg_body.encode('ascii')\n\n    result = _extract_from_html(msg_body)\n    if isinstance(result, bytes):\n        result = result.decode('utf8')\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nextracts not quoted message from provided html message body.", "response": "def _extract_from_html(msg_body):\n    \"\"\"\n    Extract not quoted message from provided html message body\n    using tags and plain text algorithm.\n\n    Cut out first some encoding html tags such as xml and doctype\n    for avoiding conflict with unicode decoding\n\n    Cut out the 'blockquote', 'gmail_quote' tags.\n    Cut Microsoft quotations.\n\n    Then use plain text algorithm to cut out splitter or\n    leftover quotation.\n    This works by adding checkpoint text to all html tags,\n    then converting html to text,\n    then extracting quotations from text,\n    then checking deleted checkpoints,\n    then deleting necessary tags.\n    \"\"\"\n    if msg_body.strip() == b'':\n        return msg_body\n\n    msg_body = msg_body.replace(b'\\r\\n', b'\\n')\n\n    msg_body = re.sub(r\"\\<\\?xml.+\\?\\>|\\<\\!DOCTYPE.+]\\>\", \"\", msg_body)\n\n    html_tree = html_document_fromstring(msg_body)\n\n    if html_tree is None:\n        return msg_body\n\n    cut_quotations = (html_quotations.cut_gmail_quote(html_tree) or\n                      html_quotations.cut_zimbra_quote(html_tree) or\n                      html_quotations.cut_blockquote(html_tree) or\n                      html_quotations.cut_microsoft_quote(html_tree) or\n                      html_quotations.cut_by_id(html_tree) or\n                      html_quotations.cut_from_block(html_tree)\n                      )\n    html_tree_copy = deepcopy(html_tree)\n\n    number_of_checkpoints = html_quotations.add_checkpoint(html_tree, 0)\n    quotation_checkpoints = [False] * number_of_checkpoints\n    plain_text = html_tree_to_text(html_tree)\n    plain_text = preprocess(plain_text, '\\n', content_type='text/html')\n    lines = plain_text.splitlines()\n\n    # Don't process too long messages\n    if len(lines) > MAX_LINES_COUNT:\n        return msg_body\n\n    # Collect checkpoints on each line\n    line_checkpoints = [\n        [int(i[4:-4])  # Only checkpoint number\n         for i in re.findall(html_quotations.CHECKPOINT_PATTERN, line)]\n        for line in lines]\n\n    # Remove checkpoints\n    lines = [re.sub(html_quotations.CHECKPOINT_PATTERN, '', line)\n             for line in lines]\n\n    # Use plain text quotation extracting algorithm\n    markers = mark_message_lines(lines)\n    return_flags = []\n    process_marked_lines(lines, markers, return_flags)\n    lines_were_deleted, first_deleted, last_deleted = return_flags\n\n    if not lines_were_deleted and not cut_quotations:\n        return msg_body\n\n    if lines_were_deleted:\n        #collect checkpoints from deleted lines\n        for i in range(first_deleted, last_deleted):\n            for checkpoint in line_checkpoints[i]:\n                quotation_checkpoints[checkpoint] = True\n\n        # Remove tags with quotation checkpoints\n        html_quotations.delete_quotation_tags(\n            html_tree_copy, 0, quotation_checkpoints\n        )\n\n    if _readable_text_empty(html_tree_copy):\n        return msg_body\n\n    return html.tostring(html_tree_copy)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive a message that may consist of multiple emails return a list of markers that can be used to split the message into multiple header blocks.", "response": "def split_emails(msg):\n    \"\"\"\n    Given a message (which may consist of an email conversation thread with\n    multiple emails), mark the lines to identify split lines, content lines and\n    empty lines.\n\n    Correct the split line markers inside header blocks. Header blocks are\n    identified by the regular expression RE_HEADER.\n\n    Return the corrected markers\n    \"\"\"\n    msg_body = _replace_link_brackets(msg)\n\n    # don't process too long messages\n    lines = msg_body.splitlines()[:MAX_LINES_COUNT]\n    markers = remove_initial_spaces_and_mark_message_lines(lines)\n\n    markers = _mark_quoted_email_splitlines(markers, lines)\n\n    # we don't want splitlines in header blocks\n    markers = _correct_splitlines_in_headers(markers, lines)\n\n    return markers"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncorrect markers by removing splitlines deemed to be inside header blocks.", "response": "def _correct_splitlines_in_headers(markers, lines):\n    \"\"\"\n    Corrects markers by removing splitlines deemed to be inside header blocks.\n    \"\"\"\n    updated_markers = \"\"\n    i = 0\n    in_header_block = False\n    for m in markers:\n        # Only set in_header_block flag when we hit an 's' and line is a header\n        if m == 's':\n            if not in_header_block:\n                if bool(re.search(RE_HEADER, lines[i])):\n                    in_header_block = True\n            else:\n                if QUOT_PATTERN.match(lines[i]):\n                    m = 'm'\n                else:\n                    m = 't'\n\n        # If the line is not a header line, set in_header_block false.\n        if not bool(re.search(RE_HEADER, lines[i])):\n            in_header_block = False\n\n        # Add the marker to the new updated markers string.\n        updated_markers += m\n        i += 1\n\n    return updated_markers"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_splitter(line):\n    '''\n    Returns Matcher object if provided string is a splitter and\n    None otherwise.\n    '''\n    for pattern in SPLITTER_PATTERNS:\n        matcher = re.match(pattern, line)\n        if matcher:\n            return matcher", "response": "Returns Matcher object if provided string is a splitter and None otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_signature_line(line, sender, classifier):\n    '''Checks if the line belongs to signature. Returns True or False.'''\n    data = numpy.array(build_pattern(line, features(sender))).reshape(1, -1)\n    return classifier.predict(data) > 0", "response": "Checks if the line belongs to signature. Returns True or False."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstrips signature from the body of the message and returns the stripped body and signature as a tuple.", "response": "def extract(body, sender):\n    \"\"\"Strips signature from the body of the message.\n\n    Returns stripped body and signature as a tuple.\n    If no signature is found the corresponding returned value is None.\n    \"\"\"\n    try:\n        delimiter = get_delimiter(body)\n\n        body = body.strip()\n\n        if has_signature(body, sender):\n            lines = body.splitlines()\n\n            markers = _mark_lines(lines, sender)\n            text, signature = _process_marked_lines(lines, markers)\n\n            if signature:\n                text = delimiter.join(text)\n                if text.strip():\n                    return (text, delimiter.join(signature))\n    except Exception as e:\n        log.exception('ERROR when extracting signature with classifiers')\n\n    return (body, None)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmark message lines with markers to distinguish signature lines.", "response": "def _mark_lines(lines, sender):\n    \"\"\"Mark message lines with markers to distinguish signature lines.\n\n    Markers:\n\n    * e - empty line\n    * s - line identified as signature\n    * t - other i.e. ordinary text line\n\n    >>> mark_message_lines(['Some text', '', 'Bob'], 'Bob')\n    'tes'\n    \"\"\"\n    global EXTRACTOR\n\n    candidate = get_signature_candidate(lines)\n\n    # at first consider everything to be text no signature\n    markers = list('t' * len(lines))\n\n    # mark lines starting from bottom up\n    # mark only lines that belong to candidate\n    # no need to mark all lines of the message\n    for i, line in reversed(list(enumerate(candidate))):\n        # markers correspond to lines not candidate\n        # so we need to recalculate our index to be\n        # relative to lines not candidate\n        j = len(lines) - len(candidate) + i\n        if not line.strip():\n            markers[j] = 'e'\n        elif is_signature_line(line, sender, EXTRACTOR):\n            markers[j] = 's'\n\n    return \"\".join(markers)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprocess the marked lines and return the message s lines.", "response": "def _process_marked_lines(lines, markers):\n    \"\"\"Run regexes against message's marked lines to strip signature.\n\n    >>> _process_marked_lines(['Some text', '', 'Bob'], 'tes')\n    (['Some text', ''], ['Bob'])\n    \"\"\"\n    # reverse lines and match signature pattern for reversed lines\n    signature = RE_REVERSE_SIGNATURE.match(markers[::-1])\n    if signature:\n        return (lines[:-signature.end()], lines[-signature.end():])\n\n    return (lines, None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntrains and saves classifier to save_classifier_filename.", "response": "def train(classifier, train_data_filename, save_classifier_filename=None):\n    \"\"\"Trains and saves classifier so that it could be easily loaded later.\"\"\"\n    file_data = genfromtxt(train_data_filename, delimiter=\",\")\n    train_data, labels = file_data[:, :-1], file_data[:, -1]\n    classifier.fit(train_data, labels)\n\n    if save_classifier_filename:\n        joblib.dump(classifier, save_classifier_filename)\n    return classifier"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_msg_sender(filename, sender_known=True):\n    import sys\n    kwargs = {}\n    if sys.version_info > (3, 0):\n        kwargs[\"encoding\"] = \"utf8\"\n\n    sender, msg = None, None\n    if os.path.isfile(filename) and not is_sender_filename(filename):\n        with open(filename, **kwargs) as f:\n            msg = f.read()\n            sender = u''\n            if sender_known:\n                sender_filename = build_sender_filename(filename)\n                if os.path.exists(sender_filename):\n                    with open(sender_filename) as sender_file:\n                        sender = sender_file.read().strip()\n                else:\n                    # if sender isn't found then the next line fails\n                    # and it is ok\n                    lines = msg.splitlines()\n                    for line in lines:\n                        match = re.match('From:(.*)', line)\n                        if match:\n                            sender = match.group(1)\n                            break\n    return (sender, msg)", "response": "Given a filename returns the sender and the message."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbuild signature detection dataset.", "response": "def build_detection_class(folder, dataset_filename,\n                          label, sender_known=True):\n    \"\"\"Builds signature detection class.\n\n    Signature detection dataset includes patterns for two classes:\n    * class for positive patterns (goes with label 1)\n    * class for negative patterns (goes with label -1)\n\n    The patterns are build of emails from `folder` and appended to\n    dataset file.\n\n    >>> build_signature_detection_class('emails/P', 'train.data', 1)\n    \"\"\"\n    with open(dataset_filename, 'a') as dataset:\n        for filename in os.listdir(folder):\n            filename = os.path.join(folder, filename)\n            sender, msg = parse_msg_sender(filename, sender_known)\n            if sender is None or msg is None:\n                continue\n            msg = re.sub('|'.join(ANNOTATIONS), '', msg)\n            X = build_pattern(msg, features(sender))\n            X.append(label)\n            labeled_pattern = ','.join([str(e) for e in X])\n            dataset.write(labeled_pattern + '\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build_detection_dataset(folder, dataset_filename,\n                            sender_known=True):\n    \"\"\"Builds signature detection dataset using emails from folder.\n\n    folder should have the following structure:\n    x-- folder\n    |    x-- P\n    |    |    | -- positive sample email 1\n    |    |    | -- positive sample email 2\n    |    |    | -- ...\n    |    x-- N\n    |    |    | -- negative sample email 1\n    |    |    | -- negative sample email 2\n    |    |    | -- ...\n\n    If the dataset file already exist it is rewritten.\n    \"\"\"\n    if os.path.exists(dataset_filename):\n        os.remove(dataset_filename)\n    build_detection_class(os.path.join(folder, u'P'),\n                          dataset_filename, 1)\n    build_detection_class(os.path.join(folder, u'N'),\n                          dataset_filename, -1)", "response": "Builds signature detection dataset using emails from folder."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build_extraction_dataset(folder, dataset_filename,\n                             sender_known=True):\n    \"\"\"Builds signature extraction dataset using emails in the `folder`.\n\n    The emails in the `folder` should be annotated i.e. signature lines\n    should be marked with `#sig#`.\n    \"\"\"\n    if os.path.exists(dataset_filename):\n        os.remove(dataset_filename)\n    with open(dataset_filename, 'a') as dataset:\n        for filename in os.listdir(folder):\n            filename = os.path.join(folder, filename)\n            sender, msg = parse_msg_sender(filename, sender_known)\n            if not sender or not msg:\n                continue\n            lines = msg.splitlines()\n            for i in range(1, min(SIGNATURE_MAX_LINES,\n                                  len(lines)) + 1):\n                line = lines[-i]\n                label = -1\n                if line[:len(SIGNATURE_ANNOTATION)] == \\\n                        SIGNATURE_ANNOTATION:\n                    label = 1\n                    line = line[len(SIGNATURE_ANNOTATION):]\n                elif line[:len(REPLY_ANNOTATION)] == REPLY_ANNOTATION:\n                    line = line[len(REPLY_ANNOTATION):]\n\n                X = build_pattern(line, features(sender))\n                X.append(label)\n                labeled_pattern = ','.join([str(e) for e in X])\n                dataset.write(labeled_pattern + '\\n')", "response": "Builds signature extraction dataset using emails in the folder."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_checkpoint(html_note, counter):\n    if html_note.text:\n        html_note.text = (html_note.text + CHECKPOINT_PREFIX +\n                          str(counter) + CHECKPOINT_SUFFIX)\n    else:\n        html_note.text = (CHECKPOINT_PREFIX + str(counter) +\n                          CHECKPOINT_SUFFIX)\n    counter += 1\n\n    for child in html_note.iterchildren():\n        counter = add_checkpoint(child, counter)\n\n    if html_note.tail:\n        html_note.tail = (html_note.tail + CHECKPOINT_PREFIX +\n                          str(counter) + CHECKPOINT_SUFFIX)\n    else:\n        html_note.tail = (CHECKPOINT_PREFIX + str(counter) +\n                          CHECKPOINT_SUFFIX)\n    counter += 1\n\n    return counter", "response": "Recursively adds checkpoints to html tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delete_quotation_tags(html_note, counter, quotation_checkpoints):\n    tag_in_quotation = True\n\n    if quotation_checkpoints[counter]:\n        html_note.text = ''\n    else:\n        tag_in_quotation = False\n    counter += 1\n\n    quotation_children = []  # Children tags which are in quotation.\n    for child in html_note.iterchildren():\n        counter, child_tag_in_quotation = delete_quotation_tags(\n            child, counter,\n            quotation_checkpoints\n        )\n        if child_tag_in_quotation:\n            quotation_children.append(child)\n\n    if quotation_checkpoints[counter]:\n        html_note.tail = ''\n    else:\n        tag_in_quotation = False\n    counter += 1\n\n    if tag_in_quotation:\n        return counter, tag_in_quotation\n    else:\n        # Remove quotation children.\n        for child in quotation_children:\n            html_note.remove(child)\n        return counter, tag_in_quotation", "response": "Deletes tags with quotation checkpoints from html tree."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cut_gmail_quote(html_message):\n    ''' Cuts the outermost block element with class gmail_quote. '''\n    gmail_quote = cssselect('div.gmail_quote', html_message)\n    if gmail_quote and (gmail_quote[0].text is None or not RE_FWD.match(gmail_quote[0].text)):\n        gmail_quote[0].getparent().remove(gmail_quote[0])\n        return True", "response": "Cuts the outermost block element with class gmail_quote."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cut_microsoft_quote(html_message):\n    ''' Cuts splitter block and all following blocks. '''\n    #use EXSLT extensions to have a regex match() function with lxml\n    ns = {\"re\": \"http://exslt.org/regular-expressions\"}\n\n    #general pattern: @style='border:none;border-top:solid <color> 1.0pt;padding:3.0pt 0<unit> 0<unit> 0<unit>'\n    #outlook 2007, 2010 (international) <color=#B5C4DF> <unit=cm>\n    #outlook 2007, 2010 (american)      <color=#B5C4DF> <unit=pt>\n    #outlook 2013       (international) <color=#E1E1E1> <unit=cm>\n    #outlook 2013       (american)      <color=#E1E1E1> <unit=pt>\n    #also handles a variant with a space after the semicolon\n    splitter = html_message.xpath(\n        #outlook 2007, 2010, 2013 (international, american)\n        \"//div[@style[re:match(., 'border:none; ?border-top:solid #(E1E1E1|B5C4DF) 1.0pt; ?\"\n        \"padding:3.0pt 0(in|cm) 0(in|cm) 0(in|cm)')]]|\"\n        #windows mail\n        \"//div[@style='padding-top: 5px; \"\n        \"border-top-color: rgb(229, 229, 229); \"\n        \"border-top-width: 1px; border-top-style: solid;']\"\n        , namespaces=ns\n    )\n\n    if splitter:\n        splitter = splitter[0]\n        #outlook 2010\n        if splitter == splitter.getparent().getchildren()[0]:\n            splitter = splitter.getparent()\n    else:\n        #outlook 2003\n        splitter = html_message.xpath(\n            \"//div\"\n            \"/div[@class='MsoNormal' and @align='center' \"\n            \"and @style='text-align:center']\"\n            \"/font\"\n            \"/span\"\n            \"/hr[@size='3' and @width='100%' and @align='center' \"\n            \"and @tabindex='-1']\"\n        )\n        if len(splitter):\n            splitter = splitter[0]\n            splitter = splitter.getparent().getparent()\n            splitter = splitter.getparent().getparent()\n\n    if len(splitter):\n        parent = splitter.getparent()\n        after_splitter = splitter.getnext()\n        while after_splitter is not None:\n            parent.remove(after_splitter)\n            after_splitter = splitter.getnext()\n        parent.remove(splitter)\n        return True\n\n    return False", "response": "Cuts splitter block and all following blocks."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncut the last non - nested blockquote with wrapping elements.", "response": "def cut_blockquote(html_message):\n    ''' Cuts the last non-nested blockquote with wrapping elements.'''\n    quote = html_message.xpath(\n        '(.//blockquote)'\n        '[not(@class=\"gmail_quote\") and not(ancestor::blockquote)]'\n        '[last()]')\n\n    if quote:\n        quote = quote[0]\n        quote.getparent().remove(quote)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cut_from_block(html_message):\n    # handle the case when From: block is enclosed in some tag\n    block = html_message.xpath(\n        (\"//*[starts-with(mg:text_content(), 'From:')]|\"\n         \"//*[starts-with(mg:text_content(), 'Date:')]\"))\n\n    if block:\n        block = block[-1]\n        parent_div = None\n        while block.getparent() is not None:\n            if block.tag == 'div':\n                parent_div = block\n                break\n            block = block.getparent()\n        if parent_div is not None:\n            maybe_body = parent_div.getparent()\n            # In cases where removing this enclosing div will remove all\n            # content, we should assume the quote is not enclosed in a tag.\n            parent_div_is_all_content = (\n                maybe_body is not None and maybe_body.tag == 'body' and\n                len(maybe_body.getchildren()) == 1)\n\n            if not parent_div_is_all_content:\n                parent = block.getparent()\n                next_sibling = block.getnext()\n\n                # remove all tags after found From block\n                # (From block and quoted message are in separate divs)\n                while next_sibling is not None:\n                    parent.remove(block)\n                    block = next_sibling\n                    next_sibling = block.getnext()\n\n                # remove the last sibling (or the\n                # From block if no siblings)\n                if block is not None:\n                    parent.remove(block)\n\n                return True\n        else:\n            return False\n\n    # handle the case when From: block goes right after e.g. <hr>\n    # and not enclosed in some tag\n    block = html_message.xpath(\n        (\"//*[starts-with(mg:tail(), 'From:')]|\"\n         \"//*[starts-with(mg:tail(), 'Date:')]\"))\n    if block:\n        block = block[0]\n\n        if RE_FWD.match(block.getparent().text or ''):\n            return False\n        \n        while(block.getnext() is not None):\n            block.getparent().remove(block.getnext())\n        block.getparent().remove(block)\n        return True", "response": "Cuts div tag which wraps block starting with From :."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntakes a password and session_id and scramble them to send to the machine. copied from commpro.c - MakeKey", "response": "def make_commkey(key, session_id, ticks=50):\n    \"\"\"\n    take a password and session_id and scramble them to send to the machine.\n    copied from commpro.c - MakeKey\n    \"\"\"\n    key = int(key)\n    session_id = int(session_id)\n    k = 0\n    for i in range(32):\n        if (key & (1 << i)):\n            k = (k << 1 | 1)\n        else:\n            k = k << 1\n    k += session_id\n\n    k = pack(b'I', k)\n    k = unpack(b'BBBB', k)\n    k = pack(\n        b'BBBB',\n        k[0] ^ ord('Z'),\n        k[1] ^ ord('K'),\n        k[2] ^ ord('S'),\n        k[3] ^ ord('O'))\n    k = unpack(b'HH', k)\n    k = pack(b'HH', k[1], k[0])\n\n    B = 0xff & ticks\n    k = unpack(b'BBBB', k)\n    k = pack(\n        b'BBBB',\n        k[0] ^ B,\n        k[1] ^ B,\n        B,\n        k[3] ^ B)\n    return k"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates the TCP top packet.", "response": "def __create_tcp_top(self, packet):\n        \"\"\"\n        witch the complete packet set top header\n        \"\"\"\n        length = len(packet)\n        top = pack('<HHI', const.MACHINE_PREPARE_DATA_1, const.MACHINE_PREPARE_DATA_2, length)\n        return top + packet"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __create_header(self, command, command_string, session_id, reply_id):\n        buf = pack('<4H', command, 0, session_id, reply_id) + command_string\n        buf = unpack('8B' + '%sB' % len(command_string), buf)\n        checksum = unpack('H', self.__create_checksum(buf))[0]\n        reply_id += 1\n        if reply_id >= const.USHRT_MAX:\n            reply_id -= const.USHRT_MAX\n\n        buf = pack('<4H', command, checksum, session_id, reply_id)\n        return buf + command_string", "response": "Creates a header for the message."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate the checksum of the given packet.", "response": "def __create_checksum(self, p):\n        \"\"\"\n        Calculates the checksum of the packet to be sent to the time clock\n        Copied from zkemsdk.c\n        \"\"\"\n        l = len(p)\n        checksum = 0\n        while l > 1:\n            checksum += unpack('H', pack('BB', p[0], p[1]))[0]\n            p = p[2:]\n            if checksum > const.USHRT_MAX:\n                checksum -= const.USHRT_MAX\n            l -= 2\n        if l:\n            checksum = checksum + p[-1]\n\n        while checksum > const.USHRT_MAX:\n            checksum -= const.USHRT_MAX\n\n        checksum = ~checksum\n\n        while checksum < 0:\n            checksum += const.USHRT_MAX\n\n        return pack('H', checksum)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsend command to the terminal", "response": "def __send_command(self, command, command_string=b'', response_size=8):\n        \"\"\"\n        send command to the terminal\n        \"\"\"\n        if command not in [const.CMD_CONNECT, const.CMD_AUTH] and not self.is_connect:\n            raise ZKErrorConnection(\"instance are not connected.\")\n\n        buf = self.__create_header(command, command_string, self.__session_id, self.__reply_id)\n        try:\n            if self.tcp:\n                top = self.__create_tcp_top(buf)\n                self.__sock.send(top)\n                self.__tcp_data_recv = self.__sock.recv(response_size + 8)\n                self.__tcp_length = self.__test_tcp_top(self.__tcp_data_recv)\n                if self.__tcp_length == 0:\n                    raise ZKNetworkError(\"TCP packet invalid\")\n                self.__header = unpack('<4H', self.__tcp_data_recv[8:16])\n                self.__data_recv = self.__tcp_data_recv[8:]\n            else:\n                self.__sock.sendto(buf, self.__address)\n                self.__data_recv = self.__sock.recv(response_size)\n                self.__header = unpack('<4H', self.__data_recv[:8])\n        except Exception as e:\n            raise ZKNetworkError(str(e))\n\n        self.__response = self.__header[0]\n        self.__reply_id = self.__header[3]\n        self.__data = self.__data_recv[8:]\n        if self.__response in [const.CMD_ACK_OK, const.CMD_PREPARE_DATA, const.CMD_DATA]:\n            return {\n                'status': True,\n                'code': self.__response\n            }\n        return {\n            'status': False,\n            'code': self.__response\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __get_data_size(self):\n        response = self.__response\n        if response == const.CMD_PREPARE_DATA:\n            size = unpack('I', self.__data[:4])[0]\n            return size\n        else:\n            return 0", "response": "Checks a returned packet to see if it returned CMD_PREPARE_DATA and returns the amount of bytes that are going to be sent\n           "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndecoding a timestamp retrieved from the timeclock copied from zkemsdk. c - DecodeTime copied from zkemsdk. c - DecodeTime", "response": "def __decode_time(self, t):\n        \"\"\"\n        Decode a timestamp retrieved from the timeclock\n\n        copied from zkemsdk.c - DecodeTime\n        \"\"\"\n\n        t = unpack(\"<I\", t)[0]\n        second = t % 60\n        t = t // 60\n\n        minute = t % 60\n        t = t // 60\n\n        hour = t % 24\n        t = t // 24\n\n        day = t % 31 + 1\n        t = t // 31\n\n        month = t % 12 + 1\n        t = t // 12\n\n        year = t + 2000\n\n        d = datetime(year, month, day, hour, minute, second)\n\n        return d"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef __decode_timehex(self, timehex):\n        year, month, day, hour, minute, second = unpack(\"6B\", timehex)\n        year += 2000\n        d = datetime(year, month, day, hour, minute, second)\n        return d", "response": "decode timehex string of six bytes"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __encode_time(self, t):\n        # formula taken from zkemsdk.c - EncodeTime\n        # can also be found in the technical manual\n        d = (\n            ((t.year % 100) * 12 * 31 + ((t.month - 1) * 31) + t.day - 1) *\n            (24 * 60 * 60) + (t.hour * 60 + t.minute) * 60 + t.second\n        )\n        return d", "response": "Encode a timestamp so that it can be read on the timeclock\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef connect(self):\n        self.end_live_capture = False\n        if not self.ommit_ping and not self.helper.test_ping():\n            raise ZKNetworkError(\"can't reach device (ping %s)\" % self.__address[0])\n        if not self.force_udp and self.helper.test_tcp() == 0:\n            self.user_packet_size = 72 # default zk8\n        self.__create_socket()\n        self.__session_id = 0\n        self.__reply_id = const.USHRT_MAX - 1\n        cmd_response = self.__send_command(const.CMD_CONNECT)\n        self.__session_id = self.__header[2]\n        if cmd_response.get('code') == const.CMD_ACK_UNAUTH:\n            if self.verbose: print (\"try auth\")\n            command_string = make_commkey(self.__password, self.__session_id)\n            cmd_response = self.__send_command(const.CMD_AUTH, command_string)\n        if cmd_response.get('status'):\n            self.is_connect = True\n            return self\n        else:\n            if cmd_response[\"code\"] == const.CMD_ACK_UNAUTH:\n                raise ZKErrorResponse(\"Unauthenticated\")\n            if self.verbose: print (\"connect err response {} \".format(cmd_response[\"code\"]))\n            raise ZKErrorResponse(\"Invalid response: Can't connect\")", "response": "Connect to the device."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef disconnect(self):\n        cmd_response = self.__send_command(const.CMD_EXIT)\n        if cmd_response.get('status'):\n            self.is_connect = False\n            if self.__sock:\n                self.__sock.close()\n            return True\n        else:\n            raise ZKErrorResponse(\"can't disconnect\")", "response": "disconnect from the connected device"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef enable_device(self):\n        cmd_response = self.__send_command(const.CMD_ENABLEDEVICE)\n        if cmd_response.get('status'):\n            self.is_enabled = True\n            return True\n        else:\n            raise ZKErrorResponse(\"Can't enable device\")", "response": "Enable the connected device and allow user activity in device again"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndisabling the current lock on the device", "response": "def disable_device(self):\n        \"\"\"\n        disable (lock) device, to ensure no user activity in device while some process run\n\n        :return: bool\n        \"\"\"\n        cmd_response = self.__send_command(const.CMD_DISABLEDEVICE)\n        if cmd_response.get('status'):\n            self.is_enabled = False\n            return True\n        else:\n            raise ZKErrorResponse(\"Can't disable device\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the frimware version of the current node", "response": "def get_firmware_version(self):\n        \"\"\"\n        :return: the firmware version\n        \"\"\"\n        cmd_response = self.__send_command(const.CMD_GET_VERSION,b'', 1024)\n        if cmd_response.get('status'):\n            firmware_version = self.__data.split(b'\\x00')[0]\n            return firmware_version.decode()\n        else:\n            raise ZKErrorResponse(\"Can't read frimware version\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_serialnumber(self):\n        command = const.CMD_OPTIONS_RRQ\n        command_string = b'~SerialNumber\\x00'\n        response_size = 1024\n        cmd_response = self.__send_command(command, command_string, response_size)\n        if cmd_response.get('status'):\n            serialnumber = self.__data.split(b'=', 1)[-1].split(b'\\x00')[0]\n            serialnumber = serialnumber.replace(b'=', b'')\n            return serialnumber.decode() # string?\n        else:\n            raise ZKErrorResponse(\"Can't read serial number\")", "response": "Get the serial number of the current node."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_platform(self):\n        command = const.CMD_OPTIONS_RRQ\n        command_string = b'~Platform\\x00'\n        response_size = 1024\n\n        cmd_response = self.__send_command(command, command_string, response_size)\n        if cmd_response.get('status'):\n            platform = self.__data.split(b'=', 1)[-1].split(b'\\x00')[0]\n            platform = platform.replace(b'=', b'')\n            return platform.decode()\n        else:\n            raise ZKErrorResponse(\"Can't read platform name\")", "response": "Get the platform name of the current node."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_mac(self):\n        command = const.CMD_OPTIONS_RRQ\n        command_string = b'MAC\\x00'\n        response_size = 1024\n\n        cmd_response = self.__send_command(command, command_string, response_size)\n        if cmd_response.get('status'):\n            mac = self.__data.split(b'=', 1)[-1].split(b'\\x00')[0]\n            return mac.decode()\n        else:\n            raise ZKErrorResponse(\"can't read mac address\")", "response": "get the mac address of the current machine"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the device name :return: str", "response": "def get_device_name(self):\n        \"\"\"\n        return the device name\n\n        :return: str\n        \"\"\"\n        command = const.CMD_OPTIONS_RRQ\n        command_string = b'~DeviceName\\x00'\n        response_size = 1024\n\n        cmd_response = self.__send_command(command, command_string, response_size)\n        if cmd_response.get('status'):\n            device = self.__data.split(b'=', 1)[-1].split(b'\\x00')[0]\n            return device.decode()\n        else:\n            return \"\""}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_face_version(self):\n        command = const.CMD_OPTIONS_RRQ\n        command_string = b'ZKFaceVersion\\x00'\n        response_size = 1024\n\n        cmd_response = self.__send_command(command, command_string, response_size)\n        if cmd_response.get('status'):\n            response = self.__data.split(b'=', 1)[-1].split(b'\\x00')[0]\n            return safe_cast(response, int, 0)  if response else 0\n        else:\n            return None", "response": "This method returns the face version of the current version of the user s zk object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _clear_error(self, command_string=b''):\n        cmd_response = self.__send_command(const.CMD_ACK_ERROR, command_string, 1024)\n        cmd_response = self.__send_command(const.CMD_ACK_UNKNOWN, command_string, 1024)\n        cmd_response = self.__send_command(const.CMD_ACK_UNKNOWN, command_string, 1024)\n        cmd_response = self.__send_command(const.CMD_ACK_UNKNOWN, command_string, 1024)", "response": "Clear the error message from the cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_face_fun_on(self):\n        command = const.CMD_OPTIONS_RRQ\n        command_string = b'FaceFunOn\\x00'\n        response_size = 1024\n\n        cmd_response = self.__send_command(command, command_string, response_size)\n        if cmd_response.get('status'):\n            response = (self.__data.split(b'=', 1)[-1].split(b'\\x00')[0])\n            return safe_cast(response, int ,0) if response else 0\n        else:\n            self._clear_error(command_string)\n            return None", "response": "get face fun on"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_pin_width(self):\n        command = const.CMD_GET_PINWIDTH\n        command_string = b' P'\n        response_size = 9\n        cmd_response = self.__send_command(command, command_string, response_size)\n        if cmd_response.get('status'):\n            width = self.__data.split(b'\\x00')[0]\n            return bytearray(width)[0]\n        else:\n            raise ZKErrorResponse(\"can0t get pin width\")", "response": "get the PIN width of the current node"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfrees data from the current buffer", "response": "def free_data(self):\n        \"\"\"\n        clear buffer\n\n        :return: bool\n        \"\"\"\n        command = const.CMD_FREE_DATA\n        cmd_response = self.__send_command(command)\n        if cmd_response.get('status'):\n            return True\n        else:\n            raise ZKErrorResponse(\"can't free data\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading the memory ussage sizes", "response": "def read_sizes(self):\n        \"\"\"\n        read the memory ussage\n        \"\"\"\n        command = const.CMD_GET_FREE_SIZES\n        response_size = 1024\n        cmd_response = self.__send_command(command,b'', response_size)\n        if cmd_response.get('status'):\n            if self.verbose: print(codecs.encode(self.__data,'hex'))\n            size = len(self.__data)\n            if len(self.__data) >= 80:\n                fields = unpack('20i', self.__data[:80])\n                self.users = fields[4]\n                self.fingers = fields[6]\n                self.records = fields[8]\n                self.dummy = fields[10] #???\n                self.cards = fields[12]\n                self.fingers_cap = fields[14]\n                self.users_cap = fields[15]\n                self.rec_cap = fields[16]\n                self.fingers_av = fields[17]\n                self.users_av = fields[18]\n                self.rec_av = fields[19]\n                self.__data = self.__data[80:]\n            if len(self.__data) >= 12: #face info\n                fields = unpack('3i', self.__data[:12]) #dirty hack! we need more information\n                self.faces = fields[0]\n                self.faces_cap = fields[2]\n            return True\n        else:\n            raise ZKErrorResponse(\"can't read sizes\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nunlock the door\\n thanks to https://github.com/SoftwareHouseMerida/pyzk/ :param time: define delay in seconds :return: bool", "response": "def unlock(self, time=3):\n        \"\"\"\n        unlock the door\\n\n        thanks to https://github.com/SoftwareHouseMerida/pyzk/\n\n        :param time: define delay in seconds\n        :return: bool\n        \"\"\"\n        command = const.CMD_UNLOCK\n        command_string = pack(\"I\",int(time)*10)\n        cmd_response = self.__send_command(command, command_string)\n        if cmd_response.get('status'):\n            return True\n        else:\n            raise ZKErrorResponse(\"Can't open door\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrestart the device :return: bool", "response": "def restart(self):\n        \"\"\"\n        restart the device\n\n        :return: bool\n        \"\"\"\n        command = const.CMD_RESTART\n        cmd_response = self.__send_command(command)\n        if cmd_response.get('status'):\n            self.is_connect = False\n            self.next_uid = 1\n            return True\n        else:\n            raise ZKErrorResponse(\"can't restart device\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the time of the current node", "response": "def get_time(self):\n        \"\"\"\n        :return: the machine's time\n        \"\"\"\n        command = const.CMD_GET_TIME\n        response_size = 1032\n        cmd_response = self.__send_command(command, b'', response_size)\n        if cmd_response.get('status'):\n            return self.__decode_time(self.__data[:4])\n        else:\n            raise ZKErrorResponse(\"can't get time\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_time(self, timestamp):\n        command = const.CMD_SET_TIME\n        command_string = pack(b'I', self.__encode_time(timestamp))\n        cmd_response = self.__send_command(command, command_string)\n        if cmd_response.get('status'):\n            return True\n        else:\n            raise ZKErrorResponse(\"can't set time\")", "response": "set the time of the device"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset user by uid", "response": "def set_user(self, uid=None, name='', privilege=0, password='', group_id='', user_id='', card=0):\n        \"\"\"\n        create or update user by uid\n\n        :param name: name ot the user\n        :param privilege: check the const.py for reference\n        :param password: int password\n        :param group_id: group ID\n        :param user_id: your own user ID\n        :param card: card\n        :return: bool\n        \"\"\"\n        command = const.CMD_USER_WRQ\n        if uid is None:\n            uid = self.next_uid\n            if not user_id:\n                user_id = self.next_user_id\n        if not user_id:\n            user_id = str(uid) #ZK6 needs uid2 == uid\n        #TODO: check what happens if name is missing...\n        if privilege not in [const.USER_DEFAULT, const.USER_ADMIN]:\n            privilege = const.USER_DEFAULT\n        privilege = int(privilege)\n        if self.user_packet_size == 28: #self.firmware == 6:\n            if not group_id:\n                group_id = 0\n            try:\n                command_string = pack('HB5s8sIxBHI', uid, privilege, password.encode(self.encoding, errors='ignore'), name.encode(self.encoding, errors='ignore'), card, int(group_id), 0, int(user_id))\n            except Exception as e:\n                if self.verbose: print(\"s_h Error pack: %s\" % e)\n                if self.verbose: print(\"Error pack: %s\" % sys.exc_info()[0])\n                raise ZKErrorResponse(\"Can't pack user\")\n        else:\n            name_pad = name.encode(self.encoding, errors='ignore').ljust(24, b'\\x00')[:24]\n            card_str = pack('<I', int(card))[:4]\n            command_string = pack('HB8s24s4sx7sx24s', uid, privilege, password.encode(self.encoding, errors='ignore'), name_pad, card_str, group_id.encode(), user_id.encode())\n        response_size = 1024 #TODO check response?\n        cmd_response = self.__send_command(command, command_string, response_size)\n        if not cmd_response.get('status'):\n            raise ZKErrorResponse(\"Can't set user\")\n        self.refresh_data()\n        if self.next_uid == uid:\n            self.next_uid += 1 # better recalculate again\n        if self.next_user_id == user_id:\n            self.next_user_id = str(self.next_uid)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef save_user_template(self, user, fingers=[]):\n        if not isinstance(user, User):\n            users = self.get_users()\n            tusers = list(filter(lambda x: x.uid==user, users))\n            if len(tusers) == 1:\n                user = tusers[0]\n            else:\n                tusers = list(filter(lambda x: x.user_id==str(user), users))\n                if len(tusers) == 1:\n                    user = tusers[0]\n                else:\n                    raise ZKErrorResponse(\"Can't find user\")\n        if isinstance(fingers, Finger):\n            fingers = [fingers]\n        fpack = b\"\"\n        table = b\"\"\n        fnum = 0x10\n        tstart = 0\n        for finger in fingers:\n            tfp = finger.repack_only()\n            table += pack(\"<bHbI\", 2, user.uid, fnum + finger.fid, tstart)\n            tstart += len(tfp)\n            fpack += tfp\n        if self.user_packet_size == 28:\n            upack = user.repack29()\n        else:\n            upack = user.repack73()\n        head = pack(\"III\", len(upack), len(table), len(fpack))\n        packet = head + upack + table + fpack\n        self._send_with_buffer(packet)\n        command = 110\n        command_string = pack('<IHH', 12,0,8)\n        cmd_response = self.__send_command(command, command_string)\n        if not cmd_response.get('status'):\n            raise ZKErrorResponse(\"Can't save utemp\")\n        self.refresh_data()", "response": "save user and template"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete_user_template(self, uid=0, temp_id=0, user_id=''):\n        if self.tcp and user_id:\n            command = 134\n            command_string = pack('<24sB', str(user_id), temp_id)\n            cmd_response = self.__send_command(command, command_string)\n            if cmd_response.get('status'):\n                return True\n            else:\n                return False # probably empty!\n        if not uid:\n            users = self.get_users()\n            users = list(filter(lambda x: x.user_id==str(user_id), users))\n            if not users:\n                return False\n            uid = users[0].uid\n        command = const.CMD_DELETE_USERTEMP\n        command_string = pack('hb', uid, temp_id)\n        cmd_response = self.__send_command(command, command_string)\n        if cmd_response.get('status'):\n            return True #refres_data (1013)?\n        else:\n            return False", "response": "Delete specific template from a specific user."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete_user(self, uid=0, user_id=''):\n        if not uid:\n            users = self.get_users()\n            users = list(filter(lambda x: x.user_id==str(user_id), users))\n            if not users:\n                return False\n            uid = users[0].uid\n        command = const.CMD_DELETE_USER\n        command_string = pack('h', uid)\n        cmd_response = self.__send_command(command, command_string)\n        if not cmd_response.get('status'):\n            raise ZKErrorResponse(\"Can't delete user\")\n        self.refresh_data()\n        if uid == (self.next_uid - 1):\n            self.next_uid = uid", "response": "delete specific user by uid or user_id"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_templates(self):\n        self.read_sizes()\n        if self.fingers == 0:\n            return []\n        templates = []\n        templatedata, size = self.read_with_buffer(const.CMD_DB_RRQ, const.FCT_FINGERTMP)\n        if size < 4:\n            if self.verbose: print(\"WRN: no user data\")\n            return []\n        total_size = unpack('i', templatedata[0:4])[0]\n        if self.verbose: print (\"get template total size {}, size {} len {}\".format(total_size, size, len(templatedata)))\n        templatedata = templatedata[4:]\n        while total_size:\n            size, uid, fid, valid = unpack('HHbb',templatedata[:6])\n            template = unpack(\"%is\" % (size-6), templatedata[6:size])[0]\n            finger = Finger(uid, fid, valid, template)\n            if self.verbose: print(finger)\n            templates.append(finger)\n            templatedata = templatedata[size:]\n            total_size -= size\n        return templates", "response": "Reads the list of Finger objects from the user s data."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading the list of users from the internal buffer.", "response": "def get_users(self):\n        \"\"\"\n        :return: list of User object\n        \"\"\"\n        self.read_sizes()\n        if self.users == 0:\n            self.next_uid = 1\n            self.next_user_id='1'\n            return []\n        users = []\n        max_uid = 0\n        userdata, size = self.read_with_buffer(const.CMD_USERTEMP_RRQ, const.FCT_USER)\n        if self.verbose: print(\"user size {} (= {})\".format(size, len(userdata)))\n        if size <= 4:\n            print(\"WRN: missing user data\")\n            return []\n        total_size = unpack(\"I\",userdata[:4])[0]\n        self.user_packet_size = total_size / self.users\n        if not self.user_packet_size in [28, 72]:\n            if self.verbose: print(\"WRN packet size would be  %i\" % self.user_packet_size)\n        userdata = userdata[4:]\n        if self.user_packet_size == 28:\n            while len(userdata) >= 28:\n                uid, privilege, password, name, card, group_id, timezone, user_id = unpack('<HB5s8sIxBhI',userdata.ljust(28, b'\\x00')[:28])\n                if uid > max_uid: max_uid = uid\n                password = (password.split(b'\\x00')[0]).decode(self.encoding, errors='ignore')\n                name = (name.split(b'\\x00')[0]).decode(self.encoding, errors='ignore').strip()\n                group_id = str(group_id)\n                user_id = str(user_id)\n                #TODO: check card value and find in ver8\n                if not name:\n                    name = \"NN-%s\" % user_id\n                user = User(uid, name, privilege, password, group_id, user_id, card)\n                users.append(user)\n                if self.verbose: print(\"[6]user:\",uid, privilege, password, name, card, group_id, timezone, user_id)\n                userdata = userdata[28:]\n        else:\n            while len(userdata) >= 72:\n                uid, privilege, password, name, card, group_id, user_id = unpack('<HB8s24sIx7sx24s', userdata.ljust(72, b'\\x00')[:72])\n                password = (password.split(b'\\x00')[0]).decode(self.encoding, errors='ignore')\n                name = (name.split(b'\\x00')[0]).decode(self.encoding, errors='ignore').strip()\n                group_id = (group_id.split(b'\\x00')[0]).decode(self.encoding, errors='ignore').strip()\n                user_id = (user_id.split(b'\\x00')[0]).decode(self.encoding, errors='ignore')\n                if uid > max_uid: max_uid = uid\n                if not name:\n                    name = \"NN-%s\" % user_id\n                user = User(uid, name, privilege, password, group_id, user_id, card)\n                users.append(user)\n                userdata = userdata[72:]\n        max_uid += 1\n        self.next_uid = max_uid\n        self.next_user_id = str(max_uid)\n        while True:\n            if any(u for u in users if u.user_id == self.next_user_id):\n                max_uid += 1\n                self.next_user_id = str(max_uid)\n            else:\n                break\n        return users"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncanceling capturing finger :return: bool", "response": "def cancel_capture(self):\n        \"\"\"\n        cancel capturing finger\n\n        :return: bool\n        \"\"\"\n        command = const.CMD_CANCELCAPTURE\n        cmd_response = self.__send_command(command)\n        return bool(cmd_response.get('status'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstarts verify finger mode", "response": "def verify_user(self):\n        \"\"\"\n        start verify finger mode (after capture)\n\n        :return: bool\n        \"\"\"\n        command = const.CMD_STARTVERIFY\n        cmd_response = self.__send_command(command)\n        if cmd_response.get('status'):\n            return True\n        else:\n            raise ZKErrorResponse(\"Cant Verify\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reg_event(self, flags):\n        command = const.CMD_REG_EVENT\n        command_string = pack (\"I\", flags)\n        cmd_response = self.__send_command(command, command_string)\n        if not cmd_response.get('status'):\n            raise ZKErrorResponse(\"cant' reg events %i\" % flags)", "response": "reg events are 1 - 31 bit"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef enroll_user(self, uid=0, temp_id=0, user_id=''):\n        command = const.CMD_STARTENROLL\n        done = False\n        if  not user_id:\n            users = self.get_users()\n            users = list(filter(lambda x: x.uid==uid, users))\n            if len(users) >= 1:\n                user_id = users[0].user_id\n            else:\n                return False\n        if self.tcp:\n            command_string = pack('<24sbb',str(user_id).encode(), temp_id, 1)\n        else:\n            command_string = pack('<Ib', int(user_id), temp_id)\n        self.cancel_capture()\n        cmd_response = self.__send_command(command, command_string)\n        if not cmd_response.get('status'):\n            raise ZKErrorResponse(\"Cant Enroll user #%i [%i]\" %(uid, temp_id))\n        self.__sock.settimeout(60)\n        attempts = 3\n        while attempts:\n            if self.verbose: print(\"A:%i esperando primer regevent\" % attempts)\n            data_recv = self.__sock.recv(1032)\n            self.__ack_ok()\n            if self.verbose: print(codecs.encode(data_recv,'hex'))\n            if self.tcp:\n                if len(data_recv) > 16:\n                    res = unpack(\"H\", data_recv.ljust(24,b\"\\x00\")[16:18])[0]\n                    if self.verbose: print(\"res %i\" % res)\n                    if res == 0 or res == 6 or res == 4:\n                        if self.verbose: print (\"posible timeout  o reg Fallido\")\n                        break\n            else:\n                if len(data_recv) > 8:\n                    res = unpack(\"H\", data_recv.ljust(16,b\"\\x00\")[8:10])[0]\n                    if self.verbose: print(\"res %i\" % res)\n                    if res == 6 or res == 4:\n                        if self.verbose: print (\"posible timeout\")\n                        break\n            if self.verbose: print (\"A:%i esperando 2do regevent\" % attempts)\n            data_recv = self.__sock.recv(1032)\n            self.__ack_ok()\n            if self.verbose: print (codecs.encode(data_recv, 'hex'))\n            if self.tcp:\n                if len(data_recv) > 8:\n                    res = unpack(\"H\", data_recv.ljust(24,b\"\\x00\")[16:18])[0]\n                    if self.verbose: print(\"res %i\" % res)\n                    if res == 6 or res == 4:\n                        if self.verbose: print (\"posible timeout  o reg Fallido\")\n                        break\n                    elif res == 0x64:\n                        if self.verbose: print (\"ok, continue?\")\n                        attempts -= 1\n            else:\n                if len(data_recv) > 8:\n                    res = unpack(\"H\", data_recv.ljust(16,b\"\\x00\")[8:10])[0]\n                    if self.verbose: print(\"res %i\" % res)\n                    if res == 6 or res == 4:\n                        if self.verbose: print (\"posible timeout  o reg Fallido\")\n                        break\n                    elif res == 0x64:\n                        if self.verbose: print (\"ok, continue?\")\n                        attempts -= 1\n        if attempts == 0:\n            data_recv = self.__sock.recv(1032)\n            self.__ack_ok()\n            if self.verbose: print (codecs.encode(data_recv, 'hex'))\n            if self.tcp:\n                res = unpack(\"H\", data_recv.ljust(24,b\"\\x00\")[16:18])[0]\n            else:\n                res = unpack(\"H\", data_recv.ljust(16,b\"\\x00\")[8:10])[0]\n            if self.verbose: print(\"res %i\" % res)\n            if res == 5:\n                if self.verbose: print (\"finger duplicate\")\n            if res == 6 or res == 4:\n                if self.verbose: print (\"posible timeout\")\n            if res  == 0:\n                size = unpack(\"H\", data_recv.ljust(16,b\"\\x00\")[10:12])[0]\n                pos = unpack(\"H\", data_recv.ljust(16,b\"\\x00\")[12:14])[0]\n                if self.verbose: print(\"enroll ok\", size, pos)\n                done = True\n        self.__sock.settimeout(self.__timeout)\n        self.reg_event(0) # TODO: test\n        self.cancel_capture()\n        self.verify_user()\n        return done", "response": "start enroll user\n\n        :param uid: uid\n        :param temp_id: template id\n        :param user_id: user ID\n        :return: bool"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntry live capture of events", "response": "def live_capture(self, new_timeout=10):\n        \"\"\"\n        try live capture of events\n        \"\"\"\n        was_enabled = self.is_enabled\n        users = self.get_users()\n        self.cancel_capture()\n        self.verify_user()\n        if not self.is_enabled:\n            self.enable_device()\n        if self.verbose: print (\"start live_capture\")\n        self.reg_event(const.EF_ATTLOG)\n        self.__sock.settimeout(new_timeout)\n        self.end_live_capture = False\n        while not self.end_live_capture:\n            try:\n                if self.verbose: print (\"esperando event\")\n                data_recv = self.__sock.recv(1032)\n                self.__ack_ok()\n                if self.tcp:\n                    size = unpack('<HHI', data_recv[:8])[2]\n                    header = unpack('HHHH', data_recv[8:16])\n                    data = data_recv[16:]\n                else:\n                    size = len(data_recv)\n                    header = unpack('<4H', data_recv[:8])\n                    data = data_recv[8:]\n                if not header[0] == const.CMD_REG_EVENT:\n                    if self.verbose: print(\"not event! %x\" % header[0])\n                    continue\n                if not len(data):\n                    if self.verbose: print (\"empty\")\n                    continue\n                while len(data) >= 12:\n                    if len(data) == 12:\n                        user_id, status, punch, timehex = unpack('<IBB6s', data)\n                        data = data[12:]\n                    elif len(data) == 32:\n                        user_id,  status, punch, timehex = unpack('<24sBB6s', data[:32])\n                        data = data[32:]\n                    elif len(data) == 36:\n                        user_id,  status, punch, timehex, _other = unpack('<24sBB6s4s', data[:36])\n                        data = data[36:]\n                    elif len(data) >= 52:\n                        user_id,  status, punch, timehex, _other = unpack('<24sBB6s20s', data[:52])\n                        data = data[52:]\n                    if isinstance(user_id, int):\n                        user_id = str(user_id)\n                    else:\n                        user_id = (user_id.split(b'\\x00')[0]).decode(errors='ignore')\n                    timestamp = self.__decode_timehex(timehex)\n                    tuser = list(filter(lambda x: x.user_id == user_id, users))\n                    if not tuser:\n                        uid = int(user_id)\n                    else:\n                        uid = tuser[0].uid\n                    yield Attendance(user_id, timestamp, status, punch, uid)\n            except timeout:\n                if self.verbose: print (\"time out\")\n                yield None # return to keep watching\n            except (KeyboardInterrupt, SystemExit):\n                if self.verbose: print (\"break\")\n                break\n        if self.verbose: print (\"exit gracefully\")\n        self.__sock.settimeout(self.__timeout)\n        self.reg_event(0)\n        if not was_enabled:\n            self.disable_device()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clear_data(self):\n        command = const.CMD_CLEAR_DATA\n        command_string = ''\n        cmd_response = self.__send_command(command, command_string)\n        if cmd_response.get('status'):\n            self.is_connect = False\n            self.next_uid = 1\n            return True\n        else:\n            raise ZKErrorResponse(\"can't clear data\")", "response": "clear all data from the current user s attendance report finger database"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __recieve_tcp_data(self, data_recv, size):\n        data = []\n        tcp_length = self.__test_tcp_top(data_recv)\n        if self.verbose: print (\"tcp_length {}, size {}\".format(tcp_length, size))\n        if tcp_length <= 0:\n            if self.verbose: print (\"Incorrect tcp packet\")\n            return None, b\"\"\n        if (tcp_length - 8) < size:\n            if self.verbose: print (\"tcp length too small... retrying\")\n            resp, bh = self.__recieve_tcp_data(data_recv, tcp_length - 8)\n            data.append(resp)\n            size -= len(resp)\n            if self.verbose: print (\"new tcp DATA packet to fill misssing {}\".format(size))\n            data_recv = bh + self.__sock.recv(size + 16 )\n            if self.verbose: print (\"new tcp DATA starting with {} bytes\".format(len(data_recv)))\n            resp, bh = self.__recieve_tcp_data(data_recv, size)\n            data.append(resp)\n            if self.verbose: print (\"for misssing {} recieved {} with extra {}\".format(size, len(resp), len(bh)))\n            return b''.join(data), bh\n        recieved = len(data_recv)\n        if self.verbose: print (\"recieved {}, size {}\".format(recieved, size))\n        response = unpack('HHHH', data_recv[8:16])[0]\n        if recieved >= (size + 32):\n            if response == const.CMD_DATA:\n                resp = data_recv[16 : size + 16]\n                if self.verbose: print (\"resp complete len {}\".format(len(resp)))\n                return resp, data_recv[size + 16:]\n            else:\n                if self.verbose: print(\"incorrect response!!! {}\".format(response))\n                return None, b\"\"\n        else:\n            if self.verbose: print (\"try DATA incomplete (actual valid {})\".format(recieved-16))\n            data.append(data_recv[16 : size + 16 ])\n            size -= recieved - 16\n            broken_header = b\"\"\n            if size < 0:\n                broken_header = data_recv[size:]\n                if self.verbose: print (\"broken\", (broken_header).encode('hex'))\n            if size > 0:\n                data_recv = self.__recieve_raw_data(size)\n                data.append(data_recv)\n            return b''.join(data), broken_header", "response": "recieve a TCP packet and return the data and broken\nAttributeNames"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __recieve_chunk(self):\n        if self.__response == const.CMD_DATA:\n            if self.tcp:\n                if self.verbose: print (\"_rc_DATA! is {} bytes, tcp length is {}\".format(len(self.__data), self.__tcp_length))\n                if len(self.__data) < (self.__tcp_length - 8):\n                    need = (self.__tcp_length - 8) - len(self.__data)\n                    if self.verbose: print (\"need more data: {}\".format(need))\n                    more_data = self.__recieve_raw_data(need)\n                    return b''.join([self.__data, more_data])\n                else:\n                    if self.verbose: print (\"Enough data\")\n                    return self.__data\n            else:\n                if self.verbose: print (\"_rc len is {}\".format(len(self.__data)))\n                return self.__data\n        elif self.__response == const.CMD_PREPARE_DATA:\n            data = []\n            size = self.__get_data_size()\n            if self.verbose: print (\"recieve chunk: prepare data size is {}\".format(size))\n            if self.tcp:\n                if len(self.__data) >= (8 + size):\n                    data_recv = self.__data[8:]\n                else:\n                    data_recv = self.__data[8:] + self.__sock.recv(size + 32)\n                resp, broken_header = self.__recieve_tcp_data(data_recv, size)\n                data.append(resp)\n                # get CMD_ACK_OK\n                if len(broken_header) < 16:\n                    data_recv = broken_header + self.__sock.recv(16)\n                else:\n                    data_recv = broken_header\n                if len(data_recv) < 16:\n                    print (\"trying to complete broken ACK %s /16\" % len(data_recv))\n                    if self.verbose: print (data_recv.encode('hex'))\n                    data_recv += self.__sock.recv(16 - len(data_recv)) #TODO: CHECK HERE_!\n                if not self.__test_tcp_top(data_recv):\n                    if self.verbose: print (\"invalid chunk tcp ACK OK\")\n                    return None\n                response = unpack('HHHH', data_recv[8:16])[0]\n                if response == const.CMD_ACK_OK:\n                    if self.verbose: print (\"chunk tcp ACK OK!\")\n                    return b''.join(data)\n                if self.verbose: print(\"bad response %s\" % data_recv)\n                if self.verbose: print (codecs.encode(data,'hex'))\n                return None\n\n                return resp\n            while True:\n                data_recv = self.__sock.recv(1024+8)\n                response = unpack('<4H', data_recv[:8])[0]\n                if self.verbose: print (\"# packet response is: {}\".format(response))\n                if response == const.CMD_DATA:\n                    data.append(data_recv[8:])\n                    size -= 1024\n                elif response == const.CMD_ACK_OK:\n                    break\n                else:\n                    if self.verbose: print (\"broken!\")\n                    break\n                if self.verbose: print (\"still needs %s\" % size)\n            return b''.join(data)\n        else:\n            if self.verbose: print (\"invalid response %s\" % self.__response)\n            return None", "response": "recieve a chunk of data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __read_chunk(self, start, size):\n        for _retries in range(3):\n            command = 1504\n            command_string = pack('<ii', start, size)\n            if self.tcp:\n                response_size = size + 32\n            else:\n                response_size = 1024 + 8\n            cmd_response = self.__send_command(command, command_string, response_size)\n            data = self.__recieve_chunk()\n            if data is not None:\n                return data\n        else:\n            raise ZKErrorResponse(\"can't read chunk %i:[%i]\" % (start, size))", "response": "read a chunk of data from the buffer"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntesting read info with buffered command", "response": "def read_with_buffer(self, command, fct=0 ,ext=0):\n        \"\"\"\n        Test read info with buffered command (ZK6: 1503)\n        \"\"\"\n        if self.tcp:\n            MAX_CHUNK = 0xFFc0\n        else:\n            MAX_CHUNK = 16 * 1024\n        command_string = pack('<bhii', 1, command, fct, ext)\n        if self.verbose: print (\"rwb cs\", command_string)\n        response_size = 1024\n        data = []\n        start = 0\n        cmd_response = self.__send_command(1503, command_string, response_size)\n        if not cmd_response.get('status'):\n            raise ZKErrorResponse(\"RWB Not supported\")\n        if cmd_response['code'] == const.CMD_DATA:\n            if self.tcp:\n                if self.verbose: print (\"DATA! is {} bytes, tcp length is {}\".format(len(self.__data), self.__tcp_length))\n                if len(self.__data) < (self.__tcp_length - 8):\n                    need = (self.__tcp_length - 8) - len(self.__data)\n                    if self.verbose: print (\"need more data: {}\".format(need))\n                    more_data = self.__recieve_raw_data(need)\n                    return b''.join([self.__data, more_data]), len(self.__data) + len(more_data)\n                else:\n                    if self.verbose: print (\"Enough data\")\n                    size = len(self.__data)\n                    return self.__data, size\n            else:\n                size = len(self.__data)\n                return self.__data, size\n        size = unpack('I', self.__data[1:5])[0]\n        if self.verbose: print (\"size fill be %i\" % size)\n        remain = size % MAX_CHUNK\n        packets = (size-remain) // MAX_CHUNK # should be size /16k\n        if self.verbose: print (\"rwb: #{} packets of max {} bytes, and extra {} bytes remain\".format(packets, MAX_CHUNK, remain))\n        for _wlk in range(packets):\n            data.append(self.__read_chunk(start,MAX_CHUNK))\n            start += MAX_CHUNK\n        if remain:\n            data.append(self.__read_chunk(start, remain))\n            start += remain\n        self.free_data()\n        if self.verbose: print (\"_read w/chunk %i bytes\" % start)\n        return b''.join(data), start"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_attendance(self):\n        self.read_sizes()\n        if self.records == 0:\n            return []\n        users = self.get_users()\n        if self.verbose: print (users)\n        attendances = []\n        attendance_data, size = self.read_with_buffer(const.CMD_ATTLOG_RRQ)\n        if size < 4:\n            if self.verbose: print (\"WRN: no attendance data\")\n            return []\n        total_size = unpack(\"I\", attendance_data[:4])[0]\n        record_size = total_size/self.records\n        if self.verbose: print (\"record_size is \", record_size)\n        attendance_data = attendance_data[4:]\n        if record_size == 8:\n            while len(attendance_data) >= 8:\n                uid, status, timestamp, punch = unpack('HB4sB', attendance_data.ljust(8, b'\\x00')[:8])\n                if self.verbose: print (codecs.encode(attendance_data[:8], 'hex'))\n                attendance_data = attendance_data[8:]\n                tuser = list(filter(lambda x: x.uid == uid, users))\n                if not tuser:\n                    user_id = str(uid)\n                else:\n                    user_id = tuser[0].user_id\n                timestamp = self.__decode_time(timestamp)\n                attendance = Attendance(user_id, timestamp, status, punch, uid)\n                attendances.append(attendance)\n        elif record_size == 16:\n            while len(attendance_data) >= 16:\n                user_id, timestamp, status, punch, reserved, workcode = unpack('<I4sBB2sI', attendance_data.ljust(16, b'\\x00')[:16])\n                user_id = str(user_id)\n                if self.verbose: print(codecs.encode(attendance_data[:16], 'hex'))\n                attendance_data = attendance_data[16:]\n                tuser = list(filter(lambda x: x.user_id == user_id, users))\n                if not tuser:\n                    if self.verbose: print(\"no uid {}\", user_id)\n                    uid = str(user_id)\n                    tuser = list(filter(lambda x: x.uid == user_id, users))\n                    if not tuser:\n                        uid = str(user_id)\n                    else:\n                        uid = tuser[0].uid\n                        user_id = tuser[0].user_id\n                else:\n                    uid = tuser[0].uid\n                timestamp = self.__decode_time(timestamp)\n                attendance = Attendance(user_id, timestamp, status, punch, uid)\n                attendances.append(attendance)\n        else:\n            while len(attendance_data) >= 40:\n                uid, user_id, status, timestamp, punch, space = unpack('<H24sB4sB8s', attendance_data.ljust(40, b'\\x00')[:40])\n                if self.verbose: print (codecs.encode(attendance_data[:40], 'hex'))\n                user_id = (user_id.split(b'\\x00')[0]).decode(errors='ignore')\n                timestamp = self.__decode_time(timestamp)\n\n                attendance = Attendance(user_id, timestamp, status, punch, uid)\n                attendances.append(attendance)\n                attendance_data = attendance_data[40:]\n        return attendances", "response": "Reads the attendance record from the file and returns it as a list of Attendance objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nclears all attendance record", "response": "def clear_attendance(self):\n        \"\"\"\n        clear all attendance record\n\n        :return: bool\n        \"\"\"\n        command = const.CMD_CLEAR_ATTLOG\n        cmd_response = self.__send_command(command)\n        if cmd_response.get('status'):\n            return True\n        else:\n            raise ZKErrorResponse(\"Can't clear response\")"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _connect_control_flow_node(control_flow_node, next_node):\n    for last in control_flow_node.last_nodes:\n        if isinstance(next_node, ControlFlowNode):\n            last.connect(next_node.test)  # connect to next if test case\n        elif isinstance(next_node, AssignmentCallNode):\n            call_node = next_node.call_node\n            inner_most_call_node = _get_inner_most_function_call(call_node)\n            last.connect(inner_most_call_node)\n        else:\n            last.connect(next_node)", "response": "Connect a ControlFlowNode properly to the next_node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef connect_nodes(nodes):\n    for n, next_node in zip(nodes, nodes[1:]):\n        if isinstance(n, ControlFlowNode):\n            _connect_control_flow_node(n, next_node)\n        elif isinstance(next_node, ControlFlowNode):\n            n.connect(next_node.test)\n        elif isinstance(next_node, RestoreNode):\n            continue\n        elif CALL_IDENTIFIER in next_node.label:\n            continue\n        else:\n            n.connect(next_node)", "response": "Connect the nodes in a list linearly."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_names(node, result):\n    if isinstance(node, ast.Name):\n        return node.id + result\n    elif isinstance(node, ast.Subscript):\n        return result\n    elif isinstance(node, ast.Starred):\n        return _get_names(node.value, result)\n    else:\n        return _get_names(node.value, result + '.' + node.attr)", "response": "Recursively finds all names."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef extract_left_hand_side(target):\n    left_hand_side = _get_names(target, '')\n\n    left_hand_side.replace('*', '')\n    if '[' in left_hand_side:\n        index = left_hand_side.index('[')\n        left_hand_side = target[:index]\n\n    return left_hand_side", "response": "Extract the left hand side variable from a target."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_first_node(\n    node,\n    node_not_to_step_past\n):\n    \"\"\"\n        This is a super hacky way of getting the first node after a statement.\n        We do this because we visit a statement and keep on visiting and get something in return that is rarely the first node.\n        So we loop and loop backwards until we hit the statement or there is nothing to step back to.\n    \"\"\"\n    ingoing = None\n    i = 0\n    current_node = node\n    while current_node.ingoing:\n        # This is used because there may be multiple ingoing and loop will cause an infinite loop if we did [0]\n        i = random.randrange(len(current_node.ingoing))\n        # e.g. We don't want to step past the Except of an Except basic block\n        if current_node.ingoing[i] == node_not_to_step_past:\n            break\n        ingoing = current_node.ingoing\n        current_node = current_node.ingoing[i]\n    if ingoing:\n        return ingoing[i]\n    return current_node", "response": "Get the first node in the tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nhandles the stmt * expressions in an AST node.", "response": "def stmt_star_handler(\n        self,\n        stmts,\n        prev_node_to_avoid=None\n    ):\n        \"\"\"Handle stmt* expressions in an AST node.\n\n        Links all statements together in a list of statements, accounting for statements with multiple last nodes.\n        \"\"\"\n        break_nodes = list()\n        cfg_statements = list()\n\n        self.prev_nodes_to_avoid.append(prev_node_to_avoid)\n        self.last_control_flow_nodes.append(None)\n\n        first_node = None\n        node_not_to_step_past = self.nodes[-1]\n\n        for stmt in stmts:\n            node = self.visit(stmt)\n\n            if isinstance(node, ControlFlowNode) and not isinstance(node.test, TryNode):\n                self.last_control_flow_nodes.append(node.test)\n            else:\n                self.last_control_flow_nodes.append(None)\n\n            if isinstance(node, ControlFlowNode):\n                break_nodes.extend(node.break_statements)\n            elif isinstance(node, BreakNode):\n                break_nodes.append(node)\n\n            if not isinstance(node, IgnoredNode):\n                cfg_statements.append(node)\n                if not first_node:\n                    if isinstance(node, ControlFlowNode):\n                        first_node = node.test\n                    else:\n                        first_node = get_first_node(\n                            node,\n                            node_not_to_step_past\n                        )\n\n        self.prev_nodes_to_avoid.pop()\n        self.last_control_flow_nodes.pop()\n\n        connect_nodes(cfg_statements)\n\n        if cfg_statements:\n            if first_node:\n                first_statement = first_node\n            else:\n                first_statement = get_first_statement(cfg_statements[0])\n\n            last_statements = get_last_statements(cfg_statements)\n\n            return ConnectStatements(\n                first_statement=first_statement,\n                last_statements=last_statements,\n                break_statements=break_nodes\n            )\n        else:  # When body of module only contains ignored nodes\n            return IgnoredNode()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef handle_or_else(self, orelse, test):\n        if isinstance(orelse[0], ast.If):\n            control_flow_node = self.visit(orelse[0])\n            # Prefix the if label with 'el'\n            control_flow_node.test.label = 'el' + control_flow_node.test.label\n\n            test.connect(control_flow_node.test)\n            return control_flow_node.last_nodes\n        else:\n            else_connect_statements = self.stmt_star_handler(\n                orelse,\n                prev_node_to_avoid=self.nodes[-1]\n            )\n            test.connect(else_connect_statements.first_statement)\n            return else_connect_statements.last_statements", "response": "Handles the orelse part of an if or try node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef assignment_call_node(self, left_hand_label, ast_node):\n        self.undecided = True  # Used for handling functions in assignments\n\n        call = self.visit(ast_node.value)\n        call_label = call.left_hand_side\n\n        call_assignment = AssignmentCallNode(\n            left_hand_label + ' = ' + call_label,\n            left_hand_label,\n            ast_node,\n            [call.left_hand_side],\n            line_number=ast_node.lineno,\n            path=self.filenames[-1],\n            call_node=call\n        )\n        call.connect(call_assignment)\n\n        self.nodes.append(call_assignment)\n        self.undecided = False\n\n        return call_assignment", "response": "Handle assignments that contain a function call on its right side."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_blackbox_or_builtin_call(self, node, blackbox):  # noqa: C901\n        self.function_call_index += 1\n        saved_function_call_index = self.function_call_index\n        self.undecided = False\n\n        call_label_visitor = LabelVisitor()\n        call_label_visitor.visit(node)\n\n        call_function_label = call_label_visitor.result[:call_label_visitor.result.find('(')]\n\n        # Check if function call matches a blackbox/built-in alias and if so, resolve it\n        # This resolves aliases like \"from os import system as mysys\" as: mysys -> os.system\n        local_definitions = self.module_definitions_stack[-1]\n        call_function_label = fully_qualify_alias_labels(call_function_label, local_definitions.import_alias_mapping)\n\n        # Create e.g. ~call_1 = ret_func_foo\n        LHS = CALL_IDENTIFIER + 'call_' + str(saved_function_call_index)\n        RHS = 'ret_' + call_function_label + '('\n\n        call_node = BBorBInode(\n            label='',\n            left_hand_side=LHS,\n            ast_node=node,\n            right_hand_side_variables=[],\n            line_number=node.lineno,\n            path=self.filenames[-1],\n            func_name=call_function_label\n        )\n        visual_args = list()\n        rhs_vars = list()\n        last_return_value_of_nested_call = None\n\n        for arg_node in itertools.chain(node.args, node.keywords):\n            arg = arg_node.value if isinstance(arg_node, ast.keyword) else arg_node\n            if isinstance(arg, ast.Call):\n                return_value_of_nested_call = self.visit(arg)\n\n                if last_return_value_of_nested_call:\n                    # connect inner to other_inner in e.g.\n                    # `scrypt.outer(scrypt.inner(image_name), scrypt.other_inner(image_name))`\n                    # I should probably loop to the inner most call of other_inner here.\n                    try:\n                        last_return_value_of_nested_call.connect(return_value_of_nested_call.first_node)\n                    except AttributeError:\n                        last_return_value_of_nested_call.connect(return_value_of_nested_call)\n                else:\n                    # I should only set this once per loop, inner in e.g.\n                    # `scrypt.outer(scrypt.inner(image_name), scrypt.other_inner(image_name))`\n                    # (inner_most_call is used when predecessor is a ControlFlowNode in connect_control_flow_node)\n                    call_node.inner_most_call = return_value_of_nested_call\n                last_return_value_of_nested_call = return_value_of_nested_call\n\n                if isinstance(arg_node, ast.keyword) and arg_node.arg is not None:\n                    visual_args.append(arg_node.arg + '=' + return_value_of_nested_call.left_hand_side)\n                else:\n                    visual_args.append(return_value_of_nested_call.left_hand_side)\n                rhs_vars.append(return_value_of_nested_call.left_hand_side)\n            else:\n                label = LabelVisitor()\n                label.visit(arg_node)\n                visual_args.append(label.result)\n\n                vv = VarsVisitor()\n                vv.visit(arg_node)\n                rhs_vars.extend(vv.result)\n        if last_return_value_of_nested_call:\n            # connect other_inner to outer in e.g.\n            # `scrypt.outer(scrypt.inner(image_name), scrypt.other_inner(image_name))`\n            last_return_value_of_nested_call.connect(call_node)\n\n        call_names = list(get_call_names(node.func))\n        if len(call_names) > 1:\n            # taint is a RHS variable (self) of taint.lower()\n            rhs_vars.append(call_names[0])\n\n        if len(visual_args) > 0:\n            for arg in visual_args:\n                RHS = RHS + arg + \", \"\n            # Replace the last \", \" with a )\n            RHS = RHS[:len(RHS) - 2] + ')'\n        else:\n            RHS = RHS + ')'\n        call_node.label = LHS + \" = \" + RHS\n\n        call_node.right_hand_side_variables = rhs_vars\n        # Used in get_sink_args\n        rhs_visitor = RHSVisitor()\n        rhs_visitor.visit(node)\n        call_node.args = rhs_visitor.result\n\n        if blackbox:\n            self.blackbox_assignments.add(call_node)\n\n        self.connect_if_allowed(self.nodes[-1], call_node)\n        self.nodes.append(call_node)\n\n        return call_node", "response": "Processes a builtin or blackbox call."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a module to the CFG.", "response": "def add_module(  # noqa: C901\n        self,\n        module,\n        module_or_package_name,\n        local_names,\n        import_alias_mapping,\n        is_init=False,\n        from_from=False,\n        from_fdid=False\n    ):\n        \"\"\"\n        Returns:\n            The ExitNode that gets attached to the CFG of the class.\n        \"\"\"\n        module_path = module[1]\n\n        parent_definitions = self.module_definitions_stack[-1]\n        # Here, in `visit_Import` and in `visit_ImportFrom` are the only places the `import_alias_mapping` is updated\n        parent_definitions.import_alias_mapping.update(import_alias_mapping)\n        parent_definitions.import_names = local_names\n\n        new_module_definitions = ModuleDefinitions(local_names, module_or_package_name)\n        new_module_definitions.is_init = is_init\n        self.module_definitions_stack.append(new_module_definitions)\n\n        # Analyse the file\n        self.filenames.append(module_path)\n        self.local_modules = get_directory_modules(module_path) if self._allow_local_modules else []\n        tree = generate_ast(module_path)\n\n        # module[0] is None during e.g. \"from . import foo\", so we must str()\n        self.nodes.append(EntryOrExitNode('Module Entry ' + str(module[0])))\n        self.visit(tree)\n        exit_node = self.append_node(EntryOrExitNode('Module Exit ' + str(module[0])))\n\n        # Done analysing, pop the module off\n        self.module_definitions_stack.pop()\n        self.filenames.pop()\n\n        if new_module_definitions.is_init:\n            for def_ in new_module_definitions.definitions:\n                module_def_alias = handle_aliases_in_init_files(\n                    def_.name,\n                    new_module_definitions.import_alias_mapping\n                )\n                parent_def_alias = handle_aliases_in_init_files(\n                    def_.name,\n                    parent_definitions.import_alias_mapping\n                )\n                # They should never both be set\n                assert not (module_def_alias and parent_def_alias)\n\n                def_name = def_.name\n                if parent_def_alias:\n                    def_name = parent_def_alias\n                if module_def_alias:\n                    def_name = module_def_alias\n\n                local_definitions = self.module_definitions_stack[-1]\n                if local_definitions != parent_definitions:\n                    raise\n                if not isinstance(module_or_package_name, str):\n                    module_or_package_name = module_or_package_name.name\n\n                if module_or_package_name:\n                    if from_from:\n                        qualified_name = def_name\n\n                        if from_fdid:\n                            alias = handle_fdid_aliases(module_or_package_name, import_alias_mapping)\n                            if alias:\n                                module_or_package_name = alias\n                            parent_definition = ModuleDefinition(\n                                parent_definitions,\n                                qualified_name,\n                                module_or_package_name,\n                                self.filenames[-1]\n                            )\n                        else:\n                            parent_definition = ModuleDefinition(\n                                parent_definitions,\n                                qualified_name,\n                                None,\n                                self.filenames[-1]\n                            )\n                    else:\n                        qualified_name = module_or_package_name + '.' + def_name\n                        parent_definition = ModuleDefinition(\n                            parent_definitions,\n                            qualified_name,\n                            parent_definitions.module_name,\n                            self.filenames[-1]\n                        )\n                    parent_definition.node = def_.node\n                    parent_definitions.definitions.append(parent_definition)\n                else:\n                    parent_definition = ModuleDefinition(\n                        parent_definitions,\n                        def_name,\n                        parent_definitions.module_name,\n                        self.filenames[-1]\n                    )\n                    parent_definition.node = def_.node\n                    parent_definitions.definitions.append(parent_definition)\n\n        return exit_node"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nhandle relative import of the current module.", "response": "def handle_relative_import(self, node):\n        \"\"\"\n            from A means node.level == 0\n            from . import B means node.level == 1\n            from .A means node.level == 1\n        \"\"\"\n        no_file = os.path.abspath(os.path.join(self.filenames[-1], os.pardir))\n        skip_init = False\n\n        if node.level == 1:\n            # Same directory as current file\n            if node.module:\n                name_with_dir = os.path.join(no_file, node.module.replace('.', '/'))\n                if not os.path.isdir(name_with_dir):\n                    name_with_dir = name_with_dir + '.py'\n            # e.g. from . import X\n            else:\n                name_with_dir = no_file\n                # We do not want to analyse the init file of the current directory\n                skip_init = True\n        else:\n            parent = os.path.abspath(os.path.join(no_file, os.pardir))\n            if node.level > 2:\n                # Perform extra `cd ..` however many times\n                for _ in range(0, node.level - 2):\n                    parent = os.path.abspath(os.path.join(parent, os.pardir))\n            if node.module:\n                name_with_dir = os.path.join(parent, node.module.replace('.', '/'))\n                if not os.path.isdir(name_with_dir):\n                    name_with_dir = name_with_dir + '.py'\n            # e.g. from .. import X\n            else:\n                name_with_dir = parent\n\n        # Is it a file?\n        if name_with_dir.endswith('.py'):\n            return self.add_module(\n                module=(node.module, name_with_dir),\n                module_or_package_name=None,\n                local_names=as_alias_handler(node.names),\n                import_alias_mapping=retrieve_import_alias_mapping(node.names),\n                from_from=True\n            )\n        return self.from_directory_import(\n            (node.module, name_with_dir),\n            not_as_alias_handler(node.names),\n            as_alias_handler(node.names),\n            retrieve_import_alias_mapping(node.names),\n            skip_init=skip_init\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsave the local scope before entering a function call.", "response": "def save_local_scope(\n        self,\n        line_number,\n        saved_function_call_index\n    ):\n        \"\"\"Save the local scope before entering a function call by saving all the LHS's of assignments so far.\n\n        Args:\n            line_number(int): Of the def of the function call about to be entered into.\n            saved_function_call_index(int): Unique number for each call.\n\n        Returns:\n            saved_variables(list[SavedVariable])\n            first_node(EntryOrExitNode or None or RestoreNode): Used to connect previous statements to this function.\n        \"\"\"\n        saved_variables = list()\n        saved_variables_so_far = set()\n        first_node = None\n\n        # Make e.g. save_N_LHS = assignment.LHS for each AssignmentNode\n        for assignment in [node for node in self.nodes\n                           if (type(node) == AssignmentNode or\n                               type(node) == AssignmentCallNode or\n                               type(Node) == BBorBInode)]:  # type() is used on purpose here\n            if assignment.left_hand_side in saved_variables_so_far:\n                continue\n            saved_variables_so_far.add(assignment.left_hand_side)\n            save_name = 'save_{}_{}'.format(saved_function_call_index, assignment.left_hand_side)\n\n            previous_node = self.nodes[-1]\n\n            saved_scope_node = RestoreNode(\n                save_name + ' = ' + assignment.left_hand_side,\n                save_name,\n                [assignment.left_hand_side],\n                line_number=line_number,\n                path=self.filenames[-1]\n            )\n            if not first_node:\n                first_node = saved_scope_node\n\n            self.nodes.append(saved_scope_node)\n            # Save LHS\n            saved_variables.append(SavedVariable(LHS=save_name,\n                                                 RHS=assignment.left_hand_side))\n            self.connect_if_allowed(previous_node, saved_scope_node)\n\n        return (saved_variables, first_node)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef save_def_args_in_temp(\n        self,\n        call_args,\n        def_args,\n        line_number,\n        saved_function_call_index,\n        first_node\n    ):\n        \"\"\"Save the arguments of the definition being called. Visit the arguments if they're calls.\n\n        Args:\n            call_args(list[ast.Name]): Of the call being made.\n            def_args(ast_helper.Arguments): Of the definition being called.\n            line_number(int): Of the call being made.\n            saved_function_call_index(int): Unique number for each call.\n            first_node(EntryOrExitNode or None or RestoreNode): Used to connect previous statements to this function.\n\n        Returns:\n            args_mapping(dict): A mapping of call argument to definition argument.\n            first_node(EntryOrExitNode or None or RestoreNode): Used to connect previous statements to this function.\n        \"\"\"\n        args_mapping = dict()\n        last_return_value_of_nested_call = None\n\n        # Create e.g. temp_N_def_arg1 = call_arg1_label_visitor.result for each argument\n        for i, call_arg in enumerate(call_args):\n            # If this results in an IndexError it is invalid Python\n            def_arg_temp_name = 'temp_' + str(saved_function_call_index) + '_' + def_args[i]\n\n            return_value_of_nested_call = None\n            if isinstance(call_arg, ast.Call):\n                return_value_of_nested_call = self.visit(call_arg)\n                restore_node = RestoreNode(\n                    def_arg_temp_name + ' = ' + return_value_of_nested_call.left_hand_side,\n                    def_arg_temp_name,\n                    [return_value_of_nested_call.left_hand_side],\n                    line_number=line_number,\n                    path=self.filenames[-1]\n                )\n                if return_value_of_nested_call in self.blackbox_assignments:\n                    self.blackbox_assignments.add(restore_node)\n            else:\n                call_arg_label_visitor = LabelVisitor()\n                call_arg_label_visitor.visit(call_arg)\n                call_arg_rhs_visitor = RHSVisitor()\n                call_arg_rhs_visitor.visit(call_arg)\n                restore_node = RestoreNode(\n                    def_arg_temp_name + ' = ' + call_arg_label_visitor.result,\n                    def_arg_temp_name,\n                    call_arg_rhs_visitor.result,\n                    line_number=line_number,\n                    path=self.filenames[-1]\n                )\n\n            # If there are no saved variables, then this is the first node\n            if not first_node:\n                first_node = restore_node\n\n            if isinstance(call_arg, ast.Call):\n                if last_return_value_of_nested_call:\n                    # connect inner to other_inner in e.g. `outer(inner(image_name), other_inner(image_name))`\n                    if isinstance(return_value_of_nested_call, BBorBInode):\n                        last_return_value_of_nested_call.connect(return_value_of_nested_call)\n                    else:\n                        last_return_value_of_nested_call.connect(return_value_of_nested_call.first_node)\n                else:\n                    # I should only set this once per loop, inner in e.g. `outer(inner(image_name), other_inner(image_name))`\n                    # (inner_most_call is used when predecessor is a ControlFlowNode in connect_control_flow_node)\n                    if isinstance(return_value_of_nested_call, BBorBInode):\n                        first_node.inner_most_call = return_value_of_nested_call\n                    else:\n                        first_node.inner_most_call = return_value_of_nested_call.first_node\n                # We purposefully should not set this as the first_node of return_value_of_nested_call, last makes sense\n                last_return_value_of_nested_call = return_value_of_nested_call\n            self.connect_if_allowed(self.nodes[-1], restore_node)\n            self.nodes.append(restore_node)\n\n            if isinstance(call_arg, ast.Call):\n                args_mapping[return_value_of_nested_call.left_hand_side] = def_args[i]\n            else:\n                args_mapping[def_args[i]] = call_arg_label_visitor.result\n\n        return (args_mapping, first_node)", "response": "Save the arguments of the function being called."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating the local scope nodes for the function call.", "response": "def create_local_scope_from_def_args(\n        self,\n        call_args,\n        def_args,\n        line_number,\n        saved_function_call_index\n    ):\n        \"\"\"Create the local scope before entering the body of a function call.\n\n        Args:\n            call_args(list[ast.Name]): Of the call being made.\n            def_args(ast_helper.Arguments): Of the definition being called.\n            line_number(int): Of the def of the function call about to be entered into.\n            saved_function_call_index(int): Unique number for each call.\n\n        Note: We do not need a connect_if_allowed because of the\n              preceding call to save_def_args_in_temp.\n        \"\"\"\n        # Create e.g. def_arg1 = temp_N_def_arg1 for each argument\n        for i in range(len(call_args)):\n            def_arg_local_name = def_args[i]\n            def_arg_temp_name = 'temp_' + str(saved_function_call_index) + '_' + def_args[i]\n            local_scope_node = RestoreNode(\n                def_arg_local_name + ' = ' + def_arg_temp_name,\n                def_arg_local_name,\n                [def_arg_temp_name],\n                line_number=line_number,\n                path=self.filenames[-1]\n            )\n            # Chain the local scope nodes together\n            self.nodes[-1].connect(local_scope_node)\n            self.nodes.append(local_scope_node)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef visit_and_get_function_nodes(\n        self,\n        definition,\n        first_node\n    ):\n        \"\"\"Visits the nodes of a user defined function.\n\n        Args:\n            definition(LocalModuleDefinition): Definition of the function being added.\n            first_node(EntryOrExitNode or None or RestoreNode): Used to connect previous statements to this function.\n\n        Returns:\n            the_new_nodes(list[Node]): The nodes added while visiting the function.\n            first_node(EntryOrExitNode or None or RestoreNode): Used to connect previous statements to this function.\n        \"\"\"\n        len_before_visiting_func = len(self.nodes)\n        previous_node = self.nodes[-1]\n        entry_node = self.append_node(EntryOrExitNode('Function Entry ' +\n                                                      definition.name))\n        if not first_node:\n            first_node = entry_node\n        self.connect_if_allowed(previous_node, entry_node)\n\n        function_body_connect_statements = self.stmt_star_handler(definition.node.body)\n        entry_node.connect(function_body_connect_statements.first_statement)\n\n        exit_node = self.append_node(EntryOrExitNode('Exit ' + definition.name))\n        exit_node.connect_predecessors(function_body_connect_statements.last_statements)\n\n        the_new_nodes = self.nodes[len_before_visiting_func:]\n        return_connection_handler(the_new_nodes, exit_node)\n\n        return (the_new_nodes, first_node)", "response": "Visits the nodes of a user defined function."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrestoring the previously saved variables to their original values.", "response": "def restore_saved_local_scope(\n        self,\n        saved_variables,\n        args_mapping,\n        line_number\n    ):\n        \"\"\"Restore the previously saved variables to their original values.\n\n        Args:\n           saved_variables(list[SavedVariable])\n           args_mapping(dict): A mapping of call argument to definition argument.\n           line_number(int): Of the def of the function call about to be entered into.\n\n        Note: We do not need connect_if_allowed because of the\n              preceding call to save_local_scope.\n        \"\"\"\n        restore_nodes = list()\n        for var in saved_variables:\n            # Is var.RHS a call argument?\n            if var.RHS in args_mapping:\n                # If so, use the corresponding definition argument for the RHS of the label.\n                restore_nodes.append(RestoreNode(\n                    var.RHS + ' = ' + args_mapping[var.RHS],\n                    var.RHS,\n                    [var.LHS],\n                    line_number=line_number,\n                    path=self.filenames[-1]\n                ))\n            else:\n                # Create a node for e.g. foo = save_1_foo\n                restore_nodes.append(RestoreNode(\n                    var.RHS + ' = ' + var.LHS,\n                    var.RHS,\n                    [var.LHS],\n                    line_number=line_number,\n                    path=self.filenames[-1]\n                ))\n\n        # Chain the restore nodes\n        for node, successor in zip(restore_nodes, restore_nodes[1:]):\n            node.connect(successor)\n\n        if restore_nodes:\n            # Connect the last node to the first restore node\n            self.nodes[-1].connect(restore_nodes[0])\n            self.nodes.extend(restore_nodes)\n\n        return restore_nodes"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef return_handler(\n        self,\n        call_node,\n        function_nodes,\n        saved_function_call_index,\n        first_node\n    ):\n        \"\"\"Handle the return from a function during a function call.\n\n        Args:\n            call_node(ast.Call) : The node that calls the definition.\n            function_nodes(list[Node]): List of nodes of the function being called.\n            saved_function_call_index(int): Unique number for each call.\n            first_node(EntryOrExitNode or RestoreNode): Used to connect previous statements to this function.\n        \"\"\"\n        if any(isinstance(node, YieldNode) for node in function_nodes):\n            # Presence of a `YieldNode` means that the function is a generator\n            rhs_prefix = 'yld_'\n        elif any(isinstance(node, ConnectToExitNode) for node in function_nodes):\n            # Only `Return`s and `Raise`s can be of type ConnectToExitNode\n            rhs_prefix = 'ret_'\n        else:\n            return  # No return value\n\n        # Create e.g. ~call_1 = ret_func_foo RestoreNode\n        LHS = CALL_IDENTIFIER + 'call_' + str(saved_function_call_index)\n        RHS = rhs_prefix + get_call_names_as_string(call_node.func)\n        return_node = RestoreNode(\n            LHS + ' = ' + RHS,\n            LHS,\n            [RHS],\n            line_number=call_node.lineno,\n            path=self.filenames[-1]\n        )\n        return_node.first_node = first_node\n        self.nodes[-1].connect(return_node)\n        self.nodes.append(return_node)", "response": "Handles the return from a function during a function call."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprocess a user defined function when it is called.", "response": "def process_function(self, call_node, definition):\n        \"\"\"Processes a user defined function when it is called.\n\n        Increments self.function_call_index each time it is called, we can refer to it as N in the comments.\n        Make e.g. save_N_LHS = assignment.LHS for each AssignmentNode. (save_local_scope)\n        Create e.g. temp_N_def_arg1 = call_arg1_label_visitor.result for each argument.\n            Visit the arguments if they're calls. (save_def_args_in_temp)\n        Create e.g. def_arg1 = temp_N_def_arg1 for each argument. (create_local_scope_from_def_args)\n        Visit and get function nodes. (visit_and_get_function_nodes)\n        Loop through each save_N_LHS node and create an e.g.\n            foo = save_1_foo or, if foo was a call arg, foo = arg_mapping[foo]. (restore_saved_local_scope)\n        Create e.g. ~call_1 = ret_func_foo RestoreNode. (return_handler)\n\n        Notes:\n            Page 31 in the original thesis, but changed a little.\n            We don't have to return the ~call_1 = ret_func_foo RestoreNode made in return_handler,\n                because it's the last node anyway, that we return in this function.\n            e.g. ret_func_foo gets assigned to visit_Return.\n\n        Args:\n            call_node(ast.Call) : The node that calls the definition.\n            definition(LocalModuleDefinition): Definition of the function being called.\n\n        Returns:\n            Last node in self.nodes, probably the return of the function appended to self.nodes in return_handler.\n        \"\"\"\n        self.function_call_index += 1\n        saved_function_call_index = self.function_call_index\n\n        def_node = definition.node\n\n        saved_variables, first_node = self.save_local_scope(\n            def_node.lineno,\n            saved_function_call_index\n        )\n\n        args_mapping, first_node = self.save_def_args_in_temp(\n            call_node.args,\n            Arguments(def_node.args),\n            call_node.lineno,\n            saved_function_call_index,\n            first_node\n        )\n        self.filenames.append(definition.path)\n        self.create_local_scope_from_def_args(\n            call_node.args,\n            Arguments(def_node.args),\n            def_node.lineno,\n            saved_function_call_index\n        )\n        function_nodes, first_node = self.visit_and_get_function_nodes(\n            definition,\n            first_node\n        )\n        self.filenames.pop()  # Should really probably move after restore_saved_local_scope!!!\n        self.restore_saved_local_scope(\n            saved_variables,\n            args_mapping,\n            def_node.lineno\n        )\n        self.return_handler(\n            call_node,\n            function_nodes,\n            saved_function_call_index,\n            first_node\n        )\n        self.function_return_stack.pop()\n        self.function_definition_stack.pop()\n\n        return self.nodes[-1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef report(\n    vulnerabilities,\n    fileobj,\n    print_sanitised,\n):\n    \"\"\"\n    Prints issues in color-coded text format.\n\n    Args:\n        vulnerabilities: list of vulnerabilities to report\n        fileobj: The output file object, which may be sys.stdout\n    \"\"\"\n    n_vulnerabilities = len(vulnerabilities)\n    unsanitised_vulnerabilities = [v for v in vulnerabilities if not isinstance(v, SanitisedVulnerability)]\n    n_unsanitised = len(unsanitised_vulnerabilities)\n    n_sanitised = n_vulnerabilities - n_unsanitised\n    heading = \"{} vulnerabilit{} found{}.\\n\".format(\n        'No' if n_unsanitised == 0 else n_unsanitised,\n        'y' if n_unsanitised == 1 else 'ies',\n        \" (plus {} sanitised)\".format(n_sanitised) if n_sanitised else \"\",\n    )\n    vulnerabilities_to_print = vulnerabilities if print_sanitised else unsanitised_vulnerabilities\n    with fileobj:\n        for i, vulnerability in enumerate(vulnerabilities_to_print, start=1):\n            fileobj.write(vulnerability_to_str(i, vulnerability))\n\n        if n_unsanitised == 0:\n            fileobj.write(color(heading, GOOD))\n        else:\n            fileobj.write(color(heading, DANGER))", "response": "Prints issues in color - coded text format."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving yield with no variables e. g. yield 123 and plain yield from vulnerability.", "response": "def _remove_non_propagating_yields(self):\n        \"\"\"Remove yield with no variables e.g. `yield 123` and plain `yield` from vulnerability.\"\"\"\n        for node in list(self.reassignment_nodes):\n            if isinstance(node, YieldNode) and len(node.right_hand_side_variables) == 1:\n                self.reassignment_nodes.remove(node)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fixpoint_runner(self):\n        q = self.cfg.nodes\n\n        while q != []:\n            x_i = constraint_table[q[0]]  # x_i = q[0].old_constraint\n            self.analysis.fixpointmethod(q[0])  # y = F_i(x_1, ..., x_n);\n            y = constraint_table[q[0]]  # y = q[0].new_constraint\n\n            if y != x_i:\n                for node in self.analysis.dep(q[0]):  # for (v in dep(v_i))\n                    q.append(node)  # q.append(v):\n                constraint_table[q[0]] = y  # q[0].old_constraint = q[0].new_constraint # x_i = y\n            q = q[1:]", "response": "Work list algorithm that runs the fixpoint algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing the file for source and sink definitions.", "response": "def parse(trigger_word_file):\n    \"\"\"Parse the file for source and sink definitions.\n\n    Returns:\n       A definitions tuple with sources and sinks.\n    \"\"\"\n    with open(trigger_word_file) as fd:\n        triggers_dict = json.load(fd)\n    sources = [Source(s) for s in triggers_dict['sources']]\n    sinks = [\n        Sink.from_json(trigger, data)\n        for trigger, data in triggers_dict['sinks'].items()\n    ]\n    return Definitions(sources, sinks)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconnect this node to its successor node by setting its outgoing and the successors ingoing.", "response": "def connect(self, successor):\n        \"\"\"Connect this node to its successor node by\n        setting its outgoing and the successors ingoing.\"\"\"\n        if isinstance(self, ConnectToExitNode) and not isinstance(successor, EntryOrExitNode):\n            return\n\n        self.outgoing.append(successor)\n        successor.ingoing.append(self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef connect_predecessors(self, predecessors):\n        for n in predecessors:\n            self.ingoing.append(n)\n            n.outgoing.append(self)", "response": "Connect all nodes in predecessors to this node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding definition to list if local or in imports", "response": "def append_if_local_or_in_imports(self, definition):\n        \"\"\"Add definition to list.\n\n        Handles local definitions and adds to project_definitions.\n        \"\"\"\n        if isinstance(definition, LocalModuleDefinition):\n            self.definitions.append(definition)\n        elif self.import_names == [\"*\"]:\n            self.definitions.append(definition)\n        elif self.import_names and definition.name in self.import_names:\n            self.definitions.append(definition)\n        elif (self.import_alias_mapping and definition.name in\n              self.import_alias_mapping.values()):\n            self.definitions.append(definition)\n\n        if definition.parent_module_name:\n            self.definitions.append(definition)\n\n        if definition.node not in project_definitions:\n            project_definitions[definition.node] = definition"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_definition(self, name):\n        for definition in self.definitions:\n            if definition.name == name:\n                return definition", "response": "Get a definition by name."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_definition_node(self, node, name):\n        definition = self.get_definition(name)\n        if definition:\n            definition.node = node", "response": "Set the node of the current definition by name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fixpointmethod(self, cfg_node):\n        JOIN = self.join(cfg_node)\n        # Assignment check\n        if isinstance(cfg_node, AssignmentNode):\n            arrow_result = JOIN\n\n            # Reassignment check\n            if cfg_node.left_hand_side not in cfg_node.right_hand_side_variables:\n                # Get previous assignments of cfg_node.left_hand_side and remove them from JOIN\n                arrow_result = self.arrow(JOIN, cfg_node.left_hand_side)\n\n            arrow_result = arrow_result | self.lattice.el2bv[cfg_node]\n            constraint_table[cfg_node] = arrow_result\n        # Default case\n        else:\n            constraint_table[cfg_node] = JOIN", "response": "This method is used to fixpoint the branching problem."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving all previous assignments from JOIN that have the same left hand side.", "response": "def arrow(self, JOIN, _id):\n        \"\"\"Removes all previous assignments from JOIN that have the same left hand side.\n        This represents the arrow id definition from Schwartzbach.\"\"\"\n        r = JOIN\n        for node in self.lattice.get_elements(JOIN):\n            if node.left_hand_side == _id:\n                r = r ^ self.lattice.el2bv[node]\n        return r"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef in_constraint(self, node1, node2):\n        constraint = constraint_table[node2]\n        if constraint == 0b0:\n            return False\n\n        try:\n            value = self.el2bv[node1]\n        except KeyError:\n            return False\n\n        return constraint & value != 0", "response": "Checks if node1 is in node2 s constraints\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nidentifying sources sinks and sanitisers in a CFG.", "response": "def identify_triggers(\n    cfg,\n    sources,\n    sinks,\n    lattice,\n    nosec_lines\n):\n    \"\"\"Identify sources, sinks and sanitisers in a CFG.\n\n    Args:\n        cfg(CFG): CFG to find sources, sinks and sanitisers in.\n        sources(tuple): list of sources, a source is a (source, sanitiser) tuple.\n        sinks(tuple): list of sources, a sink is a (sink, sanitiser) tuple.\n        nosec_lines(set): lines with # nosec whitelisting\n\n    Returns:\n        Triggers tuple with sink and source nodes and a sanitiser node dict.\n    \"\"\"\n    assignment_nodes = filter_cfg_nodes(cfg, AssignmentNode)\n    tainted_nodes = filter_cfg_nodes(cfg, TaintedNode)\n    tainted_trigger_nodes = [\n        TriggerNode(\n            Source('Framework function URL parameter'),\n            cfg_node=node\n        ) for node in tainted_nodes\n    ]\n    sources_in_file = find_triggers(assignment_nodes, sources, nosec_lines)\n    sources_in_file.extend(tainted_trigger_nodes)\n\n    find_secondary_sources(assignment_nodes, sources_in_file, lattice)\n\n    sinks_in_file = find_triggers(cfg.nodes, sinks, nosec_lines)\n\n    sanitiser_node_dict = build_sanitiser_node_dict(cfg, sinks_in_file)\n\n    return Triggers(sources_in_file, sinks_in_file, sanitiser_node_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_secondary_sources(\n    assignment_nodes,\n    sources,\n    lattice\n):\n    \"\"\"\n        Sets the secondary_nodes attribute of each source in the sources list.\n\n        Args:\n            assignment_nodes([AssignmentNode])\n            sources([tuple])\n            lattice(Lattice): the lattice we're analysing.\n    \"\"\"\n    for source in sources:\n        source.secondary_nodes = find_assignments(assignment_nodes, source, lattice)", "response": "Sets the secondary_nodes attribute of each source in the sources list."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind triggers from the trigger_word_list in the nodes.", "response": "def find_triggers(\n    nodes,\n    trigger_words,\n    nosec_lines\n):\n    \"\"\"Find triggers from the trigger_word_list in the nodes.\n\n    Args:\n        nodes(list[Node]): the nodes to find triggers in.\n        trigger_word_list(list[Union[Sink, Source]]): list of trigger words to look for.\n        nosec_lines(set): lines with # nosec whitelisting\n\n    Returns:\n        List of found TriggerNodes\n    \"\"\"\n    trigger_nodes = list()\n    for node in nodes:\n        if node.line_number not in nosec_lines:\n            trigger_nodes.extend(iter(label_contains(node, trigger_words)))\n    return trigger_nodes"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning an iterable of TriggerNodes that contain any of the trigger_words provided.", "response": "def label_contains(\n    node,\n    triggers\n):\n    \"\"\"Determine if node contains any of the trigger_words provided.\n\n    Args:\n        node(Node): CFG node to check.\n        trigger_words(list[Union[Sink, Source]]): list of trigger words to look for.\n\n    Returns:\n        Iterable of TriggerNodes found. Can be multiple because multiple\n        trigger_words can be in one node.\n    \"\"\"\n    for trigger in triggers:\n        if trigger.trigger_word in node.label:\n            yield TriggerNode(trigger, node)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbuilding a dict of string - > TriggerNode pairs where the string is the sanitiser and the TriggerNode is a TriggerNode of the sanitiser.", "response": "def build_sanitiser_node_dict(\n    cfg,\n    sinks_in_file\n):\n    \"\"\"Build a dict of string -> TriggerNode pairs, where the string\n       is the sanitiser and the TriggerNode is a TriggerNode of the sanitiser.\n\n    Args:\n        cfg(CFG): cfg to traverse.\n        sinks_in_file(list[TriggerNode]): list of TriggerNodes containing\n                                          the sinks in the file.\n\n    Returns:\n        A string -> TriggerNode dict.\n    \"\"\"\n    sanitisers = list()\n    for sink in sinks_in_file:\n        sanitisers.extend(sink.sanitisers)\n\n    sanitisers_in_file = list()\n    for sanitiser in sanitisers:\n        for cfg_node in cfg.nodes:\n            if sanitiser in cfg_node.label:\n                sanitisers_in_file.append(Sanitiser(sanitiser, cfg_node))\n\n    sanitiser_node_dict = dict()\n    for sanitiser in sanitisers:\n        sanitiser_node_dict[sanitiser] = list(find_sanitiser_nodes(\n            sanitiser,\n            sanitisers_in_file\n        ))\n    return sanitiser_node_dict"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind nodes containing a particular sanitiser.", "response": "def find_sanitiser_nodes(\n    sanitiser,\n    sanitisers_in_file\n):\n    \"\"\"Find nodes containing a particular sanitiser.\n\n    Args:\n        sanitiser(string): sanitiser to look for.\n        sanitisers_in_file(list[Node]): list of CFG nodes with the sanitiser.\n\n    Returns:\n        Iterable of sanitiser nodes.\n    \"\"\"\n    for sanitiser_tuple in sanitisers_in_file:\n        if sanitiser == sanitiser_tuple.trigger_word:\n            yield sanitiser_tuple.cfg_node"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_vulnerability_chains(\n    current_node,\n    sink,\n    def_use,\n    chain=[]\n):\n    \"\"\"Traverses the def-use graph to find all paths from source to sink that cause a vulnerability.\n\n    Args:\n        current_node()\n        sink()\n        def_use(dict):\n        chain(list(Node)): A path of nodes between source and sink.\n    \"\"\"\n    for use in def_use[current_node]:\n        if use == sink:\n            yield chain\n        else:\n            vuln_chain = list(chain)\n            vuln_chain.append(use)\n            yield from get_vulnerability_chains(\n                use,\n                sink,\n                def_use,\n                vuln_chain\n            )", "response": "Traverses the def - use graph to find all paths from source to sink that cause a vulnerability."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck the blackbox mapping and sanitiser dictionary and returns whether or not the chain is vulnerable.", "response": "def how_vulnerable(\n    chain,\n    blackbox_mapping,\n    sanitiser_nodes,\n    potential_sanitiser,\n    blackbox_assignments,\n    interactive,\n    vuln_deets\n):\n    \"\"\"Iterates through the chain of nodes and checks the blackbox nodes against the blackbox mapping and sanitiser dictionary.\n\n    Note: potential_sanitiser is the only hack here, it is because we do not take p-use's into account yet.\n    e.g. we can only say potentially instead of definitely sanitised in the path_traversal_sanitised_2.py test.\n\n    Args:\n        chain(list(Node)): A path of nodes between source and sink.\n        blackbox_mapping(dict): A map of blackbox functions containing whether or not they propagate taint.\n        sanitiser_nodes(set): A set of nodes that are sanitisers for the sink.\n        potential_sanitiser(Node): An if or elif node that can potentially cause sanitisation.\n        blackbox_assignments(set[AssignmentNode]): set of blackbox assignments, includes the ReturnNode's of BBorBInode's.\n        interactive(bool): determines if we ask the user about blackbox functions not in the mapping file.\n        vuln_deets(dict): vulnerability details.\n\n    Returns:\n        A VulnerabilityType depending on how vulnerable the chain is.\n    \"\"\"\n    for i, current_node in enumerate(chain):\n        if current_node in sanitiser_nodes:\n            vuln_deets['sanitiser'] = current_node\n            vuln_deets['confident'] = True\n            return VulnerabilityType.SANITISED, interactive\n\n        if isinstance(current_node, BBorBInode):\n            if current_node.func_name in blackbox_mapping['propagates']:\n                continue\n            elif current_node.func_name in blackbox_mapping['does_not_propagate']:\n                return VulnerabilityType.FALSE, interactive\n            elif interactive:\n                user_says = input(\n                    'Is the return value of {} with tainted argument \"{}\" vulnerable? ([Y]es/[N]o/[S]top asking)'.format(\n                        current_node.label,\n                        chain[i - 1].left_hand_side\n                    )\n                ).lower()\n                if user_says.startswith('s'):\n                    interactive = False\n                    vuln_deets['unknown_assignment'] = current_node\n                    return VulnerabilityType.UNKNOWN, interactive\n                if user_says.startswith('n'):\n                    blackbox_mapping['does_not_propagate'].append(current_node.func_name)\n                    return VulnerabilityType.FALSE, interactive\n                blackbox_mapping['propagates'].append(current_node.func_name)\n            else:\n                vuln_deets['unknown_assignment'] = current_node\n                return VulnerabilityType.UNKNOWN, interactive\n\n    if potential_sanitiser:\n        vuln_deets['sanitiser'] = potential_sanitiser\n        vuln_deets['confident'] = False\n        return VulnerabilityType.SANITISED, interactive\n\n    return VulnerabilityType.TRUE, interactive"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the vulnerability between source and sink.", "response": "def get_vulnerability(\n    source,\n    sink,\n    triggers,\n    lattice,\n    cfg,\n    interactive,\n    blackbox_mapping\n):\n    \"\"\"Get vulnerability between source and sink if it exists.\n\n    Uses triggers to find sanitisers.\n\n    Note: When a secondary node is in_constraint with the sink\n              but not the source, the secondary is a save_N_LHS\n              node made in process_function in expr_visitor.\n\n    Args:\n        source(TriggerNode): TriggerNode of the source.\n        sink(TriggerNode): TriggerNode of the sink.\n        triggers(Triggers): Triggers of the CFG.\n        lattice(Lattice): the lattice we're analysing.\n        cfg(CFG): .blackbox_assignments used in is_unknown, .nodes used in build_def_use_chain\n        interactive(bool): determines if we ask the user about blackbox functions not in the mapping file.\n        blackbox_mapping(dict): A map of blackbox functions containing whether or not they propagate taint.\n\n    Returns:\n        A Vulnerability if it exists, else None\n    \"\"\"\n    nodes_in_constraint = [\n        secondary\n        for secondary in reversed(source.secondary_nodes)\n        if lattice.in_constraint(\n            secondary,\n            sink.cfg_node\n        )\n    ]\n    nodes_in_constraint.append(source.cfg_node)\n    if sink.trigger.all_arguments_propagate_taint:\n        sink_args = get_sink_args(sink.cfg_node)\n    else:\n        sink_args = get_sink_args_which_propagate(sink, sink.cfg_node.ast_node)\n\n    tainted_node_in_sink_arg = get_tainted_node_in_sink_args(\n        sink_args,\n        nodes_in_constraint,\n    )\n\n    if tainted_node_in_sink_arg:\n        vuln_deets = {\n            'source': source.cfg_node,\n            'source_trigger_word': source.trigger_word,\n            'sink': sink.cfg_node,\n            'sink_trigger_word': sink.trigger_word\n        }\n\n        sanitiser_nodes = set()\n        potential_sanitiser = None\n        if sink.sanitisers:\n            for sanitiser in sink.sanitisers:\n                for cfg_node in triggers.sanitiser_dict[sanitiser]:\n                    if isinstance(cfg_node, AssignmentNode):\n                        sanitiser_nodes.add(cfg_node)\n                    elif isinstance(cfg_node, IfNode):\n                        potential_sanitiser = cfg_node\n\n        def_use = build_def_use_chain(\n            cfg.nodes,\n            lattice\n        )\n\n        for chain in get_vulnerability_chains(\n            source.cfg_node,\n            sink.cfg_node,\n            def_use\n        ):\n            vulnerability_type, interactive = how_vulnerable(\n                chain,\n                blackbox_mapping,\n                sanitiser_nodes,\n                potential_sanitiser,\n                cfg.blackbox_assignments,\n                interactive,\n                vuln_deets\n            )\n            if vulnerability_type == VulnerabilityType.FALSE:\n                continue\n\n            vuln_deets['reassignment_nodes'] = chain\n\n            return vuln_factory(vulnerability_type)(**vuln_deets), interactive\n\n    return None, interactive"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_vulnerabilities_in_cfg(\n    cfg,\n    definitions,\n    lattice,\n    blackbox_mapping,\n    vulnerabilities_list,\n    interactive,\n    nosec_lines\n):\n    \"\"\"Find vulnerabilities in a cfg.\n\n    Args:\n        cfg(CFG): The CFG to find vulnerabilities in.\n        definitions(trigger_definitions_parser.Definitions): Source and sink definitions.\n        lattice(Lattice): the lattice we're analysing.\n        blackbox_mapping(dict): A map of blackbox functions containing whether or not they propagate taint.\n        vulnerabilities_list(list): That we append to when we find vulnerabilities.\n        interactive(bool): determines if we ask the user about blackbox functions not in the mapping file.\n    \"\"\"\n    triggers = identify_triggers(\n        cfg,\n        definitions.sources,\n        definitions.sinks,\n        lattice,\n        nosec_lines[cfg.filename]\n    )\n    for sink in triggers.sinks:\n        for source in triggers.sources:\n            vulnerability, interactive = get_vulnerability(\n                source,\n                sink,\n                triggers,\n                lattice,\n                cfg,\n                interactive,\n                blackbox_mapping\n            )\n            if vulnerability:\n                vulnerabilities_list.append(vulnerability)", "response": "Find vulnerabilities in a CFG."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind vulnerabilities in a list of CFGs from a trigger word file.", "response": "def find_vulnerabilities(\n    cfg_list,\n    blackbox_mapping_file,\n    sources_and_sinks_file,\n    interactive=False,\n    nosec_lines=defaultdict(set)\n):\n    \"\"\"Find vulnerabilities in a list of CFGs from a trigger_word_file.\n\n    Args:\n        cfg_list(list[CFG]): the list of CFGs to scan.\n        blackbox_mapping_file(str)\n        sources_and_sinks_file(str)\n        interactive(bool): determines if we ask the user about blackbox functions not in the mapping file.\n    Returns:\n        A list of vulnerabilities.\n    \"\"\"\n    vulnerabilities = list()\n    definitions = parse(sources_and_sinks_file)\n\n    with open(blackbox_mapping_file) as infile:\n        blackbox_mapping = json.load(infile)\n    for cfg in cfg_list:\n        find_vulnerabilities_in_cfg(\n            cfg,\n            definitions,\n            Lattice(cfg.nodes),\n            blackbox_mapping,\n            vulnerabilities,\n            interactive,\n            nosec_lines\n        )\n\n    if interactive:\n        with open(blackbox_mapping_file, 'w') as outfile:\n            json.dump(blackbox_mapping, outfile, indent=4)\n\n    return vulnerabilities"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_func_nodes():\n    return [definition for definition in project_definitions.values()\n            if isinstance(definition.node, ast.FunctionDef)]", "response": "Get all function nodes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_func_cfg_with_tainted_args(self, definition):\n        log.debug(\"Getting CFG for %s\", definition.name)\n        func_cfg = make_cfg(\n            definition.node,\n            self.project_modules,\n            self.local_modules,\n            definition.path,\n            definition.module_definitions\n        )\n\n        args = Arguments(definition.node.args)\n        if args:\n            function_entry_node = func_cfg.nodes[0]\n            function_entry_node.outgoing = list()\n            first_node_after_args = func_cfg.nodes[1]\n            first_node_after_args.ingoing = list()\n\n            # We are just going to give all the tainted args the lineno of the def\n            definition_lineno = definition.node.lineno\n\n            # Taint all the arguments\n            for i, arg in enumerate(args):\n                node_type = TaintedNode\n                if i == 0 and arg == 'self':\n                    node_type = AssignmentNode\n\n                arg_node = node_type(\n                    label=arg,\n                    left_hand_side=arg,\n                    ast_node=None,\n                    right_hand_side_variables=[],\n                    line_number=definition_lineno,\n                    path=definition.path\n                )\n                function_entry_node.connect(arg_node)\n                # 1 and not 0 so that Entry Node remains first in the list\n                func_cfg.nodes.insert(1, arg_node)\n                arg_node.connect(first_node_after_args)\n\n        return func_cfg", "response": "Build a function cfg and return it with all arguments tainted."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding all route functions and taint all of their arguments.", "response": "def find_route_functions_taint_args(self):\n        \"\"\"Find all route functions and taint all of their arguments.\n\n        Yields:\n            CFG of each route function, with args marked as tainted.\n        \"\"\"\n        for definition in _get_func_nodes():\n            if self.is_route_function(definition.node):\n                yield self.get_func_cfg_with_tainted_args(definition)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrunning find_route_functions_taint_args on each CFG.", "response": "def run(self):\n        \"\"\"Run find_route_functions_taint_args on each CFG.\"\"\"\n        function_cfgs = list()\n        for _ in self.cfg_list:\n            function_cfgs.extend(self.find_route_functions_taint_args())\n        self.cfg_list.extend(function_cfgs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nvisits a JoinedStr node by appending the expression to the result", "response": "def visit_JoinedStr(self, node):\n        \"\"\"\n            JoinedStr(expr* values)\n        \"\"\"\n        self.result += \"f\\'\"\n        self.visit_joined_str(node)\n        self.result += \"'\""}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef visit_FormattedValue(self, node):\n        self.result += '{'\n        self.visit(node.value)\n        self.result += {\n            -1: '',     # no formatting\n            97: '!a',   # ascii formatting\n            114: '!r',  # repr formatting\n            115: '!s',  # string formatting\n        }[node.conversion]\n        if node.format_spec:\n            self.result += ':'\n            self.visit_joined_str(node.format_spec)\n        self.result += '}'", "response": "visit a FormattedValue node by appending to result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncollecting all given cfg nodes and initializes the constraint table with value 0.", "response": "def initialize_constraint_table(cfg_list):\n    \"\"\"Collects all given cfg nodes and initializes the table with value 0.\"\"\"\n    for cfg in cfg_list:\n        constraint_table.update(dict.fromkeys(cfg.nodes, 0))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlooks up all cfg_nodes and joins the bitvectors by using logical or.", "response": "def constraint_join(cfg_nodes):\n    \"\"\"Looks up all cfg_nodes and joins the bitvectors by using logical or.\"\"\"\n    r = 0\n    for e in cfg_nodes:\n        r = r | constraint_table[e]\n    return r"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef store_uploaded_file(title, uploaded_file):\n    upload_dir_path = '%s/static/taskManager/uploads' % (\n        os.path.dirname(os.path.realpath(__file__)))\n    if not os.path.exists(upload_dir_path):\n        os.makedirs(upload_dir_path)\n\n    # A1: Injection (shell)\n    # Let's avoid the file corruption race condition!\n    os.system(\n        \"mv \" +\n        uploaded_file.temporary_file_path() +\n        \" \" +\n        \"%s/%s\" %\n        (upload_dir_path,\n         title))\n\n    return '/static/taskManager/uploads/%s' % (title)", "response": "Stores a temporary uploaded file on disk"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef return_connection_handler(nodes, exit_node):\n    for function_body_node in nodes:\n        if isinstance(function_body_node, ConnectToExitNode):\n            if exit_node not in function_body_node.outgoing:\n                function_body_node.connect(exit_node)", "response": "Connect all return statements to the Exit node."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_my_choices_users():\n\n    user_list = User.objects.order_by('date_joined')\n    user_tuple = []\n    counter = 1\n    for user in user_list:\n        user_tuple.append((counter, user))\n        counter = counter + 1\n    return user_tuple", "response": "Returns a list of all users in the system\n        for the user management page\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_my_choices_tasks(current_proj):\n\n    task_list = []\n    tasks = Task.objects.all()\n    for task in tasks:\n        if task.project == current_proj:\n            task_list.append(task)\n\n    task_tuple = []\n    counter = 1\n    for task in task_list:\n        task_tuple.append((counter, task))\n        counter = counter + 1\n    return task_tuple", "response": "Returns all tasks in the system\nCTYPE for the task management page"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_my_choices_projects():\n\n    proj_list = Project.objects.all()\n    proj_tuple = []\n    counter = 1\n    for proj in proj_list:\n        proj_tuple.append((counter, proj))\n        counter = counter + 1\n    return proj_tuple", "response": "Retrieves all projects in the system\n    for the project management page\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef as_alias_handler(alias_list):\n    list_ = list()\n    for alias in alias_list:\n        if alias.asname:\n            list_.append(alias.asname)\n        else:\n            list_.append(alias.name)\n    return list_", "response": "Returns a list of all the names that will be called."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nhandles aliases in calls in the name.", "response": "def handle_aliases_in_calls(name, import_alias_mapping):\n    \"\"\"Returns either None or the handled alias.\n    Used in add_module.\n    \"\"\"\n    for key, val in import_alias_mapping.items():\n        # e.g. Foo == Foo\n        # e.g. Foo.Bar startswith Foo.\n        if name == key or \\\n                name.startswith(key + '.'):\n\n            # Replace key with val in name\n            # e.g. StarbucksVisitor.Tea -> Eataly.Tea because\n            #   \"from .nested_folder import StarbucksVisitor as Eataly\"\n            return name.replace(key, val)\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nhandling aliases in init files.", "response": "def handle_aliases_in_init_files(name, import_alias_mapping):\n    \"\"\"Returns either None or the handled alias.\n    Used in add_module.\n    \"\"\"\n    for key, val in import_alias_mapping.items():\n        # e.g. Foo == Foo\n        # e.g. Foo.Bar startswith Foo.\n        if name == val or \\\n                name.startswith(val + '.'):\n\n            # Replace val with key in name\n            # e.g. StarbucksVisitor.Tea -> Eataly.Tea because\n            #   \"from .nested_folder import StarbucksVisitor as Eataly\"\n            return name.replace(val, key)\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef handle_fdid_aliases(module_or_package_name, import_alias_mapping):\n    for key, val in import_alias_mapping.items():\n        if module_or_package_name == val:\n            return key\n    return None", "response": "Returns either None or the handled alias."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef not_as_alias_handler(names_list):\n    list_ = list()\n    for alias in names_list:\n        list_.append(alias.name)\n    return list_", "response": "Returns a list of names ignoring any aliases."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a dictionary mapping aliases to their respective name.", "response": "def retrieve_import_alias_mapping(names_list):\n    \"\"\"Creates a dictionary mapping aliases to their respective name.\n    import_alias_names is used in module_definitions.py and visit_Call\"\"\"\n    import_alias_names = dict()\n\n    for alias in names_list:\n        if alias.asname:\n            import_alias_names[alias.asname] = alias.name\n    return import_alias_names"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreplaces any aliases in label with the fully qualified name.", "response": "def fully_qualify_alias_labels(label, aliases):\n    \"\"\"Replace any aliases in label with the fully qualified name.\n\n    Args:\n        label -- A label : str representing a name (e.g. myos.system)\n        aliases -- A dict of {alias: real_name} (e.g. {'myos': 'os'})\n\n    >>> fully_qualify_alias_labels('myos.mycall', {'myos':'os'})\n    'os.mycall'\n    \"\"\"\n    for alias, full_name in aliases.items():\n        if label == alias:\n            return full_name\n        elif label.startswith(alias+'.'):\n            return full_name + label[len(alias):]\n    return label"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list containing tuples of e. g. ('__init__ A. py", "response": "def get_directory_modules(directory):\n    \"\"\"Return a list containing tuples of\n    e.g. ('__init__', 'example/import_test_project/__init__.py')\n    \"\"\"\n    if _local_modules and os.path.dirname(_local_modules[0][1]) == directory:\n        return _local_modules\n\n    if not os.path.isdir(directory):\n        # example/import_test_project/A.py -> example/import_test_project\n        directory = os.path.dirname(directory)\n\n    if directory == '':\n        return _local_modules\n\n    for path in os.listdir(directory):\n        if _is_python_file(path):\n            # A.py -> A\n            module_name = os.path.splitext(path)[0]\n            _local_modules.append((module_name, os.path.join(directory, path)))\n\n    return _local_modules"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of tuples of module_name and path.", "response": "def get_modules(path, prepend_module_root=True):\n    \"\"\"Return a list containing tuples of\n    e.g. ('test_project.utils', 'example/test_project/utils.py')\n    \"\"\"\n    module_root = os.path.split(path)[1]\n    modules = list()\n    for root, directories, filenames in os.walk(path):\n        for filename in filenames:\n            if _is_python_file(filename):\n                directory = os.path.dirname(\n                    os.path.realpath(\n                        os.path.join(\n                            root,\n                            filename\n                        )\n                    )\n                ).split(module_root)[-1].replace(\n                    os.sep,  # e.g. '/'\n                    '.'\n                )\n                directory = directory.replace('.', '', 1)\n\n                module_name_parts = []\n                if prepend_module_root:\n                    module_name_parts.append(module_root)\n                if directory:\n                    module_name_parts.append(directory)\n\n                if filename == '__init__.py':\n                    path = root\n                else:\n                    module_name_parts.append(os.path.splitext(filename)[0])\n                    path = os.path.join(root, filename)\n\n                modules.append(('.'.join(module_name_parts), path))\n\n    return modules"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _convert_to_3(path):  # pragma: no cover\n    try:\n        log.warn('##### Trying to convert %s to Python 3. #####', path)\n        subprocess.call(['2to3', '-w', path])\n    except subprocess.SubprocessError:\n        log.exception('Check if 2to3 is installed. https://docs.python.org/2/library/2to3.html')\n        exit(1)", "response": "Convert python 2 file to python 3."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates an Abstract Syntax Tree using the ast module.", "response": "def generate_ast(path):\n    \"\"\"Generate an Abstract Syntax Tree using the ast module.\n\n        Args:\n            path(str): The path to the file e.g. example/foo/bar.py\n    \"\"\"\n    if os.path.isfile(path):\n        with open(path, 'r') as f:\n            try:\n                tree = ast.parse(f.read())\n                return PytTransformer().visit(tree)\n            except SyntaxError:  # pragma: no cover\n                global recursive\n                if not recursive:\n                    _convert_to_3(path)\n                    recursive = True\n                    return generate_ast(path)\n                else:\n                    raise SyntaxError('The ast module can not parse the file'\n                                      ' and the python 2 to 3 conversion'\n                                      ' also failed.')\n    raise IOError('Input needs to be a file. Path: ' + path)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks whether function uses a route decorator.", "response": "def is_flask_route_function(ast_node):\n    \"\"\"Check whether function uses a route decorator.\"\"\"\n    for decorator in ast_node.decorator_list:\n        if isinstance(decorator, ast.Call):\n            if _get_last_of_iterable(get_call_names(decorator.func)) == 'route':\n                return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef report(\n    vulnerabilities,\n    fileobj,\n    print_sanitised,\n):\n    \"\"\"\n    Prints issues in JSON format.\n    Args:\n        vulnerabilities: list of vulnerabilities to report\n        fileobj: The output file object, which may be sys.stdout\n    \"\"\"\n    TZ_AGNOSTIC_FORMAT = \"%Y-%m-%dT%H:%M:%SZ\"\n    time_string = datetime.utcnow().strftime(TZ_AGNOSTIC_FORMAT)\n\n    machine_output = {\n        'generated_at': time_string,\n        'vulnerabilities': [\n            vuln.as_dict() for vuln in vulnerabilities\n            if print_sanitised or not isinstance(vuln, SanitisedVulnerability)\n        ]\n    }\n\n    result = json.dumps(\n        machine_output,\n        indent=4\n    )\n\n    with fileobj:\n        fileobj.write(result)", "response": "Prints issues in JSON format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving files in the black list that are not in the blacklist.", "response": "def _handle_blacklist(blacklist, dirnames, filenames):\n    \"\"\"remove files/directories in the black list\n\n    dirnames/filenames are usually from os.walk\n    \"\"\"\n    for norecurs in blacklist:\n        if norecurs in dirnames:\n            dirnames.remove(norecurs)\n        elif norecurs in filenames:\n            filenames.remove(norecurs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload a Python module from its name.", "response": "def load_module_from_name(dotted_name, path=None, use_sys=True):\n    \"\"\"Load a Python module from its name.\n\n    :type dotted_name: str\n    :param dotted_name: python name of a module or package\n\n    :type path: list or None\n    :param path:\n      optional list of path where the module or package should be\n      searched (use sys.path if nothing or None is given)\n\n    :type use_sys: bool\n    :param use_sys:\n      boolean indicating whether the sys.modules dictionary should be\n      used or not\n\n\n    :raise ImportError: if the module or package is not found\n\n    :rtype: module\n    :return: the loaded module\n    \"\"\"\n    return load_module_from_modpath(dotted_name.split(\".\"), path, use_sys)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_module_from_modpath(parts, path=None, use_sys=1):\n    if use_sys:\n        try:\n            return sys.modules[\".\".join(parts)]\n        except KeyError:\n            pass\n    modpath = []\n    prevmodule = None\n    for part in parts:\n        modpath.append(part)\n        curname = \".\".join(modpath)\n        module = None\n        if len(modpath) != len(parts):\n            # even with use_sys=False, should try to get outer packages from sys.modules\n            module = sys.modules.get(curname)\n        elif use_sys:\n            # because it may have been indirectly loaded through a parent\n            module = sys.modules.get(curname)\n        if module is None:\n            mp_file, mp_filename, mp_desc = imp.find_module(part, path)\n            module = imp.load_module(curname, mp_file, mp_filename, mp_desc)\n            # mp_file still needs to be closed.\n            if mp_file:\n                mp_file.close()\n        if prevmodule:\n            setattr(prevmodule, part, module)\n        _file = getattr(module, \"__file__\", \"\")\n        prevmodule = module\n        if not _file and util.is_namespace(curname):\n            continue\n        if not _file and len(modpath) != len(parts):\n            raise ImportError(\"no module in %s\" % \".\".join(parts[len(modpath) :]))\n        path = [os.path.dirname(_file)]\n    return module", "response": "Load a python module from its split name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading a Python module from it s path.", "response": "def load_module_from_file(filepath, path=None, use_sys=True, extrapath=None):\n    \"\"\"Load a Python module from it's path.\n\n    :type filepath: str\n    :param filepath: path to the python module or package\n\n    :type path: list or None\n    :param path:\n      optional list of path where the module or package should be\n      searched (use sys.path if nothing or None is given)\n\n    :type use_sys: bool\n    :param use_sys:\n      boolean indicating whether the sys.modules dictionary should be\n      used or not\n\n\n    :raise ImportError: if the module or package is not found\n\n    :rtype: module\n    :return: the loaded module\n    \"\"\"\n    modpath = modpath_from_file(filepath, extrapath)\n    return load_module_from_modpath(modpath, path, use_sys)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_modpath_has_init(path, mod_path):\n    modpath = []\n    for part in mod_path:\n        modpath.append(part)\n        path = os.path.join(path, part)\n        if not _has_init(path):\n            old_namespace = util.is_namespace(\".\".join(modpath))\n            if not old_namespace:\n                return False\n    return True", "response": "check if the path contains some __init__. py and if not return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_relative_base_path(filename, path_to_check):\n    importable_path = None\n    path_to_check = os.path.normcase(path_to_check)\n    abs_filename = os.path.abspath(filename)\n    if os.path.normcase(abs_filename).startswith(path_to_check):\n        importable_path = abs_filename\n\n    real_filename = os.path.realpath(filename)\n    if os.path.normcase(real_filename).startswith(path_to_check):\n        importable_path = real_filename\n\n    if importable_path:\n        base_path = os.path.splitext(importable_path)[0]\n        relative_base_path = base_path[len(path_to_check) :]\n        return [pkg for pkg in relative_base_path.split(os.sep) if pkg]\n\n    return None", "response": "Extracts the relative mod path of the file to import from the passed in path."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive a module path return the file_info corresponding to the file in the order they were imported.", "response": "def file_info_from_modpath(modpath, path=None, context_file=None):\n    \"\"\"given a mod path (i.e. split module / package name), return the\n    corresponding file, giving priority to source file over precompiled\n    file if it exists\n\n    :type modpath: list or tuple\n    :param modpath:\n      split module's name (i.e name of a module or package split\n      on '.')\n      (this means explicit relative imports that start with dots have\n      empty strings in this list!)\n\n    :type path: list or None\n    :param path:\n      optional list of path where the module or package should be\n      searched (use sys.path if nothing or None is given)\n\n    :type context_file: str or None\n    :param context_file:\n      context file to consider, necessary if the identifier has been\n      introduced using a relative import unresolvable in the actual\n      context (i.e. modutils)\n\n    :raise ImportError: if there is no such module in the directory\n\n    :rtype: (str or None, import type)\n    :return:\n      the path to the module's file or None if it's an integrated\n      builtin module such as 'sys'\n    \"\"\"\n    if context_file is not None:\n        context = os.path.dirname(context_file)\n    else:\n        context = context_file\n    if modpath[0] == \"xml\":\n        # handle _xmlplus\n        try:\n            return _spec_from_modpath([\"_xmlplus\"] + modpath[1:], path, context)\n        except ImportError:\n            return _spec_from_modpath(modpath, path, context)\n    elif modpath == [\"os\", \"path\"]:\n        # FIXME: currently ignoring search_path...\n        return spec.ModuleSpec(\n            name=\"os.path\", location=os.path.__file__, module_type=imp.PY_SOURCE\n        )\n    return _spec_from_modpath(modpath, path, context)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngives a dotted name return the module part of the name.", "response": "def get_module_part(dotted_name, context_file=None):\n    \"\"\"given a dotted name return the module part of the name :\n\n    >>> get_module_part('astroid.as_string.dump')\n    'astroid.as_string'\n\n    :type dotted_name: str\n    :param dotted_name: full name of the identifier we are interested in\n\n    :type context_file: str or None\n    :param context_file:\n      context file to consider, necessary if the identifier has been\n      introduced using a relative import unresolvable in the actual\n      context (i.e. modutils)\n\n\n    :raise ImportError: if there is no such module in the directory\n\n    :rtype: str or None\n    :return:\n      the module part of the name or None if we have not been able at\n      all to import the given name\n\n    XXX: deprecated, since it doesn't handle package precedence over module\n    (see #10066)\n    \"\"\"\n    # os.path trick\n    if dotted_name.startswith(\"os.path\"):\n        return \"os.path\"\n    parts = dotted_name.split(\".\")\n    if context_file is not None:\n        # first check for builtin module which won't be considered latter\n        # in that case (path != None)\n        if parts[0] in BUILTIN_MODULES:\n            if len(parts) > 2:\n                raise ImportError(dotted_name)\n            return parts[0]\n        # don't use += or insert, we want a new list to be created !\n    path = None\n    starti = 0\n    if parts[0] == \"\":\n        assert (\n            context_file is not None\n        ), \"explicit relative import, but no context_file?\"\n        path = []  # prevent resolving the import non-relatively\n        starti = 1\n    while parts[starti] == \"\":  # for all further dots: change context\n        starti += 1\n        context_file = os.path.dirname(context_file)\n    for i in range(starti, len(parts)):\n        try:\n            file_from_modpath(\n                parts[starti : i + 1], path=path, context_file=context_file\n            )\n        except ImportError:\n            if i < max(1, len(parts) - 2):\n                raise\n            return \".\".join(parts[:i])\n    return dotted_name"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_module_files(src_directory, blacklist, list_all=False):\n    files = []\n    for directory, dirnames, filenames in os.walk(src_directory):\n        if directory in blacklist:\n            continue\n        _handle_blacklist(blacklist, dirnames, filenames)\n        # check for __init__.py\n        if not list_all and \"__init__.py\" not in filenames:\n            dirnames[:] = ()\n            continue\n        for filename in filenames:\n            if _is_python_file(filename):\n                src = os.path.join(directory, filename)\n                files.append(src)\n    return files", "response": "given a package directory return a list of all available python module s files in the package and its subpackages"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngiving a python module s file name return the absolute path to the source file if it exists", "response": "def get_source_file(filename, include_no_ext=False):\n    \"\"\"given a python module's file name return the matching source file\n    name (the filename will be returned identically if it's already an\n    absolute path to a python source file...)\n\n    :type filename: str\n    :param filename: python module's file name\n\n\n    :raise NoSourceFile: if no source file exists on the file system\n\n    :rtype: str\n    :return: the absolute path of the source file if it exists\n    \"\"\"\n    filename = os.path.abspath(_path_from_filename(filename))\n    base, orig_ext = os.path.splitext(filename)\n    for ext in PY_SOURCE_EXTS:\n        source_path = \"%s.%s\" % (base, ext)\n        if os.path.exists(source_path):\n            return source_path\n    if include_no_ext and not orig_ext and os.path.exists(base):\n        return base\n    raise NoSourceFile(filename)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntry to guess if a module is a standard python module", "response": "def is_standard_module(modname, std_path=None):\n    \"\"\"try to guess if a module is a standard python module (by default,\n    see `std_path` parameter's description)\n\n    :type modname: str\n    :param modname: name of the module we are interested in\n\n    :type std_path: list(str) or tuple(str)\n    :param std_path: list of path considered has standard\n\n\n    :rtype: bool\n    :return:\n      true if the module:\n      - is located on the path listed in one of the directory in `std_path`\n      - is a built-in module\n    \"\"\"\n    modname = modname.split(\".\")[0]\n    try:\n        filename = file_from_modpath([modname])\n    except ImportError:\n        # import failed, i'm probably not so wrong by supposing it's\n        # not standard...\n        return False\n    # modules which are not living in a file are considered standard\n    # (sys and __builtin__ for instance)\n    if filename is None:\n        # we assume there are no namespaces in stdlib\n        return not util.is_namespace(modname)\n    filename = _normalize_path(filename)\n    for path in EXT_LIB_DIRS:\n        if filename.startswith(_cache_normalize_path(path)):\n            return False\n    if std_path is None:\n        std_path = STD_LIB_DIRS\n    for path in std_path:\n        if filename.startswith(_cache_normalize_path(path)):\n            return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn true if the given module name is relative to the given file name", "response": "def is_relative(modname, from_file):\n    \"\"\"return true if the given module name is relative to the given\n    file name\n\n    :type modname: str\n    :param modname: name of the module we are interested in\n\n    :type from_file: str\n    :param from_file:\n      path of the module from which modname has been imported\n\n    :rtype: bool\n    :return:\n      true if the module has been imported relatively to `from_file`\n    \"\"\"\n    if not os.path.isdir(from_file):\n        from_file = os.path.dirname(from_file)\n    if from_file in sys.path:\n        return False\n    try:\n        stream, _, _ = imp.find_module(modname.split(\".\")[0], [from_file])\n\n        # Close the stream to avoid ResourceWarnings.\n        if stream:\n            stream.close()\n        return True\n    except ImportError:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _spec_from_modpath(modpath, path=None, context=None):\n    assert modpath\n    location = None\n    if context is not None:\n        try:\n            found_spec = spec.find_spec(modpath, [context])\n            location = found_spec.location\n        except ImportError:\n            found_spec = spec.find_spec(modpath, path)\n            location = found_spec.location\n    else:\n        found_spec = spec.find_spec(modpath, path)\n    if found_spec.type == spec.ModuleType.PY_COMPILED:\n        try:\n            location = get_source_file(found_spec.location)\n            return found_spec._replace(\n                location=location, type=spec.ModuleType.PY_SOURCE\n            )\n        except NoSourceFile:\n            return found_spec._replace(location=location)\n    elif found_spec.type == spec.ModuleType.C_BUILTIN:\n        # integrated builtin module\n        return found_spec._replace(location=None)\n    elif found_spec.type == spec.ModuleType.PKG_DIRECTORY:\n        location = _has_init(found_spec.location)\n        return found_spec._replace(location=location, type=spec.ModuleType.PY_SOURCE)\n    return found_spec", "response": "given a modpath return the corresponding spec"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _has_init(directory):\n    mod_or_pack = os.path.join(directory, \"__init__\")\n    for ext in PY_SOURCE_EXTS + (\"pyc\", \"pyo\"):\n        if os.path.exists(mod_or_pack + \".\" + ext):\n            return mod_or_pack + \".\" + ext\n    return None", "response": "Returns the path of the first valid __init__ file in the given directory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn an iterator on statements inferred by each statement in *stmts*.", "response": "def _infer_stmts(stmts, context, frame=None):\n    \"\"\"Return an iterator on statements inferred by each statement in *stmts*.\"\"\"\n    inferred = False\n    if context is not None:\n        name = context.lookupname\n        context = context.clone()\n    else:\n        name = None\n        context = contextmod.InferenceContext()\n\n    for stmt in stmts:\n        if stmt is util.Uninferable:\n            yield stmt\n            inferred = True\n            continue\n        context.lookupname = stmt._infer_name(frame, name)\n        try:\n            for inferred in stmt.infer(context=context):\n                yield inferred\n                inferred = True\n        except exceptions.NameInferenceError:\n            continue\n        except exceptions.InferenceError:\n            yield util.Uninferable\n            inferred = True\n    if not inferred:\n        raise exceptions.InferenceError(\n            \"Inference failed for all members of {stmts!r}.\",\n            stmts=stmts,\n            frame=frame,\n            context=context,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninfer getattr yields INDRA statements for the attribute name.", "response": "def igetattr(self, name, context=None):\n        \"\"\"inferred getattr\"\"\"\n        if not context:\n            context = contextmod.InferenceContext()\n        try:\n            # avoid recursively inferring the same attr on the same class\n            if context.push((self._proxied, name)):\n                raise exceptions.InferenceError(\n                    message=\"Cannot infer the same attribute again\",\n                    node=self,\n                    context=context,\n                )\n\n            # XXX frame should be self._proxied, or not ?\n            get_attr = self.getattr(name, context, lookupclass=False)\n            yield from _infer_stmts(\n                self._wrap_attr(get_attr, context), context, frame=self\n            )\n        except exceptions.AttributeInferenceError as error:\n            try:\n                # fallback to class.igetattr since it has some logic to handle\n                # descriptors\n                # But only if the _proxied is the Class.\n                if self._proxied.__class__.__name__ != \"ClassDef\":\n                    raise exceptions.InferenceError(**vars(error)) from error\n                attrs = self._proxied.igetattr(name, context, class_context=False)\n                yield from self._wrap_attr(attrs, context)\n            except exceptions.AttributeInferenceError as error:\n                raise exceptions.InferenceError(**vars(error)) from error"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _wrap_attr(self, attrs, context=None):\n        for attr in attrs:\n            if isinstance(attr, UnboundMethod):\n                if _is_property(attr):\n                    yield from attr.infer_call_result(self, context)\n                else:\n                    yield BoundMethod(attr, self)\n            elif hasattr(attr, \"name\") and attr.name == \"<lambda>\":\n                if attr.args.args and attr.args.args[0].name == \"self\":\n                    yield BoundMethod(attr, self)\n                    continue\n                yield attr\n            else:\n                yield attr", "response": "wrap bound methods of attrs in a InstanceMethod proxies"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninferring what a class instance is returning when called", "response": "def infer_call_result(self, caller, context=None):\n        \"\"\"infer what a class instance is returning when called\"\"\"\n        context = contextmod.bind_context_to_node(context, self)\n        inferred = False\n        for node in self._proxied.igetattr(\"__call__\", context):\n            if node is util.Uninferable or not node.callable():\n                continue\n            for res in node.infer_call_result(caller, context):\n                inferred = True\n                yield res\n        if not inferred:\n            raise exceptions.InferenceError(node=self, caller=caller, context=context)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bool_value(self):\n        context = contextmod.InferenceContext()\n        context.callcontext = contextmod.CallContext(args=[])\n        context.boundnode = self\n\n        try:\n            result = _infer_method_result_truth(self, BOOL_SPECIAL_METHOD, context)\n        except (exceptions.InferenceError, exceptions.AttributeInferenceError):\n            # Fallback to __len__.\n            try:\n                result = _infer_method_result_truth(self, \"__len__\", context)\n            except (exceptions.AttributeInferenceError, exceptions.InferenceError):\n                return True\n        return result", "response": "Infer the truth value for an Instance\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninfer the call result of a method call.", "response": "def infer_call_result(self, caller, context):\n        \"\"\"\n        The boundnode of the regular context with a function called\n        on ``object.__new__`` will be of type ``object``,\n        which is incorrect for the argument in general.\n        If no context is given the ``object.__new__`` call argument will\n        correctly inferred except when inside a call that requires\n        the additional context (such as a classmethod) of the boundnode\n        to determine which class the method was called from\n        \"\"\"\n\n        # If we're unbound method __new__ of builtin object, the result is an\n        # instance of the class given as first argument.\n        if (\n            self._proxied.name == \"__new__\"\n            and self._proxied.parent.frame().qname() == \"%s.object\" % BUILTINS\n        ):\n            if caller.args:\n                node_context = context.extra_context.get(caller.args[0])\n                infer = caller.args[0].infer(context=node_context)\n            else:\n                infer = []\n            return (Instance(x) if x is not util.Uninferable else x for x in infer)\n        return self._proxied.infer_call_result(caller, context)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntry to infer what type.__new__ ( mcs name bases attrs ) returns.", "response": "def _infer_type_new_call(self, caller, context):\n        \"\"\"Try to infer what type.__new__(mcs, name, bases, attrs) returns.\n\n        In order for such call to be valid, the metaclass needs to be\n        a subtype of ``type``, the name needs to be a string, the bases\n        needs to be a tuple of classes\n        \"\"\"\n        from astroid import node_classes\n\n        # Verify the metaclass\n        mcs = next(caller.args[0].infer(context=context))\n        if mcs.__class__.__name__ != \"ClassDef\":\n            # Not a valid first argument.\n            return None\n        if not mcs.is_subtype_of(\"%s.type\" % BUILTINS):\n            # Not a valid metaclass.\n            return None\n\n        # Verify the name\n        name = next(caller.args[1].infer(context=context))\n        if name.__class__.__name__ != \"Const\":\n            # Not a valid name, needs to be a const.\n            return None\n        if not isinstance(name.value, str):\n            # Needs to be a string.\n            return None\n\n        # Verify the bases\n        bases = next(caller.args[2].infer(context=context))\n        if bases.__class__.__name__ != \"Tuple\":\n            # Needs to be a tuple.\n            return None\n        inferred_bases = [next(elt.infer(context=context)) for elt in bases.elts]\n        if any(base.__class__.__name__ != \"ClassDef\" for base in inferred_bases):\n            # All the bases needs to be Classes\n            return None\n\n        # Verify the attributes.\n        attrs = next(caller.args[3].infer(context=context))\n        if attrs.__class__.__name__ != \"Dict\":\n            # Needs to be a dictionary.\n            return None\n        cls_locals = collections.defaultdict(list)\n        for key, value in attrs.items:\n            key = next(key.infer(context=context))\n            value = next(value.infer(context=context))\n            # Ignore non string keys\n            if key.__class__.__name__ == \"Const\" and isinstance(key.value, str):\n                cls_locals[key.value].append(value)\n\n        # Build the class from now.\n        cls = mcs.__class__(\n            name=name.value,\n            lineno=caller.lineno,\n            col_offset=caller.col_offset,\n            parent=caller,\n        )\n        empty = node_classes.Pass()\n        cls.postinit(\n            bases=bases.elts,\n            body=[empty],\n            decorators=[],\n            newstyle=True,\n            metaclass=mcs,\n            keywords=[],\n        )\n        cls.locals = cls_locals\n        return cls"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef attributes(self):\n        return [\n            obj[len(IMPL_PREFIX) :] for obj in dir(self) if obj.startswith(IMPL_PREFIX)\n        ]", "response": "Get the attributes which are exported by this object model."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef lookup(self, name):\n\n        if name in self.attributes():\n            return getattr(self, IMPL_PREFIX + name)\n        raise exceptions.AttributeInferenceError(target=self._instance, attribute=name)", "response": "Look up the given name in the current model and return an AST or an interpreter object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating a bound method that can infer the given obj.", "response": "def _generic_dict_attribute(self, obj, name):\n        \"\"\"Generate a bound method that can infer the given *obj*.\"\"\"\n\n        class DictMethodBoundMethod(astroid.BoundMethod):\n            def infer_call_result(self, caller, context=None):\n                yield obj\n\n        meth = next(self._instance._proxied.igetattr(name))\n        return DictMethodBoundMethod(proxy=meth, bound=self._instance)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef infer_func_form(node, base_type, context=None, enum=False):\n    # node is a Call node, class name as first argument and generated class\n    # attributes as second argument\n\n    # namedtuple or enums list of attributes can be a list of strings or a\n    # whitespace-separate string\n    try:\n        name, names = _find_func_form_arguments(node, context)\n        try:\n            attributes = names.value.replace(\",\", \" \").split()\n        except AttributeError:\n            if not enum:\n                attributes = [\n                    _infer_first(const, context).value for const in names.elts\n                ]\n            else:\n                # Enums supports either iterator of (name, value) pairs\n                # or mappings.\n                if hasattr(names, \"items\") and isinstance(names.items, list):\n                    attributes = [\n                        _infer_first(const[0], context).value\n                        for const in names.items\n                        if isinstance(const[0], nodes.Const)\n                    ]\n                elif hasattr(names, \"elts\"):\n                    # Enums can support either [\"a\", \"b\", \"c\"]\n                    # or [(\"a\", 1), (\"b\", 2), ...], but they can't\n                    # be mixed.\n                    if all(isinstance(const, nodes.Tuple) for const in names.elts):\n                        attributes = [\n                            _infer_first(const.elts[0], context).value\n                            for const in names.elts\n                            if isinstance(const, nodes.Tuple)\n                        ]\n                    else:\n                        attributes = [\n                            _infer_first(const, context).value for const in names.elts\n                        ]\n                else:\n                    raise AttributeError\n                if not attributes:\n                    raise AttributeError\n    except (AttributeError, exceptions.InferenceError):\n        raise UseInferenceDefault()\n\n    # If we can't infer the name of the class, don't crash, up to this point\n    # we know it is a namedtuple anyway.\n    name = name or \"Uninferable\"\n    # we want to return a Class node instance with proper attributes set\n    class_node = nodes.ClassDef(name, \"docstring\")\n    class_node.parent = node.parent\n    # set base class=tuple\n    class_node.bases.append(base_type)\n    # XXX add __init__(*attributes) method\n    for attr in attributes:\n        fake_node = nodes.EmptyNode()\n        fake_node.parent = class_node\n        fake_node.attrname = attr\n        class_node.instance_attrs[attr] = [fake_node]\n    return class_node, name, attributes", "response": "Specific inference function for namedtuple or Python 3 enum."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef infer_named_tuple(node, context=None):\n    tuple_base_name = nodes.Name(name=\"tuple\", parent=node.root())\n    class_node, name, attributes = infer_func_form(\n        node, tuple_base_name, context=context\n    )\n    call_site = arguments.CallSite.from_call(node)\n    func = next(extract_node(\"import collections; collections.namedtuple\").infer())\n    try:\n        rename = next(call_site.infer_argument(func, \"rename\", context)).bool_value()\n    except InferenceError:\n        rename = False\n\n    if rename:\n        attributes = _get_renamed_namedtuple_attributes(attributes)\n\n    replace_args = \", \".join(\"{arg}=None\".format(arg=arg) for arg in attributes)\n    field_def = (\n        \"    {name} = property(lambda self: self[{index:d}], \"\n        \"doc='Alias for field number {index:d}')\"\n    )\n    field_defs = \"\\n\".join(\n        field_def.format(name=name, index=index)\n        for index, name in enumerate(attributes)\n    )\n    fake = AstroidBuilder(MANAGER).string_build(\n        \"\"\"\nclass %(name)s(tuple):\n    __slots__ = ()\n    _fields = %(fields)r\n    def _asdict(self):\n        return self.__dict__\n    @classmethod\n    def _make(cls, iterable, new=tuple.__new__, len=len):\n        return new(cls, iterable)\n    def _replace(self, %(replace_args)s):\n        return self\n    def __getnewargs__(self):\n        return tuple(self)\n%(field_defs)s\n    \"\"\"\n        % {\n            \"name\": name,\n            \"fields\": attributes,\n            \"field_defs\": field_defs,\n            \"replace_args\": replace_args,\n        }\n    )\n    class_node.locals[\"_asdict\"] = fake.body[0].locals[\"_asdict\"]\n    class_node.locals[\"_make\"] = fake.body[0].locals[\"_make\"]\n    class_node.locals[\"_replace\"] = fake.body[0].locals[\"_replace\"]\n    class_node.locals[\"_fields\"] = fake.body[0].locals[\"_fields\"]\n    for attr in attributes:\n        class_node.locals[attr] = fake.body[0].locals[attr]\n    # we use UseInferenceDefault, we can't be a generator so return an iterator\n    return iter([class_node])", "response": "Specific inference function for namedtuple Call node"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninferring a subclass of typing. NamedTuple.", "response": "def infer_typing_namedtuple_class(class_node, context=None):\n    \"\"\"Infer a subclass of typing.NamedTuple\"\"\"\n    # Check if it has the corresponding bases\n    annassigns_fields = [\n        annassign.target.name\n        for annassign in class_node.body\n        if isinstance(annassign, nodes.AnnAssign)\n    ]\n    code = dedent(\n        \"\"\"\n    from collections import namedtuple\n    namedtuple({typename!r}, {fields!r})\n    \"\"\"\n    ).format(typename=class_node.name, fields=\",\".join(annassigns_fields))\n    node = extract_node(code)\n    generated_class_node = next(infer_named_tuple(node, context))\n    for method in class_node.mymethods():\n        generated_class_node.locals[method.name] = [method]\n    return iter((generated_class_node,))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninfer a typing. NamedTuple.", "response": "def infer_typing_namedtuple(node, context=None):\n    \"\"\"Infer a typing.NamedTuple(...) call.\"\"\"\n    # This is essentially a namedtuple with different arguments\n    # so we extract the args and infer a named tuple.\n    try:\n        func = next(node.func.infer())\n    except InferenceError:\n        raise UseInferenceDefault\n\n    if func.qname() != \"typing.NamedTuple\":\n        raise UseInferenceDefault\n\n    if len(node.args) != 2:\n        raise UseInferenceDefault\n\n    if not isinstance(node.args[1], (nodes.List, nodes.Tuple)):\n        raise UseInferenceDefault\n\n    names = []\n    for elt in node.args[1].elts:\n        if not isinstance(elt, (nodes.List, nodes.Tuple)):\n            raise UseInferenceDefault\n        if len(elt.elts) != 2:\n            raise UseInferenceDefault\n        names.append(elt.elts[0].as_string())\n\n    typename = node.args[0].as_string()\n    if names:\n        field_names = \"({},)\".format(\",\".join(names))\n    else:\n        field_names = \"''\"\n    node = extract_node(\n        \"namedtuple({typename}, {fields})\".format(typename=typename, fields=field_names)\n    )\n    return infer_named_tuple(node, context)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _visit_or_none(node, attr, visitor, parent, visit=\"visit\", **kws):\n    value = getattr(node, attr, None)\n    if value:\n        return getattr(visitor, visit)(value, parent, **kws)\n\n    return None", "response": "Returns the node s attribute value or None if the node does not have an attribute."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nvisiting a Module node by returning a fresh instance of it", "response": "def visit_module(self, node, modname, modpath, package):\n        \"\"\"visit a Module node by returning a fresh instance of it\"\"\"\n        node, doc = self._get_doc(node)\n        newnode = nodes.Module(\n            name=modname,\n            doc=doc,\n            file=modpath,\n            path=[modpath],\n            package=package,\n            parent=None,\n        )\n        newnode.postinit([self.visit(child, newnode) for child in node.body])\n        return newnode"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsaves assignment situation since node. parent is not available yet", "response": "def _save_assignment(self, node, name=None):\n        \"\"\"save assignement situation since node.parent is not available yet\"\"\"\n        if self._global_names and node.name in self._global_names[-1]:\n            node.root().set_local(node.name, node)\n        else:\n            node.parent.set_local(node.name, node)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef visit_arguments(self, node, parent):\n        vararg, kwarg = node.vararg, node.kwarg\n        if PY34:\n            newnode = nodes.Arguments(\n                vararg.arg if vararg else None, kwarg.arg if kwarg else None, parent\n            )\n        else:\n            newnode = nodes.Arguments(vararg, kwarg, parent)\n        args = [self.visit(child, newnode) for child in node.args]\n        defaults = [self.visit(child, newnode) for child in node.defaults]\n        varargannotation = None\n        kwargannotation = None\n        # change added in 82732 (7c5c678e4164), vararg and kwarg\n        # are instances of `_ast.arg`, not strings\n        if vararg:\n            if PY34:\n                if node.vararg.annotation:\n                    varargannotation = self.visit(node.vararg.annotation, newnode)\n                vararg = vararg.arg\n        if kwarg:\n            if PY34:\n                if node.kwarg.annotation:\n                    kwargannotation = self.visit(node.kwarg.annotation, newnode)\n                kwarg = kwarg.arg\n        if PY3:\n            kwonlyargs = [self.visit(child, newnode) for child in node.kwonlyargs]\n            kw_defaults = [\n                self.visit(child, newnode) if child else None\n                for child in node.kw_defaults\n            ]\n            annotations = [\n                self.visit(arg.annotation, newnode) if arg.annotation else None\n                for arg in node.args\n            ]\n            kwonlyargs_annotations = [\n                self.visit(arg.annotation, newnode) if arg.annotation else None\n                for arg in node.kwonlyargs\n            ]\n        else:\n            kwonlyargs = []\n            kw_defaults = []\n            annotations = []\n            kwonlyargs_annotations = []\n\n        newnode.postinit(\n            args=args,\n            defaults=defaults,\n            kwonlyargs=kwonlyargs,\n            kw_defaults=kw_defaults,\n            annotations=annotations,\n            kwonlyargs_annotations=kwonlyargs_annotations,\n            varargannotation=varargannotation,\n            kwargannotation=kwargannotation,\n        )\n        # save argument names in locals:\n        if vararg:\n            newnode.parent.set_local(vararg, newnode)\n        if kwarg:\n            newnode.parent.set_local(kwarg, newnode)\n        return newnode", "response": "visit a Arguments node by returning a fresh instance of it"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nvisiting a Assert node by returning a fresh instance of it", "response": "def visit_assert(self, node, parent):\n        \"\"\"visit a Assert node by returning a fresh instance of it\"\"\"\n        newnode = nodes.Assert(node.lineno, node.col_offset, parent)\n        if node.msg:\n            msg = self.visit(node.msg, newnode)\n        else:\n            msg = None\n        newnode.postinit(self.visit(node.test, newnode), msg)\n        return newnode"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nvisit a Assign node by returning a fresh instance of it", "response": "def visit_assign(self, node, parent):\n        \"\"\"visit a Assign node by returning a fresh instance of it\"\"\"\n        type_annotation = self.check_type_comment(node)\n        newnode = nodes.Assign(node.lineno, node.col_offset, parent)\n        newnode.postinit(\n            targets=[self.visit(child, newnode) for child in node.targets],\n            value=self.visit(node.value, newnode),\n            type_annotation=type_annotation,\n        )\n        return newnode"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef visit_assignname(self, node, parent, node_name=None):\n        newnode = nodes.AssignName(\n            node_name,\n            getattr(node, \"lineno\", None),\n            getattr(node, \"col_offset\", None),\n            parent,\n        )\n        self._save_assignment(newnode)\n        return newnode", "response": "visit a AssignName node and return a fresh instance of it"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef visit_augassign(self, node, parent):\n        newnode = nodes.AugAssign(\n            self._bin_op_classes[type(node.op)] + \"=\",\n            node.lineno,\n            node.col_offset,\n            parent,\n        )\n        newnode.postinit(\n            self.visit(node.target, newnode), self.visit(node.value, newnode)\n        )\n        return newnode", "response": "visit a AugAssign node by returning a fresh instance of it"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nvisit a Backquote node by returning a fresh instance of it", "response": "def visit_repr(self, node, parent):\n        \"\"\"visit a Backquote node by returning a fresh instance of it\"\"\"\n        newnode = nodes.Repr(node.lineno, node.col_offset, parent)\n        newnode.postinit(self.visit(node.value, newnode))\n        return newnode"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef visit_binop(self, node, parent):\n        newnode = nodes.BinOp(\n            self._bin_op_classes[type(node.op)], node.lineno, node.col_offset, parent\n        )\n        newnode.postinit(\n            self.visit(node.left, newnode), self.visit(node.right, newnode)\n        )\n        return newnode", "response": "visit a BinOp node by returning a fresh instance of it"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nvisits a BoolOp node by returning a fresh instance of it", "response": "def visit_boolop(self, node, parent):\n        \"\"\"visit a BoolOp node by returning a fresh instance of it\"\"\"\n        newnode = nodes.BoolOp(\n            self._bool_op_classes[type(node.op)], node.lineno, node.col_offset, parent\n        )\n        newnode.postinit([self.visit(child, newnode) for child in node.values])\n        return newnode"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nvisiting a Break node by returning a fresh instance of it", "response": "def visit_break(self, node, parent):\n        \"\"\"visit a Break node by returning a fresh instance of it\"\"\"\n        return nodes.Break(\n            getattr(node, \"lineno\", None), getattr(node, \"col_offset\", None), parent\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nvisiting a CallFunc node by returning a fresh instance of it", "response": "def visit_call(self, node, parent):\n        \"\"\"visit a CallFunc node by returning a fresh instance of it\"\"\"\n        newnode = nodes.Call(node.lineno, node.col_offset, parent)\n        starargs = _visit_or_none(node, \"starargs\", self, newnode)\n        kwargs = _visit_or_none(node, \"kwargs\", self, newnode)\n        args = [self.visit(child, newnode) for child in node.args]\n\n        if node.keywords:\n            keywords = [self.visit(child, newnode) for child in node.keywords]\n        else:\n            keywords = None\n        if starargs:\n            new_starargs = nodes.Starred(\n                col_offset=starargs.col_offset,\n                lineno=starargs.lineno,\n                parent=starargs.parent,\n            )\n            new_starargs.postinit(value=starargs)\n            args.append(new_starargs)\n        if kwargs:\n            new_kwargs = nodes.Keyword(\n                arg=None,\n                col_offset=kwargs.col_offset,\n                lineno=kwargs.lineno,\n                parent=kwargs.parent,\n            )\n            new_kwargs.postinit(value=kwargs)\n            if keywords:\n                keywords.append(new_kwargs)\n            else:\n                keywords = [new_kwargs]\n\n        newnode.postinit(self.visit(node.func, newnode), args, keywords)\n        return newnode"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nvisit a ClassDef node to become astroid", "response": "def visit_classdef(self, node, parent, newstyle=None):\n        \"\"\"visit a ClassDef node to become astroid\"\"\"\n        node, doc = self._get_doc(node)\n        newnode = nodes.ClassDef(node.name, doc, node.lineno, node.col_offset, parent)\n        metaclass = None\n        if PY3:\n            for keyword in node.keywords:\n                if keyword.arg == \"metaclass\":\n                    metaclass = self.visit(keyword, newnode).value\n                    break\n        if node.decorator_list:\n            decorators = self.visit_decorators(node, newnode)\n        else:\n            decorators = None\n        newnode.postinit(\n            [self.visit(child, newnode) for child in node.bases],\n            [self.visit(child, newnode) for child in node.body],\n            decorators,\n            newstyle,\n            metaclass,\n            [\n                self.visit(kwd, newnode)\n                for kwd in node.keywords\n                if kwd.arg != \"metaclass\"\n            ]\n            if PY3\n            else [],\n        )\n        return newnode"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef visit_const(self, node, parent):\n        return nodes.Const(\n            node.value,\n            getattr(node, \"lineno\", None),\n            getattr(node, \"col_offset\", None),\n            parent,\n        )", "response": "visit a Const node by returning a fresh instance of it"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef visit_continue(self, node, parent):\n        return nodes.Continue(\n            getattr(node, \"lineno\", None), getattr(node, \"col_offset\", None), parent\n        )", "response": "visit a Continue node by returning a fresh instance of it"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nvisiting a Compare node by returning a fresh instance of it", "response": "def visit_compare(self, node, parent):\n        \"\"\"visit a Compare node by returning a fresh instance of it\"\"\"\n        newnode = nodes.Compare(node.lineno, node.col_offset, parent)\n        newnode.postinit(\n            self.visit(node.left, newnode),\n            [\n                (self._cmp_op_classes[op.__class__], self.visit(expr, newnode))\n                for (op, expr) in zip(node.ops, node.comparators)\n            ],\n        )\n        return newnode"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nvisit a Comprehension node by returning a fresh instance of it", "response": "def visit_comprehension(self, node, parent):\n        \"\"\"visit a Comprehension node by returning a fresh instance of it\"\"\"\n        newnode = nodes.Comprehension(parent)\n        newnode.postinit(\n            self.visit(node.target, newnode),\n            self.visit(node.iter, newnode),\n            [self.visit(child, newnode) for child in node.ifs],\n            getattr(node, \"is_async\", None),\n        )\n        return newnode"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef visit_decorators(self, node, parent):\n        # /!\\ node is actually a _ast.FunctionDef node while\n        # parent is an astroid.nodes.FunctionDef node\n        newnode = nodes.Decorators(node.lineno, node.col_offset, parent)\n        newnode.postinit([self.visit(child, newnode) for child in node.decorator_list])\n        return newnode", "response": "visit a Decorators node by returning a fresh instance of it"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef visit_delete(self, node, parent):\n        newnode = nodes.Delete(node.lineno, node.col_offset, parent)\n        newnode.postinit([self.visit(child, newnode) for child in node.targets])\n        return newnode", "response": "visit a Delete node by returning a fresh instance of it"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef visit_dict(self, node, parent):\n        newnode = nodes.Dict(node.lineno, node.col_offset, parent)\n        items = list(self._visit_dict_items(node, parent, newnode))\n        newnode.postinit(items)\n        return newnode", "response": "visit a Dict node by returning a fresh instance of it"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nvisit a DictComp node by returning a fresh instance of it", "response": "def visit_dictcomp(self, node, parent):\n        \"\"\"visit a DictComp node by returning a fresh instance of it\"\"\"\n        newnode = nodes.DictComp(node.lineno, node.col_offset, parent)\n        newnode.postinit(\n            self.visit(node.key, newnode),\n            self.visit(node.value, newnode),\n            [self.visit(child, newnode) for child in node.generators],\n        )\n        return newnode"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nvisit a Expr node by returning a fresh instance of it", "response": "def visit_expr(self, node, parent):\n        \"\"\"visit a Expr node by returning a fresh instance of it\"\"\"\n        newnode = nodes.Expr(node.lineno, node.col_offset, parent)\n        newnode.postinit(self.visit(node.value, newnode))\n        return newnode"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef visit_ellipsis(self, node, parent):\n        return nodes.Ellipsis(\n            getattr(node, \"lineno\", None), getattr(node, \"col_offset\", None), parent\n        )", "response": "visit an Ellipsis node by returning a fresh instance of it"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nvisiting an EmptyNode node by returning a fresh instance of it", "response": "def visit_emptynode(self, node, parent):\n        \"\"\"visit an EmptyNode node by returning a fresh instance of it\"\"\"\n        return nodes.EmptyNode(\n            getattr(node, \"lineno\", None), getattr(node, \"col_offset\", None), parent\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nvisit an Exec node by returning a fresh instance of it", "response": "def visit_exec(self, node, parent):\n        \"\"\"visit an Exec node by returning a fresh instance of it\"\"\"\n        newnode = nodes.Exec(node.lineno, node.col_offset, parent)\n        newnode.postinit(\n            self.visit(node.body, newnode),\n            _visit_or_none(node, \"globals\", self, newnode),\n            _visit_or_none(node, \"locals\", self, newnode),\n        )\n        return newnode"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef visit_extslice(self, node, parent):\n        newnode = nodes.ExtSlice(parent=parent)\n        newnode.postinit([self.visit(dim, newnode) for dim in node.dims])\n        return newnode", "response": "visit an ExtSlice node by returning a fresh instance of it"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nvisit a For node by returning a fresh instance of it", "response": "def _visit_for(self, cls, node, parent):\n        \"\"\"visit a For node by returning a fresh instance of it\"\"\"\n        newnode = cls(node.lineno, node.col_offset, parent)\n        type_annotation = self.check_type_comment(node)\n        newnode.postinit(\n            target=self.visit(node.target, newnode),\n            iter=self.visit(node.iter, newnode),\n            body=[self.visit(child, newnode) for child in node.body],\n            orelse=[self.visit(child, newnode) for child in node.orelse],\n            type_annotation=type_annotation,\n        )\n        return newnode"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef visit_importfrom(self, node, parent):\n        names = [(alias.name, alias.asname) for alias in node.names]\n        newnode = nodes.ImportFrom(\n            node.module or \"\",\n            names,\n            node.level or None,\n            getattr(node, \"lineno\", None),\n            getattr(node, \"col_offset\", None),\n            parent,\n        )\n        # store From names to add them to locals after building\n        self._import_from_nodes.append(newnode)\n        return newnode", "response": "visit an ImportFrom node by returning a fresh instance of it"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nvisit a FunctionDef node to become astroid", "response": "def _visit_functiondef(self, cls, node, parent):\n        \"\"\"visit an FunctionDef node to become astroid\"\"\"\n        self._global_names.append({})\n        node, doc = self._get_doc(node)\n        newnode = cls(node.name, doc, node.lineno, node.col_offset, parent)\n        if node.decorator_list:\n            decorators = self.visit_decorators(node, newnode)\n        else:\n            decorators = None\n        if PY3 and node.returns:\n            returns = self.visit(node.returns, newnode)\n        else:\n            returns = None\n\n        type_comment_args = type_comment_returns = None\n        type_comment_annotation = self.check_function_type_comment(node)\n        if type_comment_annotation:\n            type_comment_returns, type_comment_args = type_comment_annotation\n        newnode.postinit(\n            args=self.visit(node.args, newnode),\n            body=[self.visit(child, newnode) for child in node.body],\n            decorators=decorators,\n            returns=returns,\n            type_comment_returns=type_comment_returns,\n            type_comment_args=type_comment_args,\n        )\n        self._global_names.pop()\n        return newnode"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nvisit a GeneratorExp node by returning a fresh instance of it", "response": "def visit_generatorexp(self, node, parent):\n        \"\"\"visit a GeneratorExp node by returning a fresh instance of it\"\"\"\n        newnode = nodes.GeneratorExp(node.lineno, node.col_offset, parent)\n        newnode.postinit(\n            self.visit(node.elt, newnode),\n            [self.visit(child, newnode) for child in node.generators],\n        )\n        return newnode"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nvisit an Attribute node by returning a fresh instance of it", "response": "def visit_attribute(self, node, parent):\n        \"\"\"visit an Attribute node by returning a fresh instance of it\"\"\"\n        context = self._get_context(node)\n        if context == astroid.Del:\n            # FIXME : maybe we should reintroduce and visit_delattr ?\n            # for instance, deactivating assign_ctx\n            newnode = nodes.DelAttr(node.attr, node.lineno, node.col_offset, parent)\n        elif context == astroid.Store:\n            newnode = nodes.AssignAttr(node.attr, node.lineno, node.col_offset, parent)\n            # Prohibit a local save if we are in an ExceptHandler.\n            if not isinstance(parent, astroid.ExceptHandler):\n                self._delayed_assattr.append(newnode)\n        else:\n            newnode = nodes.Attribute(node.attr, node.lineno, node.col_offset, parent)\n        newnode.postinit(self.visit(node.value, newnode))\n        return newnode"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nvisit a Global node to become astroid", "response": "def visit_global(self, node, parent):\n        \"\"\"visit a Global node to become astroid\"\"\"\n        newnode = nodes.Global(\n            node.names,\n            getattr(node, \"lineno\", None),\n            getattr(node, \"col_offset\", None),\n            parent,\n        )\n        if self._global_names:  # global at the module level, no effect\n            for name in node.names:\n                self._global_names[-1].setdefault(name, []).append(newnode)\n        return newnode"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nvisits an If node by returning a fresh instance of it", "response": "def visit_if(self, node, parent):\n        \"\"\"visit an If node by returning a fresh instance of it\"\"\"\n        newnode = nodes.If(node.lineno, node.col_offset, parent)\n        newnode.postinit(\n            self.visit(node.test, newnode),\n            [self.visit(child, newnode) for child in node.body],\n            [self.visit(child, newnode) for child in node.orelse],\n        )\n        return newnode"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nvisiting a IfExp node by returning a fresh instance of it", "response": "def visit_ifexp(self, node, parent):\n        \"\"\"visit a IfExp node by returning a fresh instance of it\"\"\"\n        newnode = nodes.IfExp(node.lineno, node.col_offset, parent)\n        newnode.postinit(\n            self.visit(node.test, newnode),\n            self.visit(node.body, newnode),\n            self.visit(node.orelse, newnode),\n        )\n        return newnode"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nvisit an Import node by returning a fresh instance of it", "response": "def visit_import(self, node, parent):\n        \"\"\"visit a Import node by returning a fresh instance of it\"\"\"\n        names = [(alias.name, alias.asname) for alias in node.names]\n        newnode = nodes.Import(\n            names,\n            getattr(node, \"lineno\", None),\n            getattr(node, \"col_offset\", None),\n            parent,\n        )\n        # save import names in parent's locals:\n        for (name, asname) in newnode.names:\n            name = asname or name\n            parent.set_local(name.split(\".\")[0], newnode)\n        return newnode"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef visit_index(self, node, parent):\n        newnode = nodes.Index(parent=parent)\n        newnode.postinit(self.visit(node.value, newnode))\n        return newnode", "response": "visit a Index node by returning a fresh instance of it"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef visit_keyword(self, node, parent):\n        newnode = nodes.Keyword(node.arg, parent=parent)\n        newnode.postinit(self.visit(node.value, newnode))\n        return newnode", "response": "visit a Keyword node by returning a fresh instance of it"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef visit_lambda(self, node, parent):\n        newnode = nodes.Lambda(node.lineno, node.col_offset, parent)\n        newnode.postinit(self.visit(node.args, newnode), self.visit(node.body, newnode))\n        return newnode", "response": "visit a Lambda node by returning a fresh instance of it"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef visit_list(self, node, parent):\n        context = self._get_context(node)\n        newnode = nodes.List(\n            ctx=context, lineno=node.lineno, col_offset=node.col_offset, parent=parent\n        )\n        newnode.postinit([self.visit(child, newnode) for child in node.elts])\n        return newnode", "response": "visit a List node by returning a fresh instance of it"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nvisit a ListComp node by returning a fresh instance of it", "response": "def visit_listcomp(self, node, parent):\n        \"\"\"visit a ListComp node by returning a fresh instance of it\"\"\"\n        newnode = nodes.ListComp(node.lineno, node.col_offset, parent)\n        newnode.postinit(\n            self.visit(node.elt, newnode),\n            [self.visit(child, newnode) for child in node.generators],\n        )\n        return newnode"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef visit_name(self, node, parent):\n        context = self._get_context(node)\n        # True and False can be assigned to something in py2x, so we have to\n        # check first the context.\n        if context == astroid.Del:\n            newnode = nodes.DelName(node.id, node.lineno, node.col_offset, parent)\n        elif context == astroid.Store:\n            newnode = nodes.AssignName(node.id, node.lineno, node.col_offset, parent)\n        elif node.id in CONST_NAME_TRANSFORMS:\n            newnode = nodes.Const(\n                CONST_NAME_TRANSFORMS[node.id],\n                getattr(node, \"lineno\", None),\n                getattr(node, \"col_offset\", None),\n                parent,\n            )\n            return newnode\n        else:\n            newnode = nodes.Name(node.id, node.lineno, node.col_offset, parent)\n        # XXX REMOVE me :\n        if context in (astroid.Del, astroid.Store):  # 'Aug' ??\n            self._save_assignment(newnode)\n        return newnode", "response": "visit a Name node by returning a fresh instance of it"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef visit_constant(self, node, parent):\n        return nodes.Const(\n            node.value,\n            getattr(node, \"lineno\", None),\n            getattr(node, \"col_offset\", None),\n            parent,\n        )", "response": "visit a Constant node by returning a fresh instance of Const"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nvisit a String node by returning a fresh instance of Const", "response": "def visit_str(self, node, parent):\n        \"\"\"visit a String/Bytes node by returning a fresh instance of Const\"\"\"\n        return nodes.Const(\n            node.s,\n            getattr(node, \"lineno\", None),\n            getattr(node, \"col_offset\", None),\n            parent,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nvisit a Num node by returning a fresh instance of Const", "response": "def visit_num(self, node, parent):\n        \"\"\"visit a Num node by returning a fresh instance of Const\"\"\"\n        return nodes.Const(\n            node.n,\n            getattr(node, \"lineno\", None),\n            getattr(node, \"col_offset\", None),\n            parent,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef visit_pass(self, node, parent):\n        return nodes.Pass(node.lineno, node.col_offset, parent)", "response": "visit a Pass node by returning a fresh instance of it"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef visit_print(self, node, parent):\n        newnode = nodes.Print(node.nl, node.lineno, node.col_offset, parent)\n        newnode.postinit(\n            _visit_or_none(node, \"dest\", self, newnode),\n            [self.visit(child, newnode) for child in node.values],\n        )\n        return newnode", "response": "visit a Print node by returning a fresh instance of it"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nvisits a Raise node by returning a fresh instance of it", "response": "def visit_raise(self, node, parent):\n        \"\"\"visit a Raise node by returning a fresh instance of it\"\"\"\n        newnode = nodes.Raise(node.lineno, node.col_offset, parent)\n        # pylint: disable=too-many-function-args\n        newnode.postinit(\n            _visit_or_none(node, \"type\", self, newnode),\n            _visit_or_none(node, \"inst\", self, newnode),\n            _visit_or_none(node, \"tback\", self, newnode),\n        )\n        return newnode"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef visit_return(self, node, parent):\n        newnode = nodes.Return(node.lineno, node.col_offset, parent)\n        if node.value is not None:\n            newnode.postinit(self.visit(node.value, newnode))\n        return newnode", "response": "visit a Return node by returning a fresh instance of it"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef visit_set(self, node, parent):\n        newnode = nodes.Set(node.lineno, node.col_offset, parent)\n        newnode.postinit([self.visit(child, newnode) for child in node.elts])\n        return newnode", "response": "visit a Set node by returning a fresh instance of it"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef visit_setcomp(self, node, parent):\n        newnode = nodes.SetComp(node.lineno, node.col_offset, parent)\n        newnode.postinit(\n            self.visit(node.elt, newnode),\n            [self.visit(child, newnode) for child in node.generators],\n        )\n        return newnode", "response": "visit a SetComp node by returning a fresh instance of it"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef visit_slice(self, node, parent):\n        newnode = nodes.Slice(parent=parent)\n        newnode.postinit(\n            _visit_or_none(node, \"lower\", self, newnode),\n            _visit_or_none(node, \"upper\", self, newnode),\n            _visit_or_none(node, \"step\", self, newnode),\n        )\n        return newnode", "response": "visit a Slice node by returning a fresh instance of it"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef visit_subscript(self, node, parent):\n        context = self._get_context(node)\n        newnode = nodes.Subscript(\n            ctx=context, lineno=node.lineno, col_offset=node.col_offset, parent=parent\n        )\n        newnode.postinit(\n            self.visit(node.value, newnode), self.visit(node.slice, newnode)\n        )\n        return newnode", "response": "visit a Subscript node by returning a fresh instance of it"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef visit_tryexcept(self, node, parent):\n        newnode = nodes.TryExcept(node.lineno, node.col_offset, parent)\n        newnode.postinit(\n            [self.visit(child, newnode) for child in node.body],\n            [self.visit(child, newnode) for child in node.handlers],\n            [self.visit(child, newnode) for child in node.orelse],\n        )\n        return newnode", "response": "visit a TryExcept node by returning a fresh instance of it"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef visit_tryfinally(self, node, parent):\n        newnode = nodes.TryFinally(node.lineno, node.col_offset, parent)\n        newnode.postinit(\n            [self.visit(child, newnode) for child in node.body],\n            [self.visit(n, newnode) for n in node.finalbody],\n        )\n        return newnode", "response": "visit a TryFinally node by returning a fresh instance of it"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nvisit a Tuple node by returning a fresh instance of it", "response": "def visit_tuple(self, node, parent):\n        \"\"\"visit a Tuple node by returning a fresh instance of it\"\"\"\n        context = self._get_context(node)\n        newnode = nodes.Tuple(\n            ctx=context, lineno=node.lineno, col_offset=node.col_offset, parent=parent\n        )\n        newnode.postinit([self.visit(child, newnode) for child in node.elts])\n        return newnode"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef visit_unaryop(self, node, parent):\n        newnode = nodes.UnaryOp(\n            self._unary_op_classes[node.op.__class__],\n            node.lineno,\n            node.col_offset,\n            parent,\n        )\n        newnode.postinit(self.visit(node.operand, newnode))\n        return newnode", "response": "visit a UnaryOp node by returning a fresh instance of it"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef visit_while(self, node, parent):\n        newnode = nodes.While(node.lineno, node.col_offset, parent)\n        newnode.postinit(\n            self.visit(node.test, newnode),\n            [self.visit(child, newnode) for child in node.body],\n            [self.visit(child, newnode) for child in node.orelse],\n        )\n        return newnode", "response": "visit a While node by returning a fresh instance of it"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nvisit a Yield node by returning a fresh instance of it", "response": "def visit_yield(self, node, parent):\n        \"\"\"visit a Yield node by returning a fresh instance of it\"\"\"\n        newnode = nodes.Yield(node.lineno, node.col_offset, parent)\n        if node.value is not None:\n            newnode.postinit(self.visit(node.value, newnode))\n        return newnode"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef visit_arg(self, node, parent):\n        return self.visit_assignname(node, parent, node.arg)", "response": "visit an arg node by returning a fresh AssName instance"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nvisiting an ExceptHandler node by returning a fresh instance of it", "response": "def visit_excepthandler(self, node, parent):\n        \"\"\"visit an ExceptHandler node by returning a fresh instance of it\"\"\"\n        newnode = nodes.ExceptHandler(node.lineno, node.col_offset, parent)\n        if node.name:\n            name = self.visit_assignname(node, newnode, node.name)\n        else:\n            name = None\n        newnode.postinit(\n            _visit_or_none(node, \"type\", self, newnode),\n            name,\n            [self.visit(child, newnode) for child in node.body],\n        )\n        return newnode"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef visit_nonlocal(self, node, parent):\n        return nodes.Nonlocal(\n            node.names,\n            getattr(node, \"lineno\", None),\n            getattr(node, \"col_offset\", None),\n            parent,\n        )", "response": "visit a Nonlocal node and return a new instance of it"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nvisiting a Starred node and return a fresh instance of it", "response": "def visit_starred(self, node, parent):\n        \"\"\"visit a Starred node and return a new instance of it\"\"\"\n        context = self._get_context(node)\n        newnode = nodes.Starred(\n            ctx=context, lineno=node.lineno, col_offset=node.col_offset, parent=parent\n        )\n        newnode.postinit(self.visit(node.value, newnode))\n        return newnode"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nvisit an AnnAssign node by returning a fresh instance of it", "response": "def visit_annassign(self, node, parent):\n        \"\"\"visit an AnnAssign node by returning a fresh instance of it\"\"\"\n        newnode = nodes.AnnAssign(node.lineno, node.col_offset, parent)\n        annotation = _visit_or_none(node, \"annotation\", self, newnode)\n        newnode.postinit(\n            target=self.visit(node.target, newnode),\n            annotation=annotation,\n            simple=node.simple,\n            value=_visit_or_none(node, \"value\", self, newnode),\n        )\n        return newnode"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ast_from_file(self, filepath, modname=None, fallback=True, source=False):\n        try:\n            filepath = modutils.get_source_file(filepath, include_no_ext=True)\n            source = True\n        except modutils.NoSourceFile:\n            pass\n        if modname is None:\n            try:\n                modname = \".\".join(modutils.modpath_from_file(filepath))\n            except ImportError:\n                modname = filepath\n        if (\n            modname in self.astroid_cache\n            and self.astroid_cache[modname].file == filepath\n        ):\n            return self.astroid_cache[modname]\n        if source:\n            from astroid.builder import AstroidBuilder\n\n            return AstroidBuilder(self).file_build(filepath, modname)\n        if fallback and modname:\n            return self.ast_from_module_name(modname)\n        raise exceptions.AstroidBuildingError(\n            \"Unable to build an AST for {path}.\", path=filepath\n        )", "response": "given a file path return the astroid object"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ast_from_module_name(self, modname, context_file=None):\n        if modname in self.astroid_cache:\n            return self.astroid_cache[modname]\n        if modname == \"__main__\":\n            return self._build_stub_module(modname)\n        old_cwd = os.getcwd()\n        if context_file:\n            os.chdir(os.path.dirname(context_file))\n        try:\n            found_spec = self.file_from_module_name(modname, context_file)\n            if found_spec.type == spec.ModuleType.PY_ZIPMODULE:\n                module = self.zip_import_data(found_spec.location)\n                if module is not None:\n                    return module\n\n            elif found_spec.type in (\n                spec.ModuleType.C_BUILTIN,\n                spec.ModuleType.C_EXTENSION,\n            ):\n                if (\n                    found_spec.type == spec.ModuleType.C_EXTENSION\n                    and not self._can_load_extension(modname)\n                ):\n                    return self._build_stub_module(modname)\n                try:\n                    module = modutils.load_module_from_name(modname)\n                except Exception as ex:\n                    raise exceptions.AstroidImportError(\n                        \"Loading {modname} failed with:\\n{error}\",\n                        modname=modname,\n                        path=found_spec.location,\n                    ) from ex\n                return self.ast_from_module(module, modname)\n\n            elif found_spec.type == spec.ModuleType.PY_COMPILED:\n                raise exceptions.AstroidImportError(\n                    \"Unable to load compiled module {modname}.\",\n                    modname=modname,\n                    path=found_spec.location,\n                )\n\n            elif found_spec.type == spec.ModuleType.PY_NAMESPACE:\n                return self._build_namespace_module(\n                    modname, found_spec.submodule_search_locations\n                )\n\n            if found_spec.location is None:\n                raise exceptions.AstroidImportError(\n                    \"Can't find a file for module {modname}.\", modname=modname\n                )\n\n            return self.ast_from_file(found_spec.location, modname, fallback=False)\n        except exceptions.AstroidBuildingError as e:\n            for hook in self._failed_import_hooks:\n                try:\n                    return hook(modname)\n                except exceptions.AstroidBuildingError:\n                    pass\n            raise e\n        finally:\n            os.chdir(old_cwd)", "response": "returns the astroid object for the given module name"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ast_from_module(self, module, modname=None):\n        modname = modname or module.__name__\n        if modname in self.astroid_cache:\n            return self.astroid_cache[modname]\n        try:\n            # some builtin modules don't have __file__ attribute\n            filepath = module.__file__\n            if modutils.is_python_source(filepath):\n                return self.ast_from_file(filepath, modname)\n        except AttributeError:\n            pass\n        from astroid.builder import AstroidBuilder\n\n        return AstroidBuilder(self).module_build(module, modname)", "response": "given an imported module return the astroid object"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting astroid for the given class", "response": "def ast_from_class(self, klass, modname=None):\n        \"\"\"get astroid for the given class\"\"\"\n        if modname is None:\n            try:\n                modname = klass.__module__\n            except AttributeError as exc:\n                raise exceptions.AstroidBuildingError(\n                    \"Unable to get module for class {class_name}.\",\n                    cls=klass,\n                    class_repr=safe_repr(klass),\n                    modname=modname,\n                ) from exc\n        modastroid = self.ast_from_module_name(modname)\n        return modastroid.getattr(klass.__name__)[0]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninferring astroid for the given object", "response": "def infer_ast_from_something(self, obj, context=None):\n        \"\"\"infer astroid for the given class\"\"\"\n        if hasattr(obj, \"__class__\") and not isinstance(obj, type):\n            klass = obj.__class__\n        else:\n            klass = obj\n        try:\n            modname = klass.__module__\n        except AttributeError as exc:\n            raise exceptions.AstroidBuildingError(\n                \"Unable to get module for {class_repr}.\",\n                cls=klass,\n                class_repr=safe_repr(klass),\n            ) from exc\n        except Exception as exc:\n            raise exceptions.AstroidImportError(\n                \"Unexpected error while retrieving module for {class_repr}:\\n\"\n                \"{error}\",\n                cls=klass,\n                class_repr=safe_repr(klass),\n            ) from exc\n        try:\n            name = klass.__name__\n        except AttributeError as exc:\n            raise exceptions.AstroidBuildingError(\n                \"Unable to get name for {class_repr}:\\n\",\n                cls=klass,\n                class_repr=safe_repr(klass),\n            ) from exc\n        except Exception as exc:\n            raise exceptions.AstroidImportError(\n                \"Unexpected error while retrieving name for {class_repr}:\\n\" \"{error}\",\n                cls=klass,\n                class_repr=safe_repr(klass),\n            ) from exc\n        # take care, on living object __module__ is regularly wrong :(\n        modastroid = self.ast_from_module_name(modname)\n        if klass is obj:\n            for inferred in modastroid.igetattr(name, context):\n                yield inferred\n        else:\n            for inferred in modastroid.igetattr(name, context):\n                yield inferred.instantiate_class()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a CallSite object from the given Call node.", "response": "def from_call(cls, call_node):\n        \"\"\"Get a CallSite object from the given Call node.\"\"\"\n        callcontext = contextmod.CallContext(call_node.args, call_node.keywords)\n        return cls(callcontext)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninfer a function argument value according to the call context.", "response": "def infer_argument(self, funcnode, name, context):\n        \"\"\"infer a function argument value according to the call context\n\n        Arguments:\n            funcnode: The function being called.\n            name: The name of the argument whose value is being inferred.\n            context: Inference context object\n        \"\"\"\n        if name in self.duplicated_keywords:\n            raise exceptions.InferenceError(\n                \"The arguments passed to {func!r} \" \" have duplicate keywords.\",\n                call_site=self,\n                func=funcnode,\n                arg=name,\n                context=context,\n            )\n\n        # Look into the keywords first, maybe it's already there.\n        try:\n            return self.keyword_arguments[name].infer(context)\n        except KeyError:\n            pass\n\n        # Too many arguments given and no variable arguments.\n        if len(self.positional_arguments) > len(funcnode.args.args):\n            if not funcnode.args.vararg:\n                raise exceptions.InferenceError(\n                    \"Too many positional arguments \"\n                    \"passed to {func!r} that does \"\n                    \"not have *args.\",\n                    call_site=self,\n                    func=funcnode,\n                    arg=name,\n                    context=context,\n                )\n\n        positional = self.positional_arguments[: len(funcnode.args.args)]\n        vararg = self.positional_arguments[len(funcnode.args.args) :]\n        argindex = funcnode.args.find_argname(name)[0]\n        kwonlyargs = {arg.name for arg in funcnode.args.kwonlyargs}\n        kwargs = {\n            key: value\n            for key, value in self.keyword_arguments.items()\n            if key not in kwonlyargs\n        }\n        # If there are too few positionals compared to\n        # what the function expects to receive, check to see\n        # if the missing positional arguments were passed\n        # as keyword arguments and if so, place them into the\n        # positional args list.\n        if len(positional) < len(funcnode.args.args):\n            for func_arg in funcnode.args.args:\n                if func_arg.name in kwargs:\n                    arg = kwargs.pop(func_arg.name)\n                    positional.append(arg)\n\n        if argindex is not None:\n            # 2. first argument of instance/class method\n            if argindex == 0 and funcnode.type in (\"method\", \"classmethod\"):\n                if context.boundnode is not None:\n                    boundnode = context.boundnode\n                else:\n                    # XXX can do better ?\n                    boundnode = funcnode.parent.frame()\n\n                if isinstance(boundnode, nodes.ClassDef):\n                    # Verify that we're accessing a method\n                    # of the metaclass through a class, as in\n                    # `cls.metaclass_method`. In this case, the\n                    # first argument is always the class.\n                    method_scope = funcnode.parent.scope()\n                    if method_scope is boundnode.metaclass():\n                        return iter((boundnode,))\n\n                if funcnode.type == \"method\":\n                    if not isinstance(boundnode, bases.Instance):\n                        boundnode = bases.Instance(boundnode)\n                    return iter((boundnode,))\n                if funcnode.type == \"classmethod\":\n                    return iter((boundnode,))\n            # if we have a method, extract one position\n            # from the index, so we'll take in account\n            # the extra parameter represented by `self` or `cls`\n            if funcnode.type in (\"method\", \"classmethod\"):\n                argindex -= 1\n            # 2. search arg index\n            try:\n                return self.positional_arguments[argindex].infer(context)\n            except IndexError:\n                pass\n\n        if funcnode.args.kwarg == name:\n            # It wants all the keywords that were passed into\n            # the call site.\n            if self.has_invalid_keywords():\n                raise exceptions.InferenceError(\n                    \"Inference failed to find values for all keyword arguments \"\n                    \"to {func!r}: {unpacked_kwargs!r} doesn't correspond to \"\n                    \"{keyword_arguments!r}.\",\n                    keyword_arguments=self.keyword_arguments,\n                    unpacked_kwargs=self._unpacked_kwargs,\n                    call_site=self,\n                    func=funcnode,\n                    arg=name,\n                    context=context,\n                )\n            kwarg = nodes.Dict(\n                lineno=funcnode.args.lineno,\n                col_offset=funcnode.args.col_offset,\n                parent=funcnode.args,\n            )\n            kwarg.postinit(\n                [(nodes.const_factory(key), value) for key, value in kwargs.items()]\n            )\n            return iter((kwarg,))\n        if funcnode.args.vararg == name:\n            # It wants all the args that were passed into\n            # the call site.\n            if self.has_invalid_arguments():\n                raise exceptions.InferenceError(\n                    \"Inference failed to find values for all positional \"\n                    \"arguments to {func!r}: {unpacked_args!r} doesn't \"\n                    \"correspond to {positional_arguments!r}.\",\n                    positional_arguments=self.positional_arguments,\n                    unpacked_args=self._unpacked_args,\n                    call_site=self,\n                    func=funcnode,\n                    arg=name,\n                    context=context,\n                )\n            args = nodes.Tuple(\n                lineno=funcnode.args.lineno,\n                col_offset=funcnode.args.col_offset,\n                parent=funcnode.args,\n            )\n            args.postinit(vararg)\n            return iter((args,))\n\n        # Check if it's a default parameter.\n        try:\n            return funcnode.args.default_value(name).infer(context)\n        except exceptions.NoDefault:\n            pass\n        raise exceptions.InferenceError(\n            \"No value found for argument {name} to \" \"{func!r}\",\n            call_site=self,\n            func=funcnode,\n            arg=name,\n            context=context,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncaches decorator used for inference tips", "response": "def _inference_tip_cached(func, instance, args, kwargs, _cache={}):\n    \"\"\"Cache decorator used for inference tips\"\"\"\n    node = args[0]\n    try:\n        return iter(_cache[func, node])\n    except KeyError:\n        result = func(*args, **kwargs)\n        # Need to keep an iterator around\n        original, copy = itertools.tee(result)\n        _cache[func, node] = list(copy)\n        return original"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives an instance specific inference function return a function to be used to set the inference tip for the given node.", "response": "def inference_tip(infer_function, raise_on_overwrite=False):\n    \"\"\"Given an instance specific inference function, return a function to be\n    given to MANAGER.register_transform to set this inference function.\n\n    :param bool raise_on_overwrite: Raise an `InferenceOverwriteError`\n        if the inference tip will overwrite another. Used for debugging\n\n    Typical usage\n\n    .. sourcecode:: python\n\n       MANAGER.register_transform(Call, inference_tip(infer_named_tuple),\n                                  predicate)\n\n    .. Note::\n\n        Using an inference tip will override\n        any previously set inference tip for the given\n        node. Use a predicate in the transform to prevent\n        excess overwrites.\n    \"\"\"\n\n    def transform(node, infer_function=infer_function):\n        if (\n            raise_on_overwrite\n            and node._explicit_inference is not None\n            and node._explicit_inference is not infer_function\n        ):\n            raise InferenceOverwriteError(\n                \"Inference already set to {existing_inference}. \"\n                \"Trying to overwrite with {new_inference} for {node}\".format(\n                    existing_inference=infer_function,\n                    new_inference=node._explicit_inference,\n                    node=node,\n                )\n            )\n        # pylint: disable=no-value-for-parameter\n        node._explicit_inference = _inference_tip_cached(infer_function)\n        return node\n\n    return transform"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntransforming the given name by adding the given class as a member of the node.", "response": "def _generic_io_transform(node, name, cls):\n    \"\"\"Transform the given name, by adding the given *class* as a member of the node.\"\"\"\n\n    io_module = astroid.MANAGER.ast_from_module_name(\"_io\")\n    attribute_object = io_module[cls]\n    instance = attribute_object.instantiate_class()\n    node.locals[name] = [instance]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef path_wrapper(func):\n\n    @functools.wraps(func)\n    def wrapped(node, context=None, _func=func, **kwargs):\n        \"\"\"wrapper function handling context\"\"\"\n        if context is None:\n            context = contextmod.InferenceContext()\n        if context.push(node):\n            return None\n\n        yielded = set()\n        generator = _func(node, context, **kwargs)\n        try:\n            while True:\n                res = next(generator)\n                # unproxy only true instance, not const, tuple, dict...\n                if res.__class__.__name__ == \"Instance\":\n                    ares = res._proxied\n                else:\n                    ares = res\n                if ares not in yielded:\n                    yield res\n                    yielded.add(ares)\n        except StopIteration as error:\n            if error.args:\n                return error.args[0]\n            return None\n\n    return wrapped", "response": "return the given infer function wrapped to handle the path\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a __class__ member to the given func node if we can determine it.", "response": "def _add_dunder_class(func, member):\n    \"\"\"Add a __class__ member to the given func node, if we can determine it.\"\"\"\n    python_cls = member.__class__\n    cls_name = getattr(python_cls, \"__name__\", None)\n    if not cls_name:\n        return\n    cls_bases = [ancestor.__name__ for ancestor in python_cls.__bases__]\n    ast_klass = build_class(cls_name, cls_bases, python_cls.__doc__)\n    func.instance_attrs[\"__class__\"] = [ast_klass]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef attach_dummy_node(node, name, runtime_object=_marker):\n    enode = nodes.EmptyNode()\n    enode.object = runtime_object\n    _attach_local_node(node, enode, name)", "response": "create a dummy node and register it in the locals of the given node with the specified name"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef attach_const_node(node, name, value):\n    if name not in node.special_attributes:\n        _attach_local_node(node, nodes.const_factory(value), name)", "response": "create a Const node and register it in the locals of the given node with the specified name\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a ImportFrom node and register it in the local node of the given node with the specified name", "response": "def attach_import_node(node, modname, membername):\n    \"\"\"create a ImportFrom node and register it in the locals of the given\n    node with the specified name\n    \"\"\"\n    from_node = nodes.ImportFrom(modname, [(membername, None)])\n    _attach_local_node(node, from_node, membername)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build_module(name, doc=None):\n    node = nodes.Module(name, doc, pure_python=False)\n    node.package = False\n    node.parent = None\n    return node", "response": "create and initialize an astroid Module node"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating and initialize an astroid ClassDef node", "response": "def build_class(name, basenames=(), doc=None):\n    \"\"\"create and initialize an astroid ClassDef node\"\"\"\n    node = nodes.ClassDef(name, doc)\n    for base in basenames:\n        basenode = nodes.Name()\n        basenode.name = base\n        node.bases.append(basenode)\n        basenode.parent = node\n    return node"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate and initialize an astroid FunctionDef node", "response": "def build_function(name, args=None, defaults=None, doc=None):\n    \"\"\"create and initialize an astroid FunctionDef node\"\"\"\n    args, defaults = args or [], defaults or []\n    # first argument is now a list of decorators\n    func = nodes.FunctionDef(name, doc)\n    func.args = argsnode = nodes.Arguments()\n    argsnode.args = []\n    for arg in args:\n        argsnode.args.append(nodes.Name())\n        argsnode.args[-1].name = arg\n        argsnode.args[-1].parent = argsnode\n    argsnode.defaults = []\n    for default in defaults:\n        argsnode.defaults.append(nodes.const_factory(default))\n        argsnode.defaults[-1].parent = argsnode\n    argsnode.kwarg = None\n    argsnode.vararg = None\n    argsnode.parent = func\n    if args:\n        register_arguments(func)\n    return func"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef register_arguments(func, args=None):\n    if args is None:\n        args = func.args.args\n        if func.args.vararg:\n            func.set_local(func.args.vararg, func.args)\n        if func.args.kwarg:\n            func.set_local(func.args.kwarg, func.args)\n    for arg in args:\n        if isinstance(arg, nodes.Name):\n            func.set_local(arg.name, arg)\n        else:\n            register_arguments(func, arg.elts)", "response": "register given arguments to local\nTaxonomy"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate astroid for a living class object", "response": "def object_build_class(node, member, localname):\n    \"\"\"create astroid for a living class object\"\"\"\n    basenames = [base.__name__ for base in member.__bases__]\n    return _base_class_object_build(node, member, basenames, localname=localname)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating astroid for a living function object", "response": "def object_build_function(node, member, localname):\n    \"\"\"create astroid for a living function object\"\"\"\n    # pylint: disable=deprecated-method; completely removed in 2.0\n    args, varargs, varkw, defaults = inspect.getargspec(member)\n    if varargs is not None:\n        args.append(varargs)\n    if varkw is not None:\n        args.append(varkw)\n    func = build_function(\n        getattr(member, \"__name__\", None) or localname, args, defaults, member.__doc__\n    )\n    node.add_local_node(func, localname)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef object_build_methoddescriptor(node, member, localname):\n    # FIXME get arguments ?\n    func = build_function(\n        getattr(member, \"__name__\", None) or localname, doc=member.__doc__\n    )\n    # set node's arguments to None to notice that we have no information, not\n    # and empty argument list\n    func.args.args = None\n    node.add_local_node(func, localname)\n    _add_dunder_class(func, member)", "response": "create an astroid for a living method descriptor object"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _base_class_object_build(node, member, basenames, name=None, localname=None):\n    klass = build_class(\n        name or getattr(member, \"__name__\", None) or localname,\n        basenames,\n        member.__doc__,\n    )\n    klass._newstyle = isinstance(member, type)\n    node.add_local_node(klass, localname)\n    try:\n        # limit the instantiation trick since it's too dangerous\n        # (such as infinite test execution...)\n        # this at least resolves common case such as Exception.args,\n        # OSError.errno\n        if issubclass(member, Exception):\n            instdict = member().__dict__\n        else:\n            raise TypeError\n    except TypeError:\n        pass\n    else:\n        for item_name, obj in instdict.items():\n            valnode = nodes.EmptyNode()\n            valnode.object = obj\n            valnode.parent = klass\n            valnode.lineno = 1\n            klass.instance_attrs[item_name] = [valnode]\n    return klass", "response": "build an astroid for a base class object"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _astroid_bootstrapping():\n    # this boot strapping is necessary since we need the Const nodes to\n    # inspect_build builtins, and then we can proxy Const\n    builder = InspectBuilder()\n    astroid_builtin = builder.inspect_build(builtins)\n\n    # pylint: disable=redefined-outer-name\n    for cls, node_cls in node_classes.CONST_CLS.items():\n        if cls is type(None):\n            proxy = build_class(\"NoneType\")\n            proxy.parent = astroid_builtin\n        elif cls is type(NotImplemented):\n            proxy = build_class(\"NotImplementedType\")\n            proxy.parent = astroid_builtin\n        else:\n            proxy = astroid_builtin.getattr(cls.__name__)[0]\n        if cls in (dict, list, set, tuple):\n            node_cls._proxied = proxy\n        else:\n            _CONST_PROXY[cls] = proxy\n\n    # Set the builtin module as parent for some builtins.\n    nodes.Const._proxied = property(_set_proxied)\n\n    _GeneratorType = nodes.ClassDef(\n        types.GeneratorType.__name__, types.GeneratorType.__doc__\n    )\n    _GeneratorType.parent = astroid_builtin\n    bases.Generator._proxied = _GeneratorType\n    builder.object_build(bases.Generator._proxied, types.GeneratorType)\n\n    if hasattr(types, \"AsyncGeneratorType\"):\n        # pylint: disable=no-member; AsyncGeneratorType\n        _AsyncGeneratorType = nodes.ClassDef(\n            types.AsyncGeneratorType.__name__, types.AsyncGeneratorType.__doc__\n        )\n        _AsyncGeneratorType.parent = astroid_builtin\n        bases.AsyncGenerator._proxied = _AsyncGeneratorType\n        builder.object_build(bases.AsyncGenerator._proxied, types.AsyncGeneratorType)\n    builtin_types = (\n        types.GetSetDescriptorType,\n        types.GeneratorType,\n        types.MemberDescriptorType,\n        type(None),\n        type(NotImplemented),\n        types.FunctionType,\n        types.MethodType,\n        types.BuiltinFunctionType,\n        types.ModuleType,\n        types.TracebackType,\n    )\n    for _type in builtin_types:\n        if _type.__name__ not in astroid_builtin:\n            cls = nodes.ClassDef(_type.__name__, _type.__doc__)\n            cls.parent = astroid_builtin\n            builder.object_build(cls, _type)\n            astroid_builtin[_type.__name__] = cls", "response": "This function is used to bootstrap the builtins module"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbuilds astroid from a living module", "response": "def inspect_build(self, module, modname=None, path=None):\n        \"\"\"build astroid from a living module (i.e. using inspect)\n        this is used when there is no python source code available (either\n        because it's a built-in module or because the .py is not available)\n        \"\"\"\n        self._module = module\n        if modname is None:\n            modname = module.__name__\n        try:\n            node = build_module(modname, module.__doc__)\n        except AttributeError:\n            # in jython, java modules have no __doc__ (see #109562)\n            node = build_module(modname)\n        node.file = node.path = os.path.abspath(path) if path else path\n        node.name = modname\n        MANAGER.cache_module(node)\n        node.package = hasattr(module, \"__path__\")\n        self._done = {}\n        self.object_build(node, module)\n        return node"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef object_build(self, node, obj):\n        if obj in self._done:\n            return self._done[obj]\n        self._done[obj] = node\n        for name in dir(obj):\n            try:\n                member = getattr(obj, name)\n            except AttributeError:\n                # damned ExtensionClass.Base, I know you're there !\n                attach_dummy_node(node, name)\n                continue\n            if inspect.ismethod(member):\n                member = member.__func__\n            if inspect.isfunction(member):\n                _build_from_function(node, name, member, self._module)\n            elif inspect.isbuiltin(member):\n                if not _io_discrepancy(member) and self.imported_member(\n                    node, member, name\n                ):\n                    continue\n                object_build_methoddescriptor(node, member, name)\n            elif inspect.isclass(member):\n                if self.imported_member(node, member, name):\n                    continue\n                if member in self._done:\n                    class_node = self._done[member]\n                    if class_node not in node.locals.get(name, ()):\n                        node.add_local_node(class_node, name)\n                else:\n                    class_node = object_build_class(node, member, name)\n                    # recursion\n                    self.object_build(class_node, member)\n                if name == \"__class__\" and class_node.parent is None:\n                    class_node.parent = self._done[self._module]\n            elif inspect.ismethoddescriptor(member):\n                assert isinstance(member, object)\n                object_build_methoddescriptor(node, member, name)\n            elif inspect.isdatadescriptor(member):\n                assert isinstance(member, object)\n                object_build_datadescriptor(node, member, name)\n            elif isinstance(member, _CONSTANTS):\n                attach_const_node(node, name, member)\n            elif inspect.isroutine(member):\n                # This should be called for Jython, where some builtin\n                # methods aren't caught by isbuiltin branch.\n                _build_from_function(node, name, member, self._module)\n            else:\n                # create an empty node so that the name is actually defined\n                attach_dummy_node(node, name, member)\n        return None", "response": "recursive method which creates a partial ast from real objects\n        "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef imported_member(self, node, member, name):\n        # /!\\ some classes like ExtensionClass doesn't have a __module__\n        # attribute ! Also, this may trigger an exception on badly built module\n        # (see http://www.logilab.org/ticket/57299 for instance)\n        try:\n            modname = getattr(member, \"__module__\", None)\n        except TypeError:\n            modname = None\n        if modname is None:\n            if name in (\"__new__\", \"__subclasshook__\"):\n                # Python 2.5.1 (r251:54863, Sep  1 2010, 22:03:14)\n                # >>> print object.__new__.__module__\n                # None\n                modname = builtins.__name__\n            else:\n                attach_dummy_node(node, name, member)\n                return True\n\n        real_name = {\"gtk\": \"gtk_gtk\", \"_io\": \"io\"}.get(modname, modname)\n\n        if real_name != self._module.__name__:\n            # check if it sounds valid and then add an import node, else use a\n            # dummy node\n            try:\n                getattr(sys.modules[modname], name)\n            except (KeyError, AttributeError):\n                attach_dummy_node(node, name, member)\n            else:\n                attach_import_node(node, modname, name)\n            return True\n        return False", "response": "verify this is not an imported class or handle it"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses a source string in order to obtain an astroid AST from it", "response": "def parse(code, module_name=\"\", path=None, apply_transforms=True):\n    \"\"\"Parses a source string in order to obtain an astroid AST from it\n\n    :param str code: The code for the module.\n    :param str module_name: The name for the module, if any\n    :param str path: The path for the module\n    :param bool apply_transforms:\n        Apply the transforms for the give code. Use it if you\n        don't want the default transforms to be applied.\n    \"\"\"\n    code = textwrap.dedent(code)\n    builder = AstroidBuilder(manager=MANAGER, apply_transforms=apply_transforms)\n    return builder.string_build(code, modname=module_name, path=path)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind expressions in a call to _TRANSIENT_FUNCTION and extract them. The function walks the AST recursively to search for expressions that are wrapped into a call to _TRANSIENT_FUNCTION. If it finds such an expression, it completely removes the function call node from the tree, replacing it by the wrapped expression inside the parent. :param node: An astroid node. :type node: astroid.bases.NodeNG :yields: The sequence of wrapped expressions on the modified tree expression can be found.", "response": "def _extract_expressions(node):\n    \"\"\"Find expressions in a call to _TRANSIENT_FUNCTION and extract them.\n\n    The function walks the AST recursively to search for expressions that\n    are wrapped into a call to _TRANSIENT_FUNCTION. If it finds such an\n    expression, it completely removes the function call node from the tree,\n    replacing it by the wrapped expression inside the parent.\n\n    :param node: An astroid node.\n    :type node:  astroid.bases.NodeNG\n    :yields: The sequence of wrapped expressions on the modified tree\n    expression can be found.\n    \"\"\"\n    if (\n        isinstance(node, nodes.Call)\n        and isinstance(node.func, nodes.Name)\n        and node.func.name == _TRANSIENT_FUNCTION\n    ):\n        real_expr = node.args[0]\n        real_expr.parent = node.parent\n        # Search for node in all _astng_fields (the fields checked when\n        # get_children is called) of its parent. Some of those fields may\n        # be lists or tuples, in which case the elements need to be checked.\n        # When we find it, replace it by real_expr, so that the AST looks\n        # like no call to _TRANSIENT_FUNCTION ever took place.\n        for name in node.parent._astroid_fields:\n            child = getattr(node.parent, name)\n            if isinstance(child, (list, tuple)):\n                for idx, compound_child in enumerate(child):\n                    if compound_child is node:\n                        child[idx] = real_expr\n            elif child is node:\n                setattr(node.parent, name, real_expr)\n        yield real_expr\n    else:\n        for child in node.get_children():\n            yield from _extract_expressions(child)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _find_statement_by_line(node, line):\n    if isinstance(node, (nodes.ClassDef, nodes.FunctionDef)):\n        # This is an inaccuracy in the AST: the nodes that can be\n        # decorated do not carry explicit information on which line\n        # the actual definition (class/def), but .fromline seems to\n        # be close enough.\n        node_line = node.fromlineno\n    else:\n        node_line = node.lineno\n\n    if node_line == line:\n        return node\n\n    for child in node.get_children():\n        result = _find_statement_by_line(child, line)\n        if result:\n            return result\n\n    return None", "response": "Finds the statement on a specific line from an AST."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses some Python code and extracts a designated AST node.", "response": "def extract_node(code, module_name=\"\"):\n    \"\"\"Parses some Python code as a module and extracts a designated AST node.\n\n    Statements:\n     To extract one or more statement nodes, append #@ to the end of the line\n\n     Examples:\n       >>> def x():\n       >>>   def y():\n       >>>     return 1 #@\n\n       The return statement will be extracted.\n\n       >>> class X(object):\n       >>>   def meth(self): #@\n       >>>     pass\n\n      The function object 'meth' will be extracted.\n\n    Expressions:\n     To extract arbitrary expressions, surround them with the fake\n     function call __(...). After parsing, the surrounded expression\n     will be returned and the whole AST (accessible via the returned\n     node's parent attribute) will look like the function call was\n     never there in the first place.\n\n     Examples:\n       >>> a = __(1)\n\n       The const node will be extracted.\n\n       >>> def x(d=__(foo.bar)): pass\n\n       The node containing the default argument will be extracted.\n\n       >>> def foo(a, b):\n       >>>   return 0 < __(len(a)) < b\n\n       The node containing the function call 'len' will be extracted.\n\n    If no statements or expressions are selected, the last toplevel\n    statement will be returned.\n\n    If the selected statement is a discard statement, (i.e. an expression\n    turned into a statement), the wrapped expression is returned instead.\n\n    For convenience, singleton lists are unpacked.\n\n    :param str code: A piece of Python code that is parsed as\n    a module. Will be passed through textwrap.dedent first.\n    :param str module_name: The name of the module.\n    :returns: The designated node from the parse tree, or a list of nodes.\n    :rtype: astroid.bases.NodeNG, or a list of nodes.\n    \"\"\"\n\n    def _extract(node):\n        if isinstance(node, nodes.Expr):\n            return node.value\n\n        return node\n\n    requested_lines = []\n    for idx, line in enumerate(code.splitlines()):\n        if line.strip().endswith(_STATEMENT_SELECTOR):\n            requested_lines.append(idx + 1)\n\n    tree = parse(code, module_name=module_name)\n    if not tree.body:\n        raise ValueError(\"Empty tree, cannot extract from it\")\n\n    extracted = []\n    if requested_lines:\n        extracted = [_find_statement_by_line(tree, line) for line in requested_lines]\n\n    # Modifies the tree.\n    extracted.extend(_extract_expressions(tree))\n\n    if not extracted:\n        extracted.append(tree.body[-1])\n\n    extracted = [_extract(node) for node in extracted]\n    if len(extracted) == 1:\n        return extracted[0]\n    return extracted"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef module_build(self, module, modname=None):\n        node = None\n        path = getattr(module, \"__file__\", None)\n        if path is not None:\n            path_, ext = os.path.splitext(modutils._path_from_filename(path))\n            if ext in (\".py\", \".pyc\", \".pyo\") and os.path.exists(path_ + \".py\"):\n                node = self.file_build(path_ + \".py\", modname)\n        if node is None:\n            # this is a built-in module\n            # get a partial representation by introspection\n            node = self.inspect_build(module, modname=modname, path=path)\n            if self._apply_transforms:\n                # We have to handle transformation by ourselves since the\n                # rebuilder isn't called for builtin nodes\n                node = self._manager.visit_transforms(node)\n        return node", "response": "Build an astroid from a living module instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbuild an astroid from a source code file.", "response": "def file_build(self, path, modname=None):\n        \"\"\"Build astroid from a source code file (i.e. from an ast)\n\n        *path* is expected to be a python source file\n        \"\"\"\n        try:\n            stream, encoding, data = open_source_file(path)\n        except IOError as exc:\n            raise exceptions.AstroidBuildingError(\n                \"Unable to load file {path}:\\n{error}\",\n                modname=modname,\n                path=path,\n                error=exc,\n            ) from exc\n        except (SyntaxError, LookupError) as exc:\n            raise exceptions.AstroidSyntaxError(\n                \"Python 3 encoding specification error or unknown encoding:\\n\"\n                \"{error}\",\n                modname=modname,\n                path=path,\n                error=exc,\n            ) from exc\n        except UnicodeError as exc:  # wrong encoding\n            # detect_encoding returns utf-8 if no encoding specified\n            raise exceptions.AstroidBuildingError(\n                \"Wrong or no encoding specified for {filename}.\", filename=path\n            ) from exc\n        with stream:\n            # get module name if necessary\n            if modname is None:\n                try:\n                    modname = \".\".join(modutils.modpath_from_file(path))\n                except ImportError:\n                    modname = os.path.splitext(os.path.basename(path))[0]\n            # build astroid representation\n            module = self._data_build(data, modname, path)\n            return self._post_build(module, encoding)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding astroid from source code string.", "response": "def string_build(self, data, modname=\"\", path=None):\n        \"\"\"Build astroid from source code string.\"\"\"\n        module = self._data_build(data, modname, path)\n        module.file_bytes = data.encode(\"utf-8\")\n        return self._post_build(module, \"utf-8\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nhandle encoding and delayed nodes after a module has been built", "response": "def _post_build(self, module, encoding):\n        \"\"\"Handles encoding and delayed nodes after a module has been built\"\"\"\n        module.file_encoding = encoding\n        self._manager.cache_module(module)\n        # post tree building steps after we stored the module in the cache:\n        for from_node in module._import_from_nodes:\n            if from_node.modname == \"__future__\":\n                for symbol, _ in from_node.names:\n                    module.future_imports.add(symbol)\n            self.add_from_names_to_locals(from_node)\n        # handle delayed assattr nodes\n        for delayed in module._delayed_assattr:\n            self.delayed_assattr(delayed)\n\n        # Visit the transforms\n        if self._apply_transforms:\n            module = self._manager.visit_transforms(module)\n        return module"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild a tree node from data and add some informations", "response": "def _data_build(self, data, modname, path):\n        \"\"\"Build tree node from data and add some informations\"\"\"\n        try:\n            node = _parse(data + \"\\n\")\n        except (TypeError, ValueError, SyntaxError) as exc:\n            raise exceptions.AstroidSyntaxError(\n                \"Parsing Python code failed:\\n{error}\",\n                source=data,\n                modname=modname,\n                path=path,\n                error=exc,\n            ) from exc\n        if path is not None:\n            node_file = os.path.abspath(path)\n        else:\n            node_file = \"<?>\"\n        if modname.endswith(\".__init__\"):\n            modname = modname[:-9]\n            package = True\n        else:\n            package = (\n                path is not None\n                and os.path.splitext(os.path.basename(path))[0] == \"__init__\"\n            )\n        builder = rebuilder.TreeRebuilder(self._manager)\n        module = builder.visit_module(node, modname, node_file, package)\n        module._import_from_nodes = builder._import_from_nodes\n        module._delayed_assattr = builder._delayed_assattr\n        return module"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_from_names_to_locals(self, node):\n        _key_func = lambda node: node.fromlineno\n\n        def sort_locals(my_list):\n            my_list.sort(key=_key_func)\n\n        for (name, asname) in node.names:\n            if name == \"*\":\n                try:\n                    imported = node.do_import_module()\n                except exceptions.AstroidBuildingError:\n                    continue\n                for name in imported.public_names():\n                    node.parent.set_local(name, node)\n                    sort_locals(node.parent.scope().locals[name])\n            else:\n                node.parent.set_local(asname or name, node)\n                sort_locals(node.parent.scope().locals[asname or name])", "response": "Store imported names to the locals\n ArcGIS node"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nvisiting a AssAttr node This adds name to locals and handle members definition definition.", "response": "def delayed_assattr(self, node):\n        \"\"\"Visit a AssAttr node\n\n        This adds name to locals and handle members definition.\n        \"\"\"\n        try:\n            frame = node.frame()\n            for inferred in node.expr.infer():\n                if inferred is util.Uninferable:\n                    continue\n                try:\n                    if inferred.__class__ is bases.Instance:\n                        inferred = inferred._proxied\n                        iattrs = inferred.instance_attrs\n                        if not _can_assign_attr(inferred, node.attrname):\n                            continue\n                    elif isinstance(inferred, bases.Instance):\n                        # Const, Tuple, ... we may be wrong, may be not, but\n                        # anyway we don't want to pollute builtin's namespace\n                        continue\n                    elif inferred.is_function:\n                        iattrs = inferred.instance_attrs\n                    else:\n                        iattrs = inferred.locals\n                except AttributeError:\n                    # XXX log error\n                    continue\n                values = iattrs.setdefault(node.attrname, [])\n                if node in values:\n                    continue\n                # get assign in __init__ first XXX useful ?\n                if (\n                    frame.name == \"__init__\"\n                    and values\n                    and values[0].frame().name != \"__init__\"\n                ):\n                    values.insert(0, node)\n                else:\n                    values.append(node)\n        except exceptions.InferenceError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _resolve_looppart(parts, assign_path, context):\n    assign_path = assign_path[:]\n    index = assign_path.pop(0)\n    for part in parts:\n        if part is util.Uninferable:\n            continue\n        if not hasattr(part, \"itered\"):\n            continue\n        try:\n            itered = part.itered()\n        except TypeError:\n            continue\n        for stmt in itered:\n            index_node = nodes.Const(index)\n            try:\n                assigned = stmt.getitem(index_node, context)\n            except (\n                AttributeError,\n                exceptions.AstroidTypeError,\n                exceptions.AstroidIndexError,\n            ):\n                continue\n            if not assign_path:\n                # we achieved to resolved the assignment path,\n                # don't infer the last part\n                yield assigned\n            elif assigned is util.Uninferable:\n                break\n            else:\n                # we are not yet on the last part of the path\n                # search on each possibly inferred value\n                try:\n                    yield from _resolve_looppart(\n                        assigned.infer(context), assign_path, context\n                    )\n                except exceptions.InferenceError:\n                    break", "response": "recursive function to resolve multiple assignments on loops"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninfer names and other nodes from a with statement.", "response": "def with_assigned_stmts(self, node=None, context=None, assign_path=None):\n    \"\"\"Infer names and other nodes from a *with* statement.\n\n    This enables only inference for name binding in a *with* statement.\n    For instance, in the following code, inferring `func` will return\n    the `ContextManager` class, not whatever ``__enter__`` returns.\n    We are doing this intentionally, because we consider that the context\n    manager result is whatever __enter__ returns and what it is binded\n    using the ``as`` keyword.\n\n        class ContextManager(object):\n            def __enter__(self):\n                return 42\n        with ContextManager() as f:\n            pass\n\n        # ContextManager().infer() will return ContextManager\n        # f.infer() will return 42.\n\n    Arguments:\n        self: nodes.With\n        node: The target of the assignment, `as (a, b)` in `with foo as (a, b)`.\n        context: Inference context used for caching already inferred objects\n        assign_path:\n            A list of indices, where each index specifies what item to fetch from\n            the inference results.\n    \"\"\"\n    try:\n        mgr = next(mgr for (mgr, vars) in self.items if vars == node)\n    except StopIteration:\n        return None\n    if assign_path is None:\n        yield from _infer_context_manager(self, mgr, context)\n    else:\n        for result in _infer_context_manager(self, mgr, context):\n            # Walk the assign_path and get the item at the final index.\n            obj = result\n            for index in assign_path:\n                if not hasattr(obj, \"elts\"):\n                    raise exceptions.InferenceError(\n                        \"Wrong type ({targets!r}) for {node!r} assignment\",\n                        node=self,\n                        targets=node,\n                        assign_path=assign_path,\n                        context=context,\n                    )\n                try:\n                    obj = obj.elts[index]\n                except IndexError as exc:\n                    raise exceptions.InferenceError(\n                        \"Tried to infer a nonexistent target with index {index} \"\n                        \"in {node!r}.\",\n                        node=self,\n                        targets=node,\n                        assign_path=assign_path,\n                        context=context,\n                    ) from exc\n                except TypeError as exc:\n                    raise exceptions.InferenceError(\n                        \"Tried to unpack a non-iterable value \" \"in {node!r}.\",\n                        node=self,\n                        targets=node,\n                        assign_path=assign_path,\n                        context=context,\n                    ) from exc\n            yield obj\n    return dict(node=self, unknown=node, assign_path=assign_path, context=context)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of statements that are starred assigned to the current node.", "response": "def starred_assigned_stmts(self, node=None, context=None, assign_path=None):\n    \"\"\"\n    Arguments:\n        self: nodes.Starred\n        node: a node related to the current underlying Node.\n        context: Inference context used for caching already inferred objects\n        assign_path:\n            A list of indices, where each index specifies what item to fetch from\n            the inference results.\n    \"\"\"\n    # pylint: disable=too-many-locals,too-many-branches,too-many-statements\n    def _determine_starred_iteration_lookups(starred, target, lookups):\n        # Determine the lookups for the rhs of the iteration\n        itered = target.itered()\n        for index, element in enumerate(itered):\n            if (\n                isinstance(element, nodes.Starred)\n                and element.value.name == starred.value.name\n            ):\n                lookups.append((index, len(itered)))\n                break\n            if isinstance(element, nodes.Tuple):\n                lookups.append((index, len(element.itered())))\n                _determine_starred_iteration_lookups(starred, element, lookups)\n\n    stmt = self.statement()\n    if not isinstance(stmt, (nodes.Assign, nodes.For)):\n        raise exceptions.InferenceError(\n            \"Statement {stmt!r} enclosing {node!r} \" \"must be an Assign or For node.\",\n            node=self,\n            stmt=stmt,\n            unknown=node,\n            context=context,\n        )\n\n    if context is None:\n        context = contextmod.InferenceContext()\n\n    if isinstance(stmt, nodes.Assign):\n        value = stmt.value\n        lhs = stmt.targets[0]\n\n        if sum(1 for _ in lhs.nodes_of_class(nodes.Starred)) > 1:\n            raise exceptions.InferenceError(\n                \"Too many starred arguments in the \" \" assignment targets {lhs!r}.\",\n                node=self,\n                targets=lhs,\n                unknown=node,\n                context=context,\n            )\n\n        try:\n            rhs = next(value.infer(context))\n        except exceptions.InferenceError:\n            yield util.Uninferable\n            return\n        if rhs is util.Uninferable or not hasattr(rhs, \"itered\"):\n            yield util.Uninferable\n            return\n\n        try:\n            elts = collections.deque(rhs.itered())\n        except TypeError:\n            yield util.Uninferable\n            return\n\n        # Unpack iteratively the values from the rhs of the assignment,\n        # until the find the starred node. What will remain will\n        # be the list of values which the Starred node will represent\n        # This is done in two steps, from left to right to remove\n        # anything before the starred node and from right to left\n        # to remove anything after the starred node.\n\n        for index, left_node in enumerate(lhs.elts):\n            if not isinstance(left_node, nodes.Starred):\n                if not elts:\n                    break\n                elts.popleft()\n                continue\n            lhs_elts = collections.deque(reversed(lhs.elts[index:]))\n            for right_node in lhs_elts:\n                if not isinstance(right_node, nodes.Starred):\n                    if not elts:\n                        break\n                    elts.pop()\n                    continue\n                # We're done\n                packed = nodes.List(\n                    ctx=Store, parent=self, lineno=lhs.lineno, col_offset=lhs.col_offset\n                )\n                packed.postinit(elts=elts)\n                yield packed\n                break\n\n    if isinstance(stmt, nodes.For):\n        try:\n            inferred_iterable = next(stmt.iter.infer(context=context))\n        except exceptions.InferenceError:\n            yield util.Uninferable\n            return\n        if inferred_iterable is util.Uninferable or not hasattr(\n            inferred_iterable, \"itered\"\n        ):\n            yield util.Uninferable\n            return\n        try:\n            itered = inferred_iterable.itered()\n        except TypeError:\n            yield util.Uninferable\n            return\n\n        target = stmt.target\n\n        if not isinstance(target, nodes.Tuple):\n            raise exceptions.InferenceError(\n                \"Could not make sense of this, the target must be a tuple\",\n                context=context,\n            )\n\n        lookups = []\n        _determine_starred_iteration_lookups(self, target, lookups)\n        if not lookups:\n            raise exceptions.InferenceError(\n                \"Could not make sense of this, needs at least a lookup\", context=context\n            )\n\n        # Make the last lookup a slice, since that what we want for a Starred node\n        last_element_index, last_element_length = lookups[-1]\n        is_starred_last = last_element_index == (last_element_length - 1)\n\n        lookup_slice = slice(\n            last_element_index,\n            None if is_starred_last else (last_element_length - last_element_index),\n        )\n        lookups[-1] = lookup_slice\n\n        for element in itered:\n\n            # We probably want to infer the potential values *for each* element in an\n            # iterable, but we can't infer a list of all values, when only a list of\n            # step values are expected:\n            #\n            # for a, *b in [...]:\n            #   b\n            #\n            # *b* should now point to just the elements at that particular iteration step,\n            # which astroid can't know about.\n\n            found_element = None\n            for lookup in lookups:\n                if not hasattr(element, \"itered\"):\n                    break\n                if not isinstance(lookup, slice):\n                    # Grab just the index, not the whole length\n                    lookup = lookup[0]\n                try:\n                    itered_inner_element = element.itered()\n                    element = itered_inner_element[lookup]\n                except IndexError:\n                    break\n                except TypeError:\n                    # Most likely the itered() call failed, cannot make sense of this\n                    yield util.Uninferable\n                    return\n                else:\n                    found_element = element\n\n            unpacked = nodes.List(\n                ctx=Store, parent=self, lineno=self.lineno, col_offset=self.col_offset\n            )\n            unpacked.postinit(elts=found_element or [])\n            yield unpacked\n            return\n\n        yield util.Uninferable"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _infer_sequence_helper(node, context=None):\n    values = []\n\n    for elt in node.elts:\n        if isinstance(elt, nodes.Starred):\n            starred = helpers.safe_infer(elt.value, context)\n            if not starred:\n                raise exceptions.InferenceError(node=node, context=context)\n            if not hasattr(starred, \"elts\"):\n                raise exceptions.InferenceError(node=node, context=context)\n            values.extend(_infer_sequence_helper(starred))\n        else:\n            values.append(elt)\n    return values", "response": "Infer all values based on _BaseContainer. elts"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _update_with_replacement(lhs_dict, rhs_dict):\n    combined_dict = itertools.chain(lhs_dict.items(), rhs_dict.items())\n    # Overwrite keys which have the same string values\n    string_map = {key.as_string(): (key, value) for key, value in combined_dict}\n    # Return to dictionary\n    return dict(string_map.values())", "response": "Update a dictionary with a replacement for the nodes in the tree that are not in the tree tree."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninfer all values based on Dict. items", "response": "def _infer_map(node, context):\n    \"\"\"Infer all values based on Dict.items\"\"\"\n    values = {}\n    for name, value in node.items:\n        if isinstance(name, nodes.DictUnpack):\n            double_starred = helpers.safe_infer(value, context)\n            if not double_starred:\n                raise exceptions.InferenceError\n            if not isinstance(double_starred, nodes.Dict):\n                raise exceptions.InferenceError(node=node, context=context)\n            unpack_items = _infer_map(double_starred, context)\n            values = _update_with_replacement(values, unpack_items)\n        else:\n            key = helpers.safe_infer(name, context=context)\n            value = helpers.safe_infer(value, context=context)\n            if any(not elem for elem in (key, value)):\n                raise exceptions.InferenceError(node=node, context=context)\n            values = _update_with_replacement(values, {key: value})\n    return values"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _higher_function_scope(node):\n    current = node\n    while current.parent and not isinstance(current.parent, nodes.FunctionDef):\n        current = current.parent\n    if current and current.parent:\n        return current.parent\n    return None", "response": "Search for the first function which encloses the given node."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef infer_name(self, context=None):\n    frame, stmts = self.lookup(self.name)\n    if not stmts:\n        # Try to see if the name is enclosed in a nested function\n        # and use the higher (first function) scope for searching.\n        parent_function = _higher_function_scope(self.scope())\n        if parent_function:\n            _, stmts = parent_function.lookup(self.name)\n\n        if not stmts:\n            raise exceptions.NameInferenceError(\n                name=self.name, scope=self.scope(), context=context\n            )\n    context = contextmod.copy_context(context)\n    context.lookupname = self.name\n    return bases._infer_stmts(stmts, context, frame)", "response": "infer a Name from the name of the current object"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef infer_call(self, context=None):\n    callcontext = contextmod.copy_context(context)\n    callcontext.callcontext = contextmod.CallContext(\n        args=self.args, keywords=self.keywords\n    )\n    callcontext.boundnode = None\n    if context is not None:\n        callcontext.extra_context = _populate_context_lookup(self, context.clone())\n\n    for callee in self.func.infer(context):\n        if callee is util.Uninferable:\n            yield callee\n            continue\n        try:\n            if hasattr(callee, \"infer_call_result\"):\n                yield from callee.infer_call_result(caller=self, context=callcontext)\n        except exceptions.InferenceError:\n            continue\n    return dict(node=self, context=context)", "response": "Infer a Call node by trying to guess what the function returns"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninferring an Import node return the imported module or object", "response": "def infer_import(self, context=None, asname=True):\n    \"\"\"infer an Import node: return the imported module/object\"\"\"\n    name = context.lookupname\n    if name is None:\n        raise exceptions.InferenceError(node=self, context=context)\n\n    try:\n        if asname:\n            yield self.do_import_module(self.real_name(name))\n        else:\n            yield self.do_import_module(name)\n    except exceptions.AstroidBuildingError as exc:\n        raise exceptions.InferenceError(node=self, context=context) from exc"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninfers a ImportFrom node return the imported module or object", "response": "def infer_import_from(self, context=None, asname=True):\n    \"\"\"infer a ImportFrom node: return the imported module/object\"\"\"\n    name = context.lookupname\n    if name is None:\n        raise exceptions.InferenceError(node=self, context=context)\n    if asname:\n        name = self.real_name(name)\n\n    try:\n        module = self.do_import_module()\n    except exceptions.AstroidBuildingError as exc:\n        raise exceptions.InferenceError(node=self, context=context) from exc\n\n    try:\n        context = contextmod.copy_context(context)\n        context.lookupname = name\n        stmts = module.getattr(name, ignore_locals=module is self.root())\n        return bases._infer_stmts(stmts, context)\n    except exceptions.AttributeInferenceError as error:\n        raise exceptions.InferenceError(\n            error.message, target=self, attribute=name, context=context\n        ) from error"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef infer_attribute(self, context=None):\n    for owner in self.expr.infer(context):\n        if owner is util.Uninferable:\n            yield owner\n            continue\n\n        if context and context.boundnode:\n            # This handles the situation where the attribute is accessed through a subclass\n            # of a base class and the attribute is defined at the base class's level,\n            # by taking in consideration a redefinition in the subclass.\n            if isinstance(owner, bases.Instance) and isinstance(\n                context.boundnode, bases.Instance\n            ):\n                try:\n                    if helpers.is_subtype(\n                        helpers.object_type(context.boundnode),\n                        helpers.object_type(owner),\n                    ):\n                        owner = context.boundnode\n                except exceptions._NonDeducibleTypeHierarchy:\n                    # Can't determine anything useful.\n                    pass\n\n        try:\n            context.boundnode = owner\n            yield from owner.igetattr(self.attrname, context)\n            context.boundnode = None\n        except (exceptions.AttributeInferenceError, exceptions.InferenceError):\n            context.boundnode = None\n        except AttributeError:\n            # XXX method / function\n            context.boundnode = None\n    return dict(node=self, context=context)", "response": "Infer an attribute node by using getattr on the associated object"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef infer_subscript(self, context=None):\n\n    found_one = False\n    for value in self.value.infer(context):\n        if value is util.Uninferable:\n            yield util.Uninferable\n            return None\n        for index in self.slice.infer(context):\n            if index is util.Uninferable:\n                yield util.Uninferable\n                return None\n\n            # Try to deduce the index value.\n            index_value = _SUBSCRIPT_SENTINEL\n            if value.__class__ == bases.Instance:\n                index_value = index\n            else:\n                if index.__class__ == bases.Instance:\n                    instance_as_index = helpers.class_instance_as_index(index)\n                    if instance_as_index:\n                        index_value = instance_as_index\n                else:\n                    index_value = index\n            if index_value is _SUBSCRIPT_SENTINEL:\n                raise exceptions.InferenceError(node=self, context=context)\n\n            try:\n                assigned = value.getitem(index_value, context)\n            except (\n                exceptions.AstroidTypeError,\n                exceptions.AstroidIndexError,\n                exceptions.AttributeInferenceError,\n                AttributeError,\n            ) as exc:\n                raise exceptions.InferenceError(node=self, context=context) from exc\n\n            # Prevent inferring if the inferred subscript\n            # is the same as the original subscripted object.\n            if self is assigned or assigned is util.Uninferable:\n                yield util.Uninferable\n                return None\n            yield from assigned.infer(context)\n            found_one = True\n\n    if found_one:\n        return dict(node=self, context=context)\n    return None", "response": "Infer the subscript of the node."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _infer_boolop(self, context=None):\n    values = self.values\n    if self.op == \"or\":\n        predicate = operator.truth\n    else:\n        predicate = operator.not_\n\n    try:\n        values = [value.infer(context=context) for value in values]\n    except exceptions.InferenceError:\n        yield util.Uninferable\n        return None\n\n    for pair in itertools.product(*values):\n        if any(item is util.Uninferable for item in pair):\n            # Can't infer the final result, just yield Uninferable.\n            yield util.Uninferable\n            continue\n\n        bool_values = [item.bool_value() for item in pair]\n        if any(item is util.Uninferable for item in bool_values):\n            # Can't infer the final result, just yield Uninferable.\n            yield util.Uninferable\n            continue\n\n        # Since the boolean operations are short circuited operations,\n        # this code yields the first value for which the predicate is True\n        # and if no value respected the predicate, then the last value will\n        # be returned (or Uninferable if there was no last value).\n        # This is conforming to the semantics of `and` and `or`:\n        #   1 and 0 -> 1\n        #   0 and 1 -> 0\n        #   1 or 0 -> 1\n        #   0 or 1 -> 1\n        value = util.Uninferable\n        for value, bool_value in zip(pair, bool_values):\n            if predicate(bool_value):\n                yield value\n                break\n        else:\n            yield value\n\n    return dict(node=self, context=context)", "response": "Infer a boolean operation."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninfer what an UnaryOp should return when evaluated.", "response": "def _infer_unaryop(self, context=None):\n    \"\"\"Infer what an UnaryOp should return when evaluated.\"\"\"\n    for operand in self.operand.infer(context):\n        try:\n            yield operand.infer_unary_op(self.op)\n        except TypeError as exc:\n            # The operand doesn't support this operation.\n            yield util.BadUnaryOperationMessage(operand, self.op, exc)\n        except AttributeError as exc:\n            meth = protocols.UNARY_OP_METHOD[self.op]\n            if meth is None:\n                # `not node`. Determine node's boolean\n                # value and negate its result, unless it is\n                # Uninferable, which will be returned as is.\n                bool_value = operand.bool_value()\n                if bool_value is not util.Uninferable:\n                    yield nodes.const_factory(not bool_value)\n                else:\n                    yield util.Uninferable\n            else:\n                if not isinstance(operand, (bases.Instance, nodes.ClassDef)):\n                    # The operation was used on something which\n                    # doesn't support it.\n                    yield util.BadUnaryOperationMessage(operand, self.op, exc)\n                    continue\n\n                try:\n                    try:\n                        methods = dunder_lookup.lookup(operand, meth)\n                    except exceptions.AttributeInferenceError:\n                        yield util.BadUnaryOperationMessage(operand, self.op, exc)\n                        continue\n\n                    meth = methods[0]\n                    inferred = next(meth.infer(context=context))\n                    if inferred is util.Uninferable or not inferred.callable():\n                        continue\n\n                    context = contextmod.copy_context(context)\n                    context.callcontext = contextmod.CallContext(args=[operand])\n                    call_results = inferred.infer_call_result(self, context=context)\n                    result = next(call_results, None)\n                    if result is None:\n                        # Failed to infer, return the same type.\n                        yield operand\n                    else:\n                        yield result\n                except exceptions.AttributeInferenceError as exc:\n                    # The unary operation special method was not found.\n                    yield util.BadUnaryOperationMessage(operand, self.op, exc)\n                except exceptions.InferenceError:\n                    yield util.Uninferable"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef infer_unaryop(self, context=None):\n    yield from _filter_operation_errors(\n        self, _infer_unaryop, context, util.BadUnaryOperationMessage\n    )\n    return dict(node=self, context=context)", "response": "Infer what an UnaryOp should return when evaluated."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninvokes the given instance s binary operation inference method.", "response": "def _invoke_binop_inference(instance, opnode, op, other, context, method_name):\n    \"\"\"Invoke binary operation inference on the given instance.\"\"\"\n    methods = dunder_lookup.lookup(instance, method_name)\n    context = contextmod.bind_context_to_node(context, instance)\n    method = methods[0]\n    inferred = next(method.infer(context=context))\n    if inferred is util.Uninferable:\n        raise exceptions.InferenceError\n    return instance.infer_binary_op(opnode, op, other, context, inferred)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget an augmented binary operation inference callable.", "response": "def _aug_op(instance, opnode, op, other, context, reverse=False):\n    \"\"\"Get an inference callable for an augmented binary operation.\"\"\"\n    method_name = protocols.AUGMENTED_OP_METHOD[op]\n    return functools.partial(\n        _invoke_binop_inference,\n        instance=instance,\n        op=op,\n        opnode=opnode,\n        other=other,\n        context=context,\n        method_name=method_name,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _bin_op(instance, opnode, op, other, context, reverse=False):\n    if reverse:\n        method_name = protocols.REFLECTED_BIN_OP_METHOD[op]\n    else:\n        method_name = protocols.BIN_OP_METHOD[op]\n    return functools.partial(\n        _invoke_binop_inference,\n        instance=instance,\n        op=op,\n        opnode=opnode,\n        other=other,\n        context=context,\n        method_name=method_name,\n    )", "response": "Returns a inference callable for a normal binary operation."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_binop_contexts(context, left, right):\n    # The order is important, since the first one should be\n    # left.__op__(right).\n    for arg in (right, left):\n        new_context = context.clone()\n        new_context.callcontext = contextmod.CallContext(args=[arg])\n        new_context.boundnode = None\n        yield new_context", "response": "Get contexts for binary operations."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the flow for binary operations.", "response": "def _get_binop_flow(\n    left, left_type, binary_opnode, right, right_type, context, reverse_context\n):\n    \"\"\"Get the flow for binary operations.\n\n    The rules are a bit messy:\n\n        * if left and right have the same type, then only one\n          method will be called, left.__op__(right)\n        * if left and right are unrelated typewise, then first\n          left.__op__(right) is tried and if this does not exist\n          or returns NotImplemented, then right.__rop__(left) is tried.\n        * if left is a subtype of right, then only left.__op__(right)\n          is tried.\n        * if left is a supertype of right, then right.__rop__(left)\n          is first tried and then left.__op__(right)\n    \"\"\"\n    op = binary_opnode.op\n    if _same_type(left_type, right_type):\n        methods = [_bin_op(left, binary_opnode, op, right, context)]\n    elif helpers.is_subtype(left_type, right_type):\n        methods = [_bin_op(left, binary_opnode, op, right, context)]\n    elif helpers.is_supertype(left_type, right_type):\n        methods = [\n            _bin_op(right, binary_opnode, op, left, reverse_context, reverse=True),\n            _bin_op(left, binary_opnode, op, right, context),\n        ]\n    else:\n        methods = [\n            _bin_op(left, binary_opnode, op, right, context),\n            _bin_op(right, binary_opnode, op, left, reverse_context, reverse=True),\n        ]\n    return methods"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_aug_flow(\n    left, left_type, aug_opnode, right, right_type, context, reverse_context\n):\n    \"\"\"Get the flow for augmented binary operations.\n\n    The rules are a bit messy:\n\n        * if left and right have the same type, then left.__augop__(right)\n          is first tried and then left.__op__(right).\n        * if left and right are unrelated typewise, then\n          left.__augop__(right) is tried, then left.__op__(right)\n          is tried and then right.__rop__(left) is tried.\n        * if left is a subtype of right, then left.__augop__(right)\n          is tried and then left.__op__(right).\n        * if left is a supertype of right, then left.__augop__(right)\n          is tried, then right.__rop__(left) and then\n          left.__op__(right)\n    \"\"\"\n    bin_op = aug_opnode.op.strip(\"=\")\n    aug_op = aug_opnode.op\n    if _same_type(left_type, right_type):\n        methods = [\n            _aug_op(left, aug_opnode, aug_op, right, context),\n            _bin_op(left, aug_opnode, bin_op, right, context),\n        ]\n    elif helpers.is_subtype(left_type, right_type):\n        methods = [\n            _aug_op(left, aug_opnode, aug_op, right, context),\n            _bin_op(left, aug_opnode, bin_op, right, context),\n        ]\n    elif helpers.is_supertype(left_type, right_type):\n        methods = [\n            _aug_op(left, aug_opnode, aug_op, right, context),\n            _bin_op(right, aug_opnode, bin_op, left, reverse_context, reverse=True),\n            _bin_op(left, aug_opnode, bin_op, right, context),\n        ]\n    else:\n        methods = [\n            _aug_op(left, aug_opnode, aug_op, right, context),\n            _bin_op(left, aug_opnode, bin_op, right, context),\n            _bin_op(right, aug_opnode, bin_op, left, reverse_context, reverse=True),\n        ]\n    return methods", "response": "Get the flow for augmented binary operations."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _infer_binary_operation(left, right, binary_opnode, context, flow_factory):\n\n    context, reverse_context = _get_binop_contexts(context, left, right)\n    left_type = helpers.object_type(left)\n    right_type = helpers.object_type(right)\n    methods = flow_factory(\n        left, left_type, binary_opnode, right, right_type, context, reverse_context\n    )\n    for method in methods:\n        try:\n            results = list(method())\n        except AttributeError:\n            continue\n        except exceptions.AttributeInferenceError:\n            continue\n        except exceptions.InferenceError:\n            yield util.Uninferable\n            return\n        else:\n            if any(result is util.Uninferable for result in results):\n                yield util.Uninferable\n                return\n\n            if all(map(_is_not_implemented, results)):\n                continue\n            not_implemented = sum(\n                1 for result in results if _is_not_implemented(result)\n            )\n            if not_implemented and not_implemented != len(results):\n                # Can't infer yet what this is.\n                yield util.Uninferable\n                return\n\n            yield from results\n            return\n    # The operation doesn't seem to be supported so let the caller know about it\n    yield util.BadBinaryOperationMessage(left_type, binary_opnode.op, right_type)", "response": "Infer a binary operation between two operands."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _infer_binop(self, context):\n    left = self.left\n    right = self.right\n\n    # we use two separate contexts for evaluating lhs and rhs because\n    # 1. evaluating lhs may leave some undesired entries in context.path\n    #    which may not let us infer right value of rhs\n    context = context or contextmod.InferenceContext()\n    lhs_context = contextmod.copy_context(context)\n    rhs_context = contextmod.copy_context(context)\n    lhs_iter = left.infer(context=lhs_context)\n    rhs_iter = right.infer(context=rhs_context)\n    for lhs, rhs in itertools.product(lhs_iter, rhs_iter):\n        if any(value is util.Uninferable for value in (rhs, lhs)):\n            # Don't know how to process this.\n            yield util.Uninferable\n            return\n\n        try:\n            yield from _infer_binary_operation(lhs, rhs, self, context, _get_binop_flow)\n        except exceptions._NonDeducibleTypeHierarchy:\n            yield util.Uninferable", "response": "Binary operation inference logic."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninfers a AssignName or AssignAttr", "response": "def infer_assign(self, context=None):\n    \"\"\"infer a AssignName/AssignAttr: need to inspect the RHS part of the\n    assign node\n    \"\"\"\n    stmt = self.statement()\n    if isinstance(stmt, nodes.AugAssign):\n        return stmt.infer(context)\n\n    stmts = list(self.assigned_stmts(context=context))\n    return bases._infer_stmts(stmts, context)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _nose_tools_functions():\n    module = _BUILDER.string_build(\n        textwrap.dedent(\n            \"\"\"\n    import unittest\n\n    class Test(unittest.TestCase):\n        pass\n    a = Test()\n    \"\"\"\n        )\n    )\n    try:\n        case = next(module[\"a\"].infer())\n    except astroid.InferenceError:\n        return\n    for method in case.methods():\n        if method.name.startswith(\"assert\") and \"_\" not in method.name:\n            pep8_name = _pep8(method.name)\n            yield pep8_name, astroid.BoundMethod(method, case)\n        if method.name == \"assertEqual\":\n            # nose also exports assert_equals.\n            yield \"assert_equals\", astroid.BoundMethod(method, case)", "response": "Get an iterator of names and bound methods."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning True if the current node correspond to the function inside the numpy module in parameters", "response": "def _looks_like_numpy_function(func_name, numpy_module_name, node):\n    \"\"\"\n    Return True if the current node correspond to the function inside\n    the numpy module in parameters\n\n    :param node: the current node\n    :type node: FunctionDef\n    :param func_name: name of the function\n    :type func_name: str\n    :param numpy_module_name: name of the numpy module\n    :type numpy_module_name: str\n    :return: True if the current node correspond to the function looked for\n    :rtype: bool\n    \"\"\"\n    return node.name == func_name and node.parent.name == numpy_module_name"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef lookup(node, name):\n    if isinstance(\n        node, (astroid.List, astroid.Tuple, astroid.Const, astroid.Dict, astroid.Set)\n    ):\n        return _builtin_lookup(node, name)\n    if isinstance(node, astroid.Instance):\n        return _lookup_in_mro(node, name)\n    if isinstance(node, astroid.ClassDef):\n        return _class_lookup(node, name)\n\n    raise exceptions.AttributeInferenceError(attribute=name, target=node)", "response": "Lookup the given name in the given node."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef unpack_infer(stmt, context=None):\n    if isinstance(stmt, (List, Tuple)):\n        for elt in stmt.elts:\n            if elt is util.Uninferable:\n                yield elt\n                continue\n            yield from unpack_infer(elt, context)\n        return dict(node=stmt, context=context)\n    # if inferred is a final node, return it and stop\n    inferred = next(stmt.infer(context))\n    if inferred is stmt:\n        yield inferred\n        return dict(node=stmt, context=context)\n    # else, infer recursively, except Uninferable object that should be returned as is\n    for inferred in stmt.infer(context):\n        if inferred is util.Uninferable:\n            yield inferred\n        else:\n            yield from unpack_infer(inferred, context)\n\n    return dict(node=stmt, context=context)", "response": "recursively generate nodes inferred by the given statement."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef are_exclusive(\n    stmt1, stmt2, exceptions=None\n):  # pylint: disable=redefined-outer-name\n    \"\"\"return true if the two given statements are mutually exclusive\n\n    `exceptions` may be a list of exception names. If specified, discard If\n    branches and check one of the statement is in an exception handler catching\n    one of the given exceptions.\n\n    algorithm :\n     1) index stmt1's parents\n     2) climb among stmt2's parents until we find a common parent\n     3) if the common parent is a If or TryExcept statement, look if nodes are\n        in exclusive branches\n    \"\"\"\n    # index stmt1's parents\n    stmt1_parents = {}\n    children = {}\n    node = stmt1.parent\n    previous = stmt1\n    while node:\n        stmt1_parents[node] = 1\n        children[node] = previous\n        previous = node\n        node = node.parent\n    # climb among stmt2's parents until we find a common parent\n    node = stmt2.parent\n    previous = stmt2\n    while node:\n        if node in stmt1_parents:\n            # if the common parent is a If or TryExcept statement, look if\n            # nodes are in exclusive branches\n            if isinstance(node, If) and exceptions is None:\n                if (\n                    node.locate_child(previous)[1]\n                    is not node.locate_child(children[node])[1]\n                ):\n                    return True\n            elif isinstance(node, TryExcept):\n                c2attr, c2node = node.locate_child(previous)\n                c1attr, c1node = node.locate_child(children[node])\n                if c1node is not c2node:\n                    first_in_body_caught_by_handlers = (\n                        c2attr == \"handlers\"\n                        and c1attr == \"body\"\n                        and previous.catch(exceptions)\n                    )\n                    second_in_body_caught_by_handlers = (\n                        c2attr == \"body\"\n                        and c1attr == \"handlers\"\n                        and children[node].catch(exceptions)\n                    )\n                    first_in_else_other_in_handlers = (\n                        c2attr == \"handlers\" and c1attr == \"orelse\"\n                    )\n                    second_in_else_other_in_handlers = (\n                        c2attr == \"orelse\" and c1attr == \"handlers\"\n                    )\n                    if any(\n                        (\n                            first_in_body_caught_by_handlers,\n                            second_in_body_caught_by_handlers,\n                            first_in_else_other_in_handlers,\n                            second_in_else_other_in_handlers,\n                        )\n                    ):\n                        return True\n                elif c2attr == \"handlers\" and c1attr == \"handlers\":\n                    return previous is not children[node]\n            return False\n        previous = node\n        node = node.parent\n    return False", "response": "Return True if two statements are mutually exclusive."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _slice_value(index, context=None):\n\n    if isinstance(index, Const):\n        if isinstance(index.value, (int, type(None))):\n            return index.value\n    elif index is None:\n        return None\n    else:\n        # Try to infer what the index actually is.\n        # Since we can't return all the possible values,\n        # we'll stop at the first possible value.\n        try:\n            inferred = next(index.infer(context=context))\n        except exceptions.InferenceError:\n            pass\n        else:\n            if isinstance(inferred, Const):\n                if isinstance(inferred.value, (int, type(None))):\n                    return inferred.value\n\n    # Use a sentinel, because None can be a valid\n    # value that this function can return,\n    # as it is the case for unspecified bounds.\n    return _SLICE_SENTINEL", "response": "Get the value of the given slice index."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a slice or an item for the given sequence.", "response": "def _container_getitem(instance, elts, index, context=None):\n    \"\"\"Get a slice or an item, using the given *index*, for the given sequence.\"\"\"\n    try:\n        if isinstance(index, Slice):\n            index_slice = _infer_slice(index, context=context)\n            new_cls = instance.__class__()\n            new_cls.elts = elts[index_slice]\n            new_cls.parent = instance.parent\n            return new_cls\n        if isinstance(index, Const):\n            return elts[index.value]\n    except IndexError as exc:\n        raise exceptions.AstroidIndexError(\n            message=\"Index {index!s} out of range\",\n            node=instance,\n            index=index,\n            context=context,\n        ) from exc\n    except TypeError as exc:\n        raise exceptions.AstroidTypeError(\n            message=\"Type error {error!r}\", node=instance, index=index, context=context\n        ) from exc\n\n    raise exceptions.AstroidTypeError(\"Could not use %s as subscript index\" % index)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _update_const_classes():\n    klasses = (bool, int, float, complex, str, bytes)\n    for kls in klasses:\n        CONST_CLS[kls] = Const", "response": "update CONST_CLS so the keys of CONST_CLS can be reused"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning an astroid node for a python value", "response": "def const_factory(value):\n    \"\"\"return an astroid node for a python value\"\"\"\n    # XXX we should probably be stricter here and only consider stuff in\n    # CONST_CLS or do better treatment: in case where value is not in CONST_CLS,\n    # we should rather recall the builder on this value than returning an empty\n    # node (another option being that const_factory shouldn't be called with something\n    # not in CONST_CLS)\n    assert not isinstance(value, NodeNG)\n\n    # Hack for ignoring elements of a sequence\n    # or a mapping, in order to avoid transforming\n    # each element to an AST. This is fixed in 2.0\n    # and this approach is a temporary hack.\n    if isinstance(value, (list, set, tuple, dict)):\n        elts = []\n    else:\n        elts = value\n\n    try:\n        initializer_cls = CONST_CLS[value.__class__]\n        initializer = _CONST_CLS_CONSTRUCTORS[initializer_cls]\n        return initializer(initializer_cls, elts)\n    except (KeyError, AttributeError):\n        node = EmptyNode()\n        node.object = value\n        return node"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning True if the given node is the child of a decorator.", "response": "def is_from_decorator(node):\n    \"\"\"Return True if the given node is the child of a decorator\"\"\"\n    parent = node.parent\n    while parent is not None:\n        if isinstance(parent, Decorators):\n            return True\n        parent = parent.parent\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget a generator of the inferred values.", "response": "def infer(self, context=None, **kwargs):\n        \"\"\"Get a generator of the inferred values.\n\n        This is the main entry point to the inference system.\n\n        .. seealso:: :ref:`inference`\n\n        If the instance has some explicit inference function set, it will be\n        called instead of the default interface.\n\n        :returns: The inferred values.\n        :rtype: iterable\n        \"\"\"\n        if context is not None:\n            context = context.extra_context.get(self, context)\n        if self._explicit_inference is not None:\n            # explicit_inference is not bound, give it self explicitly\n            try:\n                # pylint: disable=not-callable\n                return self._explicit_inference(self, context, **kwargs)\n            except exceptions.UseInferenceDefault:\n                pass\n\n        if not context:\n            return self._infer(context, **kwargs)\n\n        key = (self, context.lookupname, context.callcontext, context.boundnode)\n        if key in context.inferred:\n            return iter(context.inferred[key])\n\n        gen = context.cache_generator(key, self._infer(context, **kwargs))\n        return util.limit_inference(gen, MANAGER.max_inferable_values)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a nice name for nice representation.", "response": "def _repr_name(self):\n        \"\"\"Get a name for nice representation.\n\n        This is either :attr:`name`, :attr:`attrname`, or the empty string.\n\n        :returns: The nice name.\n        :rtype: str\n        \"\"\"\n        names = {\"name\", \"attrname\"}\n        if all(name not in self._astroid_fields for name in names):\n            return getattr(self, \"name\", getattr(self, \"attrname\", \"\"))\n        return \"\""}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nvisiting this node using the given visitor.", "response": "def accept(self, visitor):\n        \"\"\"Visit this node using the given visitor.\"\"\"\n        func = getattr(visitor, \"visit_\" + self.__class__.__name__.lower())\n        return func(self)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_children(self):\n        for field in self._astroid_fields:\n            attr = getattr(self, field)\n            if attr is None:\n                continue\n            if isinstance(attr, (list, tuple)):\n                yield from attr\n            else:\n                yield attr", "response": "Get the children of this node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if this node is the parent of the given node.", "response": "def parent_of(self, node):\n        \"\"\"Check if this node is the parent of the given node.\n\n        :param node: The node to check if it is the child.\n        :type node: NodeNG\n\n        :returns: True if this node is the parent of the given node,\n            False otherwise.\n        :rtype: bool\n        \"\"\"\n        parent = node.parent\n        while parent is not None:\n            if self is parent:\n                return True\n            parent = parent.parent\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsearching for the sequence that contains this child.", "response": "def child_sequence(self, child):\n        \"\"\"Search for the sequence that contains this child.\n\n        :param child: The child node to search sequences for.\n        :type child: NodeNG\n\n        :returns: The sequence containing the given child node.\n        :rtype: iterable(NodeNG)\n\n        :raises AstroidError: If no sequence could be found that contains\n            the given child.\n        \"\"\"\n        for field in self._astroid_fields:\n            node_or_sequence = getattr(self, field)\n            if node_or_sequence is child:\n                return [node_or_sequence]\n            # /!\\ compiler.ast Nodes have an __iter__ walking over child nodes\n            if (\n                isinstance(node_or_sequence, (tuple, list))\n                and child in node_or_sequence\n            ):\n                return node_or_sequence\n\n        msg = \"Could not find %s in %s's children\"\n        raise exceptions.AstroidError(msg % (repr(child), repr(self)))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef nearest(self, nodes):\n        myroot = self.root()\n        mylineno = self.fromlineno\n        nearest = None, 0\n        for node in nodes:\n            assert node.root() is myroot, (\n                \"nodes %s and %s are not from the same module\" % (self, node)\n            )\n            lineno = node.fromlineno\n            if node.fromlineno > mylineno:\n                break\n            if lineno > nearest[1]:\n                nearest = node, lineno\n        # FIXME: raise an exception if nearest is None ?\n        return nearest[0]", "response": "Returns the node closest to this one in the source code."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the line number of the source code line that this node appears on in the source code.", "response": "def tolineno(self):\n        \"\"\"The last line that this node appears on in the source code.\n\n        :type: int or None\n        \"\"\"\n        if not self._astroid_fields:\n            # can't have children\n            lastchild = None\n        else:\n            lastchild = self.last_child()\n        if lastchild is None:\n            return self.fromlineno\n\n        return lastchild.tolineno"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _fixed_source_line(self):\n        line = self.lineno\n        _node = self\n        try:\n            while line is None:\n                _node = next(_node.get_children())\n                line = _node.lineno\n        except StopIteration:\n            _node = self.parent\n            while _node and line is None:\n                line = _node.lineno\n                _node = _node.parent\n        return line", "response": "Attempt to find the line that this node appears on. This method attempts to find the line that this node appears on."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef nodes_of_class(self, klass, skip_klass=None):\n        if isinstance(self, klass):\n            yield self\n\n        if skip_klass is None:\n            for child_node in self.get_children():\n                yield from child_node.nodes_of_class(klass, skip_klass)\n\n            return\n\n        for child_node in self.get_children():\n            if isinstance(child_node, skip_klass):\n                continue\n            yield from child_node.nodes_of_class(klass, skip_klass)", "response": "Get the nodes of the given type."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a string representation of the tree.", "response": "def repr_tree(\n        self,\n        ids=False,\n        include_linenos=False,\n        ast_state=False,\n        indent=\"   \",\n        max_depth=0,\n        max_width=80,\n    ):\n        \"\"\"Get a string representation of the AST from this node.\n\n        :param ids: If true, includes the ids with the node type names.\n        :type ids: bool\n\n        :param include_linenos: If true, includes the line numbers and\n            column offsets.\n        :type include_linenos: bool\n\n        :param ast_state: If true, includes information derived from\n            the whole AST like local and global variables.\n        :type ast_state: bool\n\n        :param indent: A string to use to indent the output string.\n        :type indent: str\n\n        :param max_depth: If set to a positive integer, won't return\n            nodes deeper than max_depth in the string.\n        :type max_depth: int\n\n        :param max_width: Attempt to format the output string to stay\n            within this number of characters, but can exceed it under some\n            circumstances. Only positive integer values are valid, the default is 80.\n        :type max_width: int\n\n        :returns: The string representation of the AST.\n        :rtype: str\n        \"\"\"\n        # pylint: disable=too-many-statements\n        @_singledispatch\n        def _repr_tree(node, result, done, cur_indent=\"\", depth=1):\n            \"\"\"Outputs a representation of a non-tuple/list, non-node that's\n            contained within an AST, including strings.\n            \"\"\"\n            lines = pprint.pformat(\n                node, width=max(max_width - len(cur_indent), 1)\n            ).splitlines(True)\n            result.append(lines[0])\n            result.extend([cur_indent + line for line in lines[1:]])\n            return len(lines) != 1\n\n        # pylint: disable=unused-variable; doesn't understand singledispatch\n        @_repr_tree.register(tuple)\n        @_repr_tree.register(list)\n        def _repr_seq(node, result, done, cur_indent=\"\", depth=1):\n            \"\"\"Outputs a representation of a sequence that's contained within an AST.\"\"\"\n            cur_indent += indent\n            result.append(\"[\")\n            if not node:\n                broken = False\n            elif len(node) == 1:\n                broken = _repr_tree(node[0], result, done, cur_indent, depth)\n            elif len(node) == 2:\n                broken = _repr_tree(node[0], result, done, cur_indent, depth)\n                if not broken:\n                    result.append(\", \")\n                else:\n                    result.append(\",\\n\")\n                    result.append(cur_indent)\n                broken = _repr_tree(node[1], result, done, cur_indent, depth) or broken\n            else:\n                result.append(\"\\n\")\n                result.append(cur_indent)\n                for child in node[:-1]:\n                    _repr_tree(child, result, done, cur_indent, depth)\n                    result.append(\",\\n\")\n                    result.append(cur_indent)\n                _repr_tree(node[-1], result, done, cur_indent, depth)\n                broken = True\n            result.append(\"]\")\n            return broken\n\n        # pylint: disable=unused-variable; doesn't understand singledispatch\n        @_repr_tree.register(NodeNG)\n        def _repr_node(node, result, done, cur_indent=\"\", depth=1):\n            \"\"\"Outputs a strings representation of an astroid node.\"\"\"\n            if node in done:\n                result.append(\n                    indent\n                    + \"<Recursion on %s with id=%s\" % (type(node).__name__, id(node))\n                )\n                return False\n            done.add(node)\n\n            if max_depth and depth > max_depth:\n                result.append(\"...\")\n                return False\n            depth += 1\n            cur_indent += indent\n            if ids:\n                result.append(\"%s<0x%x>(\\n\" % (type(node).__name__, id(node)))\n            else:\n                result.append(\"%s(\" % type(node).__name__)\n            fields = []\n            if include_linenos:\n                fields.extend((\"lineno\", \"col_offset\"))\n            fields.extend(node._other_fields)\n            fields.extend(node._astroid_fields)\n            if ast_state:\n                fields.extend(node._other_other_fields)\n            if not fields:\n                broken = False\n            elif len(fields) == 1:\n                result.append(\"%s=\" % fields[0])\n                broken = _repr_tree(\n                    getattr(node, fields[0]), result, done, cur_indent, depth\n                )\n            else:\n                result.append(\"\\n\")\n                result.append(cur_indent)\n                for field in fields[:-1]:\n                    result.append(\"%s=\" % field)\n                    _repr_tree(getattr(node, field), result, done, cur_indent, depth)\n                    result.append(\",\\n\")\n                    result.append(cur_indent)\n                result.append(\"%s=\" % fields[-1])\n                _repr_tree(getattr(node, fields[-1]), result, done, cur_indent, depth)\n                broken = True\n            result.append(\")\")\n            return broken\n\n        result = []\n        _repr_tree(self, result, set())\n        return \"\".join(result)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the next sibling statement node.", "response": "def next_sibling(self):\n        \"\"\"The next sibling statement node.\n\n        :returns: The next sibling statement node.\n        :rtype: NodeNG or None\n        \"\"\"\n        stmts = self.parent.child_sequence(self)\n        index = stmts.index(self)\n        try:\n            return stmts[index + 1]\n        except IndexError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef previous_sibling(self):\n        stmts = self.parent.child_sequence(self)\n        index = stmts.index(self)\n        if index >= 1:\n            return stmts[index - 1]\n        return None", "response": "Returns the previous sibling statement node."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_elements(cls, elts=None):\n        node = cls()\n        if elts is None:\n            node.elts = []\n        else:\n            node.elts = [const_factory(e) if _is_const(e) else e for e in elts]\n        return node", "response": "Create a node of this type from the given list of elements."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _filter_stmts(self, stmts, frame, offset):\n        # if offset == -1, my actual frame is not the inner frame but its parent\n        #\n        # class A(B): pass\n        #\n        # we need this to resolve B correctly\n        if offset == -1:\n            myframe = self.frame().parent.frame()\n        else:\n            myframe = self.frame()\n            # If the frame of this node is the same as the statement\n            # of this node, then the node is part of a class or\n            # a function definition and the frame of this node should be the\n            # the upper frame, not the frame of the definition.\n            # For more information why this is important,\n            # see Pylint issue #295.\n            # For example, for 'b', the statement is the same\n            # as the frame / scope:\n            #\n            # def test(b=1):\n            #     ...\n\n            if self.statement() is myframe and myframe.parent:\n                myframe = myframe.parent.frame()\n        mystmt = self.statement()\n        # line filtering if we are in the same frame\n        #\n        # take care node may be missing lineno information (this is the case for\n        # nodes inserted for living objects)\n        if myframe is frame and mystmt.fromlineno is not None:\n            assert mystmt.fromlineno is not None, mystmt\n            mylineno = mystmt.fromlineno + offset\n        else:\n            # disabling lineno filtering\n            mylineno = 0\n\n        _stmts = []\n        _stmt_parents = []\n        statements = self._get_filtered_node_statements(stmts)\n\n        for node, stmt in statements:\n            # line filtering is on and we have reached our location, break\n            if stmt.fromlineno > mylineno > 0:\n                break\n            # Ignore decorators with the same name as the\n            # decorated function\n            # Fixes issue #375\n            if mystmt is stmt and is_from_decorator(self):\n                continue\n            assert hasattr(node, \"assign_type\"), (\n                node,\n                node.scope(),\n                node.scope().locals,\n            )\n            assign_type = node.assign_type()\n            if node.has_base(self):\n                break\n\n            _stmts, done = assign_type._get_filtered_stmts(self, node, _stmts, mystmt)\n            if done:\n                break\n\n            optional_assign = assign_type.optional_assign\n            if optional_assign and assign_type.parent_of(self):\n                # we are inside a loop, loop var assignment is hiding previous\n                # assignment\n                _stmts = [node]\n                _stmt_parents = [stmt.parent]\n                continue\n\n            # XXX comment various branches below!!!\n            try:\n                pindex = _stmt_parents.index(stmt.parent)\n            except ValueError:\n                pass\n            else:\n                # we got a parent index, this means the currently visited node\n                # is at the same block level as a previously visited node\n                if _stmts[pindex].assign_type().parent_of(assign_type):\n                    # both statements are not at the same block level\n                    continue\n                # if currently visited node is following previously considered\n                # assignment and both are not exclusive, we can drop the\n                # previous one. For instance in the following code ::\n                #\n                #   if a:\n                #     x = 1\n                #   else:\n                #     x = 2\n                #   print x\n                #\n                # we can't remove neither x = 1 nor x = 2 when looking for 'x'\n                # of 'print x'; while in the following ::\n                #\n                #   x = 1\n                #   x = 2\n                #   print x\n                #\n                # we can remove x = 1 when we see x = 2\n                #\n                # moreover, on loop assignment types, assignment won't\n                # necessarily be done if the loop has no iteration, so we don't\n                # want to clear previous assignments if any (hence the test on\n                # optional_assign)\n                if not (optional_assign or are_exclusive(_stmts[pindex], node)):\n                    if (\n                        # In case of partial function node, if the statement is different\n                        # from the origin function then it can be deleted otherwise it should\n                        # remain to be able to correctly infer the call to origin function.\n                        not node.is_function\n                        or node.qname() != \"PartialFunction\"\n                        or node.name != _stmts[pindex].name\n                    ):\n                        del _stmt_parents[pindex]\n                        del _stmts[pindex]\n            if isinstance(node, AssignName):\n                if not optional_assign and stmt.parent is mystmt.parent:\n                    _stmts = []\n                    _stmt_parents = []\n            elif isinstance(node, DelName):\n                _stmts = []\n                _stmt_parents = []\n                continue\n            if not are_exclusive(self, node):\n                _stmts.append(node)\n                _stmt_parents.append(stmt.parent)\n        return _stmts", "response": "Filter the given list of statements to remove ignorable statements."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef postinit(\n        self,\n        args,\n        defaults,\n        kwonlyargs,\n        kw_defaults,\n        annotations,\n        kwonlyargs_annotations=None,\n        varargannotation=None,\n        kwargannotation=None,\n    ):\n        \"\"\"Do some setup after initialisation.\n\n        :param args: The names of the required arguments.\n        :type args: list(AssignName)\n\n        :param defaults: The default values for arguments that can be passed\n            positionally.\n        :type defaults: list(NodeNG)\n\n        :param kwonlyargs: The keyword arguments that cannot be passed\n            positionally.\n        :type kwonlyargs: list(AssignName)\n\n        :param kw_defaults: The default values for keyword arguments that\n            cannot be passed positionally.\n        :type kw_defaults: list(NodeNG)\n\n        :param annotations: The type annotations of arguments that can be\n            passed positionally.\n        :type annotations: list(NodeNG)\n\n        :param kwonlyargs_annotations: The type annotations of arguments that\n            cannot be passed positionally. This should always be passed in\n            Python 3.\n        :type kwonlyargs_annotations: list(NodeNG)\n\n        :param varargannotation: The type annotation for the variable length\n            arguments.\n        :type varargannotation: NodeNG\n\n        :param kwargannotation: The type annotation for the variable length\n            keyword arguments.\n        :type kwargannotation: NodeNG\n        \"\"\"\n        self.args = args\n        self.defaults = defaults\n        self.kwonlyargs = kwonlyargs\n        self.kw_defaults = kw_defaults\n        self.annotations = annotations\n        self.kwonlyargs_annotations = kwonlyargs_annotations\n        self.varargannotation = varargannotation\n        self.kwargannotation = kwargannotation", "response": "Do some setup after initialization of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef format_args(self):\n        result = []\n        if self.args:\n            result.append(\n                _format_args(\n                    self.args, self.defaults, getattr(self, \"annotations\", None)\n                )\n            )\n        if self.vararg:\n            result.append(\"*%s\" % self.vararg)\n        if self.kwonlyargs:\n            if not self.vararg:\n                result.append(\"*\")\n            result.append(\n                _format_args(\n                    self.kwonlyargs, self.kw_defaults, self.kwonlyargs_annotations\n                )\n            )\n        if self.kwarg:\n            result.append(\"**%s\" % self.kwarg)\n        return \", \".join(result)", "response": "Get the arguments formatted as string."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the default value for the given argument.", "response": "def default_value(self, argname):\n        \"\"\"Get the default value for an argument.\n\n        :param argname: The name of the argument to get the default value for.\n        :type argname: str\n\n        :raises NoDefault: If there is no default value defined for the\n            given argument.\n        \"\"\"\n        i = _find_arg(argname, self.args)[0]\n        if i is not None:\n            idx = i - (len(self.args) - len(self.defaults))\n            if idx >= 0:\n                return self.defaults[idx]\n        i = _find_arg(argname, self.kwonlyargs)[0]\n        if i is not None and self.kw_defaults[i] is not None:\n            return self.kw_defaults[i]\n        raise exceptions.NoDefault(func=self.parent, name=argname)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if the given name is defined in the arguments.", "response": "def is_argument(self, name):\n        \"\"\"Check if the given name is defined in the arguments.\n\n        :param name: The name to check for.\n        :type name: str\n\n        :returns: True if the given name is defined in the arguments,\n            False otherwise.\n        :rtype: bool\n        \"\"\"\n        if name == self.vararg:\n            return True\n        if name == self.kwarg:\n            return True\n        return (\n            self.find_argname(name, True)[1] is not None\n            or self.kwonlyargs\n            and _find_arg(name, self.kwonlyargs, True)[1] is not None\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding the index and node for the given name.", "response": "def find_argname(self, argname, rec=False):\n        \"\"\"Get the index and :class:`AssignName` node for given name.\n\n        :param argname: The name of the argument to search for.\n        :type argname: str\n\n        :param rec: Whether or not to include arguments in unpacked tuples\n            in the search.\n        :type rec: bool\n\n        :returns: The index and node for the argument.\n        :rtype: tuple(str or None, AssignName or None)\n        \"\"\"\n        if self.args:  # self.args may be None in some cases (builtin function)\n            return _find_arg(argname, self.args, rec)\n        return None, None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndoes some setup after initialisation.", "response": "def postinit(self, test=None, fail=None):\n        \"\"\"Do some setup after initialisation.\n\n        :param test: The test that passes or fails the assertion.\n        :type test: NodeNG or None\n\n        :param fail: The message shown when the assertion fails.\n        :type fail: NodeNG or None\n        \"\"\"\n        self.fail = fail\n        self.test = test"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndoing some setup after initialization.", "response": "def postinit(self, targets=None, value=None, type_annotation=None):\n        \"\"\"Do some setup after initialisation.\n\n        :param targets: What is being assigned to.\n        :type targets: list(NodeNG) or None\n\n        :param value: The value being assigned to the variables.\n        :type: NodeNG or None\n        \"\"\"\n        self.targets = targets\n        self.value = value\n        self.type_annotation = type_annotation"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef postinit(self, target, annotation, simple, value=None):\n        self.target = target\n        self.annotation = annotation\n        self.value = value\n        self.simple = simple", "response": "Do some setup after initialization."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndo some setup after initialisation.", "response": "def postinit(self, target=None, value=None):\n        \"\"\"Do some setup after initialisation.\n\n        :param target: What is being assigned to.\n        :type target: NodeNG or None\n\n        :param value: The value being assigned to the variable.\n        :type: NodeNG or None\n        \"\"\"\n        self.target = target\n        self.value = value"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef type_errors(self, context=None):\n        try:\n            results = self._infer_augassign(context=context)\n            return [\n                result\n                for result in results\n                if isinstance(result, util.BadBinaryOperationMessage)\n            ]\n        except exceptions.InferenceError:\n            return []", "response": "Get a list of type errors which can occur during inference."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndo some setup after initialisation.", "response": "def postinit(self, left=None, right=None):\n        \"\"\"Do some setup after initialisation.\n\n        :param left: What is being applied to the operator on the left side.\n        :type left: NodeNG or None\n\n        :param right: What is being applied to the operator on the right side.\n        :type right: NodeNG or None\n        \"\"\"\n        self.left = left\n        self.right = right"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndoes some setup after initialisation.", "response": "def postinit(self, func=None, args=None, keywords=None):\n        \"\"\"Do some setup after initialisation.\n\n        :param func: What is being called.\n        :type func: NodeNG or None\n\n        :param args: The positional arguments being given to the call.\n        :type args: list(NodeNG) or None\n\n        :param keywords: The keyword arguments being given to the call.\n        :type keywords: list(NodeNG) or None\n        \"\"\"\n        self.func = func\n        self.args = args\n        self.keywords = keywords"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef kwargs(self):\n        keywords = self.keywords or []\n        return [keyword for keyword in keywords if keyword.arg is None]", "response": "The keyword arguments that unpack something."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndoes some setup after initialisation.", "response": "def postinit(self, left=None, ops=None):\n        \"\"\"Do some setup after initialisation.\n\n        :param left: The value at the left being applied to a comparison\n            operator.\n        :type left: NodeNG or None\n\n        :param ops: The remainder of the operators\n            and their relevant right hand value.\n        :type ops: list(tuple(str, NodeNG)) or None\n        \"\"\"\n        self.left = left\n        self.ops = ops"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndoes some setup after initialisation.", "response": "def postinit(self, target=None, iter=None, ifs=None, is_async=None):\n        \"\"\"Do some setup after initialisation.\n\n        :param target: What is assigned to by the comprehension.\n        :type target: NodeNG or None\n\n        :param iter: What is iterated over by the comprehension.\n        :type iter: NodeNG or None\n\n        :param ifs: The contents of any if statements that filter\n            the comprehension.\n        :type ifs: list(NodeNG) or None\n\n        :param is_async: Whether this is an asynchronous comprehension or not.\n        :type: bool or None\n        \"\"\"\n        self.target = target\n        self.iter = iter\n        self.ifs = ifs\n        self.is_async = is_async"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget an item from this node if subscriptable.", "response": "def getitem(self, index, context=None):\n        \"\"\"Get an item from this node if subscriptable.\n\n        :param index: The node to use as a subscript index.\n        :type index: Const or Slice\n\n        :raises AstroidTypeError: When the given index cannot be used as a\n            subscript index, or if this node is not subscriptable.\n        \"\"\"\n        if isinstance(index, Const):\n            index_value = index.value\n        elif isinstance(index, Slice):\n            index_value = _infer_slice(index, context=context)\n\n        else:\n            raise exceptions.AstroidTypeError(\n                \"Could not use type {} as subscript index\".format(type(index))\n            )\n\n        try:\n            if isinstance(self.value, (str, bytes)):\n                return Const(self.value[index_value])\n        except IndexError as exc:\n            raise exceptions.AstroidIndexError(\n                message=\"Index {index!r} out of range\",\n                node=self,\n                index=index,\n                context=context,\n            ) from exc\n        except TypeError as exc:\n            raise exceptions.AstroidTypeError(\n                message=\"Type error {error!r}\", node=self, index=index, context=context\n            ) from exc\n\n        raise exceptions.AstroidTypeError(\"%r (value=%s)\" % (self, self.value))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a : class : Dict of constants from a live dictionary.", "response": "def from_elements(cls, items=None):\n        \"\"\"Create a :class:`Dict` of constants from a live dictionary.\n\n        :param items: The items to store in the node.\n        :type items: dict\n\n        :returns: The created dictionary node.\n        :rtype: Dict\n        \"\"\"\n        node = cls()\n        if items is None:\n            node.items = []\n        else:\n            node.items = [\n                (const_factory(k), const_factory(v) if _is_const(v) else v)\n                for k, v in items.items()\n                # The keys need to be constants\n                if _is_const(k)\n            ]\n        return node"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getitem(self, index, context=None):\n        for key, value in self.items:\n            # TODO(cpopa): no support for overriding yet, {1:2, **{1: 3}}.\n            if isinstance(key, DictUnpack):\n                try:\n                    return value.getitem(index, context)\n                except (exceptions.AstroidTypeError, exceptions.AstroidIndexError):\n                    continue\n            for inferredkey in key.infer(context):\n                if inferredkey is util.Uninferable:\n                    continue\n                if isinstance(inferredkey, Const) and isinstance(index, Const):\n                    if inferredkey.value == index.value:\n                        return value\n\n        raise exceptions.AstroidIndexError(index)", "response": "Get an item from this node."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef postinit(self, type=None, name=None, body=None):\n        self.type = type\n        self.name = name\n        self.body = body", "response": "Do some setup after initialization."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef blockstart_tolineno(self):\n        if self.name:\n            return self.name.tolineno\n        if self.type:\n            return self.type.tolineno\n        return self.lineno", "response": "The line on which the beginning of this block ends."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if this node handles any of the given exceptions.", "response": "def catch(self, exceptions):  # pylint: disable=redefined-outer-name\n        \"\"\"Check if this node handles any of the given exceptions.\n\n        If ``exceptions`` is empty, this will default to ``True``.\n\n        :param exceptions: The name of the exceptions to check for.\n        :type exceptions: list(str)\n        \"\"\"\n        if self.type is None or exceptions is None:\n            return True\n        for node in self.type._get_name_nodes():\n            if node.name in exceptions:\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndoing some setup after initialization.", "response": "def postinit(self, expr=None, globals=None, locals=None):\n        \"\"\"Do some setup after initialisation.\n\n        :param expr: The expression to be executed.\n        :type expr: NodeNG or None\n\n        :param globals:The globals dictionary to execute with.\n        :type globals: NodeNG or None\n\n        :param locals: The locals dictionary to execute with.\n        :type locals: NodeNG or None\n        \"\"\"\n        self.expr = expr\n        self.globals = globals\n        self.locals = locals"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef postinit(\n        self, target=None, iter=None, body=None, orelse=None, type_annotation=None\n    ):\n        \"\"\"Do some setup after initialisation.\n\n        :param target: What the loop assigns to.\n        :type target: NodeNG or None\n\n        :param iter: What the loop iterates over.\n        :type iter: NodeNG or None\n\n        :param body: The contents of the body of the loop.\n        :type body: list(NodeNG) or None\n\n        :param orelse: The contents of the ``else`` block of the loop.\n        :type orelse: list(NodeNG) or None\n        \"\"\"\n        self.target = target\n        self.iter = iter\n        self.body = body\n        self.orelse = orelse\n        self.type_annotation = type_annotation", "response": "Do some setup after initialisation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef postinit(self, test=None, body=None, orelse=None):\n        self.test = test\n        self.body = body\n        self.orelse = orelse", "response": "Do some setup after initialisation."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef block_range(self, lineno):\n        if lineno == self.body[0].fromlineno:\n            return lineno, lineno\n        if lineno <= self.body[-1].tolineno:\n            return lineno, self.body[-1].tolineno\n        return self._elsed_block_range(lineno, self.orelse, self.body[0].fromlineno - 1)", "response": "Get a range from the given line number to where this node ends."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getitem(self, index, context=None):\n        return _container_getitem(self, self.elts, index, context=context)", "response": "Get an item from this node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef postinit(self, dest=None, values=None):\n        self.dest = dest\n        self.values = values", "response": "Do some setup after initialisation."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef postinit(self, exc=None, cause=None):\n        self.exc = exc\n        self.cause = cause", "response": "Do some setup after initialisation."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef raises_not_implemented(self):\n        if not self.exc:\n            return False\n        for name in self.exc._get_name_nodes():\n            if name.name == \"NotImplementedError\":\n                return True\n        return False", "response": "Check if this node raises a : class:`NotImplementedError`, and return True if it raises a : class:`NotImplementedError`, False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndoing some setup after initialization.", "response": "def postinit(self, lower=None, upper=None, step=None):\n        \"\"\"Do some setup after initialisation.\n\n        :param lower: The lower index in the slice.\n        :value lower: NodeNG or None\n\n        :param upper: The upper index in the slice.\n        :value upper: NodeNG or None\n\n        :param step: The step to take between index.\n        :param step: NodeNG or None\n        \"\"\"\n        self.lower = lower\n        self.upper = upper\n        self.step = step"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwraps the empty attributes of the Slice in a Const node.", "response": "def _wrap_attribute(self, attr):\n        \"\"\"Wrap the empty attributes of the Slice in a Const node.\"\"\"\n        if not attr:\n            const = const_factory(attr)\n            const.parent = self\n            return const\n        return attr"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninfers the possible values of the given attribute on the slice.", "response": "def igetattr(self, attrname, context=None):\n        \"\"\"Infer the possible values of the given attribute on the slice.\n\n        :param attrname: The name of the attribute to infer.\n        :type attrname: str\n\n        :returns: The inferred possible values.\n        :rtype: iterable(NodeNG)\n        \"\"\"\n        if attrname == \"start\":\n            yield self._wrap_attribute(self.lower)\n        elif attrname == \"stop\":\n            yield self._wrap_attribute(self.upper)\n        elif attrname == \"step\":\n            yield self._wrap_attribute(self.step)\n        else:\n            yield from self.getattr(attrname, context=context)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndoing some setup after initialisation.", "response": "def postinit(self, value=None, slice=None):\n        \"\"\"Do some setup after initialisation.\n\n        :param value: What is being indexed.\n        :type value: NodeNG or None\n\n        :param slice: The slice being used to lookup.\n        :type slice: NodeNG or None\n        \"\"\"\n        self.value = value\n        self.slice = slice"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef postinit(self, body=None, handlers=None, orelse=None):\n        self.body = body\n        self.handlers = handlers\n        self.orelse = orelse", "response": "Do some setup after initialisation."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a range from the given line number to where this node ends.", "response": "def block_range(self, lineno):\n        \"\"\"Get a range from the given line number to where this node ends.\n\n        :param lineno: The line number to start the range at.\n        :type lineno: int\n\n        :returns: The range of line numbers that this node belongs to,\n            starting at the given line number.\n        :rtype: tuple(int, int)\n        \"\"\"\n        last = None\n        for exhandler in self.handlers:\n            if exhandler.type and lineno == exhandler.type.fromlineno:\n                return lineno, lineno\n            if exhandler.body[0].fromlineno <= lineno <= exhandler.body[-1].tolineno:\n                return lineno, exhandler.body[-1].tolineno\n            if last is None:\n                last = exhandler.body[0].fromlineno - 1\n        return self._elsed_block_range(lineno, self.orelse, last)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndoing some setup after initialisation.", "response": "def postinit(self, body=None, finalbody=None):\n        \"\"\"Do some setup after initialisation.\n\n        :param body: The try-except that the finally is attached to.\n        :type body: list(TryExcept) or None\n\n        :param finalbody: The contents of the ``finally`` block.\n        :type finalbody: list(NodeNG) or None\n        \"\"\"\n        self.body = body\n        self.finalbody = finalbody"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a range from the given line number to where this node ends.", "response": "def block_range(self, lineno):\n        \"\"\"Get a range from the given line number to where this node ends.\n\n        :param lineno: The line number to start the range at.\n        :type lineno: int\n\n        :returns: The range of line numbers that this node belongs to,\n            starting at the given line number.\n        :rtype: tuple(int, int)\n        \"\"\"\n        child = self.body[0]\n        # py2.5 try: except: finally:\n        if (\n            isinstance(child, TryExcept)\n            and child.fromlineno == self.fromlineno\n            and child.tolineno >= lineno > self.fromlineno\n        ):\n            return child.block_range(lineno)\n        return self._elsed_block_range(lineno, self.finalbody)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a list of type errors which can occur during inference.", "response": "def type_errors(self, context=None):\n        \"\"\"Get a list of type errors which can occur during inference.\n\n        Each TypeError is represented by a :class:`BadBinaryOperationMessage`,\n        which holds the original exception.\n\n        :returns: The list of possible type errors.\n        :rtype: list(BadBinaryOperationMessage)\n        \"\"\"\n        try:\n            results = self._infer_unaryop(context=context)\n            return [\n                result\n                for result in results\n                if isinstance(result, util.BadUnaryOperationMessage)\n            ]\n        except exceptions.InferenceError:\n            return []"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef postinit(self, items=None, body=None, type_annotation=None):\n        self.items = items\n        self.body = body\n        self.type_annotation = type_annotation", "response": "Do some setup after initialization."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the children of this node.", "response": "def get_children(self):\n        \"\"\"Get the child nodes below this node.\n\n        :returns: The children.\n        :rtype: iterable(NodeNG)\n        \"\"\"\n        for expr, var in self.items:\n            yield expr\n            if var:\n                yield var\n        yield from self.body"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef postinit(self, value, conversion=None, format_spec=None):\n        self.value = value\n        self.conversion = conversion\n        self.format_spec = format_spec", "response": "Do some setup after initialisation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninfer a typing. TypeVar or typing. NewType call", "response": "def infer_typing_typevar_or_newtype(node, context=None):\n    \"\"\"Infer a typing.TypeVar(...) or typing.NewType(...) call\"\"\"\n    try:\n        func = next(node.func.infer(context=context))\n    except InferenceError as exc:\n        raise UseInferenceDefault from exc\n\n    if func.qname() not in TYPING_TYPEVARS_QUALIFIED:\n        raise UseInferenceDefault\n    if not node.args:\n        raise UseInferenceDefault\n\n    typename = node.args[0].as_string().strip(\"'\")\n    node = extract_node(TYPING_TYPE_TEMPLATE.format(typename))\n    return node.infer(context=context)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntries to figure out if a Name node might be a typing - related subscript", "response": "def _looks_like_typing_subscript(node):\n    \"\"\"Try to figure out if a Subscript node *might* be a typing-related subscript\"\"\"\n    if isinstance(node, nodes.Name):\n        return node.name in TYPING_MEMBERS\n    elif isinstance(node, nodes.Attribute):\n        return node.attrname in TYPING_MEMBERS\n    elif isinstance(node, nodes.Subscript):\n        return _looks_like_typing_subscript(node.value)\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninfer a typing. X [... subscript", "response": "def infer_typing_attr(node, context=None):\n    \"\"\"Infer a typing.X[...] subscript\"\"\"\n    try:\n        value = next(node.value.infer())\n    except InferenceError as exc:\n        raise UseInferenceDefault from exc\n\n    if not value.qname().startswith(\"typing.\"):\n        raise UseInferenceDefault\n\n    node = extract_node(TYPING_TYPE_TEMPLATE.format(value.qname().split(\".\")[-1]))\n    return node.infer(context=context)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild the stubs for functions classes constants methods etc.", "response": "def _gi_build_stub(parent):\n    \"\"\"\n    Inspect the passed module recursively and build stubs for functions,\n    classes, etc.\n    \"\"\"\n    classes = {}\n    functions = {}\n    constants = {}\n    methods = {}\n    for name in dir(parent):\n        if name.startswith(\"__\"):\n            continue\n\n        # Check if this is a valid name in python\n        if not re.match(_identifier_re, name):\n            continue\n\n        try:\n            obj = getattr(parent, name)\n        except:\n            continue\n\n        if inspect.isclass(obj):\n            classes[name] = obj\n        elif inspect.isfunction(obj) or inspect.isbuiltin(obj):\n            functions[name] = obj\n        elif inspect.ismethod(obj) or inspect.ismethoddescriptor(obj):\n            methods[name] = obj\n        elif (\n            str(obj).startswith(\"<flags\")\n            or str(obj).startswith(\"<enum \")\n            or str(obj).startswith(\"<GType \")\n            or inspect.isdatadescriptor(obj)\n        ):\n            constants[name] = 0\n        elif isinstance(obj, (int, str)):\n            constants[name] = obj\n        elif callable(obj):\n            # Fall back to a function for anything callable\n            functions[name] = obj\n        else:\n            # Assume everything else is some manner of constant\n            constants[name] = 0\n\n    ret = \"\"\n\n    if constants:\n        ret += \"# %s constants\\n\\n\" % parent.__name__\n    for name in sorted(constants):\n        if name[0].isdigit():\n            # GDK has some busted constant names like\n            # Gdk.EventType.2BUTTON_PRESS\n            continue\n\n        val = constants[name]\n\n        strval = str(val)\n        if isinstance(val, str):\n            strval = '\"%s\"' % str(val).replace(\"\\\\\", \"\\\\\\\\\")\n        ret += \"%s = %s\\n\" % (name, strval)\n\n    if ret:\n        ret += \"\\n\\n\"\n    if functions:\n        ret += \"# %s functions\\n\\n\" % parent.__name__\n    for name in sorted(functions):\n        ret += \"def %s(*args, **kwargs):\\n\" % name\n        ret += \"    pass\\n\"\n\n    if ret:\n        ret += \"\\n\\n\"\n    if methods:\n        ret += \"# %s methods\\n\\n\" % parent.__name__\n    for name in sorted(methods):\n        ret += \"def %s(self, *args, **kwargs):\\n\" % name\n        ret += \"    pass\\n\"\n\n    if ret:\n        ret += \"\\n\\n\"\n    if classes:\n        ret += \"# %s classes\\n\\n\" % parent.__name__\n    for name, obj in sorted(classes.items()):\n        base = \"object\"\n        if issubclass(obj, Exception):\n            base = \"Exception\"\n        ret += \"class %s(%s):\\n\" % (name, base)\n\n        classret = _gi_build_stub(obj)\n        if not classret:\n            classret = \"pass\\n\"\n\n        for line in classret.splitlines():\n            ret += \"    \" + line + \"\\n\"\n        ret += \"\\n\"\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef object_type(node, context=None):\n\n    try:\n        types = set(_object_type(node, context))\n    except exceptions.InferenceError:\n        return util.Uninferable\n    if len(types) > 1 or not types:\n        return util.Uninferable\n    return list(types)[0]", "response": "Obtain the type of the given node"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if a node is an instance of any node in class_or_seq.", "response": "def object_isinstance(node, class_or_seq, context=None):\n    \"\"\"Check if a node 'isinstance' any node in class_or_seq\n\n    :param node: A given node\n    :param class_or_seq: Union[nodes.NodeNG, Sequence[nodes.NodeNG]]\n    :rtype: bool\n\n    :raises AstroidTypeError: if the given ``classes_or_seq`` are not types\n    \"\"\"\n    obj_type = object_type(node, context)\n    if obj_type is util.Uninferable:\n        return util.Uninferable\n    return _object_type_is_subclass(obj_type, class_or_seq, context=context)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef object_issubclass(node, class_or_seq, context=None):\n    if not isinstance(node, nodes.ClassDef):\n        raise TypeError(\"{node} needs to be a ClassDef node\".format(node=node))\n    return _object_type_is_subclass(node, class_or_seq, context=context)", "response": "Check if a type is a subclass of any node in class_or_seq."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the inferred value for the given node.", "response": "def safe_infer(node, context=None):\n    \"\"\"Return the inferred value for the given node.\n\n    Return None if inference failed or if there is some ambiguity (more than\n    one node has been inferred).\n    \"\"\"\n    try:\n        inferit = node.infer(context=context)\n        value = next(inferit)\n    except exceptions.InferenceError:\n        return None\n    try:\n        next(inferit)\n        return None  # None if there is ambiguity on the inferred node\n    except exceptions.InferenceError:\n        return None  # there is some kind of ambiguity\n    except StopIteration:\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef has_known_bases(klass, context=None):\n    try:\n        return klass._all_bases_known\n    except AttributeError:\n        pass\n    for base in klass.bases:\n        result = safe_infer(base, context=context)\n        # TODO: check for A->B->A->B pattern in class structure too?\n        if (\n            not isinstance(result, scoped_nodes.ClassDef)\n            or result is klass\n            or not has_known_bases(result, context=context)\n        ):\n            klass._all_bases_known = False\n            return False\n    klass._all_bases_known = True\n    return True", "response": "Return True if all base classes of a class could be inferred."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the value as an index for the given instance.", "response": "def class_instance_as_index(node):\n    \"\"\"Get the value as an index for the given instance.\n\n    If an instance provides an __index__ method, then it can\n    be used in some scenarios where an integer is expected,\n    for instance when multiplying or subscripting a list.\n    \"\"\"\n    context = contextmod.InferenceContext()\n    context.callcontext = contextmod.CallContext(args=[node])\n\n    try:\n        for inferred in node.igetattr(\"__index__\", context=context):\n            if not isinstance(inferred, bases.BoundMethod):\n                continue\n\n            for result in inferred.infer_call_result(node, context=context):\n                if isinstance(result, nodes.Const) and isinstance(result.value, int):\n                    return result\n    except exceptions.InferenceError:\n        pass\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninferring length of given node object", "response": "def object_len(node, context=None):\n    \"\"\"Infer length of given node object\n\n    :param Union[nodes.ClassDef, nodes.Instance] node:\n    :param node: Node to infer length of\n\n    :raises AstroidTypeError: If an invalid node is returned\n        from __len__ method or no __len__ method exists\n    :raises InferenceError: If the given node cannot be inferred\n        or if multiple nodes are inferred\n    :rtype int: Integer length of node\n    \"\"\"\n    from astroid.objects import FrozenSet\n\n    inferred_node = safe_infer(node, context=context)\n    if inferred_node is None or inferred_node is util.Uninferable:\n        raise exceptions.InferenceError(node=node)\n    if isinstance(inferred_node, nodes.Const) and isinstance(\n        inferred_node.value, (bytes, str)\n    ):\n        return len(inferred_node.value)\n    if isinstance(inferred_node, (nodes.List, nodes.Set, nodes.Tuple, FrozenSet)):\n        return len(inferred_node.elts)\n    if isinstance(inferred_node, nodes.Dict):\n        return len(inferred_node.items)\n    try:\n        node_type = object_type(inferred_node, context=context)\n        len_call = next(node_type.igetattr(\"__len__\", context=context))\n    except exceptions.AttributeInferenceError:\n        raise exceptions.AstroidTypeError(\n            \"object of type '{}' has no len()\".format(len_call.pytype())\n        )\n\n    result_of_len = next(len_call.infer_call_result(node, context))\n    if (\n        isinstance(result_of_len, nodes.Const)\n        and result_of_len.pytype() == \"builtins.int\"\n    ):\n        return result_of_len.value\n    raise exceptions.AstroidTypeError(\n        \"'{}' object cannot be interpreted as an integer\".format(result_of_len)\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding prefix to the beginning of selected lines in text.", "response": "def _indent(text, prefix, predicate=None):\n    \"\"\"Adds 'prefix' to the beginning of selected lines in 'text'.\n\n    If 'predicate' is provided, 'prefix' will only be added to the lines\n    where 'predicate(line)' is True. If 'predicate' is not provided,\n    it will default to adding 'prefix' to all non-empty lines that do not\n    consist solely of whitespace characters.\n    \"\"\"\n    if predicate is None:\n        predicate = lambda line: line.strip()\n\n    def prefixed_lines():\n        for line in text.splitlines(True):\n            yield prefix + line if predicate(line) else line\n\n    return \"\".join(prefixed_lines())"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfixes six. moves imports due to dynamic nature of this .", "response": "def _six_fail_hook(modname):\n    \"\"\"Fix six.moves imports due to the dynamic nature of this\n    class.\n\n    Construct a pseudo-module which contains all the necessary imports\n    for six\n\n    :param modname: Name of failed module\n    :type modname: str\n\n    :return: An astroid module\n    :rtype: nodes.Module\n    \"\"\"\n\n    attribute_of = modname != \"six.moves\" and modname.startswith(\"six.moves\")\n    if modname != \"six.moves\" and not attribute_of:\n        raise AstroidBuildingError(modname=modname)\n    module = AstroidBuilder(MANAGER).string_build(_IMPORTS)\n    module.name = \"six.moves\"\n    if attribute_of:\n        # Facilitate import of submodules in Moves\n        start_index = len(module.name)\n        attribute = modname[start_index:].lstrip(\".\").replace(\".\", \"_\")\n        try:\n            import_attr = module.getattr(attribute)[0]\n        except AttributeInferenceError:\n            raise AstroidBuildingError(modname=modname)\n        if isinstance(import_attr, nodes.Import):\n            submodule = MANAGER.ast_from_module_name(import_attr.names[0][0])\n            return submodule\n    # Let dummy submodule imports pass through\n    # This will cause an Uninferable result, which is okay\n    return module"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if the given class node is decorated with *six. add_metaclass* and inject its argument as the metaclass of the underlying class.", "response": "def transform_six_add_metaclass(node):\n    \"\"\"Check if the given class node is decorated with *six.add_metaclass*\n\n    If so, inject its argument as the metaclass of the underlying class.\n    \"\"\"\n    if not node.decorators:\n        return\n\n    for decorator in node.decorators.nodes:\n        if not isinstance(decorator, nodes.Call):\n            continue\n\n        try:\n            func = next(decorator.func.infer())\n        except InferenceError:\n            continue\n        if func.qname() == SIX_ADD_METACLASS and decorator.args:\n            metaclass = decorator.args[0]\n            node._metaclass = metaclass\n            return node"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind a module spec for the given module or package.", "response": "def find_spec(modpath, path=None):\n    \"\"\"Find a spec for the given module.\n\n    :type modpath: list or tuple\n    :param modpath:\n      split module's name (i.e name of a module or package split\n      on '.'), with leading empty strings for explicit relative import\n\n    :type path: list or None\n    :param path:\n      optional list of path where the module or package should be\n      searched (use sys.path if nothing or None is given)\n\n    :rtype: ModuleSpec\n    :return: A module spec, which describes how the module was\n             found and where.\n    \"\"\"\n    _path = path or sys.path\n\n    # Need a copy for not mutating the argument.\n    modpath = modpath[:]\n\n    submodule_path = None\n    module_parts = modpath[:]\n    processed = []\n\n    while modpath:\n        modname = modpath.pop(0)\n        finder, spec = _find_spec_with_path(\n            _path, modname, module_parts, processed, submodule_path or path\n        )\n        processed.append(modname)\n        if modpath:\n            submodule_path = finder.contribute_to_path(spec, processed)\n\n        if spec.type == ModuleType.PKG_DIRECTORY:\n            spec = spec._replace(submodule_search_locations=submodule_path)\n\n    return spec"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the MRO which will be used to lookup attributes in this super class.", "response": "def super_mro(self):\n        \"\"\"Get the MRO which will be used to lookup attributes in this super.\"\"\"\n        if not isinstance(self.mro_pointer, scoped_nodes.ClassDef):\n            raise exceptions.SuperError(\n                \"The first argument to super must be a subtype of \"\n                \"type, not {mro_pointer}.\",\n                super_=self,\n            )\n\n        if isinstance(self.type, scoped_nodes.ClassDef):\n            # `super(type, type)`, most likely in a class method.\n            self._class_based = True\n            mro_type = self.type\n        else:\n            mro_type = getattr(self.type, \"_proxied\", None)\n            if not isinstance(mro_type, (bases.Instance, scoped_nodes.ClassDef)):\n                raise exceptions.SuperError(\n                    \"The second argument to super must be an \"\n                    \"instance or subtype of type, not {type}.\",\n                    super_=self,\n                )\n\n        if not mro_type.newstyle:\n            raise exceptions.SuperError(\n                \"Unable to call super on old-style classes.\", super_=self\n            )\n\n        mro = mro_type.mro()\n        if self.mro_pointer not in mro:\n            raise exceptions.SuperError(\n                \"The second argument to super must be an \"\n                \"instance or subtype of type, not {type}.\",\n                super_=self,\n            )\n\n        index = mro.index(self.mro_pointer)\n        return mro[index + 1 :]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef igetattr(self, name, context=None):\n\n        if name in self.special_attributes:\n            yield self.special_attributes.lookup(name)\n            return\n\n        try:\n            mro = self.super_mro()\n        # Don't let invalid MROs or invalid super calls\n        # leak out as is from this function.\n        except exceptions.SuperError as exc:\n            raise exceptions.AttributeInferenceError(\n                (\n                    \"Lookup for {name} on {target!r} because super call {super!r} \"\n                    \"is invalid.\"\n                ),\n                target=self,\n                attribute=name,\n                context=context,\n                super_=exc.super_,\n            ) from exc\n        except exceptions.MroError as exc:\n            raise exceptions.AttributeInferenceError(\n                (\n                    \"Lookup for {name} on {target!r} failed because {cls!r} has an \"\n                    \"invalid MRO.\"\n                ),\n                target=self,\n                attribute=name,\n                context=context,\n                mros=exc.mros,\n                cls=exc.cls,\n            ) from exc\n        found = False\n        for cls in mro:\n            if name not in cls.locals:\n                continue\n\n            found = True\n            for inferred in bases._infer_stmts([cls[name]], context, frame=self):\n                if not isinstance(inferred, scoped_nodes.FunctionDef):\n                    yield inferred\n                    continue\n\n                # We can obtain different descriptors from a super depending\n                # on what we are accessing and where the super call is.\n                if inferred.type == \"classmethod\":\n                    yield bases.BoundMethod(inferred, cls)\n                elif self._scope.type == \"classmethod\" and inferred.type == \"method\":\n                    yield inferred\n                elif self._class_based or inferred.type == \"staticmethod\":\n                    yield inferred\n                elif bases._is_property(inferred):\n                    # TODO: support other descriptors as well.\n                    try:\n                        yield from inferred.infer_call_result(self, context)\n                    except exceptions.InferenceError:\n                        yield util.Uninferable\n                else:\n                    yield bases.BoundMethod(inferred, cls)\n\n        if not found:\n            raise exceptions.AttributeInferenceError(\n                target=self, attribute=name, context=context\n            )", "response": "Retrieve the inferred values of the given attribute name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncall the transforms for the given node if any and return the the SetException transformed node.", "response": "def _transform(self, node):\n        \"\"\"Call matching transforms for the given node if any and return the\n        transformed node.\n        \"\"\"\n        cls = node.__class__\n        if cls not in self.transforms:\n            # no transform registered for this class of node\n            return node\n\n        transforms = self.transforms[cls]\n        for transform_func, predicate in transforms:\n            if predicate is None or predicate(node):\n                ret = transform_func(node)\n                # if the transformation function returns something, it's\n                # expected to be a replacement for the node\n                if ret is not None:\n                    node = ret\n                if ret.__class__ != cls:\n                    # Can no longer apply the rest of the transforms.\n                    break\n        return node"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef register_transform(self, node_class, transform, predicate=None):\n        self.transforms[node_class].append((transform, predicate))", "response": "Register a transform function to be applied on the given node_class astroid s node_class."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef visit(self, module):\n        module.body = [self._visit(child) for child in module.body]\n        return self._transform(module)", "response": "Walk the given astroid tree and transform each node in the tree."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _looks_like_lru_cache(node):\n    if not node.decorators:\n        return False\n    for decorator in node.decorators.nodes:\n        if not isinstance(decorator, astroid.Call):\n            continue\n        if _looks_like_functools_member(decorator, \"lru_cache\"):\n            return True\n    return False", "response": "Check if the given function node is decorated with lru_cache."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _looks_like_functools_member(node, member):\n    if isinstance(node.func, astroid.Name):\n        return node.func.name == member\n    elif isinstance(node.func, astroid.Attribute):\n        return (\n            node.func.attrname == member\n            and isinstance(node.func.expr, astroid.Name)\n            and node.func.expr.name == \"functools\"\n        )", "response": "Check if the given Call node is a functools. partial call"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nhandling block line numbers range for try finally for while orelse", "response": "def _elsed_block_range(self, lineno, orelse, last=None):\n        \"\"\"handle block line numbers range for try/finally, for, if and while\n        statements\n        \"\"\"\n        if lineno == self.fromlineno:\n            return lineno, lineno\n        if orelse:\n            if lineno >= orelse[0].fromlineno:\n                return lineno, orelse[-1].tolineno\n            return lineno, orelse[0].fromlineno - 1\n        return lineno, last or self.tolineno"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_filtered_stmts(self, _, node, _stmts, mystmt):\n        if self.statement() is mystmt:\n            # original node's statement is the assignment, only keep\n            # current node (gen exp, list comp)\n            return [node], True\n        return _stmts, False", "response": "method used in _filter_stmts to get statements and trigger break"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef do_import_module(self, modname=None):\n        # handle special case where we are on a package node importing a module\n        # using the same name as the package, which may end in an infinite loop\n        # on relative imports\n        # XXX: no more needed ?\n        mymodule = self.root()\n        level = getattr(self, \"level\", None)  # Import as no level\n        if modname is None:\n            modname = self.modname\n        # XXX we should investigate deeper if we really want to check\n        # importing itself: modname and mymodule.name be relative or absolute\n        if mymodule.relative_to_absolute_name(modname, level) == mymodule.name:\n            # FIXME: we used to raise InferenceError here, but why ?\n            return mymodule\n\n        return mymodule.import_module(\n            modname, level=level, relative_only=level and level >= 1\n        )", "response": "return the ast for a module whose name is modname imported by self"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets name from as name", "response": "def real_name(self, asname):\n        \"\"\"get name from 'as' name\"\"\"\n        for name, _asname in self.names:\n            if name == \"*\":\n                return asname\n            if not _asname:\n                name = name.split(\".\", 1)[0]\n                _asname = name\n            if asname == _asname:\n                return name\n        raise exceptions.AttributeInferenceError(\n            \"Could not find original name for {attribute} in {target!r}\",\n            target=self,\n            attribute=asname,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_function_type_comment(type_comment: str) -> Optional[FunctionType]:\n    if _ast_py3 is None:\n        return None\n\n    func_type = _ast_py3.parse(type_comment, \"<type_comment>\", \"func_type\")\n    return FunctionType(argtypes=func_type.argtypes, returns=func_type.returns)", "response": "Given a correct type comment obtain a FunctionType object"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_decorated_with_attrs(node, decorator_names=ATTRS_NAMES):\n    if not node.decorators:\n        return False\n    for decorator_attribute in node.decorators.nodes:\n        if isinstance(decorator_attribute, astroid.Call):  # decorator with arguments\n            decorator_attribute = decorator_attribute.func\n        if decorator_attribute.as_string() in decorator_names:\n            return True\n    return False", "response": "Return True if a decorated node has\n    an attr decorator applied."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive that the ClassNode has an attr decorator transform class attributes as instance attributes", "response": "def attr_attributes_transform(node):\n    \"\"\"Given that the ClassNode has an attr decorator,\n    rewrite class attributes as instance attributes\n    \"\"\"\n    # Astroid can't infer this attribute properly\n    # Prevents https://github.com/PyCQA/pylint/issues/1884\n    node.locals[\"__attrs_attrs__\"] = [astroid.Unknown(parent=node)]\n\n    for cdefbodynode in node.body:\n        if not isinstance(cdefbodynode, astroid.Assign):\n            continue\n        if isinstance(cdefbodynode.value, astroid.Call):\n            if cdefbodynode.value.func.as_string() not in ATTRIB_NAMES:\n                continue\n        else:\n            continue\n        for target in cdefbodynode.targets:\n\n            rhs_node = astroid.Unknown(\n                lineno=cdefbodynode.lineno,\n                col_offset=cdefbodynode.col_offset,\n                parent=cdefbodynode,\n            )\n            node.locals[target.name] = [rhs_node]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a Proxy from the given name to the given node type.", "response": "def proxy_alias(alias_name, node_type):\n    \"\"\"Get a Proxy from the given name to the given node type.\"\"\"\n    proxy = type(\n        alias_name,\n        (lazy_object_proxy.Proxy,),\n        {\n            \"__class__\": object.__dict__[\"__class__\"],\n            \"__instancecheck__\": _instancecheck,\n        },\n    )\n    return proxy(lambda: node_type)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlimit inference amount. Limit inference amount to help with performance issues with exponentially exploding possible results. :param iterator: Inference generator to limit :type iterator: Iterator(NodeNG) :param size: Maximum mount of nodes yielded plus an Uninferable at the end if limit reached :type size: int :yields: A possibly modified generator :rtype param: Iterable", "response": "def limit_inference(iterator, size):\n    \"\"\"Limit inference amount.\n\n    Limit inference amount to help with performance issues with\n    exponentially exploding possible results.\n\n    :param iterator: Inference generator to limit\n    :type iterator: Iterator(NodeNG)\n\n    :param size: Maximum mount of nodes yielded plus an\n        Uninferable at the end if limit reached\n    :type size: int\n\n    :yields: A possibly modified generator\n    :rtype param: Iterable\n    \"\"\"\n    yield from islice(iterator, size)\n    has_more = next(iterator, False)\n    if has_more is not False:\n        yield Uninferable\n        return"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _c3_merge(sequences, cls, context):\n    result = []\n    while True:\n        sequences = [s for s in sequences if s]  # purge empty sequences\n        if not sequences:\n            return result\n        for s1 in sequences:  # find merge candidates among seq heads\n            candidate = s1[0]\n            for s2 in sequences:\n                if candidate in s2[1:]:\n                    candidate = None\n                    break  # reject the current head, it appears later\n            else:\n                break\n        if not candidate:\n            # Show all the remaining bases, which were considered as\n            # candidates for the next mro sequence.\n            raise exceptions.InconsistentMroError(\n                message=\"Cannot create a consistent method resolution order \"\n                \"for MROs {mros} of class {cls!r}.\",\n                mros=sequences,\n                cls=cls,\n                context=context,\n            )\n\n        result.append(candidate)\n        # remove the chosen candidate\n        for seq in sequences:\n            if seq[0] == candidate:\n                del seq[0]\n    return None", "response": "Merges MROs in sequences into a single MRO."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef builtin_lookup(name):\n    builtin_astroid = MANAGER.ast_from_module(builtins)\n    if name == \"__dict__\":\n        return builtin_astroid, ()\n    try:\n        stmts = builtin_astroid.locals[name]\n    except KeyError:\n        stmts = ()\n    return builtin_astroid, stmts", "response": "lookup a name into the builtin module\n    return the list of matching statements and the astroid for the builtin module\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndetecting the decorator call chaining and see if the end result is a classmethod or a staticmethod.", "response": "def _infer_decorator_callchain(node):\n    \"\"\"Detect decorator call chaining and see if the end result is a\n    static or a classmethod.\n    \"\"\"\n    if not isinstance(node, FunctionDef):\n        return None\n    if not node.parent:\n        return None\n    try:\n        result = next(node.infer_call_result(node.parent))\n    except exceptions.InferenceError:\n        return None\n    if isinstance(result, bases.Instance):\n        result = result._proxied\n    if isinstance(result, ClassDef):\n        if result.is_subtype_of(\"%s.classmethod\" % BUILTINS):\n            return \"classmethod\"\n        if result.is_subtype_of(\"%s.staticmethod\" % BUILTINS):\n            return \"staticmethod\"\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of all argument names", "response": "def _rec_get_names(args, names=None):\n    \"\"\"return a list of all argument names\"\"\"\n    if names is None:\n        names = []\n    for arg in args:\n        if isinstance(arg, node_classes.Tuple):\n            _rec_get_names(arg.elts, names)\n        else:\n            names.append(arg.name)\n    return names"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _is_metaclass(klass, seen=None):\n    if klass.name == \"type\":\n        return True\n    if seen is None:\n        seen = set()\n    for base in klass.bases:\n        try:\n            for baseobj in base.infer():\n                baseobj_name = baseobj.qname()\n                if baseobj_name in seen:\n                    continue\n                else:\n                    seen.add(baseobj_name)\n                if isinstance(baseobj, bases.Instance):\n                    # not abstract\n                    return False\n                if baseobj is util.Uninferable:\n                    continue\n                if baseobj is klass:\n                    continue\n                if not isinstance(baseobj, ClassDef):\n                    continue\n                if baseobj._type == \"metaclass\":\n                    return True\n                if _is_metaclass(baseobj, seen):\n                    return True\n        except exceptions.InferenceError:\n            continue\n    return False", "response": "Return True if the given class can be used as a metaclass."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a ClassDef node type to differ metaclass and exception from regular classes", "response": "def _class_type(klass, ancestors=None):\n    \"\"\"return a ClassDef node type to differ metaclass and exception\n    from 'regular' classes\n    \"\"\"\n    # XXX we have to store ancestors in case we have an ancestor loop\n    if klass._type is not None:\n        return klass._type\n    if _is_metaclass(klass):\n        klass._type = \"metaclass\"\n    elif klass.name.endswith(\"Exception\"):\n        klass._type = \"exception\"\n    else:\n        if ancestors is None:\n            ancestors = set()\n        klass_name = klass.qname()\n        if klass_name in ancestors:\n            # XXX we are in loop ancestors, and have found no type\n            klass._type = \"class\"\n            return \"class\"\n        ancestors.add(klass_name)\n        for base in klass.ancestors(recurs=False):\n            name = _class_type(base, ancestors)\n            if name != \"class\":\n                if name == \"metaclass\" and not _is_metaclass(klass):\n                    # don't propagate it if the current class\n                    # can't be a metaclass\n                    continue\n                klass._type = base.type\n                break\n    if klass._type is None:\n        klass._type = \"class\"\n    return klass._type"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the class that wraps the given node.", "response": "def get_wrapping_class(node):\n    \"\"\"Get the class that wraps the given node.\n\n    We consider that a class wraps a node if the class\n    is a parent for the said node.\n\n    :returns: The class that wraps the given node\n    :rtype: ClassDef or None\n    \"\"\"\n\n    klass = node.frame()\n    while klass is not None and not isinstance(klass, ClassDef):\n        if klass.parent is None:\n            klass = None\n        else:\n            klass = klass.parent.frame()\n    return klass"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the qualified name of the node.", "response": "def qname(self):\n        \"\"\"Get the 'qualified' name of the node.\n\n        For example: module.name, module.class.name ...\n\n        :returns: The qualified name.\n        :rtype: str\n        \"\"\"\n        # pylint: disable=no-member; github.com/pycqa/astroid/issues/278\n        if self.parent is None:\n            return self.name\n        return \"%s.%s\" % (self.parent.frame().qname(), self.name)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _scope_lookup(self, node, name, offset=0):\n        try:\n            stmts = node._filter_stmts(self.locals[name], self, offset)\n        except KeyError:\n            stmts = ()\n        if stmts:\n            return self, stmts\n        if self.parent:  # i.e. not Module\n            # nested scope: if parent scope is a function, that's fine\n            # else jump to the module\n            pscope = self.parent.scope()\n            if not pscope.is_function:\n                pscope = pscope.root()\n            return pscope.scope_lookup(node, name)\n        return builtin_lookup(name)", "response": "XXX method for interfacing the scope lookup"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_local(self, name, stmt):\n        # assert not stmt in self.locals.get(name, ()), (self, stmt)\n        self.locals.setdefault(name, []).append(stmt)", "response": "Define that the given name is declared in the given statement node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _append_node(self, child):\n        # pylint: disable=no-member; depending by the class\n        # which uses the current class as a mixin or base class.\n        # It's rewritten in 2.0, so it makes no sense for now\n        # to spend development time on it.\n        self.body.append(child)\n        child.parent = self", "response": "append a child node linking it in the tree"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_local_node(self, child_node, name=None):\n        if name != \"__class__\":\n            # add __class__ node as a child will cause infinite recursion later!\n            self._append_node(child_node)\n        self.set_local(name or child_node.name, child_node)", "response": "Append a child that will alter the locals of this scope node."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef scope_lookup(self, node, name, offset=0):\n        if name in self.scope_attrs and name not in self.locals:\n            try:\n                return self, self.getattr(name)\n            except exceptions.AttributeInferenceError:\n                return self, ()\n        return self._scope_lookup(node, name, offset)", "response": "Lookup where the given variable is assigned."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef import_module(self, modname, relative_only=False, level=None):\n        if relative_only and level is None:\n            level = 0\n        absmodname = self.relative_to_absolute_name(modname, level)\n\n        try:\n            return MANAGER.ast_from_module_name(absmodname)\n        except exceptions.AstroidBuildingError:\n            # we only want to import a sub module or package of this module,\n            # skip here\n            if relative_only:\n                raise\n        return MANAGER.ast_from_module_name(modname)", "response": "Get the ast for a given module."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert the module name to the absolute name for a relative import.", "response": "def relative_to_absolute_name(self, modname, level):\n        \"\"\"Get the absolute module name for a relative import.\n\n        The relative import can be implicit or explicit.\n\n        :param modname: The module name to convert.\n        :type modname: str\n\n        :param level: The level of relative import.\n        :type level: int\n\n        :returns: The absolute module name.\n        :rtype: str\n\n        :raises TooManyLevelsError: When the relative import refers to a\n            module too far above this one.\n        \"\"\"\n        # XXX this returns non sens when called on an absolute import\n        # like 'pylint.checkers.astroid.utils'\n        # XXX doesn't return absolute name if self.name isn't absolute name\n        if self.absolute_import_activated() and level is None:\n            return modname\n        if level:\n            if self.package:\n                level = level - 1\n            if level and self.name.count(\".\") < level:\n                raise exceptions.TooManyLevelsError(level=level, name=self.name)\n\n            package_name = self.name.rsplit(\".\", level)[0]\n        elif self.package:\n            package_name = self.name\n        else:\n            package_name = self.name.rsplit(\".\", 1)[0]\n\n        if package_name:\n            if not modname:\n                return package_name\n            return \"%s.%s\" % (package_name, modname)\n        return modname"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the list of imported names when this module is wildcard imported.", "response": "def wildcard_import_names(self):\n        \"\"\"The list of imported names when this module is 'wildcard imported'.\n\n        It doesn't include the '__builtins__' name which is added by the\n        current CPython implementation of wildcard imports.\n\n        :returns: The list of imported names.\n        :rtype: list(str)\n        \"\"\"\n        # We separate the different steps of lookup in try/excepts\n        # to avoid catching too many Exceptions\n        default = [name for name in self.keys() if not name.startswith(\"_\")]\n        try:\n            all_values = self[\"__all__\"]\n        except KeyError:\n            return default\n\n        try:\n            explicit = next(all_values.assigned_stmts())\n        except exceptions.InferenceError:\n            return default\n        except AttributeError:\n            # not an assignment node\n            # XXX infer?\n            return default\n\n        # Try our best to detect the exported name.\n        inferred = []\n        try:\n            explicit = next(explicit.infer())\n        except exceptions.InferenceError:\n            return default\n        if not isinstance(explicit, (node_classes.Tuple, node_classes.List)):\n            return default\n\n        str_const = lambda node: (\n            isinstance(node, node_classes.Const) and isinstance(node.value, str)\n        )\n        for node in explicit.elts:\n            if str_const(node):\n                inferred.append(node.value)\n            else:\n                try:\n                    inferred_node = next(node.infer())\n                except exceptions.InferenceError:\n                    continue\n                if str_const(inferred_node):\n                    inferred.append(inferred_node.value)\n        return inferred"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndoes some setup after initialisation.", "response": "def postinit(self, key=None, value=None, generators=None):\n        \"\"\"Do some setup after initialisation.\n\n        :param key: What produces the keys.\n        :type key: NodeNG or None\n\n        :param value: What produces the values.\n        :type value: NodeNG or None\n\n        :param generators: The generators that are looped through.\n        :type generators: list(Comprehension) or None\n        \"\"\"\n        self.key = key\n        self.value = value\n        if generators is None:\n            self.generators = []\n        else:\n            self.generators = generators"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndoes some setup after initialisation.", "response": "def postinit(self, elt=None, generators=None):\n        \"\"\"Do some setup after initialisation.\n\n        :param elt: The element that forms the output of the expression.\n        :type elt: NodeNG or None\n\n        :param generators: The generators that are looped through.\n        :type generators: list(Comprehension) or None\n        \"\"\"\n        self.elt = elt\n        self.generators = generators"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndoing some setup after initialization.", "response": "def postinit(self, args, body):\n        \"\"\"Do some setup after initialisation.\n\n        :param args: The arguments that the function takes.\n        :type args: Arguments\n\n        :param body: The contents of the function body.\n        :type body: list(NodeNG)\n        \"\"\"\n        self.args = args\n        self.body = body"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the names of each of the arguments.", "response": "def argnames(self):\n        \"\"\"Get the names of each of the arguments.\n\n        :returns: The names of the arguments.\n        :rtype: list(str)\n        \"\"\"\n        # pylint: disable=no-member; github.com/pycqa/astroid/issues/291\n        # args is in fact redefined later on by postinit. Can't be changed\n        # to None due to a strong interaction between Lambda and FunctionDef.\n\n        if self.args.args:  # maybe None with builtin functions\n            names = _rec_get_names(self.args.args)\n        else:\n            names = []\n        if self.args.vararg:\n            names.append(self.args.vararg)\n        if self.args.kwarg:\n            names.append(self.args.kwarg)\n        return names"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef postinit(\n        self,\n        args,\n        body,\n        decorators=None,\n        returns=None,\n        type_comment_returns=None,\n        type_comment_args=None,\n    ):\n        \"\"\"Do some setup after initialisation.\n\n        :param args: The arguments that the function takes.\n        :type args: Arguments or list\n\n        :param body: The contents of the function body.\n        :type body: list(NodeNG)\n\n        :param decorators: The decorators that are applied to this\n            method or function.\n        :type decorators: Decorators or None\n        :params type_comment_returns:\n            The return type annotation passed via a type comment.\n        :params type_comment_args:\n            The args type annotation passed via a type comment.\n        \"\"\"\n        self.args = args\n        self.body = body\n        self.decorators = decorators\n        self.returns = returns\n        self.type_comment_returns = type_comment_returns\n        self.type_comment_args = type_comment_args\n\n        if isinstance(self.parent.frame(), ClassDef):\n            self.set_local(\"__class__\", self.parent.frame())", "response": "Do some setup after initialization of the object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the extra decorators that this function can have.", "response": "def extra_decorators(self):\n        \"\"\"The extra decorators that this function can have.\n\n        Additional decorators are considered when they are used as\n        assignments, as in ``method = staticmethod(method)``.\n        The property will return all the callables that are used for\n        decoration.\n\n        :type: list(NodeNG)\n        \"\"\"\n        frame = self.parent.frame()\n        if not isinstance(frame, ClassDef):\n            return []\n\n        decorators = []\n        for assign in frame._get_assign_nodes():\n            if isinstance(assign.value, node_classes.Call) and isinstance(\n                assign.value.func, node_classes.Name\n            ):\n                for assign_node in assign.targets:\n                    if not isinstance(assign_node, node_classes.AssignName):\n                        # Support only `name = callable(name)`\n                        continue\n\n                    if assign_node.name != self.name:\n                        # Interested only in the assignment nodes that\n                        # decorates the current method.\n                        continue\n                    try:\n                        meth = frame[self.name]\n                    except KeyError:\n                        continue\n                    else:\n                        # Must be a function and in the same frame as the\n                        # original method.\n                        if (\n                            isinstance(meth, FunctionDef)\n                            and assign_node.frame() == frame\n                        ):\n                            decorators.append(assign.value)\n        return decorators"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fromlineno(self):\n        # lineno is the line number of the first decorator, we want the def\n        # statement lineno\n        lineno = self.lineno\n        if self.decorators is not None:\n            lineno += sum(\n                node.tolineno - node.lineno + 1 for node in self.decorators.nodes\n            )\n\n        return lineno", "response": "Returns the line number of this node in the source code."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getattr(self, name, context=None):\n        if name in self.instance_attrs:\n            return self.instance_attrs[name]\n        if name in self.special_attributes:\n            return [self.special_attributes.lookup(name)]\n        raise exceptions.AttributeInferenceError(target=self, attribute=name)", "response": "getattr - returns the value of the attribute with the given name"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninfers getattr which returns an iterator of inferred statements.", "response": "def igetattr(self, name, context=None):\n        \"\"\"Inferred getattr, which returns an iterator of inferred statements.\"\"\"\n        try:\n            return bases._infer_stmts(self.getattr(name, context), context, frame=self)\n        except exceptions.AttributeInferenceError as error:\n            raise exceptions.InferenceError(\n                error.message, target=self, attribute=name, context=context\n            ) from error"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the qualified names of each of the decorators on this function.", "response": "def decoratornames(self):\n        \"\"\"Get the qualified names of each of the decorators on this function.\n\n        :returns: The names of the decorators.\n        :rtype: set(str)\n        \"\"\"\n        result = set()\n        decoratornodes = []\n        if self.decorators is not None:\n            decoratornodes += self.decorators.nodes\n        decoratornodes += self.extra_decorators\n        for decnode in decoratornodes:\n            try:\n                for infnode in decnode.infer():\n                    result.add(infnode.qname())\n            except exceptions.InferenceError:\n                continue\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_abstract(self, pass_is_abstract=True):\n        if self.decorators:\n            for node in self.decorators.nodes:\n                try:\n                    inferred = next(node.infer())\n                except exceptions.InferenceError:\n                    continue\n                if inferred and inferred.qname() in (\n                    \"abc.abstractproperty\",\n                    \"abc.abstractmethod\",\n                ):\n                    return True\n\n        for child_node in self.body:\n            if isinstance(child_node, node_classes.Raise):\n                if child_node.raises_not_implemented():\n                    return True\n            return pass_is_abstract and isinstance(child_node, node_classes.Pass)\n        # empty function is the same as function with a single \"pass\" statement\n        if pass_is_abstract:\n            return True", "response": "Checks if the method is abstract."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef infer_call_result(self, caller=None, context=None):\n        if self.is_generator():\n            if isinstance(self, AsyncFunctionDef):\n                generator_cls = bases.AsyncGenerator\n            else:\n                generator_cls = bases.Generator\n            result = generator_cls(self)\n            yield result\n            return\n        # This is really a gigantic hack to work around metaclass generators\n        # that return transient class-generating functions. Pylint's AST structure\n        # cannot handle a base class object that is only used for calling __new__,\n        # but does not contribute to the inheritance structure itself. We inject\n        # a fake class into the hierarchy here for several well-known metaclass\n        # generators, and filter it out later.\n        if (\n            self.name == \"with_metaclass\"\n            and len(self.args.args) == 1\n            and self.args.vararg is not None\n        ):\n            metaclass = next(caller.args[0].infer(context))\n            if isinstance(metaclass, ClassDef):\n                class_bases = [next(arg.infer(context)) for arg in caller.args[1:]]\n                new_class = ClassDef(name=\"temporary_class\")\n                new_class.hide = True\n                new_class.parent = self\n                new_class.postinit(\n                    bases=[base for base in class_bases if base != util.Uninferable],\n                    body=[],\n                    decorators=[],\n                    metaclass=metaclass,\n                )\n                yield new_class\n                return\n        returns = self._get_return_nodes_skip_functions()\n\n        first_return = next(returns, None)\n        if not first_return:\n            raise exceptions.InferenceError(\"Empty return iterator\")\n\n        for returnnode in itertools.chain((first_return,), returns):\n            if returnnode.value is None:\n                yield node_classes.Const(None)\n            else:\n                try:\n                    yield from returnnode.value.infer(context)\n                except exceptions.InferenceError:\n                    yield util.Uninferable", "response": "Infer what the function returns when called."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef implicit_locals(self):\n        locals_ = ((\"__module__\", self.special_attributes.attr___module__),)\n        if sys.version_info >= (3, 3):\n            # __qualname__ is defined in PEP3155\n            locals_ += ((\"__qualname__\", self.special_attributes.attr___qualname__),)\n        return locals_", "response": "Get implicit class definition locals."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef postinit(\n        self, bases, body, decorators, newstyle=None, metaclass=None, keywords=None\n    ):\n        \"\"\"Do some setup after initialisation.\n\n        :param bases: What the class inherits from.\n        :type bases: list(NodeNG)\n\n        :param body: The contents of the class body.\n        :type body: list(NodeNG)\n\n        :param decorators: The decorators that are applied to this class.\n        :type decorators: Decorators or None\n\n        :param newstyle: Whether this is a new style class or not.\n        :type newstyle: bool or None\n\n        :param metaclass: The metaclass of this class.\n        :type metaclass: NodeNG or None\n\n        :param keywords: The keywords given to the class definition.\n        :type keywords: list(Keyword) or None\n        \"\"\"\n        self.keywords = keywords\n        self.bases = bases\n        self.body = body\n        self.decorators = decorators\n        if newstyle is not None:\n            self._newstyle = newstyle\n        if metaclass is not None:\n            self._metaclass = metaclass", "response": "Do some setup after initialization."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef infer_call_result(self, caller, context=None):\n        if (\n            self.is_subtype_of(\"%s.type\" % (BUILTINS,), context)\n            and len(caller.args) == 3\n        ):\n            result = self._infer_type_call(caller, context)\n            yield result\n            return\n\n        dunder_call = None\n        try:\n            metaclass = self.metaclass(context=context)\n            if metaclass is not None:\n                dunder_call = next(metaclass.igetattr(\"__call__\", context))\n        except exceptions.AttributeInferenceError:\n            pass\n        if dunder_call and dunder_call.qname() != \"builtins.type.__call__\":\n            context = contextmod.bind_context_to_node(context, self)\n            yield from dunder_call.infer_call_result(caller, context)\n        else:\n            # Call type.__call__ if not set metaclass\n            # (since type is the default metaclass)\n            yield bases.Instance(self)", "response": "infer what a class is returning when called"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef scope_lookup(self, node, name, offset=0):\n        # If the name looks like a builtin name, just try to look\n        # into the upper scope of this class. We might have a\n        # decorator that it's poorly named after a builtin object\n        # inside this class.\n        lookup_upper_frame = (\n            isinstance(node.parent, node_classes.Decorators)\n            and name in MANAGER.builtins_module\n        )\n        if (\n            any(node == base or base.parent_of(node) for base in self.bases)\n            or lookup_upper_frame\n        ):\n            # Handle the case where we have either a name\n            # in the bases of a class, which exists before\n            # the actual definition or the case where we have\n            # a Getattr node, with that name.\n            #\n            # name = ...\n            # class A(name):\n            #     def name(self): ...\n            #\n            # import name\n            # class A(name.Name):\n            #     def name(self): ...\n\n            frame = self.parent.frame()\n            # line offset to avoid that class A(A) resolve the ancestor to\n            # the defined class\n            offset = -1\n        else:\n            frame = self\n        return frame._scope_lookup(node, name, offset)", "response": "Look up the assignments associated to the given name in the current scope."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\niterate over the base classes in prefixed depth first order.", "response": "def ancestors(self, recurs=True, context=None):\n        \"\"\"Iterate over the base classes in prefixed depth first order.\n\n        :param recurs: Whether to recurse or return direct ancestors only.\n        :type recurs: bool\n\n        :returns: The base classes\n        :rtype: iterable(NodeNG)\n        \"\"\"\n        # FIXME: should be possible to choose the resolution order\n        # FIXME: inference make infinite loops possible here\n        yielded = {self}\n        if context is None:\n            context = contextmod.InferenceContext()\n        if not self.bases and self.qname() != \"builtins.object\":\n            yield builtin_lookup(\"object\")[1][0]\n            return\n\n        for stmt in self.bases:\n            with context.restore_path():\n                try:\n                    for baseobj in stmt.infer(context):\n                        if not isinstance(baseobj, ClassDef):\n                            if isinstance(baseobj, bases.Instance):\n                                baseobj = baseobj._proxied\n                            else:\n                                continue\n                        if not baseobj.hide:\n                            if baseobj in yielded:\n                                continue\n                            yielded.add(baseobj)\n                            yield baseobj\n                        if not recurs:\n                            continue\n                        for grandpa in baseobj.ancestors(recurs=True, context=context):\n                            if grandpa is self:\n                                # This class is the ancestor of itself.\n                                break\n                            if grandpa in yielded:\n                                continue\n                            yielded.add(grandpa)\n                            yield grandpa\n                except exceptions.InferenceError:\n                    continue"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef local_attr_ancestors(self, name, context=None):\n        # Look up in the mro if we can. This will result in the\n        # attribute being looked up just as Python does it.\n        try:\n            ancestors = self.mro(context)[1:]\n        except exceptions.MroError:\n            # Fallback to use ancestors, we can't determine\n            # a sane MRO.\n            ancestors = self.ancestors(context=context)\n        for astroid in ancestors:\n            if name in astroid:\n                yield astroid", "response": "Iterate over the parents that define the given name."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef instance_attr_ancestors(self, name, context=None):\n        for astroid in self.ancestors(context=context):\n            if name in astroid.instance_attrs:\n                yield astroid", "response": "Iterate over the parents that define the given name as an instance attribute."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the list of assign nodes associated to the given name.", "response": "def local_attr(self, name, context=None):\n        \"\"\"Get the list of assign nodes associated to the given name.\n\n        Assignments are looked for in both this class and in parents.\n\n        :returns: The list of assignments to the given name.\n        :rtype: list(NodeNG)\n\n        :raises AttributeInferenceError: If no attribute with this name\n            can be found in this class or parent classes.\n        \"\"\"\n        result = []\n        if name in self.locals:\n            result = self.locals[name]\n        else:\n            class_node = next(self.local_attr_ancestors(name, context), None)\n            if class_node:\n                result = class_node.locals[name]\n        result = [n for n in result if not isinstance(n, node_classes.DelAttr)]\n        if result:\n            return result\n        raise exceptions.AttributeInferenceError(\n            target=self, attribute=name, context=context\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef instance_attr(self, name, context=None):\n        # Return a copy, so we don't modify self.instance_attrs,\n        # which could lead to infinite loop.\n        values = list(self.instance_attrs.get(name, []))\n        # get all values from parents\n        for class_node in self.instance_attr_ancestors(name, context):\n            values += class_node.instance_attrs[name]\n        values = [n for n in values if not isinstance(n, node_classes.DelAttr)]\n        if values:\n            return values\n        raise exceptions.AttributeInferenceError(\n            target=self, attribute=name, context=context\n        )", "response": "Returns the list of nodes associated to the given attribute name."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getattr(self, name, context=None, class_context=True):\n        values = self.locals.get(name, [])\n        if name in self.special_attributes and class_context and not values:\n            result = [self.special_attributes.lookup(name)]\n            if name == \"__bases__\":\n                # Need special treatment, since they are mutable\n                # and we need to return all the values.\n                result += values\n            return result\n\n        # don't modify the list in self.locals!\n        values = list(values)\n        for classnode in self.ancestors(recurs=True, context=context):\n            values += classnode.locals.get(name, [])\n\n        if class_context:\n            values += self._metaclass_lookup_attribute(name, context)\n\n        if not values:\n            raise exceptions.AttributeInferenceError(\n                target=self, attribute=name, context=context\n            )\n\n        # Look for AnnAssigns, which are not attributes in the purest sense.\n        for value in values:\n            if isinstance(value, node_classes.AssignName):\n                stmt = value.statement()\n                if isinstance(stmt, node_classes.AnnAssign) and stmt.value is None:\n                    raise exceptions.AttributeInferenceError(\n                        target=self, attribute=name, context=context\n                    )\n        return values", "response": "Get an attribute from this class using Python s attribute semantic."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _metaclass_lookup_attribute(self, name, context):\n        attrs = set()\n        implicit_meta = self.implicit_metaclass()\n        metaclass = self.metaclass()\n        for cls in {implicit_meta, metaclass}:\n            if cls and cls != self and isinstance(cls, ClassDef):\n                cls_attributes = self._get_attribute_from_metaclass(cls, name, context)\n                attrs.update(set(cls_attributes))\n        return attrs", "response": "Search the given name in the implicit and the explicit metaclass."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninfers the possible values of the given variable.", "response": "def igetattr(self, name, context=None, class_context=True):\n        \"\"\"Infer the possible values of the given variable.\n\n        :param name: The name of the variable to infer.\n        :type name: str\n\n        :returns: The inferred possible values.\n        :rtype: iterable(NodeNG or Uninferable)\n        \"\"\"\n        # set lookup name since this is necessary to infer on import nodes for\n        # instance\n        context = contextmod.copy_context(context)\n        context.lookupname = name\n        try:\n            attr = self.getattr(name, context, class_context=class_context)[0]\n            for inferred in bases._infer_stmts([attr], context, frame=self):\n                # yield Uninferable object instead of descriptors when necessary\n                if not isinstance(inferred, node_classes.Const) and isinstance(\n                    inferred, bases.Instance\n                ):\n                    try:\n                        inferred._proxied.getattr(\"__get__\", context)\n                    except exceptions.AttributeInferenceError:\n                        yield inferred\n                    else:\n                        yield util.Uninferable\n                else:\n                    yield function_to_method(inferred, self)\n        except exceptions.AttributeInferenceError as error:\n            if not name.startswith(\"__\") and self.has_dynamic_getattr(context):\n                # class handle some dynamic attributes, return a Uninferable object\n                yield util.Uninferable\n            else:\n                raise exceptions.InferenceError(\n                    error.message, target=self, attribute=name, context=context\n                )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if the class has a custom __getattr__ or __getattribute__ method.", "response": "def has_dynamic_getattr(self, context=None):\n        \"\"\"Check if the class has a custom __getattr__ or __getattribute__.\n\n        If any such method is found and it is not from\n        builtins, nor from an extension module, then the function\n        will return True.\n\n        :returns: True if the class has a custom\n            __getattr__ or __getattribute__, False otherwise.\n        :rtype: bool\n        \"\"\"\n\n        def _valid_getattr(node):\n            root = node.root()\n            return root.name != BUILTINS and getattr(root, \"pure_python\", None)\n\n        try:\n            return _valid_getattr(self.getattr(\"__getattr__\", context)[0])\n        except exceptions.AttributeInferenceError:\n            # if self.newstyle: XXX cause an infinite recursion error\n            try:\n                getattribute = self.getattr(\"__getattribute__\", context)[0]\n                return _valid_getattr(getattribute)\n            except exceptions.AttributeInferenceError:\n                pass\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the inference of a subscript.", "response": "def getitem(self, index, context=None):\n        \"\"\"Return the inference of a subscript.\n\n        This is basically looking up the method in the metaclass and calling it.\n\n        :returns: The inferred value of a subscript to this class.\n        :rtype: NodeNG\n\n        :raises AstroidTypeError: If this class does not define a\n            ``__getitem__`` method.\n        \"\"\"\n        try:\n            methods = dunder_lookup.lookup(self, \"__getitem__\")\n        except exceptions.AttributeInferenceError as exc:\n            raise exceptions.AstroidTypeError(node=self, context=context) from exc\n\n        method = methods[0]\n\n        # Create a new callcontext for providing index as an argument.\n        new_context = contextmod.bind_context_to_node(context, self)\n        new_context.callcontext = contextmod.CallContext(args=[index])\n\n        try:\n            return next(method.infer_call_result(self, new_context))\n        except exceptions.InferenceError:\n            return util.Uninferable"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\niterating over all of the methods defined in this class and its parents.", "response": "def methods(self):\n        \"\"\"Iterate over all of the method defined in this class and its parents.\n\n        :returns: The methods defined on the class.\n        :rtype: iterable(FunctionDef)\n        \"\"\"\n        done = {}\n        for astroid in itertools.chain(iter((self,)), self.ancestors()):\n            for meth in astroid.mymethods():\n                if meth.name in done:\n                    continue\n                done[meth.name] = None\n                yield meth"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the explicit declared metaclass for the current class.", "response": "def declared_metaclass(self, context=None):\n        \"\"\"Return the explicit declared metaclass for the current class.\n\n        An explicit declared metaclass is defined\n        either by passing the ``metaclass`` keyword argument\n        in the class definition line (Python 3) or (Python 2) by\n        having a ``__metaclass__`` class attribute, or if there are\n        no explicit bases but there is a global ``__metaclass__`` variable.\n\n        :returns: The metaclass of this class,\n            or None if one could not be found.\n        :rtype: NodeNG or None\n        \"\"\"\n        for base in self.bases:\n            try:\n                for baseobj in base.infer(context=context):\n                    if isinstance(baseobj, ClassDef) and baseobj.hide:\n                        self._metaclass = baseobj._metaclass\n                        self._metaclass_hack = True\n                        break\n            except exceptions.InferenceError:\n                pass\n\n        if self._metaclass:\n            # Expects this from Py3k TreeRebuilder\n            try:\n                return next(\n                    node\n                    for node in self._metaclass.infer(context=context)\n                    if node is not util.Uninferable\n                )\n            except (exceptions.InferenceError, StopIteration):\n                return None\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn an iterator with the inferred slots.", "response": "def _islots(self):\n        \"\"\" Return an iterator with the inferred slots. \"\"\"\n        if \"__slots__\" not in self.locals:\n            return None\n        for slots in self.igetattr(\"__slots__\"):\n            # check if __slots__ is a valid type\n            for meth in ITER_METHODS:\n                try:\n                    slots.getattr(meth)\n                    break\n                except exceptions.AttributeInferenceError:\n                    continue\n            else:\n                continue\n\n            if isinstance(slots, node_classes.Const):\n                # a string. Ignore the following checks,\n                # but yield the node, only if it has a value\n                if slots.value:\n                    yield slots\n                continue\n            if not hasattr(slots, \"itered\"):\n                # we can't obtain the values, maybe a .deque?\n                continue\n\n            if isinstance(slots, node_classes.Dict):\n                values = [item[0] for item in slots.items]\n            else:\n                values = slots.itered()\n            if values is util.Uninferable:\n                continue\n            if not values:\n                # Stop the iteration, because the class\n                # has an empty list of slots.\n                return values\n\n            for elt in values:\n                try:\n                    for inferred in elt.infer():\n                        if inferred is util.Uninferable:\n                            continue\n                        if not isinstance(\n                            inferred, node_classes.Const\n                        ) or not isinstance(inferred.value, str):\n                            continue\n                        if not inferred.value:\n                            continue\n                        yield inferred\n                except exceptions.InferenceError:\n                    continue\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef slots(self):\n\n        def grouped_slots():\n            # Not interested in object, since it can't have slots.\n            for cls in self.mro()[:-1]:\n                try:\n                    cls_slots = cls._slots()\n                except NotImplementedError:\n                    continue\n                if cls_slots is not None:\n                    yield from cls_slots\n                else:\n                    yield None\n\n        if not self.newstyle:\n            raise NotImplementedError(\n                \"The concept of slots is undefined for old-style classes.\"\n            )\n\n        slots = list(grouped_slots())\n        if not all(slot is not None for slot in slots):\n            return None\n\n        return sorted(slots, key=lambda item: item.value)", "response": "Get all the slots for this class."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nextend builtin str class", "response": "def _extend_str(class_node, rvalue):\n    \"\"\"function to extend builtin str/unicode class\"\"\"\n    code = dedent(\n        \"\"\"\n    class whatever(object):\n        def join(self, iterable):\n            return {rvalue}\n        def replace(self, old, new, count=None):\n            return {rvalue}\n        def format(self, *args, **kwargs):\n            return {rvalue}\n        def encode(self, encoding='ascii', errors=None):\n            return ''\n        def decode(self, encoding='ascii', errors=None):\n            return u''\n        def capitalize(self):\n            return {rvalue}\n        def title(self):\n            return {rvalue}\n        def lower(self):\n            return {rvalue}\n        def upper(self):\n            return {rvalue}\n        def swapcase(self):\n            return {rvalue}\n        def index(self, sub, start=None, end=None):\n            return 0\n        def find(self, sub, start=None, end=None):\n            return 0\n        def count(self, sub, start=None, end=None):\n            return 0\n        def strip(self, chars=None):\n            return {rvalue}\n        def lstrip(self, chars=None):\n            return {rvalue}\n        def rstrip(self, chars=None):\n            return {rvalue}\n        def rjust(self, width, fillchar=None):\n            return {rvalue}\n        def center(self, width, fillchar=None):\n            return {rvalue}\n        def ljust(self, width, fillchar=None):\n            return {rvalue}\n    \"\"\"\n    )\n    code = code.format(rvalue=rvalue)\n    fake = AstroidBuilder(MANAGER).string_build(code)[\"whatever\"]\n    for method in fake.mymethods():\n        method.parent = class_node\n        method.lineno = None\n        method.col_offset = None\n        if \"__class__\" in method.locals:\n            method.locals[\"__class__\"] = [class_node]\n        class_node.locals[method.name] = [method]\n        method.parent = class_node"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef register_builtin_transform(transform, builtin_name):\n\n    def _transform_wrapper(node, context=None):\n        result = transform(node, context=context)\n        if result:\n            if not result.parent:\n                # Let the transformation function determine\n                # the parent for its result. Otherwise,\n                # we set it to be the node we transformed from.\n                result.parent = node\n\n            if result.lineno is None:\n                result.lineno = node.lineno\n            if result.col_offset is None:\n                result.col_offset = node.col_offset\n        return iter([result])\n\n    MANAGER.register_transform(\n        nodes.Call,\n        inference_tip(_transform_wrapper),\n        partial(_builtin_filter_predicate, builtin_name=builtin_name),\n    )", "response": "Register a new transform function for the given builtin_name."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef infer_dict(node, context=None):\n    call = arguments.CallSite.from_call(node)\n    if call.has_invalid_arguments() or call.has_invalid_keywords():\n        raise UseInferenceDefault\n\n    args = call.positional_arguments\n    kwargs = list(call.keyword_arguments.items())\n\n    if not args and not kwargs:\n        # dict()\n        return nodes.Dict()\n    elif kwargs and not args:\n        # dict(a=1, b=2, c=4)\n        items = [(nodes.Const(key), value) for key, value in kwargs]\n    elif len(args) == 1 and kwargs:\n        # dict(some_iterable, b=2, c=4)\n        elts = _get_elts(args[0], context)\n        keys = [(nodes.Const(key), value) for key, value in kwargs]\n        items = elts + keys\n    elif len(args) == 1:\n        items = _get_elts(args[0], context)\n    else:\n        raise UseInferenceDefault()\n\n    value = nodes.Dict(\n        col_offset=node.col_offset, lineno=node.lineno, parent=node.parent\n    )\n    value.postinit(items)\n    return value", "response": "Try to infer a Dict node."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef infer_super(node, context=None):\n    if len(node.args) == 1:\n        # Ignore unbounded super.\n        raise UseInferenceDefault\n\n    scope = node.scope()\n    if not isinstance(scope, nodes.FunctionDef):\n        # Ignore non-method uses of super.\n        raise UseInferenceDefault\n    if scope.type not in (\"classmethod\", \"method\"):\n        # Not interested in staticmethods.\n        raise UseInferenceDefault\n\n    cls = scoped_nodes.get_wrapping_class(scope)\n    if not len(node.args):\n        mro_pointer = cls\n        # In we are in a classmethod, the interpreter will fill\n        # automatically the class as the second argument, not an instance.\n        if scope.type == \"classmethod\":\n            mro_type = cls\n        else:\n            mro_type = cls.instantiate_class()\n    else:\n        try:\n            mro_pointer = next(node.args[0].infer(context=context))\n        except InferenceError:\n            raise UseInferenceDefault\n        try:\n            mro_type = next(node.args[1].infer(context=context))\n        except InferenceError:\n            raise UseInferenceDefault\n\n    if mro_pointer is util.Uninferable or mro_type is util.Uninferable:\n        # No way we could understand this.\n        raise UseInferenceDefault\n\n    super_obj = objects.Super(\n        mro_pointer=mro_pointer, mro_type=mro_type, self_class=cls, scope=scope\n    )\n    super_obj.parent = node\n    return super_obj", "response": "Understand super calls.\n\n    There are some restrictions for what can be understood:\n\n        * unbounded super (one argument form) is not understood.\n\n        * if the super call is not inside a function (classmethod or method),\n          then the default inference will be used.\n\n        * if the super arguments can't be inferred, the default inference\n          will be used."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef infer_getattr(node, context=None):\n    obj, attr = _infer_getattr_args(node, context)\n    if (\n        obj is util.Uninferable\n        or attr is util.Uninferable\n        or not hasattr(obj, \"igetattr\")\n    ):\n        return util.Uninferable\n\n    try:\n        return next(obj.igetattr(attr, context=context))\n    except (StopIteration, InferenceError, AttributeInferenceError):\n        if len(node.args) == 3:\n            # Try to infer the default and return it instead.\n            try:\n                return next(node.args[2].infer(context=context))\n            except InferenceError:\n                raise UseInferenceDefault\n\n    raise UseInferenceDefault", "response": "Understand getattr calls\n\n    If one of the arguments is an Uninferable object, then the\n    result will be an Uninferable object. Otherwise, the normal attribute\n    lookup will be done."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef infer_hasattr(node, context=None):\n    try:\n        obj, attr = _infer_getattr_args(node, context)\n        if (\n            obj is util.Uninferable\n            or attr is util.Uninferable\n            or not hasattr(obj, \"getattr\")\n        ):\n            return util.Uninferable\n        obj.getattr(attr, context=context)\n    except UseInferenceDefault:\n        # Can't infer something from this function call.\n        return util.Uninferable\n    except AttributeInferenceError:\n        # Doesn't have it.\n        return nodes.Const(False)\n    return nodes.Const(True)", "response": "Understand hasattr calls\n\n    This always guarantees three possible outcomes for calling\n    hasattr: Const(False) when we are sure that the object\n    doesn't have the intended attribute, Const(True) when\n    we know that the object has the attribute and Uninferable\n    when we are unsure of the outcome of the function call."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef infer_callable(node, context=None):\n    if len(node.args) != 1:\n        # Invalid callable call.\n        raise UseInferenceDefault\n\n    argument = node.args[0]\n    try:\n        inferred = next(argument.infer(context=context))\n    except InferenceError:\n        return util.Uninferable\n    if inferred is util.Uninferable:\n        return util.Uninferable\n    return nodes.Const(inferred.callable())", "response": "Understand callable calls\n\n    This follows Python's semantics, where an object\n    is callable if it provides an attribute __call__,\n    even though that attribute is something which can't be\n    called."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nunderstanding the one - argument form of type *.", "response": "def infer_type(node, context=None):\n    \"\"\"Understand the one-argument form of *type*.\"\"\"\n    if len(node.args) != 1:\n        raise UseInferenceDefault\n\n    return helpers.object_type(node.args[0], context)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninfers issubclass from a call node.", "response": "def infer_issubclass(callnode, context=None):\n    \"\"\"Infer issubclass() calls\n\n    :param nodes.Call callnode: an `issubclass` call\n    :param InferenceContext: the context for the inference\n    :rtype nodes.Const: Boolean Const value of the `issubclass` call\n    :raises UseInferenceDefault: If the node cannot be inferred\n    \"\"\"\n    call = arguments.CallSite.from_call(callnode)\n    if call.keyword_arguments:\n        # issubclass doesn't support keyword arguments\n        raise UseInferenceDefault(\"TypeError: issubclass() takes no keyword arguments\")\n    if len(call.positional_arguments) != 2:\n        raise UseInferenceDefault(\n            \"Expected two arguments, got {count}\".format(\n                count=len(call.positional_arguments)\n            )\n        )\n    # The left hand argument is the obj to be checked\n    obj_node, class_or_tuple_node = call.positional_arguments\n\n    try:\n        obj_type = next(obj_node.infer(context=context))\n    except InferenceError as exc:\n        raise UseInferenceDefault from exc\n    if not isinstance(obj_type, nodes.ClassDef):\n        raise UseInferenceDefault(\"TypeError: arg 1 must be class\")\n\n    # The right hand argument is the class(es) that the given\n    # object is to be checked against.\n    try:\n        class_container = _class_or_tuple_to_container(\n            class_or_tuple_node, context=context\n        )\n    except InferenceError as exc:\n        raise UseInferenceDefault from exc\n    try:\n        issubclass_bool = helpers.object_issubclass(obj_type, class_container, context)\n    except AstroidTypeError as exc:\n        raise UseInferenceDefault(\"TypeError: \" + str(exc)) from exc\n    except MroError as exc:\n        raise UseInferenceDefault from exc\n    return nodes.Const(issubclass_bool)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef infer_isinstance(callnode, context=None):\n    call = arguments.CallSite.from_call(callnode)\n    if call.keyword_arguments:\n        # isinstance doesn't support keyword arguments\n        raise UseInferenceDefault(\"TypeError: isinstance() takes no keyword arguments\")\n    if len(call.positional_arguments) != 2:\n        raise UseInferenceDefault(\n            \"Expected two arguments, got {count}\".format(\n                count=len(call.positional_arguments)\n            )\n        )\n    # The left hand argument is the obj to be checked\n    obj_node, class_or_tuple_node = call.positional_arguments\n    # The right hand argument is the class(es) that the given\n    # obj is to be check is an instance of\n    try:\n        class_container = _class_or_tuple_to_container(\n            class_or_tuple_node, context=context\n        )\n    except InferenceError:\n        raise UseInferenceDefault\n    try:\n        isinstance_bool = helpers.object_isinstance(obj_node, class_container, context)\n    except AstroidTypeError as exc:\n        raise UseInferenceDefault(\"TypeError: \" + str(exc))\n    except MroError as exc:\n        raise UseInferenceDefault from exc\n    if isinstance_bool is util.Uninferable:\n        raise UseInferenceDefault\n    return nodes.Const(isinstance_bool)", "response": "Infer the isinstance of a node"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef infer_len(node, context=None):\n    call = arguments.CallSite.from_call(node)\n    if call.keyword_arguments:\n        raise UseInferenceDefault(\"TypeError: len() must take no keyword arguments\")\n    if len(call.positional_arguments) != 1:\n        raise UseInferenceDefault(\n            \"TypeError: len() must take exactly one argument \"\n            \"({len}) given\".format(len=len(call.positional_arguments))\n        )\n    [argument_node] = call.positional_arguments\n    try:\n        return nodes.Const(helpers.object_len(argument_node, context=context))\n    except (AstroidTypeError, InferenceError) as exc:\n        raise UseInferenceDefault(str(exc)) from exc", "response": "Infer the length of a node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninferring a string from a call site", "response": "def infer_str(node, context=None):\n    \"\"\"Infer str() calls\n\n    :param nodes.Call node: str() call to infer\n    :param context.InferenceContext: node context\n    :rtype nodes.Const: a Const containing an empty string\n    \"\"\"\n    call = arguments.CallSite.from_call(node)\n    if call.keyword_arguments:\n        raise UseInferenceDefault(\"TypeError: str() must take no keyword arguments\")\n    try:\n        return nodes.Const(\"\")\n    except (AstroidTypeError, InferenceError) as exc:\n        raise UseInferenceDefault(str(exc)) from exc"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef infer_int(node, context=None):\n    call = arguments.CallSite.from_call(node)\n    if call.keyword_arguments:\n        raise UseInferenceDefault(\"TypeError: int() must take no keyword arguments\")\n\n    if call.positional_arguments:\n        try:\n            first_value = next(call.positional_arguments[0].infer(context=context))\n        except InferenceError as exc:\n            raise UseInferenceDefault(str(exc)) from exc\n\n        if first_value is util.Uninferable:\n            raise UseInferenceDefault\n\n        if isinstance(first_value, nodes.Const) and isinstance(\n            first_value.value, (int, str)\n        ):\n            try:\n                actual_value = int(first_value.value)\n            except ValueError:\n                return nodes.Const(0)\n            return nodes.Const(actual_value)\n\n    return nodes.Const(0)", "response": "Infer the integer value of the node"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef infer_dict_fromkeys(node, context=None):\n\n    def _build_dict_with_elements(elements):\n        new_node = nodes.Dict(\n            col_offset=node.col_offset, lineno=node.lineno, parent=node.parent\n        )\n        new_node.postinit(elements)\n        return new_node\n\n    call = arguments.CallSite.from_call(node)\n    if call.keyword_arguments:\n        raise UseInferenceDefault(\"TypeError: int() must take no keyword arguments\")\n    if len(call.positional_arguments) not in {1, 2}:\n        raise UseInferenceDefault(\n            \"TypeError: Needs between 1 and 2 positional arguments\"\n        )\n\n    default = nodes.Const(None)\n    values = call.positional_arguments[0]\n    try:\n        inferred_values = next(values.infer(context=context))\n    except InferenceError:\n        return _build_dict_with_elements([])\n    if inferred_values is util.Uninferable:\n        return _build_dict_with_elements([])\n\n    # Limit to a couple of potential values, as this can become pretty complicated\n    accepted_iterable_elements = (nodes.Const,)\n    if isinstance(inferred_values, (nodes.List, nodes.Set, nodes.Tuple)):\n        elements = inferred_values.elts\n        for element in elements:\n            if not isinstance(element, accepted_iterable_elements):\n                # Fallback to an empty dict\n                return _build_dict_with_elements([])\n\n        elements_with_value = [(element, default) for element in elements]\n        return _build_dict_with_elements(elements_with_value)\n\n    elif isinstance(inferred_values, nodes.Const) and isinstance(\n        inferred_values.value, (str, bytes)\n    ):\n        elements = [\n            (nodes.Const(element), default) for element in inferred_values.value\n        ]\n        return _build_dict_with_elements(elements)\n    elif isinstance(inferred_values, nodes.Dict):\n        keys = inferred_values.itered()\n        for key in keys:\n            if not isinstance(key, accepted_iterable_elements):\n                # Fallback to an empty dict\n                return _build_dict_with_elements([])\n\n        elements_with_value = [(element, default) for element in keys]\n        return _build_dict_with_elements(elements_with_value)\n\n    # Fallback to an empty dictionary\n    return _build_dict_with_elements([])", "response": "Infer the dictionary from keys."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _import_string(names):\n    _names = []\n    for name, asname in names:\n        if asname is not None:\n            _names.append(\"%s as %s\" % (name, asname))\n        else:\n            _names.append(name)\n    return \", \".join(_names)", "response": "return a list of ( name asname ) formatted as a string"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of nodes to string", "response": "def _stmt_list(self, stmts, indent=True):\n        \"\"\"return a list of nodes to string\"\"\"\n        stmts = \"\\n\".join(nstr for nstr in [n.accept(self) for n in stmts] if nstr)\n        if indent:\n            return self.indent + stmts.replace(\"\\n\", \"\\n\" + self.indent)\n\n        return stmts"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwrapping child in parens only if required to keep same semantics", "response": "def _precedence_parens(self, node, child, is_left=True):\n        \"\"\"Wrap child in parens only if required to keep same semantics\"\"\"\n        if self._should_wrap(node, child, is_left):\n            return \"(%s)\" % child.accept(self)\n\n        return child.accept(self)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn True if the child should be wrapped.", "response": "def _should_wrap(self, node, child, is_left):\n        \"\"\"Wrap child if:\n            - it has lower precedence\n            - same precedence with position opposite to associativity direction\n        \"\"\"\n        node_precedence = node.op_precedence()\n        child_precedence = child.op_precedence()\n\n        if node_precedence > child_precedence:\n            # 3 * (4 + 5)\n            return True\n\n        if (\n            node_precedence == child_precedence\n            and is_left != node.op_left_associative()\n        ):\n            # 3 - (4 - 5)\n            # (2**3)**4\n            return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef visit_assert(self, node):\n        if node.fail:\n            return \"assert %s, %s\" % (node.test.accept(self), node.fail.accept(self))\n        return \"assert %s\" % node.test.accept(self)", "response": "return an astroid. Assert node as string"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef visit_assign(self, node):\n        lhs = \" = \".join(n.accept(self) for n in node.targets)\n        return \"%s = %s\" % (lhs, node.value.accept(self))", "response": "return an astroid. Assign node as string"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns an astroid. AugAssign node as string", "response": "def visit_augassign(self, node):\n        \"\"\"return an astroid.AugAssign node as string\"\"\"\n        return \"%s %s %s\" % (node.target.accept(self), node.op, node.value.accept(self))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef visit_annassign(self, node):\n\n        target = node.target.accept(self)\n        annotation = node.annotation.accept(self)\n        if node.value is None:\n            return \"%s: %s\" % (target, annotation)\n        return \"%s: %s = %s\" % (target, annotation, node.value.accept(self))", "response": "Return an astroid. AugAssign node as string"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nvisit a binop node by returning a string representation of it", "response": "def visit_binop(self, node):\n        \"\"\"return an astroid.BinOp node as string\"\"\"\n        left = self._precedence_parens(node, node.left)\n        right = self._precedence_parens(node, node.right, is_left=False)\n        if node.op == \"**\":\n            return \"%s%s%s\" % (left, node.op, right)\n\n        return \"%s %s %s\" % (left, node.op, right)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn an astroid. BoolOp node as string", "response": "def visit_boolop(self, node):\n        \"\"\"return an astroid.BoolOp node as string\"\"\"\n        values = [\"%s\" % self._precedence_parens(node, n) for n in node.values]\n        return (\" %s \" % node.op).join(values)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning an astroid. Call node as string", "response": "def visit_call(self, node):\n        \"\"\"return an astroid.Call node as string\"\"\"\n        expr_str = self._precedence_parens(node, node.func)\n        args = [arg.accept(self) for arg in node.args]\n        if node.keywords:\n            keywords = [kwarg.accept(self) for kwarg in node.keywords]\n        else:\n            keywords = []\n\n        args.extend(keywords)\n        return \"%s(%s)\" % (expr_str, \", \".join(args))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef visit_classdef(self, node):\n        decorate = node.decorators.accept(self) if node.decorators else \"\"\n        bases = \", \".join(n.accept(self) for n in node.bases)\n        metaclass = node.metaclass()\n        if metaclass and not node.has_metaclass_hack():\n            if bases:\n                bases = \"(%s, metaclass=%s)\" % (bases, metaclass.name)\n            else:\n                bases = \"(metaclass=%s)\" % metaclass.name\n        else:\n            bases = \"(%s)\" % bases if bases else \"\"\n        docs = self._docs_dedent(node.doc) if node.doc else \"\"\n        return \"\\n\\n%sclass %s%s:%s\\n%s\\n\" % (\n            decorate,\n            node.name,\n            bases,\n            docs,\n            self._stmt_list(node.body),\n        )", "response": "return an astroid. ClassDef node as string"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef visit_compare(self, node):\n        rhs_str = \" \".join(\n            [\n                \"%s %s\" % (op, self._precedence_parens(node, expr, is_left=False))\n                for op, expr in node.ops\n            ]\n        )\n        return \"%s %s\" % (self._precedence_parens(node, node.left), rhs_str)", "response": "return an astroid. Compare node as string"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn an astroid. Comprehension node as string", "response": "def visit_comprehension(self, node):\n        \"\"\"return an astroid.Comprehension node as string\"\"\"\n        ifs = \"\".join(\" if %s\" % n.accept(self) for n in node.ifs)\n        return \"for %s in %s%s\" % (\n            node.target.accept(self),\n            node.iter.accept(self),\n            ifs,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef visit_delete(self, node):  # XXX check if correct\n        return \"del %s\" % \", \".join(child.accept(self) for child in node.targets)", "response": "return an astroid. Delete node as string"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn an astroid. Decorators node as string", "response": "def visit_decorators(self, node):\n        \"\"\"return an astroid.Decorators node as string\"\"\"\n        return \"@%s\\n\" % \"\\n@\".join(item.accept(self) for item in node.nodes)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns an astroid. DictComp node as string", "response": "def visit_dictcomp(self, node):\n        \"\"\"return an astroid.DictComp node as string\"\"\"\n        return \"{%s: %s %s}\" % (\n            node.key.accept(self),\n            node.value.accept(self),\n            \" \".join(n.accept(self) for n in node.generators),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef visit_exec(self, node):\n        if node.locals:\n            return \"exec %s in %s, %s\" % (\n                node.expr.accept(self),\n                node.locals.accept(self),\n                node.globals.accept(self),\n            )\n        if node.globals:\n            return \"exec %s in %s\" % (node.expr.accept(self), node.globals.accept(self))\n        return \"exec %s\" % node.expr.accept(self)", "response": "return an astroid. Exec node as string"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef visit_extslice(self, node):\n        return \", \".join(dim.accept(self) for dim in node.dims)", "response": "return an astroid. ExtSlice node as string"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn an astroid. For node as string", "response": "def visit_for(self, node):\n        \"\"\"return an astroid.For node as string\"\"\"\n        fors = \"for %s in %s:\\n%s\" % (\n            node.target.accept(self),\n            node.iter.accept(self),\n            self._stmt_list(node.body),\n        )\n        if node.orelse:\n            fors = \"%s\\nelse:\\n%s\" % (fors, self._stmt_list(node.orelse))\n        return fors"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns an astroid. ImportFrom node as string", "response": "def visit_importfrom(self, node):\n        \"\"\"return an astroid.ImportFrom node as string\"\"\"\n        return \"from %s import %s\" % (\n            \".\" * (node.level or 0) + node.modname,\n            _import_string(node.names),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef visit_functiondef(self, node):\n        decorate = node.decorators.accept(self) if node.decorators else \"\"\n        docs = self._docs_dedent(node.doc) if node.doc else \"\"\n        trailer = \":\"\n        if node.returns:\n            return_annotation = \"->\" + node.returns.as_string()\n            trailer = return_annotation + \":\"\n        def_format = \"\\n%sdef %s(%s)%s%s\\n%s\"\n        return def_format % (\n            decorate,\n            node.name,\n            node.args.accept(self),\n            trailer,\n            docs,\n            self._stmt_list(node.body),\n        )", "response": "return an astroid. Function node as string"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn an astroid. GeneratorExp node as string", "response": "def visit_generatorexp(self, node):\n        \"\"\"return an astroid.GeneratorExp node as string\"\"\"\n        return \"(%s %s)\" % (\n            node.elt.accept(self),\n            \" \".join(n.accept(self) for n in node.generators),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn an astroid. Attr node as string", "response": "def visit_attribute(self, node):\n        \"\"\"return an astroid.Getattr node as string\"\"\"\n        return \"%s.%s\" % (self._precedence_parens(node, node.expr), node.attrname)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn an astroid. If node as string", "response": "def visit_if(self, node):\n        \"\"\"return an astroid.If node as string\"\"\"\n        ifs = [\"if %s:\\n%s\" % (node.test.accept(self), self._stmt_list(node.body))]\n        if node.has_elif_block():\n            ifs.append(\"el%s\" % self._stmt_list(node.orelse, indent=False))\n        elif node.orelse:\n            ifs.append(\"else:\\n%s\" % self._stmt_list(node.orelse))\n        return \"\\n\".join(ifs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning an astroid. IfExp node as string", "response": "def visit_ifexp(self, node):\n        \"\"\"return an astroid.IfExp node as string\"\"\"\n        return \"%s if %s else %s\" % (\n            self._precedence_parens(node, node.body, is_left=True),\n            self._precedence_parens(node, node.test, is_left=True),\n            self._precedence_parens(node, node.orelse, is_left=False),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns an astroid. Keyword node as string", "response": "def visit_keyword(self, node):\n        \"\"\"return an astroid.Keyword node as string\"\"\"\n        if node.arg is None:\n            return \"**%s\" % node.value.accept(self)\n        return \"%s=%s\" % (node.arg, node.value.accept(self))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef visit_lambda(self, node):\n        args = node.args.accept(self)\n        body = node.body.accept(self)\n        if args:\n            return \"lambda %s: %s\" % (args, body)\n\n        return \"lambda: %s\" % body", "response": "return an astroid. Lambda node as string"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns an astroid. List node as string", "response": "def visit_list(self, node):\n        \"\"\"return an astroid.List node as string\"\"\"\n        return \"[%s]\" % \", \".join(child.accept(self) for child in node.elts)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn an astroid. ListComp node as string", "response": "def visit_listcomp(self, node):\n        \"\"\"return an astroid.ListComp node as string\"\"\"\n        return \"[%s %s]\" % (\n            node.elt.accept(self),\n            \" \".join(n.accept(self) for n in node.generators),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning an astroid. Module node as string", "response": "def visit_module(self, node):\n        \"\"\"return an astroid.Module node as string\"\"\"\n        docs = '\"\"\"%s\"\"\"\\n\\n' % node.doc if node.doc else \"\"\n        return docs + \"\\n\".join(n.accept(self) for n in node.body) + \"\\n\\n\""}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning an astroid. Print node as string", "response": "def visit_print(self, node):\n        \"\"\"return an astroid.Print node as string\"\"\"\n        nodes = \", \".join(n.accept(self) for n in node.values)\n        if not node.nl:\n            nodes = \"%s,\" % nodes\n        if node.dest:\n            return \"print >> %s, %s\" % (node.dest.accept(self), nodes)\n        return \"print %s\" % nodes"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef visit_raise(self, node):\n        if node.exc:\n            if node.inst:\n                if node.tback:\n                    return \"raise %s, %s, %s\" % (\n                        node.exc.accept(self),\n                        node.inst.accept(self),\n                        node.tback.accept(self),\n                    )\n                return \"raise %s, %s\" % (node.exc.accept(self), node.inst.accept(self))\n            return \"raise %s\" % node.exc.accept(self)\n        return \"raise\"", "response": "return an astroid. Raise node as string"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn an astroid. Return node as string", "response": "def visit_return(self, node):\n        \"\"\"return an astroid.Return node as string\"\"\"\n        if node.is_tuple_return() and len(node.value.elts) > 1:\n            elts = [child.accept(self) for child in node.value.elts]\n            return \"return %s\" % \", \".join(elts)\n\n        if node.value:\n            return \"return %s\" % node.value.accept(self)\n\n        return \"return\""}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef visit_set(self, node):\n        return \"{%s}\" % \", \".join(child.accept(self) for child in node.elts)", "response": "return an astroid. Set node as string"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef visit_setcomp(self, node):\n        return \"{%s %s}\" % (\n            node.elt.accept(self),\n            \" \".join(n.accept(self) for n in node.generators),\n        )", "response": "return an astroid. SetComp node as string"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef visit_slice(self, node):\n        lower = node.lower.accept(self) if node.lower else \"\"\n        upper = node.upper.accept(self) if node.upper else \"\"\n        step = node.step.accept(self) if node.step else \"\"\n        if step:\n            return \"%s:%s:%s\" % (lower, upper, step)\n        return \"%s:%s\" % (lower, upper)", "response": "return an astroid. Slice node as string"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef visit_subscript(self, node):\n        idx = node.slice\n        if idx.__class__.__name__.lower() == \"index\":\n            idx = idx.value\n        idxstr = idx.accept(self)\n        if idx.__class__.__name__.lower() == \"tuple\" and idx.elts:\n            # Remove parenthesis in tuple and extended slice.\n            # a[(::1, 1:)] is not valid syntax.\n            idxstr = idxstr[1:-1]\n        return \"%s[%s]\" % (self._precedence_parens(node, node.value), idxstr)", "response": "return an astroid. Subscript node as string"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef visit_tryexcept(self, node):\n        trys = [\"try:\\n%s\" % self._stmt_list(node.body)]\n        for handler in node.handlers:\n            trys.append(handler.accept(self))\n        if node.orelse:\n            trys.append(\"else:\\n%s\" % self._stmt_list(node.orelse))\n        return \"\\n\".join(trys)", "response": "return an astroid. TryExcept node as string"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef visit_tryfinally(self, node):\n        return \"try:\\n%s\\nfinally:\\n%s\" % (\n            self._stmt_list(node.body),\n            self._stmt_list(node.finalbody),\n        )", "response": "return an astroid. TryFinally node as string"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef visit_tuple(self, node):\n        if len(node.elts) == 1:\n            return \"(%s, )\" % node.elts[0].accept(self)\n        return \"(%s)\" % \", \".join(child.accept(self) for child in node.elts)", "response": "return an astroid. Tuple node as string"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef visit_unaryop(self, node):\n        if node.op == \"not\":\n            operator = \"not \"\n        else:\n            operator = node.op\n        return \"%s%s\" % (operator, self._precedence_parens(node, node.operand))", "response": "return an astroid. UnaryOp node as string"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn an astroid. While node as string", "response": "def visit_while(self, node):\n        \"\"\"return an astroid.While node as string\"\"\"\n        whiles = \"while %s:\\n%s\" % (node.test.accept(self), self._stmt_list(node.body))\n        if node.orelse:\n            whiles = \"%s\\nelse:\\n%s\" % (whiles, self._stmt_list(node.orelse))\n        return whiles"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef visit_with(self, node):  # 'with' without 'as' is possible\n        items = \", \".join(\n            (\"%s\" % expr.accept(self)) + (vars and \" as %s\" % (vars.accept(self)) or \"\")\n            for expr, vars in node.items\n        )\n        return \"with %s:\\n%s\" % (items, self._stmt_list(node.body))", "response": "return an astroid. With node as string"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef visit_raise(self, node):\n        if node.exc:\n            if node.cause:\n                return \"raise %s from %s\" % (\n                    node.exc.accept(self),\n                    node.cause.accept(self),\n                )\n            return \"raise %s\" % node.exc.accept(self)\n        return \"raise\"", "response": "return an astroid. Raise node as string"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns an astroid. YieldFrom node as string.", "response": "def visit_yieldfrom(self, node):\n        \"\"\" Return an astroid.YieldFrom node as string. \"\"\"\n        yi_val = (\" \" + node.value.accept(self)) if node.value else \"\"\n        expr = \"yield from\" + yi_val\n        if node.parent.is_statement:\n            return expr\n\n        return \"(%s)\" % (expr,)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn an astroid. Comprehension node as string", "response": "def visit_comprehension(self, node):\n        \"\"\"return an astroid.Comprehension node as string\"\"\"\n        return \"%s%s\" % (\n            \"async \" if node.is_async else \"\",\n            super(AsStringVisitor3, self).visit_comprehension(node),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bind_context_to_node(context, node):\n    context = copy_context(context)\n    context.boundnode = node\n    return context", "response": "Bind a context to a node that can be used to retrieve the correct function name or attribute value of a particular node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\npush a node into the inference path", "response": "def push(self, node):\n        \"\"\"Push node into inference path\n\n        :return: True if node is already in context path else False\n        :rtype: bool\n\n        Allows one to see if the given node has already\n        been looked at for this inference context\"\"\"\n        name = self.lookupname\n        if (node, name) in self.path:\n            return True\n\n        self.path.add((node, name))\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nclones inference path For example, each side of a binary operation (BinOp) starts with the same context but diverge as each side is inferred so the InferenceContext will need be cloned", "response": "def clone(self):\n        \"\"\"Clone inference path\n\n        For example, each side of a binary operation (BinOp)\n        starts with the same context but diverge as each side is inferred\n        so the InferenceContext will need be cloned\"\"\"\n        # XXX copy lookupname/callcontext ?\n        clone = InferenceContext(self.path, inferred=self.inferred)\n        clone.callcontext = self.callcontext\n        clone.boundnode = self.boundnode\n        clone.extra_context = self.extra_context\n        return clone"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncaches result of generator into dictionary Used to cache inference results", "response": "def cache_generator(self, key, generator):\n        \"\"\"Cache result of generator into dictionary\n\n        Used to cache inference results\"\"\"\n        results = []\n        for result in generator:\n            results.append(result)\n            yield result\n\n        self.inferred[key] = tuple(results)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_url(self, obj, view_name, request, format):\n        # Unsaved objects will not yet have a valid URL.\n        if hasattr(obj, 'pk') and obj.pk in (None, ''):\n            return None\n\n        # default lookup from rest_framework.relations.HyperlinkedRelatedField\n        lookup_value = getattr(obj, self.lookup_field)\n        kwargs = {self.lookup_url_kwarg: lookup_value}\n\n        # multi-level lookup\n        for parent_lookup_kwarg in list(self.parent_lookup_kwargs.keys()):\n            underscored_lookup = self.parent_lookup_kwargs[parent_lookup_kwarg]\n\n            # split each lookup by their __, e.g. \"parent__pk\" will be split into \"parent\" and \"pk\", or\n            # \"parent__super__pk\" would be split into \"parent\", \"super\" and \"pk\"\n            lookups = underscored_lookup.split('__')\n\n            # use the Django ORM to lookup this value, e.g., obj.parent.pk\n            lookup_value = reduce(getattr, [obj] + lookups)\n\n            # store the lookup_name and value in kwargs, which is later passed to the reverse method\n            kwargs.update({parent_lookup_kwarg: lookup_value})\n\n        return self.reverse(view_name, kwargs=kwargs, request=request, format=format)", "response": "Given an object return the URL that hyperlinks to the object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_object(self, view_name, view_args, view_kwargs):\n\n        # default lookup from rest_framework.relations.HyperlinkedRelatedField\n        lookup_value = view_kwargs[self.lookup_url_kwarg]\n        kwargs = {self.lookup_url_kwarg: lookup_value}\n\n        # multi-level lookup\n        for parent_lookup_kwarg in list(self.parent_lookup_kwargs.keys()):\n            lookup_value = view_kwargs[parent_lookup_kwarg]\n            kwargs.update({self.parent_lookup_kwargs[parent_lookup_kwarg]: lookup_value})\n\n        return self.get_queryset().get(**kwargs)", "response": "Returns the object corresponding to a matched URL."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfiltering the QuerySet based on its parents as defined in the serializer_class. parent_lookup_kwargs.", "response": "def get_queryset(self):\n        \"\"\"\n        Filter the `QuerySet` based on its parents as defined in the\n        `serializer_class.parent_lookup_kwargs`.\n        \"\"\"\n        queryset = super(NestedViewSetMixin, self).get_queryset()\n        if hasattr(self.serializer_class, 'parent_lookup_kwargs'):\n            orm_filters = {}\n            for query_param, field_name in self.serializer_class.parent_lookup_kwargs.items():\n                orm_filters[field_name] = self.kwargs[query_param]\n            return queryset.filter(**orm_filters)\n        return queryset"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprovide a python2 / 3 compatible string representation of the value", "response": "def compat_string(value):\n        \"\"\"\n        Provide a python2/3 compatible string representation of the value\n        :type value:\n        :rtype :\n        \"\"\"\n        if isinstance(value, bytes):\n            return value.decode(encoding='utf-8')\n        return str(value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns the script args every interval", "response": "def script(klass, args, interval):\n        \"\"\"\n        Run the script *args* every *interval* (e.g. \"10s\") to peform health\n        check\n        \"\"\"\n        if isinstance(args, six.string_types) \\\n                or isinstance(args, six.binary_type):\n            warnings.warn(\n                \"Check.script should take a list of args\", DeprecationWarning)\n            args = [\"sh\", \"-c\", args]\n        return {'args': args, 'interval': interval}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a dictionary with the tcp information for the specified host and port.", "response": "def tcp(klass, host, port, interval, timeout=None, deregister=None):\n        \"\"\"\n        Attempt to establish a tcp connection to the specified *host* and\n        *port* at a specified *interval* with optional *timeout* and optional\n        *deregister* after which a failing service will be automatically\n        deregistered.\n        \"\"\"\n        ret = {\n            'tcp': '{host:s}:{port:d}'.format(host=host, port=port),\n            'interval': interval\n        }\n        if timeout:\n            ret['timeout'] = timeout\n        if deregister:\n            ret['DeregisterCriticalServiceAfter'] = deregister\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a dictionary of all the information required to invoke a given script packaged within a running docker container.", "response": "def docker(klass, container_id, shell, script, interval, deregister=None):\n        \"\"\"\n        Invoke *script* packaged within a running docker container with\n        *container_id* at a specified *interval* on the configured\n        *shell* using the Docker Exec API.  Optional *register* after which a\n        failing service will be automatically deregistered.\n        \"\"\"\n        ret = {\n            'docker_container_id': container_id,\n            'shell': shell,\n            'script': script,\n            'interval': interval\n        }\n        if deregister:\n            ret['DeregisterCriticalServiceAfter'] = deregister\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef changes(self, adding=None, deleting=None):\n        if deleting is not None:\n            for deleted in deleting:\n                self.root_node.remove(deleted)\n\n        if adding is not None:\n            for added in adding:\n                self.root_node.add(added)\n\n        added = list()\n        removed = list()\n\n        for csn in self._get_conflict_set_nodes():\n            c_added, c_removed = csn.get_activations()\n            added.extend(c_added)\n            removed.extend(c_removed)\n\n        return (added, removed)", "response": "Pass the given changes to the root_node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef build_alpha_part(ruleset, root_node):\n        # Adds a dummy rule with InitialFact as LHS for always generate\n        # the alpha part matching InitialFact(). This is needed for the\n        # CE using InitialFact\n        ruleset = ruleset.copy()\n        ruleset.add(Rule(InitialFact()))\n\n        # Generate a dictionary with rules and the set of facts of the\n        # rule.\n        rule_facts = {rule: extract_facts(rule) for rule in ruleset}\n\n        # For each fact build a list of checker function capable of\n        # check for each part in the fact.\n        fact_checks = {fact: set(generate_checks(fact))\n                       for fact in chain.from_iterable(rule_facts.values())}\n\n        # Make a ranking of the most used checks\n        check_rank = Counter(chain.from_iterable(fact_checks.values()))\n\n        def weighted_check_sort(check):\n            \"\"\"Sort check by its type and number of times seen.\"\"\"\n            if isinstance(check, TypeCheck):\n                return (float('inf'), hash(check))\n            elif isinstance(check, FactCapture):\n                return (float('-inf'), hash(check))\n            elif isinstance(check, FeatureCheck):\n                return (check_rank[check], hash(check))\n            else:\n                raise TypeError(\"Unknown check type.\")  # pragma: no cover\n\n        def weighted_rule_sort(rule):\n            \"\"\"Sort rules by the average weight of its checks.\"\"\"\n            total = 0\n            for fact in rule_facts[rule]:\n                for check in fact_checks[fact]:\n                    total += check_rank[check]\n            return total / len(rule_facts[rule])\n\n        sorted_rules = sorted(ruleset, key=weighted_rule_sort, reverse=True)\n\n        fact_terminal_nodes = dict()\n        # For rule in rank order and for each rule fact also in rank\n        # order, build the alpha brank looking for an existing node\n        # first.\n        for rule in sorted_rules:\n            for fact in rule_facts[rule]:\n                current_node = root_node\n                fact_sorted_checks = sorted(\n                    fact_checks[fact],\n                    key=weighted_check_sort,\n                    reverse=True)\n\n                for check in fact_sorted_checks:\n                    # Look for a child node with the given check in the\n                    # current parent node.\n                    for child in current_node.children:\n                        if child.node.matcher is check:\n                            current_node = child.node\n                            break\n                    else:\n                        # Create a new node and append as child\n                        new_node = FeatureTesterNode(check)\n                        current_node.add_child(new_node, new_node.activate)\n                        current_node = new_node\n\n                fact_terminal_nodes[fact] = current_node\n\n        # Return this dictionary containing the last alpha node for each\n        # fact.\n        return fact_terminal_nodes", "response": "Given a set of already adapted rules build the alpha part of the RETE network starting at root_node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive a set of already adapted rules, and a dictionary of patterns and alpha_nodes, wire up the beta part of the RETE network.", "response": "def build_beta_part(ruleset, alpha_terminals):\n        \"\"\"\n        Given a set of already adapted rules, and a dictionary of\n        patterns and alpha_nodes, wire up the beta part of the RETE\n        network.\n\n        \"\"\"\n        for rule in ruleset:\n            if isinstance(rule[0], OR):\n                for subrule in rule[0]:\n                    wire_rule(rule, alpha_terminals, lhs=subrule)\n            else:\n                wire_rule(rule, alpha_terminals, lhs=rule)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef print_network(self):  # pragma: no cover\n        edges = set()\n\n        def gen_edges(node):\n            nonlocal edges\n            name = str(id(node))\n\n            yield '{name} [label=\"{cls_name}\"];'.format(\n                name=name,\n                cls_name=str(node))\n\n            for child in node.children:\n                if (node, child.callback) not in edges:\n                    yield ('{parent} -> {child} '\n                           '[label=\"{child_label}\"];').format(\n                        parent=name,\n                        child=str(id(child.node)),\n                        child_label=child.callback.__name__)\n                    edges.add((node, child.callback))\n                yield from gen_edges(child.node)\n\n        return \"digraph {\\n %s \\n}\" % (\"\\n\".join(\n            gen_edges(self.root_node)))", "response": "Generate a graphviz compatible graph."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reset(self):\n        watchers.MATCHER.debug(\"Node <%s> reset\", self)\n        self._reset()\n        for child in self.children:\n            child.node.reset()", "response": "Reset itself and recursively all its children."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nactivates the node with the given token.", "response": "def activate(self, token):\n        \"\"\"Make a copy of the received token and call `self._activate`.\"\"\"\n\n        if watchers.worth('MATCHER', 'DEBUG'):  # pragma: no cover\n            watchers.MATCHER.debug(\n                \"Node <%s> activated with token %r\", self, token)\n\n        return self._activate(token.copy())"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nactivating the node with the given token.", "response": "def activate_left(self, token):\n        \"\"\"Make a copy of the received token and call `_activate_left`.\"\"\"\n        watchers.MATCHER.debug(\n            \"Node <%s> activated left with token %r\", self, token)\n        return self._activate_left(token.copy())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef activate_right(self, token):\n        watchers.MATCHER.debug(\n            \"Node <%s> activated right with token %r\", self, token)\n        return self._activate_right(token.copy())", "response": "Activate the right node with the given token."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef duplicate(self, template_fact, **modifiers):\n\n        newfact = template_fact.copy()\n        newfact.update(dict(self._get_real_modifiers(**modifiers)))\n\n        return self.declare(newfact)", "response": "Create a new fact from an existing one."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_deffacts(self):\n        return sorted(self._get_by_type(DefFacts), key=lambda d: d.order)", "response": "Return the existing deffacts sorted by the internal order"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef retract(self, idx_or_declared_fact):\n        self.facts.retract(idx_or_declared_fact)\n\n        if not self.running:\n            added, removed = self.get_activations()\n            self.strategy.update_agenda(self.agenda, added, removed)", "response": "Retracts a specific fact using its index."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run(self, steps=float('inf')):\n\n        self.running = True\n        activation = None\n        execution = 0\n        while steps > 0 and self.running:\n\n            added, removed = self.get_activations()\n            self.strategy.update_agenda(self.agenda, added, removed)\n\n            if watchers.worth('AGENDA', 'DEBUG'):  # pragma: no cover\n                for idx, act in enumerate(self.agenda.activations):\n                    watchers.AGENDA.debug(\n                        \"%d: %r %r\",\n                        idx,\n                        act.rule.__name__,\n                        \", \".join(str(f) for f in act.facts))\n\n            activation = self.agenda.get_next()\n\n            if activation is None:\n                break\n            else:\n                steps -= 1\n                execution += 1\n\n                watchers.RULES.info(\n                    \"FIRE %s %s: %s\",\n                    execution,\n                    activation.rule.__name__,\n                    \", \".join(str(f) for f in activation.facts))\n\n                activation.rule(\n                    self,\n                    **{k: v\n                       for k, v in activation.context.items()\n                       if not k.startswith('__')})\n\n        self.running = False", "response": "Execute agenda activations and execute rules"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreset the internal state of the internal state of the internal state of the persistent state.", "response": "def reset(self, **kwargs):\n        \"\"\"\n        Performs a reset as per CLIPS behaviour (resets the\n        agenda and factlist and declares InitialFact())\n\n        Any keyword argument passed to `reset` will be passed to @DefFacts\n        which have those arguments on their signature.\n\n        .. note:: If persistent facts have been added, they'll be\n                  re-declared.\n        \"\"\"\n\n        self.agenda = Agenda()\n        self.facts = FactList()\n\n        self.matcher.reset()\n\n        deffacts = []\n        for deffact in self.get_deffacts():\n            signature = inspect.signature(deffact)\n            if not any(p.kind == inspect.Parameter.VAR_KEYWORD\n                       for p in signature.parameters.values()):\n                # There is not **kwargs defined. Pass only the defined\n                # names.\n                args = set(signature.parameters.keys())\n                deffacts.append(\n                    deffact(**{k: v for k, v in kwargs.items()\n                               if k in args}))\n            else:\n                deffacts.append(deffact(**kwargs))\n\n        # Declare all facts yielded by deffacts\n        self.__declare(*chain.from_iterable(deffacts))\n\n        self.running = False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndeclaring from inside a fact equivalent to assert in clips. This updates the agenda.", "response": "def declare(self, *facts):\n        \"\"\"\n        Declare from inside a fact, equivalent to ``assert`` in clips.\n\n        .. note::\n\n            This updates the agenda.\n        \"\"\"\n\n        if not self.facts:\n            watchers.ENGINE.warning(\"Declaring fact before reset()\")\n        return self.__declare(*facts)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef BETWEEN(a, b):\n    if any((isinstance(x, ConditionalElement) for x in (a, b))):\n        raise TypeError(\n            \"A ConditionalElement can't be used as an operator condition.\")\n    else:\n        return P(lambda x: a <= x <= b)", "response": "A BETWEEN operator selects values within a given range."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ILIKE(pattern):\n    return P(lambda x: fnmatch.fnmatch(x.lower(), pattern.lower()))", "response": "Unix shell - style wildcards. Case - insensitive"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef worth(what, level_name):\n    return (logging.NOTSET\n            < globals()[what].level\n            <= getattr(logging, level_name))", "response": "Returns True if the watcher what would log under level_name."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef watch(*what, level=logging.DEBUG):\n    if not what:\n        what = ALL\n\n    for watcher_name in what:\n        watcher = globals()[watcher_name]\n        watcher.setLevel(level)", "response": "Enable or disable the specified watchers."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngives a rule return a set containing all rule LHS facts.", "response": "def extract_facts(rule):\n    \"\"\"Given a rule, return a set containing all rule LHS facts.\"\"\"\n    def _extract_facts(ce):\n        if isinstance(ce, Fact):\n            yield ce\n        elif isinstance(ce, TEST):\n            pass\n        else:\n            for e in ce:\n                yield from _extract_facts(e)\n\n    return set(_extract_facts(rule))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef generate_checks(fact):\n\n    yield TypeCheck(type(fact))\n\n    fact_captured = False\n    for key, value in fact.items():\n        if (isinstance(key, str)\n                and key.startswith('__')\n                and key.endswith('__')):\n            # Special fact feature\n            if key == '__bind__':\n                yield FactCapture(value)\n                fact_captured = True\n            else:  # pragma: no cover\n                yield FeatureCheck(key, value)\n        else:\n            yield FeatureCheck(key, value)\n\n    # Assign the matching fact to the context\n    if not fact_captured:\n        yield FactCapture(\"__pattern_%s__\" % id(fact))", "response": "Given a fact generate a list of Check objects for checking it."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add(self, fact):\n        token = Token.valid(fact)\n        MATCHER.debug(\"<BusNode> added %r\", token)\n        for child in self.children:\n            child.callback(token)", "response": "Create a VALID token and send it to all children."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating an INVALID token and send it to all children.", "response": "def remove(self, fact):\n        \"\"\"Create an INVALID token and send it to all children.\"\"\"\n        token = Token.invalid(fact)\n        MATCHER.debug(\"<BusNode> added %r\", token)\n        for child in self.children:\n            child.callback(token)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _activate(self, token):\n\n        info = token.to_info()\n\n        activation = Activation(\n            self.rule,\n            frozenset(info.data),\n            {k: v for k, v in info.context if isinstance(k, str)})\n\n        if token.is_valid():\n            if info not in self.memory:\n                self.memory.add(info)\n                if activation in self.removed:\n                    self.removed.remove(activation)\n                else:\n                    self.added.add(activation)\n        else:\n            try:\n                self.memory.remove(info)\n            except ValueError:\n                pass\n            else:\n                if activation in self.added:\n                    self.added.remove(activation)\n                else:\n                    self.removed.add(activation)", "response": "Activate this node for the given token."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of activations.", "response": "def get_activations(self):\n        \"\"\"Return a list of activations.\"\"\"\n        res = (self.added, self.removed)\n\n        self.added = set()\n        self.removed = set()\n\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _activate_left(self, token):\n        if not self.right_memory:\n            if token.is_valid():\n                self.left_memory[token.to_info()] = 0\n            else:\n                del self.left_memory[token.to_info()]\n\n            for child in self.children:\n                child.callback(token)\n\n        elif token.is_valid():\n            count = 0\n\n            for _, right_context in self.right_memory:\n                if self.matcher(token.context, dict(right_context)):\n                    count += 1\n\n            if count == 0:\n                for child in self.children:\n                    child.callback(token)\n\n            self.left_memory[token.to_info()] = count\n        else:\n            count = 0\n            for _, right_context in self.right_memory:\n                if self.matcher(token.context, dict(right_context)):\n                    count += 1\n                    break\n\n            if count == 0:\n                for child in self.children:\n                    child.callback(token)\n\n            del self.left_memory[token.to_info()]", "response": "Activates from the left."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nactivating from the right.", "response": "def _activate_right(self, token):\n        \"\"\"\n        Activate from the right.\n\n        Go over the left memory and find matching data, when found\n        update the counter (substracting if the given token is invalid\n        and adding otherwise). Depending on the result of this operation\n        a new token is generated and passing to all children.\n\n        \"\"\"\n        if token.is_valid():\n            self.right_memory.append(token.to_info())\n            inc = 1\n        else:\n            inc = -1\n            self.right_memory.remove(token.to_info())\n\n        for left in self.left_memory:\n            if self.matcher(dict(left.context), token.context):\n                self.left_memory[left] += inc\n                newcount = self.left_memory[left]\n                if (newcount == 0 and inc == -1) or \\\n                        (newcount == 1 and inc == 1):\n                    if inc == -1:\n                        newtoken = left.to_valid_token()\n                    else:\n                        newtoken = left.to_invalid_token()\n\n                    for child in self.children:\n                        child.callback(newtoken)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndeclare a fact in the list.", "response": "def declare(self, fact):\n        \"\"\"\n        Assert (in clips terminology) a fact.\n\n        This keeps insertion order.\n\n        .. warning:: This will reject any object that not descend\n                     from the Fact class.\n\n        :param fact: The fact to declare, **must** be derived from\n                     :obj:`pyknow.fact.Fact`.\n        :return: (int) The index of the fact in the list.\n        :throws ValueError: If the fact providen is not a Fact object\n\n        \"\"\"\n\n        if not isinstance(fact, Fact):\n            raise ValueError('The fact must descend the Fact class.')\n\n        # Validate fact, will raise on validation error.\n        fact.validate()\n\n        fact_id = self._get_fact_id(fact)\n\n        if self.duplication or fact_id not in self.reference_counter:\n            # Assign the ID to the fact\n            idx = self.last_index\n            fact.__factid__ = idx\n\n            # Insert the fact in the factlist\n            self[idx] = fact\n\n            self.last_index += 1\n\n            self.added.append(fact)\n            self.reference_counter[fact_id] += 1\n\n            watchers.FACTS.info(\" ==> %s: %r\", fact, fact)\n            return fact\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretracts a previously asserted fact in the factlist.", "response": "def retract(self, idx_or_fact):\n        \"\"\"\n        Retract a previously asserted fact.\n\n        See `\"Retract that fact\" in Clips User Guide\n        <http://clipsrules.sourceforge.net/doc\\\n                umentation/v624/ug.htm#_Toc412126077>`_.\n\n        :param idx: The index of the fact to retract in the factlist\n        :return: (int) The retracted fact's index\n        :throws IndexError: If the fact's index providen does not exist\n        \"\"\"\n\n        if isinstance(idx_or_fact, int):\n            idx = idx_or_fact\n        else:\n            idx = idx_or_fact.__factid__\n\n        if idx not in self:\n            raise IndexError('Fact not found.')\n\n        fact = self[idx]\n\n        # Decrement value reference counter\n        fact_id = self._get_fact_id(fact)\n        self.reference_counter[fact_id] -= 1\n        if self.reference_counter[fact_id] == 0:\n            self.reference_counter.pop(fact_id)\n\n        watchers.FACTS.info(\" <== %s: %r\", fact, fact)\n        self.removed.append(fact)\n\n        del self[idx]\n\n        return idx"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef changes(self):\n        try:\n            return self.added, self.removed\n        finally:\n            self.added = list()\n            self.removed = list()", "response": "Return a tuple with the removed and added facts since last run."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a new rule with the same attributes but with the given conditions.", "response": "def new_conditions(self, *args):\n        \"\"\"\n        Generate a new rule with the same attributes but with the given\n        conditions.\n\n        \"\"\"\n        obj = self.__class__(*args, salience=self.salience)\n\n        if self._wrapped:\n            obj = obj(self._wrapped)\n\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef as_dict(self):\n        return {k: unfreeze(v)\n                for k, v in self.items()\n                if not self.is_special(k)}", "response": "Return a dictionary containing this Fact data."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a copy of this Fact.", "response": "def copy(self):\n        \"\"\"Return a copy of this `Fact`.\"\"\"\n        content = [(k, v) for k, v in self.items()]\n\n        intidx = [(k, v) for k, v in content if isinstance(k, int)]\n        args = [v for k, v in sorted(intidx)]\n\n        kwargs = {k: v\n                  for k, v in content\n                  if not isinstance(k, int) and not self.is_special(k)}\n        return self.__class__(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_child(self, node, callback):\n        if node not in self.children:\n            self.children.append(ChildNode(node, callback))", "response": "Add a child node and callback to the children set."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes a copy of this Token.", "response": "def copy(self):\n        \"\"\"\n        Make a new instance of this Token.\n\n        This method makes a copy of the mutable part of the token before\n        making the instance.\n\n        \"\"\"\n        return self.__class__(self.tag, self.data.copy(), self.context.copy())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing a datetime string and returns a new object.", "response": "def parse(datetime_str, timezone=None, isofirst=True, dayfirst=True, yearfirst=True):\n    \"\"\"\n    Parses a datetime string and returns a `Delorean` object.\n\n    :param datetime_str: The string to be interpreted into a `Delorean` object.\n    :param timezone: Pass this parameter and the returned Delorean object will be normalized to this timezone. Any\n        offsets passed as part of datetime_str will be ignored.\n    :param isofirst: try to parse string as date in ISO format before everything else.\n    :param dayfirst: Whether to interpret the first value in an ambiguous 3-integer date (ex. 01/05/09) as the day\n        (True) or month (False). If yearfirst is set to True, this distinguishes between YDM and YMD.\n    :param yearfirst: Whether to interpret the first value in an ambiguous 3-integer date (ex. 01/05/09) as the\n        year. If True, the first number is taken to be the year, otherwise the last number is taken to be the year.\n\n    .. testsetup::\n\n        from delorean import Delorean\n        from delorean import parse\n\n    .. doctest::\n\n        >>> parse('2015-01-01 00:01:02')\n        Delorean(datetime=datetime.datetime(2015, 1, 1, 0, 1, 2), timezone='UTC')\n\n    If a fixed offset is provided in the datetime_str, it will be parsed and the returned `Delorean` object will store a\n    `pytz.FixedOffest` as it's timezone.\n\n    .. doctest::\n\n        >>> parse('2015-01-01 00:01:02 -0800')\n        Delorean(datetime=datetime.datetime(2015, 1, 1, 0, 1, 2), timezone=pytz.FixedOffset(-480))\n\n    If the timezone argument is supplied, the returned Delorean object will be in the timezone supplied. Any offsets in\n    the datetime_str will be ignored.\n\n    .. doctest::\n\n        >>> parse('2015-01-01 00:01:02 -0500', timezone='US/Pacific')\n        Delorean(datetime=datetime.datetime(2015, 1, 1, 0, 1, 2), timezone='US/Pacific')\n\n    If an unambiguous timezone is detected in the datetime string, a Delorean object with that datetime and\n    timezone will be returned.\n\n    .. doctest::\n\n        >>> parse('2015-01-01 00:01:02 PST')\n        Delorean(datetime=datetime.datetime(2015, 1, 1, 0, 1, 2), timezone='America/Los_Angeles')\n\n    However if the provided timezone is ambiguous, parse will ignore the timezone and return a `Delorean` object in UTC\n    time.\n\n        >>> parse('2015-01-01 00:01:02 EST')\n        Delorean(datetime=datetime.datetime(2015, 1, 1, 0, 1, 2), timezone='UTC')\n\n    \"\"\"\n    # parse string to datetime object\n    dt = None\n    if isofirst:\n        try:\n            dt = isocapture(datetime_str)\n        except Exception:\n            pass\n    if dt is None:\n        dt = capture(datetime_str, dayfirst=dayfirst, yearfirst=yearfirst)\n\n    if timezone:\n        dt = dt.replace(tzinfo=None)\n        do = Delorean(datetime=dt, timezone=timezone)\n    elif dt.tzinfo is None:\n        # assuming datetime object passed in is UTC\n        do = Delorean(datetime=dt, timezone='UTC')\n    elif isinstance(dt.tzinfo, tzoffset):\n        utcoffset = dt.tzinfo.utcoffset(None)\n        total_seconds = (\n            (utcoffset.microseconds + (utcoffset.seconds + utcoffset.days * 24 * 3600) * 10**6) / 10**6)\n\n        tz = pytz.FixedOffset(total_seconds / 60)\n        dt = dt.replace(tzinfo=None)\n        do = Delorean(dt, timezone=tz)\n    elif isinstance(dt.tzinfo, tzlocal):\n        tz = get_localzone()\n        dt = dt.replace(tzinfo=None)\n        do = Delorean(dt, timezone=tz)\n    else:\n        dt = pytz.utc.normalize(dt)\n        # making dt naive so we can pass it to Delorean\n        dt = dt.replace(tzinfo=None)\n        # if parse string has tzinfo we return a normalized UTC\n        # delorean object that represents the time.\n        do = Delorean(datetime=dt, timezone='UTC')\n\n    return do"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a sequence of Delorean objects with a start stop and frequency equal to DAILY.", "response": "def range_daily(start=None, stop=None, timezone='UTC', count=None):\n    \"\"\"\n    This an alternative way to generating sets of Delorean objects with\n    DAILY stops\n    \"\"\"\n    return stops(start=start, stop=stop, freq=DAILY, timezone=timezone, count=count)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef range_hourly(start=None, stop=None, timezone='UTC', count=None):\n    return stops(start=start, stop=stop, freq=HOURLY, timezone=timezone, count=count)", "response": "Returns a sequence of Delorean objects with HOURLY stops."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a sequence of Delorean objects with MONTHLY stops.", "response": "def range_monthly(start=None, stop=None, timezone='UTC', count=None):\n    \"\"\"\n    This an alternative way to generating sets of Delorean objects with\n    MONTHLY stops\n    \"\"\"\n    return stops(start=start, stop=stop, freq=MONTHLY, timezone=timezone, count=count)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a sequence of Delorean objects with YEARLY stops.", "response": "def range_yearly(start=None, stop=None, timezone='UTC', count=None):\n    \"\"\"\n    This an alternative way to generating sets of Delorean objects with\n    YEARLY stops\n    \"\"\"\n    return stops(start=start, stop=stop, freq=YEARLY, timezone=timezone, count=count)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef stops(freq, interval=1, count=None, wkst=None, bysetpos=None,\n          bymonth=None, bymonthday=None, byyearday=None, byeaster=None,\n          byweekno=None, byweekday=None, byhour=None, byminute=None,\n          bysecond=None, timezone='UTC', start=None, stop=None):\n    \"\"\"\n    This will create a list of delorean objects the apply to\n    setting possed in.\n    \"\"\"\n    # check to see if datetimees passed in are naive if so process them\n    # with given timezone.\n    if all([(start is None or is_datetime_naive(start)),\n            (stop is None or is_datetime_naive(stop))]):\n        pass\n    else:\n        raise DeloreanInvalidDatetime('Provide a naive datetime object')\n\n    # if no datetimes are passed in create a proper datetime object for\n    # start default because default in dateutil is datetime.now() :(\n    if start is None:\n        start = datetime_timezone(timezone)\n\n    for dt in rrule(freq, interval=interval, count=count, wkst=wkst, bysetpos=bysetpos,\n          bymonth=bymonth, bymonthday=bymonthday, byyearday=byyearday, byeaster=byeaster,\n          byweekno=byweekno, byweekday=byweekday, byhour=byhour, byminute=byminute,\n          bysecond=bysecond, until=stop, dtstart=start):\n        # make the delorean object\n        # yield it.\n        # doing this to make sure delorean receives a naive datetime.\n        dt = dt.replace(tzinfo=None)\n        d = Delorean(datetime=dt, timezone=timezone)\n        yield d", "response": "This function returns a generator that yields Delorean objects for the given frequency interval and start and stop dates."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmove datetime given delta by given direction", "response": "def _move_datetime(dt, direction, delta):\n    \"\"\"\n    Move datetime given delta by given direction\n    \"\"\"\n    if direction == 'next':\n        dt = dt + delta\n    elif direction == 'last':\n        dt = dt - delta\n    else:\n        pass\n        # raise some delorean error here\n    return dt"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmoving datetime 1 month in the chosen direction.", "response": "def move_datetime_month(dt, direction, num_shifts):\n    \"\"\"\n    Move datetime 1 month in the chosen direction.\n    unit is a no-op, to keep the API the same as the day case\n    \"\"\"\n    delta = relativedelta(months=+num_shifts)\n    return _move_datetime(dt, direction, delta)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmove datetime 1 week in the chosen direction.", "response": "def move_datetime_week(dt, direction, num_shifts):\n    \"\"\"\n    Move datetime 1 week in the chosen direction.\n    unit is a no-op, to keep the API the same as the day case\n    \"\"\"\n    delta = relativedelta(weeks=+num_shifts)\n    return _move_datetime(dt, direction, delta)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef move_datetime_year(dt, direction, num_shifts):\n    delta = relativedelta(years=+num_shifts)\n    return _move_datetime(dt, direction, delta)", "response": "Move datetime 1 year in the chosen direction."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef datetime_timezone(tz):\n    utc_datetime_naive = datetime.utcnow()\n    # return a localized datetime to UTC\n    utc_localized_datetime = localize(utc_datetime_naive, 'UTC')\n    # normalize the datetime to given timezone\n    normalized_datetime = normalize(utc_localized_datetime, tz)\n    return normalized_datetime", "response": "This method returns a localized datetime object given a timezone returns a localized datetime object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef localize(dt, tz):\n    if not isinstance(tz, tzinfo):\n        tz = pytz.timezone(tz)\n\n    return tz.localize(dt)", "response": "Returns a localized version of the given datetime object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef normalize(dt, tz):\n    if not isinstance(tz, tzinfo):\n        tz = pytz.timezone(tz)\n    dt = tz.normalize(dt)\n    return dt", "response": "Given a datetime object with a timezone return a datetime object with a proper timezone."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nshifts datetime in direction by some multiple of the specified units and return a Delorean object.", "response": "def _shift_date(self, direction, unit, *args):\n        \"\"\"\n        Shift datetime in `direction` in _VALID_SHIFT_DIRECTIONS and by some\n        unit in _VALID_SHIFTS and shift that amount by some multiple,\n        defined by by args[0] if it exists\n        \"\"\"\n        this_module = sys.modules[__name__]\n\n        num_shifts = 1\n        if len(args) > 0:\n            num_shifts = int(args[0])\n\n        if unit in ['monday', 'tuesday', 'wednesday', 'thursday', 'friday',\n                    'saturday', 'sunday']:\n            shift_func = move_datetime_namedday\n            dt = shift_func(self._dt, direction, unit)\n            if num_shifts > 1:\n                for n in range(num_shifts - 1):\n                    dt = shift_func(dt, direction, unit)\n        else:\n            shift_func = getattr(this_module, 'move_datetime_%s' % unit)\n            dt = shift_func(self._dt, direction, num_shifts)\n\n        return Delorean(datetime=dt.replace(tzinfo=None), timezone=self.timezone)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef truncate(self, s):\n        if s == 'second':\n            self._dt = self._dt.replace(microsecond=0)\n        elif s == 'minute':\n            self._dt = self._dt.replace(second=0, microsecond=0)\n        elif s == 'hour':\n            self._dt = self._dt.replace(minute=0, second=0, microsecond=0)\n        elif s == 'day':\n            self._dt = self._dt.replace(hour=0, minute=0, second=0, microsecond=0)\n        elif s == 'month':\n            self._dt = self._dt.replace(day=1, hour=0, minute=0, second=0, microsecond=0)\n        elif s == 'year':\n            self._dt = self._dt.replace(month=1, day=1, hour=0, minute=0, second=0, microsecond=0)\n        else:\n            raise ValueError(\"Invalid truncation level\")\n\n        return self", "response": "Truncate the internal datetime object to the nearest s\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nshifts the timezone from the current timezone to the specified timezone. Returns the modified object.", "response": "def shift(self, timezone):\n        \"\"\"\n        Shifts the timezone from the current timezone to the specified timezone associated with the Delorean object,\n        modifying the Delorean object and returning the modified object.\n\n        .. testsetup::\n\n            from datetime import datetime\n            from delorean import Delorean\n\n        .. doctest::\n\n            >>> d = Delorean(datetime(2015, 1, 1), timezone='US/Pacific')\n            >>> d.shift('UTC')\n            Delorean(datetime=datetime.datetime(2015, 1, 1, 8, 0), timezone='UTC')\n\n        \"\"\"\n        try:\n            self._tzinfo = pytz.timezone(timezone)\n        except pytz.UnknownTimeZoneError:\n            raise DeloreanInvalidTimezone('Provide a valid timezone')\n        self._dt = self._tzinfo.normalize(self._dt.astimezone(self._tzinfo))\n        self._tzinfo = self._dt.tzinfo\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the total seconds since epoch associated with .", "response": "def epoch(self):\n        \"\"\"\n        Returns the total seconds since epoch associated with\n        the Delorean object.\n\n        .. testsetup::\n\n            from datetime import datetime\n            from delorean import Delorean\n\n        .. doctest::\n\n            >>> d = Delorean(datetime(2015, 1, 1), timezone='US/Pacific')\n            >>> d.epoch\n            1420099200.0\n\n        \"\"\"\n        epoch_sec = pytz.utc.localize(datetime.utcfromtimestamp(0))\n        now_sec = pytz.utc.normalize(self._dt)\n        delta_sec = now_sec - epoch_sec\n        return get_total_second(delta_sec)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a new Delorean object after applying replace on the existing datetime object.", "response": "def replace(self, **kwargs):\n        \"\"\"\n            Returns a new Delorean object after applying replace on the\n            existing datetime object.\n\n            .. testsetup::\n\n                from datetime import datetime\n                from delorean import Delorean\n\n            .. doctest::\n\n                >>> d = Delorean(datetime(2015, 1, 1, 12, 15), timezone='UTC')\n                >>> d.replace(hour=8)\n                Delorean(datetime=datetime.datetime(2015, 1, 1, 8, 15), timezone='UTC')\n            \"\"\"\n\n        return Delorean(datetime=self._dt.replace(**kwargs), timezone=self.timezone)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef humanize(self):\n        now = self.now(self.timezone)\n\n        return humanize.naturaltime(now - self)", "response": "Humanize relative to now"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef format_datetime(self, format='medium', locale='en_US'):\n        return format_datetime(self._dt, format=format, locale=locale)", "response": "Return a date string formatted to the given format."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmasking a phone number in the alphabetical order.", "response": "def mask_phone_number(number):\n    \"\"\"\n    Masks a phone number, only first 3 and last 2 digits visible.\n\n    Examples:\n\n    * `+31 * ******58`\n\n    :param number: str or phonenumber object\n    :return: str\n    \"\"\"\n    if isinstance(number, phonenumbers.PhoneNumber):\n        number = format_phone_number(number)\n    return phone_mask.sub('*', number)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef format_phone_number(number):\n    if not isinstance(number, phonenumbers.PhoneNumber):\n        number = phonenumbers.parse(number)\n    return phonenumbers.format_number(number, phonenumbers.PhoneNumberFormat.INTERNATIONAL)", "response": "Formats a phone number in international notation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef device_action(device):\n    assert isinstance(device, PhoneDevice)\n    number = mask_phone_number(format_phone_number(device.number))\n    if device.method == 'sms':\n        return ugettext('Send text message to %s') % number\n    elif device.method == 'call':\n        return ugettext('Call number %s') % number\n    raise NotImplementedError('Unknown method: %s' % device.method)", "response": "Generates an actionable text for a : class : ~two_factor. models. PhoneDevice object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate a challenge for the current TOTP token.", "response": "def generate_challenge(self):\n        # local import to avoid circular import\n        from two_factor.utils import totp_digits\n\n        \"\"\"\n        Sends the current TOTP token to `self.number` using `self.method`.\n        \"\"\"\n        no_digits = totp_digits()\n        token = str(totp(self.bin_key, digits=no_digits)).zfill(no_digits)\n        if self.method == 'call':\n            make_call(device=self, token=token)\n        else:\n            send_sms(device=self, token=token)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn True if the given HttpRequest has permission to view at least one page in the admin site.", "response": "def has_permission(self, request):\n        \"\"\"\n        Returns True if the given HttpRequest has permission to view\n        *at least one* page in the admin site.\n        \"\"\"\n        if not super(AdminSiteOTPRequiredMixin, self).has_permission(request):\n            return False\n        return request.user.is_verified()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef login(self, request, extra_context=None):\n        redirect_to = request.POST.get(REDIRECT_FIELD_NAME, request.GET.get(REDIRECT_FIELD_NAME))\n\n        if not redirect_to or not is_safe_url(url=redirect_to, allowed_hosts=[request.get_host()]):\n            redirect_to = resolve_url(settings.LOGIN_REDIRECT_URL)\n\n        return redirect_to_login(redirect_to)", "response": "Redirects to the site login page for the given HttpRequest."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a function based decorator into a class based decorator usable on class based Views.", "response": "def class_view_decorator(function_decorator):\n    \"\"\"\n    Converts a function based decorator into a class based decorator usable\n    on class based Views.\n\n    Can't subclass the `View` as it breaks inheritance (super in particular),\n    so we monkey-patch instead.\n\n    From: http://stackoverflow.com/a/8429311/58107\n    \"\"\"\n    def simple_decorator(View):\n        View.dispatch = method_decorator(function_decorator)(View.dispatch)\n        return View\n    return simple_decorator"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_prev_step(self, step=None):\n        if step is None:\n            step = self.steps.current\n        form_list = self.get_form_list()\n        keys = list(form_list.keys())\n        key = keys.index(step) - 1\n        if key >= 0:\n            for prev_step in keys[key::-1]:\n                if self.is_step_visible(prev_step):\n                    return prev_step\n        return None", "response": "Returns the previous step before the given step."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the next step after the given step.", "response": "def get_next_step(self, step=None):\n        \"\"\"\n        Returns the next step after the given `step`. If no more steps are\n        available, None will be returned. If the `step` argument is None, the\n        current step will be determined automatically.\n        \"\"\"\n        if step is None:\n            step = self.steps.current\n        form_list = self.get_form_list()\n        keys = list(form_list.keys())\n        key = keys.index(step) + 1\n        for next_step in keys[key:]:\n            if self.is_step_visible(next_step):\n                return next_step\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef post(self, *args, **kwargs):\n        if self.steps.current not in self.steps.all:\n            logger.warning(\"Current step '%s' is no longer valid, returning \"\n                           \"to last valid step in the wizard.\",\n                           self.steps.current)\n            return self.render_goto_step(self.steps.all[-1])\n\n        # -- Duplicated code from upstream\n        # Look for a wizard_goto_step element in the posted data which\n        # contains a valid step name. If one was found, render the requested\n        # form. (This makes stepping back a lot easier).\n        wizard_goto_step = self.request.POST.get('wizard_goto_step', None)\n        if wizard_goto_step and wizard_goto_step in self.get_form_list():\n            return self.render_goto_step(wizard_goto_step)\n\n        # Check if form was refreshed\n        management_form = ManagementForm(self.request.POST, prefix=self.prefix)\n        if not management_form.is_valid():\n            raise SuspiciousOperation(_('ManagementForm data is missing or has been tampered with'))\n\n        form_current_step = management_form.cleaned_data['current_step']\n        if (form_current_step != self.steps.current and\n                self.storage.current_step is not None):\n            # form refreshed, change current step\n            self.storage.current_step = form_current_step\n        # -- End duplicated code from upstream\n\n        # This is different from the first check, as this checks\n        # if the new step is available. See issue #65.\n        if self.steps.current not in self.steps.all:\n            logger.warning(\"Requested step '%s' is no longer valid, returning \"\n                           \"to last valid step in the wizard.\",\n                           self.steps.current)\n            return self.render_goto_step(self.steps.all[-1])\n\n        return super(IdempotentSessionWizardView, self).post(*args, **kwargs)", "response": "This method is called by the IdempotentSessionWizard when a new step is available."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstoring the validated data for the current step and cleans out validated formsfor next steps.", "response": "def process_step(self, form):\n        \"\"\"\n        Stores the validated data for `form` and cleans out validated forms\n        for next steps, as those might be affected by the current step. Note\n        that this behaviour is relied upon by the `LoginView` to prevent users\n        from bypassing the `TokenForm` by going steps back and changing\n        credentials.\n        \"\"\"\n        step = self.steps.current\n\n        # If the form is not-idempotent (cannot be validated multiple times),\n        # the cleaned data should be stored; marking the form as validated.\n        self.storage.validated_step_data[step] = form.cleaned_data\n\n        # It is assumed that earlier steps affect later steps; so even though\n        # those forms might not be idempotent, we'll remove the validated data\n        # to force re-entry.\n        # form_list = self.get_form_list(idempotent=False)\n        form_list = self.get_form_list()\n        keys = list(form_list.keys())\n        key = keys.index(step) + 1\n        for next_step in keys[key:]:\n            self.storage.validated_step_data.pop(next_step, None)\n\n        return super(IdempotentSessionWizardView, self).process_step(form)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef download(query, num_results):\n\t# https://stackoverflow.com/questions/11818362/how-to-deal-with-unicode-string-in-url-in-python3\n\tname = quote(query)\n\n\tname  = name.replace(' ','+')\n\turl = 'http://www.google.com/search?q=' + name\n\tif num_results != 10:\n\t\turl += '&num=' + str(num_results)  # adding this param might hint Google towards a bot\n\treq = request.Request(url, headers={\n\t\t'User-Agent' : choice(user_agents),\n\t\t# 'Referer': 'google.com'\n\t})\n\ttry:\n\t\tresponse = request.urlopen(req)\n\texcept Exception:  # catch connection issues\n\t\t# may also catch 503 rate limit exceed\n\t\tprint('ERROR\\n')\n\t\ttraceback.print_exc()\n\t\treturn ''\n\t# response.read is bytes in Py 3\n\tif isPython2:\n\t\t# trick: decode unicode as early as possible\n\t\tdata = response.read().decode('utf8', errors='ignore')\n\telse:\n\t\tdata = str(response.read(), 'utf-8', errors='ignore')\n\t# print(data)\n\treturn data", "response": "downloads HTML after google search\n\t"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef convert_unicode(text):\n\tif isPython2:\n\t\th = HTMLParser()\n\t\ts = h.unescape(text)\n\telse:\n\t\ttry:\n\t\t\ts = unescape(text)\n\t\texcept Exception:\n\t\t\t# Python 3.3 and below\n\t\t\t# https://stackoverflow.com/a/2360639/2295672\n\t\t\ts = HTMLParser().unescape(text)\n\treturn s", "response": "converts unicode HTML to real Unicode"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef search(query, num_results=10):\n\tdata = download(query, num_results)\n\tresults = re.findall(r'\\<h3.*?\\>.*?\\<\\/h3\\>', data, re.IGNORECASE)\n\tif results is None or len(results) == 0:\n\t\tprint('No results where found. Did the rate limit exceed?')\n\t\treturn []\n\t# search has results\n\tlinks = []\n\tfor r in results:\n\t\tmtch = re.match(r'.*?a\\s*?href=\\\"(.*?)\\\".*?\\>(.*?)\\<\\/a\\>.*$', r, flags=re.IGNORECASE)\n\t\tif mtch is None:\n\t\t\tcontinue\n\t\t# parse url\n\t\turl = mtch.group(1)\n\t\t# clean url https://github.com/aviaryan/pythons/blob/master/Others/GoogleSearchLinks.py\n\t\turl = re.sub(r'^.*?=', '', url, count=1) # prefixed over urls \\url=q?\n\t\turl = re.sub(r'\\&amp.*$', '', url, count=1) # suffixed google things\n\t\turl = unquote(url)\n\t\t# url = re.sub(r'\\%.*$', '', url) # NOT SAFE, causes issues with Youtube watch url\n\t\t# parse name\n\t\tname = prune_html(mtch.group(2))\n\t\tname = convert_unicode(name)\n\t\t# append to links\n\t\tif is_url(url): # can be google images result\n\t\t\tlinks.append((name, url))\n\treturn links", "response": "search google for query and returns a list of tuples containing name url"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nverify CAS ticket and gets or creates User object", "response": "def authenticate(self, request, ticket, service):\n        \"\"\"Verifies CAS ticket and gets or creates User object\"\"\"\n        client = get_cas_client(service_url=service, request=request)\n        username, attributes, pgtiou = client.verify_ticket(ticket)\n        if attributes and request:\n            request.session['attributes'] = attributes\n\n        if settings.CAS_USERNAME_ATTRIBUTE != 'uid' and settings.CAS_VERSION != 'CAS_2_SAML_1_0':\n            if attributes:\n                username = attributes.get(settings.CAS_USERNAME_ATTRIBUTE)\n            else:\n                return None\n\n        if not username:\n            return None\n        user = None\n        username = self.clean_username(username)\n\n        if attributes:\n            reject = self.bad_attributes_reject(request, username, attributes)\n            if reject:\n                return None\n\n            # If we can, we rename the attributes as described in the settings file\n            # Existing attributes will be overwritten\n            for cas_attr_name, req_attr_name in settings.CAS_RENAME_ATTRIBUTES.items():\n                if cas_attr_name in attributes and cas_attr_name is not req_attr_name:\n                    attributes[req_attr_name] = attributes[cas_attr_name]\n                    attributes.pop(cas_attr_name)\n\n        UserModel = get_user_model()\n\n        # Note that this could be accomplished in one try-except clause, but\n        # instead we use get_or_create when creating unknown users since it has\n        # built-in safeguards for multiple threads.\n        if settings.CAS_CREATE_USER:\n            user_kwargs = {\n                UserModel.USERNAME_FIELD: username\n            }\n            if settings.CAS_CREATE_USER_WITH_ID:\n                user_kwargs['id'] = self.get_user_id(attributes)\n\n            user, created = UserModel._default_manager.get_or_create(**user_kwargs)\n            if created:\n                user = self.configure_user(user)\n        else:\n            created = False\n            try:\n                if settings.CAS_LOCAL_NAME_FIELD:\n                    user_kwargs = {\n                        settings.CAS_LOCAL_NAME_FIELD: username\n\n                    }\n                    user = UserModel._default_manager.get(**user_kwargs)\n                else:\n                    user = UserModel._default_manager.get_by_natural_key(username)\n            except UserModel.DoesNotExist:\n                pass\n\n        if not self.user_can_authenticate(user):\n            return None\n\n        if pgtiou and settings.CAS_PROXY_CALLBACK and request:\n            request.session['pgtiou'] = pgtiou\n\n        if settings.CAS_APPLY_ATTRIBUTES_TO_USER and attributes:\n            # If we are receiving None for any values which cannot be NULL\n            # in the User model, set them to an empty string instead.\n            # Possibly it would be desirable to let these throw an error\n            # and push the responsibility to the CAS provider or remove\n            # them from the dictionary entirely instead. Handling these\n            # is a little ambiguous.\n            user_model_fields = UserModel._meta.fields\n            for field in user_model_fields:\n                # Handle null -> '' conversions mentioned above\n                if not field.null:\n                    try:\n                        if attributes[field.name] is None:\n                            attributes[field.name] = ''\n                    except KeyError:\n                        continue\n                # Coerce boolean strings into true booleans\n                if field.get_internal_type() == 'BooleanField':\n                    try:\n                        boolean_value = attributes[field.name] == 'True'\n                        attributes[field.name] = boolean_value\n                    except KeyError:\n                        continue\n\n            user.__dict__.update(attributes)\n\n            # If we are keeping a local copy of the user model we\n            # should save these attributes which have a corresponding\n            # instance in the DB.\n            if settings.CAS_CREATE_USER:\n                user.save()\n\n        # send the `cas_user_authenticated` signal\n        cas_user_authenticated.send(\n            sender=self,\n            user=user,\n            created=created,\n            username=username,\n            attributes=attributes,\n            pgtiou=pgtiou,\n            ticket=ticket,\n            service=service,\n            request=request\n        )\n        return user"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the user_id from the attributes.", "response": "def get_user_id(self, attributes):\n        \"\"\"\n        For use when CAS_CREATE_USER_WITH_ID is True. Will raise ImproperlyConfigured\n        exceptions when a user_id cannot be accessed. This is important because we\n        shouldn't create Users with automatically assigned ids if we are trying to\n        keep User primary key's in sync.\n        \"\"\"\n        if not attributes:\n            raise ImproperlyConfigured(\"CAS_CREATE_USER_WITH_ID is True, but \"\n                                       \"no attributes were provided\")\n\n        user_id = attributes.get('id')\n\n        if not user_id:\n            raise ImproperlyConfigured(\"CAS_CREATE_USER_WITH_ID is True, but \"\n                                       \"`'id'` is not part of attributes.\")\n\n        return user_id"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncleans the username according to the CAS_FORCE_CHANGE_USERNAME_CASE setting.", "response": "def clean_username(self, username):\n        \"\"\"\n        Performs any cleaning on the \"username\" prior to using it to get or\n        create the user object.  Returns the cleaned username.\n\n        By default, changes the username case according to\n        `settings.CAS_FORCE_CHANGE_USERNAME_CASE`.\n        \"\"\"\n        username_case = settings.CAS_FORCE_CHANGE_USERNAME_CASE\n        if username_case == 'lower':\n            username = username.lower()\n        elif username_case == 'upper':\n            username = username.upper()\n        elif username_case is not None:\n            raise ImproperlyConfigured(\n                \"Invalid value for the CAS_FORCE_CHANGE_USERNAME_CASE setting. \"\n                \"Valid values are `'lower'`, `'upper'`, and `None`.\")\n        return username"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nredirect to referring page or CAS_REDIRECT_URL if no referrer is set.", "response": "def get_redirect_url(request):\n    \"\"\"Redirects to referring page, or CAS_REDIRECT_URL if no referrer is\n    set.\n    \"\"\"\n\n    next_ = request.GET.get(REDIRECT_FIELD_NAME)\n    if not next_:\n        redirect_url = resolve_url(django_settings.CAS_REDIRECT_URL)\n        if django_settings.CAS_IGNORE_REFERER:\n            next_ = redirect_url\n        else:\n            next_ = request.META.get('HTTP_REFERER', redirect_url)\n        prefix = urllib_parse.urlunparse(\n            (get_protocol(request), request.get_host(), '', '', '', ''),\n        )\n        if next_.startswith(prefix):\n            next_ = next_[len(prefix):]\n    return next_"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_service_url(request, redirect_to=None):\n    if hasattr(django_settings, 'CAS_ROOT_PROXIED_AS'):\n        service = django_settings.CAS_ROOT_PROXIED_AS + request.path\n    else:\n        protocol = get_protocol(request)\n        host = request.get_host()\n        service = urllib_parse.urlunparse(\n            (protocol, host, request.path, '', '', ''),\n        )\n    if not django_settings.CAS_STORE_NEXT:\n        if '?' in service:\n            service += '&'\n        else:\n            service += '?'\n        service += urllib_parse.urlencode({\n            REDIRECT_FIELD_NAME: redirect_to or get_redirect_url(request)\n        })\n    return service", "response": "Generates application django service URL for CAS"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_cas_client(service_url=None, request=None):\n    # Handle CAS_SERVER_URL without protocol and hostname\n    server_url = django_settings.CAS_SERVER_URL\n    if server_url and request and server_url.startswith('/'):\n        scheme = request.META.get(\"X-Forwarded-Proto\", request.scheme)\n        server_url = scheme + \"://\" + request.META['HTTP_HOST'] + server_url\n    # assert server_url.startswith('http'), \"settings.CAS_SERVER_URL invalid\"\n\n    if not django_settings.CAS_VERIFY_SSL_CERTIFICATE:\n        warnings.warn(\n            \"`CAS_VERIFY_SSL_CERTIFICATE` is disabled, meaning that SSL certificates \"\n            \"are not being verified by a certificate authority. This can expose your \"\n            \"system to various attacks and should _never_ be disabled in a production \"\n            \"environment.\"\n        )\n\n    return CASClient(\n        service_url=service_url,\n        version=django_settings.CAS_VERSION,\n        server_url=server_url,\n        extra_login_params=django_settings.CAS_EXTRA_LOGIN_PARAMS,\n        renew=django_settings.CAS_RENEW,\n        username_attribute=django_settings.CAS_USERNAME_ATTRIBUTE,\n        proxy_callback=django_settings.CAS_PROXY_CALLBACK,\n        verify_ssl_certificate=django_settings.CAS_VERIFY_SSL_CERTIFICATE\n    )", "response": "Returns a CASClient according to the CAS_SERVER_URL and CAS_VERSION settings."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process_view(self, request, view_func, view_args, view_kwargs):\n\n        if view_func == login:\n            return cas_login(request, *view_args, **view_kwargs)\n        elif view_func == logout:\n            return cas_logout(request, *view_args, **view_kwargs)\n\n        if view_func in (cas_login, cas_logout):\n            return None\n\n        if settings.CAS_ADMIN_PREFIX:\n            if not request.path.startswith(settings.CAS_ADMIN_PREFIX):\n                return None\n        elif not view_func.__module__.startswith('django.contrib.admin.'):\n            return None\n\n        if request.user.is_authenticated:\n            if request.user.is_staff:\n                return None\n            else:\n                raise PermissionDenied(_('You do not have staff privileges.'))\n        params = urllib_parse.urlencode({REDIRECT_FIELD_NAME: request.get_full_path()})\n        return HttpResponseRedirect(reverse(settings.CAS_LOGIN_URL_NAME) + '?' + params)", "response": "Forwards unauthenticated requests to the admin page and the CAS\n        login URL and CAS\n        logout URL."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef retrieve_pt(cls, request, service):\n        try:\n            pgt = cls.objects.get(user=request.user, session_key=request.session.session_key).pgt\n        except cls.DoesNotExist:\n            raise ProxyError(\n                \"INVALID_TICKET\",\n                \"No proxy ticket found for this HttpRequest object\"\n            )\n        else:\n            client = get_cas_client(service_url=service, request=request)\n            try:\n                return client.get_proxy_ticket(pgt)\n            # change CASError to ProxyError nicely\n            except CASError as error:\n                raise ProxyError(*error.args)\n            # just embed other errors\n            except Exception as e:\n                raise ProxyError(e)", "response": "Retrieve a Proxy Ticket for a given HttpRequest object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a CAS entry.", "response": "def get(self, request):\n        \"\"\"\n        Forwards to CAS login URL or verifies CAS ticket\n\n        :param request:\n        :return:\n        \"\"\"\n        next_page = request.GET.get('next')\n        required = request.GET.get('required', False)\n\n        service_url = get_service_url(request, next_page)\n        client = get_cas_client(service_url=service_url, request=request)\n\n        if not next_page and settings.CAS_STORE_NEXT and 'CASNEXT' in request.session:\n            next_page = request.session['CASNEXT']\n            del request.session['CASNEXT']\n\n        if not next_page:\n            next_page = get_redirect_url(request)\n\n        if request.user.is_authenticated:\n            if settings.CAS_LOGGED_MSG is not None:\n                message = settings.CAS_LOGGED_MSG % request.user.get_username()\n                messages.success(request, message)\n            return self.successful_login(request=request, next_page=next_page)\n\n        ticket = request.GET.get('ticket')\n        if ticket:\n            user = authenticate(ticket=ticket,\n                                service=service_url,\n                                request=request)\n            pgtiou = request.session.get(\"pgtiou\")\n            if user is not None:\n                auth_login(request, user)\n                if not request.session.exists(request.session.session_key):\n                    request.session.create()\n                SessionTicket.objects.create(\n                    session_key=request.session.session_key,\n                    ticket=ticket\n                )\n\n                if pgtiou and settings.CAS_PROXY_CALLBACK:\n                    # Delete old PGT\n                    ProxyGrantingTicket.objects.filter(\n                        user=user,\n                        session_key=request.session.session_key\n                    ).delete()\n                    # Set new PGT ticket\n                    try:\n                        pgt = ProxyGrantingTicket.objects.get(pgtiou=pgtiou)\n                        pgt.user = user\n                        pgt.session_key = request.session.session_key\n                        pgt.save()\n                    except ProxyGrantingTicket.DoesNotExist:\n                        pass\n\n                if settings.CAS_LOGIN_MSG is not None:\n                    name = user.get_username()\n                    message = settings.CAS_LOGIN_MSG % name\n                    messages.success(request, message)\n                return self.successful_login(request=request, next_page=next_page)\n            elif settings.CAS_RETRY_LOGIN or required:\n                return HttpResponseRedirect(client.get_login_url())\n            else:\n                raise PermissionDenied(_('Login failed.'))\n        else:\n            if settings.CAS_STORE_NEXT:\n                request.session['CASNEXT'] = next_page\n            return HttpResponseRedirect(client.get_login_url())"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nredirects to CAS logout page :param request: :return:", "response": "def get(self, request):\n        \"\"\"\n        Redirects to CAS logout page\n\n        :param request:\n        :return:\n        \"\"\"\n        next_page = request.GET.get('next')\n\n        # try to find the ticket matching current session for logout signal\n        try:\n            st = SessionTicket.objects.get(session_key=request.session.session_key)\n            ticket = st.ticket\n        except SessionTicket.DoesNotExist:\n            ticket = None\n        # send logout signal\n        cas_user_logout.send(\n            sender=\"manual\",\n            user=request.user,\n            session=request.session,\n            ticket=ticket,\n        )\n        auth_logout(request)\n        # clean current session ProxyGrantingTicket and SessionTicket\n        ProxyGrantingTicket.objects.filter(session_key=request.session.session_key).delete()\n        SessionTicket.objects.filter(session_key=request.session.session_key).delete()\n        next_page = next_page or get_redirect_url(request)\n        if settings.CAS_LOGOUT_COMPLETELY:\n            protocol = get_protocol(request)\n            host = request.get_host()\n            redirect_url = urllib_parse.urlunparse(\n                (protocol, host, next_page, '', '', ''),\n            )\n            client = get_cas_client(request=request)\n            return HttpResponseRedirect(client.get_logout_url(redirect_url))\n        else:\n            # This is in most cases pointless if not CAS_RENEW is set. The user will\n            # simply be logged in again on next request requiring authorization.\n            return HttpResponseRedirect(next_page)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreset the state of the state machine.", "response": "def reset(self, document, parent, level):\n        \"\"\"Reset the state of state machine.\n\n        After reset, self and self.state can be used to\n        passed to docutils.parsers.rst.Directive.run\n\n        Parameters\n        ----------\n        document: docutils document\n            Current document of the node.\n        parent: parent node\n            Parent node that will be used to interpret role and directives.\n        level: int\n            Current section level.\n        \"\"\"\n        self.language = languages.get_language(\n            document.settings.language_code)\n        # setup memo\n        self.memo.document = document\n        self.memo.reporter = document.reporter\n        self.memo.language = self.language\n        self.memo.section_level = level\n        # setup inliner\n        if self.memo.inliner is None:\n            self.memo.inliner = Inliner()\n            self.memo.inliner.init_customizations(document.settings)\n        inliner = self.memo.inliner\n        inliner.reporter = document.reporter\n        inliner.document = document\n        inliner.language = self.language\n        inliner.parent = parent\n        # setup self\n        self.document = document\n        self.reporter = self.memo.reporter\n        self.node = parent\n        self.state.runtime_init()\n        self.input_lines = document['source']"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run_directive(self, name,\n                      arguments=None,\n                      options=None,\n                      content=None):\n        \"\"\"Generate directive node given arguments.\n\n        Parameters\n        ----------\n        name : str\n            name of directive.\n        arguments : list\n            list of positional arguments.\n        options : dict\n            key value arguments.\n        content : content\n            content of the directive\n\n        Returns\n        -------\n        node : docutil Node\n            Node generated by the arguments.\n        \"\"\"\n        if options is None:\n            options = {}\n        if content is None:\n            content = []\n        if arguments is None:\n            arguments = []\n        direc, _ = directive(name, self.language, self.document)\n        direc = direc(name=name,\n                      arguments=arguments,\n                      options=options,\n                      content=content,\n                      lineno=self.node.line,\n                      content_offset=0,\n                      block_text='Dummy BlockText',\n                      state=self.state,\n                      state_machine=self)\n        return direc.run()", "response": "Generate a node from a directive given arguments."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run_role(self, name,\n                 options=None,\n                 content=None):\n        \"\"\"Generate a role node.\n\n        options : dict\n            key value arguments.\n        content : content\n            content of the directive\n\n        Returns\n        -------\n        node : docutil Node\n            Node generated by the arguments.\n        \"\"\"\n        if options is None:\n            options = {}\n        if content is None:\n            content = []\n        role_fn, _ = role(name,\n                          self.language,\n                          self.node.line,\n                          self.reporter)\n        vec, _ = role_fn(name,\n                         rawtext=str(content),\n                         text=str(content),\n                         lineno=self.node.line,\n                         inliner=self.memo.inliner,\n                         options=options,\n                         content=content)\n        assert len(vec) == 1, 'only support one list in role'\n        return vec[0]", "response": "Generate a role node."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndefault node depart handler for container nodes.", "response": "def default_depart(self, mdnode):\n        \"\"\"Default node depart handler\n\n        If there is a matching ``visit_<type>`` method for a container node,\n        then we should make sure to back up to it's parent element when the node\n        is exited.\n        \"\"\"\n        if mdnode.is_container():\n            fn_name = 'visit_{0}'.format(mdnode.t)\n            if not hasattr(self, fn_name):\n                warn(\"Container node skipped: type={0}\".format(mdnode.t))\n            else:\n                self.current_node = self.current_node.parent"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef depart_heading(self, _):\n        assert isinstance(self.current_node, nodes.title)\n        # The title node has a tree of text nodes, use the whole thing to\n        # determine the section id and names\n        text = self.current_node.astext()\n        if self.translate_section_name:\n            text = self.translate_section_name(text)\n        name = nodes.fully_normalize_name(text)\n        section = self.current_node.parent\n        section['names'].append(name)\n        self.document.note_implicit_target(section, section)\n        self.current_node = section", "response": "Finish establishing section\n        Wrap up title node and stick in the section node."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nanalyzing the ref block and return the information needed.", "response": "def parse_ref(self, ref):\n        \"\"\"Analyze the ref block, and return the information needed.\n\n        Parameters\n        ----------\n        ref : nodes.reference\n\n        Returns\n        -------\n        result : tuple of (str, str, str)\n            The returned result is tuple of (title, uri, docpath).\n            title is the display title of the ref.\n            uri is the html uri of to the ref after resolve.\n            docpath is the absolute document path to the document, if\n            the target corresponds to an internal document, this can bex None\n        \"\"\"\n        title = None\n        if len(ref.children) == 0:\n            title = ref['name'] if 'name' in ref else None\n        elif isinstance(ref.children[0], nodes.Text):\n            title = ref.children[0].astext()\n        uri = ref['refuri']\n        if uri.find('://') != -1:\n            return (title, uri, None)\n        anchor = None\n        arr = uri.split('#')\n        if len(arr) == 2:\n            anchor = arr[1]\n        if len(arr) > 2 or len(arr[0]) == 0:\n            return (title, uri, None)\n        uri = arr[0]\n\n        abspath = os.path.abspath(os.path.join(self.file_dir, uri))\n        relpath = os.path.relpath(abspath, self.root_dir)\n        suffix = abspath.rsplit('.', 1)\n        if len(suffix) == 2 and suffix[1] in AutoStructify.suffix_set and (\n                os.path.exists(abspath) and abspath.startswith(self.root_dir)):\n            # replace the path separator if running on non-UNIX environment\n            if os.path.sep != '/':\n                relpath = relpath.replace(os.path.sep, '/')\n            docpath = '/' + relpath.rsplit('.', 1)[0]\n            # rewrite suffix to html, this is suboptimal\n            uri = docpath + '.html'\n            if anchor is None:\n                return (title, uri, docpath)\n            else:\n                return (title, uri + '#' + anchor, None)\n        else:\n            # use url resolver\n            if self.url_resolver:\n                uri = self.url_resolver(relpath)\n            if anchor:\n                uri += '#' + anchor\n            return (title, uri, None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef auto_toc_tree(self, node):  # pylint: disable=too-many-branches\n        if not self.config['enable_auto_toc_tree']:\n            return None\n        # when auto_toc_tree_section is set\n        # only auto generate toctree under the specified section title\n        sec = self.config['auto_toc_tree_section']\n        if sec is not None:\n            if node.parent is None:\n                return None\n            title = None\n            if isinstance(node.parent, nodes.section):\n                child = node.parent.first_child_matching_class(nodes.title)\n                if child is not None:\n                    title = node.parent.children[child]\n            elif isinstance(node.parent, nodes.paragraph):\n                child = node.parent.parent.first_child_matching_class(nodes.title)\n                if child is not None:\n                    title = node.parent.parent.children[child]\n            if not title:\n                return None\n            if title.astext().strip() != sec:\n                return None\n\n        numbered = None\n        if isinstance(node, nodes.bullet_list):\n            numbered = 0\n        elif isinstance(node, nodes.enumerated_list):\n            numbered = 1\n\n        if numbered is None:\n            return None\n        refs = []\n        for nd in node.children[:]:\n            assert isinstance(nd, nodes.list_item)\n            if len(nd.children) != 1:\n                return None\n            par = nd.children[0]\n            if not isinstance(par, nodes.paragraph):\n                return None\n            if len(par.children) != 1:\n                return None\n            ref = par.children[0]\n            if isinstance(ref, addnodes.pending_xref):\n                ref = ref.children[0]\n            if not isinstance(ref, nodes.reference):\n                return None\n            title, uri, docpath = self.parse_ref(ref)\n            if title is None or uri.startswith('#'):\n                return None\n            if docpath:\n                refs.append((title, docpath))\n            else:\n                refs.append((title, uri))\n        self.state_machine.reset(self.document,\n                                 node.parent,\n                                 self.current_level)\n        return self.state_machine.run_directive(\n            'toctree',\n            options={'maxdepth': 1, 'numbered': numbered},\n            content=['%s <%s>' % (k, v) for k, v in refs])", "response": "Try to convert a list block to toctree in rst."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef auto_inline_code(self, node):\n        assert isinstance(node, nodes.literal)\n        if len(node.children) != 1:\n            return None\n        content = node.children[0]\n        if not isinstance(content, nodes.Text):\n            return None\n        content = content.astext().strip()\n        if content.startswith('$') and content.endswith('$'):\n            if not self.config['enable_inline_math']:\n                return None\n            content = content[1:-1]\n            self.state_machine.reset(self.document,\n                                     node.parent,\n                                     self.current_level)\n            return self.state_machine.run_role('math', content=content)\n        else:\n            return None", "response": "Try to auto generate nodes for inline literals."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef auto_code_block(self, node):\n        assert isinstance(node, nodes.literal_block)\n        original_node = node\n        if 'language' not in node:\n            return None\n        self.state_machine.reset(self.document,\n                                 node.parent,\n                                 self.current_level)\n        content = node.rawsource.split('\\n')\n        language = node['language']\n        if language == 'math':\n            if self.config['enable_math']:\n                return self.state_machine.run_directive(\n                    'math', content=content)\n        elif language == 'eval_rst':\n            if self.config['enable_eval_rst']:\n                # allow embed non section level rst\n                node = nodes.section()\n                self.state_machine.state.nested_parse(\n                    StringList(content, source=original_node.source),\n                    0, node=node, match_titles=True)\n                return node.children[:]\n        else:\n            match = re.search('[ ]?[\\w_-]+::.*', language)\n            if match:\n                parser = Parser()\n                new_doc = new_document(None, self.document.settings)\n                newsource = u'.. ' + match.group(0) + '\\n' + node.rawsource\n                parser.parse(newsource, new_doc)\n                return new_doc.children[:]\n            else:\n                return self.state_machine.run_directive(\n                    'code-block', arguments=[language],\n                    content=content)\n        return None", "response": "Try to automatically generate nodes for codeblock syntax."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntries to find replace node for current node.", "response": "def find_replace(self, node):\n        \"\"\"Try to find replace node for current node.\n\n        Parameters\n        ----------\n        node : docutil node\n            Node to find replacement for.\n\n        Returns\n        -------\n        nodes : node or list of node\n            The replacement nodes of current node.\n            Returns None if no replacement can be found.\n        \"\"\"\n        newnode = None\n        if isinstance(node, nodes.Sequential):\n            newnode = self.auto_toc_tree(node)\n        elif isinstance(node, nodes.literal_block):\n            newnode = self.auto_code_block(node)\n        elif isinstance(node, nodes.literal):\n            newnode = self.auto_inline_code(node)\n        return newnode"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef traverse(self, node):\n        old_level = self.current_level\n        if isinstance(node, nodes.section):\n            if 'level' in node:\n                self.current_level = node['level']\n        to_visit = []\n        to_replace = []\n        for c in node.children[:]:\n            newnode = self.find_replace(c)\n            if newnode is not None:\n                to_replace.append((c, newnode))\n            else:\n                to_visit.append(c)\n\n        for oldnode, newnodes in to_replace:\n            node.replace(oldnode, newnodes)\n\n        for child in to_visit:\n            self.traverse(child)\n        self.current_level = old_level", "response": "Traverse the document tree rooted at node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef apply(self):\n        source = self.document['source']\n\n        self.reporter.info('AutoStructify: %s' % source)\n\n        # only transform markdowns\n        if not source.endswith(tuple(self.config['commonmark_suffixes'])):\n            return\n\n        self.url_resolver = self.config['url_resolver']\n        assert callable(self.url_resolver)\n\n        self.state_machine = DummyStateMachine()\n        self.current_level = 0\n        self.file_dir = os.path.abspath(os.path.dirname(self.document['source']))\n        self.root_dir = os.path.abspath(self.document.settings.env.srcdir)\n        self.traverse(self.document)", "response": "Apply the transformation by configuration."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef correction(sentence, pos):\n    \"Most probable spelling correction for word.\"\n    word = sentence[pos]\n    cands = candidates(word)\n    if not cands:\n        cands = candidates(word, False)\n    if not cands:\n        return word\n    cands = sorted(cands, key=lambda w: P(w, sentence, pos), reverse=True)\n    cands = [c[0] for c in cands]\n    return cands", "response": "Most probable spelling correction for word."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef graph_from_dot_file(path, encoding=None):\n    with io.open(path, 'rt', encoding=encoding) as f:\n        s = f.read()\n    if not PY3:\n        s = unicode(s)\n    graphs = graph_from_dot_data(s)\n    return graphs", "response": "Load graphs from DOT file at path."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_string(self):\n\n\n        # RMF: special case defaults for node, edge and graph properties.\n        #\n        node = quote_if_necessary(self.obj_dict['name'])\n\n        node_attr = list()\n\n        for attr in sorted(self.obj_dict['attributes']):\n            value = self.obj_dict['attributes'][attr]\n            if value == '':\n                value = '\"\"'\n            if value is not None:\n                node_attr.append(\n                    '%s=%s' % (attr, quote_if_necessary(value) ) )\n            else:\n                node_attr.append( attr )\n\n\n        # No point in having nodes setting any defaults if the don't set\n        # any attributes...\n        #\n        if node in ('graph', 'node', 'edge') and len(node_attr) == 0:\n            return ''\n\n        node_attr = ', '.join(node_attr)\n\n        if node_attr:\n            node += ' [' + node_attr + ']'\n\n        return node + ';'", "response": "Return string representation of node in DOT language."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a node object to the graph. It takes a node object as its only argument and returns None.", "response": "def add_node(self, graph_node):\n        \"\"\"Adds a node object to the graph.\n\n        It takes a node object as its only argument and returns\n        None.\n        \"\"\"\n\n        if not isinstance(graph_node, Node):\n            raise TypeError(\n                'add_node() received ' +\n                'a non node class object: ' + str(graph_node))\n\n\n        node = self.get_node(graph_node.get_name())\n\n        if not node:\n\n            self.obj_dict['nodes'][graph_node.get_name()] = [\n                graph_node.obj_dict ]\n\n            #self.node_dict[graph_node.get_name()] = graph_node.attributes\n            graph_node.set_parent_graph(self.get_parent_graph())\n\n        else:\n\n            self.obj_dict['nodes'][graph_node.get_name()].append(\n                graph_node.obj_dict )\n\n        graph_node.set_sequence(self.get_next_sequence_number())"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting a node from the graph.", "response": "def del_node(self, name, index=None):\n        \"\"\"Delete a node from the graph.\n\n        Given a node's name all node(s) with that same name\n        will be deleted if 'index' is not specified or set\n        to None.\n        If there are several nodes with that same name and\n        'index' is given, only the node in that position\n        will be deleted.\n\n        'index' should be an integer specifying the position\n        of the node to delete. If index is larger than the\n        number of nodes with that name, no action is taken.\n\n        If nodes are deleted it returns True. If no action\n        is taken it returns False.\n        \"\"\"\n\n        if isinstance(name, Node):\n            name = name.get_name()\n\n        if name in self.obj_dict['nodes']:\n\n            if (index is not None and\n                index < len(self.obj_dict['nodes'][name])):\n                del self.obj_dict['nodes'][name][index]\n                return True\n            else:\n                del self.obj_dict['nodes'][name]\n                return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting an edge from the graph. Given an edge s source and destination the corresponding edge instance will be returned.", "response": "def get_edge(self, src_or_list, dst=None):\n        \"\"\"Retrieved an edge from the graph.\n\n        Given an edge's source and destination the corresponding\n        Edge instance(s) will be returned.\n\n        If one or more edges exist with that source and destination\n        a list of Edge instances is returned.\n        An empty list is returned otherwise.\n        \"\"\"\n\n        if isinstance( src_or_list, (list, tuple)) and dst is None:\n            edge_points = tuple(src_or_list)\n            edge_points_reverse = (edge_points[1], edge_points[0])\n        else:\n            edge_points = (src_or_list, dst)\n            edge_points_reverse = (dst, src_or_list)\n\n        match = list()\n\n        if edge_points in self.obj_dict['edges'] or (\n            self.get_top_graph_type() == 'graph' and\n            edge_points_reverse in self.obj_dict['edges']):\n\n            edges_obj_dict = self.obj_dict['edges'].get(\n                edge_points,\n                self.obj_dict['edges'].get( edge_points_reverse, None ))\n\n            for edge_obj_dict in edges_obj_dict:\n                match.append(\n                    Edge(edge_points[0],\n                         edge_points[1],\n                         obj_dict=edge_obj_dict))\n\n        return match"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving a subgraph from the graph. Given a subgraph s name the corresponding Subgraph instance will be returned.", "response": "def get_subgraph(self, name):\n        \"\"\"Retrieved a subgraph from the graph.\n\n        Given a subgraph's name the corresponding\n        Subgraph instance will be returned.\n\n        If one or more subgraphs exist with the same name, a list of\n        Subgraph instances is returned.\n        An empty list is returned otherwise.\n        \"\"\"\n\n        match = list()\n\n        if name in self.obj_dict['subgraphs']:\n\n            sgraphs_obj_dict = self.obj_dict['subgraphs'].get( name )\n\n            for obj_dict_list in sgraphs_obj_dict:\n                #match.extend( Subgraph( obj_dict = obj_d )\n                #             for obj_d in obj_dict_list )\n                match.append( Subgraph( obj_dict = obj_dict_list ) )\n\n        return match"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_subgraph_list(self):\n\n        sgraph_objs = list()\n\n        for sgraph in self.obj_dict['subgraphs']:\n                obj_dict_list = self.obj_dict['subgraphs'][sgraph]\n                sgraph_objs.extend(\n                    [Subgraph(obj_dict=obj_d)\n                     for obj_d in obj_dict_list])\n\n        return sgraph_objs", "response": "Get the list of Subgraph instances in the graph. This method returns the list of Subgraph instances in the graph."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_string(self):\n\n\n        graph = list()\n\n        if self.obj_dict.get('strict', None) is not None:\n\n            if (self == self.get_parent_graph() and\n                    self.obj_dict['strict']):\n\n                graph.append('strict ')\n\n        graph_type = self.obj_dict['type']\n        if (graph_type == 'subgraph' and\n                not self.obj_dict.get('show_keyword', True)):\n            graph_type = ''\n        s = '{type} {name} {{\\n'.format(\n            type=graph_type,\n            name=self.obj_dict['name'])\n        graph.append(s)\n\n        for attr in sorted(self.obj_dict['attributes']):\n\n            if self.obj_dict['attributes'].get(attr, None) is not None:\n\n                val = self.obj_dict['attributes'].get(attr)\n                if val == '':\n                    val = '\"\"'\n                if val is not None:\n                    graph.append('%s=%s' %\n                                 (attr, quote_if_necessary(val)))\n                else:\n                    graph.append( attr )\n\n                graph.append( ';\\n' )\n\n\n        edges_done = set()\n\n        edge_obj_dicts = list()\n        for k in self.obj_dict['edges']:\n            edge_obj_dicts.extend(self.obj_dict['edges'][k])\n\n        if edge_obj_dicts:\n            edge_src_set, edge_dst_set = list(zip(\n                *[obj['points'] for obj in edge_obj_dicts]))\n            edge_src_set, edge_dst_set = set(edge_src_set), set(edge_dst_set)\n        else:\n            edge_src_set, edge_dst_set = set(), set()\n\n        node_obj_dicts = list()\n        for k in self.obj_dict['nodes']:\n            node_obj_dicts.extend(self.obj_dict['nodes'][k])\n\n        sgraph_obj_dicts = list()\n        for k in self.obj_dict['subgraphs']:\n            sgraph_obj_dicts.extend(self.obj_dict['subgraphs'][k])\n\n\n        obj_list = [(obj['sequence'], obj)\n                    for obj in (edge_obj_dicts +\n                                node_obj_dicts + sgraph_obj_dicts) ]\n        obj_list.sort(key=lambda x: x[0])\n\n        for idx, obj in obj_list:\n\n            if obj['type'] == 'node':\n\n                node = Node(obj_dict=obj)\n\n                if self.obj_dict.get('suppress_disconnected', False):\n\n                    if (node.get_name() not in edge_src_set and\n                        node.get_name() not in edge_dst_set):\n\n                        continue\n\n                graph.append( node.to_string()+'\\n' )\n\n            elif obj['type'] == 'edge':\n\n                edge = Edge(obj_dict=obj)\n\n                if (self.obj_dict.get('simplify', False) and\n                        edge in edges_done):\n                    continue\n\n                graph.append( edge.to_string() + '\\n' )\n                edges_done.add(edge)\n\n            else:\n\n                sgraph = Subgraph(obj_dict=obj)\n\n                graph.append( sgraph.to_string()+'\\n' )\n\n        graph.append( '}\\n' )\n\n        return ''.join(graph)", "response": "Return string representation of the object in DOT language."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write(self, path, prog=None, format='raw', encoding=None):\n        if prog is None:\n            prog = self.prog\n        if format == 'raw':\n            s = self.to_string()\n            if not PY3:\n                s = unicode(s)\n            with io.open(path, mode='wt', encoding=encoding) as f:\n                f.write(s)\n        else:\n            s = self.create(prog, format, encoding=encoding)\n            with io.open(path, mode='wb') as f:\n                f.write(s)\n        return True", "response": "Writes a graph to a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create(self, prog=None, format='ps', encoding=None):\n\n        if prog is None:\n            prog = self.prog\n\n        assert prog is not None\n\n        if isinstance(prog, (list, tuple)):\n            prog, args = prog[0], prog[1:]\n        else:\n            args = []\n\n        # temp file\n        tmp_fd, tmp_name = tempfile.mkstemp()\n        os.close(tmp_fd)\n        self.write(tmp_name, encoding=encoding)\n        tmp_dir = os.path.dirname(tmp_name)\n\n        # For each of the image files...\n        for img in self.shape_files:\n            # Get its data\n            f = open(img, 'rb')\n            f_data = f.read()\n            f.close()\n            # And copy it under a file with the same name in\n            # the temporary directory\n            f = open(os.path.join(tmp_dir, os.path.basename(img)), 'wb')\n            f.write(f_data)\n            f.close()\n\n        arguments = ['-T{}'.format(format), ] + args + [tmp_name]\n\n        try:\n            stdout_data, stderr_data, process = call_graphviz(\n                program=prog,\n                arguments=arguments,\n                working_dir=tmp_dir,\n            )\n        except OSError as e:\n            if e.errno == errno.ENOENT:\n                args = list(e.args)\n                args[1] = '\"{prog}\" not found in path.'.format(\n                    prog=prog)\n                raise OSError(*args)\n            else:\n                raise\n\n        # clean file litter\n        for img in self.shape_files:\n            os.unlink(os.path.join(tmp_dir, os.path.basename(img)))\n\n        os.unlink(tmp_name)\n\n        if process.returncode != 0:\n            message = (\n                '\"{prog}\" with args {arguments} returned code: {code}\\n\\n'\n                'stdout, stderr:\\n {out}\\n{err}\\n'\n            ).format(\n                prog=prog,\n                arguments=arguments,\n                code=process.returncode,\n                out=stdout_data,\n                err=stderr_data,\n            )\n            print(message)\n\n        assert process.returncode == 0, process.returncode\n\n        return stdout_data", "response": "Creates and returns a binary image for the graph."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse DOT description in ( unicode ) string s.", "response": "def parse_dot_data(s):\n    \"\"\"Parse DOT description in (unicode) string `s`.\n\n    @return: Graphs that result from parsing.\n    @rtype: `list` of `pydot.Dot`\n    \"\"\"\n    global top_graphs\n    top_graphs = list()\n    try:\n        graphparser = graph_definition()\n        graphparser.parseWithTabs()\n        tokens = graphparser.parseString(s)\n        return list(tokens)\n    except ParseException as err:\n        print(\n            err.line +\n            \" \"*(err.column-1) + \"^\" +\n            err)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _priority(s):\n    if type(s) in (list, tuple, set, frozenset):\n        return ITERABLE\n    if type(s) is dict:\n        return DICT\n    if issubclass(type(s), type):\n        return TYPE\n    if hasattr(s, \"validate\"):\n        return VALIDATOR\n    if callable(s):\n        return CALLABLE\n    else:\n        return COMPARABLE", "response": "Return priority for a given object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the code of the current state of the object.", "response": "def code(self):\n        \"\"\"\n        Removes duplicates values in auto and error list.\n        parameters.\n        \"\"\"\n\n        def uniq(seq):\n            \"\"\"\n            Utility function that removes duplicate.\n            \"\"\"\n            seen = set()\n            seen_add = seen.add\n            # This way removes duplicates while preserving the order.\n            return [x for x in seq if x not in seen and not seen_add(x)]\n\n        data_set = uniq(i for i in self.autos if i is not None)\n        error_list = uniq(i for i in self.errors if i is not None)\n        if error_list:\n            return \"\\n\".join(error_list)\n        return \"\\n\".join(data_set)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nvalidate data using defined schemas ensuring all the keys in the data are valid.", "response": "def validate(self, data):\n        \"\"\"\n        Validate data using defined sub schema/expressions ensuring all\n        values are valid.\n        :param data: to be validated with sub defined schemas.\n        :return: returns validated data\n        \"\"\"\n        for s in [self._schema(s, error=self._error, ignore_extra_keys=self._ignore_extra_keys) for s in self._args]:\n            data = s.validate(data)\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nvalidate data using schema and expressions ensuring at least one value is valid.", "response": "def validate(self, data):\n        \"\"\"\n        Validate data using sub defined schema/expressions ensuring at least\n        one value is valid.\n        :param data: data to be validated by provided schema.\n        :return: return validated data if not validation\n        \"\"\"\n        autos, errors = [], []\n        for s in [self._schema(s, error=self._error, ignore_extra_keys=self._ignore_extra_keys) for s in self._args]:\n            try:\n                validation = s.validate(data)\n                self.match_count += 1\n                if self.match_count > 1 and self.only_one:\n                    break\n                return validation\n            except SchemaError as _x:\n                autos, errors = _x.autos, _x.errors\n        raise SchemaError(\n            [\"%r did not validate %r\" % (self, data)] + autos,\n            [self._error.format(data) if self._error else None] + errors,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef validate(self, data):\n        e = self._error\n\n        try:\n            if self._pattern.search(data):\n                return data\n            else:\n                raise SchemaError(\"%r does not match %r\" % (self, data), e)\n        except TypeError:\n            raise SchemaError(\"%r is not string nor buffer\" % data, e)", "response": "Validate the data using defined regex."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _dict_key_priority(s):\n        if isinstance(s, Hook):\n            return _priority(s._schema) - 0.5\n        if isinstance(s, Optional):\n            return _priority(s._schema) + 0.5\n        return _priority(s)", "response": "Return priority for a given key object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _prepend_schema_name(self, message):\n        if self._name:\n            message = \"{0!r} {1!s}\".format(self._name, message)\n        return message", "response": "Prepends the schema name to the error message."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef json_schema(self, schema_id=None, is_main_schema=True):\n        Schema = self.__class__\n        s = self._schema\n        i = self._ignore_extra_keys\n        flavor = _priority(s)\n\n        if flavor != DICT and is_main_schema:\n            raise ValueError(\"The main schema must be a dict.\")\n\n        if flavor == TYPE:\n            # Handle type\n            return {\"type\": {int: \"integer\", float: \"number\", bool: \"boolean\"}.get(s, \"string\")}\n        elif flavor == ITERABLE and len(s) == 1:\n            # Handle arrays of a single type or dict schema\n            return {\"type\": \"array\", \"items\": Schema(s[0]).json_schema(is_main_schema=False)}\n        elif isinstance(s, Or):\n            # Handle Or values\n            values = [Schema(or_key).json_schema(is_main_schema=False) for or_key in s._args]\n            any_of = []\n            for value in values:\n                if value not in any_of:\n                    any_of.append(value)\n            return {\"anyOf\": any_of}\n\n        if flavor != DICT:\n            # If not handled, do not check\n            return {}\n\n        if is_main_schema and not schema_id:\n            raise ValueError(\"schema_id is required.\")\n\n        # Handle dict\n        required_keys = []\n        expanded_schema = {}\n        for key in s:\n            if isinstance(key, Hook):\n                continue\n\n            if isinstance(s[key], Schema):\n                sub_schema = s[key]\n            else:\n                sub_schema = Schema(s[key], ignore_extra_keys=i)\n            sub_schema_json = sub_schema.json_schema(is_main_schema=False)\n\n            is_optional = False\n            if isinstance(key, Optional):\n                key = key._schema\n                is_optional = True\n\n            if isinstance(key, str):\n                if not is_optional:\n                    required_keys.append(key)\n                expanded_schema[key] = sub_schema_json\n            elif isinstance(key, Or):\n                for or_key in key._args:\n                    expanded_schema[or_key] = sub_schema_json\n        schema_dict = {\n            \"type\": \"object\",\n            \"properties\": expanded_schema,\n            \"required\": required_keys,\n            \"additionalProperties\": i,\n        }\n        if is_main_schema:\n            schema_dict.update({\"id\": schema_id, \"$schema\": \"http://json-schema.org/draft-07/schema#\"})\n        return schema_dict", "response": "Generate a draft -07 JSON schema dict representing the schema."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ls(self, directory=\"/\", long_format=True, recursive=False):\n\n        # Disabling for now, see https://github.com/adafruit/ampy/issues/55.\n        # # Make sure directory ends in a slash.\n        # if not directory.endswith(\"/\"):\n        #     directory += \"/\"\n\n        # Make sure directory starts with slash, for consistency.\n        if not directory.startswith(\"/\"):\n            directory = \"/\" + directory\n\n        command = \"\"\"\\\n                try:        \n                    import os\n                except ImportError:\n                    import uos as os\\n\"\"\"\n\n        if recursive:\n            command += \"\"\"\\\n                def listdir(directory):\n                    result = set()\n\n                    def _listdir(dir_or_file):\n                        try:\n                            # if its a directory, then it should provide some children.\n                            children = os.listdir(dir_or_file)\n                        except OSError:                        \n                            # probably a file. run stat() to confirm.\n                            os.stat(dir_or_file)\n                            result.add(dir_or_file) \n                        else:\n                            # probably a directory, add to result if empty.\n                            if children:\n                                # queue the children to be dealt with in next iteration.\n                                for child in children:\n                                    # create the full path.\n                                    if dir_or_file == '/':\n                                        next = dir_or_file + child\n                                    else:\n                                        next = dir_or_file + '/' + child\n                                    \n                                    _listdir(next)\n                            else:\n                                result.add(dir_or_file)                     \n\n                    _listdir(directory)\n                    return sorted(result)\\n\"\"\"\n        else:\n            command += \"\"\"\\\n                def listdir(directory):\n                    if directory == '/':                \n                        return sorted([directory + f for f in os.listdir(directory)])\n                    else:\n                        return sorted([directory + '/' + f for f in os.listdir(directory)])\\n\"\"\"\n\n        # Execute os.listdir() command on the board.\n        if long_format:\n            command += \"\"\"\n                r = []\n                for f in listdir('{0}'):\n                    size = os.stat(f)[6]                    \n                    r.append('{{0}} - {{1}} bytes'.format(f, size))\n                print(r)\n            \"\"\".format(\n                directory\n            )\n        else:\n            command += \"\"\"\n                print(listdir('{0}'))\n            \"\"\".format(\n                directory\n            )\n        self._pyboard.enter_raw_repl()\n        try:\n            out = self._pyboard.exec_(textwrap.dedent(command))\n        except PyboardError as ex:\n            # Check if this is an OSError #2, i.e. directory doesn't exist and\n            # rethrow it as something more descriptive.\n            if ex.args[2].decode(\"utf-8\").find(\"OSError: [Errno 2] ENOENT\") != -1:\n                raise RuntimeError(\"No such directory: {0}\".format(directory))\n            else:\n                raise ex\n        self._pyboard.exit_raw_repl()\n        # Parse the result list and return it.\n        return ast.literal_eval(out.decode(\"utf-8\"))", "response": "List the contents of the specified directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates the specified directory.", "response": "def mkdir(self, directory, exists_okay=False):\n        \"\"\"Create the specified directory.  Note this cannot create a recursive\n        hierarchy of directories, instead each one should be created separately.\n        \"\"\"\n        # Execute os.mkdir command on the board.\n        command = \"\"\"\n            try:\n                import os\n            except ImportError:\n                import uos as os\n            os.mkdir('{0}')\n        \"\"\".format(\n            directory\n        )\n        self._pyboard.enter_raw_repl()\n        try:\n            out = self._pyboard.exec_(textwrap.dedent(command))\n        except PyboardError as ex:\n            # Check if this is an OSError #17, i.e. directory already exists.\n            if ex.args[2].decode(\"utf-8\").find(\"OSError: [Errno 17] EEXIST\") != -1:\n                if not exists_okay:\n                    raise DirectoryExistsError(\n                        \"Directory already exists: {0}\".format(directory)\n                    )\n            else:\n                raise ex\n        self._pyboard.exit_raw_repl()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef put(self, filename, data):\n        # Open the file for writing on the board and write chunks of data.\n        self._pyboard.enter_raw_repl()\n        self._pyboard.exec_(\"f = open('{0}', 'wb')\".format(filename))\n        size = len(data)\n        # Loop through and write a buffer size chunk of data at a time.\n        for i in range(0, size, BUFFER_SIZE):\n            chunk_size = min(BUFFER_SIZE, size - i)\n            chunk = repr(data[i : i + chunk_size])\n            # Make sure to send explicit byte strings (handles python 2 compatibility).\n            if not chunk.startswith(\"b\"):\n                chunk = \"b\" + chunk\n            self._pyboard.exec_(\"f.write({0})\".format(chunk))\n        self._pyboard.exec_(\"f.close()\")\n        self._pyboard.exit_raw_repl()", "response": "Create or update the specified file with the provided data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove the specified file or directory.", "response": "def rm(self, filename):\n        \"\"\"Remove the specified file or directory.\"\"\"\n        command = \"\"\"\n            try:\n                import os\n            except ImportError:\n                import uos as os\n            os.remove('{0}')\n        \"\"\".format(\n            filename\n        )\n        self._pyboard.enter_raw_repl()\n        try:\n            out = self._pyboard.exec_(textwrap.dedent(command))\n        except PyboardError as ex:\n            message = ex.args[2].decode(\"utf-8\")\n            # Check if this is an OSError #2, i.e. file/directory doesn't exist\n            # and rethrow it as something more descriptive.\n            if message.find(\"OSError: [Errno 2] ENOENT\") != -1:\n                raise RuntimeError(\"No such file/directory: {0}\".format(filename))\n            # Check for OSError #13, the directory isn't empty.\n            if message.find(\"OSError: [Errno 13] EACCES\") != -1:\n                raise RuntimeError(\"Directory is not empty: {0}\".format(filename))\n            else:\n                raise ex\n        self._pyboard.exit_raw_repl()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rmdir(self, directory, missing_okay=False):\n        # Build a script to walk an entire directory structure and delete every\n        # file and subfolder.  This is tricky because MicroPython has no os.walk\n        # or similar function to walk folders, so this code does it manually\n        # with recursion and changing directories.  For each directory it lists\n        # the files and deletes everything it can, i.e. all the files.  Then\n        # it lists the files again and assumes they are directories (since they\n        # couldn't be deleted in the first pass) and recursively clears those\n        # subdirectories.  Finally when finished clearing all the children the\n        # parent directory is deleted.\n        command = \"\"\"\n            try:\n                import os\n            except ImportError:\n                import uos as os\n            def rmdir(directory):\n                os.chdir(directory)\n                for f in os.listdir():\n                    try:\n                        os.remove(f)\n                    except OSError:\n                        pass\n                for f in os.listdir():\n                    rmdir(f)\n                os.chdir('..')\n                os.rmdir(directory)\n            rmdir('{0}')\n        \"\"\".format(\n            directory\n        )\n        self._pyboard.enter_raw_repl()\n        try:\n            out = self._pyboard.exec_(textwrap.dedent(command))\n        except PyboardError as ex:\n            message = ex.args[2].decode(\"utf-8\")\n            # Check if this is an OSError #2, i.e. directory doesn't exist\n            # and rethrow it as something more descriptive.\n            if message.find(\"OSError: [Errno 2] ENOENT\") != -1:\n                if not missing_okay:\n                    raise RuntimeError(\"No such directory: {0}\".format(directory))\n            else:\n                raise ex\n        self._pyboard.exit_raw_repl()", "response": "Forcefully remove the specified directory and all its children."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run(self, filename, wait_output=True):\n        self._pyboard.enter_raw_repl()\n        out = None\n        if wait_output:\n            # Run the file and wait for output to return.\n            out = self._pyboard.execfile(filename)\n        else:\n            # Read the file and run it using lower level pyboard functions that\n            # won't wait for it to finish or return output.\n            with open(filename, \"rb\") as infile:\n                self._pyboard.exec_raw_no_follow(infile.read())\n        self._pyboard.exit_raw_repl()\n        return out", "response": "Run the provided script and return its output."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cli(port, baud, delay):\n    global _board\n    # On Windows fix the COM port path name for ports above 9 (see comment in\n    # windows_full_port_name function).\n    if platform.system() == \"Windows\":\n        port = windows_full_port_name(port)\n    _board = pyboard.Pyboard(port, baudrate=baud, rawdelay=delay)", "response": "A simple micropython tool to control the MicroPython boards on a serial connection."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get(remote_file, local_file):\n    # Get the file contents.\n    board_files = files.Files(_board)\n    contents = board_files.get(remote_file)\n    # Print the file out if no local file was provided, otherwise save it.\n    if local_file is None:\n        print(contents.decode(\"utf-8\"))\n    else:\n        local_file.write(contents)", "response": "Retrieve a file from the board and print it out if local_file is None."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef mkdir(directory, exists_okay):\n    # Run the mkdir command.\n    board_files = files.Files(_board)\n    board_files.mkdir(directory, exists_okay=exists_okay)", "response": "Create a directory on the board."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ls(directory, long_format, recursive):\n    # List each file/directory on a separate line.\n    board_files = files.Files(_board)\n    for f in board_files.ls(directory, long_format=long_format, recursive=recursive):\n        print(f)", "response": "List contents of a directory on the board."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef put(local, remote):\n    # Use the local filename if no remote filename is provided.\n    if remote is None:\n        remote = os.path.basename(os.path.abspath(local))\n    # Check if path is a folder and do recursive copy of everything inside it.\n    # Otherwise it's a file and should simply be copied over.\n    if os.path.isdir(local):\n        # Directory copy, create the directory and walk all children to copy\n        # over the files.\n        board_files = files.Files(_board)\n        for parent, child_dirs, child_files in os.walk(local):\n            # Create board filesystem absolute path to parent directory.\n            remote_parent = posixpath.normpath(\n                posixpath.join(remote, os.path.relpath(parent, local))\n            )\n            try:\n                # Create remote parent directory.\n                board_files.mkdir(remote_parent)\n                # Loop through all the files and put them on the board too.\n                for filename in child_files:\n                    with open(os.path.join(parent, filename), \"rb\") as infile:\n                        remote_filename = posixpath.join(remote_parent, filename)\n                        board_files.put(remote_filename, infile.read())\n            except files.DirectoryExistsError:\n                # Ignore errors for directories that already exist.\n                pass\n\n    else:\n        # File copy, open the file and copy its contents to the board.\n        # Put the file on the board.\n        with open(local, \"rb\") as infile:\n            board_files = files.Files(_board)\n            board_files.put(remote, infile.read())", "response": "Put a file or folder to the board."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run(local_file, no_output):\n    # Run the provided file and print its output.\n    board_files = files.Files(_board)\n    try:\n        output = board_files.run(local_file, not no_output)\n        if output is not None:\n            print(output.decode(\"utf-8\"), end=\"\")\n    except IOError:\n        click.echo(\n            \"Failed to find or read input file: {0}\".format(local_file), err=True\n        )", "response": "Run a script and print its output."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nperform soft reset of the board.", "response": "def reset(mode):\n    \"\"\"Perform soft reset/reboot of the board.\n\n    Will connect to the board and perform a reset.  Depending on the board\n    and firmware, several different types of reset may be supported.\n\n      ampy --port /board/serial/port reset\n    \"\"\"\n    _board.enter_raw_repl()\n    if mode == \"SOFT\":\n        _board.exit_raw_repl()\n        return\n\n    _board.exec_(\n        \"\"\"if 1:\n        def on_next_reset(x):\n            try:\n                import microcontroller\n            except:\n                if x == 'NORMAL': return ''\n                return 'Reset mode only supported on CircuitPython'\n            try:\n                microcontroller.on_next_reset(getattr(microcontroller.RunMode, x))\n            except ValueError as e:\n                return str(e)\n            return ''\n        def reset():\n            try:\n                import microcontroller\n            except:\n                import machine as microcontroller\n            microcontroller.reset()\n    \"\"\"\n    )\n    r = _board.eval(\"on_next_reset({})\".format(repr(mode)))\n    print(\"here we are\", repr(r))\n    if r:\n        click.echo(r, err=True)\n        return\n\n    try:\n        _board.exec_(\"reset()\")\n    except serial.serialutil.SerialException as e:\n        # An error is expected to occur, as the board should disconnect from\n        # serial when restarted via microcontroller.reset()\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ean8(self, data, **kwargs):\n        if not re.match(r'\\d{8}', data):\n            raise ValueError('JAN-8/EAN-8 symbology requires 8 digits of data; '\n                    'got {:d} digits: {!r}'.format(len(data), data))\n        barcode.validate_barcode_args(**kwargs)\n        return self._ean8_impl(data, **kwargs)", "response": "Render given data as JAN - 8 / EAN - 8 barcode symbology."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ean13(self, data, **kwargs):\n        if not re.match(r'\\d{13}', data):\n            raise ValueError('JAN-13/EAN-13 symbology requires 13 digits of '\n                    'data; got {:d} digits: {!r}'.format(len(data), data))\n        barcode.validate_barcode_args(**kwargs)\n        return self._ean13_impl(data, **kwargs)", "response": "Render given data as JAN - 13 barcode symbology."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef code128(self, data, **kwargs):\n        if not re.match(r'^[\\x20-\\x7F]+$', data):\n            raise ValueError('Invalid Code 128 symbology. Code 128 can encode '\n                    'any ASCII character ranging from 32 (20h) to 127 (7Fh); '\n                    'got {!r}'.format(data))\n        codeset = kwargs.pop('codeset', barcode.CODE128_A)\n        barcode.validate_barcode_args(**kwargs)\n        return self._code128_impl(data, codeset=codeset, **kwargs)", "response": "Renders given data as **Code 128 ** barcode symbology."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrender given data as QRCode", "response": "def qrcode(self, data, **kwargs):\n        \"\"\"\n        Render given ``data`` as `QRCode <http://www.qrcode.com/en/>`_.\n        \"\"\"\n        barcode.validate_qrcode_args(**kwargs)\n        return self._qrcode_impl(data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cut(self, partial=True):\n        if self.hardware_features.get(feature.CUTTER, False):\n            # TODO: implement hardware alternative for unavailable features\n            # For example:\n            #\n            #       self.hardware_alternatives.get('cutter-full-cut')(self)\n            #\n            # So, implementations or end-user-applications can replace\n            # certain hardware functionalites, based on available features.\n            #\n            # The above mapping can replace full cuts with line feeds for\n            # printer hardware that do not have an automatic paper cutter:\n            #\n            #       self.hardware_alternatives.update({\n            #               # skip 7 lines if we do not have a paper cutter\n            #               'cutter-full-cut': lambda impl: impl.lf(7)\n            #           })\n            #\n            param = '\\x01' if partial else '\\x00'\n            self.device.write('\\x1D\\x56' + param)", "response": "Perform a full or partial cut of the current entry."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef kick_drawer(self, port=0, **kwargs):\n        if self.hardware_features.get(feature.CASHDRAWER_PORTS, False):\n            # if feature is available assume at least one port is available\n            max_ports = self.hardware_features.get(\n                    feature.CASHDRAWER_AVAILABLE_PORTS, 1)\n\n            if port not in range(max_ports):\n                raise CashDrawerException('invalid cash drawer port: {!r} '\n                        '(available ports are {!r})'.format(\n                                port, range(max_ports)))\n\n            return self._kick_drawer_impl(port=port, **kwargs)", "response": "Kick drawer connected to the given port."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef gs_k_barcode(symbology, data, **kwargs):\n    commands = gs_k_barcode_configure(**kwargs)\n\n    if symbology in (\n            NUL_TERM_UPC_A,\n            NUL_TERM_UPC_E,\n            NUL_TERM_JAN13_EAN13,\n            NUL_TERM_JAN8_EAN8,\n            NUL_TERM_CODE39,\n            NUL_TERM_ITF,\n            NUL_TERM_CODABAR_NW_7,):\n        # null-terminated\n        commands.append('\\x1D\\x6B{}{}\\x00'.format(symbology, data))\n\n    else:\n        commands.append('\\x1D\\x6B{}{}{}\\x00'.format(\n                symbology, chr(len(data)), data))\n\n    return commands", "response": "Builds a standard ESC or POS barcode command set."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef scan_ports():\n    names = []\n    for number in range(256):\n        try:\n            # attempt attr `name` for PySerial >= 2.5-rc2,\n            # where attr `portstr` is for older PySerial versions\n            s = pyserial.Serial(number)\n            name = getattr(s, 'name', getattr(s, 'portstr', str(number)))\n            names.append((number, name,))\n        except:\n            pass\n    return tuple(names)", "response": "Scan for known serial ports available in the underling system."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns supported baud rates in a Django - like choices tuple.", "response": "def get_baudrates():\n    \"\"\"\n    Returns supported baud rates in a Django-like choices tuples.\n    \"\"\"\n    baudrates = []\n    s = pyserial.Serial()\n    for name, value in s.getSupportedBaudrates():\n        baudrates.append((value, name,))\n    return tuple(baudrates)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_databits():\n    databits = []\n    s = pyserial.Serial()\n    for name, value in s.getSupportedByteSizes():\n        databits.append((value, name,))\n    return tuple(databits)", "response": "Returns supported byte sizes in a Django - like choices tuples."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a Django - like choices tuple of the supported stop bit lengths in a Django - like choices tuples.", "response": "def get_stopbits():\n    \"\"\"\n    Returns supported stop bit lengths in a Django-like choices tuples.\n    \"\"\"\n    stopbits = []\n    s = pyserial.Serial()\n    for name, value in s.getSupportedStopbits():\n        stopbits.append((value, name,))\n    return tuple(stopbits)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_parities():\n    parities = []\n    s = pyserial.Serial()\n    for name, value in s.getSupportedParities():\n        parities.append((value, name,))\n    return tuple(parities)", "response": "Returns supported parities in a Django - like choices tuples."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_connection(self, **kwargs):\n        if self.is_rtscts():\n            return RTSCTSConnection(self, **kwargs)\n\n        if self.is_dsrdtr():\n            return DSRDTRConnection(self, **kwargs)\n\n        else:\n            raise RuntimeError('Serial protocol \"%s\" is not available.' % (\n                    self.protocol))", "response": "Returns a serial connection implementation suitable for the specified\n            protocol."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _fix_port_assignment(self):\n\n        port_name = ''\n        port_number = None\n\n        if self._port:\n            if isinstance(self._port, int):\n                port_number = self._port\n                port_name = get_port_name(port_number)\n            elif isinstance(self._port, string_types):\n                port_name = self._port\n                port_number = get_port_number(port_name)\n            else:\n                raise ValueError('Cannot assign port name/number '\n                        'based on port assignment type %r' % self._port)\n\n            self._port = port_number\n            self._portname = port_name or ''", "response": "This method attempts to fix the port assignment of the related object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef as_from(value):\n        keys = ['port','baudrate','databits','stopbits','parity','protocol']\n        values = value.replace(',', ':').split(':')\n\n        if len(values) == 5:\n            values.append(RTSCTS)\n\n        if len(keys) != len(values):\n            raise ValueError('Unknown serial port string format: %s '\n                    '(expecting something like \"COM1:9600,8,1,N,RTSCTS\")' % (\n                    value,))\n\n        kwargs = dict(zip(keys, values))\n        kwargs['baudrate'] = int(kwargs['baudrate'])\n        kwargs['databits'] = int(kwargs['databits'])\n        kwargs['stopbits'] = int(kwargs['stopbits'])\n\n        return SerialSettings(**kwargs)", "response": "Constructs an instance of : class : SerialSettings from a string representation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting data to serial port.", "response": "def write(self, data):\n        \"\"\"Write data to serial port.\"\"\"\n        for chunk in chunks(data, 512):\n            self.wait_to_write()\n            self.comport.write(chunk)\n        self.comport.flush()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread data from serial port and returns a bytearray.", "response": "def read(self):\n        \"\"\"Read data from serial port and returns a ``bytearray``.\"\"\"\n        data = bytearray()\n        while True:\n            incoming_bytes = self.comport.inWaiting()\n            if incoming_bytes == 0:\n                break\n            else:\n                content = self.comport.read(size=incoming_bytes)\n                data.extend(bytearray(content))\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write(self, data):\n        self.device.write(data)\n        if self.auto_flush:\n            self.flush()", "response": "Print any command sent in raw format."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the value of mnemonic. Raises ValueError if mnemonic is not in the alphabetical table.", "response": "def value(mnemonic):\n    \"\"\"\n    Returns the value of mnemonic (case-insensitive). Raises ``ValueError`` if\n    the given mnemonic does not exists.\n    \"\"\"\n    codes, mnemonics = zip(*MNEMONIC_TABLE)\n    return mnemonics.index(mnemonic.upper())"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef backoff(\n        max_tries=constants.BACKOFF_DEFAULT_MAXTRIES,\n        delay=constants.BACKOFF_DEFAULT_DELAY,\n        factor=constants.BACKOFF_DEFAULT_FACTOR,\n        exceptions=None):\n    \"\"\"Implements an exponential backoff decorator which will retry decorated\n    function upon given exceptions. This implementation is based on\n    `Retry <https://wiki.python.org/moin/PythonDecoratorLibrary#Retry>`_ from\n    the *Python Decorator Library*.\n\n    :param int max_tries: Number of tries before give up. Defaults to\n        :const:`~escpos.constants.BACKOFF_DEFAULT_MAXTRIES`.\n\n    :param int delay: Delay between retries (in seconds). Defaults to\n        :const:`~escpos.constants.BACKOFF_DEFAULT_DELAY`.\n\n    :param int factor: Multiply factor in which delay will be increased for the\n        next retry. Defaults to :const:`~escpos.constants.BACKOFF_DEFAULT_FACTOR`.\n\n    :param exceptions: Tuple of exception types to catch that triggers retry.\n        Any exception not listed will break the decorator and retry routines\n        will not run.\n\n    :type exceptions: tuple[Exception]\n\n    \"\"\"\n    if max_tries <= 0:\n        raise ValueError('Max tries must be greater than 0; got {!r}'.format(max_tries))\n\n    if delay <= 0:\n        raise ValueError('Delay must be greater than 0; got {!r}'.format(delay))\n\n    if factor <= 1:\n        raise ValueError('Backoff factor must be greater than 1; got {!r}'.format(factor))\n\n    def outter(f):\n        def inner(*args, **kwargs):\n            m_max_tries, m_delay = max_tries, delay # make mutable\n            while m_max_tries > 0:\n                try:\n                    retval = f(*args, **kwargs)\n                except exceptions:\n                    logger.exception('backoff retry for: %r (max_tries=%r, delay=%r, '\n                            'factor=%r, exceptions=%r)', f, max_tries, delay, factor, exceptions)\n                    m_max_tries -= 1 # consume an attempt\n                    if m_max_tries <= 0:\n                        raise # run out of tries\n                    time.sleep(m_delay) # wait...\n                    m_delay *= factor # make future wait longer\n                else:\n                    # we're done without errors\n                    return retval\n        return inner\n    return outter", "response": "Creates a new exponential backoff function which retries the given function on given exceptions."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new instance of a specific a .", "response": "def create(cls, settings):\n        \"\"\"Create a :class:`BluetoothConnection`:\n\n        .. sourcecode:: python\n\n            from escpos import BluetoothConnection\n            from escpos.impl.epson import GenericESCPOS\n\n            conn = BluetoothConnection.create('00:01:02:03:04:05')\n            printer = GenericESCPOS(conn)\n            printer.init()\n            printer.text('Hello World!')\n\n        :param str settings: Bluetooth settings. You must specify bluetooth\n            address as six hexadecimal octets, like ``00:01:02:03:04:05``.\n            You can also specify a port number after address using a forward\n            slash, like ``00:01:02:03:04:05/2``. If there is no port number,\n            this method will use SPD (*Service Discovery Protocol*) to find a\n            suitable port number for the given address at RFCOMM protocol.\n\n        :raises BluetoothPortDiscoveryError: If port is not specified and the\n            algorithm cannot find a RFCOMM port for the given address.\n        \"\"\"\n        fields = settings.rsplit('/', 1)\n        address = fields[0]\n\n        if len(fields) == 1:\n            port = find_rfcomm_port(address)\n        else:\n            try:\n                port = int(fields[1])\n            except ValueError:\n                raise BluetoothConnectionError('Invalid settings: {!r}'.format(settings))\n\n        return cls(address, port=port)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef configure(filename=None):\n    global retry\n\n    if getattr(configure, '_configured', False):\n        return\n\n    filename = filename or DEFAULT_CONFIG_FILENAME\n    _ensure_directory(filename)\n\n    parser = SafeConfigParser()\n\n    if os.path.isfile(filename):\n        with open(filename, 'r') as fp:\n            parser.readfp(fp)\n\n    if not parser.has_section(RETRY_SECTION):\n        parser.add_section(RETRY_SECTION)\n        parser.set(RETRY_SECTION, 'max_tries', str(constants.BACKOFF_DEFAULT_MAXTRIES))\n        parser.set(RETRY_SECTION, 'delay', str(constants.BACKOFF_DEFAULT_DELAY))\n        parser.set(RETRY_SECTION, 'factor', str(constants.BACKOFF_DEFAULT_FACTOR))\n\n        with open(filename, 'wb') as fp:\n            parser.write(fp)\n\n    retry = RetrySettings(\n            max_tries=parser.getint(RETRY_SECTION, 'max_tries'),\n            delay=parser.getint(RETRY_SECTION, 'delay'),\n            factor=parser.getint(RETRY_SECTION, 'factor'))\n\n    setattr(configure, '_configured', True)\n    setattr(configure, '_configured_filename', filename)", "response": "This function configures the current configuration file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_implementations(sort_by=None):\n    impls = [_describe_impl(t) for t in _list_impls()]\n    if sort_by:\n        impls.sort(key=attrgetter(sort_by))\n    return tuple(impls)", "response": "Returns a tuple of all known ESCPOS implementations."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_value_in(constants_group, value):\n    for const_value, label in constants_group:\n        if const_value == value:\n            return True\n    return False", "response": "Checks whether a value is found in the given constants group."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run_interactive(query, editor=None, just_count=False, default_no=False):\n    global yes_to_all\n\n    # Load start from bookmark, if appropriate.\n    bookmark = _load_bookmark()\n    if bookmark:\n        print('Resume where you left off, at %s (y/n)? '\n              % str(bookmark), end=' ')\n        sys.stdout.flush()\n        if (_prompt(default='y') == 'y'):\n            query.start_position = bookmark\n\n    # Okay, enough of this foolishness of computing start and end.\n    # Let's ask the user about some one line diffs!\n    print('Searching for first instance...')\n    suggestions = query.generate_patches()\n\n    if just_count:\n        for count, _ in enumerate(suggestions):\n            terminal.terminal_move_to_beginning_of_line()\n            print(count, end=\" \")\n            sys.stdout.flush()  # since print statement ends in comma\n        print()\n        return\n\n    for patch in suggestions:\n        _save_bookmark(patch.start_position)\n        _ask_about_patch(patch, editor, default_no)\n        print('Searching...')\n    _delete_bookmark()\n    if yes_to_all:\n        terminal.terminal_clear()\n        print(\n            \"You MUST indicate in your code review:\"\n            \" \\\"codemod with 'Yes to all'\\\".\"\n            \"Make sure you and other people review the changes.\\n\\n\"\n            \"With great power, comes great responsibility.\"\n        )", "response": "Runs the interactive command."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef line_transformation_suggestor(line_transformation, line_filter=None):\n    def suggestor(lines):\n        for line_number, line in enumerate(lines):\n            if line_filter and not line_filter(line):\n                continue\n            candidate = line_transformation(line)\n            if candidate is None:\n                yield Patch(line_number)\n            else:\n                yield Patch(line_number, new_lines=[candidate])\n    return suggestor", "response": "Returns a suggestor that returns a list of patches where suggestions are the result of line - by - line transformations."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef multiline_regex_suggestor(regex, substitution=None, ignore_case=False):\n    if isinstance(regex, str):\n        if ignore_case is False:\n            regex = re.compile(regex, re.DOTALL)\n        else:\n            regex = re.compile(regex, re.DOTALL | re.IGNORECASE)\n\n    if isinstance(substitution, str):\n        def substitution_func(match):\n            return match.expand(substitution)\n    else:\n        substitution_func = substitution\n\n    def suggestor(lines):\n        pos = 0\n        while True:\n            match = regex.search(''.join(lines), pos)\n            if not match:\n                break\n            start_row, start_col = _index_to_row_col(lines, match.start())\n            end_row, end_col = _index_to_row_col(lines, match.end() - 1)\n\n            if substitution is None:\n                new_lines = None\n            else:\n                # TODO: ugh, this is hacky.  Clearly I need to rewrite\n                # this to use\n                # character-level patches, rather than line-level patches.\n                new_lines = substitution_func(match)\n                if new_lines is not None:\n                    new_lines = ''.join((\n                        lines[start_row][:start_col],\n                        new_lines,\n                        lines[end_row][end_col + 1:]\n                    ))\n\n            yield Patch(\n                start_line_number=start_row,\n                end_line_number=end_row + 1,\n                new_lines=new_lines\n            )\n            delta = 1 if new_lines is None else min(1, len(new_lines))\n            pos = match.start() + delta\n\n    return suggestor", "response": "Generates a suggestor function which returns a list of lines that match the given regex."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _index_to_row_col(lines, index):\n    if index < 0:\n        raise IndexError('negative index')\n    current_index = 0\n    for line_number, line in enumerate(lines):\n        line_length = len(line)\n        if current_index + line_length > index:\n            return line_number, index - current_index\n        current_index += line_length\n    raise IndexError('index %d out of range' % index)", "response": "r Returns the line number and column of the first line of the list that is at index."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprompt the user for a character.", "response": "def _prompt(letters='yn', default=None):\n    \"\"\"\n    Wait for the user to type a character (and hit Enter).  If the user enters\n    one of the characters in `letters`, return that character.  If the user\n    hits Enter without entering a character, and `default` is specified,\n    returns `default`.  Otherwise, asks the user to enter a character again.\n    \"\"\"\n    while True:\n        try:\n            input_text = sys.stdin.readline().strip()\n        except KeyboardInterrupt:\n            sys.exit(0)\n        if input_text and input_text in letters:\n            return input_text\n        if default is not None and input_text == '':\n            return default\n        print('Come again?')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of all patches matching this query.", "response": "def get_all_patches(self, dont_use_cache=False):\n        \"\"\"\n        Computes a list of all patches matching this query, though ignoreing\n        self.start_position and self.end_position.\n\n        @param dont_use_cache   If False, and get_all_patches has been called\n                                before, compute the list computed last time.\n        \"\"\"\n        if not dont_use_cache and self._all_patches_cache is not None:\n            return self._all_patches_cache\n\n        print(\n            'Computing full change list (since you specified a percentage)...'\n        ),\n        sys.stdout.flush()  # since print statement ends in comma\n\n        endless_query = self.clone()\n        endless_query.start_position = endless_query.end_position = None\n        self._all_patches_cache = list(endless_query.generate_patches())\n        return self._all_patches_cache"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef compute_percentile(self, percentage):\n        all_patches = self.get_all_patches()\n        return all_patches[\n            int(len(all_patches) * percentage / 100)\n        ].start_position", "response": "Compute the percentile of the current entry in the set of all patches."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate a list of patches for each file underneath that satisfy the given conditions given query conditions.", "response": "def generate_patches(self):\n        \"\"\"\n        Generates a list of patches for each file underneath\n        self.root_directory\n        that satisfy the given conditions given\n        query conditions, where patches for\n        each file are suggested by self.suggestor.\n        \"\"\"\n        start_pos = self.start_position or Position(None, None)\n        end_pos = self.end_position or Position(None, None)\n\n        path_list = Query._walk_directory(self.root_directory)\n        path_list = Query._sublist(path_list, start_pos.path, end_pos.path)\n        path_list = (\n            path for path in path_list if\n            Query._path_looks_like_code(path) and\n            (self.path_filter(path)) or\n            (self.inc_extensionless and helpers.is_extensionless(path))\n        )\n        for path in path_list:\n            try:\n                lines = list(open(path))\n            except (IOError, UnicodeDecodeError):\n                # If we can't open the file--perhaps it's a symlink whose\n                # destination no loner exists--then short-circuit.\n                continue\n\n            for patch in self.suggestor(lines):\n                if path == start_pos.path:\n                    if patch.start_line_number < start_pos.line_number:\n                        continue  # suggestion is pre-start_pos\n                if path == end_pos.path:\n                    if patch.end_line_number >= end_pos.line_number:\n                        break  # suggestion is post-end_pos\n\n                old_lines = lines[\n                    patch.start_line_number:patch.end_line_number]\n                if patch.new_lines is None or patch.new_lines != old_lines:\n                    patch.path = path\n                    yield patch\n                    # re-open file, in case contents changed\n                    lines[:] = list(open(path))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _walk_directory(root_directory):\n\n        paths = [os.path.join(root, name)\n                 for root, dirs, files in os.walk(root_directory)  # noqa\n                 for name in files]\n        paths.sort()\n        return paths", "response": "Generate the paths of all files that are ancestors of root_directory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning True if path has the given extension or if the last path component matches the extension.", "response": "def matches_extension(path, extension):\n    \"\"\"\n    Returns True if path has the given extension, or if\n    the last path component matches the extension. Supports\n    Unix glob matching.\n\n    >>> matches_extension(\"./www/profile.php\", \"php\")\n    True\n    >>> matches_extension(\"./scripts/menu.js\", \"html\")\n    False\n    >>> matches_extension(\"./LICENSE\", \"LICENSE\")\n    True\n    \"\"\"\n    _, ext = os.path.splitext(path)\n    if ext == '':\n        # If there is no extension, grab the file name and\n        # compare it to the given extension.\n        return os.path.basename(path) == extension\n    else:\n        # If the is an extension, drop the leading period and\n        # compare it to the extension.\n        return fnmatch.fnmatch(ext[1:], extension)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a filter function that returns True if a filepath is acceptable under the given extensions.", "response": "def path_filter(extensions, exclude_paths=None):\n    \"\"\"\n    Returns a function that returns True if a filepath is acceptable.\n\n    @param extensions     An array of strings. Specifies what file\n                          extensions should be accepted by the\n                          filter. If None, we default to the Unix glob\n                          `*` and match every file extension.\n    @param exclude_paths  An array of strings which represents filepaths\n                          that should never be accepted by the filter. Unix\n                          shell-style wildcards are supported.\n\n    @return function      A filter function that will only return True\n                          when a filepath is acceptable under the above\n                          conditions.\n\n    >>> list(map(path_filter(extensions=['js', 'php']),\n    ...     ['./profile.php', './q.jjs']))\n    [True, False]\n    >>> list(map(path_filter(extensions=['*'],\n    ...                 exclude_paths=['html']),\n    ...     ['./html/x.php', './lib/y.js']))\n    [False, True]\n    >>> list(map(path_filter(extensions=['js', 'BUILD']),\n    ...     ['./a.js', './BUILD', './profile.php']))\n    [True, True, False]\n    >>> list(map(path_filter(extensions=['js'],\n    ...     exclude_paths=['*/node_modules/*']),\n    ...     ['./a.js', './tools/node_modules/dep.js']))\n    [True, False]\n    \"\"\"\n    exclude_paths = exclude_paths or []\n\n    def the_filter(path):\n        if not any(matches_extension(path, extension)\n                   for extension in extensions):\n            return False\n        if exclude_paths:\n            for excluded in exclude_paths:\n                if (path.startswith(excluded) or\n                        path.startswith('./' + excluded) or\n                        fnmatch.fnmatch(path, excluded)):\n                    return False\n        return True\n    return the_filter"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef terminal_get_size(default_size=(25, 80)):\n\n    def ioctl_gwinsz(fd):  # TABULATION FUNCTIONS\n        try:  # Discover terminal width\n            return struct.unpack(\n                'hh', fcntl.ioctl(fd, termios.TIOCGWINSZ, '1234')\n            )\n        except Exception:\n            return None\n\n    # try open fds\n    size = ioctl_gwinsz(0) or ioctl_gwinsz(1) or ioctl_gwinsz(2)\n    if not size:\n        # ...then ctty\n        try:\n            fd = os.open(os.ctermid(), os.O_RDONLY)\n            size = ioctl_gwinsz(fd)\n            os.close(fd)\n        except Exception:\n            pass\n    if not size:\n        # env vars or finally defaults\n        try:\n            size = (os.environ['LINES'], os.environ['COLUMNS'])\n        except Exception:\n            return default_size\n\n    return map(int, size)", "response": "Return the number of rows and columns for the terminal."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns True if the terminal supports the given capability.", "response": "def _terminal_use_capability(capability_name):\n    \"\"\"\n    If the terminal supports the given capability, output it.  Return whether\n    it was output.\n    \"\"\"\n    curses.setupterm()\n    capability = curses.tigetstr(capability_name)\n    if capability:\n        sys.stdout.write(_unicode(capability))\n    return bool(capability)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a configuration option from the options attribute.", "response": "def get_option(self, option):\n        \"\"\"\n        Get a configuration option, trying the options attribute first and\n        falling back to a Django project setting.\n        \"\"\"\n        value = getattr(self, option, None)\n        if value is not None:\n            return value\n        return getattr(settings, \"COUNTRIES_{0}\".format(option.upper()))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef countries(self):\n        if not hasattr(self, \"_countries\"):\n            only = self.get_option(\"only\")\n            if only:\n                only_choices = True\n                if not isinstance(only, dict):\n                    for item in only:\n                        if isinstance(item, six.string_types):\n                            only_choices = False\n                            break\n            if only and only_choices:\n                self._countries = dict(only)\n            else:\n                # Local import so that countries aren't loaded into memory\n                # until first used.\n                from django_countries.data import COUNTRIES\n\n                self._countries = dict(COUNTRIES)\n                if self.get_option(\"common_names\"):\n                    self._countries.update(self.COMMON_NAMES)\n                override = self.get_option(\"override\")\n                if override:\n                    self._countries.update(override)\n                    self._countries = dict(\n                        (code, name)\n                        for code, name in self._countries.items()\n                        if name is not None\n                    )\n            if only and not only_choices:\n                countries = {}\n                for item in only:\n                    if isinstance(item, six.string_types):\n                        countries[item] = self._countries[item]\n                    else:\n                        key, value = item\n                        countries[key] = value\n                self._countries = countries\n            self.countries_first = []\n            first = self.get_option(\"first\") or []\n            for code in first:\n                code = self.alpha2(code)\n                if code in self._countries:\n                    self.countries_first.append(code)\n        return self._countries", "response": "Returns a dictionary of countries modified by any overriding\n        options."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef translate_pair(self, code):\n        name = self.countries[code]\n        if code in self.OLD_NAMES:\n            # Check if there's an older translation available if there's no\n            # translation for the newest name.\n            with override(None):\n                source_name = force_text(name)\n            name = force_text(name)\n            if name == source_name:\n                for old_name in self.OLD_NAMES[code]:\n                    with override(None):\n                        source_old_name = force_text(old_name)\n                    old_name = force_text(old_name)\n                    if old_name != source_old_name:\n                        name = old_name\n                        break\n        else:\n            name = force_text(name)\n        return CountryTuple(code, name)", "response": "Translate a country to the current activated translation."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the two letter country code when passed any type of ISO 3166 - 1 - 1 country code.", "response": "def alpha2(self, code):\n        \"\"\"\n        Return the two letter country code when passed any type of ISO 3166-1\n        country code.\n\n        If no match is found, returns an empty string.\n        \"\"\"\n        code = force_text(code).upper()\n        if code.isdigit():\n            lookup_code = int(code)\n\n            def find(alt_codes):\n                return alt_codes[1] == lookup_code\n\n        elif len(code) == 3:\n            lookup_code = code\n\n            def find(alt_codes):\n                return alt_codes[0] == lookup_code\n\n        else:\n            find = None\n        if find:\n            code = None\n            for alpha2, alt_codes in self.alt_codes.items():\n                if find(alt_codes):\n                    code = alpha2\n                    break\n        if code in self.countries:\n            return code\n        return \"\""}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef name(self, code):\n        code = self.alpha2(code)\n        if code not in self.countries:\n            return \"\"\n        return self.translate_pair(code)[1]", "response": "Return the name of a country based on the code."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef by_name(self, country, language=\"en\"):\n        with override(language):\n            for code, name in self:\n                if name.lower() == country.lower():\n                    return code\n                if code in self.OLD_NAMES:\n                    for old_name in self.OLD_NAMES[code]:\n                        if old_name.lower() == country.lower():\n                            return code\n        return \"\"", "response": "Fetch a country s ISO3166 - 1 two letter country code from its name."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef alpha3(self, code):\n        code = self.alpha2(code)\n        try:\n            return self.alt_codes[code][0]\n        except KeyError:\n            return \"\"", "response": "Return ISO 3166 - 1 three letter country code matching the provided ISO 3166 - 1 three letter country code."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn ISO 3166 - 1 numeric country code matching the provided ISO 3166 - 1 country code.", "response": "def numeric(self, code, padded=False):\n        \"\"\"\n        Return the ISO 3166-1 numeric country code matching the provided\n        country code.\n\n        If no match is found, returns ``None``.\n\n        :param padded: Pass ``True`` to return a 0-padded three character\n            string, otherwise an integer will be returned.\n        \"\"\"\n        code = self.alpha2(code)\n        try:\n            num = self.alt_codes[code][1]\n        except KeyError:\n            return None\n        if padded:\n            return \"%03d\" % num\n        return num"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the list of available choices.", "response": "def choices(self):\n        \"\"\"\n        When it's time to get the choices, if it was a lazy then figure it out\n        now and memoize the result.\n        \"\"\"\n        if isinstance(self._choices, Promise):\n            self._choices = list(self._choices)\n        return self._choices"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_ioc_countries(verbosity=1):\n    from django_countries.data import COUNTRIES\n\n    if verbosity:  # pragma: no cover\n        print(\"Checking if all IOC codes map correctly\")\n    for key in ISO_TO_IOC:\n        assert COUNTRIES.get(key), \"No ISO code for %s\" % key\n    if verbosity:  # pragma: no cover\n        print(\"Finished checking IOC codes\")", "response": "Check if all IOC codes map to ISO codes correctly"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef self_generate(output_filename, filename=\"iso3166-1.csv\"):  # pragma: no cover\n    import csv\n    import re\n\n    countries = []\n    alt_codes = []\n    with open(filename, \"r\") as csv_file:\n        for row in csv.reader(csv_file):\n            name = row[0].rstrip(\"*\")\n            name = re.sub(r\"\\(the\\)\", \"\", name)\n            if name:\n                countries.append((name, row[1]))\n                alt_codes.append((row[1], row[2], int(row[3])))\n    with open(__file__, \"r\") as source_file:\n        contents = source_file.read()\n    # Write countries.\n    bits = re.match(\"(.*\\nCOUNTRIES = \\{\\n)(.*?)(\\n\\}.*)\", contents, re.DOTALL).groups()\n    country_list = []\n    for name, code in countries:\n        name = name.replace('\"', r\"\\\"\").strip()\n        country_list.append('    \"{code}\": _(\"{name}\"),'.format(name=name, code=code))\n    content = bits[0]\n    content += \"\\n\".join(country_list)\n    # Write alt codes.\n    alt_bits = re.match(\n        \"(.*\\nALT_CODES = \\{\\n)(.*)(\\n\\}.*)\", bits[2], re.DOTALL\n    ).groups()\n    alt_list = []\n    for code, code3, codenum in alt_codes:\n        name = name.replace('\"', r\"\\\"\").strip()\n        alt_list.append(\n            '    \"{code}\": (\"{code3}\", {codenum}),'.format(\n                code=code, code3=code3, codenum=codenum\n            )\n        )\n    content += alt_bits[0]\n    content += \"\\n\".join(alt_list)\n    content += alt_bits[2]\n    # Generate file.\n    with open(output_filename, \"w\") as output_file:\n        output_file.write(content)\n    return countries", "response": "This function generates the self - generation code for the ISO 3166 - 1. csv file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the widget s choices.", "response": "def _set_choices(self, value):\n        \"\"\"\n        Also update the widget's choices.\n        \"\"\"\n        super(LazyChoicesMixin, self)._set_choices(value)\n        self.widget.choices = value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning field s value just before saving.", "response": "def pre_save(self, *args, **kwargs):\n        \"Returns field's value just before saving.\"\n        value = super(CharField, self).pre_save(*args, **kwargs)\n        return self.get_prep_value(value)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_prep_value(self, value):\n        \"Returns field's value prepared for saving into a database.\"\n        value = self.get_clean_value(value)\n        if self.multiple:\n            if value:\n                value = \",\".join(value)\n            else:\n                value = \"\"\n        return super(CharField, self).get_prep_value(value)", "response": "Returns field s value prepared for saving into a database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving choices from deconstructed field, as this is the country list and not user editable. Not including the ``blank_label`` property, as this isn't database related.", "response": "def deconstruct(self):\n        \"\"\"\n        Remove choices from deconstructed field, as this is the country list\n        and not user editable.\n\n        Not including the ``blank_label`` property, as this isn't database\n        related.\n        \"\"\"\n        name, path, args, kwargs = super(CountryField, self).deconstruct()\n        kwargs.pop(\"choices\")\n        if self.multiple:  # multiple determines the length of the field\n            kwargs[\"multiple\"] = self.multiple\n        if self.countries is not countries:\n            # Include the countries class if it's not the default countries\n            # instance.\n            kwargs[\"countries\"] = self.countries.__class__\n        return name, path, args, kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nvalidating the value of the ISO 3166 - 1 country field.", "response": "def validate(self, value, model_instance):\n        \"\"\"\n        Use custom validation for when using a multiple countries field.\n        \"\"\"\n        if not self.multiple:\n            return super(CountryField, self).validate(value, model_instance)\n\n        if not self.editable:\n            # Skip validation for non-editable fields.\n            return\n\n        if value:\n            choices = [option_key for option_key, option_value in self.choices]\n            for single_value in value:\n                if single_value not in choices:\n                    raise exceptions.ValidationError(\n                        self.error_messages[\"invalid_choice\"],\n                        code=\"invalid_choice\",\n                        params={\"value\": single_value},\n                    )\n\n        if not self.blank and value in self.empty_values:\n            raise exceptions.ValidationError(self.error_messages[\"blank\"], code=\"blank\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef value_to_string(self, obj):\n        value = self.value_from_object(obj)\n        return self.get_prep_value(value)", "response": "Converts the object to a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _pquery(scheduler, data, ndata, ndim, leafsize,\n            x, nx, d, i, k, eps, p, dub, ierr):\n    \"\"\"\n    Function that parallelly queries the K-D tree based on chunks of data returned by the scheduler\n    \"\"\"\n    try:\n        _data = shmem_as_nparray(data).reshape((ndata, ndim))\n        _x = shmem_as_nparray(x).reshape((nx, ndim))\n        _d = shmem_as_nparray(d).reshape((nx, k))\n        _i = shmem_as_nparray(i).reshape((nx, k))\n\n        kdtree = cKDTree(_data, leafsize=leafsize)\n\n        for s in scheduler:\n            d_out, i_out = kdtree.query(_x[s, :], k=k, eps=eps, p=p, distance_upper_bound=dub)\n            m_d = d_out.shape[0]\n            m_i = i_out.shape[0]\n            _d[s, :], _i[s, :] = d_out.reshape(m_d, 1), i_out.reshape(m_i, 1)\n    except:\n        ierr.value += 1", "response": "Function that queries the K - D tree based on chunks of data returned by the scheduler"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfunctioning to parallelly query the K - D Tree", "response": "def pquery(self, x_list, k=1, eps=0, p=2,\n               distance_upper_bound=np.inf):\n        \"\"\"\n        Function to parallelly query the K-D Tree\n        \"\"\"\n        x = np.array(x_list)\n        nx, mx = x.shape\n        shmem_x = mp.Array(ctypes.c_double, nx*mx)\n        shmem_d = mp.Array(ctypes.c_double, nx*k)\n        shmem_i = mp.Array(ctypes.c_double, nx*k)\n\n        _x = shmem_as_nparray(shmem_x).reshape((nx, mx))\n        _d = shmem_as_nparray(shmem_d).reshape((nx, k))\n\n        _i = shmem_as_nparray(shmem_i)\n        if k != 1:\n            _i = _i.reshape((nx, k))\n\n        _x[:, :] = x\n\n        nprocs = num_cpus()\n        scheduler = Scheduler(nx, nprocs)\n\n        ierr = mp.Value(ctypes.c_int, 0)\n\n        query_args = (scheduler,\n                      self.shmem_data, self.n, self.m, self.leafsize,\n                      shmem_x, nx, shmem_d, shmem_i,\n                      k, eps, p, distance_upper_bound,\n                      ierr)\n        pool = [mp.Process(target=_pquery, args=query_args) for _ in range(nprocs)]\n        for p in pool: p.start()\n        for p in pool: p.join()\n        if ierr.value != 0:\n            raise RuntimeError('%d errors in worker processes' % (ierr.value))\n\n        return _d.copy(), _i.astype(int).copy()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef singleton(cls):\n    instances = {}\n    def getinstance(**kwargs):\n        \"\"\"\n        Creates a new RGeocoder instance if not created already\n        \"\"\"\n        if cls not in instances:\n            instances[cls] = cls(**kwargs)\n        return instances[cls]\n    return getinstance", "response": "Returns a function that returns a single instance of the class cls."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef rel_path(filename):\n    return os.path.join(os.getcwd(), os.path.dirname(__file__), filename)", "response": "Function that gets relative path to the filename"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfunction to query for a single coordinate", "response": "def get(geo_coord, mode=2, verbose=True):\n    \"\"\"\n    Function to query for a single coordinate\n    \"\"\"\n    if not isinstance(geo_coord, tuple) or not isinstance(geo_coord[0], float):\n        raise TypeError('Expecting a tuple')\n\n    _rg = RGeocoder(mode=mode, verbose=verbose)\n    return _rg.query([geo_coord])[0]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef search(geo_coords, mode=2, verbose=True):\n    if not isinstance(geo_coords, tuple) and not isinstance(geo_coords, list):\n        raise TypeError('Expecting a tuple or a tuple/list of tuples')\n    elif not isinstance(geo_coords[0], tuple):\n        geo_coords = [geo_coords]\n\n    _rg = RGeocoder(mode=mode, verbose=verbose)\n    return _rg.query(geo_coords)", "response": "Function to query for a list of coordinates"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef art_msg(self, arttag, colorname, file=sys.stdout):\n        self.msg(self.art[arttag], colorname, file=file)", "response": "Wrapper for easy emission of the calendar borders"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _cal_monday(self, day_num):\n        if self.options['cal_monday'] or not self.options['cal_weekend']:\n            day_num -= 1\n            if day_num < 0:\n                day_num = 6\n        return day_num", "response": "Shift the day number if we re doing cal monday or cal weekend is False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwrapping around Google Calendar API s quickAdd event", "response": "def QuickAddEvent(self, event_text, reminders=None):\n        \"\"\"Wrapper around Google Calendar API's quickAdd\"\"\"\n        if not event_text:\n            raise GcalcliError('event_text is required for a quickAdd')\n\n        if len(self.cals) != 1:\n            # TODO: get a better name for this exception class\n            # and use it elsewhere\n            raise GcalcliError('You must only specify a single calendar\\n')\n\n        new_event = self._retry_with_backoff(\n            self.get_cal_service()\n                .events()\n                .quickAdd(\n                    calendarId=self.cals[0]['id'],\n                    text=event_text\n                )\n        )\n\n        if reminders or not self.options['default_reminders']:\n            rem = {}\n            rem['reminders'] = {'useDefault': False,\n                                'overrides': []}\n            for r in reminders:\n                n, m = utils.parse_reminder(r)\n                rem['reminders']['overrides'].append({'minutes': n,\n                                                      'method': m})\n\n            new_event = self._retry_with_backoff(\n                            self.get_cal_service()\n                                .events()\n                                .patch(\n                                    calendarId=self.cals[0]['id'],\n                                    eventId=new_event['id'],\n                                    body=rem\n                                )\n                        )\n\n        if self.details.get('url'):\n            hlink = new_event['htmlLink']\n            self.printer.msg('New event added: %s\\n' % hlink, 'green')\n\n        return new_event"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremind the user from a specific time.", "response": "def Remind(self, minutes, command, use_reminders=False):\n        \"\"\"\n        Check for events between now and now+minutes.\n\n        If use_reminders then only remind if now >= event['start'] - reminder\n        \"\"\"\n\n        # perform a date query for now + minutes + slip\n        start = self.now\n        end = (start + timedelta(minutes=(minutes + 5)))\n\n        event_list = self._search_for_events(start, end, None)\n\n        message = ''\n\n        for event in event_list:\n\n            # skip this event if it already started\n            # XXX maybe add a 2+ minute grace period here...\n            if event['s'] < self.now:\n                continue\n\n            # not sure if 'reminders' always in event\n            if use_reminders and 'reminders' in event \\\n                    and 'overrides' in event['reminders']:\n                if all(event['s'] - timedelta(minutes=r['minutes']) > self.now\n                        for r in event['reminders']['overrides']):\n                    # don't remind if all reminders haven't arrived yet\n                    continue\n\n            if self.options.get('military'):\n                tmp_time_str = event['s'].strftime('%H:%M')\n            else:\n                tmp_time_str = \\\n                    event['s'].strftime('%I:%M').lstrip('0') + \\\n                    event['s'].strftime('%p').lower()\n\n            message += '%s  %s\\n' % \\\n                       (tmp_time_str, self._valid_title(event).strip())\n\n        if not message:\n            return\n\n        cmd = shlex.split(command)\n\n        for i, a in zip(range(len(cmd)), cmd):\n            if a == '%s':\n                cmd[i] = message\n\n        pid = os.fork()\n        if not pid:\n            os.execvp(cmd[0], cmd)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a string to a time", "response": "def get_time_from_str(when):\n    \"\"\"Convert a string to a time: first uses the dateutil parser, falls back\n    on fuzzy matching with parsedatetime\n    \"\"\"\n    zero_oclock_today = datetime.now(tzlocal()).replace(\n            hour=0, minute=0, second=0, microsecond=0)\n\n    try:\n        event_time = dateutil_parse(when, default=zero_oclock_today)\n    except ValueError:\n        struct, result = fuzzy_date_parse(when)\n        if not result:\n            raise ValueError('Date and time is invalid: %s' % (when))\n        event_time = datetime.fromtimestamp(time.mktime(struct), tzlocal())\n\n    return event_time"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef color_validator(input_str):\n    try:\n        assert input_str in VALID_OVERRIDE_COLORS + ['']\n        return input_str\n    except AssertionError:\n        raise ValidationError(\n                'Expected colors are: ' +\n                ', '.join(color for color in VALID_OVERRIDE_COLORS) +\n                '. (Ctrl-C to exit)\\n')", "response": "A filter allowing only the particular colors used by the Google Calendar\n    API\n    Raises ValidationError otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reminder_validator(input_str):\n    match = re.match(REMINDER_REGEX, input_str)\n    if match or input_str == '.':\n        return input_str\n    else:\n        raise ValidationError('Expected format: <number><w|d|h|m> '\n                              '<popup|email|sms>. (Ctrl-C to exit)\\n')", "response": "Validates a string that matches utils. REMINDER_REGEX. Returns the string that matches utils. REMINDER_REGEX Raises a ValidationError otherwise."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsplits registration ids in several lists of max 1000 registration ids per list", "response": "def registration_id_chunks(self, registration_ids):\n        \"\"\"\n        Splits registration ids in several lists of max 1000 registration ids per list\n\n        Args:\n            registration_ids (list): FCM device registration ID\n\n        Yields:\n            generator: list including lists with registration ids\n        \"\"\"\n        try:\n            xrange\n        except NameError:\n            xrange = range\n\n        # Yield successive 1000-sized (max fcm recipients per request) chunks from registration_ids\n        for i in xrange(0, len(registration_ids), self.FCM_MAX_RECIPIENTS):\n            yield registration_ids[i:i + self.FCM_MAX_RECIPIENTS]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef json_dumps(self, data):\n        return json.dumps(\n            data, \n            separators=(',', ':'), \n            sort_keys=True, \n            cls=self.json_encoder, \n            ensure_ascii=False\n        ).encode('utf8')", "response": "Standardized json. dumps function with separators and sorted keys set\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_payload(self,\n                      registration_ids=None,\n                      topic_name=None,\n                      message_body=None,\n                      message_title=None,\n                      message_icon=None,\n                      sound=None,\n                      condition=None,\n                      collapse_key=None,\n                      delay_while_idle=False,\n                      time_to_live=None,\n                      restricted_package_name=None,\n                      low_priority=False,\n                      dry_run=False,\n                      data_message=None,\n                      click_action=None,\n                      badge=None,\n                      color=None,\n                      tag=None,\n                      body_loc_key=None,\n                      body_loc_args=None,\n                      title_loc_key=None,\n                      title_loc_args=None,\n                      content_available=None,\n                      remove_notification=False,\n                      android_channel_id=None,\n                      extra_notification_kwargs={},\n                      **extra_kwargs):\n        \"\"\"\n        Parses parameters of FCMNotification's methods to FCM nested json\n\n        Args:\n            registration_ids (list, optional): FCM device registration IDs\n            topic_name (str, optional): Name of the topic to deliver messages to\n            message_body (str, optional): Message string to display in the notification tray\n            message_title (str, optional): Message title to display in the notification tray\n            message_icon (str, optional): Icon that apperas next to the notification\n            sound (str, optional): The sound file name to play. Specify \"Default\" for device default sound.\n            condition (str, optiona): Topic condition to deliver messages to\n            collapse_key (str, optional): Identifier for a group of messages\n                that can be collapsed so that only the last message gets sent\n                when delivery can be resumed. Defaults to `None`.\n            delay_while_idle (bool, optional): deprecated\n            time_to_live (int, optional): How long (in seconds) the message\n                should be kept in FCM storage if the device is offline. The\n                maximum time to live supported is 4 weeks. Defaults to `None`\n                which uses the FCM default of 4 weeks.\n            restricted_package_name (str, optional): Name of package\n            low_priority (bool, optional): Whether to send notification with\n                the low priority flag. Defaults to `False`.\n            dry_run (bool, optional): If `True` no message will be sent but request will be tested.\n            data_message (dict, optional): Custom key-value pairs\n            click_action (str, optional): Action associated with a user click on the notification\n            badge (str, optional): Badge of notification\n            color (str, optional): Color of the icon\n            tag (str, optional): Group notification by tag\n            body_loc_key (str, optional): Indicates the key to the body string for localization\n            body_loc_args (list, optional): Indicates the string value to replace format\n                specifiers in body string for localization\n            title_loc_key (str, optional): Indicates the key to the title string for localization\n            title_loc_args (list, optional): Indicates the string value to replace format\n                specifiers in title string for localization\n            content_available (bool, optional): Inactive client app is awoken\n            remove_notification (bool, optional): Only send a data message\n            android_channel_id (str, optional): Starting in Android 8.0 (API level 26),\n                all notifications must be assigned to a channel. For each channel, you can set the\n                visual and auditory behavior that is applied to all notifications in that channel.\n                Then, users can change these settings and decide which notification channels from\n                your app should be intrusive or visible at all.\n            extra_notification_kwargs (dict, optional): More notification keyword arguments\n            **extra_kwargs (dict, optional): More keyword arguments\n\n        Returns:\n            string: json\n\n        Raises:\n            InvalidDataError: parameters do have the wrong type or format\n        \"\"\"\n        fcm_payload = dict()\n        if registration_ids:\n            if len(registration_ids) > 1:\n                fcm_payload['registration_ids'] = registration_ids\n            else:\n                fcm_payload['to'] = registration_ids[0]\n        if condition:\n            fcm_payload['condition'] = condition\n        else:\n            # In the `to` reference at: https://firebase.google.com/docs/cloud-messaging/http-server-ref#send-downstream\n            # We have `Do not set this field (to) when sending to multiple topics`\n            # Which is why it's in the `else` block since `condition` is used when multiple topics are being targeted\n            if topic_name:\n                fcm_payload['to'] = '/topics/%s' % topic_name\n        # Revert to legacy API compatible priority\n        if low_priority:\n            fcm_payload['priority'] = self.FCM_LOW_PRIORITY\n        else:\n            fcm_payload['priority'] = self.FCM_HIGH_PRIORITY\n\n        if delay_while_idle:\n            fcm_payload['delay_while_idle'] = delay_while_idle\n        if collapse_key:\n            fcm_payload['collapse_key'] = collapse_key\n        if time_to_live:\n            if isinstance(time_to_live, int):\n                fcm_payload['time_to_live'] = time_to_live\n            else:\n                raise InvalidDataError(\"Provided time_to_live is not an integer\")\n        if restricted_package_name:\n            fcm_payload['restricted_package_name'] = restricted_package_name\n        if dry_run:\n            fcm_payload['dry_run'] = dry_run\n\n        if data_message:\n            if isinstance(data_message, dict):\n                fcm_payload['data'] = data_message\n            else:\n                raise InvalidDataError(\"Provided data_message is in the wrong format\")\n\n        fcm_payload['notification'] = {}\n        if message_icon:\n            fcm_payload['notification']['icon'] = message_icon\n        # If body is present, use it\n        if message_body:\n            fcm_payload['notification']['body'] = message_body\n        # Else use body_loc_key and body_loc_args for body\n        else:\n            if body_loc_key:\n                fcm_payload['notification']['body_loc_key'] = body_loc_key\n            if body_loc_args:\n                if isinstance(body_loc_args, list):\n                    fcm_payload['notification']['body_loc_args'] = body_loc_args\n                else:\n                    raise InvalidDataError('body_loc_args should be an array')\n        # If title is present, use it\n        if message_title:\n            fcm_payload['notification']['title'] = message_title\n        # Else use title_loc_key and title_loc_args for title\n        else:\n            if title_loc_key:\n                fcm_payload['notification']['title_loc_key'] = title_loc_key\n            if title_loc_args:\n                if isinstance(title_loc_args, list):\n                    fcm_payload['notification']['title_loc_args'] = title_loc_args\n                else:\n                    raise InvalidDataError('title_loc_args should be an array')\n\n        if android_channel_id:\n            fcm_payload['notification']['android_channel_id'] = android_channel_id\n\n        # This is needed for iOS when we are sending only custom data messages\n        if content_available and isinstance(content_available, bool):\n            fcm_payload['content_available'] = content_available\n\n        if click_action:\n            fcm_payload['notification']['click_action'] = click_action\n        if isinstance(badge, int) and badge >= 0:\n            fcm_payload['notification']['badge'] = badge\n        if color:\n            fcm_payload['notification']['color'] = color\n        if tag:\n            fcm_payload['notification']['tag'] = tag\n        # only add the 'sound' key if sound is not None\n        # otherwise a default sound will play -- even with empty string args.\n        if sound:\n            fcm_payload['notification']['sound'] = sound\n\n        if extra_kwargs:\n            fcm_payload.update(extra_kwargs)\n\n        if extra_notification_kwargs:\n            fcm_payload['notification'].update(extra_notification_kwargs)\n\n        # Do this if you only want to send a data message.\n        if remove_notification:\n            del fcm_payload['notification']\n\n        return self.json_dumps(fcm_payload)", "response": "Parses the JSON payload of the FCMNotification s methods into a nested dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes a request to the info endpoint of the specified registration id and returns the response object", "response": "def registration_info_request(self, registration_id):\n        \"\"\"\n        Makes a request for registration info and returns the response object\n\n        Args:\n            registration_id: id to be checked\n\n        Returns:\n            response of registration info request\n        \"\"\"\n        return self.requests_session.get(\n            self.INFO_END_POINT + registration_id,\n            params={'details': 'true'}\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clean_registration_ids(self, registration_ids=[]):\n        valid_registration_ids = []\n        for registration_id in registration_ids:\n            details = self.registration_info_request(registration_id)\n            if details.status_code == 200:\n                valid_registration_ids.append(registration_id)\n        return valid_registration_ids", "response": "Checks registration ids and excludes inactive ids\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_registration_id_info(self, registration_id):\n        response = self.registration_info_request(registration_id)\n        if response.status_code == 200:\n            return response.json()\n        return None", "response": "Returns details related to a registration id if it exists otherwise return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef subscribe_registration_ids_to_topic(self, registration_ids, topic_name):\n        url = 'https://iid.googleapis.com/iid/v1:batchAdd'\n        payload = {\n            'to': '/topics/' + topic_name,\n            'registration_tokens': registration_ids,\n        }\n        response = self.requests_session.post(url, json=payload)\n        if response.status_code == 200:\n            return True\n        elif response.status_code == 400:\n            error = response.json()\n            raise InvalidDataError(error['error'])\n        else:\n            raise FCMError()", "response": "Subscribes a list of registration ids to a topic"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the json response sent back by the FCM and returns a dict of the relevant fields.", "response": "def parse_responses(self):\n        \"\"\"\n        Parses the json response sent back by the server and tries to get out the important return variables\n\n        Returns:\n            dict: multicast_ids (list), success (int), failure (int), canonical_ids (int),\n                results (list) and optional topic_message_id (str but None by default)\n\n        Raises:\n            FCMServerError: FCM is temporary not available\n            AuthenticationError: error authenticating the sender account\n            InvalidDataError: data passed to FCM was incorrecly structured\n        \"\"\"\n        response_dict = {\n            'multicast_ids': [],\n            'success': 0,\n            'failure': 0,\n            'canonical_ids': 0,\n            'results': [],\n            'topic_message_id': None\n        }\n\n        for response in self.send_request_responses:\n            if response.status_code == 200:\n                if 'content-length' in response.headers and int(response.headers['content-length']) <= 0:\n                    raise FCMServerError(\"FCM server connection error, the response is empty\")\n                else:\n                    parsed_response = response.json()\n\n                    multicast_id = parsed_response.get('multicast_id', None)\n                    success = parsed_response.get('success', 0)\n                    failure = parsed_response.get('failure', 0)\n                    canonical_ids = parsed_response.get('canonical_ids', 0)\n                    results = parsed_response.get('results', [])\n                    message_id = parsed_response.get('message_id', None)  # for topic messages\n                    if message_id:\n                        success = 1\n                    if multicast_id:\n                        response_dict['multicast_ids'].append(multicast_id)\n                    response_dict['success'] += success\n                    response_dict['failure'] += failure\n                    response_dict['canonical_ids'] += canonical_ids\n                    response_dict['results'].extend(results)\n                    response_dict['topic_message_id'] = message_id\n\n            elif response.status_code == 401:\n                raise AuthenticationError(\"There was an error authenticating the sender account\")\n            elif response.status_code == 400:\n                raise InvalidDataError(response.text)\n            else:\n                raise FCMServerError(\"FCM server is temporarily unavailable\")\n        return response_dict"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef notify_single_device(self,\n                             registration_id=None,\n                             message_body=None,\n                             message_title=None,\n                             message_icon=None,\n                             sound=None,\n                             condition=None,\n                             collapse_key=None,\n                             delay_while_idle=False,\n                             time_to_live=None,\n                             restricted_package_name=None,\n                             low_priority=False,\n                             dry_run=False,\n                             data_message=None,\n                             click_action=None,\n                             badge=None,\n                             color=None,\n                             tag=None,\n                             body_loc_key=None,\n                             body_loc_args=None,\n                             title_loc_key=None,\n                             title_loc_args=None,\n                             content_available=None,\n                             android_channel_id=None,\n                             timeout=5,\n                             extra_notification_kwargs=None,\n                             extra_kwargs={}):\n        \"\"\"\n        Send push notification to a single device\n\n        Args:\n            registration_id (list, optional): FCM device registration ID\n            message_body (str, optional): Message string to display in the notification tray\n            message_title (str, optional): Message title to display in the notification tray\n            message_icon (str, optional): Icon that apperas next to the notification\n            sound (str, optional): The sound file name to play. Specify \"Default\" for device default sound.\n            condition (str, optiona): Topic condition to deliver messages to\n            collapse_key (str, optional): Identifier for a group of messages\n                that can be collapsed so that only the last message gets sent\n                when delivery can be resumed. Defaults to `None`.\n            delay_while_idle (bool, optional): deprecated\n            time_to_live (int, optional): How long (in seconds) the message\n                should be kept in FCM storage if the device is offline. The\n                maximum time to live supported is 4 weeks. Defaults to `None`\n                which uses the FCM default of 4 weeks.\n            restricted_package_name (str, optional): Name of package\n            low_priority (bool, optional): Whether to send notification with\n                the low priority flag. Defaults to `False`.\n            dry_run (bool, optional): If `True` no message will be sent but request will be tested.\n            data_message (dict, optional): Custom key-value pairs\n            click_action (str, optional): Action associated with a user click on the notification\n            badge (str, optional): Badge of notification\n            color (str, optional): Color of the icon\n            tag (str, optional): Group notification by tag\n            body_loc_key (str, optional): Indicates the key to the body string for localization\n            body_loc_args (list, optional): Indicates the string value to replace format\n                specifiers in body string for localization\n            title_loc_key (str, optional): Indicates the key to the title string for localization\n            title_loc_args (list, optional): Indicates the string value to replace format\n                specifiers in title string for localization\n            content_available (bool, optional): Inactive client app is awoken\n            android_channel_id (str, optional): Starting in Android 8.0 (API level 26),\n                all notifications must be assigned to a channel. For each channel, you can set the\n                visual and auditory behavior that is applied to all notifications in that channel.\n                Then, users can change these settings and decide which notification channels from\n                your app should be intrusive or visible at all.\n            timeout (int, optional): set time limit for the request\n            extra_notification_kwargs (dict, optional): More notification keyword arguments\n            extra_kwargs (dict, optional): More keyword arguments\n\n        Returns:\n            dict: Response from FCM server (`multicast_id`, `success`, `failure`, `canonical_ids`, `results`)\n\n        Raises:\n            AuthenticationError: If :attr:`api_key` is not set or provided\n                or there is an error authenticating the sender.\n            FCMServerError: Internal server error or timeout error on Firebase cloud messaging server\n            InvalidDataError: Invalid data provided\n            InternalPackageError: Mostly from changes in the response of FCM,\n                contact the project owner to resolve the issue\n        \"\"\"\n        if registration_id is None:\n            raise InvalidDataError('Invalid registration ID')\n        # [registration_id] cos we're sending to a single device\n        payload = self.parse_payload(\n            registration_ids=[registration_id],\n            message_body=message_body,\n            message_title=message_title,\n            message_icon=message_icon,\n            sound=sound,\n            condition=condition,\n            collapse_key=collapse_key,\n            delay_while_idle=delay_while_idle,\n            time_to_live=time_to_live,\n            restricted_package_name=restricted_package_name,\n            low_priority=low_priority,\n            dry_run=dry_run, data_message=data_message, click_action=click_action,\n            badge=badge,\n            color=color,\n            tag=tag,\n            body_loc_key=body_loc_key,\n            body_loc_args=body_loc_args,\n            title_loc_key=title_loc_key,\n            title_loc_args=title_loc_args,\n            android_channel_id=android_channel_id,\n            content_available=content_available,\n            extra_notification_kwargs=extra_notification_kwargs,\n            **extra_kwargs\n        )\n\n        self.send_request([payload], timeout)\n        return self.parse_responses()", "response": "Send push notification to a single device."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsending push message to a single device.", "response": "def single_device_data_message(self,\n                                   registration_id=None,\n                                   condition=None,\n                                   collapse_key=None,\n                                   delay_while_idle=False,\n                                   time_to_live=None,\n                                   restricted_package_name=None,\n                                   low_priority=False,\n                                   dry_run=False,\n                                   data_message=None,\n                                   content_available=None,\n                                   android_channel_id=None,\n                                   timeout=5,\n                                   extra_notification_kwargs=None,\n                                   extra_kwargs={}):\n        \"\"\"\n        Send push message to a single device\n\n        Args:\n            registration_id (list, optional): FCM device registration ID\n            condition (str, optiona): Topic condition to deliver messages to\n            collapse_key (str, optional): Identifier for a group of messages\n                that can be collapsed so that only the last message gets sent\n                when delivery can be resumed. Defaults to `None`.\n            delay_while_idle (bool, optional): deprecated\n            time_to_live (int, optional): How long (in seconds) the message\n                should be kept in FCM storage if the device is offline. The\n                maximum time to live supported is 4 weeks. Defaults to `None`\n                which uses the FCM default of 4 weeks.\n            restricted_package_name (str, optional): Name of package\n            low_priority (bool, optional): Whether to send notification with\n                the low priority flag. Defaults to `False`.\n            dry_run (bool, optional): If `True` no message will be sent but request will be tested.\n            data_message (dict, optional): Custom key-value pairs\n            content_available (bool, optional): Inactive client app is awoken\n            android_channel_id (str, optional): Starting in Android 8.0 (API level 26),\n                all notifications must be assigned to a channel. For each channel, you can set the\n                visual and auditory behavior that is applied to all notifications in that channel.\n                Then, users can change these settings and decide which notification channels from\n                your app should be intrusive or visible at all.\n            timeout (int, optional): set time limit for the request\n            extra_notification_kwargs (dict, optional): More notification keyword arguments\n            extra_kwargs (dict, optional): More keyword arguments\n\n        Returns:\n            dict: Response from FCM server (`multicast_id`, `success`, `failure`, `canonical_ids`, `results`)\n\n        Raises:\n            AuthenticationError: If :attr:`api_key` is not set or provided\n                or there is an error authenticating the sender.\n            FCMServerError: Internal server error or timeout error on Firebase cloud messaging server\n            InvalidDataError: Invalid data provided\n            InternalPackageError: Mostly from changes in the response of FCM,\n                contact the project owner to resolve the issue\n        \"\"\"\n        if registration_id is None:\n            raise InvalidDataError('Invalid registration ID')\n        # [registration_id] cos we're sending to a single device\n        payload = self.parse_payload(\n            registration_ids=[registration_id],\n            condition=condition,\n            collapse_key=collapse_key,\n            delay_while_idle=delay_while_idle,\n            time_to_live=time_to_live,\n            restricted_package_name=restricted_package_name,\n            low_priority=low_priority,\n            dry_run=dry_run,\n            data_message=data_message,\n            content_available=content_available,\n            remove_notification=True,\n            android_channel_id=android_channel_id,\n            extra_notification_kwargs=extra_notification_kwargs,\n            **extra_kwargs\n        )\n\n        self.send_request([payload], timeout)\n        return self.parse_responses()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsending a push notification to multiple FCM devices.", "response": "def notify_multiple_devices(self,\n                                registration_ids=None,\n                                message_body=None,\n                                message_title=None,\n                                message_icon=None,\n                                sound=None,\n                                condition=None,\n                                collapse_key=None,\n                                delay_while_idle=False,\n                                time_to_live=None,\n                                restricted_package_name=None,\n                                low_priority=False,\n                                dry_run=False,\n                                data_message=None,\n                                click_action=None,\n                                badge=None,\n                                color=None,\n                                tag=None,\n                                body_loc_key=None,\n                                body_loc_args=None,\n                                title_loc_key=None,\n                                title_loc_args=None,\n                                content_available=None,\n                                android_channel_id=None,\n                                timeout=5,\n                                extra_notification_kwargs=None,\n                                extra_kwargs={}):\n        \"\"\"\n        Sends push notification to multiple devices, can send to over 1000 devices\n\n        Args:\n            registration_ids (list, optional): FCM device registration IDs\n            message_body (str, optional): Message string to display in the notification tray\n            message_title (str, optional): Message title to display in the notification tray\n            message_icon (str, optional): Icon that apperas next to the notification\n            sound (str, optional): The sound file name to play. Specify \"Default\" for device default sound.\n            condition (str, optiona): Topic condition to deliver messages to\n            collapse_key (str, optional): Identifier for a group of messages\n                that can be collapsed so that only the last message gets sent\n                when delivery can be resumed. Defaults to `None`.\n            delay_while_idle (bool, optional): deprecated\n            time_to_live (int, optional): How long (in seconds) the message\n                should be kept in FCM storage if the device is offline. The\n                maximum time to live supported is 4 weeks. Defaults to `None`\n                which uses the FCM default of 4 weeks.\n            restricted_package_name (str, optional): Name of package\n            low_priority (bool, optional): Whether to send notification with\n                the low priority flag. Defaults to `False`.\n            dry_run (bool, optional): If `True` no message will be sent but request will be tested.\n            data_message (dict, optional): Custom key-value pairs\n            click_action (str, optional): Action associated with a user click on the notification\n            badge (str, optional): Badge of notification\n            color (str, optional): Color of the icon\n            tag (str, optional): Group notification by tag\n            body_loc_key (str, optional): Indicates the key to the body string for localization\n            body_loc_args (list, optional): Indicates the string value to replace format\n                specifiers in body string for localization\n            title_loc_key (str, optional): Indicates the key to the title string for localization\n            title_loc_args (list, optional): Indicates the string value to replace format\n                specifiers in title string for localization\n            content_available (bool, optional): Inactive client app is awoken\n            android_channel_id (str, optional): Starting in Android 8.0 (API level 26),\n                all notifications must be assigned to a channel. For each channel, you can set the\n                visual and auditory behavior that is applied to all notifications in that channel.\n                Then, users can change these settings and decide which notification channels from\n                your app should be intrusive or visible at all.\n            timeout (int, optional): set time limit for the request\n            extra_notification_kwargs (dict, optional): More notification keyword arguments\n            extra_kwargs (dict, optional): More keyword arguments\n\n        Returns:\n            dict: Response from FCM server (`multicast_id`, `success`, `failure`, `canonical_ids`, `results`)\n\n        Raises:\n            AuthenticationError: If :attr:`api_key` is not set or provided\n                or there is an error authenticating the sender.\n            FCMServerError: Internal server error or timeout error on Firebase cloud messaging server\n            InvalidDataError: Invalid data provided\n            InternalPackageError: JSON parsing error, mostly from changes in the response of FCM,\n                create a new github issue to resolve it.\n        \"\"\"\n        if not isinstance(registration_ids, list):\n            raise InvalidDataError('Invalid registration IDs (should be list)')\n\n        payloads = []\n\n        registration_id_chunks = self.registration_id_chunks(registration_ids)\n        for registration_ids in registration_id_chunks:\n            # appends a payload with a chunk of registration ids here\n            payloads.append(self.parse_payload(\n                registration_ids=registration_ids,\n                message_body=message_body,\n                message_title=message_title,\n                message_icon=message_icon,\n                sound=sound,\n                condition=condition,\n                collapse_key=collapse_key,\n                delay_while_idle=delay_while_idle,\n                time_to_live=time_to_live,\n                restricted_package_name=restricted_package_name,\n                low_priority=low_priority,\n                dry_run=dry_run, data_message=data_message,\n                click_action=click_action,\n                badge=badge,\n                color=color,\n                tag=tag,\n                body_loc_key=body_loc_key,\n                body_loc_args=body_loc_args,\n                title_loc_key=title_loc_key,\n                title_loc_args=title_loc_args,\n                content_available=content_available,\n                android_channel_id=android_channel_id,\n                extra_notification_kwargs=extra_notification_kwargs,\n                **extra_kwargs\n            ))\n        self.send_request(payloads, timeout)\n        return self.parse_responses()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsending a push message to multiple devices.", "response": "def multiple_devices_data_message(self,\n                                      registration_ids=None,\n                                      condition=None,\n                                      collapse_key=None,\n                                      delay_while_idle=False,\n                                      time_to_live=None,\n                                      restricted_package_name=None,\n                                      low_priority=False,\n                                      dry_run=False,\n                                      data_message=None,\n                                      content_available=None,\n                                      timeout=5,\n                                      extra_notification_kwargs=None,\n                                      extra_kwargs={}):\n        \"\"\"\n        Sends push message to multiple devices, can send to over 1000 devices\n\n        Args:\n            registration_ids (list, optional): FCM device registration IDs\n            condition (str, optiona): Topic condition to deliver messages to\n            collapse_key (str, optional): Identifier for a group of messages\n                that can be collapsed so that only the last message gets sent\n                when delivery can be resumed. Defaults to `None`.\n            delay_while_idle (bool, optional): deprecated\n            time_to_live (int, optional): How long (in seconds) the message\n                should be kept in FCM storage if the device is offline. The\n                maximum time to live supported is 4 weeks. Defaults to `None`\n                which uses the FCM default of 4 weeks.\n            restricted_package_name (str, optional): Name of package\n            low_priority (bool, optional): Whether to send notification with\n                the low priority flag. Defaults to `False`.\n            dry_run (bool, optional): If `True` no message will be sent but request will be tested.\n            data_message (dict, optional): Custom key-value pairs\n            content_available (bool, optional): Inactive client app is awoken\n            timeout (int, optional): set time limit for the request\n            extra_notification_kwargs (dict, optional): More notification keyword arguments\n            extra_kwargs (dict, optional): More keyword arguments\n\n        Returns:\n            dict: Response from FCM server (`multicast_id`, `success`, `failure`, `canonical_ids`, `results`)\n\n        Raises:\n            AuthenticationError: If :attr:`api_key` is not set or provided\n                or there is an error authenticating the sender.\n            FCMServerError: Internal server error or timeout error on Firebase cloud messaging server\n            InvalidDataError: Invalid data provided\n            InternalPackageError: JSON parsing error, mostly from changes in the response of FCM,\n                create a new github issue to resolve it.\n        \"\"\"\n        if not isinstance(registration_ids, list):\n            raise InvalidDataError('Invalid registration IDs (should be list)')\n\n        payloads = []\n        registration_id_chunks = self.registration_id_chunks(registration_ids)\n        for registration_ids in registration_id_chunks:\n            # appends a payload with a chunk of registration ids here\n            payloads.append(self.parse_payload(\n                registration_ids=registration_ids,\n                condition=condition,\n                collapse_key=collapse_key,\n                delay_while_idle=delay_while_idle,\n                time_to_live=time_to_live,\n                restricted_package_name=restricted_package_name,\n                low_priority=low_priority,\n                dry_run=dry_run,\n                data_message=data_message,\n                content_available=content_available,\n                remove_notification=True,\n                extra_notification_kwargs=extra_notification_kwargs,\n                **extra_kwargs)\n            )\n        self.send_request(payloads, timeout)\n        return self.parse_responses()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef topic_subscribers_data_message(self,\n                                       topic_name=None,\n                                       condition=None,\n                                       collapse_key=None,\n                                       delay_while_idle=False,\n                                       time_to_live=None,\n                                       restricted_package_name=None,\n                                       low_priority=False,\n                                       dry_run=False,\n                                       data_message=None,\n                                       content_available=None,\n                                       timeout=5,\n                                       extra_notification_kwargs=None,\n                                       extra_kwargs={}):                                 \n        \"\"\"\n        Sends data notification to multiple devices subscribed to a topic\n        Args:\n            topic_name (topic_name): Name of the topic to deliver messages to\n            condition (condition): Topic condition to deliver messages to\n            A topic name is a string that can be formed with any character in [a-zA-Z0-9-_.~%]\n            data_message (dict): Data message payload to send alone or with the notification message\n            \n        Keyword Args:\n            collapse_key (str, optional): Identifier for a group of messages\n                that can be collapsed so that only the last message gets sent\n                when delivery can be resumed. Defaults to ``None``.\n            delay_while_idle (bool, optional): If ``True`` indicates that the\n                message should not be sent until the device becomes active.\n            time_to_live (int, optional): How long (in seconds) the message\n                should be kept in FCM storage if the device is offline. The\n                maximum time to live supported is 4 weeks. Defaults to ``None``\n                which uses the FCM default of 4 weeks.\n            low_priority (boolean, optional): Whether to send notification with\n                the low priority flag. Defaults to ``False``.\n            restricted_package_name (str, optional): Package name of the\n                application where the registration IDs must match in order to\n                receive the message. Defaults to ``None``.\n            dry_run (bool, optional): If ``True`` no message will be sent but\n                request will be tested.\n            \n        Returns:\n            :tuple:`multicast_id(long), success(int), failure(int), canonical_ids(int), results(list)`:\n            Response from FCM server.\n        Raises:\n            AuthenticationError: If :attr:`api_key` is not set or provided or there is an error authenticating the sender.\n            FCMServerError: Internal server error or timeout error on Firebase cloud messaging server\n            InvalidDataError: Invalid data provided\n            InternalPackageError: JSON parsing error, mostly from changes in the response of FCM, create a new github issue to resolve it.\n        \"\"\"\n        if extra_kwargs is None:\n            extra_kwargs = {}\n        payload = self.parse_payload(topic_name=topic_name,\n                                     condition=condition,\n                                     collapse_key=collapse_key,\n                                     delay_while_idle=delay_while_idle,\n                                     time_to_live=time_to_live,\n                                     restricted_package_name=restricted_package_name,\n                                     low_priority=low_priority,\n                                     dry_run=dry_run,\n                                     data_message=data_message,\n                                     content_available=content_available,\n                                     remove_notification=True,\n                                     extra_notification_kwargs=extra_notification_kwargs,\n                                     **extra_kwargs)\n        self.send_request([payload], timeout)\n        return self.parse_responses()", "response": "This function is used to send a data message to multiple topics."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncopies the value of keys from source object to destination object", "response": "def get_keys(src, dst, keys):\n    \"\"\"\n    Copies the value of keys from source object to dest object\n\n    :param src:\n    :param dst:\n    :param keys:\n    :return:\n    \"\"\"\n    for key in keys:\n        #dst[no_camel(key)] = src[key] if key in src else None\n        dst[key] = src[key] if key in src else None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef no_camel(name):\n    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()", "response": "Converts CamelCase to camel_case"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_throttled(e):\n    return True if (hasattr(e, 'response') and\n                    e.response is not None and\n                    'Error' in e.response and\n                    e.response['Error']['Code'] in ['Throttling', 'RequestLimitExceeded', 'ThrottlingException']) else \\\n        False", "response": "Determines whether the exception is due to API throttling."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_instance(self, global_params, region, reservation):\n        for i in reservation['Instances']:\n            instance = {}\n            vpc_id = i['VpcId'] if 'VpcId' in i and i['VpcId'] else ec2_classic\n            manage_dictionary(self.vpcs, vpc_id, VPCConfig(self.vpc_resource_types))\n            instance['reservation_id'] = reservation['ReservationId']\n            instance['id'] = i['InstanceId']\n            get_name(i, instance, 'InstanceId')\n            get_keys(i, instance, ['KeyName', 'LaunchTime', 'InstanceType', 'State', 'IamInstanceProfile', 'SubnetId'])\n            # Network interfaces & security groups\n            manage_dictionary(instance, 'network_interfaces', {})\n            for eni in i['NetworkInterfaces']:\n                nic = {}\n                get_keys(eni, nic, ['Association', 'Groups', 'PrivateIpAddresses', 'SubnetId', 'Ipv6Addresses'])\n                instance['network_interfaces'][eni['NetworkInterfaceId']] = nic\n            self.vpcs[vpc_id].instances[i['InstanceId']] = instance", "response": "Parse a single EC2 instance and store it in self. sids"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_security_group(self, global_params, region, group):\n        vpc_id = group['VpcId'] if 'VpcId' in group and group['VpcId'] else ec2_classic\n        manage_dictionary(self.vpcs, vpc_id, VPCConfig(self.vpc_resource_types))\n        security_group = {}\n        security_group['name'] = group['GroupName']\n        security_group['id'] = group['GroupId']\n        security_group['description'] = group['Description']\n        security_group['owner_id'] = group['OwnerId']\n        security_group['rules'] = {'ingress': {}, 'egress': {}}\n        security_group['rules']['ingress']['protocols'], security_group['rules']['ingress']['count'] = self.__parse_security_group_rules(group['IpPermissions'])\n        security_group['rules']['egress']['protocols'], security_group['rules']['egress']['count'] = self.__parse_security_group_rules(group['IpPermissionsEgress'])\n        self.vpcs[vpc_id].security_groups[group['GroupId']] = security_group", "response": "Parse a single security group into a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_cluster(self, global_params, region, cluster):\n        cluster_name = cluster.pop('CacheClusterId')\n        cluster['name'] = cluster_name\n        # Must fetch info about the subnet group to retrieve the VPC ID...\n        if 'CacheSubnetGroupName' in cluster:\n            subnet_group = api_clients[region].describe_cache_subnet_groups(CacheSubnetGroupName = cluster['CacheSubnetGroupName'])['CacheSubnetGroups'][0]\n            vpc_id = subnet_group['VpcId']\n        else:\n            vpc_id = ec2_classic\n            subnet_group = None\n        manage_dictionary(self.vpcs, vpc_id, VPCConfig(self.vpc_resource_types))\n        self.vpcs[vpc_id].clusters[cluster_name] = cluster\n        if subnet_group:\n            self.vpcs[vpc_id].subnet_groups[subnet_group['CacheSubnetGroupName']] = subnet_group", "response": "Parse a single ElastiCache cluster and store the result in self. vpc_config"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse a single Redshift cluster into a dictionary of attributes.", "response": "def parse_cluster(self, global_params, region, cluster):\n        \"\"\"\n        Parse a single Redshift cluster\n\n        :param global_params:           Parameters shared for all regions\n        :param region:                  Name of the AWS region\n        :param cluster:                 Cluster\n        \"\"\"\n        vpc_id = cluster.pop('VpcId') if 'VpcId' in cluster else ec2_classic\n        manage_dictionary(self.vpcs, vpc_id, VPCConfig(self.vpc_resource_types))\n        name = cluster.pop('ClusterIdentifier')\n        cluster['name'] = name\n        self.vpcs[vpc_id].clusters[name] = cluster"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_parameter_group(self, global_params, region, parameter_group):\n        pg_name = parameter_group.pop('ParameterGroupName')\n        pg_id = self.get_non_aws_id(pg_name) # Name could be used as only letters digits or hyphens\n        parameter_group['name'] = pg_name\n        parameter_group['parameters'] = {}\n        api_client = api_clients[region]\n        parameters = handle_truncated_response(api_client.describe_cluster_parameters, {'ParameterGroupName': pg_name}, ['Parameters'])['Parameters']\n        for parameter in parameters:\n            param = {}\n            param['value'] = parameter['ParameterValue']\n            param['source'] = parameter['Source']\n            parameter_group['parameters'][parameter['ParameterName']] = param\n        (self).parameter_groups[pg_id] = parameter_group", "response": "Parse a single Redshift parameter group and fetch all of its parameters"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a single Redsfhit security group", "response": "def parse_security_group(self, global_params, region, security_group):\n        \"\"\"\n        Parse a single Redsfhit security group\n\n        :param global_params:           Parameters shared for all regions\n        :param region:                  Name of the AWS region\n        :param security)_group:         Security group\n        \"\"\"\n        name = security_group.pop('ClusterSecurityGroupName')\n        security_group['name'] = name\n        self.security_groups['name'] = security_group"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_domains(self, domain, params):\n        domain_id = self.get_non_aws_id(domain['DomainName'])\n        domain['name'] = domain.pop('DomainName')\n        #TODO: Get Dnssec info when available\n        #api_client = params['api_client']\n        #details = api_client.get_domain_detail(DomainName = domain['name'])\n        #get_keys(details, domain, ['Dnssec'])\n        self.domains[domain_id] = domain", "response": "Parse a single Route53Domains object"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_hosted_zones(self, hosted_zone, params):\n        # When resuming upon throttling error, skip if already fetched\n        hosted_zone_id = hosted_zone.pop('Id')\n        hosted_zone['name'] = hosted_zone.pop('Name')\n        api_client = params['api_client']\n        record_sets = handle_truncated_response(api_client.list_resource_record_sets, {'HostedZoneId': hosted_zone_id}, ['ResourceRecordSets'])\n        hosted_zone.update(record_sets)\n        #print(str(record_sets))\n        #record_sets = api_client.list_resource_record_sets()\n        #hosted_zone['RecordSets'] = record_sets['Resourc']\n        self.hosted_zones[hosted_zone_id] = hosted_zone", "response": "Parse a single Route53hosted_zoness hosted_zone"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_stack(self, global_params, region, stack):\n        stack['id'] = stack.pop('StackId')\n        stack['name'] = stack.pop('StackName')\n        stack_policy = api_clients[region].get_stack_policy(StackName = stack['name'])\n        if 'StackPolicyBody' in stack_policy:\n            stack['policy'] = json.loads(stack_policy['StackPolicyBody'])\n        self.stacks[stack['name']] = stack", "response": "Parse a single stack and fetch additional attributes"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_subscription(self, params, region, subscription):\n        topic_arn = subscription.pop('TopicArn')\n        topic_name = topic_arn.split(':')[-1]\n        if topic_name in self.topics:\n            topic = self.topics[topic_name]\n            manage_dictionary(topic['subscriptions'], 'protocol', {})\n            protocol = subscription.pop('Protocol')\n            manage_dictionary(topic['subscriptions']['protocol'], protocol, [])\n            topic['subscriptions']['protocol'][protocol].append(subscription)\n            topic['subscriptions_count'] += 1", "response": "Parse a single subscription and reference it in its corresponding topic"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_topic(self, params, region, topic):\n        topic['arn'] = topic.pop('TopicArn')\n        topic['name'] = topic['arn'].split(':')[-1]\n        (prefix, partition, service, region, account, name) = topic['arn'].split(':')\n        api_client = api_clients[region]\n        attributes = api_client.get_topic_attributes(TopicArn=topic['arn'])['Attributes']\n        for k in ['Owner', 'DisplayName']:\n            topic[k] = attributes[k] if k in attributes else None\n        for k in ['Policy', 'DeliveryPolicy', 'EffectiveDeliveryPolicy']:\n            topic[k] = json.loads(attributes[k]) if k in attributes else None\n        topic['name'] = topic['arn'].split(':')[-1]\n        manage_dictionary(topic, 'subscriptions', {})\n        manage_dictionary(topic, 'subscriptions_count', 0)\n        self.topics[topic['name']] = topic", "response": "Parse a single topic and fetch additional attributes"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses a single EMR cluster", "response": "def parse_cluster(self, global_params, region, cluster):\n        \"\"\"\n        Parse a single EMR cluster\n\n        :param global_params:           Parameters shared for all regions\n        :param region:                  Name of the AWS region\n        :param cluster:                 EMR cluster\n        \"\"\"\n        cluster_id = cluster['Id']\n        cluster = api_clients[region].describe_cluster(ClusterId = cluster_id)['Cluster']\n        cluster['id'] = cluster.pop('Id')\n        cluster['name'] = cluster.pop('Name')\n        vpc_id = 'TODO' # The EMR API won't disclose the VPC ID, so wait until all configs have been fetch and look up the VPC based on the subnet ID\n        manage_dictionary(self.vpcs, vpc_id, VPCConfig(self.vpc_resource_types))\n        self.vpcs[vpc_id].clusters[cluster_id] = cluster"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a unique ID for the given name.", "response": "def get_non_aws_id(self, name):\n        \"\"\"\n        Not all AWS resources have an ID and some services allow the use of \".\" in names, which break's Scout2's\n        recursion scheme if name is used as an ID. Use SHA1(name) instead.\n\n        :param name:                    Name of the resource to\n        :return:                        SHA1(name)\n        \"\"\"\n        m = sha1()\n        m.update(name.encode('utf-8'))\n        return m.hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_identitie(self, global_params, region, identity_name):\n        identity = {'name': identity_name, 'policies': {}}\n        policy_names = api_clients[region].list_identity_policies(Identity = identity_name)['PolicyNames']\n        if len(policy_names):\n            policies = api_clients[region].get_identity_policies(Identity = identity_name, PolicyNames = policy_names)['Policies']\n            for policy_name in policies:\n                identity['policies'][policy_name] = json.loads(policies[policy_name])\n        dkim = api_clients[region].get_identity_dkim_attributes(Identities = [ identity_name ])['DkimAttributes'][identity_name]\n        identity['DkimEnabled'] = dkim['DkimEnabled']\n        identity['DkimVerificationStatus'] = dkim['DkimVerificationStatus']\n        self.identities[self.get_non_aws_id(identity_name)] = identity", "response": "Parse a single identity and fetch additional attributes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npassing all conditions? :param all_info: :param current_path: :param conditions: :param unknown_as_pass_condition: Consider an undetermined condition as passed :return:", "response": "def pass_conditions(all_info, current_path, conditions, unknown_as_pass_condition = False):\n    \"\"\"\n    Pass all conditions?\n\n    :param all_info:\n    :param current_path:\n    :param conditions:\n    :param unknown_as_pass_condition:   Consider an undetermined condition as passed\n    :return:\n    \"\"\"\n    result = False\n    if len(conditions) == 0:\n        return True\n    condition_operator = conditions.pop(0)\n    for condition in conditions:\n        if condition[0] in condition_operators:\n            res = pass_conditions(all_info, current_path, condition, unknown_as_pass_condition)\n        else:\n            # Conditions are formed as \"path to value\", \"type of test\", \"value(s) for test\"\n            path_to_value, test_name, test_values = condition\n            path_to_value = fix_path_string(all_info, current_path, path_to_value)\n            target_obj = get_value_at(all_info, current_path, path_to_value)\n            if type(test_values) != list:\n                dynamic_value = re_get_value_at.match(test_values)\n                if dynamic_value:\n                    test_values = get_value_at(all_info, current_path, dynamic_value.groups()[0], True)\n            try:\n                res = pass_condition(target_obj, test_name, test_values)\n            except Exception as e:\n                res = True if unknown_as_pass_condition else False\n                printError('Unable to process testcase \\'%s\\' on value \\'%s\\', interpreted as %s.' % (test_name, str(target_obj), res))\n                printException(e, True)\n        # Quick exit and + false\n        if condition_operator == 'and' and not res:\n            return False\n        # Quick exit or + true\n        if condition_operator == 'or' and res:\n            return True\n    # Still here ?\n    # or -> false\n    # and -> true\n    if condition_operator == 'or':\n        return False\n    else:\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses a single connection and fetch additional attributes", "response": "def parse_connection(self, global_params, region, connection):\n        \"\"\"\n        Parse a single connection and fetch additional attributes\n\n        :param global_params:           Parameters shared for all regions\n        :param region:                  Name of the AWS region\n        :param connection_url:               URL of the AWS connection\n        \"\"\"\n        connection['id'] = connection.pop('connectionId')\n        connection['name'] = connection.pop('connectionName')\n        self.connections[connection['id']] = connection"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef format_listall_output(format_file, format_item_dir, format, rule, option_prefix = None, template = None, skip_options = False):\n    # Set the list of keys if printing from a file spec\n    # _LINE_(whatever)_EOL_\n    # _ITEM_(resource)_METI_\n    # _KEY_(path_to_value)\n    if format_file and os.path.isfile(format_file):\n        if not template:\n            with open(format_file, 'rt') as f:\n                template = f.read()\n        # Optional files\n        if not skip_options:\n            re_option = re.compile(r'(%_OPTION_\\((.*?)\\)_NOITPO_)')\n            optional_files = re_option.findall(template)\n            for optional_file in optional_files:\n                if optional_file[1].startswith(option_prefix + '-'):\n                    with open(os.path.join(format_item_dir, optional_file[1].strip()), 'rt') as f:\n                        template = template.replace(optional_file[0].strip(), f.read())\n        # Include files if needed\n        re_file = re.compile(r'(_FILE_\\((.*?)\\)_ELIF_)')\n        while True:\n            requested_files = re_file.findall(template)\n            available_files = os.listdir(format_item_dir) if format_item_dir else []\n            for requested_file in requested_files:\n                if requested_file[1].strip() in available_files:\n                    with open(os.path.join(format_item_dir, requested_file[1].strip()), 'rt') as f:\n                        template = template.replace(requested_file[0].strip(), f.read())\n            # Find items and keys to be printed\n            re_line = re.compile(r'(_ITEM_\\((.*?)\\)_METI_)')\n            re_key = re.compile(r'_KEY_\\(*(.*?)\\)', re.DOTALL|re.MULTILINE) # Remove the multiline ?\n            lines = re_line.findall(template)\n            for (i, line) in enumerate(lines):\n                lines[i] = line + (re_key.findall(line[1]),)\n            requested_files = re_file.findall(template)\n            if len(requested_files) == 0:\n                break\n    elif format and format[0] == 'csv':\n        keys = rule.keys\n        line = ', '.join('_KEY_(%s)' % k for k in keys)\n        lines = [ (line, line, keys) ]\n        template = line\n    return (lines, template)", "response": "Format the listall output of a single item in a single file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generate_listall_output(lines, resources, aws_config, template, arguments, nodup = False):\n    for line in lines:\n        output = []\n        for resource in resources:\n            current_path = resource.split('.')\n            outline = line[1]\n            for key in line[2]:\n                outline = outline.replace('_KEY_('+key+')', get_value_at(aws_config['services'], current_path, key, True))\n            output.append(outline)\n        output = '\\n'.join(line for line in sorted(set(output)))\n        template = template.replace(line[0], output)\n    for (i, argument) in enumerate(arguments):\n        template = template.replace('_ARG_%d_' % i, argument)\n    return template", "response": "Format and print the output of ListAll\n archive"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing a single CloudTrail trail and store it in self. trails.", "response": "def parse_trail(self, global_params, region, trail):\n        \"\"\"\n        Parse a single CloudTrail trail\n\n        :param global_params:           Parameters shared for all regions\n        :param region:                  Name of the AWS region\n        :param cluster:                 Trail\n        \"\"\"\n        trail_config = {}\n        trail_config['name'] = trail.pop('Name')\n        trail_id = self.get_non_aws_id(trail_config['name'])\n        trail_details = None\n\n        api_client = api_clients[region]\n\n        # Do not duplicate entries for multiregion trails\n        if 'IsMultiRegionTrail' in trail and trail['IsMultiRegionTrail'] and trail['HomeRegion'] != region:\n            for key in ['HomeRegion', 'TrailARN']:\n                trail_config[key] = trail[key]\n            trail_config['scout2_link'] = 'services.cloudtrail.regions.%s.trails.%s' % (trail['HomeRegion'], trail_id)\n        else:\n            for key in trail:\n                trail_config[key] = trail[key]\n            trail_config['bucket_id'] = self.get_non_aws_id(trail_config.pop('S3BucketName'))\n            for key in ['IsMultiRegionTrail', 'LogFileValidationEnabled']:\n                if key not in trail_config:\n                    trail_config[key] = False\n            trail_details = api_client.get_trail_status(Name=trail['TrailARN'])\n            for key in ['IsLogging', 'LatestDeliveryTime', 'LatestDeliveryError', 'StartLoggingTime', 'StopLoggingTime', 'LatestNotificationTime', 'LatestNotificationError', 'LatestCloudWatchLogsDeliveryError', 'LatestCloudWatchLogsDeliveryTime']:\n                trail_config[key] = trail_details[key] if key in trail_details else None\n\n        if trail_details:\n            trail_config['wildcard_data_logging'] = self.data_logging_status(trail_config['name'], trail_details, api_client)\n\n        self.trails[trail_id] = trail_config"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfetch the credential report from the API.", "response": "def fetch_credential_report(self, credentials, ignore_exception = False):\n        \"\"\"\n        Fetch the credential report\n\n        :param: api_client\n        :type: FOO\n        :param: ignore_exception : initiate credential report creation as not  always ready\n        :type: Boolean\n        \"\"\"\n        iam_report = {}\n        try:\n            api_client = connect_service('iam', credentials, silent = True)\n            response = api_client.generate_credential_report()\n            if response['State'] != 'COMPLETE':\n                if not ignore_exception:\n                    printError('Failed to generate a credential report.')\n                return\n            report = api_client.get_credential_report()['Content']\n            lines = report.splitlines()\n            keys = lines[0].decode('utf-8').split(',')\n            for line in lines[1:]:\n                values = line.decode('utf-8').split(',')\n                manage_dictionary(iam_report, values[0], {})\n                for key, value in zip(keys, values):\n                    iam_report[values[0]][key] = value\n            self.credential_report = iam_report\n            self.fetchstatuslogger.counts['credential_report']['fetched'] = 1\n        except Exception as e:\n            if ignore_exception:\n                return\n            printError('Failed to download a credential report.')\n            printException(e)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses a single IAM group and fetch additional information", "response": "def parse_groups(self, group, params):\n        \"\"\"\n        Parse a single IAM group and fetch additional information\n        \"\"\"\n        # When resuming upon throttling error, skip if already fetched\n        if group['GroupName'] in self.groups:\n            return\n        api_client = params['api_client']\n        # Ensure consistent attribute names across resource types\n        group['id'] = group.pop('GroupId')\n        group['name'] = group.pop('GroupName')\n        group['arn'] = group.pop('Arn')\n        # Get group's members\n        group['users'] = self.__fetch_group_users(api_client, group['name']);\n        # Get inline policies\n        policies = self.__get_inline_policies(api_client, 'group', group['id'], group['name'])\n        if len(policies):\n            group['inline_policies'] = policies\n        group['inline_policies_count'] = len(policies)\n        self.groups[group['id']] = group"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses a single IAM policy and fetch additional information", "response": "def parse_policies(self, fetched_policy, params):\n        \"\"\"\n        Parse a single IAM policy and fetch additional information\n        \"\"\"\n        api_client = params['api_client']\n        policy = {}\n        policy['name'] = fetched_policy.pop('PolicyName')\n        policy['id'] = fetched_policy.pop('PolicyId')\n        policy['arn'] = fetched_policy.pop('Arn')\n        # Download version and document\n        policy_version = api_client.get_policy_version(PolicyArn = policy['arn'], VersionId = fetched_policy['DefaultVersionId'])\n        policy_version = policy_version['PolicyVersion']\n        policy['PolicyDocument'] = policy_version['Document']\n        # Get attached IAM entities\n        policy['attached_to'] = {}\n        attached_entities = handle_truncated_response(api_client.list_entities_for_policy, {'PolicyArn': policy['arn']}, ['PolicyGroups', 'PolicyRoles', 'PolicyUsers'])\n        for entity_type in attached_entities:\n            resource_type = entity_type.replace('Policy', '').lower()\n            if len(attached_entities[entity_type]):\n                policy['attached_to'][resource_type] = []\n            for entity in attached_entities[entity_type]:\n                name_field = entity_type.replace('Policy', '')[:-1] + 'Name'\n                resource_name = entity[name_field]\n                policy['attached_to'][resource_type].append({'name': resource_name})\n        # Save policy\n        self.policies[policy['id']] = policy"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfetch the password policy that applies to all IAM users within the AWS account.", "response": "def fetch_password_policy(self, credentials):\n        \"\"\"\n        Fetch the password policy that applies to all IAM users within the AWS account\n        \"\"\"\n        self.fetchstatuslogger.counts['password_policy']['discovered'] = 0\n        self.fetchstatuslogger.counts['password_policy']['fetched'] = 0\n        try:\n            api_client = connect_service('iam', credentials, silent = True)\n            self.password_policy = api_client.get_account_password_policy()['PasswordPolicy']\n            if 'PasswordReusePrevention' not in self.password_policy:\n                self.password_policy['PasswordReusePrevention'] = False\n            else:\n                self.password_policy['PreviousPasswordPrevented'] = self.password_policy['PasswordReusePrevention']\n                self.password_policy['PasswordReusePrevention'] = True\n            # There is a bug in the API: ExpirePasswords always returns false\n            if 'MaxPasswordAge' in self.password_policy:\n                self.password_policy['ExpirePasswords'] = True\n            self.fetchstatuslogger.counts['password_policy']['discovered'] = 1\n            self.fetchstatuslogger.counts['password_policy']['fetched'] = 1\n\n        except ClientError as e:\n            if e.response['Error']['Code'] == 'NoSuchEntity':\n                    self.password_policy = {}\n                    self.password_policy['MinimumPasswordLength'] = '1' # As of 10/10/2016, 1-character passwords were authorized when no policy exists, even though the console displays 6\n                    self.password_policy['RequireUppercaseCharacters'] = False\n                    self.password_policy['RequireLowercaseCharacters'] = False\n                    self.password_policy['RequireNumbers'] = False\n                    self.password_policy['RequireSymbols'] = False\n                    self.password_policy['PasswordReusePrevention'] = False\n                    self.password_policy['ExpirePasswords'] = False\n            else:\n                raise e\n        except Exception as e:\n            printError(str(e))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_roles(self, fetched_role, params):\n        role = {}\n        role['instances_count'] = 'N/A'\n        # When resuming upon throttling error, skip if already fetched\n        if fetched_role['RoleName'] in self.roles:\n            return\n        api_client = params['api_client']\n        # Ensure consistent attribute names across resource types\n        role['id'] = fetched_role.pop('RoleId')\n        role['name'] = fetched_role.pop('RoleName')\n        role['arn'] = fetched_role.pop('Arn')\n        # Get other attributes\n        get_keys(fetched_role, role, [ 'CreateDate', 'Path'])\n        # Get role policies\n        policies = self.__get_inline_policies(api_client, 'role', role['id'], role['name'])\n        if len(policies):\n            role['inline_policies'] = policies\n        role['inline_policies_count'] = len(policies)\n        # Get instance profiles\n        profiles = handle_truncated_response(api_client.list_instance_profiles_for_role, {'RoleName': role['name']}, ['InstanceProfiles'])\n        manage_dictionary(role, 'instance_profiles', {})\n        for profile in profiles['InstanceProfiles']:\n            manage_dictionary(role['instance_profiles'], profile['InstanceProfileId'], {})\n            role['instance_profiles'][profile['InstanceProfileId']]['arn'] = profile['Arn']\n            role['instance_profiles'][profile['InstanceProfileId']]['name'] = profile['InstanceProfileName']\n        # Get trust relationship\n        role['assume_role_policy'] = {}\n        role['assume_role_policy']['PolicyDocument'] = fetched_role.pop('AssumeRolePolicyDocument')\n        # Save role\n        self.roles[role['id']] = role", "response": "Parse a single IAM role and fetch additional data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_users(self, user, params):\n        if user['UserName'] in self.users:\n            return\n        api_client = params['api_client']\n        # Ensure consistent attribute names across resource types\n        user['id'] = user.pop('UserId')\n        user['name'] = user.pop('UserName')\n        user['arn'] = user.pop('Arn')\n        policies = self.__get_inline_policies(api_client, 'user', user['id'], user['name'])\n        if len(policies):\n            user['inline_policies'] = policies\n        user['inline_policies_count'] = len(policies)\n        user['groups'] = []\n        groups = handle_truncated_response(api_client.list_groups_for_user, {'UserName': user['name']}, ['Groups'])['Groups']\n        for group in groups:\n            user['groups'].append(group['GroupName'])\n        try:\n            user['LoginProfile'] = api_client.get_login_profile(UserName = user['name'])['LoginProfile']\n        except Exception as e:\n            pass\n        user['AccessKeys'] = api_client.list_access_keys(UserName = user['name'])['AccessKeyMetadata']\n        user['MFADevices'] = api_client.list_mfa_devices(UserName = user['name'])['MFADevices']\n        # TODO: Users signing certss\n        self.users[user['id']] = user", "response": "Parse a single IAM user and fetch additional data"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a display name for all known CIDRs", "response": "def put_cidr_name(aws_config, current_config, path, current_path, resource_id, callback_args):\n    \"\"\"\n    Add a display name for all known CIDRs\n    :param aws_config:\n    :param current_config:\n    :param path:\n    :param current_path:\n    :param resource_id:\n    :param callback_args:\n    :return:\n    \"\"\"\n    if 'cidrs' in current_config:\n        cidr_list = []\n        for cidr in current_config['cidrs']:\n            if type(cidr) == dict:\n                cidr = cidr['CIDR']\n            if cidr in known_cidrs:\n                cidr_name = known_cidrs[cidr]\n            else:\n                cidr_name = get_cidr_name(cidr, callback_args['ip_ranges'], callback_args['ip_ranges_name_key'])\n                known_cidrs[cidr] = cidr_name\n            cidr_list.append({'CIDR': cidr, 'CIDRName': cidr_name})\n        current_config['cidrs'] = cidr_list"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading display name for CIDRs from ip - ranges files and return it", "response": "def get_cidr_name(cidr, ip_ranges_files, ip_ranges_name_key):\n    \"\"\"\n    Read display name for CIDRs from ip-ranges files\n    :param cidr:\n    :param ip_ranges_files:\n    :param ip_ranges_name_key:\n    :return:\n    \"\"\"\n    for filename in ip_ranges_files:\n        ip_ranges = read_ip_ranges(filename, local_file=True)\n        for ip_range in ip_ranges:\n            ip_prefix = netaddr.IPNetwork(ip_range['ip_prefix'])\n            cidr = netaddr.IPNetwork(cidr)\n            if cidr in ip_prefix:\n                return ip_range[ip_ranges_name_key].strip()\n    for ip_range in aws_ip_ranges:\n        ip_prefix = netaddr.IPNetwork(ip_range['ip_prefix'])\n        cidr = netaddr.IPNetwork(cidr)\n        if cidr in ip_prefix:\n            return 'Unknown CIDR in %s %s' % (ip_range['service'], ip_range['region'])\n    return 'Unknown CIDR'"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef propagate_vpc_names(aws_config, current_config, path, current_path, resource_id, callback_args):\n    if resource_id == ec2_classic:\n        current_config['name'] = ec2_classic\n    else:\n        target_path = copy.deepcopy(current_path)\n        target_path[1] = 'ec2'\n        target_path.append(resource_id)\n        target_path.append('Name')\n        target_path = '.'.join(target_path)\n        current_config['name'] = get_value_at(aws_config, target_path, target_path)", "response": "Propagate VPC names in VPC - related services"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the flow logs that cover a given subnet", "response": "def get_subnet_flow_logs_list(current_config, subnet):\n    \"\"\"\n    Return the flow logs that cover a given subnet\n\n    :param current_config:\n    :param subnet: the subnet that the flow logs should cover\n    :return:\n    \"\"\"\n    flow_logs_list = []\n    for flow_log in current_config.flow_logs:\n        if current_config.flow_logs[flow_log]['ResourceId'] == subnet['SubnetId'] or \\\n                current_config.flow_logs[flow_log]['ResourceId'] == subnet['VpcId']:\n            flow_logs_list.append(flow_log)\n    return flow_logs_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_subnet(self, global_params, region, subnet):\n        vpc_id = subnet['VpcId']\n        manage_dictionary(self.vpcs, vpc_id, SingleVPCConfig(self.vpc_resource_types))\n        subnet_id = subnet['SubnetId']\n        get_name(subnet, subnet, 'SubnetId')\n        # set flow logs that cover this subnet\n        subnet['flow_logs'] = get_subnet_flow_logs_list(self, subnet)\n        # Save\n        manage_dictionary(self.vpcs, vpc_id, SingleVPCConfig(self.vpc_resource_types))\n        self.vpcs[vpc_id].subnets[subnet_id] = subnet", "response": "Parse subnet object.\n\n        :param global_params:\n        :param region:\n        :param subnet:\n        :return:"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_alarm(self, global_params, region, alarm):\n        alarm['arn'] = alarm.pop('AlarmArn')\n        alarm['name'] = alarm.pop('AlarmName')\n        # Drop some data\n        for k in ['AlarmConfigurationUpdatedTimestamp', 'StateReason', 'StateReasonData', 'StateUpdatedTimestamp']:\n            foo = alarm.pop(k) if k in alarm else None\n        alarm_id = self.get_non_aws_id(alarm['arn'])\n        self.alarms[alarm_id] = alarm", "response": "Parse a single CloudWatch trail and store the result in self. alarms."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the value of a given key in a given path.", "response": "def get_attribute_at(config, target_path, key, default_value=None):\n    \"\"\"\n    Return attribute value at a given path\n\n    :param config:\n    :param target_path:\n    :param key:\n    :param default_value:\n    :return:\n    \"\"\"\n    for target in target_path:\n        config = config[target]\n    return config[key] if key in config else default_value"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets arbitrary object given a dictionary and a list of keys", "response": "def get_object_at(dictionary, path, attribute_name=None):\n    \"\"\"\n    Get arbitrary object given a dictionary and path (list of keys)\n\n    :param dictionary:\n    :param path:\n    :param attribute_name:\n    :return:\n    \"\"\"\n    o = dictionary\n    try:\n        for p in path:\n            o = o[p]\n        if attribute_name:\n            return o[attribute_name]\n        else:\n            return o\n    # failed to find attribute/key in dictionary\n    except Exception:\n        return {}"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_value_at(all_info, current_path, key, to_string=False):\n    keys = key.split('.')\n    if keys[-1] == 'id':\n        target_obj = current_path[len(keys)-1]\n    else:\n        if key == 'this':\n            target_path = current_path\n        elif '.' in key:\n            target_path = []\n            for i, key in enumerate(keys):\n                if key == 'id':\n                    target_path.append(current_path[i])\n                else:\n                    target_path.append(key)\n            if len(keys) > len(current_path):\n                target_path = target_path + keys[len(target_path):]\n        else:\n            target_path = copy.deepcopy(current_path)\n            target_path.append(key)\n        target_obj = all_info\n        for p in target_path:\n          try:\n            if type(target_obj) == list and type(target_obj[0]) == dict:\n                target_obj = target_obj[int(p)]\n            elif type(target_obj) == list:\n                target_obj = p\n            elif p == '':\n                target_obj = target_obj\n            else:\n              try:\n                target_obj = target_obj[p]\n              except Exception as e:\n                printInfo('Info: %s\\n'\n                          'Path: %s\\n'\n                          'Key: %s' % (str(all_info),\n                                       str(current_path),\n                                       str(key)))\n                printException(e)\n                raise Exception\n          except Exception as e:\n            printInfo('Info: %s\\n'\n                      'Path: %s\\n'\n                      'Key: %s' % (str(all_info),\n                                   str(current_path),\n                                   str(key)))\n            printException(e)\n            raise Exception\n    if to_string:\n        return str(target_obj)\n    else:\n        return target_obj", "response": "Get value located at a given path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load(self):\n        file_name_valid = False\n        rule_type_valid = False\n        # Look for a locally-defined rule\n        for rule_dir in self.rule_dirs:\n            file_path = os.path.join(rule_dir, self.file_name) if rule_dir else self.file_name\n            if os.path.isfile(file_path):\n                self.file_path = file_path\n                file_name_valid = True\n                break\n        # Look for a built-in rule\n        if not file_name_valid:\n            for rule_type in self.rule_types:\n                if self.file_name.startswith(rule_type):\n                    self.file_path = os.path.join(self.rules_data_path, self.file_name)\n                    rule_type_valid = True\n                    file_name_valid = True\n                    break\n            if not rule_type_valid:\n                for rule_type in self.rule_types:\n                    self.file_path = os.path.join(self.rules_data_path, rule_type, self.file_name)\n                    if os.path.isfile(self.file_path):\n                        file_name_valid = True\n                        break\n            else:\n                if os.path.isfile(self.file_path):\n                    file_name_valid = True\n        if not file_name_valid:\n            printError('Error: could not find %s' % self.file_name)\n        else:\n            try:\n                with open(self.file_path, 'rt') as f:\n                    self.string_definition = f.read()\n                    self.load_from_string_definition()\n            except Exception as e:\n                printException(e)\n                printError('Failed to load rule defined in %s' % file_path)", "response": "Load the definition of the rule from the specified file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninitializing the region s configuration", "response": "def init_region_config(self, region):\n        \"\"\"\n        Initialize the region's configuration\n\n        :param region:                  Name of the region\n        \"\"\"\n        self.regions[region] = self.region_config_class(region_name = region, resource_types = self.resource_types)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfetching all the configuration supported by Scout2 for a given service and regions.", "response": "def fetch_all(self, credentials, regions = [], partition_name = 'aws', targets = None):\n        \"\"\"\n        Fetch all the configuration supported by Scout2 for a given service\n\n        :param credentials:             F\n        :param service:                 Name of the service\n        :param regions:                 Name of regions to fetch data from\n        :param partition_name:          AWS partition to connect to\n        :param targets:                 Type of resources to be fetched; defaults to all.\n\n        \"\"\"\n        # Initialize targets\n        # Tweak params\n        realtargets = ()\n        if not targets:\n            targets = self.targets\n        for i, target in enumerate(targets['first_region']):\n            params = self.tweak_params(target[3], credentials)\n            realtargets = realtargets + ((target[0], target[1], target[2], params, target[4]),)\n        targets['first_region'] = realtargets\n        realtargets = ()\n        for i, target in enumerate(targets['other_regions']):\n            params = self.tweak_params(target[3], credentials)\n            realtargets = realtargets + ((target[0], target[1], target[2], params, target[4]),)\n        targets['other_regions'] = realtargets\n\n        printInfo('Fetching %s config...' % format_service_name(self.service))\n        self.fetchstatuslogger = FetchStatusLogger(targets['first_region'], True)\n        api_service = 'ec2' if self.service.lower() == 'vpc' else self.service.lower()\n        # Init regions\n        regions = build_region_list(api_service, regions, partition_name) # TODO: move this code within this class\n        self.fetchstatuslogger.counts['regions']['discovered'] = len(regions)\n        # Threading to fetch & parse resources (queue consumer)\n        q = self._init_threading(self._fetch_target, {}, self.thread_config['parse'])\n        # Threading to list resources (queue feeder)\n        qr = self._init_threading(self._fetch_region,\n                                  {'api_service': api_service, 'credentials': credentials, 'q': q, 'targets': ()},\n                                  self.thread_config['list'])\n        # Go\n        for i, region in enumerate(regions):\n            qr.put((region, targets['first_region'] if i == 0 else targets['other_regions']))\n        # Join\n        qr.join()\n        q.join()\n        # Show completion and force newline\n        self.fetchstatuslogger.show(True)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _init_threading(self, function, params={}, num_threads=10):\n        q = Queue(maxsize=0) # TODO: find something appropriate\n        for i in range(num_threads):\n            worker = Thread(target=function, args=(q, params))\n            worker.setDaemon(True)\n            worker.start()\n        return q", "response": "Initialize queue and threads"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmaking all API calls as defined in metadata. json", "response": "def fetch_all(self, api_client, fetchstatuslogger, q, targets):\n        '''\n        Make all API calls as defined in metadata.json\n\n        :param api_client:\n        :param fetchstatuslogger:\n        :param q:\n        :param targets:\n        :return:\n        '''\n        self.fetchstatuslogger = fetchstatuslogger\n        if targets != None:\n            # Ensure targets is a tuple\n            if type(targets) != list and type(targets) != tuple:\n                targets = tuple(targets,)\n            elif type(targets) != tuple:\n                targets = tuple(targets)\n        for target in targets:\n            self._fetch_targets(api_client, q, target)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _fetch_targets(self, api_client, q, target):\n        '''\n        Make an API call defined in metadata.json.\n        Parse the returned object as implemented in the \"parse_[object name]\" method.\n\n        :param api_client:\n        :param q:\n        :param target:\n        :return:\n        '''\n        # Handle & format the target type\n        target_type, response_attribute, list_method_name, list_params, ignore_list_error = target\n        list_method = getattr(api_client, list_method_name)\n        try:\n            targets = handle_truncated_response(list_method, list_params, [response_attribute])[response_attribute]\n        except Exception as e:\n            if not ignore_list_error:\n                printException(e)\n            targets = []\n        setattr(self, '%s_count' % target_type, len(targets))\n        self.fetchstatuslogger.counts[target_type]['discovered'] += len(targets)\n        region = api_client._client_config.region_name\n        # Queue resources\n        for target in targets:\n            # call callback methods\n            try:\n                callback = getattr(self, 'parse_%s' % target_type[0:-1])\n            except:\n                callback = self.store_target\n                target['scout2_target_type'] = target_type\n            if q:\n                # Add to the queue\n                q.put((callback, region, target))", "response": "Fetch the resources from the API and store them in the target dict"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse a single queue and fetch additional attributes", "response": "def parse_queue(self, global_params, region, queue_url):\n        \"\"\"\n        Parse a single queue and fetch additional attributes\n\n        :param global_params:           Parameters shared for all regions\n        :param region:                  Name of the AWS region\n        :param queue_url:               URL of the AWS queue\n        \"\"\"\n        queue = {'QueueUrl': queue_url}\n        attributes = api_clients[region].get_queue_attributes(QueueUrl = queue_url, AttributeNames = ['CreatedTimestamp', 'Policy', 'QueueArn'])['Attributes']\n        queue['arn'] = attributes.pop('QueueArn')\n        for k in ['CreatedTimestamp']:\n            queue[k] = attributes[k] if k in attributes else None\n        if 'Policy' in attributes:\n            queue['Policy'] = json.loads(attributes['Policy'])\n        else:\n            queue['Policy'] = {'Statement': []}\n\n        queue['name'] = queue['arn'].split(':')[-1]\n        self.queues[queue['name']] = queue"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_s3_buckets(api_client, s3_info, s3_params):\n    manage_dictionary(s3_info, 'buckets', {})\n    buckets = api_client[get_s3_list_region(s3_params['selected_regions'])].list_buckets()['Buckets']\n    targets = []\n    for b in buckets:\n        # Abort if bucket is not of interest\n        if (b['Name'] in s3_params['skipped_buckets']) or (len(s3_params['checked_buckets']) and b['Name'] not in s3_params['checked_buckets']):\n            continue\n        targets.append(b)\n    s3_info['buckets_count'] = len(targets)\n    s3_params['api_clients'] = api_client\n    s3_params['s3_info'] = s3_info\n    thread_work(targets, get_s3_bucket, params = s3_params, num_threads = 30)\n    show_status(s3_info)\n    s3_info['buckets_count'] = len(s3_info['buckets'])\n    return s3_info", "response": "Get all available buckets"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget key - specific information for a given S3 bucket.", "response": "def get_s3_bucket_keys(api_client, bucket_name, bucket, check_encryption, check_acls):\n    \"\"\"\n    Get key-specific information (server-side encryption, acls, etc...)\n\n    :param api_client:\n    :param bucket_name:\n    :param bucket:\n    :param check_encryption:\n    :param check_acls:\n    :return:\n    \"\"\"\n    bucket['keys'] = []\n    keys = handle_truncated_response(api_client.list_objects, {'Bucket': bucket_name}, ['Contents'])\n    bucket['keys_count'] = len(keys['Contents'])\n    key_count = 0\n    update_status(key_count, bucket['keys_count'], 'keys')\n    for key in keys['Contents']:\n        key_count += 1\n        key['name'] = key.pop('Key')\n        key['LastModified'] = str(key['LastModified'])\n        if check_encryption:\n            try:\n                # The encryption configuration is only accessible via an HTTP header, only returned when requesting one object at a time...\n                k = api_client.get_object(Bucket = bucket_name, Key = key['name'])\n                key['ServerSideEncryption'] = k['ServerSideEncryption'] if 'ServerSideEncryption' in k else None\n                key['SSEKMSKeyId'] = k['SSEKMSKeyId'] if 'SSEKMSKeyId' in k else None\n            except Exception as e:\n                printException(e)\n                continue\n        if check_acls:\n            try:\n                key['grantees'] = get_s3_acls(api_client, bucket_name, bucket, key_name = key['name'])\n            except Exception as e:\n                continue\n        # Save it\n        bucket['keys'].append(key)\n        update_status(key_count, bucket['keys_count'], 'keys')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_buckets(self, bucket, params):\n        bucket['name'] = bucket.pop('Name')\n        api_client = params['api_clients'][get_s3_list_region(list(params['api_clients'].keys())[0])]\n\n        bucket['CreationDate'] = str(bucket['CreationDate'])\n        bucket['region'] = get_s3_bucket_location(api_client, bucket['name'])\n        # h4ck :: fix issue #59, location constraint can be EU or eu-west-1 for Ireland...\n        if bucket['region'] == 'EU':\n            bucket['region'] = 'eu-west-1'\n        # h4ck :: S3 is global but region-aware...\n        if bucket['region'] not in params['api_clients']:\n            printInfo('Skipping bucket %s (region %s outside of scope)' % (bucket['name'], bucket['region']))\n            self.buckets_count -= 1\n            return\n\n        api_client = params['api_clients'][bucket['region']]\n        get_s3_bucket_logging(api_client, bucket['name'], bucket)\n        get_s3_bucket_versioning(api_client, bucket['name'], bucket)\n        get_s3_bucket_webhosting(api_client, bucket['name'], bucket)\n        get_s3_bucket_default_encryption(api_client, bucket['name'], bucket)\n        bucket['grantees'] = get_s3_acls(api_client, bucket['name'], bucket)\n        get_s3_bucket_policy(api_client, bucket['name'], bucket)\n        get_s3_bucket_secure_transport(api_client, bucket['name'], bucket)\n        # If requested, get key properties\n        #if params['check_encryption'] or params['check_acls']:\n        #    get_s3_bucket_keys(api_client, bucket['name'], bucket, params['check_encryption'],\n        #                       params['check_acls'])\n        bucket['id'] = self.get_non_aws_id(bucket['name'])\n        self.buckets[bucket['id']] = bucket", "response": "Parse a single S3 bucket and return a set of key - properties."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nopening a JSON file definiting a ruleset and load it into a Ruleset object.", "response": "def load(self, rule_type, quiet = False):\n        \"\"\"\n        Open a JSON file definiting a ruleset and load it into a Ruleset object\n\n        :param quiet:\n        :return:\n        \"\"\"\n        if self.filename and os.path.exists(self.filename):\n            try:\n                with open(self.filename, 'rt') as f:\n                    ruleset = json.load(f)\n                    self.about = ruleset['about'] if 'about' in ruleset else ''\n                    self.rules = {}\n                    for filename in ruleset['rules']:\n                        self.rules[filename] = []\n                        for rule in ruleset['rules'][filename]:\n                            self.handle_rule_versions(filename, rule_type, rule)\n            except Exception as e:\n                printException(e)\n                printError('Error: ruleset file %s contains malformed JSON.' % self.filename)\n                self.rules = []\n                self.about = ''\n        else:\n            self.rules = []\n            if not quiet:\n                printError('Error: the file %s does not exist.' % self.filename)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nhandles the rules versions of a rule.", "response": "def handle_rule_versions(self, filename, rule_type, rule):\n        \"\"\"\n        For each version of a rule found in the ruleset, append a new Rule object\n        \"\"\"\n        if 'versions' in rule:\n            versions = rule.pop('versions')\n            for version_key_suffix in versions:\n                version = versions[version_key_suffix]\n                version['key_suffix'] = version_key_suffix\n                tmp_rule = dict(rule, **version)\n                self.rules[filename].append(Rule(filename, rule_type, tmp_rule))\n        else:\n            self.rules[filename].append(Rule(filename, rule_type, rule))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate the ruleset s rules by duplicating fields as required by the HTML ruleset generator", "response": "def prepare_rules(self, attributes = [], ip_ranges = [], params = {}):\n        \"\"\"\n        Update the ruleset's rules by duplicating fields as required by the HTML ruleset generator\n\n        :return:\n        \"\"\"\n        for filename in self.rule_definitions:\n            if filename in self.rules:\n                for rule in self.rules[filename]:\n                    rule.set_definition(self.rule_definitions, attributes, ip_ranges, params)\n            else:\n                self.rules[filename] = []\n                new_rule = Rule(filename, self.rule_type, {'enabled': False, 'level': 'danger'})\n                new_rule.set_definition(self.rule_definitions, attributes, ip_ranges, params)\n                self.rules[filename].append(new_rule)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_rule_definitions(self, ruleset_generator = False, rule_dirs = []):\n\n        # Load rules from JSON files\n        self.rule_definitions = {}\n        for rule_filename in self.rules:\n            for rule in self.rules[rule_filename]:\n                if not rule.enabled and not ruleset_generator:\n                    continue\n            self.rule_definitions[os.path.basename(rule_filename)] = RuleDefinition(rule_filename, rule_dirs = rule_dirs)\n        # In case of the ruleset generator, list all available built-in rules\n        if ruleset_generator:\n            rule_dirs.append(os.path.join(os.path.dirname(os.path.realpath(__file__)), 'data/findings'))\n            rule_filenames = []\n            for rule_dir in rule_dirs:\n                rule_filenames += [f for f in os.listdir(rule_dir) if os.path.isfile(os.path.join(rule_dir, f))]\n            for rule_filename in rule_filenames:\n                if rule_filename not in self.rule_definitions:\n                    self.rule_definitions[os.path.basename(rule_filename)] = RuleDefinition(rule_filename)", "response": "Load the rules from the JSON files and store them in the self. rule_definitions attribute."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nask a question and prompt for yes or no", "response": "def prompt_4_yes_no(question):\n    \"\"\"\n    Ask a question and prompt for yes or no\n\n    :param question:                    Question to ask; answer is yes/no\n    :return:                            :boolean\n    \"\"\"\n    while True:\n        sys.stdout.write(question + ' (y/n)? ')\n        try:\n            choice = raw_input().lower()\n        except:\n            choice = input().lower()\n        if choice == 'yes' or choice == 'y':\n            return True\n        elif choice == 'no' or choice == 'n':\n            return False\n        else:\n            printError('\\'%s\\' is not a valid answer. Enter \\'yes\\'(y) or \\'no\\'(n).' % choice)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses a single RDS instance and store it in self. instances.", "response": "def parse_instance(self, global_params, region, dbi):\n        \"\"\"\n        Parse a single RDS instance\n\n        :param global_params:           Parameters shared for all regions\n        :param region:                  Name of the AWS region\n        :param instance:                Instance\n        \"\"\"\n        vpc_id = dbi['DBSubnetGroup']['VpcId'] if 'DBSubnetGroup' in dbi and 'VpcId' in dbi['DBSubnetGroup'] and dbi['DBSubnetGroup']['VpcId'] else ec2_classic\n        instance = {}\n        instance['name'] = dbi.pop('DBInstanceIdentifier')\n        for key in ['InstanceCreateTime', 'Engine', 'DBInstanceStatus', 'AutoMinorVersionUpgrade',\n                    'DBInstanceClass', 'MultiAZ', 'Endpoint', 'BackupRetentionPeriod', 'PubliclyAccessible',\n                    'StorageEncrypted', 'VpcSecurityGroups', 'DBSecurityGroups', 'DBParameterGroups',\n                    'EnhancedMonitoringResourceArn', 'StorageEncrypted']:\n                    # parameter_groups , security_groups, vpc_security_groups\n            instance[key] = dbi[key] if key in dbi else None\n        # If part of a cluster, multi AZ information is only available via cluster information\n        if 'DBClusterIdentifier' in dbi:\n            api_client = api_clients[region]\n            cluster = api_client.describe_db_clusters(DBClusterIdentifier = dbi['DBClusterIdentifier'])['DBClusters'][0]\n            instance['MultiAZ'] = cluster['MultiAZ']\n        # Save\n        manage_dictionary(self.vpcs, vpc_id, VPCConfig(self.vpc_resource_types))\n        self.vpcs[vpc_id].instances[instance['name']] = instance"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef preprocessing(aws_config, ip_ranges = [], ip_ranges_name_key = None):\n\n    map_all_sgs(aws_config)\n    map_all_subnets(aws_config)\n    set_emr_vpc_ids(aws_config)\n    #parse_elb_policies(aws_config)\n\n    # Various data processing calls\n    add_security_group_name_to_ec2_grants(aws_config['services']['ec2'], aws_config['aws_account_id'])\n    process_cloudtrail_trails(aws_config['services']['cloudtrail'])\n    add_cidr_display_name(aws_config, ip_ranges, ip_ranges_name_key)\n    merge_route53_and_route53domains(aws_config)\n    match_instances_and_roles(aws_config)\n    match_iam_policies_and_buckets(aws_config)\n\n    # Preprocessing dictated by metadata\n    process_metadata_callbacks(aws_config)", "response": "Preprocessing function for the cross - service resources."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef process_metadata_callbacks(aws_config):\n    for service_group in aws_config['metadata']:\n        for service in aws_config['metadata'][service_group]:\n            if service == 'summaries':\n                continue\n            # Reset external attack surface\n            if 'summaries' in aws_config['metadata'][service_group][service]:\n                for summary in aws_config['metadata'][service_group][service]['summaries']:\n                    if summary == 'external attack surface' and service in aws_config['services'] and 'external_attack_surface' in aws_config['services'][service]:\n                        aws_config['services'][service].pop('external_attack_surface')\n            # Reset all global summaries\n            if 'service_groups' in aws_config:\n                aws_config.pop('service_groups')\n            # Resources\n            for resource_type in aws_config['metadata'][service_group][service]['resources']:\n                if 'callbacks' in aws_config['metadata'][service_group][service]['resources'][resource_type]:\n                    current_path = [ 'services', service ]\n                    target_path = aws_config['metadata'][service_group][service]['resources'][resource_type]['path'].replace('.id', '').split('.')[2:]\n                    callbacks = aws_config['metadata'][service_group][service]['resources'][resource_type]['callbacks']\n                    new_go_to_and_do(aws_config, get_object_at(aws_config, current_path), target_path, current_path, callbacks)\n            # Summaries\n            if 'summaries' in aws_config['metadata'][service_group][service]:\n              for summary in aws_config['metadata'][service_group][service]['summaries']:\n                if 'callbacks' in aws_config['metadata'][service_group][service]['summaries'][summary]:\n                    current_path = [ 'services', service ]\n                    for callback in aws_config['metadata'][service_group][service]['summaries'][summary]['callbacks']:\n                        callback_name = callback[0]\n                        callback_args = copy.deepcopy(callback[1])\n                        target_path = callback_args.pop('path').replace('.id', '').split('.')[2:]\n                        callbacks = [ [callback_name, callback_args] ]\n                        new_go_to_and_do(aws_config, get_object_at(aws_config, current_path), target_path, current_path, callbacks)\n    # Group-level summaries\n    for service_group in aws_config['metadata']:\n        if 'summaries' in aws_config['metadata'][service_group]:\n            for summary in aws_config['metadata'][service_group]['summaries']:\n                    current_path = [ 'services', service ]\n                    for callback in aws_config['metadata'][service_group]['summaries'][summary]['callbacks']:\n                        callback_name = callback[0]\n                        callback_args = copy.deepcopy(callback[1])\n                        target_path = aws_config['metadata'][service_group]['summaries'][summary]['path'].split('.')\n                        target_object = aws_config\n                        for p in target_path:\n                            manage_dictionary(target_object, p, {})\n                            target_object = target_object[p]\n                        if callback_name == 'merge':\n                            for service in aws_config['metadata'][service_group]:\n                                if service == 'summaries':\n                                    continue\n                                if 'summaries' in aws_config['metadata'][service_group][service] and summary in aws_config['metadata'][service_group][service]['summaries']:\n                                    try:\n                                        source = get_object_at(aws_config, aws_config['metadata'][service_group][service]['summaries'][summary]['path'].split('.'))\n                                    except:\n                                        source = {}\n                                    target_object.update(source)\n\n    return None", "response": "Processes the metadata callbacks for each resource and each summary in the config."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef match_instances_and_roles(aws_config):\n    printInfo('Matching EC2 instances and IAM roles...')\n    ec2_config = aws_config['services']['ec2']\n    iam_config = aws_config['services']['iam']\n    role_instances = {}\n    for r in ec2_config['regions']:\n        for v in ec2_config['regions'][r]['vpcs']:\n            if 'instances' in ec2_config['regions'][r]['vpcs'][v]:\n                for i in ec2_config['regions'][r]['vpcs'][v]['instances']:\n                    instance_profile = ec2_config['regions'][r]['vpcs'][v]['instances'][i]['IamInstanceProfile']\n                    instance_profile_id = instance_profile['Id'] if instance_profile else None\n                    if instance_profile_id:\n                        manage_dictionary(role_instances, instance_profile_id, [])\n                        role_instances[instance_profile_id].append(i)\n    for role_id in iam_config['roles']:\n        iam_config['roles'][role_id]['instances_count'] = 0\n        for instance_profile_id in iam_config['roles'][role_id]['instance_profiles']:\n            if instance_profile_id in role_instances:\n                iam_config['roles'][role_id]['instance_profiles'][instance_profile_id]['instances'] = role_instances[instance_profile_id]\n                iam_config['roles'][role_id]['instances_count'] += len(role_instances[instance_profile_id])", "response": "Match EC2 instances and IAM roles."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprocess a callback for VPC peering connections", "response": "def process_vpc_peering_connections_callback(aws_config, current_config, path, current_path, pc_id, callback_args):\n    \"\"\"\n    Create a list of peering connection IDs in each VPC\n\n    :param aws_config:\n    :param current_config:\n    :param path:\n    :param current_path:\n    :param pc_id:\n    :param callback_args:\n    :return:\n    \"\"\"\n    info = 'AccepterVpcInfo' if current_config['AccepterVpcInfo']['OwnerId'] == aws_config['aws_account_id'] else 'RequesterVpcInfo'\n    region = current_path[current_path.index('regions')+1]\n    vpc_id = current_config[info]['VpcId']\n    target = aws_config['services']['vpc']['regions'][region]['vpcs'][vpc_id]\n    manage_dictionary(target, 'peering_connections', [])\n    if pc_id not in target['peering_connections']:\n        target['peering_connections'].append(pc_id)\n\n    # VPC information for the peer'd VPC\n    current_config['peer_info'] = copy.deepcopy(current_config['AccepterVpcInfo' if info == 'RequesterVpcInfo' else 'RequesterVpcInfo'])\n    if 'PeeringOptions' in current_config['peer_info']:\n        current_config['peer_info'].pop('PeeringOptions')\n    if 'organization' in aws_config and current_config['peer_info']['OwnerId'] in aws_config['organization']:\n        current_config['peer_info']['name'] = aws_config['organization'][current_config['peer_info']['OwnerId']]['Name']\n    else:\n        current_config['peer_info']['name'] = current_config['peer_info']['OwnerId']"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating every attribute of the rule by setting the argument values as necessary.", "response": "def set_definition(self, rule_definitions, attributes = [], ip_ranges = [], params = {}):\n        \"\"\"\n        Update every attribute of the rule by setting the argument values as necessary\n\n        :param parameterized_input:\n        :param arg_values:\n        :param convert:\n        :return:\n        \"\"\"\n        string_definition = rule_definitions[self.filename].string_definition\n        # Load condition dependencies\n        definition = json.loads(string_definition)\n        definition['conditions'] += self.conditions\n        loaded_conditions = []\n        for condition in definition['conditions']:\n            if condition[0].startswith('_INCLUDE_('):\n                include = re.findall(r'_INCLUDE_\\((.*?)\\)', condition[0])[0]\n                #new_conditions = load_data(include, key_name = 'conditions')\n                with open(os.path.join(os.path.dirname(os.path.realpath(__file__)), 'data/%s' % include), 'rt') as f:\n                    new_conditions = f.read()\n                    for (i, value) in enumerate(condition[1]):\n                        new_conditions = re.sub(condition[1][i], condition[2][i], new_conditions)\n                    new_conditions = json.loads(new_conditions)['conditions']\n                loaded_conditions.append(new_conditions)\n            else:\n                loaded_conditions.append(condition)\n        definition['conditions'] = loaded_conditions\n        string_definition = json.dumps(definition)\n        # Set parameters\n        parameters = re.findall(r'(_ARG_([a-zA-Z0-9]+)_)', string_definition)\n        for param in parameters:\n            index = int(param[1])\n            if len(self.args) <= index:\n                string_definition = string_definition.replace(param[0], '')\n            elif type(self.args[index]) == list:\n                value = '[ %s ]' % ', '.join('\"%s\"' % v for v in self.args[index])\n                string_definition = string_definition.replace('\"%s\"' % param[0], value)\n            else:\n                string_definition = string_definition.replace(param[0], self.args[index])\n        # Strip dots if necessary\n        stripdots = re_strip_dots.findall(string_definition)\n        for value in stripdots:\n            string_definition = string_definition.replace(value[0], value[1].replace('.', ''))\n        definition = json.loads(string_definition)\n        # Set special values (IP ranges, AWS account ID, ...)\n        for condition in definition['conditions']:\n            if type(condition) != list or len(condition) == 1 or type(condition[2]) == list:\n                continue\n            for testcase in testcases:\n                result = testcase['regex'].match(condition[2])\n                if result and (testcase['name'] == 'ip_ranges_from_file' or testcase['name'] == 'ip_ranges_from_local_file'):\n                    filename = result.groups()[0]\n                    conditions = result.groups()[1] if len(result.groups()) > 1 else []\n                    # TODO :: handle comma here...\n                    if filename == ip_ranges_from_args:\n                        prefixes = []\n                        for filename in ip_ranges:\n                            prefixes += read_ip_ranges(filename, local_file = True, ip_only = True, conditions = conditions)\n                        condition[2] = prefixes\n                        break\n                    else:\n                        local_file = True if testcase['name'] == 'ip_ranges_from_local_file' else False\n                        condition[2] = read_ip_ranges(filename, local_file = local_file, ip_only = True, conditions = conditions)\n                        break\n                    break\n                elif result:\n                    condition[2] = params[testcase['name']]\n                    break\n\n        if len(attributes) == 0:\n            attributes = [attr for attr in definition]\n        for attr in attributes:\n            if attr in definition:\n                setattr(self, attr, definition[attr])\n        if hasattr(self, 'path'):\n            self.service = format_service_name(self.path.split('.')[0])\n        if not hasattr(self, 'key'):\n            setattr(self, 'key', self.filename)\n        setattr(self, 'key', self.key.replace('.json', ''))\n        if self.key_suffix:\n            setattr(self, 'key', '%s-%s' % (self.key, self.key_suffix))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning True if the specified error category is suppressed on this line.", "response": "def IsErrorSuppressedByNolint(category, linenum):\n  \"\"\"Returns true if the specified error category is suppressed on this line.\n\n  Consults the global error_suppressions map populated by\n  ParseNolintSuppressions/ResetNolintSuppressions.\n\n  Args:\n    category: str, the category of the error.\n    linenum: int, the current line number.\n  Returns:\n    bool, True iff the error should be suppressed due to a NOLINT comment.\n  \"\"\"\n  return (linenum in _error_suppressions.get(category, set()) or\n          linenum in _error_suppressions.get(None, set()))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef Error(filename, linenum, category, confidence, message):\n  if _ShouldPrintError(category, confidence, linenum):\n    _cpplint_state.IncrementErrorCount(category)\n    if _cpplint_state.output_format == 'vs7':\n      sys.stderr.write('%s(%s):  %s  [%s] [%d]\\n' % (\n          filename, linenum, message, category, confidence))\n    elif _cpplint_state.output_format == 'eclipse':\n      sys.stderr.write('%s:%s: warning: %s  [%s] [%d]\\n' % (\n          filename, linenum, message, category, confidence))\n    else:\n      sys.stderr.write('%s:%s:  %s  [%s] [%d]\\n' % (\n          filename, linenum, message, category, confidence))", "response": "Logs the fact we ve found an error."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef CheckHeaderFileIncluded(filename, include_state, error):\n\n  # Do not check test files\n  if filename.endswith('_test.cc') or filename.endswith('_unittest.cc'):\n    return\n\n  fileinfo = FileInfo(filename)\n  headerfile = filename[0:len(filename) - 2] + 'h'\n  if not os.path.exists(headerfile):\n    return\n  headername = FileInfo(headerfile).RepositoryName()\n  first_include = 0\n  for section_list in include_state.include_list:\n    for f in section_list:\n      if headername in f[0] or f[0] in headername:\n        return\n      if not first_include:\n        first_include = f[1]\n\n  error(filename, first_include, 'build/include', 5,\n        '%s should include its header file %s' % (fileinfo.RepositoryName(),\n                                                  headername))", "response": "Logs an error if a. cc file does not include its header."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlogs an error for each line containing bad characters.", "response": "def CheckForBadCharacters(filename, lines, error):\n  \"\"\"Logs an error for each line containing bad characters.\n\n  Two kinds of bad characters:\n\n  1. Unicode replacement characters: These indicate that either the file\n  contained invalid UTF-8 (likely) or Unicode replacement characters (which\n  it shouldn't).  Note that it's possible for this to throw off line\n  numbering if the invalid UTF-8 occurred adjacent to a newline.\n\n  2. NUL bytes.  These are problematic for some tools.\n\n  Args:\n    filename: The name of the current file.\n    lines: An array of strings, each representing a line of the file.\n    error: The function to call with any errors found.\n  \"\"\"\n  for linenum, line in enumerate(lines):\n    if u'\\ufffd' in line:\n      error(filename, linenum, 'readability/utf8', 5,\n            'Line contains invalid UTF-8 (or Unicode replacement character).')\n    if '\\0' in line:\n      error(filename, linenum, 'readability/nul', 5, 'Line contains NUL byte.')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef CheckForNonStandardConstructs(filename, clean_lines, linenum,\n                                  nesting_state, error):\n  r\"\"\"Logs an error if we see certain non-ANSI constructs ignored by gcc-2.\n\n  Complain about several constructs which gcc-2 accepts, but which are\n  not standard C++.  Warning about these in lint is one way to ease the\n  transition to new compilers.\n  - put storage class first (e.g. \"static const\" instead of \"const static\").\n  - \"%lld\" instead of %qd\" in printf-type functions.\n  - \"%1$d\" is non-standard in printf-type functions.\n  - \"\\%\" is an undefined character escape sequence.\n  - text after #endif is not allowed.\n  - invalid inner-style forward declaration.\n  - >? and <? operators, and their >?= and <?= cousins.\n\n  Additionally, check for constructor/destructor style violations and reference\n  members, as it is very convenient to do so while checking for\n  gcc-2 compliance.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    nesting_state: A NestingState instance which maintains information about\n                   the current stack of nested blocks being parsed.\n    error: A callable to which errors are reported, which takes 4 arguments:\n           filename, line number, error level, and message\n  \"\"\"\n\n  # Remove comments from the line, but leave in strings for now.\n  line = clean_lines.lines[linenum]\n\n  if Search(r'printf\\s*\\(.*\".*%[-+ ]?\\d*q', line):\n    error(filename, linenum, 'runtime/printf_format', 3,\n          '%q in format strings is deprecated.  Use %ll instead.')\n\n  if Search(r'printf\\s*\\(.*\".*%\\d+\\$', line):\n    error(filename, linenum, 'runtime/printf_format', 2,\n          '%N$ formats are unconventional.  Try rewriting to avoid them.')\n\n  # Remove escaped backslashes before looking for undefined escapes.\n  line = line.replace('\\\\\\\\', '')\n\n  if Search(r'(\"|\\').*\\\\(%|\\[|\\(|{)', line):\n    error(filename, linenum, 'build/printf_format', 3,\n          '%, [, (, and { are undefined character escapes.  Unescape them.')\n\n  # For the rest, work with both comments and strings removed.\n  line = clean_lines.elided[linenum]\n\n  if Search(r'\\b(const|volatile|void|char|short|int|long'\n            r'|float|double|signed|unsigned'\n            r'|schar|u?int8|u?int16|u?int32|u?int64)'\n            r'\\s+(register|static|extern|typedef)\\b',\n            line):\n    error(filename, linenum, 'build/storage_class', 5,\n          'Storage class (static, extern, typedef, etc) should be first.')\n\n  if Match(r'\\s*#\\s*endif\\s*[^/\\s]+', line):\n    error(filename, linenum, 'build/endif_comment', 5,\n          'Uncommented text after #endif is non-standard.  Use a comment.')\n\n  if Match(r'\\s*class\\s+(\\w+\\s*::\\s*)+\\w+\\s*;', line):\n    error(filename, linenum, 'build/forward_decl', 5,\n          'Inner-style forward declarations are invalid.  Remove this line.')\n\n  if Search(r'(\\w+|[+-]?\\d+(\\.\\d*)?)\\s*(<|>)\\?=?\\s*(\\w+|[+-]?\\d+)(\\.\\d*)?',\n            line):\n    error(filename, linenum, 'build/deprecated', 3,\n          '>? and <? (max and min) operators are non-standard and deprecated.')\n\n  if Search(r'^\\s*const\\s*string\\s*&\\s*\\w+\\s*;', line):\n    # TODO(unknown): Could it be expanded safely to arbitrary references,\n    # without triggering too many false positives? The first\n    # attempt triggered 5 warnings for mostly benign code in the regtest, hence\n    # the restriction.\n    # Here's the original regexp, for the reference:\n    # type_name = r'\\w+((\\s*::\\s*\\w+)|(\\s*<\\s*\\w+?\\s*>))?'\n    # r'\\s*const\\s*' + type_name + '\\s*&\\s*\\w+\\s*;'\n    error(filename, linenum, 'runtime/member_string_references', 2,\n          'const string& members are dangerous. It is much better to use '\n          'alternatives, such as pointers or simple constants.')\n\n  # Everything else in this function operates on class declarations.\n  # Return early if the top of the nesting stack is not a class, or if\n  # the class head is not completed yet.\n  classinfo = nesting_state.InnermostClass()\n  if not classinfo or not classinfo.seen_open_brace:\n    return\n\n  # The class may have been declared with namespace or classname qualifiers.\n  # The constructor and destructor will not have those qualifiers.\n  base_classname = classinfo.name.split('::')[-1]\n\n  # Look for single-argument constructors that aren't marked explicit.\n  # Technically a valid construct, but against style. Also look for\n  # non-single-argument constructors which are also technically valid, but\n  # strongly suggest something is wrong.\n  explicit_constructor_match = Match(\n      r'\\s+(?:inline\\s+)?(explicit\\s+)?(?:inline\\s+)?%s\\s*'\n      r'\\(((?:[^()]|\\([^()]*\\))*)\\)'\n      % re.escape(base_classname),\n      line)\n\n  if explicit_constructor_match:\n    is_marked_explicit = explicit_constructor_match.group(1)\n\n    if not explicit_constructor_match.group(2):\n      constructor_args = []\n    else:\n      constructor_args = explicit_constructor_match.group(2).split(',')\n\n    # collapse arguments so that commas in template parameter lists and function\n    # argument parameter lists don't split arguments in two\n    i = 0\n    while i < len(constructor_args):\n      constructor_arg = constructor_args[i]\n      while (constructor_arg.count('<') > constructor_arg.count('>') or\n             constructor_arg.count('(') > constructor_arg.count(')')):\n        constructor_arg += ',' + constructor_args[i + 1]\n        del constructor_args[i + 1]\n      constructor_args[i] = constructor_arg\n      i += 1\n\n    defaulted_args = [arg for arg in constructor_args if '=' in arg]\n    noarg_constructor = (not constructor_args or  # empty arg list\n                         # 'void' arg specifier\n                         (len(constructor_args) == 1 and\n                          constructor_args[0].strip() == 'void'))\n    onearg_constructor = ((len(constructor_args) == 1 and  # exactly one arg\n                           not noarg_constructor) or\n                          # all but at most one arg defaulted\n                          (len(constructor_args) >= 1 and\n                           not noarg_constructor and\n                           len(defaulted_args) >= len(constructor_args) - 1))\n    initializer_list_constructor = bool(\n        onearg_constructor and\n        Search(r'\\bstd\\s*::\\s*initializer_list\\b', constructor_args[0]))\n    copy_constructor = bool(\n        onearg_constructor and\n        Match(r'(const\\s+)?%s(\\s*<[^>]*>)?(\\s+const)?\\s*(?:<\\w+>\\s*)?&'\n              % re.escape(base_classname), constructor_args[0].strip()))\n\n    if (not is_marked_explicit and\n        onearg_constructor and\n        not initializer_list_constructor and\n        not copy_constructor):\n      if defaulted_args:\n        error(filename, linenum, 'runtime/explicit', 5,\n              'Constructors callable with one argument '\n              'should be marked explicit.')\n      else:\n        error(filename, linenum, 'runtime/explicit', 5,\n              'Single-parameter constructors should be marked explicit.')\n    elif is_marked_explicit and not onearg_constructor:\n      if noarg_constructor:\n        error(filename, linenum, 'runtime/explicit', 5,\n              'Zero-parameter constructors should not be marked explicit.')\n      else:\n        error(filename, linenum, 'runtime/explicit', 0,\n              'Constructors that require multiple arguments '\n              'should not be marked explicit.')", "response": "Checks if a file contains non - standard C ++ constructs."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef CheckOperatorSpacing(filename, clean_lines, linenum, error):\n  line = clean_lines.elided[linenum]\n\n  # Don't try to do spacing checks for operator methods.  Do this by\n  # replacing the troublesome characters with something else,\n  # preserving column position for all other characters.\n  #\n  # The replacement is done repeatedly to avoid false positives from\n  # operators that call operators.\n  while True:\n    match = Match(r'^(.*\\boperator\\b)(\\S+)(\\s*\\(.*)$', line)\n    if match:\n      line = match.group(1) + ('_' * len(match.group(2))) + match.group(3)\n    else:\n      break\n\n  # We allow no-spaces around = within an if: \"if ( (a=Foo()) == 0 )\".\n  # Otherwise not.  Note we only check for non-spaces on *both* sides;\n  # sometimes people put non-spaces on one side when aligning ='s among\n  # many lines (not that this is behavior that I approve of...)\n  if ((Search(r'[\\w.]=', line) or\n       Search(r'=[\\w.]', line))\n      and not Search(r'\\b(if|while|for) ', line)\n      # Operators taken from [lex.operators] in C++11 standard.\n      and not Search(r'(>=|<=|==|!=|&=|\\^=|\\|=|\\+=|\\*=|\\/=|\\%=)', line)\n      and not Search(r'operator=', line)):\n    error(filename, linenum, 'whitespace/operators', 4,\n          'Missing spaces around =')\n\n  # It's ok not to have spaces around binary operators like + - * /, but if\n  # there's too little whitespace, we get concerned.  It's hard to tell,\n  # though, so we punt on this one for now.  TODO.\n\n  # You should always have whitespace around binary operators.\n  #\n  # Check <= and >= first to avoid false positives with < and >, then\n  # check non-include lines for spacing around < and >.\n  #\n  # If the operator is followed by a comma, assume it's be used in a\n  # macro context and don't do any checks.  This avoids false\n  # positives.\n  #\n  # Note that && is not included here.  Those are checked separately\n  # in CheckRValueReference\n  match = Search(r'[^<>=!\\s](==|!=|<=|>=|\\|\\|)[^<>=!\\s,;\\)]', line)\n  if match:\n    error(filename, linenum, 'whitespace/operators', 3,\n          'Missing spaces around %s' % match.group(1))\n  elif not Match(r'#.*include', line):\n    # Look for < that is not surrounded by spaces.  This is only\n    # triggered if both sides are missing spaces, even though\n    # technically should should flag if at least one side is missing a\n    # space.  This is done to avoid some false positives with shifts.\n    match = Match(r'^(.*[^\\s<])<[^\\s=<,]', line)\n    if match:\n      (_, _, end_pos) = CloseExpression(\n          clean_lines, linenum, len(match.group(1)))\n      if end_pos <= -1:\n        error(filename, linenum, 'whitespace/operators', 3,\n              'Missing spaces around <')\n\n    # Look for > that is not surrounded by spaces.  Similar to the\n    # above, we only trigger if both sides are missing spaces to avoid\n    # false positives with shifts.\n    match = Match(r'^(.*[^-\\s>])>[^\\s=>,]', line)\n    if match:\n      (_, _, start_pos) = ReverseCloseExpression(\n          clean_lines, linenum, len(match.group(1)))\n      if start_pos <= -1:\n        error(filename, linenum, 'whitespace/operators', 3,\n              'Missing spaces around >')\n\n  # We allow no-spaces around << when used like this: 10<<20, but\n  # not otherwise (particularly, not when used as streams)\n  #\n  # We also allow operators following an opening parenthesis, since\n  # those tend to be macros that deal with operators.\n  match = Search(r'(operator|[^\\s(<])(?:L|UL|ULL|l|ul|ull)?<<([^\\s,=<])', line)\n  if (match and not (match.group(1).isdigit() and match.group(2).isdigit()) and\n      not (match.group(1) == 'operator' and match.group(2) == ';')):\n    error(filename, linenum, 'whitespace/operators', 3,\n          'Missing spaces around <<')\n\n  # We allow no-spaces around >> for almost anything.  This is because\n  # C++11 allows \">>\" to close nested templates, which accounts for\n  # most cases when \">>\" is not followed by a space.\n  #\n  # We still warn on \">>\" followed by alpha character, because that is\n  # likely due to \">>\" being used for right shifts, e.g.:\n  #   value >> alpha\n  #\n  # When \">>\" is used to close templates, the alphanumeric letter that\n  # follows would be part of an identifier, and there should still be\n  # a space separating the template type and the identifier.\n  #   type<type<type>> alpha\n  match = Search(r'>>[a-zA-Z_]', line)\n  if match:\n    error(filename, linenum, 'whitespace/operators', 3,\n          'Missing spaces around >>')\n\n  # There shouldn't be space around unary operators\n  match = Search(r'(!\\s|~\\s|[\\s]--[\\s;]|[\\s]\\+\\+[\\s;])', line)\n  if match:\n    error(filename, linenum, 'whitespace/operators', 4,\n          'Extra space for operator %s' % match.group(1))", "response": "Checks for horizontal spacing around operators."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if the token ending on ( linenum column ) is the end of template<.", "response": "def IsTemplateParameterList(clean_lines, linenum, column):\n  \"\"\"Check if the token ending on (linenum, column) is the end of template<>.\n\n  Args:\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: the number of the line to check.\n    column: end column of the token to check.\n  Returns:\n    True if this token is end of a template parameter list, False otherwise.\n  \"\"\"\n  (_, startline, startpos) = ReverseCloseExpression(\n      clean_lines, linenum, column)\n  if (startpos > -1 and\n      Search(r'\\btemplate\\s*$', clean_lines.elided[startline][0:startpos])):\n    return True\n  return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef IsRValueType(typenames, clean_lines, nesting_state, linenum, column):\n  prefix = clean_lines.elided[linenum][0:column]\n\n  # Get one word to the left.  If we failed to do so, this is most\n  # likely not a type, since it's unlikely that the type name and \"&&\"\n  # would be split across multiple lines.\n  match = Match(r'^(.*)(\\b\\w+|[>*)&])\\s*$', prefix)\n  if not match:\n    return False\n\n  # Check text following the token.  If it's \"&&>\" or \"&&,\" or \"&&...\", it's\n  # most likely a rvalue reference used inside a template.\n  suffix = clean_lines.elided[linenum][column:]\n  if Match(r'&&\\s*(?:[>,]|\\.\\.\\.)', suffix):\n    return True\n\n  # Check for known types and end of templates:\n  #   int&& variable\n  #   vector<int>&& variable\n  #\n  # Because this function is called recursively, we also need to\n  # recognize pointer and reference types:\n  #   int* Function()\n  #   int& Function()\n  if (match.group(2) in typenames or\n      match.group(2) in ['char', 'char16_t', 'char32_t', 'wchar_t', 'bool',\n                         'short', 'int', 'long', 'signed', 'unsigned',\n                         'float', 'double', 'void', 'auto', '>', '*', '&']):\n    return True\n\n  # If we see a close parenthesis, look for decltype on the other side.\n  # decltype would unambiguously identify a type, anything else is\n  # probably a parenthesized expression and not a type.\n  if match.group(2) == ')':\n    return IsDecltype(\n        clean_lines, linenum, len(match.group(1)) + len(match.group(2)) - 1)\n\n  # Check for casts and cv-qualifiers.\n  #   match.group(1)  remainder\n  #   --------------  ---------\n  #   const_cast<     type&&\n  #   const           type&&\n  #   type            const&&\n  if Search(r'\\b(?:const_cast\\s*<|static_cast\\s*<|dynamic_cast\\s*<|'\n            r'reinterpret_cast\\s*<|\\w+\\s)\\s*$',\n            match.group(1)):\n    return True\n\n  # Look for a preceding symbol that might help differentiate the context.\n  # These are the cases that would be ambiguous:\n  #   match.group(1)  remainder\n  #   --------------  ---------\n  #   Call         (   expression &&\n  #   Declaration  (   type&&\n  #   sizeof       (   type&&\n  #   if           (   expression &&\n  #   while        (   expression &&\n  #   for          (   type&&\n  #   for(         ;   expression &&\n  #   statement    ;   type&&\n  #   block        {   type&&\n  #   constructor  {   expression &&\n  start = linenum\n  line = match.group(1)\n  match_symbol = None\n  while start >= 0:\n    # We want to skip over identifiers and commas to get to a symbol.\n    # Commas are skipped so that we can find the opening parenthesis\n    # for function parameter lists.\n    match_symbol = Match(r'^(.*)([^\\w\\s,])[\\w\\s,]*$', line)\n    if match_symbol:\n      break\n    start -= 1\n    line = clean_lines.elided[start]\n\n  if not match_symbol:\n    # Probably the first statement in the file is an rvalue reference\n    return True\n\n  if match_symbol.group(2) == '}':\n    # Found closing brace, probably an indicate of this:\n    #   block{} type&&\n    return True\n\n  if match_symbol.group(2) == ';':\n    # Found semicolon, probably one of these:\n    #   for(; expression &&\n    #   statement; type&&\n\n    # Look for the previous 'for(' in the previous lines.\n    before_text = match_symbol.group(1)\n    for i in xrange(start - 1, max(start - 6, 0), -1):\n      before_text = clean_lines.elided[i] + before_text\n    if Search(r'for\\s*\\([^{};]*$', before_text):\n      # This is the condition inside a for-loop\n      return False\n\n    # Did not find a for-init-statement before this semicolon, so this\n    # is probably a new statement and not a condition.\n    return True\n\n  if match_symbol.group(2) == '{':\n    # Found opening brace, probably one of these:\n    #   block{ type&& = ... ; }\n    #   constructor{ expression && expression }\n\n    # Look for a closing brace or a semicolon.  If we see a semicolon\n    # first, this is probably a rvalue reference.\n    line = clean_lines.elided[start][0:len(match_symbol.group(1)) + 1]\n    end = start\n    depth = 1\n    while True:\n      for ch in line:\n        if ch == ';':\n          return True\n        elif ch == '{':\n          depth += 1\n        elif ch == '}':\n          depth -= 1\n          if depth == 0:\n            return False\n      end += 1\n      if end >= clean_lines.NumLines():\n        break\n      line = clean_lines.elided[end]\n    # Incomplete program?\n    return False\n\n  if match_symbol.group(2) == '(':\n    # Opening parenthesis.  Need to check what's to the left of the\n    # parenthesis.  Look back one extra line for additional context.\n    before_text = match_symbol.group(1)\n    if linenum > 1:\n      before_text = clean_lines.elided[linenum - 1] + before_text\n    before_text = match_symbol.group(1)\n\n    # Patterns that are likely to be types:\n    #   [](type&&\n    #   for (type&&\n    #   sizeof(type&&\n    #   operator=(type&&\n    #\n    if Search(r'(?:\\]|\\bfor|\\bsizeof|\\boperator\\s*\\S+\\s*)\\s*$', before_text):\n      return True\n\n    # Patterns that are likely to be expressions:\n    #   if (expression &&\n    #   while (expression &&\n    #   : initializer(expression &&\n    #   , initializer(expression &&\n    #   ( FunctionCall(expression &&\n    #   + FunctionCall(expression &&\n    #   + (expression &&\n    #\n    # The last '+' represents operators such as '+' and '-'.\n    if Search(r'(?:\\bif|\\bwhile|[-+=%^(<!?:,&*]\\s*)$', before_text):\n      return False\n\n    # Something else.  Check that tokens to the left look like\n    #   return_type function_name\n    match_func = Match(r'^(.*\\S.*)\\s+\\w(?:\\w|::)*(?:<[^<>]*>)?\\s*$',\n                       match_symbol.group(1))\n    if match_func:\n      # Check for constructors, which don't have return types.\n      if Search(r'\\b(?:explicit|inline)$', match_func.group(1)):\n        return True\n      implicit_constructor = Match(r'\\s*(\\w+)\\((?:const\\s+)?(\\w+)', prefix)\n      if (implicit_constructor and\n          implicit_constructor.group(1) == implicit_constructor.group(2)):\n        return True\n      return IsRValueType(typenames, clean_lines, nesting_state, linenum,\n                          len(match_func.group(1)))\n\n    # Nothing before the function name.  If this is inside a block scope,\n    # this is probably a function call.\n    return not (nesting_state.previous_stack_top and\n                nesting_state.previous_stack_top.IsBlockInfo())\n\n  if match_symbol.group(2) == '>':\n    # Possibly a closing bracket, check that what's on the other side\n    # looks like the start of a template.\n    return IsTemplateParameterList(\n        clean_lines, start, len(match_symbol.group(1)))\n\n  # Some other symbol, usually something like \"a=b&&c\".  This is most\n  # likely not a type.\n  return False", "response": "Checks if the token ending on the line is a type."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if the current constructor or operator is deleted or default.", "response": "def IsDeletedOrDefault(clean_lines, linenum):\n  \"\"\"Check if current constructor or operator is deleted or default.\n\n  Args:\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n  Returns:\n    True if this is a deleted or default constructor.\n  \"\"\"\n  open_paren = clean_lines.elided[linenum].find('(')\n  if open_paren < 0:\n    return False\n  (close_line, _, close_paren) = CloseExpression(\n      clean_lines, linenum, open_paren)\n  if close_paren < 0:\n    return False\n  return Match(r'\\s*=\\s*(?:delete|default)\\b', close_line[close_paren:])"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if a particular line is allowed on a particular line.", "response": "def IsRValueAllowed(clean_lines, linenum, typenames):\n  \"\"\"Check if RValue reference is allowed on a particular line.\n\n  Args:\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    typenames: set of type names from template-argument-list.\n  Returns:\n    True if line is within the region where RValue references are allowed.\n  \"\"\"\n  # Allow region marked by PUSH/POP macros\n  for i in xrange(linenum, 0, -1):\n    line = clean_lines.elided[i]\n    if Match(r'GOOGLE_ALLOW_RVALUE_REFERENCES_(?:PUSH|POP)', line):\n      if not line.endswith('PUSH'):\n        return False\n      for j in xrange(linenum, clean_lines.NumLines(), 1):\n        line = clean_lines.elided[j]\n        if Match(r'GOOGLE_ALLOW_RVALUE_REFERENCES_(?:PUSH|POP)', line):\n          return line.endswith('POP')\n\n  # Allow operator=\n  line = clean_lines.elided[linenum]\n  if Search(r'\\boperator\\s*=\\s*\\(', line):\n    return IsDeletedOrDefault(clean_lines, linenum)\n\n  # Allow constructors\n  match = Match(r'\\s*(?:[\\w<>]+::)*([\\w<>]+)\\s*::\\s*([\\w<>]+)\\s*\\(', line)\n  if match and match.group(1) == match.group(2):\n    return IsDeletedOrDefault(clean_lines, linenum)\n  if Search(r'\\b(?:explicit|inline)\\s+[\\w<>]+\\s*\\(', line):\n    return IsDeletedOrDefault(clean_lines, linenum)\n\n  if Match(r'\\s*[\\w<>]+\\s*\\(', line):\n    previous_line = 'ReturnType'\n    if linenum > 0:\n      previous_line = clean_lines.elided[linenum - 1]\n    if Match(r'^\\s*$', previous_line) or Search(r'[{}:;]\\s*$', previous_line):\n      return IsDeletedOrDefault(clean_lines, linenum)\n\n  # Reject types not mentioned in template-argument-list\n  while line:\n    match = Match(r'^.*?(\\w+)\\s*&&(.*)$', line)\n    if not match:\n      break\n    if match.group(1) not in typenames:\n      return False\n    line = match.group(2)\n\n  # All RValue types that were in template-argument-list should have\n  # been removed by now.  Those were allowed, assuming that they will\n  # be forwarded.\n  #\n  # If there are no remaining RValue types left (i.e. types that were\n  # not found in template-argument-list), flag those as not allowed.\n  return line.find('&&') < 0"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a set of type names that are used in the template - argument - list for this function declaration.", "response": "def GetTemplateArgs(clean_lines, linenum):\n  \"\"\"Find list of template arguments associated with this function declaration.\n\n  Args:\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: Line number containing the start of the function declaration,\n             usually one line after the end of the template-argument-list.\n  Returns:\n    Set of type names, or empty set if this does not appear to have\n    any template parameters.\n  \"\"\"\n  # Find start of function\n  func_line = linenum\n  while func_line > 0:\n    line = clean_lines.elided[func_line]\n    if Match(r'^\\s*$', line):\n      return set()\n    if line.find('(') >= 0:\n      break\n    func_line -= 1\n  if func_line == 0:\n    return set()\n\n  # Collapse template-argument-list into a single string\n  argument_list = ''\n  match = Match(r'^(\\s*template\\s*)<', clean_lines.elided[func_line])\n  if match:\n    # template-argument-list on the same line as function name\n    start_col = len(match.group(1))\n    _, end_line, end_col = CloseExpression(clean_lines, func_line, start_col)\n    if end_col > -1 and end_line == func_line:\n      start_col += 1  # Skip the opening bracket\n      argument_list = clean_lines.elided[func_line][start_col:end_col]\n\n  elif func_line > 1:\n    # template-argument-list one line before function name\n    match = Match(r'^(.*)>\\s*$', clean_lines.elided[func_line - 1])\n    if match:\n      end_col = len(match.group(1))\n      _, start_line, start_col = ReverseCloseExpression(\n          clean_lines, func_line - 1, end_col)\n      if start_col > -1:\n        start_col += 1  # Skip the opening bracket\n        while start_line < func_line - 1:\n          argument_list += clean_lines.elided[start_line][start_col:]\n          start_col = 0\n          start_line += 1\n        argument_list += clean_lines.elided[func_line - 1][start_col:end_col]\n\n  if not argument_list:\n    return set()\n\n  # Extract type names\n  typenames = set()\n  while True:\n    match = Match(r'^[,\\s]*(?:typename|class)(?:\\.\\.\\.)?\\s+(\\w+)(.*)$',\n                  argument_list)\n    if not match:\n      break\n    typenames.add(match.group(1))\n    argument_list = match.group(2)\n  return typenames"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck for rvalue references.", "response": "def CheckRValueReference(filename, clean_lines, linenum, nesting_state, error):\n  \"\"\"Check for rvalue references.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    nesting_state: A NestingState instance which maintains information about\n                   the current stack of nested blocks being parsed.\n    error: The function to call with any errors found.\n  \"\"\"\n  # Find lines missing spaces around &&.\n  # TODO(unknown): currently we don't check for rvalue references\n  # with spaces surrounding the && to avoid false positives with\n  # boolean expressions.\n  line = clean_lines.elided[linenum]\n  match = Match(r'^(.*\\S)&&', line)\n  if not match:\n    match = Match(r'(.*)&&\\S', line)\n  if (not match) or '(&&)' in line or Search(r'\\boperator\\s*$', match.group(1)):\n    return\n\n  # Either poorly formed && or an rvalue reference, check the context\n  # to get a more accurate error message.  Mostly we want to determine\n  # if what's to the left of \"&&\" is a type or not.\n  typenames = GetTemplateArgs(clean_lines, linenum)\n  and_pos = len(match.group(1))\n  if IsRValueType(typenames, clean_lines, nesting_state, linenum, and_pos):\n    if not IsRValueAllowed(clean_lines, linenum, typenames):\n      error(filename, linenum, 'build/c++11', 3,\n            'RValue references are an unapproved C++ feature.')\n  else:\n    error(filename, linenum, 'whitespace/operators', 3,\n          'Missing spaces around &&')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef CheckBraces(filename, clean_lines, linenum, error):\n\n  line = clean_lines.elided[linenum]        # get rid of comments and strings\n\n  if Match(r'\\s*{\\s*$', line):\n    # We allow an open brace to start a line in the case where someone is using\n    # braces in a block to explicitly create a new scope, which is commonly used\n    # to control the lifetime of stack-allocated variables.  Braces are also\n    # used for brace initializers inside function calls.  We don't detect this\n    # perfectly: we just don't complain if the last non-whitespace character on\n    # the previous non-blank line is ',', ';', ':', '(', '{', or '}', or if the\n    # previous line starts a preprocessor block.\n    prevline = GetPreviousNonBlankLine(clean_lines, linenum)[0]\n    if (not Search(r'[,;:}{(]\\s*$', prevline) and\n        not Match(r'\\s*#', prevline)):\n      error(filename, linenum, 'whitespace/braces', 4,\n            '{ should almost always be at the end of the previous line')\n\n  # An else clause should be on the same line as the preceding closing brace.\n  if Match(r'\\s*else\\b\\s*(?:if\\b|\\{|$)', line):\n    prevline = GetPreviousNonBlankLine(clean_lines, linenum)[0]\n    if Match(r'\\s*}\\s*$', prevline):\n      error(filename, linenum, 'whitespace/newline', 4,\n            'An else should appear on the same line as the preceding }')\n\n  # If braces come on one side of an else, they should be on both.\n  # However, we have to worry about \"else if\" that spans multiple lines!\n  if Search(r'else if\\s*\\(', line):       # could be multi-line if\n    brace_on_left = bool(Search(r'}\\s*else if\\s*\\(', line))\n    # find the ( after the if\n    pos = line.find('else if')\n    pos = line.find('(', pos)\n    if pos > 0:\n      (endline, _, endpos) = CloseExpression(clean_lines, linenum, pos)\n      brace_on_right = endline[endpos:].find('{') != -1\n      if brace_on_left != brace_on_right:    # must be brace after if\n        error(filename, linenum, 'readability/braces', 5,\n              'If an else has a brace on one side, it should have it on both')\n  elif Search(r'}\\s*else[^{]*$', line) or Match(r'[^}]*else\\s*{', line):\n    error(filename, linenum, 'readability/braces', 5,\n          'If an else has a brace on one side, it should have it on both')\n\n  # Likewise, an else should never have the else clause on the same line\n  if Search(r'\\belse [^\\s{]', line) and not Search(r'\\belse if\\b', line):\n    error(filename, linenum, 'whitespace/newline', 4,\n          'Else clause should never be on same line as else (use 2 lines)')\n\n  # In the same way, a do/while should never be on one line\n  if Match(r'\\s*do [^\\s{]', line):\n    error(filename, linenum, 'whitespace/newline', 4,\n          'do/while clauses should not be on a single line')\n\n  # Check single-line if/else bodies. The style guide says 'curly braces are not\n  # required for single-line statements'. We additionally allow multi-line,\n  # single statements, but we reject anything with more than one semicolon in\n  # it. This means that the first semicolon after the if should be at the end of\n  # its line, and the line after that should have an indent level equal to or\n  # lower than the if. We also check for ambiguous if/else nesting without\n  # braces.\n  if_else_match = Search(r'\\b(if\\s*\\(|else\\b)', line)\n  if if_else_match and not Match(r'\\s*#', line):\n    if_indent = GetIndentLevel(line)\n    endline, endlinenum, endpos = line, linenum, if_else_match.end()\n    if_match = Search(r'\\bif\\s*\\(', line)\n    if if_match:\n      # This could be a multiline if condition, so find the end first.\n      pos = if_match.end() - 1\n      (endline, endlinenum, endpos) = CloseExpression(clean_lines, linenum, pos)\n    # Check for an opening brace, either directly after the if or on the next\n    # line. If found, this isn't a single-statement conditional.\n    if (not Match(r'\\s*{', endline[endpos:])\n        and not (Match(r'\\s*$', endline[endpos:])\n                 and endlinenum < (len(clean_lines.elided) - 1)\n                 and Match(r'\\s*{', clean_lines.elided[endlinenum + 1]))):\n      while (endlinenum < len(clean_lines.elided)\n             and ';' not in clean_lines.elided[endlinenum][endpos:]):\n        endlinenum += 1\n        endpos = 0\n      if endlinenum < len(clean_lines.elided):\n        endline = clean_lines.elided[endlinenum]\n        # We allow a mix of whitespace and closing braces (e.g. for one-liner\n        # methods) and a single \\ after the semicolon (for macros)\n        endpos = endline.find(';')\n        if not Match(r';[\\s}]*(\\\\?)$', endline[endpos:]):\n          # Semicolon isn't the last character, there's something trailing.\n          # Output a warning if the semicolon is not contained inside\n          # a lambda expression.\n          if not Match(r'^[^{};]*\\[[^\\[\\]]*\\][^{}]*\\{[^{}]*\\}\\s*\\)*[;,]\\s*$',\n                       endline):\n            error(filename, linenum, 'readability/braces', 4,\n                  'If/else bodies with multiple statements require braces')\n        elif endlinenum < len(clean_lines.elided) - 1:\n          # Make sure the next line is dedented\n          next_line = clean_lines.elided[endlinenum + 1]\n          next_indent = GetIndentLevel(next_line)\n          # With ambiguous nested if statements, this will error out on the\n          # if that *doesn't* match the else, regardless of whether it's the\n          # inner one or outer one.\n          if (if_match and Match(r'\\s*else\\b', next_line)\n              and next_indent != if_indent):\n            error(filename, linenum, 'readability/braces', 4,\n                  'Else clause should be indented at the same level as if. '\n                  'Ambiguous nested if/else chains require braces.')\n          elif next_indent > if_indent:\n            error(filename, linenum, 'readability/braces', 4,\n                  'If/else bodies with multiple statements require braces')", "response": "Checks for misplaced braces."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef CheckEmptyBlockBody(filename, clean_lines, linenum, error):\n\n  # Search for loop keywords at the beginning of the line.  Because only\n  # whitespaces are allowed before the keywords, this will also ignore most\n  # do-while-loops, since those lines should start with closing brace.\n  #\n  # We also check \"if\" blocks here, since an empty conditional block\n  # is likely an error.\n  line = clean_lines.elided[linenum]\n  matched = Match(r'\\s*(for|while|if)\\s*\\(', line)\n  if matched:\n    # Find the end of the conditional expression\n    (end_line, end_linenum, end_pos) = CloseExpression(\n        clean_lines, linenum, line.find('('))\n\n    # Output warning if what follows the condition expression is a semicolon.\n    # No warning for all other cases, including whitespace or newline, since we\n    # have a separate check for semicolons preceded by whitespace.\n    if end_pos >= 0 and Match(r';', end_line[end_pos:]):\n      if matched.group(1) == 'if':\n        error(filename, end_linenum, 'whitespace/empty_conditional_body', 5,\n              'Empty conditional bodies should use {}')\n      else:\n        error(filename, end_linenum, 'whitespace/empty_loop_body', 5,\n              'Empty loop bodies should use {} or continue')", "response": "Checks for empty loop body with only a single semicolon."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef CheckStyle(filename, clean_lines, linenum, file_extension, nesting_state,\n               error):\n  \"\"\"Checks rules from the 'C++ style rules' section of cppguide.html.\n\n  Most of these rules are hard to test (naming, comment style), but we\n  do what we can.  In particular we check for 2-space indents, line lengths,\n  tab usage, spaces inside code, etc.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    file_extension: The extension (without the dot) of the filename.\n    nesting_state: A NestingState instance which maintains information about\n                   the current stack of nested blocks being parsed.\n    error: The function to call with any errors found.\n  \"\"\"\n\n  # Don't use \"elided\" lines here, otherwise we can't check commented lines.\n  # Don't want to use \"raw\" either, because we don't want to check inside C++11\n  # raw strings,\n  raw_lines = clean_lines.lines_without_raw_strings\n  line = raw_lines[linenum]\n\n  if line.find('\\t') != -1:\n    error(filename, linenum, 'whitespace/tab', 1,\n          'Tab found; better to use spaces')\n\n  # One or three blank spaces at the beginning of the line is weird; it's\n  # hard to reconcile that with 2-space indents.\n  # NOTE: here are the conditions rob pike used for his tests.  Mine aren't\n  # as sophisticated, but it may be worth becoming so:  RLENGTH==initial_spaces\n  # if(RLENGTH > 20) complain = 0;\n  # if(match($0, \" +(error|private|public|protected):\")) complain = 0;\n  # if(match(prev, \"&& *$\")) complain = 0;\n  # if(match(prev, \"\\\\|\\\\| *$\")) complain = 0;\n  # if(match(prev, \"[\\\",=><] *$\")) complain = 0;\n  # if(match($0, \" <<\")) complain = 0;\n  # if(match(prev, \" +for \\\\(\")) complain = 0;\n  # if(prevodd && match(prevprev, \" +for \\\\(\")) complain = 0;\n  scope_or_label_pattern = r'\\s*\\w+\\s*:\\s*\\\\?$'\n  classinfo = nesting_state.InnermostClass()\n  initial_spaces = 0\n  cleansed_line = clean_lines.elided[linenum]\n  while initial_spaces < len(line) and line[initial_spaces] == ' ':\n    initial_spaces += 1\n  if line and line[-1].isspace():\n    error(filename, linenum, 'whitespace/end_of_line', 4,\n          'Line ends in whitespace.  Consider deleting these extra spaces.')\n  # There are certain situations we allow one space, notably for\n  # section labels, and also lines containing multi-line raw strings.\n  elif ((initial_spaces == 1 or initial_spaces == 3) and\n        not Match(scope_or_label_pattern, cleansed_line) and\n        not (clean_lines.raw_lines[linenum] != line and\n             Match(r'^\\s*\"\"', line))):\n    error(filename, linenum, 'whitespace/indent', 3,\n          'Weird number of spaces at line-start.  '\n          'Are you using a 2-space indent?')\n\n  # Check if the line is a header guard.\n  is_header_guard = False\n  if file_extension == 'h':\n    cppvar = GetHeaderGuardCPPVariable(filename)\n    if (line.startswith('#ifndef %s' % cppvar) or\n        line.startswith('#define %s' % cppvar) or\n        line.startswith('#endif  // %s' % cppvar)):\n      is_header_guard = True\n  # #include lines and header guards can be long, since there's no clean way to\n  # split them.\n  #\n  # URLs can be long too.  It's possible to split these, but it makes them\n  # harder to cut&paste.\n  #\n  # The \"$Id:...$\" comment may also get very long without it being the\n  # developers fault.\n  if (not line.startswith('#include') and not is_header_guard and\n      not Match(r'^\\s*//.*http(s?)://\\S*$', line) and\n      not Match(r'^// \\$Id:.*#[0-9]+ \\$$', line)):\n    line_width = GetLineWidth(line)\n    extended_length = int((_line_length * 1.25))\n    if line_width > extended_length:\n      error(filename, linenum, 'whitespace/line_length', 4,\n            'Lines should very rarely be longer than %i characters' %\n            extended_length)\n    elif line_width > _line_length:\n      error(filename, linenum, 'whitespace/line_length', 2,\n            'Lines should be <= %i characters long' % _line_length)\n\n  if (cleansed_line.count(';') > 1 and\n      # for loops are allowed two ;'s (and may run over two lines).\n      cleansed_line.find('for') == -1 and\n      (GetPreviousNonBlankLine(clean_lines, linenum)[0].find('for') == -1 or\n       GetPreviousNonBlankLine(clean_lines, linenum)[0].find(';') != -1) and\n      # It's ok to have many commands in a switch case that fits in 1 line\n      not ((cleansed_line.find('case ') != -1 or\n            cleansed_line.find('default:') != -1) and\n           cleansed_line.find('break;') != -1)):\n    error(filename, linenum, 'whitespace/newline', 0,\n          'More than one command on the same line')\n\n  # Some more style checks\n  CheckBraces(filename, clean_lines, linenum, error)\n  CheckTrailingSemicolon(filename, clean_lines, linenum, error)\n  CheckEmptyBlockBody(filename, clean_lines, linenum, error)\n  CheckAccess(filename, clean_lines, linenum, nesting_state, error)\n  CheckSpacing(filename, clean_lines, linenum, nesting_state, error)\n  CheckOperatorSpacing(filename, clean_lines, linenum, error)\n  CheckParenthesisSpacing(filename, clean_lines, linenum, error)\n  CheckCommaSpacing(filename, clean_lines, linenum, error)\n  CheckBracesSpacing(filename, clean_lines, linenum, error)\n  CheckSpacingForFunctionCall(filename, clean_lines, linenum, error)\n  CheckRValueReference(filename, clean_lines, linenum, nesting_state, error)\n  CheckCheck(filename, clean_lines, linenum, error)\n  CheckAltTokens(filename, clean_lines, linenum, error)\n  classinfo = nesting_state.InnermostClass()\n  if classinfo:\n    CheckSectionSpacing(filename, clean_lines, classinfo, linenum, error)", "response": "Checks the rules from the C ++ style rules section of cppguide. html."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _DropCommonSuffixes(filename):\n  for suffix in ('test.cc', 'regtest.cc', 'unittest.cc',\n                 'inl.h', 'impl.h', 'internal.h'):\n    if (filename.endswith(suffix) and len(filename) > len(suffix) and\n        filename[-len(suffix) - 1] in ('-', '_')):\n      return filename[:-len(suffix) - 1]\n  return os.path.splitext(filename)[0]", "response": "Drops common suffixes like _test. cc or - inl. h from filename.\n    Returns the filename with the common suffix removed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef CheckGlobalStatic(filename, clean_lines, linenum, error):\n  line = clean_lines.elided[linenum]\n\n  # Match two lines at a time to support multiline declarations\n  if linenum + 1 < clean_lines.NumLines() and not Search(r'[;({]', line):\n    line += clean_lines.elided[linenum + 1].strip()\n\n  # Check for people declaring static/global STL strings at the top level.\n  # This is dangerous because the C++ language does not guarantee that\n  # globals with constructors are initialized before the first access.\n  match = Match(\n      r'((?:|static +)(?:|const +))string +([a-zA-Z0-9_:]+)\\b(.*)',\n      line)\n\n  # Remove false positives:\n  # - String pointers (as opposed to values).\n  #    string *pointer\n  #    const string *pointer\n  #    string const *pointer\n  #    string *const pointer\n  #\n  # - Functions and template specializations.\n  #    string Function<Type>(...\n  #    string Class<Type>::Method(...\n  #\n  # - Operators.  These are matched separately because operator names\n  #   cross non-word boundaries, and trying to match both operators\n  #   and functions at the same time would decrease accuracy of\n  #   matching identifiers.\n  #    string Class::operator*()\n  if (match and\n      not Search(r'\\bstring\\b(\\s+const)?\\s*\\*\\s*(const\\s+)?\\w', line) and\n      not Search(r'\\boperator\\W', line) and\n      not Match(r'\\s*(<.*>)?(::[a-zA-Z0-9_]+)*\\s*\\(([^\"]|$)', match.group(3))):\n    error(filename, linenum, 'runtime/string', 4,\n          'For a static/global string constant, use a C style string instead: '\n          '\"%schar %s[]\".' %\n          (match.group(1), match.group(2)))\n\n  if Search(r'\\b([A-Za-z0-9_]*_)\\(\\1\\)', line):\n    error(filename, linenum, 'runtime/init', 4,\n          'You seem to be initializing a member variable with itself.')", "response": "Checks for unsafe global or static objects at the current line."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef CheckCStyleCast(filename, clean_lines, linenum, cast_type, pattern, error):\n  line = clean_lines.elided[linenum]\n  match = Search(pattern, line)\n  if not match:\n    return False\n\n  # Exclude lines with keywords that tend to look like casts\n  context = line[0:match.start(1) - 1]\n  if Match(r'.*\\b(?:sizeof|alignof|alignas|[_A-Z][_A-Z0-9]*)\\s*$', context):\n    return False\n\n  # Try expanding current context to see if we one level of\n  # parentheses inside a macro.\n  if linenum > 0:\n    for i in xrange(linenum - 1, max(0, linenum - 5), -1):\n      context = clean_lines.elided[i] + context\n  if Match(r'.*\\b[_A-Z][_A-Z0-9]*\\s*\\((?:\\([^()]*\\)|[^()])*$', context):\n    return False\n\n  # operator++(int) and operator--(int)\n  if context.endswith(' operator++') or context.endswith(' operator--'):\n    return False\n\n  # A single unnamed argument for a function tends to look like old\n  # style cast.  If we see those, don't issue warnings for deprecated\n  # casts, instead issue warnings for unnamed arguments where\n  # appropriate.\n  #\n  # These are things that we want warnings for, since the style guide\n  # explicitly require all parameters to be named:\n  #   Function(int);\n  #   Function(int) {\n  #   ConstMember(int) const;\n  #   ConstMember(int) const {\n  #   ExceptionMember(int) throw (...);\n  #   ExceptionMember(int) throw (...) {\n  #   PureVirtual(int) = 0;\n  #   [](int) -> bool {\n  #\n  # These are functions of some sort, where the compiler would be fine\n  # if they had named parameters, but people often omit those\n  # identifiers to reduce clutter:\n  #   (FunctionPointer)(int);\n  #   (FunctionPointer)(int) = value;\n  #   Function((function_pointer_arg)(int))\n  #   Function((function_pointer_arg)(int), int param)\n  #   <TemplateArgument(int)>;\n  #   <(FunctionPointerTemplateArgument)(int)>;\n  remainder = line[match.end(0):]\n  if Match(r'^\\s*(?:;|const\\b|throw\\b|final\\b|override\\b|[=>{),]|->)',\n           remainder):\n    # Looks like an unnamed parameter.\n\n    # Don't warn on any kind of template arguments.\n    if Match(r'^\\s*>', remainder):\n      return False\n\n    # Don't warn on assignments to function pointers, but keep warnings for\n    # unnamed parameters to pure virtual functions.  Note that this pattern\n    # will also pass on assignments of \"0\" to function pointers, but the\n    # preferred values for those would be \"nullptr\" or \"NULL\".\n    matched_zero = Match(r'^\\s=\\s*(\\S+)\\s*;', remainder)\n    if matched_zero and matched_zero.group(1) != '0':\n      return False\n\n    # Don't warn on function pointer declarations.  For this we need\n    # to check what came before the \"(type)\" string.\n    if Match(r'.*\\)\\s*$', line[0:match.start(0)]):\n      return False\n\n    # Don't warn if the parameter is named with block comments, e.g.:\n    #  Function(int /*unused_param*/);\n    raw_line = clean_lines.raw_lines[linenum]\n    if '/*' in raw_line:\n      return False\n\n    # Passed all filters, issue warning here.\n    error(filename, linenum, 'readability/function', 3,\n          'All parameters should be named in a function')\n    return True\n\n  # At this point, all that should be left is actual casts.\n  error(filename, linenum, 'readability/casting', 4,\n        'Using C-style cast.  Use %s<%s>(...) instead' %\n        (cast_type, match.group(1)))\n\n  return True", "response": "Checks if a C - style cast is found."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if two files belong to the same module.", "response": "def FilesBelongToSameModule(filename_cc, filename_h):\n  \"\"\"Check if these two filenames belong to the same module.\n\n  The concept of a 'module' here is a as follows:\n  foo.h, foo-inl.h, foo.cc, foo_test.cc and foo_unittest.cc belong to the\n  same 'module' if they are in the same directory.\n  some/path/public/xyzzy and some/path/internal/xyzzy are also considered\n  to belong to the same module here.\n\n  If the filename_cc contains a longer path than the filename_h, for example,\n  '/absolute/path/to/base/sysinfo.cc', and this file would include\n  'base/sysinfo.h', this function also produces the prefix needed to open the\n  header. This is used by the caller of this function to more robustly open the\n  header file. We don't have access to the real include paths in this context,\n  so we need this guesswork here.\n\n  Known bugs: tools/base/bar.cc and base/bar.h belong to the same module\n  according to this implementation. Because of this, this function gives\n  some false positives. This should be sufficiently rare in practice.\n\n  Args:\n    filename_cc: is the path for the .cc file\n    filename_h: is the path for the header path\n\n  Returns:\n    Tuple with a bool and a string:\n    bool: True if filename_cc and filename_h belong to the same module.\n    string: the additional prefix needed to open the header file.\n  \"\"\"\n\n  if not filename_cc.endswith('.cc'):\n    return (False, '')\n  filename_cc = filename_cc[:-len('.cc')]\n  if filename_cc.endswith('_unittest'):\n    filename_cc = filename_cc[:-len('_unittest')]\n  elif filename_cc.endswith('_test'):\n    filename_cc = filename_cc[:-len('_test')]\n  filename_cc = filename_cc.replace('/public/', '/')\n  filename_cc = filename_cc.replace('/internal/', '/')\n\n  if not filename_h.endswith('.h'):\n    return (False, '')\n  filename_h = filename_h[:-len('.h')]\n  if filename_h.endswith('-inl'):\n    filename_h = filename_h[:-len('-inl')]\n  filename_h = filename_h.replace('/public/', '/')\n  filename_h = filename_h.replace('/internal/', '/')\n\n  files_belong_to_same_module = filename_cc.endswith(filename_h)\n  common_path = ''\n  if files_belong_to_same_module:\n    common_path = filename_cc[:-len(filename_h)]\n  return files_belong_to_same_module, common_path"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef CheckForIncludeWhatYouUse(filename, clean_lines, include_state, error,\n                              io=codecs):\n  \"\"\"Reports for missing stl includes.\n\n  This function will output warnings to make sure you are including the headers\n  necessary for the stl containers and functions that you use. We only give one\n  reason to include a header. For example, if you use both equal_to<> and\n  less<> in a .h file, only one (the latter in the file) of these will be\n  reported as a reason to include the <functional>.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    include_state: An _IncludeState instance.\n    error: The function to call with any errors found.\n    io: The IO factory to use to read the header file. Provided for unittest\n        injection.\n  \"\"\"\n  required = {}  # A map of header name to linenumber and the template entity.\n                 # Example of required: { '<functional>': (1219, 'less<>') }\n\n  for linenum in xrange(clean_lines.NumLines()):\n    line = clean_lines.elided[linenum]\n    if not line or line[0] == '#':\n      continue\n\n    # String is special -- it is a non-templatized type in STL.\n    matched = _RE_PATTERN_STRING.search(line)\n    if matched:\n      # Don't warn about strings in non-STL namespaces:\n      # (We check only the first match per line; good enough.)\n      prefix = line[:matched.start()]\n      if prefix.endswith('std::') or not prefix.endswith('::'):\n        required['<string>'] = (linenum, 'string')\n\n    for pattern, template, header in _re_pattern_algorithm_header:\n      if pattern.search(line):\n        required[header] = (linenum, template)\n\n    # The following function is just a speed up, no semantics are changed.\n    if not '<' in line:  # Reduces the cpu time usage by skipping lines.\n      continue\n\n    for pattern, template, header in _re_pattern_templates:\n      if pattern.search(line):\n        required[header] = (linenum, template)\n\n  # The policy is that if you #include something in foo.h you don't need to\n  # include it again in foo.cc. Here, we will look at possible includes.\n  # Let's flatten the include_state include_list and copy it into a dictionary.\n  include_dict = dict([item for sublist in include_state.include_list\n                       for item in sublist])\n\n  # Did we find the header for this file (if any) and successfully load it?\n  header_found = False\n\n  # Use the absolute path so that matching works properly.\n  abs_filename = FileInfo(filename).FullName()\n\n  # For Emacs's flymake.\n  # If cpplint is invoked from Emacs's flymake, a temporary file is generated\n  # by flymake and that file name might end with '_flymake.cc'. In that case,\n  # restore original file name here so that the corresponding header file can be\n  # found.\n  # e.g. If the file name is 'foo_flymake.cc', we should search for 'foo.h'\n  # instead of 'foo_flymake.h'\n  abs_filename = re.sub(r'_flymake\\.cc$', '.cc', abs_filename)\n\n  # include_dict is modified during iteration, so we iterate over a copy of\n  # the keys.\n  header_keys = include_dict.keys()\n  for header in header_keys:\n    (same_module, common_path) = FilesBelongToSameModule(abs_filename, header)\n    fullpath = common_path + header\n    if same_module and UpdateIncludeState(fullpath, include_dict, io):\n      header_found = True\n\n  # If we can't find the header file for a .cc, assume it's because we don't\n  # know where to look. In that case we'll give up as we're not sure they\n  # didn't include it in the .h file.\n  # TODO(unknown): Do a better job of finding .h files so we are confident that\n  # not having the .h file means there isn't one.\n  if filename.endswith('.cc') and not header_found:\n    return\n\n  # All the lines have been processed, report the errors found.\n  for required_header_unstripped in required:\n    template = required[required_header_unstripped][1]\n    if required_header_unstripped.strip('<>\"') not in include_dict:\n      error(filename, required[required_header_unstripped][0],\n            'build/include_what_you_use', 4,\n            'Add #include ' + required_header_unstripped + ' for ' + template)", "response": "Checks if the file contains the required header."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking that default lambda captures are not used.", "response": "def CheckDefaultLambdaCaptures(filename, clean_lines, linenum, error):\n  \"\"\"Check that default lambda captures are not used.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n  line = clean_lines.elided[linenum]\n\n  # A lambda introducer specifies a default capture if it starts with \"[=\"\n  # or if it starts with \"[&\" _not_ followed by an identifier.\n  match = Match(r'^(.*)\\[\\s*(?:=|&[^\\w])', line)\n  if match:\n    # Found a potential error, check what comes after the lambda-introducer.\n    # If it's not open parenthesis (for lambda-declarator) or open brace\n    # (for compound-statement), it's not a lambda.\n    line, _, pos = CloseExpression(clean_lines, linenum, len(match.group(1)))\n    if pos >= 0 and Match(r'^\\s*[{(]', line[pos:]):\n      error(filename, linenum, 'build/c++11',\n            4,  # 4 = high confidence\n            'Default lambda captures are an unapproved C++ feature.')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprocessing a single line in the file.", "response": "def ProcessLine(filename, file_extension, clean_lines, line,\n                include_state, function_state, nesting_state, error,\n                extra_check_functions=[]):\n  \"\"\"Processes a single line in the file.\n\n  Args:\n    filename: Filename of the file that is being processed.\n    file_extension: The extension (dot not included) of the file.\n    clean_lines: An array of strings, each representing a line of the file,\n                 with comments stripped.\n    line: Number of line being processed.\n    include_state: An _IncludeState instance in which the headers are inserted.\n    function_state: A _FunctionState instance which counts function lines, etc.\n    nesting_state: A NestingState instance which maintains information about\n                   the current stack of nested blocks being parsed.\n    error: A callable to which errors are reported, which takes 4 arguments:\n           filename, line number, error level, and message\n    extra_check_functions: An array of additional check functions that will be\n                           run on each source line. Each function takes 4\n                           arguments: filename, clean_lines, line, error\n  \"\"\"\n  raw_lines = clean_lines.raw_lines\n  ParseNolintSuppressions(filename, raw_lines[line], line, error)\n  nesting_state.Update(filename, clean_lines, line, error)\n  CheckForNamespaceIndentation(filename, nesting_state, clean_lines, line,\n                               error)\n  if nesting_state.InAsmBlock(): return\n  CheckForFunctionLengths(filename, clean_lines, line, function_state, error)\n  CheckForMultilineCommentsAndStrings(filename, clean_lines, line, error)\n  CheckStyle(filename, clean_lines, line, file_extension, nesting_state, error)\n  CheckLanguage(filename, clean_lines, line, file_extension, include_state,\n                nesting_state, error)\n  CheckForNonConstReference(filename, clean_lines, line, nesting_state, error)\n  CheckForNonStandardConstructs(filename, clean_lines, line,\n                                nesting_state, error)\n  CheckVlogArguments(filename, clean_lines, line, error)\n  CheckPosixThreading(filename, clean_lines, line, error)\n  CheckInvalidIncrement(filename, clean_lines, line, error)\n  CheckMakePairUsesDeduction(filename, clean_lines, line, error)\n  CheckDefaultLambdaCaptures(filename, clean_lines, line, error)\n  CheckRedundantVirtual(filename, clean_lines, line, error)\n  CheckRedundantOverrideOrFinal(filename, clean_lines, line, error)\n  for check_fn in extra_check_functions:\n    check_fn(filename, clean_lines, line, error)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nflagging those c ++ 11 features that we only allow in certain places.", "response": "def FlagCxx11Features(filename, clean_lines, linenum, error):\n  \"\"\"Flag those c++11 features that we only allow in certain places.\n\n  Args:\n    filename: The name of the current file.\n    clean_lines: A CleansedLines instance containing the file.\n    linenum: The number of the line to check.\n    error: The function to call with any errors found.\n  \"\"\"\n  line = clean_lines.elided[linenum]\n\n  # Flag unapproved C++11 headers.\n  include = Match(r'\\s*#\\s*include\\s+[<\"]([^<\"]+)[\">]', line)\n  if include and include.group(1) in ('cfenv',\n                                      'condition_variable',\n                                      'fenv.h',\n                                      'future',\n                                      'mutex',\n                                      'thread',\n                                      'chrono',\n                                      'ratio',\n                                      'regex',\n                                      'system_error',\n                                     ):\n    error(filename, linenum, 'build/c++11', 5,\n          ('<%s> is an unapproved C++11 header.') % include.group(1))\n\n  # The only place where we need to worry about C++11 keywords and library\n  # features in preprocessor directives is in macro definitions.\n  if Match(r'\\s*#', line) and not Match(r'\\s*#\\s*define\\b', line): return\n\n  # These are classes and free functions.  The classes are always\n  # mentioned as std::*, but we only catch the free functions if\n  # they're not found by ADL.  They're alphabetical by header.\n  for top_name in (\n      # type_traits\n      'alignment_of',\n      'aligned_union',\n      ):\n    if Search(r'\\bstd::%s\\b' % top_name, line):\n      error(filename, linenum, 'build/c++11', 5,\n            ('std::%s is an unapproved C++11 class or function.  Send c-style '\n             'an example of where it would make your code more readable, and '\n             'they may let you use it.') % top_name)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ProcessConfigOverrides(filename):\n\n  abs_filename = os.path.abspath(filename)\n  cfg_filters = []\n  keep_looking = True\n  while keep_looking:\n    abs_path, base_name = os.path.split(abs_filename)\n    if not base_name:\n      break  # Reached the root directory.\n\n    cfg_file = os.path.join(abs_path, \"CPPLINT.cfg\")\n    abs_filename = abs_path\n    if not os.path.isfile(cfg_file):\n      continue\n\n    try:\n      with open(cfg_file) as file_handle:\n        for line in file_handle:\n          line, _, _ = line.partition('#')  # Remove comments.\n          if not line.strip():\n            continue\n\n          name, _, val = line.partition('=')\n          name = name.strip()\n          val = val.strip()\n          if name == 'set noparent':\n            keep_looking = False\n          elif name == 'filter':\n            cfg_filters.append(val)\n          elif name == 'exclude_files':\n            # When matching exclude_files pattern, use the base_name of\n            # the current file name or the directory name we are processing.\n            # For example, if we are checking for lint errors in /foo/bar/baz.cc\n            # and we found the .cfg file at /foo/CPPLINT.cfg, then the config\n            # file's \"exclude_files\" filter is meant to be checked against \"bar\"\n            # and not \"baz\" nor \"bar/baz.cc\".\n            if base_name:\n              pattern = re.compile(val)\n              if pattern.match(base_name):\n                sys.stderr.write('Ignoring \"%s\": file excluded by \"%s\". '\n                                 'File path component \"%s\" matches '\n                                 'pattern \"%s\"\\n' %\n                                 (filename, cfg_file, base_name, val))\n                return False\n          elif name == 'linelength':\n            global _line_length\n            try:\n                _line_length = int(val)\n            except ValueError:\n                sys.stderr.write('Line length must be numeric.')\n          else:\n            sys.stderr.write(\n                'Invalid configuration option (%s) in file %s\\n' %\n                (name, cfg_file))\n\n    except IOError:\n      sys.stderr.write(\n          \"Skipping config file '%s': Can't open for reading\\n\" % cfg_file)\n      keep_looking = False\n\n  # Apply all the accumulated filters in reverse order (top-level directory\n  # config options having the least priority).\n  for filter in reversed(cfg_filters):\n     _AddFilters(filter)\n\n  return True", "response": "Loads the configuration files and processes the config overrides."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the command line arguments and sets the output format and verbosity level as side - effectss.", "response": "def ParseArguments(args):\n  \"\"\"Parses the command line arguments.\n\n  This may set the output format and verbosity level as side-effects.\n\n  Args:\n    args: The command line arguments:\n\n  Returns:\n    The list of filenames to lint.\n  \"\"\"\n  try:\n    (opts, filenames) = getopt.getopt(args, '', ['help', 'output=', 'verbose=',\n                                                 'counting=',\n                                                 'filter=',\n                                                 'root=',\n                                                 'linelength=',\n                                                 'extensions='])\n  except getopt.GetoptError:\n    PrintUsage('Invalid arguments.')\n\n  verbosity = _VerboseLevel()\n  output_format = _OutputFormat()\n  filters = ''\n  counting_style = ''\n\n  for (opt, val) in opts:\n    if opt == '--help':\n      PrintUsage(None)\n    elif opt == '--output':\n      if val not in ('emacs', 'vs7', 'eclipse'):\n        PrintUsage('The only allowed output formats are emacs, vs7 and eclipse.')\n      output_format = val\n    elif opt == '--verbose':\n      verbosity = int(val)\n    elif opt == '--filter':\n      filters = val\n      if not filters:\n        PrintCategories()\n    elif opt == '--counting':\n      if val not in ('total', 'toplevel', 'detailed'):\n        PrintUsage('Valid counting options are total, toplevel, and detailed')\n      counting_style = val\n    elif opt == '--root':\n      global _root\n      _root = val\n    elif opt == '--linelength':\n      global _line_length\n      try:\n          _line_length = int(val)\n      except ValueError:\n          PrintUsage('Line length must be digits.')\n    elif opt == '--extensions':\n      global _valid_extensions\n      try:\n          _valid_extensions = set(val.split(','))\n      except ValueError:\n          PrintUsage('Extensions must be comma seperated list.')\n\n  if not filenames:\n    PrintUsage('No files were specified.')\n\n  _SetOutputFormat(output_format)\n  _SetVerboseLevel(verbosity)\n  _SetFilters(filters)\n  _SetCountingStyle(counting_style)\n\n  return filenames"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef PrintErrorCounts(self):\n    for category, count in self.errors_by_category.iteritems():\n      sys.stderr.write('Category \\'%s\\' errors found: %d\\n' %\n                       (category, count))\n    sys.stderr.write('Total errors found: %d\\n' % self.error_count)", "response": "Print a summary of errors by category and the total."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if too many lines in function body.", "response": "def Check(self, error, filename, linenum):\n    \"\"\"Report if too many lines in function body.\n\n    Args:\n      error: The function to call with any errors found.\n      filename: The name of the current file.\n      linenum: The number of the line to check.\n    \"\"\"\n    if Match(r'T(EST|est)', self.current_function):\n      base_trigger = self._TEST_TRIGGER\n    else:\n      base_trigger = self._NORMAL_TRIGGER\n    trigger = base_trigger * 2**_VerboseLevel()\n\n    if self.lines_in_function > trigger:\n      error_level = int(math.log(self.lines_in_function / base_trigger, 2))\n      # 50 => 0, 100 => 1, 200 => 2, 400 => 3, 800 => 4, 1600 => 5, ...\n      if error_level > 5:\n        error_level = 5\n      error(filename, linenum, 'readability/fn_size', error_level,\n            'Small and focused functions are preferred:'\n            ' %s has %d non-comment lines'\n            ' (error triggered by exceeding %d lines).'  % (\n                self.current_function, self.lines_in_function, trigger))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef RepositoryName(self):\n    fullname = self.FullName()\n\n    if os.path.exists(fullname):\n      project_dir = os.path.dirname(fullname)\n\n      if os.path.exists(os.path.join(project_dir, \".svn\")):\n        # If there's a .svn file in the current directory, we recursively look\n        # up the directory tree for the top of the SVN checkout\n        root_dir = project_dir\n        one_up_dir = os.path.dirname(root_dir)\n        while os.path.exists(os.path.join(one_up_dir, \".svn\")):\n          root_dir = os.path.dirname(root_dir)\n          one_up_dir = os.path.dirname(one_up_dir)\n\n        prefix = os.path.commonprefix([root_dir, project_dir])\n        return fullname[len(prefix) + 1:]\n\n      # Not SVN <= 1.6? Try to find a git, hg, or svn top level directory by\n      # searching up from the current path.\n      root_dir = os.path.dirname(fullname)\n      while (root_dir != os.path.dirname(root_dir) and\n             not os.path.exists(os.path.join(root_dir, \".git\")) and\n             not os.path.exists(os.path.join(root_dir, \".hg\")) and\n             not os.path.exists(os.path.join(root_dir, \".svn\"))):\n        root_dir = os.path.dirname(root_dir)\n\n      if (os.path.exists(os.path.join(root_dir, \".git\")) or\n          os.path.exists(os.path.join(root_dir, \".hg\")) or\n          os.path.exists(os.path.join(root_dir, \".svn\"))):\n        prefix = os.path.commonprefix([root_dir, project_dir])\n        return fullname[len(prefix) + 1:]\n\n    # Don't know what to do; header guard warnings may be wrong...\n    return fullname", "response": "Returns the full path name of the repository."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if the end of a namespace comment is correct.", "response": "def CheckEnd(self, filename, clean_lines, linenum, error):\n    \"\"\"Check end of namespace comments.\"\"\"\n    line = clean_lines.raw_lines[linenum]\n\n    # Check how many lines is enclosed in this namespace.  Don't issue\n    # warning for missing namespace comments if there aren't enough\n    # lines.  However, do apply checks if there is already an end of\n    # namespace comment and it's incorrect.\n    #\n    # TODO(unknown): We always want to check end of namespace comments\n    # if a namespace is large, but sometimes we also want to apply the\n    # check if a short namespace contained nontrivial things (something\n    # other than forward declarations).  There is currently no logic on\n    # deciding what these nontrivial things are, so this check is\n    # triggered by namespace size only, which works most of the time.\n    if (linenum - self.starting_linenum < 10\n        and not Match(r'};*\\s*(//|/\\*).*\\bnamespace\\b', line)):\n      return\n\n    # Look for matching comment at end of namespace.\n    #\n    # Note that we accept C style \"/* */\" comments for terminating\n    # namespaces, so that code that terminate namespaces inside\n    # preprocessor macros can be cpplint clean.\n    #\n    # We also accept stuff like \"// end of namespace <name>.\" with the\n    # period at the end.\n    #\n    # Besides these, we don't accept anything else, otherwise we might\n    # get false negatives when existing comment is a substring of the\n    # expected namespace.\n    if self.name:\n      # Named namespace\n      if not Match((r'};*\\s*(//|/\\*).*\\bnamespace\\s+' + re.escape(self.name) +\n                    r'[\\*/\\.\\\\\\s]*$'),\n                   line):\n        error(filename, linenum, 'readability/namespace', 5,\n              'Namespace should be terminated with \"// namespace %s\"' %\n              self.name)\n    else:\n      # Anonymous namespace\n      if not Match(r'};*\\s*(//|/\\*).*\\bnamespace[\\*/\\.\\\\\\s]*$', line):\n        # If \"// namespace anonymous\" or \"// anonymous namespace (more text)\",\n        # mention \"// anonymous namespace\" as an acceptable form\n        if Match(r'}.*\\b(namespace anonymous|anonymous namespace)\\b', line):\n          error(filename, linenum, 'readability/namespace', 5,\n                'Anonymous namespace should be terminated with \"// namespace\"'\n                ' or \"// anonymous namespace\"')\n        else:\n          error(filename, linenum, 'readability/namespace', 5,\n                'Anonymous namespace should be terminated with \"// namespace\"')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a deque containing the elements of iterable.", "response": "def pdeque(iterable=(), maxlen=None):\n    \"\"\"\n    Return deque containing the elements of iterable. If maxlen is specified then\n    len(iterable) - maxlen elements are discarded from the left to if len(iterable) > maxlen.\n\n    >>> pdeque([1, 2, 3])\n    pdeque([1, 2, 3])\n    >>> pdeque([1, 2, 3, 4], maxlen=2)\n    pdeque([3, 4], maxlen=2)\n    \"\"\"\n    t = tuple(iterable)\n    if maxlen is not None:\n        t = t[-maxlen:]\n    length = len(t)\n    pivot = int(length / 2)\n    left = plist(t[:pivot])\n    right = plist(t[pivot:], reverse=True)\n    return PDeque(left, right, length, maxlen)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns new deque with rightmost elements removed.", "response": "def pop(self, count=1):\n        \"\"\"\n        Return new deque with rightmost element removed. Popping the empty queue\n        will return the empty queue. A optional count can be given to indicate the\n        number of elements to pop. Popping with a negative index is the same as\n        popleft. Executes in amortized O(k) where k is the number of elements to pop.\n\n        >>> pdeque([1, 2]).pop()\n        pdeque([1])\n        >>> pdeque([1, 2]).pop(2)\n        pdeque([])\n        >>> pdeque([1, 2]).pop(-1)\n        pdeque([2])\n        \"\"\"\n        if count < 0:\n            return self.popleft(-count)\n\n        new_right_list, new_left_list = PDeque._pop_lists(self._right_list, self._left_list, count)\n        return PDeque(new_left_list, new_right_list, max(self._length - count, 0), self._maxlen)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn new deque with elem as the rightmost element.", "response": "def append(self, elem):\n        \"\"\"\n        Return new deque with elem as the rightmost element.\n\n        >>> pdeque([1, 2]).append(3)\n        pdeque([1, 2, 3])\n        \"\"\"\n        new_left_list, new_right_list, new_length = self._append(self._left_list, self._right_list, elem)\n        return PDeque(new_left_list, new_right_list, new_length, self._maxlen)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef extend(self, iterable):\n        new_right_list, new_left_list, extend_count = self._extend(self._right_list, self._left_list, iterable)\n        return PDeque(new_left_list, new_right_list, self._length + extend_count, self._maxlen)", "response": "Return new deque with all elements of iterable appended to the left deque."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the number of elements equal to elem in the queue.", "response": "def count(self, elem):\n        \"\"\"\n        Return the number of elements equal to elem present in the queue\n\n        >>> pdeque([1, 2, 1]).count(1)\n        2\n        \"\"\"\n        return self._left_list.count(elem) + self._right_list.count(elem)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn new deque with first element from left equal to elem removed.", "response": "def remove(self, elem):\n        \"\"\"\n        Return new deque with first element from left equal to elem removed. If no such element is found\n        a ValueError is raised.\n\n        >>> pdeque([2, 1, 2]).remove(2)\n        pdeque([1, 2])\n        \"\"\"\n        try:\n            return PDeque(self._left_list.remove(elem), self._right_list, self._length - 1)\n        except ValueError:\n            # Value not found in left list, try the right list\n            try:\n                # This is severely inefficient with a double reverse, should perhaps implement a remove_last()?\n                return PDeque(self._left_list,\n                               self._right_list.reverse().remove(elem).reverse(), self._length - 1)\n            except ValueError:\n                raise ValueError('{0} not found in PDeque'.format(elem))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn deque with elements rotated steps steps.", "response": "def rotate(self, steps):\n        \"\"\"\n        Return deque with elements rotated steps steps.\n\n        >>> x = pdeque([1, 2, 3])\n        >>> x.rotate(1)\n        pdeque([3, 1, 2])\n        >>> x.rotate(-2)\n        pdeque([3, 1, 2])\n        \"\"\"\n        popped_deque = self.pop(steps)\n        if steps >= 0:\n            return popped_deque.extendleft(islice(self.reverse(), steps))\n\n        return popped_deque.extend(islice(self, -steps))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set(self, key, val):\n        return self.evolver().set(key, val).persistent()", "response": "Return a new PMap with key and val inserted."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a new PMap with the items in Mappings maps inserted using update_fn.", "response": "def update_with(self, update_fn, *maps):\n        \"\"\"\n        Return a new PMap with the items in Mappings maps inserted. If the same key is present in multiple\n        maps the values will be merged using merge_fn going from left to right.\n\n        >>> from operator import add\n        >>> m1 = m(a=1, b=2)\n        >>> m1.update_with(add, m(a=2))\n        pmap({'a': 3, 'b': 2})\n\n        The reverse behaviour of the regular merge. Keep the leftmost element instead of the rightmost.\n\n        >>> m1 = m(a=1)\n        >>> m1.update_with(lambda l, r: l, m(a=2), {'a':3})\n        pmap({'a': 1})\n        \"\"\"\n        evolver = self.evolver()\n        for map in maps:\n            for key, value in map.items():\n                evolver.set(key, update_fn(evolver[key], value) if key in evolver else value)\n\n        return evolver.persistent()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rex(expr):\n    r = re.compile(expr)\n    return lambda key: isinstance(key, six.string_types) and r.match(key)", "response": "Returns a function that returns True if the key is a string and False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update(self, iterable):\n        if iterable:\n            return PBag(reduce(_add_to_counters, iterable, self._counts))\n\n        return self", "response": "Update the bag with all elements in iterable."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef remove(self, element):\n        if element not in self._counts:\n            raise KeyError(element)\n        elif self._counts[element] == 1:\n            newc = self._counts.remove(element)\n        else:\n            newc = self._counts.set(element, self._counts[element] - 1)\n        return PBag(newc)", "response": "Remove an element from the bag."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef freeze(o):\n    typ = type(o)\n    if typ is dict:\n        return pmap(dict((k, freeze(v)) for k, v in six.iteritems(o)))\n    if typ is list:\n        return pvector(map(freeze, o))\n    if typ is tuple:\n        return tuple(map(freeze, o))\n    if typ is set:\n        return pset(o)\n    return o", "response": "Recursively convert simple Python containers into pyrsistent versions\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mutant(fn):\n    @wraps(fn)\n    def inner_f(*args, **kwargs):\n        return freeze(fn(*[freeze(e) for e in args], **dict(freeze(item) for item in kwargs.items())))\n\n    return inner_f", "response": "A decorator to isolate mutation to within a function."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a persistent list containing all elements of iterable.", "response": "def plist(iterable=(), reverse=False):\n    \"\"\"\n    Creates a new persistent list containing all elements of iterable.\n    Optional parameter reverse specifies if the elements should be inserted in\n    reverse order or not.\n\n    >>> plist([1, 2, 3])\n    plist([1, 2, 3])\n    >>> plist([1, 2, 3], reverse=True)\n    plist([3, 2, 1])\n    \"\"\"\n    if not reverse:\n        iterable = list(iterable)\n        iterable.reverse()\n\n    return reduce(lambda pl, elem: pl.cons(elem), iterable, _EMPTY_PLIST)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a new list with all elements of iterable repeatedly consed to the current list.", "response": "def mcons(self, iterable):\n        \"\"\"\n        Return a new list with all elements of iterable repeatedly cons:ed to the current list.\n        NB! The elements will be inserted in the reverse order of the iterable.\n        Runs in O(len(iterable)).\n\n        >>> plist([1, 2]).mcons([3, 4])\n        plist([4, 3, 1, 2])\n        \"\"\"\n        head = self\n        for elem in iterable:\n            head = head.cons(elem)\n\n        return head"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reverse(self):\n        result = plist()\n        head = self\n        while head:\n            result = result.cons(head.first)\n            head = head.rest\n\n        return result", "response": "Return a reversed version of the list. Runs in O ( n ) where n is the length of the list."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsplits the list at the specified index. Returns a tuple containing the list up to and after the index. Returns an empty list if no elements are found.", "response": "def split(self, index):\n        \"\"\"\n        Spilt the list at position specified by index. Returns a tuple containing the\n        list up until index and the list after the index. Runs in O(index).\n\n        >>> plist([1, 2, 3, 4]).split(2)\n        (plist([1, 2]), plist([3, 4]))\n        \"\"\"\n        lb = _PListBuilder()\n        right_list = self\n        i = 0\n        while right_list and i < index:\n            lb.append_elem(right_list.first)\n            right_list = right_list.rest\n            i += 1\n\n        if not right_list:\n            # Just a small optimization in the cases where no split occurred\n            return self, _EMPTY_PLIST\n\n        return lb.build(), right_list"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove(self, elem):\n\n        builder = _PListBuilder()\n        head = self\n        while head:\n            if head.first == elem:\n                return builder.append_plist(head.rest)\n\n            builder.append_elem(head.first)\n            head = head.rest\n\n        raise ValueError('{0} not found in PList'.format(elem))", "response": "Return new list with first element equal to elem removed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a persistent set from iterable. Optionally takes a sizing parameter equivalent to that used for pmap.", "response": "def pset(iterable=(), pre_size=8):\n    \"\"\"\n    Creates a persistent set from iterable. Optionally takes a sizing parameter equivalent to that\n    used for :py:func:`pmap`.\n\n    >>> s1 = pset([1, 2, 3, 2])\n    >>> s1\n    pset([1, 2, 3])\n    \"\"\"\n    if not iterable:\n        return _EMPTY_PSET\n\n    return PSet._from_iterable(iterable, pre_size=pre_size)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update(self, iterable):\n        e = self.evolver()\n        for element in iterable:\n            e.add(element)\n\n        return e.persistent()", "response": "Return a new PSet with elements in iterable added\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a new PSet with element removed. Raises KeyError if element is not present in the PSet.", "response": "def remove(self, element):\n        \"\"\"\n        Return a new PSet with element removed. Raises KeyError if element is not present.\n\n        >>> s1 = s(1, 2)\n        >>> s1.remove(2)\n        pset([1])\n        \"\"\"\n        if element in self._map:\n            return self.evolver().remove(element).persistent()\n\n        raise KeyError(\"Element '%s' not present in PSet\" % element)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef discard(self, element):\n        if element in self._map:\n            return self.evolver().remove(element).persistent()\n\n        return self", "response": "Return a new PSet with element removed."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef maybe_parse_user_type(t):\n    is_type = isinstance(t, type)\n    is_preserved = isinstance(t, type) and issubclass(t, _preserved_iterable_types)\n    is_string = isinstance(t, string_types)\n    is_iterable = isinstance(t, Iterable)\n\n    if is_preserved:\n        return [t]\n    elif is_string:\n        return [t]\n    elif is_type and not is_iterable:\n        return [t]\n    elif is_iterable:\n        # Recur to validate contained types as well.\n        ts = t\n        return tuple(e for t in ts for e in maybe_parse_user_type(t))\n    else:\n        # If this raises because `t` cannot be formatted, so be it.\n        raise TypeError(\n            'Type specifications must be types or strings. Input: {}'.format(t)\n        )", "response": "Try to coerce a user - supplied type directive into a list of types."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _all_dicts(bases, seen=None):\n    if seen is None:\n        seen = set()\n    for cls in bases:\n        if cls in seen:\n            continue\n        seen.add(cls)\n        yield cls.__dict__\n        for b in _all_dicts(cls.__bases__, seen):\n            yield b", "response": "Yield all dictionaries of all base classes and their base classes."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef immutable(members='', name='Immutable', verbose=False):\n\n    if isinstance(members, six.string_types):\n        members = members.replace(',', ' ').split()\n\n    def frozen_member_test():\n        frozen_members = [\"'%s'\" % f for f in members if f.endswith('_')]\n        if frozen_members:\n            return \"\"\"\n        frozen_fields = fields_to_modify & set([{frozen_members}])\n        if frozen_fields:\n            raise AttributeError('Cannot set frozen members %s' % ', '.join(frozen_fields))\n            \"\"\".format(frozen_members=', '.join(frozen_members))\n\n        return ''\n\n    verbose_string = \"\"\n    if sys.version_info < (3, 7):\n        # Verbose is no longer supported in Python 3.7\n        verbose_string = \", verbose={verbose}\".format(verbose=verbose)\n\n    quoted_members = ', '.join(\"'%s'\" % m for m in members)\n    template = \"\"\"\nclass {class_name}(namedtuple('ImmutableBase', [{quoted_members}]{verbose_string})):\n    __slots__ = tuple()\n\n    def __repr__(self):\n        return super({class_name}, self).__repr__().replace('ImmutableBase', self.__class__.__name__)\n\n    def set(self, **kwargs):\n        if not kwargs:\n            return self\n\n        fields_to_modify = set(kwargs.keys())\n        if not fields_to_modify <= {member_set}:\n            raise AttributeError(\"'%s' is not a member\" % ', '.join(fields_to_modify - {member_set}))\n\n        {frozen_member_test}\n\n        return self.__class__.__new__(self.__class__, *map(kwargs.pop, [{quoted_members}], self))\n\"\"\".format(quoted_members=quoted_members,\n               member_set=\"set([%s])\" % quoted_members if quoted_members else 'set()',\n               frozen_member_test=frozen_member_test(),\n               verbose_string=verbose_string,\n               class_name=name)\n\n    if verbose:\n        print(template)\n\n    from collections import namedtuple\n    namespace = dict(namedtuple=namedtuple, __name__='pyrsistent_immutable')\n    try:\n        six.exec_(template, namespace)\n    except SyntaxError as e:\n        raise SyntaxError(e.message + ':\\n' + template)\n\n    return namespace[name]", "response": "Returns a new immutable object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting in - place nested data structures.", "response": "def get_in(keys, coll, default=None, no_default=False):\n    \"\"\"\n    NB: This is a straight copy of the get_in implementation found in\n        the toolz library (https://github.com/pytoolz/toolz/). It works\n        with persistent data structures as well as the corresponding\n        datastructures from the stdlib.\n\n    Returns coll[i0][i1]...[iX] where [i0, i1, ..., iX]==keys.\n\n    If coll[i0][i1]...[iX] cannot be found, returns ``default``, unless\n    ``no_default`` is specified, then it raises KeyError or IndexError.\n\n    ``get_in`` is a generalization of ``operator.getitem`` for nested data\n    structures such as dictionaries and lists.\n    >>> from pyrsistent import freeze\n    >>> transaction = freeze({'name': 'Alice',\n    ...                       'purchase': {'items': ['Apple', 'Orange'],\n    ...                                    'costs': [0.50, 1.25]},\n    ...                       'credit card': '5555-1234-1234-1234'})\n    >>> get_in(['purchase', 'items', 0], transaction)\n    'Apple'\n    >>> get_in(['name'], transaction)\n    'Alice'\n    >>> get_in(['purchase', 'total'], transaction)\n    >>> get_in(['purchase', 'items', 'apple'], transaction)\n    >>> get_in(['purchase', 'items', 10], transaction)\n    >>> get_in(['purchase', 'total'], transaction, 0)\n    0\n    >>> get_in(['y'], {}, no_default=True)\n    Traceback (most recent call last):\n        ...\n    KeyError: 'y'\n    \"\"\"\n    try:\n        return reduce(operator.getitem, keys, coll)\n    except (KeyError, IndexError, TypeError):\n        if no_default:\n            raise\n        return default"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of the entries in the current set.", "response": "def tolist(self):\n        \"\"\"\n        The fastest way to convert the vector into a python list.\n        \"\"\"\n        the_list = []\n        self._fill_list(self._root, self._shift, the_list)\n        the_list.extend(self._tail)\n        return the_list"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a new field specification for the given type and optional initial value.", "response": "def field(type=PFIELD_NO_TYPE, invariant=PFIELD_NO_INVARIANT, initial=PFIELD_NO_INITIAL,\n          mandatory=False, factory=PFIELD_NO_FACTORY, serializer=PFIELD_NO_SERIALIZER):\n    \"\"\"\n    Field specification factory for :py:class:`PRecord`.\n\n    :param type: a type or iterable with types that are allowed for this field\n    :param invariant: a function specifying an invariant that must hold for the field\n    :param initial: value of field if not specified when instantiating the record\n    :param mandatory: boolean specifying if the field is mandatory or not\n    :param factory: function called when field is set.\n    :param serializer: function that returns a serialized version of the field\n    \"\"\"\n\n    # NB: We have to check this predicate separately from the predicates in\n    # `maybe_parse_user_type` et al. because this one is related to supporting\n    # the argspec for `field`, while those are related to supporting the valid\n    # ways to specify types.\n\n    # Multiple types must be passed in one of the following containers. Note\n    # that a type that is a subclass of one of these containers, like a\n    # `collections.namedtuple`, will work as expected, since we check\n    # `isinstance` and not `issubclass`.\n    if isinstance(type, (list, set, tuple)):\n        types = set(maybe_parse_many_user_types(type))\n    else:\n        types = set(maybe_parse_user_type(type))\n\n    invariant_function = wrap_invariant(invariant) if invariant != PFIELD_NO_INVARIANT and callable(invariant) else invariant\n    field = _PField(type=types, invariant=invariant_function, initial=initial,\n                    mandatory=mandatory, factory=factory, serializer=serializer)\n\n    _check_field_parameters(field)\n\n    return field"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _make_seq_field_type(checked_class, item_type):\n    type_ = _seq_field_types.get((checked_class, item_type))\n    if type_ is not None:\n        return type_\n\n    class TheType(checked_class):\n        __type__ = item_type\n\n        def __reduce__(self):\n            return (_restore_seq_field_pickle,\n                    (checked_class, item_type, list(self)))\n\n    suffix = SEQ_FIELD_TYPE_SUFFIXES[checked_class]\n    TheType.__name__ = _types_to_names(TheType._checked_types) + suffix\n    _seq_field_types[checked_class, item_type] = TheType\n    return TheType", "response": "Create a subclass of the given checked class with the given item type."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _sequence_field(checked_class, item_type, optional, initial):\n    TheType = _make_seq_field_type(checked_class, item_type)\n\n    if optional:\n        def factory(argument):\n            if argument is None:\n                return None\n            else:\n                return TheType.create(argument)\n    else:\n        factory = TheType.create\n\n    return field(type=optional_type(TheType) if optional else TheType,\n                 factory=factory, mandatory=True,\n                 initial=factory(initial))", "response": "Create a checked field for either PSet or PVector."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a checked PSet field.", "response": "def pset_field(item_type, optional=False, initial=()):\n    \"\"\"\n    Create checked ``PSet`` field.\n\n    :param item_type: The required type for the items in the set.\n    :param optional: If true, ``None`` can be used as a value for\n        this field.\n    :param initial: Initial value to pass to factory if no value is given\n        for the field.\n\n    :return: A ``field`` containing a ``CheckedPSet`` of the given type.\n    \"\"\"\n    return _sequence_field(CheckedPSet, item_type, optional,\n                           initial)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a checked PVector field.", "response": "def pvector_field(item_type, optional=False, initial=()):\n    \"\"\"\n    Create checked ``PVector`` field.\n\n    :param item_type: The required type for the items in the vector.\n    :param optional: If true, ``None`` can be used as a value for\n        this field.\n    :param initial: Initial value to pass to factory if no value is given\n        for the field.\n\n    :return: A ``field`` containing a ``CheckedPVector`` of the given type.\n    \"\"\"\n    return _sequence_field(CheckedPVector, item_type, optional,\n                           initial)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _restore_pmap_field_pickle(key_type, value_type, data):\n    type_ = _pmap_field_types[key_type, value_type]\n    return _restore_pickle(type_, data)", "response": "Unpickling function for auto - generated PMap field types."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _make_pmap_field_type(key_type, value_type):\n    type_ = _pmap_field_types.get((key_type, value_type))\n    if type_ is not None:\n        return type_\n\n    class TheMap(CheckedPMap):\n        __key_type__ = key_type\n        __value_type__ = value_type\n\n        def __reduce__(self):\n            return (_restore_pmap_field_pickle,\n                    (self.__key_type__, self.__value_type__, dict(self)))\n\n    TheMap.__name__ = \"{0}To{1}PMap\".format(\n        _types_to_names(TheMap._checked_key_types),\n        _types_to_names(TheMap._checked_value_types))\n    _pmap_field_types[key_type, value_type] = TheMap\n    return TheMap", "response": "Create a subclass of CheckedPMap with the given key and value types."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pmap_field(key_type, value_type, optional=False, invariant=PFIELD_NO_INVARIANT):\n    TheMap = _make_pmap_field_type(key_type, value_type)\n\n    if optional:\n        def factory(argument):\n            if argument is None:\n                return None\n            else:\n                return TheMap.create(argument)\n    else:\n        factory = TheMap.create\n\n    return field(mandatory=True, initial=TheMap(),\n                 type=optional_type(TheMap) if optional else TheMap,\n                 factory=factory, invariant=invariant)", "response": "Create a checked PMap field."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntrap *X* errors and raises an :class:``X11Error`` at the end if any error occurred. This handler also ensures that the :class:`Xlib.display.Display` being managed is sync'd. :param Xlib.display.Display display: The *X* display. :return: the display :rtype: Xlib.display.Display", "response": "def display_manager(display):\n    \"\"\"Traps *X* errors and raises an :class:``X11Error`` at the end if any\n    error occurred.\n\n    This handler also ensures that the :class:`Xlib.display.Display` being\n    managed is sync'd.\n\n    :param Xlib.display.Display display: The *X* display.\n\n    :return: the display\n    :rtype: Xlib.display.Display\n    \"\"\"\n    from contextlib import contextmanager\n\n    @contextmanager\n    def manager():\n        errors = []\n\n        def handler(*args):\n            errors.append(args)\n\n        old_handler = display.set_error_handler(handler)\n        yield display\n        display.sync()\n        display.set_error_handler(old_handler)\n        if errors:\n            raise X11Error(errors)\n\n    return manager()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npressing and release a given character key n times.", "response": "def tap_key(self, character='', n=1, interval=0):\n        \"\"\"Press and release a given character key n times.\"\"\"\n        for i in range(n):\n            self.press_key(character)\n            self.release_key(character)\n            time.sleep(interval)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npresses a given character key.", "response": "def press_keys(self,characters=[]):\n        \"\"\"Press a given character key.\"\"\"\n        for character in characters:\n            self.press_key(character)\n        for character in characters:\n            self.release_key(character)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates a new object for typing longer strings of characters.", "response": "def type_string(self, char_string, interval=0):\n        \"\"\"\n        A convenience method for typing longer strings of characters. Generates\n        as few Shift events as possible.\"\"\"\n        shift = False\n        for char in char_string:\n            if self.is_char_shifted(char):\n                if not shift:  # Only press Shift as needed\n                    time.sleep(interval)\n                    self.press_key(self.shift_key)\n                    shift = True\n                #In order to avoid tap_key pressing Shift, we need to pass the\n                #unshifted form of the character\n                if char in '<>?:\"{}|~!@#$%^&*()_+':\n                    ch_index = '<>?:\"{}|~!@#$%^&*()_+'.index(char)\n                    unshifted_char = \",./;'[]\\\\`1234567890-=\"[ch_index]\n                else:\n                    unshifted_char = char.lower()\n                time.sleep(interval)\n                self.tap_key(unshifted_char)\n            else:  # Unshifted already\n                if shift and char != ' ':  # Only release Shift as needed\n                    self.release_key(self.shift_key)\n                    shift = False\n                time.sleep(interval)\n                self.tap_key(char)\n\n        if shift:  # Turn off Shift if it's still ON\n            self.release_key(self.shift_key)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nyields all keysym definitions parsed as tuples.", "response": "def keysym_definitions():\n    \"\"\"Yields all keysym definitions parsed as tuples.\n    \"\"\"\n    for keysym_line in keysym_lines():\n        # As described in the input text, the format of a line is:\n        # 0x20 U0020 . # space /* optional comment */\n        keysym_number, codepoint, status, _, name_part = [\n            p.strip() for p in keysym_line.split(None, 4)]\n        name = name_part.split()[0]\n\n        yield (int(keysym_number, 16), codepoint[1:], status, name)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nyielding the tuple of character symbol names for all keysyms.", "response": "def keysyms_from_strings():\n    \"\"\"Yields the tuple ``(character, symbol name)`` for all keysyms.\n    \"\"\"\n    for number, codepoint, status, name in keysym_definitions():\n        # Ignore keysyms that do not map to unicode characters\n        if all(c == '0' for c in codepoint):\n            continue\n\n        # Ignore keysyms that are not well established\n        if status != '.':\n            continue\n\n        yield (codepoint, name)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npressing a given character key.", "response": "def press_key(self, character=''):\n        \"\"\"\n        Press a given character key.\n        \"\"\"\n        try:\n            shifted = self.is_char_shifted(character)\n        except AttributeError:\n            win32api.keybd_event(character, 0, 0, 0)\n        else:\n            if shifted:\n                win32api.keybd_event(self.shift_key, 0, 0, 0)\n            char_vk = win32api.VkKeyScan(character)\n            win32api.keybd_event(char_vk, 0, 0, 0)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrelease a given character key.", "response": "def release_key(self, character=''):\n        \"\"\"\n        Release a given character key.\n        \"\"\"\n        try:\n            shifted = self.is_char_shifted(character)\n        except AttributeError:\n            win32api.keybd_event(character, 0, KEYEVENTF_KEYUP, 0)\n        else:\n            if shifted:\n                win32api.keybd_event(self.shift_key, 0, KEYEVENTF_KEYUP, 0)\n            char_vk = win32api.VkKeyScan(character)\n            win32api.keybd_event(char_vk, 0, KEYEVENTF_KEYUP, 0)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nstart listening for keyboard input events.", "response": "def run(self):\n        \"\"\"Begin listening for keyboard input events.\"\"\"\n        self.state = True\n        self.hm.KeyAll = self.handler\n        self.hm.HookKeyboard()\n        while self.state:\n            time.sleep(0.01)\n            pythoncom.PumpWaitingMessages()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _diagnostic(self, event):\n        print('\\n---Keyboard Event Diagnostic---')\n        print('MessageName:', event.MessageName)\n        print('Message:', event.Message)\n        print('Time:', event.Time)\n        print('Window:', event.Window)\n        print('WindowName:', event.WindowName)\n        print('Ascii:', event.Ascii, ',', chr(event.Ascii))\n        print('Key:', event.Key)\n        print('KeyID:', event.KeyID)\n        print('ScanCode:', event.ScanCode)\n        print('Extended:', event.Extended)\n        print('Injected:', event.Injected)\n        print('Alt', event.Alt)\n        print('Transition', event.Transition)\n        print('---')", "response": "Print out diagnostic information about the event."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _handle_key(self, character, event):\n        try:\n            # Detect uppercase or shifted character\n            shifted = self.is_char_shifted(character)\n        except AttributeError:\n            # Handle the case of integer keycode argument\n            with display_manager(self.display) as d:\n                fake_input(d, event, character)\n        else:\n            with display_manager(self.display) as d:\n                if shifted:\n                    fake_input(d, event, self.shift_key)\n                keycode = self.lookup_character_keycode(character)\n                fake_input(d, event, keycode)", "response": "Handles either a key press or release depending on event."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef special_key_assignment(self):\n        #This set of keys compiled using the X11 keysymdef.h file as reference\n        #They comprise a relatively universal set of keys, though there may be\n        #exceptions which may come up for other OSes and vendors. Countless\n        #special cases exist which are not handled here, but may be extended.\n        #TTY Function Keys\n        self.backspace_key = self.lookup_character_keycode('BackSpace')\n        self.tab_key = self.lookup_character_keycode('Tab')\n        self.linefeed_key = self.lookup_character_keycode('Linefeed')\n        self.clear_key = self.lookup_character_keycode('Clear')\n        self.return_key = self.lookup_character_keycode('Return')\n        self.enter_key = self.return_key  # Because many keyboards call it \"Enter\"\n        self.pause_key = self.lookup_character_keycode('Pause')\n        self.scroll_lock_key = self.lookup_character_keycode('Scroll_Lock')\n        self.sys_req_key = self.lookup_character_keycode('Sys_Req')\n        self.escape_key = self.lookup_character_keycode('Escape')\n        self.delete_key = self.lookup_character_keycode('Delete')\n        #Modifier Keys\n        self.shift_l_key = self.lookup_character_keycode('Shift_L')\n        self.shift_r_key = self.lookup_character_keycode('Shift_R')\n        self.shift_key = self.shift_l_key  # Default Shift is left Shift\n        self.alt_l_key = self.lookup_character_keycode('Alt_L')\n        self.alt_r_key = self.lookup_character_keycode('Alt_R')\n        self.altgr_key = self.lookup_character_keycode('ISO_Level3_Shift')\n        self.alt_key = self.alt_l_key  # Default Alt is left Alt\n        self.control_l_key = self.lookup_character_keycode('Control_L')\n        self.control_r_key = self.lookup_character_keycode('Control_R')\n        self.control_key = self.control_l_key  # Default Ctrl is left Ctrl\n        self.caps_lock_key = self.lookup_character_keycode('Caps_Lock')\n        self.capital_key = self.caps_lock_key  # Some may know it as Capital\n        self.shift_lock_key = self.lookup_character_keycode('Shift_Lock')\n        self.meta_l_key = self.lookup_character_keycode('Meta_L')\n        self.meta_r_key = self.lookup_character_keycode('Meta_R')\n        self.super_l_key = self.lookup_character_keycode('Super_L')\n        self.windows_l_key = self.super_l_key  # Cross-support; also it's printed there\n        self.super_r_key = self.lookup_character_keycode('Super_R')\n        self.windows_r_key = self.super_r_key  # Cross-support; also it's printed there\n        self.hyper_l_key = self.lookup_character_keycode('Hyper_L')\n        self.hyper_r_key = self.lookup_character_keycode('Hyper_R')\n        #Cursor Control and Motion\n        self.home_key = self.lookup_character_keycode('Home')\n        self.up_key = self.lookup_character_keycode('Up')\n        self.down_key = self.lookup_character_keycode('Down')\n        self.left_key = self.lookup_character_keycode('Left')\n        self.right_key = self.lookup_character_keycode('Right')\n        self.end_key = self.lookup_character_keycode('End')\n        self.begin_key = self.lookup_character_keycode('Begin')\n        self.page_up_key = self.lookup_character_keycode('Page_Up')\n        self.page_down_key = self.lookup_character_keycode('Page_Down')\n        self.prior_key = self.lookup_character_keycode('Prior')\n        self.next_key = self.lookup_character_keycode('Next')\n        #Misc Functions\n        self.select_key = self.lookup_character_keycode('Select')\n        self.print_key = self.lookup_character_keycode('Print')\n        self.print_screen_key = self.print_key  # Seems to be the same thing\n        self.snapshot_key = self.print_key  # Another name for printscreen\n        self.execute_key = self.lookup_character_keycode('Execute')\n        self.insert_key = self.lookup_character_keycode('Insert')\n        self.undo_key = self.lookup_character_keycode('Undo')\n        self.redo_key = self.lookup_character_keycode('Redo')\n        self.menu_key = self.lookup_character_keycode('Menu')\n        self.apps_key = self.menu_key  # Windows...\n        self.find_key = self.lookup_character_keycode('Find')\n        self.cancel_key = self.lookup_character_keycode('Cancel')\n        self.help_key = self.lookup_character_keycode('Help')\n        self.break_key = self.lookup_character_keycode('Break')\n        self.mode_switch_key = self.lookup_character_keycode('Mode_switch')\n        self.script_switch_key = self.lookup_character_keycode('script_switch')\n        self.num_lock_key = self.lookup_character_keycode('Num_Lock')\n        #Keypad Keys: Dictionary structure\n        keypad = ['Space', 'Tab', 'Enter', 'F1', 'F2', 'F3', 'F4', 'Home',\n                  'Left', 'Up', 'Right', 'Down', 'Prior', 'Page_Up', 'Next',\n                  'Page_Down', 'End', 'Begin', 'Insert', 'Delete', 'Equal',\n                  'Multiply', 'Add', 'Separator', 'Subtract', 'Decimal',\n                  'Divide', 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n        self.keypad_keys = dict((k, self.lookup_character_keycode('KP_'+str(k))) for k in keypad)\n        self.numpad_keys = self.keypad_keys\n        #Function Keys/ Auxilliary Keys\n        #FKeys\n        self.function_keys = [None] + [self.lookup_character_keycode('F'+str(i)) for i in range(1,36)]\n        #LKeys\n        self.l_keys = [None] + [self.lookup_character_keycode('L'+str(i)) for i in range(1,11)]\n        #RKeys\n        self.r_keys = [None] + [self.lookup_character_keycode('R'+str(i)) for i in range(1,16)]\n\n        #Unsupported keys from windows\n        self.kana_key = None\n        self.hangeul_key = None # old name - should be here for compatibility\n        self.hangul_key = None\n        self.junjua_key = None\n        self.final_key = None\n        self.hanja_key = None\n        self.kanji_key = None\n        self.convert_key = None\n        self.nonconvert_key = None\n        self.accept_key = None\n        self.modechange_key = None\n        self.sleep_key = None", "response": "This method is used to assign special key codes to the current keyboard."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef lookup_character_keycode(self, character):\n        keysym = Xlib.XK.string_to_keysym(character)\n        if not keysym:\n            try:\n                keysym = getattr(Xlib.keysymdef.xkb, 'XK_' + character, 0)\n            except:\n                keysym = 0\n        if not keysym:\n            keysym = Xlib.XK.string_to_keysym(KEYSYMS[character])\n        return self.display.keysym_to_keycode(keysym)", "response": "Look up the keysym for the character then returns the keycode mapping\n        for that keysym."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbegins listening for keyboard input events.", "response": "def run(self):\n        \"\"\"Begin listening for keyboard input events.\"\"\"\n        self.state = True\n        if self.capture:\n            self.display2.screen().root.grab_keyboard(X.KeyPressMask | X.KeyReleaseMask, X.GrabModeAsync, X.GrabModeAsync, X.CurrentTime)\n\n        self.display2.record_enable_context(self.ctx, self.handler)\n        self.display2.record_free_context(self.ctx)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstop listening for keyboard input events.", "response": "def stop(self):\n        \"\"\"Stop listening for keyboard input events.\"\"\"\n        self.state = False\n        with display_manager(self.display) as d:\n            d.record_disable_context(self.ctx)\n            d.ungrab_keyboard(X.CurrentTime)\n        with display_manager(self.display2):\n            d.record_disable_context(self.ctx)\n            d.ungrab_keyboard(X.CurrentTime)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef lookup_character_keycode(self, character):\n        keysym = self.string_to_keysym.get(character, 0)\n        if keysym == 0:\n            keysym = self.string_to_keysym.get(KEYSYMS[character], 0)\n        return self.display.keysym_to_keycode(keysym)", "response": "Look up the keysym for the character then returns the keycode mapping\n        for that keysym."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of dictionaries for the translation of keysyms to strings and from strings to keysyms.", "response": "def get_translation_dicts(self):\n        \"\"\"\n        Returns dictionaries for the translation of keysyms to strings and from\n        strings to keysyms.\n        \"\"\"\n        keysym_to_string_dict = {}\n        string_to_keysym_dict = {}\n        #XK loads latin1 and miscellany on its own; load latin2-4 and greek\n        Xlib.XK.load_keysym_group('latin2')\n        Xlib.XK.load_keysym_group('latin3')\n        Xlib.XK.load_keysym_group('latin4')\n        Xlib.XK.load_keysym_group('greek')\n\n        #Make a standard dict and the inverted dict\n        for string, keysym in Xlib.XK.__dict__.items():\n            if string.startswith('XK_'):\n                string_to_keysym_dict[string[3:]] = keysym\n                keysym_to_string_dict[keysym] = string[3:]\n        return keysym_to_string_dict, string_to_keysym_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns True if the keysym is printable False otherwise.", "response": "def ascii_printable(self, keysym):\n        \"\"\"\n        If the keysym corresponds to a non-printable ascii character this will\n        return False. If it is printable, then True will be returned.\n\n        ascii 11 (vertical tab) and ascii 12 are printable, chr(11) and chr(12)\n        will return '\\x0b' and '\\x0c' respectively.\n        \"\"\"\n        if 0 <= keysym < 9:\n            return False\n        elif 13 < keysym < 32:\n            return False\n        elif keysym > 126:\n            return False\n        else:\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef click(self, x, y, button, press):\n        '''Print Fibonacci numbers when the left click is pressed.'''\n        if button == 1:\n            if press:\n                print(self.fibo.next())\n        else:  # Exit if any other mouse button used\n            self.stop()", "response": "Print Fibonacci numbers when the left click is pressed."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nclicks a mouse button n times on a given x y.", "response": "def click(self, x, y, button=1, n=1):\n        \"\"\"\n        Click a mouse button n times on a given x, y.\n        Button is defined as 1 = left, 2 = right, 3 = middle.\n        \"\"\"\n\n        for i in range(n):\n            self.press(x, y, button)\n            self.release(x, y, button)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pressAndHold(*args):\n    '''\n    press and hold. Do NOT release.\n    accepts as many arguments as you want.\n    e.g. pressAndHold('left_arrow', 'a','b').\n    '''\n    for i in args:\n        win32api.keybd_event(VK_CODE[i], 0,0,0)\n        time.sleep(.05)", "response": "Press and hold. Do NOT release."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\npress and hold passed in strings. Once held, release accepts as many arguments as you want. e.g. pressAndHold('left_arrow', 'a','b'). this is useful for issuing shortcut command or shift commands. e.g. pressHoldRelease('ctrl', 'alt', 'del'), pressHoldRelease('shift','a')", "response": "def pressHoldRelease(*args):\n    '''\n    press and hold passed in strings. Once held, release\n    accepts as many arguments as you want.\n    e.g. pressAndHold('left_arrow', 'a','b').\n\n    this is useful for issuing shortcut command or shift commands.\n    e.g. pressHoldRelease('ctrl', 'alt', 'del'), pressHoldRelease('shift','a')\n    '''\n    for i in args:\n        win32api.keybd_event(VK_CODE[i], 0,0,0)\n        time.sleep(.05)\n            \n    for i in args:\n            win32api.keybd_event(VK_CODE[i],0 ,win32con.KEYEVENTF_KEYUP ,0)\n            time.sleep(.1)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef release(*args):\n    '''\n    release depressed keys\n    accepts as many arguments as you want.\n    e.g. release('left_arrow', 'a','b').\n    '''\n    for i in args:\n           win32api.keybd_event(VK_CODE[i],0 ,win32con.KEYEVENTF_KEYUP ,0)", "response": "Release the specified keys in a new order."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute Dynamic Time Warping path for two sequences.", "response": "def dtw(x, y, dist, warp=1, w=inf, s=1.0):\n    \"\"\"\n    Computes Dynamic Time Warping (DTW) of two sequences.\n\n    :param array x: N1*M array\n    :param array y: N2*M array\n    :param func dist: distance used as cost measure\n    :param int warp: how many shifts are computed.\n    :param int w: window size limiting the maximal distance between indices of matched entries |i,j|.\n    :param float s: weight applied on off-diagonal moves of the path. As s gets larger, the warping path is increasingly biased towards the diagonal\n    Returns the minimum distance, the cost matrix, the accumulated cost matrix, and the wrap path.\n    \"\"\"\n    assert len(x)\n    assert len(y)\n    assert isinf(w) or (w >= abs(len(x) - len(y)))\n    assert s > 0\n    r, c = len(x), len(y)\n    if not isinf(w):\n        D0 = full((r + 1, c + 1), inf)\n        for i in range(1, r + 1):\n            D0[i, max(1, i - w):min(c + 1, i + w + 1)] = 0\n        D0[0, 0] = 0\n    else:\n        D0 = zeros((r + 1, c + 1))\n        D0[0, 1:] = inf\n        D0[1:, 0] = inf\n    D1 = D0[1:, 1:]  # view\n    for i in range(r):\n        for j in range(c):\n            if (isinf(w) or (max(0, i - w) <= j <= min(c, i + w))):\n                D1[i, j] = dist(x[i], y[j])\n    C = D1.copy()\n    jrange = range(c)\n    for i in range(r):\n        if not isinf(w):\n            jrange = range(max(0, i - w), min(c, i + w + 1))\n        for j in jrange:\n            min_list = [D0[i, j]]\n            for k in range(1, warp + 1):\n                i_k = min(i + k, r)\n                j_k = min(j + k, c)\n                min_list += [D0[i_k, j] * s, D0[i, j_k] * s]\n            D1[i, j] += min(min_list)\n    if len(x) == 1:\n        path = zeros(len(y)), range(len(y))\n    elif len(y) == 1:\n        path = range(len(x)), zeros(len(x))\n    else:\n        path = _traceback(D0)\n    return D1[-1, -1] / sum(D1.shape), C, D1, path"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef accelerated_dtw(x, y, dist, warp=1):\n    assert len(x)\n    assert len(y)\n    if ndim(x) == 1:\n        x = x.reshape(-1, 1)\n    if ndim(y) == 1:\n        y = y.reshape(-1, 1)\n    r, c = len(x), len(y)\n    D0 = zeros((r + 1, c + 1))\n    D0[0, 1:] = inf\n    D0[1:, 0] = inf\n    D1 = D0[1:, 1:]\n    D0[1:, 1:] = cdist(x, y, dist)\n    C = D1.copy()\n    for i in range(r):\n        for j in range(c):\n            min_list = [D0[i, j]]\n            for k in range(1, warp + 1):\n                min_list += [D0[min(i + k, r), j],\n                             D0[i, min(j + k, c)]]\n            D1[i, j] += min(min_list)\n    if len(x) == 1:\n        path = zeros(len(y)), range(len(y))\n    elif len(y) == 1:\n        path = range(len(x)), zeros(len(x))\n    else:\n        path = _traceback(D0)\n    return D1[-1, -1] / sum(D1.shape), C, D1, path", "response": "Compute Dynamic Time Warping of two sequences."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef siblings(self):\n        parent = self.parent\n        if parent is None:\n            return tuple()\n        else:\n            return tuple([node for node in parent.children if node != self])", "response": "Tuple of nodes with the same parent."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef height(self):\n        if self.__children_:\n            return max([child.height for child in self.__children_]) + 1\n        else:\n            return 0", "response": "Returns the number of edges on the longest path to a leaf Node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef import_(self, data):\n        return self.__import(json.loads(data, **self.kwargs))", "response": "Read JSON from data."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read(self, filehandle):\n        return self.__import(json.load(filehandle, **self.kwargs))", "response": "Read JSON from filehandle."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwalks from start to end and return a list of the top level nodes that are at least common to the root node.", "response": "def walk(self, start, end):\n        \"\"\"\n        Walk from `start` node to `end` node.\n\n        Returns:\n            (upwards, common, downwards): `upwards` is a list of nodes to go upward to.\n            `common` top node. `downwards` is a list of nodes to go downward to.\n\n        Raises:\n            WalkError: on no common root node.\n\n        >>> from anytree import Node, RenderTree, AsciiStyle\n        >>> f = Node(\"f\")\n        >>> b = Node(\"b\", parent=f)\n        >>> a = Node(\"a\", parent=b)\n        >>> d = Node(\"d\", parent=b)\n        >>> c = Node(\"c\", parent=d)\n        >>> e = Node(\"e\", parent=d)\n        >>> g = Node(\"g\", parent=f)\n        >>> i = Node(\"i\", parent=g)\n        >>> h = Node(\"h\", parent=i)\n        >>> print(RenderTree(f, style=AsciiStyle()))\n        Node('/f')\n        |-- Node('/f/b')\n        |   |-- Node('/f/b/a')\n        |   +-- Node('/f/b/d')\n        |       |-- Node('/f/b/d/c')\n        |       +-- Node('/f/b/d/e')\n        +-- Node('/f/g')\n            +-- Node('/f/g/i')\n                +-- Node('/f/g/i/h')\n\n        Create a walker:\n\n        >>> w = Walker()\n\n        This class is made for walking:\n\n        >>> w.walk(f, f)\n        ((), Node('/f'), ())\n        >>> w.walk(f, b)\n        ((), Node('/f'), (Node('/f/b'),))\n        >>> w.walk(b, f)\n        ((Node('/f/b'),), Node('/f'), ())\n        >>> w.walk(h, e)\n        ((Node('/f/g/i/h'), Node('/f/g/i'), Node('/f/g')), Node('/f'), (Node('/f/b'), Node('/f/b/d'), Node('/f/b/d/e')))\n        >>> w.walk(d, e)\n        ((), Node('/f/b/d'), (Node('/f/b/d/e'),))\n\n        For a proper walking the nodes need to be part of the same tree:\n\n        >>> w.walk(Node(\"a\"), Node(\"b\"))\n        Traceback (most recent call last):\n          ...\n        anytree.walker.WalkError: Node('/a') and Node('/b') are not part of the same tree.\n        \"\"\"\n        s = start.path\n        e = end.path\n        if start.root != end.root:\n            msg = \"%r and %r are not part of the same tree.\" % (start, end)\n            raise WalkError(msg)\n        # common\n        c = Walker.__calc_common(s, e)\n        assert c[0] is start.root\n        len_c = len(c)\n        # up\n        if start is c[-1]:\n            up = tuple()\n        else:\n            up = tuple(reversed(s[len_c:]))\n        # down\n        if end is c[-1]:\n            down = tuple()\n        else:\n            down = e[len_c:]\n        return up, c[-1], down"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning JSON for tree starting at node.", "response": "def export(self, node):\n        \"\"\"Return JSON for tree starting at `node`.\"\"\"\n        dictexporter = self.dictexporter or DictExporter()\n        data = dictexporter.export(node)\n        return json.dumps(data, **self.kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write(self, node, filehandle):\n        dictexporter = self.dictexporter or DictExporter()\n        data = dictexporter.export(node)\n        return json.dump(data, filehandle, **self.kwargs)", "response": "Write JSON to filehandle starting at node."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef findall(node, filter_=None, stop=None, maxlevel=None, mincount=None, maxcount=None):\n    return _findall(node, filter_=filter_, stop=stop,\n                    maxlevel=maxlevel, mincount=mincount, maxcount=maxcount)", "response": "Find all nodes matching filter_ but stop at maxlevel or stop at maxcount or maxcount."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef findall_by_attr(node, value, name=\"name\", maxlevel=None, mincount=None, maxcount=None):\n    return _findall(node, filter_=lambda n: _filter_by_name(n, name, value),\n                    maxlevel=maxlevel, mincount=mincount, maxcount=maxcount)", "response": "Find nodes with attribute name having value but stop at maxlevel."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfind a single node matching filter_ but stop at maxlevel or stop at maxlevel or stop at maxlevel.", "response": "def find(node, filter_=None, stop=None, maxlevel=None):\n    \"\"\"\n    Search for *single* node matching `filter_` but stop at `maxlevel` or `stop`.\n\n    Return matching node.\n\n    Args:\n        node: top node, start searching.\n\n    Keyword Args:\n        filter_: function called with every `node` as argument, `node` is returned if `True`.\n        stop: stop iteration at `node` if `stop` function returns `True` for `node`.\n        maxlevel (int): maximum decending in the node hierarchy.\n\n    Example tree:\n\n    >>> from anytree import Node, RenderTree, AsciiStyle\n    >>> f = Node(\"f\")\n    >>> b = Node(\"b\", parent=f)\n    >>> a = Node(\"a\", parent=b)\n    >>> d = Node(\"d\", parent=b)\n    >>> c = Node(\"c\", parent=d)\n    >>> e = Node(\"e\", parent=d)\n    >>> g = Node(\"g\", parent=f)\n    >>> i = Node(\"i\", parent=g)\n    >>> h = Node(\"h\", parent=i)\n    >>> print(RenderTree(f, style=AsciiStyle()).by_attr())\n    f\n    |-- b\n    |   |-- a\n    |   +-- d\n    |       |-- c\n    |       +-- e\n    +-- g\n        +-- i\n            +-- h\n\n    >>> find(f, lambda node: node.name == \"d\")\n    Node('/f/b/d')\n    >>> find(f, lambda node: node.name == \"z\")\n    >>> find(f, lambda node: b in node.path)  # doctest: +ELLIPSIS\n    Traceback (most recent call last):\n        ...\n    anytree.search.CountError: Expecting 1 elements at maximum, but found 5. (Node('/f/b')... Node('/f/b/d/e'))\n    \"\"\"\n    return _find(node, filter_=filter_, stop=stop, maxlevel=maxlevel)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_by_attr(node, value, name=\"name\", maxlevel=None):\n    return _find(node, filter_=lambda n: _filter_by_name(n, name, value),\n                 maxlevel=maxlevel)", "response": "Find a node with attribute name having value but stop at maxlevel."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the current instance of the node at the given path.", "response": "def get(self, node, path):\n        \"\"\"\n        Return instance at `path`.\n\n        An example module tree:\n\n        >>> from anytree import Node\n        >>> top = Node(\"top\", parent=None)\n        >>> sub0 = Node(\"sub0\", parent=top)\n        >>> sub0sub0 = Node(\"sub0sub0\", parent=sub0)\n        >>> sub0sub1 = Node(\"sub0sub1\", parent=sub0)\n        >>> sub1 = Node(\"sub1\", parent=top)\n\n        A resolver using the `name` attribute:\n\n        >>> r = Resolver('name')\n\n        Relative paths:\n\n        >>> r.get(top, \"sub0/sub0sub0\")\n        Node('/top/sub0/sub0sub0')\n        >>> r.get(sub1, \"..\")\n        Node('/top')\n        >>> r.get(sub1, \"../sub0/sub0sub1\")\n        Node('/top/sub0/sub0sub1')\n        >>> r.get(sub1, \".\")\n        Node('/top/sub1')\n        >>> r.get(sub1, \"\")\n        Node('/top/sub1')\n        >>> r.get(top, \"sub2\")\n        Traceback (most recent call last):\n          ...\n        anytree.resolver.ChildResolverError: Node('/top') has no child sub2. Children are: 'sub0', 'sub1'.\n\n        Absolute paths:\n\n        >>> r.get(sub0sub0, \"/top\")\n        Node('/top')\n        >>> r.get(sub0sub0, \"/top/sub0\")\n        Node('/top/sub0')\n        >>> r.get(sub0sub0, \"/\")\n        Traceback (most recent call last):\n          ...\n        anytree.resolver.ResolverError: root node missing. root is '/top'.\n        >>> r.get(sub0sub0, \"/bar\")\n        Traceback (most recent call last):\n          ...\n        anytree.resolver.ResolverError: unknown root node '/bar'. root is '/top'.\n        \"\"\"\n        node, parts = self.__start(node, path)\n        for part in parts:\n            if part == \"..\":\n                node = node.parent\n            elif part in (\"\", \".\"):\n                pass\n            else:\n                node = self.__get(node, part)\n        return node"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn instances at `path` supporting wildcards. Behaves identical to :any:`get`, but accepts wildcards and returns a list of found nodes. * `*` matches any characters, except '/'. * `?` matches a single character, except '/'. An example module tree: >>> from anytree import Node >>> top = Node(\"top\", parent=None) >>> sub0 = Node(\"sub0\", parent=top) >>> sub0sub0 = Node(\"sub0\", parent=sub0) >>> sub0sub1 = Node(\"sub1\", parent=sub0) >>> sub1 = Node(\"sub1\", parent=top) >>> sub1sub0 = Node(\"sub0\", parent=sub1) A resolver using the `name` attribute: >>> r = Resolver('name') Relative paths: >>> r.glob(top, \"sub0/sub?\") [Node('/top/sub0/sub0'), Node('/top/sub0/sub1')] >>> r.glob(sub1, \".././*\") [Node('/top/sub0'), Node('/top/sub1')] >>> r.glob(top, \"*/*\") [Node('/top/sub0/sub0'), Node('/top/sub0/sub1'), Node('/top/sub1/sub0')] >>> r.glob(top, \"*/sub0\") [Node('/top/sub0/sub0'), Node('/top/sub1/sub0')] >>> r.glob(top, \"sub1/sub1\") Traceback (most recent call last): ... anytree.resolver.ChildResolverError: Node('/top/sub1') has no child sub1. Children are: 'sub0'. Non-matching wildcards are no error: >>> r.glob(top, \"bar*\") [] >>> r.glob(top, \"sub2\") Traceback (most recent call last): ... anytree.resolver.ChildResolverError: Node('/top') has no child sub2. Children are: 'sub0', 'sub1'. Absolute paths: >>> r.glob(sub0sub0, \"/top/*\") [Node('/top/sub0'), Node('/top/sub1')] >>> r.glob(sub0sub0, \"/\") Traceback (most recent call last): ... anytree.resolver.ResolverError: root node missing. root is '/top'. >>> r.glob(sub0sub0, \"/bar\") Traceback (most recent call last): ... anytree.resolver.ResolverError: unknown root node '/bar'. root is '/top'.", "response": "def glob(self, node, path):\n        \"\"\"\n        Return instances at `path` supporting wildcards.\n\n        Behaves identical to :any:`get`, but accepts wildcards and returns\n        a list of found nodes.\n\n        * `*` matches any characters, except '/'.\n        * `?` matches a single character, except '/'.\n\n        An example module tree:\n\n        >>> from anytree import Node\n        >>> top = Node(\"top\", parent=None)\n        >>> sub0 = Node(\"sub0\", parent=top)\n        >>> sub0sub0 = Node(\"sub0\", parent=sub0)\n        >>> sub0sub1 = Node(\"sub1\", parent=sub0)\n        >>> sub1 = Node(\"sub1\", parent=top)\n        >>> sub1sub0 = Node(\"sub0\", parent=sub1)\n\n        A resolver using the `name` attribute:\n\n        >>> r = Resolver('name')\n\n        Relative paths:\n\n        >>> r.glob(top, \"sub0/sub?\")\n        [Node('/top/sub0/sub0'), Node('/top/sub0/sub1')]\n        >>> r.glob(sub1, \".././*\")\n        [Node('/top/sub0'), Node('/top/sub1')]\n        >>> r.glob(top, \"*/*\")\n        [Node('/top/sub0/sub0'), Node('/top/sub0/sub1'), Node('/top/sub1/sub0')]\n        >>> r.glob(top, \"*/sub0\")\n        [Node('/top/sub0/sub0'), Node('/top/sub1/sub0')]\n        >>> r.glob(top, \"sub1/sub1\")\n        Traceback (most recent call last):\n            ...\n        anytree.resolver.ChildResolverError: Node('/top/sub1') has no child sub1. Children are: 'sub0'.\n\n        Non-matching wildcards are no error:\n\n        >>> r.glob(top, \"bar*\")\n        []\n        >>> r.glob(top, \"sub2\")\n        Traceback (most recent call last):\n          ...\n        anytree.resolver.ChildResolverError: Node('/top') has no child sub2. Children are: 'sub0', 'sub1'.\n\n        Absolute paths:\n\n        >>> r.glob(sub0sub0, \"/top/*\")\n        [Node('/top/sub0'), Node('/top/sub1')]\n        >>> r.glob(sub0sub0, \"/\")\n        Traceback (most recent call last):\n          ...\n        anytree.resolver.ResolverError: root node missing. root is '/top'.\n        >>> r.glob(sub0sub0, \"/bar\")\n        Traceback (most recent call last):\n          ...\n        anytree.resolver.ResolverError: unknown root node '/bar'. root is '/top'.\n        \"\"\"\n        node, parts = self.__start(node, path)\n        return self.__glob(node, parts)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting the current set of items to a DOT file.", "response": "def to_dotfile(self, filename):\n        \"\"\"\n        Write graph to `filename`.\n\n        >>> from anytree import Node\n        >>> root = Node(\"root\")\n        >>> s0 = Node(\"sub0\", parent=root)\n        >>> s0b = Node(\"sub0B\", parent=s0)\n        >>> s0a = Node(\"sub0A\", parent=s0)\n        >>> s1 = Node(\"sub1\", parent=root)\n        >>> s1a = Node(\"sub1A\", parent=s1)\n        >>> s1b = Node(\"sub1B\", parent=s1)\n        >>> s1c = Node(\"sub1C\", parent=s1)\n        >>> s1ca = Node(\"sub1Ca\", parent=s1c)\n\n        >>> from anytree.exporter import DotExporter\n        >>> DotExporter(root).to_dotfile(\"tree.dot\")\n\n        The generated file should be handed over to the `dot` tool from the\n        http://www.graphviz.org/ package::\n\n            $ dot tree.dot -T png -o tree.png\n        \"\"\"\n        with codecs.open(filename, \"w\", \"utf-8\") as file:\n            for line in self:\n                file.write(\"%s\\n\" % line)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites the current set of attributes to a temporary file and invoke dot.", "response": "def to_picture(self, filename):\n        \"\"\"\n        Write graph to a temporary file and invoke `dot`.\n\n        The output file type is automatically detected from the file suffix.\n\n        *`graphviz` needs to be installed, before usage of this method.*\n        \"\"\"\n        fileformat = path.splitext(filename)[1][1:]\n        with NamedTemporaryFile(\"wb\", delete=False) as dotfile:\n            dotfilename = dotfile.name\n            for line in self:\n                dotfile.write((\"%s\\n\" % line).encode(\"utf-8\"))\n            dotfile.flush()\n            cmd = [\"dot\", dotfilename, \"-T\", fileformat, \"-o\", filename]\n            check_call(cmd)\n        try:\n            remove(dotfilename)\n        except Exception:  # pragma: no cover\n            msg = 'Could not remove temporary file %s' % dotfilename\n            logging.getLogger(__name__).warn(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef commonancestors(*nodes):\n    ancestors = [node.ancestors for node in nodes]\n    common = []\n    for parentnodes in zip(*ancestors):\n        parentnode = parentnodes[0]\n        if all([parentnode is p for p in parentnodes[1:]]):\n            common.append(parentnode)\n        else:\n            break\n    return tuple(common)", "response": "Determines common ancestors of nodes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning Left Sibling of node.", "response": "def leftsibling(node):\n    \"\"\"\n    Return Left Sibling of `node`.\n\n    >>> from anytree import Node\n    >>> dan = Node(\"Dan\")\n    >>> jet = Node(\"Jet\", parent=dan)\n    >>> jan = Node(\"Jan\", parent=dan)\n    >>> joe = Node(\"Joe\", parent=dan)\n    >>> leftsibling(dan)\n    >>> leftsibling(jet)\n    >>> leftsibling(jan)\n    Node('/Dan/Jet')\n    >>> leftsibling(joe)\n    Node('/Dan/Jan')\n    \"\"\"\n    if node.parent:\n        pchildren = node.parent.children\n        idx = pchildren.index(node)\n        if idx:\n            return pchildren[idx - 1]\n        else:\n            return None\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning Right Sibling of node.", "response": "def rightsibling(node):\n    \"\"\"\n    Return Right Sibling of `node`.\n\n    >>> from anytree import Node\n    >>> dan = Node(\"Dan\")\n    >>> jet = Node(\"Jet\", parent=dan)\n    >>> jan = Node(\"Jan\", parent=dan)\n    >>> joe = Node(\"Joe\", parent=dan)\n    >>> rightsibling(dan)\n    >>> rightsibling(jet)\n    Node('/Dan/Jan')\n    >>> rightsibling(jan)\n    Node('/Dan/Joe')\n    >>> rightsibling(joe)\n    \"\"\"\n    if node.parent:\n        pchildren = node.parent.children\n        idx = pchildren.index(node)\n        try:\n            return pchildren[idx + 1]\n        except IndexError:\n            return None\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nexport tree starting at node.", "response": "def export(self, node):\n        \"\"\"Export tree starting at `node`.\"\"\"\n        attriter = self.attriter or (lambda attr_values: attr_values)\n        return self.__export(node, self.dictcls, attriter, self.childiter)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef compute_u_val_for_sky_loc_stat(hplus, hcross, hphccorr,\n                                 hpnorm=None, hcnorm=None, indices=None):\n    \"\"\"The max-over-sky location detection statistic maximizes over a phase,\n    an amplitude and the ratio of F+ and Fx, encoded in a variable called u.\n    Here we return the value of u for the given indices.\n    \"\"\"\n    if indices is not None:\n        hplus = hplus[indices]\n        hcross = hcross[indices]\n\n    if hpnorm is not None:\n        hplus = hplus * hpnorm\n    if hcnorm is not None:\n        hcross = hcross * hcnorm\n\n    # Sanity checking in func. above should already have identified any points\n    # which are bad, and should be used to construct indices for input here\n    hplus_magsq = numpy.real(hplus) * numpy.real(hplus) + \\\n                       numpy.imag(hplus) * numpy.imag(hplus)\n    hcross_magsq = numpy.real(hcross) * numpy.real(hcross) + \\\n                       numpy.imag(hcross) * numpy.imag(hcross)\n    rho_pluscross = numpy.real(hplus) * numpy.real(hcross) + \\\n                       numpy.imag(hplus)*numpy.imag(hcross)\n\n    a = hphccorr * hplus_magsq - rho_pluscross\n    b = hplus_magsq - hcross_magsq\n    c = rho_pluscross - hphccorr * hcross_magsq\n\n    sq_root = b*b - 4*a*c\n    sq_root = sq_root**0.5\n    sq_root = -sq_root\n    # Catch the a->0 case\n    bad_lgc = (a == 0)\n    dbl_bad_lgc = numpy.logical_and(c == 0, b == 0)\n    dbl_bad_lgc = numpy.logical_and(bad_lgc, dbl_bad_lgc)\n    # Initialize u\n    u = sq_root * 0.\n    # In this case u is completely degenerate, so set it to 1\n    u[dbl_bad_lgc] = 1.\n    # If a->0 avoid overflow by just setting to a large value\n    u[bad_lgc & ~dbl_bad_lgc] = 1E17\n    # Otherwise normal statistic\n    u[~bad_lgc] = (-b[~bad_lgc] + sq_root[~bad_lgc]) / (2*a[~bad_lgc])\n\n    snr_cplx = hplus * u + hcross\n    coa_phase = numpy.angle(snr_cplx)\n\n    return u, coa_phase", "response": "Compute the value of u for the max - over - sky location detection statistic."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the match maximized over polarization phase. In contrast to compute_max_snr_over_sky_loc_stat_no_phase this function performs no maximization over orbital phase, treating that as an intrinsic parameter. In the case of aligned-spin 2,2-mode only waveforms, this collapses to the normal statistic (at twice the computational cost!) Parameters ----------- hplus : TimeSeries This is the IFFTed complex SNR time series of (h+, data). If not normalized, supply the normalization factor so this can be done! It is recommended to normalize this before sending through this function hcross : TimeSeries This is the IFFTed complex SNR time series of (hx, data). If not normalized, supply the normalization factor so this can be done! hphccorr : float The real component of the overlap between the two polarizations Re[(h+, hx)]. Note that the imaginary component does not enter the detection statistic. This must be normalized and is sign-sensitive. thresh : float Used for optimization. If we do not care about the value of SNR values below thresh we can calculate a quick statistic that will always overestimate SNR and then only calculate the proper, more expensive, statistic at points where the quick SNR is above thresh. hpsigmasq : float The normalization factor (h+, h+). Default = None (=1, already normalized) hcsigmasq : float The normalization factor (hx, hx). Default = None (=1, already normalized) out : TimeSeries (optional, default=None) If given, use this array to store the output. Returns -------- det_stat : TimeSeries The SNR maximized over sky location", "response": "def compute_max_snr_over_sky_loc_stat_no_phase(hplus, hcross, hphccorr,\n                                               hpnorm=None, hcnorm=None,\n                                               out=None, thresh=0,\n                                               analyse_slice=None):\n    \"\"\"\n    Compute the match maximized over polarization phase.\n\n    In contrast to compute_max_snr_over_sky_loc_stat_no_phase this function\n    performs no maximization over orbital phase, treating that as an intrinsic\n    parameter. In the case of aligned-spin 2,2-mode only waveforms, this\n    collapses to the normal statistic (at twice the computational cost!)\n\n    Parameters\n    -----------\n    hplus : TimeSeries\n        This is the IFFTed complex SNR time series of (h+, data). If not\n        normalized, supply the normalization factor so this can be done!\n        It is recommended to normalize this before sending through this\n        function\n    hcross : TimeSeries\n        This is the IFFTed complex SNR time series of (hx, data). If not\n        normalized, supply the normalization factor so this can be done!\n    hphccorr : float\n        The real component of the overlap between the two polarizations\n        Re[(h+, hx)]. Note that the imaginary component does not enter the\n        detection statistic. This must be normalized and is sign-sensitive.\n    thresh : float\n        Used for optimization. If we do not care about the value of SNR\n        values below thresh we can calculate a quick statistic that will\n        always overestimate SNR and then only calculate the proper, more\n        expensive, statistic at points where the quick SNR is above thresh.\n    hpsigmasq : float\n        The normalization factor (h+, h+). Default = None (=1, already\n        normalized)\n    hcsigmasq : float\n        The normalization factor (hx, hx). Default = None (=1, already\n        normalized)\n    out : TimeSeries (optional, default=None)\n        If given, use this array to store the output.\n\n    Returns\n    --------\n    det_stat : TimeSeries\n        The SNR maximized over sky location\n    \"\"\"\n    # NOTE: Not much optimization has been done here! This may need to be\n    # C-ified using scipy.weave.\n\n    if out is None:\n        out = zeros(len(hplus))\n        out.non_zero_locs = numpy.array([], dtype=out.dtype)\n    else:\n        if not hasattr(out, 'non_zero_locs'):\n            # Doing this every time is not a zero-cost operation\n            out.data[:] = 0\n            out.non_zero_locs = numpy.array([], dtype=out.dtype)\n        else:\n            # Only set non zero locations to zero\n            out.data[out.non_zero_locs] = 0\n\n    # If threshold is given we can limit the points at which to compute the\n    # full statistic\n    if thresh:\n        # This is the statistic that always overestimates the SNR...\n        # It allows some unphysical freedom that the full statistic does not\n        #\n        # For now this is copied from the max-over-phase statistic. One could\n        # probably make this faster by removing the imaginary components of\n        # the matched filter, as these are not used here.\n        idx_p, _ = events.threshold_only(hplus[analyse_slice],\n                                                    thresh / (2**0.5 * hpnorm))\n        idx_c, _ = events.threshold_only(hcross[analyse_slice],\n                                                    thresh / (2**0.5 * hcnorm))\n        idx_p = idx_p + analyse_slice.start\n        idx_c = idx_c + analyse_slice.start\n        hp_red = hplus[idx_p] * hpnorm\n        hc_red = hcross[idx_p] * hcnorm\n        stat_p = hp_red.real**2 + hp_red.imag**2 + \\\n                     hc_red.real**2 + hc_red.imag**2\n        locs_p = idx_p[stat_p > (thresh*thresh)]\n        hp_red = hplus[idx_c] * hpnorm\n        hc_red = hcross[idx_c] * hcnorm\n        stat_c = hp_red.real**2 + hp_red.imag**2 + \\\n                     hc_red.real**2 + hc_red.imag**2\n        locs_c = idx_c[stat_c > (thresh*thresh)]\n        locs = numpy.unique(numpy.concatenate((locs_p, locs_c)))\n\n        hplus = hplus[locs]\n        hcross = hcross[locs]\n\n    hplus = hplus * hpnorm\n    hcross = hcross * hcnorm\n\n\n    # Calculate and sanity check the denominator\n    denom = 1 - hphccorr*hphccorr\n    if denom < 0:\n        if hphccorr > 1:\n            err_msg = \"Overlap between hp and hc is given as %f. \" %(hphccorr)\n            err_msg += \"How can an overlap be bigger than 1?\"\n            raise ValueError(err_msg)\n        else:\n            err_msg = \"There really is no way to raise this error!?! \"\n            err_msg += \"If you're seeing this, it is bad.\"\n            raise ValueError(err_msg)\n    if denom == 0:\n        # This case, of hphccorr==1, makes the statistic degenerate\n        # This case should not physically be possible luckily.\n        err_msg = \"You have supplied a real overlap between hp and hc of 1. \"\n        err_msg += \"Ian is reasonably certain this is physically impossible \"\n        err_msg += \"so why are you seeing this?\"\n        raise ValueError(err_msg)\n\n    assert(len(hplus) == len(hcross))\n\n    # Now the stuff where comp. cost may be a problem\n    hplus_magsq = numpy.real(hplus) * numpy.real(hplus)\n    hcross_magsq = numpy.real(hcross) * numpy.real(hcross)\n    rho_pluscross = numpy.real(hplus) * numpy.real(hcross)\n\n    det_stat_sq = (hplus_magsq + hcross_magsq - 2 * rho_pluscross*hphccorr)\n\n    det_stat = numpy.sqrt(det_stat_sq / denom)\n\n    if thresh:\n        out.data[locs] = det_stat\n        out.non_zero_locs = locs\n        return out\n    else:\n        return Array(det_stat, copy=False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes max - over - sky location detection statistic for given indices.", "response": "def compute_u_val_for_sky_loc_stat_no_phase(hplus, hcross, hphccorr,\n                                 hpnorm=None , hcnorm=None, indices=None):\n    \"\"\"The max-over-sky location (no phase) detection statistic maximizes over\n    an amplitude and the ratio of F+ and Fx, encoded in a variable called u.\n    Here we return the value of u for the given indices.\n\n\n    \"\"\"\n    if indices is not None:\n        hplus = hplus[indices]\n        hcross = hcross[indices]\n\n    if hpnorm is not None:\n        hplus = hplus * hpnorm\n    if hcnorm is not None:\n        hcross = hcross * hcnorm\n\n    rhoplusre=numpy.real(hplus)\n    rhocrossre=numpy.real(hcross)\n    overlap=numpy.real(hphccorr)\n\n    denom = (-rhocrossre+overlap*rhoplusre)\n    # Initialize tan_kappa array\n    u_val = denom * 0.\n    # Catch the denominator -> 0 case\n    bad_lgc = (denom == 0)\n    u_val[bad_lgc] = 1E17\n    # Otherwise do normal statistic\n    u_val[~bad_lgc] = (-rhoplusre+overlap*rhocrossre) / \\\n        (-rhocrossre+overlap*rhoplusre)\n    coa_phase = numpy.zeros(len(indices), dtype=numpy.float32)\n\n    return u_val, coa_phase"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a frequency series of the input vector.", "response": "def make_frequency_series(vec):\n    \"\"\"Return a frequency series of the input vector.\n\n    If the input is a frequency series it is returned, else if the input\n    vector is a real time series it is fourier transformed and returned as a\n    frequency series.\n\n    Parameters\n    ----------\n    vector : TimeSeries or FrequencySeries\n\n    Returns\n    -------\n    Frequency Series: FrequencySeries\n        A frequency domain version of the input vector.\n    \"\"\"\n    if isinstance(vec, FrequencySeries):\n        return vec\n    if isinstance(vec, TimeSeries):\n        N = len(vec)\n        n = N/2+1\n        delta_f = 1.0 / N / vec.delta_t\n        vectilde =  FrequencySeries(zeros(n, dtype=complex_same_precision_as(vec)),\n                                    delta_f=delta_f, copy=False)\n        fft(vec, vectilde)\n        return vectilde\n    else:\n        raise TypeError(\"Can only convert a TimeSeries to a FrequencySeries\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sigmasq_series(htilde, psd=None, low_frequency_cutoff=None,\n            high_frequency_cutoff=None):\n    \"\"\"Return a cumulative sigmasq frequency series.\n\n    Return a frequency series containing the accumulated power in the input\n    up to that frequency.\n\n    Parameters\n    ----------\n    htilde : TimeSeries or FrequencySeries\n        The input vector\n    psd : {None, FrequencySeries}, optional\n        The psd used to weight the accumulated power.\n    low_frequency_cutoff : {None, float}, optional\n        The frequency to begin accumulating power. If None, start at the beginning\n        of the vector.\n    high_frequency_cutoff : {None, float}, optional\n        The frequency to stop considering accumulated power. If None, continue\n        until the end of the input vector.\n\n    Returns\n    -------\n    Frequency Series: FrequencySeries\n        A frequency series containing the cumulative sigmasq.\n    \"\"\"\n    htilde = make_frequency_series(htilde)\n    N = (len(htilde)-1) * 2\n    norm = 4.0 * htilde.delta_f\n    kmin, kmax = get_cutoff_indices(low_frequency_cutoff,\n                                   high_frequency_cutoff, htilde.delta_f, N)\n\n    sigma_vec = FrequencySeries(zeros(len(htilde), dtype=real_same_precision_as(htilde)),\n                                delta_f = htilde.delta_f, copy=False)\n\n    mag = htilde.squared_norm()\n\n    if psd is not None:\n        mag /= psd\n\n    sigma_vec[kmin:kmax] = mag[kmin:kmax].cumsum()\n\n    return sigma_vec*norm", "response": "Return a cumulative sigmasq frequency series."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sigmasq(htilde, psd = None, low_frequency_cutoff=None,\n            high_frequency_cutoff=None):\n    \"\"\"Return the loudness of the waveform. This is defined (see Duncan\n    Brown's thesis) as the unnormalized matched-filter of the input waveform,\n    htilde, with itself. This quantity is usually referred to as (sigma)^2\n    and is then used to normalize matched-filters with the data.\n\n    Parameters\n    ----------\n    htilde : TimeSeries or FrequencySeries\n        The input vector containing a waveform.\n    psd : {None, FrequencySeries}, optional\n        The psd used to weight the accumulated power.\n    low_frequency_cutoff : {None, float}, optional\n        The frequency to begin considering waveform power.\n    high_frequency_cutoff : {None, float}, optional\n        The frequency to stop considering waveform power.\n\n    Returns\n    -------\n    sigmasq: float\n    \"\"\"\n    htilde = make_frequency_series(htilde)\n    N = (len(htilde)-1) * 2\n    norm = 4.0 * htilde.delta_f\n    kmin, kmax = get_cutoff_indices(low_frequency_cutoff,\n                                   high_frequency_cutoff, htilde.delta_f, N)\n    ht = htilde[kmin:kmax]\n\n    if psd:\n        try:\n            numpy.testing.assert_almost_equal(ht.delta_f, psd.delta_f)\n        except:\n            raise ValueError('Waveform does not have same delta_f as psd')\n\n    if psd is None:\n        sq = ht.inner(ht)\n    else:\n        sq = ht.weighted_inner(ht, psd[kmin:kmax])\n\n    return sq.real * norm", "response": "Return the loudness of the waveform."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sigma(htilde, psd = None, low_frequency_cutoff=None,\n        high_frequency_cutoff=None):\n    \"\"\" Return the sigma of the waveform. See sigmasq for more details.\n\n    Parameters\n    ----------\n    htilde : TimeSeries or FrequencySeries\n        The input vector containing a waveform.\n    psd : {None, FrequencySeries}, optional\n        The psd used to weight the accumulated power.\n    low_frequency_cutoff : {None, float}, optional\n        The frequency to begin considering waveform power.\n    high_frequency_cutoff : {None, float}, optional\n        The frequency to stop considering waveform power.\n\n    Returns\n    -------\n    sigmasq: float\n    \"\"\"\n    return sqrt(sigmasq(htilde, psd, low_frequency_cutoff, high_frequency_cutoff))", "response": "Returns the sigma of the waveform."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_cutoff_indices(flow, fhigh, df, N):\n    if flow:\n        kmin = int(flow / df)\n        if kmin < 0:\n            err_msg = \"Start frequency cannot be negative. \"\n            err_msg += \"Supplied value and kmin {} and {}\".format(flow, kmin)\n            raise ValueError(err_msg)\n    else:\n        kmin = 1\n    if fhigh:\n        kmax = int(fhigh / df )\n        if kmax > int((N + 1)/2.):\n            kmax = int((N + 1)/2.)\n    else:\n        # int() truncates towards 0, so this is\n        # equivalent to the floor of the float\n        kmax = int((N + 1)/2.)\n\n    if kmax <= kmin:\n        err_msg = \"Kmax cannot be less than or equal to kmin. \"\n        err_msg += \"Provided values of freqencies (min,max) were \"\n        err_msg += \"{} and {} \".format(flow, fhigh)\n        err_msg += \"corresponding to (kmin, kmax) of \"\n        err_msg += \"{} and {}.\".format(kmin, kmax)\n        raise ValueError(err_msg)\n\n    return kmin,kmax", "response": "Returns the indices of a frequency series at which to stop an overlap\n    calculation."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef matched_filter_core(template, data, psd=None, low_frequency_cutoff=None,\n                  high_frequency_cutoff=None, h_norm=None, out=None, corr_out=None):\n    \"\"\" Return the complex snr and normalization.\n\n    Return the complex snr, along with its associated normalization of the template,\n    matched filtered against the data.\n\n    Parameters\n    ----------\n    template : TimeSeries or FrequencySeries\n        The template waveform\n    data : TimeSeries or FrequencySeries\n        The strain data to be filtered.\n    psd : {FrequencySeries}, optional\n        The noise weighting of the filter.\n    low_frequency_cutoff : {None, float}, optional\n        The frequency to begin the filter calculation. If None, begin at the\n        first frequency after DC.\n    high_frequency_cutoff : {None, float}, optional\n        The frequency to stop the filter calculation. If None, continue to the\n        the nyquist frequency.\n    h_norm : {None, float}, optional\n        The template normalization. If none, this value is calculated internally.\n    out : {None, Array}, optional\n        An array to use as memory for snr storage. If None, memory is allocated\n        internally.\n    corr_out : {None, Array}, optional\n        An array to use as memory for correlation storage. If None, memory is allocated\n        internally. If provided, management of the vector is handled externally by the\n        caller. No zero'ing is done internally.\n\n    Returns\n    -------\n    snr : TimeSeries\n        A time series containing the complex snr.\n    corrrelation: FrequencySeries\n        A frequency series containing the correlation vector.\n    norm : float\n        The normalization of the complex snr.\n    \"\"\"\n    htilde = make_frequency_series(template)\n    stilde = make_frequency_series(data)\n\n    if len(htilde) != len(stilde):\n        raise ValueError(\"Length of template and data must match\")\n\n    N = (len(stilde)-1) * 2\n    kmin, kmax = get_cutoff_indices(low_frequency_cutoff,\n                                   high_frequency_cutoff, stilde.delta_f, N)\n\n    if corr_out is not None:\n        qtilde = corr_out\n    else:\n        qtilde = zeros(N, dtype=complex_same_precision_as(data))\n\n    if out is None:\n        _q = zeros(N, dtype=complex_same_precision_as(data))\n    elif (len(out) == N) and type(out) is Array and out.kind =='complex':\n        _q = out\n    else:\n        raise TypeError('Invalid Output Vector: wrong length or dtype')\n\n    correlate(htilde[kmin:kmax], stilde[kmin:kmax], qtilde[kmin:kmax])\n\n    if psd is not None:\n        if isinstance(psd, FrequencySeries):\n            if psd.delta_f == stilde.delta_f :\n                qtilde[kmin:kmax] /= psd[kmin:kmax]\n            else:\n                raise TypeError(\"PSD delta_f does not match data\")\n        else:\n            raise TypeError(\"PSD must be a FrequencySeries\")\n\n    ifft(qtilde, _q)\n\n    if h_norm is None:\n        h_norm = sigmasq(htilde, psd, low_frequency_cutoff, high_frequency_cutoff)\n\n    norm = (4.0 * stilde.delta_f) / sqrt( h_norm)\n\n    return (TimeSeries(_q, epoch=stilde._epoch, delta_t=stilde.delta_t, copy=False),\n           FrequencySeries(qtilde, epoch=stilde._epoch, delta_f=stilde.delta_f, copy=False),\n           norm)", "response": "Return the complex snr and normalization of the template and data."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef smear(idx, factor):\n\n\n    s = [idx]\n    for i in range(factor+1):\n        a = i - factor/2\n        s += [idx + a]\n    return numpy.unique(numpy.concatenate(s))", "response": "This function will take as input an array of indexes and return every unique index within the specified factor of the inputs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef matched_filter(template, data, psd=None, low_frequency_cutoff=None,\n                  high_frequency_cutoff=None, sigmasq=None):\n    \"\"\" Return the complex snr.\n\n    Return the complex snr, along with its associated normalization of the\n    template, matched filtered against the data.\n\n    Parameters\n    ----------\n    template : TimeSeries or FrequencySeries\n        The template waveform\n    data : TimeSeries or FrequencySeries\n        The strain data to be filtered.\n    psd : FrequencySeries\n        The noise weighting of the filter.\n    low_frequency_cutoff : {None, float}, optional\n        The frequency to begin the filter calculation. If None, begin at the\n        first frequency after DC.\n    high_frequency_cutoff : {None, float}, optional\n        The frequency to stop the filter calculation. If None, continue to the\n        the nyquist frequency.\n    sigmasq : {None, float}, optional\n        The template normalization. If none, this value is calculated\n        internally.\n\n    Returns\n    -------\n    snr : TimeSeries\n        A time series containing the complex snr.\n    \"\"\"\n    snr, _, norm = matched_filter_core(template, data, psd=psd,\n            low_frequency_cutoff=low_frequency_cutoff,\n            high_frequency_cutoff=high_frequency_cutoff, h_norm=sigmasq)\n    return snr * norm", "response": "Return the complex snr that is matched against the data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef match(vec1, vec2, psd=None, low_frequency_cutoff=None,\n          high_frequency_cutoff=None, v1_norm=None, v2_norm=None):\n    \"\"\" Return the match between the two TimeSeries or FrequencySeries.\n\n    Return the match between two waveforms. This is equivelant to the overlap\n    maximized over time and phase.\n\n    Parameters\n    ----------\n    vec1 : TimeSeries or FrequencySeries\n        The input vector containing a waveform.\n    vec2 : TimeSeries or FrequencySeries\n        The input vector containing a waveform.\n    psd : Frequency Series\n        A power spectral density to weight the overlap.\n    low_frequency_cutoff : {None, float}, optional\n        The frequency to begin the match.\n    high_frequency_cutoff : {None, float}, optional\n        The frequency to stop the match.\n    v1_norm : {None, float}, optional\n        The normalization of the first waveform. This is equivalent to its\n        sigmasq value. If None, it is internally calculated.\n    v2_norm : {None, float}, optional\n        The normalization of the second waveform. This is equivalent to its\n        sigmasq value. If None, it is internally calculated.\n\n    Returns\n    -------\n    match: float\n    index: int\n        The number of samples to shift to get the match.\n    \"\"\"\n\n    htilde = make_frequency_series(vec1)\n    stilde = make_frequency_series(vec2)\n\n    N = (len(htilde)-1) * 2\n\n    global _snr\n    if _snr is None or _snr.dtype != htilde.dtype or len(_snr) != N:\n        _snr = zeros(N,dtype=complex_same_precision_as(vec1))\n    snr, _, snr_norm = matched_filter_core(htilde,stilde,psd,low_frequency_cutoff,\n                             high_frequency_cutoff, v1_norm, out=_snr)\n    maxsnr, max_id = snr.abs_max_loc()\n    if v2_norm is None:\n        v2_norm = sigmasq(stilde, psd, low_frequency_cutoff, high_frequency_cutoff)\n    return maxsnr * snr_norm / sqrt(v2_norm), max_id", "response": "Return the match between two waveforms."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the overlap between two TimeSeries or FrequencySeries.", "response": "def overlap(vec1, vec2, psd=None, low_frequency_cutoff=None,\n          high_frequency_cutoff=None, normalized=True):\n    \"\"\" Return the overlap between the two TimeSeries or FrequencySeries.\n\n    Parameters\n    ----------\n    vec1 : TimeSeries or FrequencySeries\n        The input vector containing a waveform.\n    vec2 : TimeSeries or FrequencySeries\n        The input vector containing a waveform.\n    psd : Frequency Series\n        A power spectral density to weight the overlap.\n    low_frequency_cutoff : {None, float}, optional\n        The frequency to begin the overlap.\n    high_frequency_cutoff : {None, float}, optional\n        The frequency to stop the overlap.\n    normalized : {True, boolean}, optional\n        Set if the overlap is normalized. If true, it will range from 0 to 1.\n\n    Returns\n    -------\n    overlap: float\n    \"\"\"\n\n    return overlap_cplx(vec1, vec2, psd=psd, \\\n            low_frequency_cutoff=low_frequency_cutoff,\\\n            high_frequency_cutoff=high_frequency_cutoff,\\\n            normalized=normalized).real"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the complex overlap between two TimeSeries or FrequencySeries.", "response": "def overlap_cplx(vec1, vec2, psd=None, low_frequency_cutoff=None,\n          high_frequency_cutoff=None, normalized=True):\n    \"\"\"Return the complex overlap between the two TimeSeries or FrequencySeries.\n\n    Parameters\n    ----------\n    vec1 : TimeSeries or FrequencySeries\n        The input vector containing a waveform.\n    vec2 : TimeSeries or FrequencySeries\n        The input vector containing a waveform.\n    psd : Frequency Series\n        A power spectral density to weight the overlap.\n    low_frequency_cutoff : {None, float}, optional\n        The frequency to begin the overlap.\n    high_frequency_cutoff : {None, float}, optional\n        The frequency to stop the overlap.\n    normalized : {True, boolean}, optional\n        Set if the overlap is normalized. If true, it will range from 0 to 1.\n\n    Returns\n    -------\n    overlap: complex\n    \"\"\"\n    htilde = make_frequency_series(vec1)\n    stilde = make_frequency_series(vec2)\n\n    kmin, kmax = get_cutoff_indices(low_frequency_cutoff,\n            high_frequency_cutoff, stilde.delta_f, (len(stilde)-1) * 2)\n\n    if psd:\n        inner = (htilde[kmin:kmax]).weighted_inner(stilde[kmin:kmax], psd[kmin:kmax])\n    else:\n        inner = (htilde[kmin:kmax]).inner(stilde[kmin:kmax])\n\n    if normalized:\n        sig1 = sigma(vec1, psd=psd, low_frequency_cutoff=low_frequency_cutoff,\n                     high_frequency_cutoff=high_frequency_cutoff)\n        sig2 = sigma(vec2, psd=psd, low_frequency_cutoff=low_frequency_cutoff,\n                     high_frequency_cutoff=high_frequency_cutoff)\n        norm = 1 / sig1 / sig2\n    else:\n        norm = 1\n\n    return 4 * htilde.delta_f * inner * norm"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef quadratic_interpolate_peak(left, middle, right):\n    bin_offset = 1.0/2.0 * (left - right) / (left - 2 * middle + right)\n    peak_value = middle + 0.25 * (left - right) * bin_offset\n    return bin_offset, peak_value", "response": "Interpolate the peak and offset using a quadratic approximation"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngives a StrainBuffer a frequency series and a trigger time compute a portion of the SNR time series that follows the trigger and followup.", "response": "def compute_followup_snr_series(data_reader, htilde, trig_time,\n                                duration=0.095, check_state=True,\n                                coinc_window=0.05):\n    \"\"\"Given a StrainBuffer, a template frequency series and a trigger time,\n    compute a portion of the SNR time series centered on the trigger for its\n    rapid sky localization and followup.\n\n    If the trigger time is too close to the boundary of the valid data segment\n    the SNR series is calculated anyway and might be slightly contaminated by\n    filter and wrap-around effects. For reasonable durations this will only\n    affect a small fraction of the triggers and probably in a negligible way.\n\n    Parameters\n    ----------\n    data_reader : StrainBuffer\n        The StrainBuffer object to read strain data from.\n\n    htilde : FrequencySeries\n        The frequency series containing the template waveform.\n\n    trig_time : {float, lal.LIGOTimeGPS}\n        The trigger time.\n\n    duration : float (optional)\n        Duration of the computed SNR series in seconds. If omitted, it defaults\n        to twice the Earth light travel time plus 10 ms of timing uncertainty.\n\n    check_state : boolean\n        If True, and the detector was offline or flagged for bad data quality\n        at any point during the inspiral, then return (None, None) instead.\n\n    coinc_window : float (optional)\n        Maximum possible time between coincident triggers at different\n        detectors. This is needed to properly determine data padding.\n\n    Returns\n    -------\n    snr : TimeSeries\n        The portion of SNR around the trigger. None if the detector is offline\n        or has bad data quality, and check_state is True.\n    \"\"\"\n    if check_state:\n        # was the detector observing for the full amount of involved data?\n        state_start_time = trig_time - duration / 2 - htilde.length_in_time\n        state_end_time = trig_time + duration / 2\n        state_duration = state_end_time - state_start_time\n        if data_reader.state is not None:\n            if not data_reader.state.is_extent_valid(state_start_time,\n                                                     state_duration):\n                return None\n\n        # was the data quality ok for the full amount of involved data?\n        dq_start_time = state_start_time - data_reader.dq_padding\n        dq_duration = state_duration + 2 * data_reader.dq_padding\n        if data_reader.dq is not None:\n            if not data_reader.dq.is_extent_valid(dq_start_time, dq_duration):\n                return None\n\n    stilde = data_reader.overwhitened_data(htilde.delta_f)\n    snr, _, norm = matched_filter_core(htilde, stilde,\n                                          h_norm=htilde.sigmasq(stilde.psd))\n\n    valid_end = int(len(snr) - data_reader.trim_padding)\n    valid_start = int(valid_end - data_reader.blocksize * snr.sample_rate)\n\n    half_dur_samples = int(snr.sample_rate * duration / 2)\n    coinc_samples = int(snr.sample_rate * coinc_window)\n    valid_start -= half_dur_samples + coinc_samples\n    valid_end += half_dur_samples\n    if valid_start < 0 or valid_end > len(snr)-1:\n        raise ValueError(('Requested SNR duration ({0} s)'\n                          ' too long').format(duration))\n\n    # Onsource slice for Bayestar followup\n    onsource_idx = float(trig_time - snr.start_time) * snr.sample_rate\n    onsource_idx = int(round(onsource_idx))\n    onsource_slice = slice(onsource_idx - half_dur_samples,\n                           onsource_idx + half_dur_samples + 1)\n    return snr[onsource_slice] * norm"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef full_matched_filter_and_cluster_symm(self, segnum, template_norm, window, epoch=None):\n        norm = (4.0 * self.delta_f) / sqrt(template_norm)\n        self.correlators[segnum].correlate()\n        self.ifft.execute()\n        snrv, idx = self.threshold_and_clusterers[segnum].threshold_and_cluster(self.snr_threshold / norm, window)\n\n        if len(idx) == 0:\n            return [], [], [], [], []\n\n        logging.info(\"%s points above threshold\" % str(len(idx)))\n\n        snr = TimeSeries(self.snr_mem, epoch=epoch, delta_t=self.delta_t, copy=False)\n        corr = FrequencySeries(self.corr_mem, delta_f=self.delta_f, copy=False)\n        return snr, norm, corr, idx, snrv", "response": "This method calculates the full matched filter and cluster symmetrical frequency series and returns the complex snr timeseries normalization of the complex snr and correlation vector frequency series and the list of indices of the complex snr values at the trigger locations and the snr values at the trigger locations."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef full_matched_filter_and_cluster_fc(self, segnum, template_norm, window, epoch=None):\n        norm = (4.0 * self.delta_f) / sqrt(template_norm)\n        self.correlators[segnum].correlate()\n        self.ifft.execute()\n        idx, snrv = events.threshold(self.snr_mem[self.segments[segnum].analyze],\n                                     self.snr_threshold / norm)\n        idx, snrv = events.cluster_reduce(idx, snrv, window)\n\n        if len(idx) == 0:\n            return [], [], [], [], []\n\n        logging.info(\"%s points above threshold\" % str(len(idx)))\n\n        snr = TimeSeries(self.snr_mem, epoch=epoch, delta_t=self.delta_t, copy=False)\n        corr = FrequencySeries(self.corr_mem, delta_f=self.delta_f, copy=False)\n        return snr, norm, corr, idx, snrv", "response": "This function calculates the full matched filter and cluster."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef heirarchical_matched_filter_and_cluster(self, segnum, template_norm, window):\n        from pycbc.fft.fftw_pruned import pruned_c2cifft, fft_transpose\n        htilde = self.htilde\n        stilde = self.segments[segnum]\n\n        norm = (4.0 * stilde.delta_f) / sqrt(template_norm)\n\n        correlate(htilde[self.kmin_red:self.kmax_red],\n                  stilde[self.kmin_red:self.kmax_red],\n                  self.corr_mem[self.kmin_red:self.kmax_red])\n\n        ifft(self.corr_mem, self.snr_mem)\n\n        if not hasattr(stilde, 'red_analyze'):\n            stilde.red_analyze = \\\n                             slice(stilde.analyze.start/self.downsample_factor,\n                                   stilde.analyze.stop/self.downsample_factor)\n\n\n        idx_red, snrv_red = events.threshold(self.snr_mem[stilde.red_analyze],\n                                self.snr_threshold / norm * self.upsample_threshold)\n        if len(idx_red) == 0:\n            return [], None, [], [], []\n\n        idx_red, _ = events.cluster_reduce(idx_red, snrv_red, window / self.downsample_factor)\n        logging.info(\"%s points above threshold at reduced resolution\"\\\n                      %(str(len(idx_red)),))\n\n        # The fancy upsampling is here\n        if self.upsample_method=='pruned_fft':\n            idx = (idx_red + stilde.analyze.start/self.downsample_factor)\\\n                   * self.downsample_factor\n\n            idx = smear(idx, self.downsample_factor)\n\n            # cache transposed  versions of htilde and stilde\n            if not hasattr(self.corr_mem_full, 'transposed'):\n                self.corr_mem_full.transposed = zeros(len(self.corr_mem_full), dtype=self.dtype)\n\n            if not hasattr(htilde, 'transposed'):\n                htilde.transposed = zeros(len(self.corr_mem_full), dtype=self.dtype)\n                htilde.transposed[self.kmin_full:self.kmax_full] = htilde[self.kmin_full:self.kmax_full]\n                htilde.transposed = fft_transpose(htilde.transposed)\n\n            if not hasattr(stilde, 'transposed'):\n                stilde.transposed = zeros(len(self.corr_mem_full), dtype=self.dtype)\n                stilde.transposed[self.kmin_full:self.kmax_full] = stilde[self.kmin_full:self.kmax_full]\n                stilde.transposed = fft_transpose(stilde.transposed)\n\n            correlate(htilde.transposed, stilde.transposed, self.corr_mem_full.transposed)\n            snrv = pruned_c2cifft(self.corr_mem_full.transposed, self.inter_vec, idx, pretransposed=True)\n            idx = idx - stilde.analyze.start\n            idx2, snrv = events.threshold(Array(snrv, copy=False), self.snr_threshold / norm)\n\n            if len(idx2) > 0:\n                correlate(htilde[self.kmax_red:self.kmax_full],\n                          stilde[self.kmax_red:self.kmax_full],\n                          self.corr_mem_full[self.kmax_red:self.kmax_full])\n                idx, snrv = events.cluster_reduce(idx[idx2], snrv, window)\n            else:\n                idx, snrv = [], []\n\n            logging.info(\"%s points at full rate and clustering\" % len(idx))\n            return self.snr_mem, norm, self.corr_mem_full, idx, snrv\n        else:\n            raise ValueError(\"Invalid upsample method\")", "response": "Calculates the heirarchical matched filter and cluster for a given set of segments."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef full_matched_filter_and_cluster(self, hplus, hcross, hplus_norm,\n                                        hcross_norm, psd, stilde, window):\n        \"\"\"\n        Return the complex snr and normalization.\n\n        Calculated the matched filter, threshold, and cluster.\n\n        Parameters\n        ----------\n        h_quantities : Various\n            FILL ME IN\n        stilde : FrequencySeries\n            The strain data to be filtered.\n        window : int\n            The size of the cluster window in samples.\n\n        Returns\n        -------\n        snr : TimeSeries\n            A time series containing the complex snr.\n        norm : float\n            The normalization of the complex snr.\n        correlation: FrequencySeries\n            A frequency series containing the correlation vector.\n        idx : Array\n            List of indices of the triggers.\n        snrv : Array\n            The snr values at the trigger locations.\n        \"\"\"\n\n        I_plus, Iplus_corr, Iplus_norm = matched_filter_core(hplus, stilde,\n                                          h_norm=hplus_norm,\n                                          low_frequency_cutoff=self.flow,\n                                          high_frequency_cutoff=self.fhigh,\n                                          out=self.snr_plus_mem,\n                                          corr_out=self.corr_plus_mem)\n\n\n        I_cross, Icross_corr, Icross_norm = matched_filter_core(hcross,\n                                          stilde, h_norm=hcross_norm,\n                                          low_frequency_cutoff=self.flow,\n                                          high_frequency_cutoff=self.fhigh,\n                                          out=self.snr_cross_mem,\n                                          corr_out=self.corr_cross_mem)\n\n        # The information on the complex side of this overlap is important\n        # we may want to use this in the future.\n        if not id(hplus) == self.cached_hplus_hcross_hplus:\n            self.cached_hplus_hcross_correlation = None\n        if not id(hcross) == self.cached_hplus_hcross_hcross:\n            self.cached_hplus_hcross_correlation = None\n        if not id(psd) == self.cached_hplus_hcross_psd:\n            self.cached_hplus_hcross_correlation = None\n        if self.cached_hplus_hcross_correlation is None:\n            hplus_cross_corr = overlap_cplx(hplus, hcross, psd=psd,\n                                           low_frequency_cutoff=self.flow,\n                                           high_frequency_cutoff=self.fhigh,\n                                           normalized=False)\n            hplus_cross_corr = numpy.real(hplus_cross_corr)\n            hplus_cross_corr = hplus_cross_corr / (hcross_norm*hplus_norm)**0.5\n            self.cached_hplus_hcross_correlation = hplus_cross_corr\n            self.cached_hplus_hcross_hplus = id(hplus)\n            self.cached_hplus_hcross_hcross = id(hcross)\n            self.cached_hplus_hcross_psd = id(psd)\n        else:\n            hplus_cross_corr = self.cached_hplus_hcross_correlation\n\n        snr = self._maximized_snr(I_plus,I_cross,\n                                  hplus_cross_corr,\n                                  hpnorm=Iplus_norm,\n                                  hcnorm=Icross_norm,\n                                  out=self.snr_mem,\n                                  thresh=self.snr_threshold,\n                                  analyse_slice=stilde.analyze)\n        # FIXME: This should live further down\n        # Convert output to pycbc TimeSeries\n        delta_t = 1.0 / (self.tlen * stilde.delta_f)\n\n        snr = TimeSeries(snr, epoch=stilde.start_time, delta_t=delta_t,\n                         copy=False)\n\n        idx, snrv = events.threshold_real_numpy(snr[stilde.analyze],\n                                                self.snr_threshold)\n\n        if len(idx) == 0:\n            return [], 0, 0, [], [], [], [], 0, 0, 0\n        logging.info(\"%s points above threshold\", str(len(idx)))\n\n\n        idx, snrv = events.cluster_reduce(idx, snrv, window)\n        logging.info(\"%s clustered points\", str(len(idx)))\n        # erased self.\n        u_vals, coa_phase = self._maximized_extrinsic_params\\\n            (I_plus.data, I_cross.data, hplus_cross_corr,\n             indices=idx+stilde.analyze.start, hpnorm=Iplus_norm,\n             hcnorm=Icross_norm)\n\n\n\n        return snr, Iplus_corr, Icross_corr, idx, snrv, u_vals, coa_phase,\\\n                                      hplus_cross_corr, Iplus_norm, Icross_norm", "response": "This function calculates the full matched filter and cluster for the given complex snr and normalization."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncombine results from different batches of filtering", "response": "def combine_results(self, results):\n        \"\"\"Combine results from different batches of filtering\"\"\"\n        result = {}\n        for key in results[0]:\n            result[key] = numpy.concatenate([r[key] for r in results])\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef process_all(self):\n        results = []\n        veto_info = []\n        while 1:\n            result, veto = self._process_batch()\n            if result is False: return False\n            if result is None: break\n            results.append(result)\n            veto_info += veto\n\n        result = self.combine_results(results)\n\n        if self.max_triggers_in_batch:\n            sort = result['snr'].argsort()[::-1][:self.max_triggers_in_batch]\n            for key in result:\n                result[key] = result[key][sort]\n\n            tmp = veto_info\n            veto_info = [tmp[i] for i in sort]\n\n        result = self._process_vetoes(result, veto_info)\n        return result", "response": "Process every batch group and return as single result"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates signal based vetoes", "response": "def _process_vetoes(self, results, veto_info):\n        \"\"\"Calculate signal based vetoes\"\"\"\n        chisq = numpy.array(numpy.zeros(len(veto_info)), numpy.float32, ndmin=1)\n        dof = numpy.array(numpy.zeros(len(veto_info)), numpy.uint32, ndmin=1)\n        sg_chisq = numpy.array(numpy.zeros(len(veto_info)), numpy.float32,\n                               ndmin=1)\n        results['chisq'] = chisq\n        results['chisq_dof'] = dof\n        results['sg_chisq'] = sg_chisq\n\n        keep = []\n        for i, (snrv, norm, l, htilde, stilde) in enumerate(veto_info):\n            correlate(htilde, stilde, htilde.cout)\n            c, d = self.power_chisq.values(htilde.cout, snrv,\n                                           norm, stilde.psd, [l], htilde)\n            chisq[i] = c[0] / d[0]\n            dof[i] = d[0]\n\n            sgv = self.sg_chisq.values(stilde, htilde, stilde.psd,\n                                       snrv, norm, c, d, [l])\n            if sgv is not None:\n                sg_chisq[i] = sgv[0]\n\n            if self.newsnr_threshold:\n                newsnr = ranking.newsnr(results['snr'][i], chisq[i])\n                if newsnr >= self.newsnr_threshold:\n                    keep.append(i)\n\n        if self.newsnr_threshold:\n            keep = numpy.array(keep, dtype=numpy.uint32)\n            for key in results:\n                results[key] = results[key][keep]\n\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _process_batch(self):\n        if self.block_id == len(self.tgroups):\n            return None, None\n\n        tgroup = self.tgroups[self.block_id]\n        psize = self.chunk_tsamples[self.block_id]\n        mid = self.mids[self.block_id]\n        stilde = self.data.overwhitened_data(tgroup[0].delta_f)\n        psd = stilde.psd\n\n        valid_end = int(psize - self.data.trim_padding)\n        valid_start = int(valid_end - self.data.blocksize * self.data.sample_rate)\n\n        seg = slice(valid_start, valid_end)\n\n        self.corr[self.block_id].execute(stilde)\n        self.ifts[mid].execute()\n\n        self.block_id += 1\n\n        snr = numpy.zeros(len(tgroup), dtype=numpy.complex64)\n        time = numpy.zeros(len(tgroup), dtype=numpy.float64)\n        templates = numpy.zeros(len(tgroup), dtype=numpy.uint64)\n        sigmasq = numpy.zeros(len(tgroup), dtype=numpy.float32)\n\n        time[:] = self.data.start_time\n\n        result = {}\n        tkeys = tgroup[0].params.dtype.names\n        for key in tkeys:\n            result[key] = []\n\n        veto_info = []\n\n        # Find the peaks in our SNR times series from the various templates\n        i = 0\n        for htilde in tgroup:\n            l = htilde.out[seg].abs_arg_max()\n\n            sgm = htilde.sigmasq(psd)\n            norm = 4.0 * htilde.delta_f / (sgm ** 0.5)\n\n            l += valid_start\n            snrv = numpy.array([htilde.out[l]])\n\n            # If nothing is above threshold we can exit this template\n            s = abs(snrv[0]) * norm\n            if s < self.snr_threshold:\n                continue\n\n            time[i] += float(l - valid_start) / self.data.sample_rate\n\n            # We have an SNR so high that we will drop the entire analysis\n            # of this chunk of time!\n            if self.snr_abort_threshold is not None and s > self.snr_abort_threshold:\n                logging.info(\"We are seeing some *really* high SNRs, lets\"\n                             \" assume they aren't signals and just give up\")\n                return False, []\n\n            veto_info.append((snrv, norm, l, htilde, stilde))\n\n            snr[i] = snrv[0] * norm\n            sigmasq[i] = sgm\n            templates[i] = htilde.id\n            if not hasattr(htilde, 'dict_params'):\n                htilde.dict_params = {}\n                for key in tkeys:\n                    htilde.dict_params[key] = htilde.params[key]\n\n            for key in tkeys:\n                result[key].append(htilde.dict_params[key])\n            i += 1\n\n\n        result['snr'] = abs(snr[0:i])\n        result['coa_phase'] = numpy.angle(snr[0:i])\n        result['end_time'] = time[0:i]\n        result['template_id'] = templates[0:i]\n        result['sigmasq'] = sigmasq[0:i]\n\n        for key in tkeys:\n            result[key] = numpy.array(result[key])\n\n        return result, veto_info", "response": "Process a single batch of data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef calc_psd_variation(strain, psd_short_segment, psd_long_segment,\n                       short_psd_duration, short_psd_stride, psd_avg_method,\n                       low_freq, high_freq):\n    \"\"\"Calculates time series of PSD variability\n\n    This function first splits the segment up into 512 second chunks. It\n    then calculates the PSD over this 512 second period as well as in 4\n    second chunks throughout each 512 second period. Next the function\n    estimates how different the 4 second PSD is to the 512 second PSD and\n    produces a timeseries of this variability.\n\n    Parameters\n    ----------\n    strain : TimeSeries\n        Input strain time series to estimate PSDs\n    psd_short_segment : {float, 8}\n        Duration of the short segments for PSD estimation in seconds.\n    psd_long_segment : {float, 512}\n        Duration of the long segments for PSD estimation in seconds.\n    short_psd_duration : {float, 4}\n        Duration of the segments for PSD estimation in seconds.\n    short_psd_stride : {float, 2}\n        Separation between PSD estimation segments in seconds.\n    psd_avg_method : {string, 'median'}\n        Method for averaging PSD estimation segments.\n    low_freq : {float, 20}\n        Minimum frequency to consider the comparison between PSDs.\n    high_freq : {float, 480}\n        Maximum frequency to consider the comparison between PSDs.\n\n    Returns\n    -------\n    psd_var : TimeSeries\n        Time series of the variability in the PSD estimation\n    \"\"\"\n\n    # Calculate strain precision\n    if strain.precision == 'single':\n        fs_dtype = numpy.float32\n    elif strain.precision == 'double':\n        fs_dtype = numpy.float64\n\n    # Convert start and end times immediately to floats\n    start_time = numpy.float(strain.start_time)\n    end_time = numpy.float(strain.end_time)\n\n    # Find the times of the long segments\n    times_long = numpy.arange(start_time, end_time, psd_long_segment)\n\n    # Set up the empty time series for the PSD variation estimate\n    psd_var = TimeSeries(zeros(int(numpy.ceil((end_time -\n                                   start_time) / psd_short_segment))),\n                         delta_t=psd_short_segment, copy=False,\n                         epoch=start_time)\n\n    ind = 0\n    for tlong in times_long:\n        # Calculate PSD for long segment and separate the long segment in to\n        # overlapping shorter segments\n        if tlong + psd_long_segment <= end_time:\n            psd_long = pycbc.psd.welch(\n                           strain.time_slice(tlong, tlong + psd_long_segment),\n                           seg_len=int(short_psd_duration * strain.sample_rate),\n                           seg_stride=int(short_psd_stride *\n                                          strain.sample_rate),\n                           avg_method=psd_avg_method)\n            times_short = numpy.arange(tlong, tlong + psd_long_segment,\n                                       psd_short_segment)\n        else:\n            psd_long = pycbc.psd.welch(\n                           strain.time_slice(end_time - psd_long_segment,\n                                             end_time),\n                           seg_len=int(short_psd_duration * strain.sample_rate),\n                           seg_stride=int(short_psd_stride *\n                                          strain.sample_rate),\n                           avg_method=psd_avg_method)\n            times_short = numpy.arange(tlong, end_time, psd_short_segment)\n\n        # Calculate the PSD of the shorter segments\n        psd_short = []\n        for tshort in times_short:\n            if tshort + psd_short_segment <= end_time:\n                pshort = pycbc.psd.welch(\n                            strain.time_slice(tshort, tshort +\n                                              psd_short_segment),\n                            seg_len=int(short_psd_duration *\n                                        strain.sample_rate),\n                            seg_stride=int(short_psd_stride *\n                                           strain.sample_rate),\n                            avg_method=psd_avg_method)\n            else:\n                pshort = pycbc.psd.welch(\n                            strain.time_slice(tshort - psd_short_segment,\n                                              end_time),\n                            seg_len=int(short_psd_duration *\n                                        strain.sample_rate),\n                            seg_stride=int(short_psd_stride *\n                                           strain.sample_rate),\n                            avg_method=psd_avg_method)\n            psd_short.append(pshort)\n\n        # Estimate the range of the PSD to compare\n        kmin = int(low_freq / psd_long.delta_f)\n        kmax = int(high_freq / psd_long.delta_f)\n        # Comapre the PSD of the short segment to the long segment\n        # The weight factor gives the rough response of a cbc template across\n        # the defined frequency range given the expected PSD (i.e. long PSD)\n        # Then integrate the weighted ratio of the actual PSD (i.e. short PSD)\n        # with the expected PSD (i.e. long PSD) over the specified frequency\n        # range\n        freqs = FrequencySeries(psd_long.sample_frequencies,\n                                delta_f=psd_long.delta_f,\n                                epoch=psd_long.epoch, dtype=fs_dtype)\n        weight = numpy.array(\n                     freqs[kmin:kmax]**(-7./3.) / psd_long[kmin:kmax])\n        weight /= weight.sum()\n        diff = numpy.array([(weight * numpy.array(p_short[kmin:kmax] /\n                             psd_long[kmin:kmax])).sum()\n                             for p_short in psd_short])\n\n        # Store variation value\n        for i, val in enumerate(diff):\n            psd_var[ind+i] = val\n\n        ind = ind+len(diff)\n\n    return psd_var", "response": "Calculates the PSD variation of a given strain time series."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds the PSD variation value at a particular time", "response": "def find_trigger_value(psd_var, idx, start, sample_rate):\n    \"\"\" Find the PSD variation value at a particular time\n\n    Parameters\n    ----------\n    psd_var : TimeSeries\n        Time series of the varaibility in the PSD estimation\n    idx : numpy.ndarray\n        Time indices of the triggers\n    start : float\n        GPS start time\n    sample_rate : float\n        Sample rate defined in ini file\n\n    Returns\n    -------\n    vals : Array\n        PSD variation value at a particular time\n    \"\"\"\n\n    # Find gps time of the trigger\n    time = start + idx / sample_rate\n    # Find where in the psd variation time series the trigger belongs\n    ind = numpy.digitize(time, psd_var.sample_times)\n    ind -= 1\n    vals = psd_var[ind]\n    return vals"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setup_foreground_minifollowups(workflow, coinc_file, single_triggers,\n                       tmpltbank_file, insp_segs, insp_data_name,\n                       insp_anal_name, dax_output, out_dir, tags=None):\n    \"\"\" Create plots that followup the Nth loudest coincident injection\n    from a statmap produced HDF file.\n\n    Parameters\n    ----------\n    workflow: pycbc.workflow.Workflow\n        The core workflow instance we are populating\n    coinc_file:\n    single_triggers: list of pycbc.workflow.File\n        A list cointaining the file objects associated with the merged\n        single detector trigger files for each ifo.\n    tmpltbank_file: pycbc.workflow.File\n        The file object pointing to the HDF format template bank\n    insp_segs: SegFile\n       The segment file containing the data read and analyzed by each inspiral\n       job.\n    insp_data_name: str\n        The name of the segmentlist storing data read.\n    insp_anal_name: str\n        The name of the segmentlist storing data analyzed.\n    out_dir: path\n        The directory to store minifollowups result plots and files\n    tags: {None, optional}\n        Tags to add to the minifollowups executables\n\n    Returns\n    -------\n    layout: list\n        A list of tuples which specify the displayed file layout for the\n        minifollops plots.\n    \"\"\"\n    logging.info('Entering minifollowups module')\n\n    if not workflow.cp.has_section('workflow-minifollowups'):\n        logging.info('There is no [workflow-minifollowups] section in configuration file')\n        logging.info('Leaving minifollowups')\n        return\n\n    tags = [] if tags is None else tags\n    makedir(dax_output)\n\n    # turn the config file into a File class\n    config_path = os.path.abspath(dax_output + '/' + '_'.join(tags) + 'foreground_minifollowup.ini')\n    workflow.cp.write(open(config_path, 'w'))\n\n    config_file = wdax.File(os.path.basename(config_path))\n    config_file.PFN(urlparse.urljoin('file:', urllib.pathname2url(config_path)),\n                    site='local')\n\n    exe = Executable(workflow.cp, 'foreground_minifollowup', ifos=workflow.ifos, out_dir=dax_output)\n\n    node = exe.create_node()\n    node.add_input_opt('--config-files', config_file)\n    node.add_input_opt('--bank-file', tmpltbank_file)\n    node.add_input_opt('--statmap-file', coinc_file)\n    node.add_multiifo_input_list_opt('--single-detector-triggers', single_triggers)\n    node.add_input_opt('--inspiral-segments', insp_segs)\n    node.add_opt('--inspiral-data-read-name', insp_data_name)\n    node.add_opt('--inspiral-data-analyzed-name', insp_anal_name)\n    node.new_output_file_opt(workflow.analysis_time, '.dax', '--output-file', tags=tags)\n    node.new_output_file_opt(workflow.analysis_time, '.dax.map', '--output-map', tags=tags)\n    node.new_output_file_opt(workflow.analysis_time, '.tc.txt', '--transformation-catalog', tags=tags)\n\n    name = node.output_files[0].name\n    map_file = node.output_files[1]\n    tc_file = node.output_files[2]\n\n    node.add_opt('--workflow-name', name)\n    node.add_opt('--output-dir', out_dir)\n\n    workflow += node\n\n    # execute this in a sub-workflow\n    fil = node.output_files[0]\n\n    # determine if a staging site has been specified\n    try:\n        staging_site = workflow.cp.get('workflow-foreground_minifollowups',\n                                       'staging-site')\n    except:\n        staging_site = None\n\n    job = dax.DAX(fil)\n    job.addArguments('--basename %s' % os.path.splitext(os.path.basename(name))[0])\n    Workflow.set_job_properties(job, map_file, tc_file, staging_site=staging_site)\n    workflow._adag.addJob(job)\n    dep = dax.Dependency(parent=node._dax_node, child=job)\n    workflow._adag.addDependency(dep)\n    logging.info('Leaving minifollowups module')", "response": "Create the minifollowups plots that followup the Nth loudest coincident injection from a statmap produced by each ifo."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_single_template_plots(workflow, segs, data_read_name, analyzed_name,\n                                  params, out_dir, inj_file=None, exclude=None,\n                                  require=None, tags=None, params_str=None,\n                                  use_exact_inj_params=False):\n    \"\"\"Function for creating jobs to run the pycbc_single_template code and\n    to run the associated plotting code pycbc_single_template_plots and add\n    these jobs to the workflow.\n\n    Parameters\n    -----------\n    workflow : workflow.Workflow instance\n        The pycbc.workflow.Workflow instance to add these jobs to.\n    segs : workflow.File instance\n        The pycbc.workflow.File instance that points to the XML file containing\n        the segment lists of data read in and data analyzed.\n    data_read_name : str\n        The name of the segmentlist containing the data read in by each\n        inspiral job in the segs file.\n    analyzed_name : str\n        The name of the segmentlist containing the data analyzed by each\n        inspiral job in the segs file.\n    params : dictionary\n        A dictionary containing the parameters of the template to be used.\n        params[ifo+'end_time'] is required for all ifos in workflow.ifos.\n        If use_exact_inj_params is False then also need to supply values for\n        [mass1, mass2, spin1z, spin2x]. For precessing templates one also\n        needs to supply [spin1y, spin1x, spin2x, spin2y, inclination]\n        additionally for precession one must supply u_vals or\n        u_vals_+ifo for all ifos. u_vals is the ratio between h_+ and h_x to\n        use when constructing h(t). h(t) = (h_+ * u_vals) + h_x.\n    out_dir : str\n        Directory in which to store the output files.\n    inj_file : workflow.File (optional, default=None)\n        If given send this injection file to the job so that injections are\n        made into the data.\n    exclude : list (optional, default=None)\n        If given, then when considering which subsections in the ini file to\n        parse for options to add to single_template_plot, only use subsections\n        that *do not* match strings in this list.\n    require : list (optional, default=None)\n        If given, then when considering which subsections in the ini file to\n        parse for options to add to single_template_plot, only use subsections\n        matching strings in this list.\n    tags : list (optional, default=None)\n        Add this list of tags to all jobs.\n    params_str : str (optional, default=None)\n        If given add this string to plot title and caption to describe the\n        template that was used.\n    use_exact_inj_params : boolean (optional, default=False)\n        If True do not use masses and spins listed in the params dictionary\n        but instead use the injection closest to the filter time as a template.\n\n    Returns\n    --------\n    output_files : workflow.FileList\n        The list of workflow.Files created in this function.\n    \"\"\"\n    tags = [] if tags is None else tags\n    makedir(out_dir)\n    name = 'single_template_plot'\n    secs = requirestr(workflow.cp.get_subsections(name), require)\n    secs = excludestr(secs, exclude)\n    files = FileList([])\n    for tag in secs:\n        for ifo in workflow.ifos:\n            if params['%s_end_time' % ifo] == -1.0:\n                continue\n            # Reanalyze the time around the trigger in each detector\n            node = SingleTemplateExecutable(workflow.cp, 'single_template',\n                                            ifos=[ifo], out_dir=out_dir,\n                                            tags=[tag] + tags).create_node()\n            if use_exact_inj_params:\n                node.add_opt('--use-params-of-closest-injection')\n            else:\n                node.add_opt('--mass1', \"%.6f\" % params['mass1'])\n                node.add_opt('--mass2', \"%.6f\" % params['mass2'])\n                node.add_opt('--spin1z',\"%.6f\" % params['spin1z'])\n                node.add_opt('--spin2z',\"%.6f\" % params['spin2z'])\n                node.add_opt('--template-start-frequency',\n                             \"%.6f\" % params['f_lower'])\n                # Is this precessing?\n                if 'u_vals' in params or 'u_vals_%s' % ifo in params:\n                    node.add_opt('--spin1x',\"%.6f\" % params['spin1x'])\n                    node.add_opt('--spin1y',\"%.6f\" % params['spin1y'])\n                    node.add_opt('--spin2x',\"%.6f\" % params['spin2x'])\n                    node.add_opt('--spin2y',\"%.6f\" % params['spin2y'])\n                    node.add_opt('--inclination',\"%.6f\" % params['inclination'])\n                    try:\n                        node.add_opt('--u-val',\"%.6f\" % params['u_vals'])\n                    except:\n                        node.add_opt('--u-val',\n                                     \"%.6f\" % params['u_vals_%s' % ifo])\n\n            # str(numpy.float64) restricts to 2d.p. BE CAREFUL WITH THIS!!!\n            str_trig_time = '%.6f' %(params[ifo + '_end_time'])\n            node.add_opt('--trigger-time', str_trig_time)\n            node.add_input_opt('--inspiral-segments', segs)\n            if inj_file is not None:\n                node.add_input_opt('--injection-file', inj_file)\n            node.add_opt('--data-read-name', data_read_name)\n            node.add_opt('--data-analyzed-name', analyzed_name)\n            node.new_output_file_opt(workflow.analysis_time, '.hdf',\n                                     '--output-file', store_file=False)\n            data = node.output_files[0]\n            workflow += node\n            # Make the plot for this trigger and detector\n            node = PlotExecutable(workflow.cp, name, ifos=[ifo],\n                              out_dir=out_dir, tags=[tag] + tags).create_node()\n            node.add_input_opt('--single-template-file', data)\n            node.new_output_file_opt(workflow.analysis_time, '.png',\n                                     '--output-file')\n            title=\"'%s SNR and chi^2 timeseries\" %(ifo)\n            if params_str is not None:\n                title+= \" using %s\" %(params_str)\n            title+=\"'\"\n            node.add_opt('--plot-title', title)\n            caption = \"'The SNR and chi^2 timeseries around the injection\"\n            if params_str is not None:\n                caption += \" using %s\" %(params_str)\n            if use_exact_inj_params:\n                caption += \". The injection itself was used as the template.'\"\n            else:\n                caption += \". The template used has the following parameters: \"\n                caption += \"mass1=%s, mass2=%s, spin1z=%s, spin2z=%s'\"\\\n                       %(params['mass1'], params['mass2'], params['spin1z'],\n                         params['spin2z'])\n            node.add_opt('--plot-caption', caption)\n            workflow += node\n            files += node.output_files\n    return files", "response": "Function that creates the plot files for a single template."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_plot_waveform_plot(workflow, params, out_dir, ifos, exclude=None,\n                            require=None, tags=None):\n    \"\"\" Add plot_waveform jobs to the workflow.\n    \"\"\"\n    tags = [] if tags is None else tags\n    makedir(out_dir)\n    name = 'single_template_plot'\n    secs = requirestr(workflow.cp.get_subsections(name), require)\n    secs = excludestr(secs, exclude)\n    files = FileList([])\n    for tag in secs:\n        node = PlotExecutable(workflow.cp, 'plot_waveform', ifos=ifos,\n                              out_dir=out_dir, tags=[tag] + tags).create_node()\n        node.add_opt('--mass1', \"%.6f\" % params['mass1'])\n        node.add_opt('--mass2', \"%.6f\" % params['mass2'])\n        node.add_opt('--spin1z',\"%.6f\" % params['spin1z'])\n        node.add_opt('--spin2z',\"%.6f\" % params['spin2z'])\n        if 'u_vals' in params:\n            # Precessing options\n            node.add_opt('--spin1x',\"%.6f\" % params['spin1x'])\n            node.add_opt('--spin2x',\"%.6f\" % params['spin2x'])\n            node.add_opt('--spin1y',\"%.6f\" % params['spin1y'])\n            node.add_opt('--spin2y',\"%.6f\" % params['spin2y'])\n            node.add_opt('--inclination',\"%.6f\" % params['inclination'])\n            node.add_opt('--u-val', \"%.6f\" % params['u_vals'])\n        node.new_output_file_opt(workflow.analysis_time, '.png',\n                                     '--output-file')\n        workflow += node\n        files += node.output_files\n    return files", "response": "Create a single template plot for each section of the workflow."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_qscan_plot(workflow, ifo, trig_time, out_dir, injection_file=None,\n                    data_segments=None, time_window=100, tags=None):\n    \"\"\" Generate a make_qscan node and add it to workflow.\n\n    This function generates a single node of the singles_timefreq executable\n    and adds it to the current workflow. Parent/child relationships are set by\n    the input/output files automatically.\n\n    Parameters\n    -----------\n    workflow: pycbc.workflow.core.Workflow\n        The workflow class that stores the jobs that will be run.\n    ifo: str\n        Which interferometer are we using?\n    trig_time: int\n        The time of the trigger being followed up.\n    out_dir: str\n        Location of directory to output to\n    injection_file: pycbc.workflow.File (optional, default=None)\n        If given, add the injections in the file to strain before making the\n        plot.\n    data_segments: ligo.segments.segmentlist (optional, default=None)\n        The list of segments for which data exists and can be read in. If given\n        the start/end times given to singles_timefreq will be adjusted if\n        [trig_time - time_window, trig_time + time_window] does not completely\n        lie within a valid data segment. A ValueError will be raised if the\n        trig_time is not within a valid segment, or if it is not possible to\n        find 2*time_window (plus the padding) of continuous data around the\n        trigger. This **must** be coalesced.\n    time_window: int (optional, default=None)\n        The amount of data (not including padding) that will be read in by the\n        singles_timefreq job. The default value of 100s should be fine for most\n        cases.\n    tags: list (optional, default=None)\n        List of tags to add to the created nodes, which determine file naming.\n    \"\"\"\n    tags = [] if tags is None else tags\n    makedir(out_dir)\n    name = 'plot_qscan'\n\n    curr_exe = PlotQScanExecutable(workflow.cp, name, ifos=[ifo],\n                          out_dir=out_dir, tags=tags)\n    node = curr_exe.create_node()\n\n    # Determine start/end times, using data segments if needed.\n    # Begin by choosing \"optimal\" times\n    start = trig_time - time_window\n    end = trig_time + time_window\n    # Then if data_segments is available, check against that, and move if\n    # needed\n    if data_segments is not None:\n        # Assumes coalesced, so trig_time can only be within one segment\n        for seg in data_segments:\n            if trig_time in seg:\n                data_seg = seg\n                break\n            elif trig_time == -1.0:\n                node.add_opt('--gps-start-time', int(trig_time))\n                node.add_opt('--gps-end-time', int(trig_time))\n                node.add_opt('--center-time', trig_time)\n                caption_string = \"'No trigger in %s'\" % ifo\n                node.add_opt('--plot-caption', caption_string)\n                node.new_output_file_opt(workflow.analysis_time, '.png', '--output-file')\n                workflow += node\n                return node.output_files\n        else:\n            err_msg = \"Trig time {} \".format(trig_time)\n            err_msg += \"does not seem to lie within any data segments. \"\n            err_msg += \"This shouldn't be possible, please ask for help!\"\n            raise ValueError(err_msg)\n        # Check for pad-data\n        if curr_exe.has_opt('pad-data'):\n            pad_data = int(curr_exe.get_opt('pad-data'))\n        else:\n            pad_data = 0\n        # We only read data that's available. The code must handle the case\n        # of not much data being available.\n        if end > (data_seg[1] - pad_data):\n            end = data_seg[1] - pad_data\n        if start < (data_seg[0] + pad_data):\n            start = data_seg[0] + pad_data\n\n    node.add_opt('--gps-start-time', int(start))\n    node.add_opt('--gps-end-time', int(end))\n    node.add_opt('--center-time', trig_time)\n\n    if injection_file is not None:\n        node.add_input_opt('--injection-file', injection_file)\n\n    node.new_output_file_opt(workflow.analysis_time, '.png', '--output-file')\n    workflow += node\n    return node.output_files", "response": "This function generates a single node of the singles_timefreq executable and adds it to the current workflow."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a singles_timefreq node and add it to workflow. This function generates a single node of the singles_timefreq executable and adds it to the current workflow. Parent/child relationships are set by the input/output files automatically. Parameters ----------- workflow: pycbc.workflow.core.Workflow The workflow class that stores the jobs that will be run. single: pycbc.workflow.core.File instance The File object storing the single-detector triggers to followup. bank_file: pycbc.workflow.core.File instance The File object storing the template bank. trig_time: int The time of the trigger being followed up. out_dir: str Location of directory to output to veto_file: pycbc.workflow.core.File (optional, default=None) If given use this file to veto triggers to determine the loudest event. FIXME: Veto files *should* be provided a definer argument and not just assume that all segments should be read. time_window: int (optional, default=None) The amount of data (not including padding) that will be read in by the singles_timefreq job. The default value of 10s should be fine for most cases. data_segments: ligo.segments.segmentlist (optional, default=None) The list of segments for which data exists and can be read in. If given the start/end times given to singles_timefreq will be adjusted if [trig_time - time_window, trig_time + time_window] does not completely lie within a valid data segment. A ValueError will be raised if the trig_time is not within a valid segment, or if it is not possible to find 2*time_window (plus the padding) of continuous data around the trigger. This **must** be coalesced. tags: list (optional, default=None) List of tags to add to the created nodes, which determine file naming.", "response": "def make_singles_timefreq(workflow, single, bank_file, trig_time, out_dir,\n                          veto_file=None, time_window=10, data_segments=None,\n                          tags=None):\n    \"\"\" Generate a singles_timefreq node and add it to workflow.\n\n    This function generates a single node of the singles_timefreq executable\n    and adds it to the current workflow. Parent/child relationships are set by\n    the input/output files automatically.\n\n    Parameters\n    -----------\n    workflow: pycbc.workflow.core.Workflow\n        The workflow class that stores the jobs that will be run.\n    single: pycbc.workflow.core.File instance\n        The File object storing the single-detector triggers to followup.\n    bank_file: pycbc.workflow.core.File instance\n        The File object storing the template bank.\n    trig_time: int\n        The time of the trigger being followed up.\n    out_dir: str\n        Location of directory to output to\n    veto_file: pycbc.workflow.core.File (optional, default=None)\n        If given use this file to veto triggers to determine the loudest event.\n        FIXME: Veto files *should* be provided a definer argument and not just\n        assume that all segments should be read.\n    time_window: int (optional, default=None)\n        The amount of data (not including padding) that will be read in by the\n        singles_timefreq job. The default value of 10s should be fine for most\n        cases.\n    data_segments: ligo.segments.segmentlist (optional, default=None)\n        The list of segments for which data exists and can be read in. If given\n        the start/end times given to singles_timefreq will be adjusted if\n        [trig_time - time_window, trig_time + time_window] does not completely\n        lie within a valid data segment. A ValueError will be raised if the\n        trig_time is not within a valid segment, or if it is not possible to\n        find 2*time_window (plus the padding) of continuous data around the\n        trigger. This **must** be coalesced.\n    tags: list (optional, default=None)\n        List of tags to add to the created nodes, which determine file naming.\n    \"\"\"\n    tags = [] if tags is None else tags\n    makedir(out_dir)\n    name = 'plot_singles_timefreq'\n\n    curr_exe = SingleTimeFreqExecutable(workflow.cp, name, ifos=[single.ifo],\n                          out_dir=out_dir, tags=tags)\n    node = curr_exe.create_node()\n    node.add_input_opt('--trig-file', single)\n    node.add_input_opt('--bank-file', bank_file)\n\n    # Determine start/end times, using data segments if needed.\n    # Begin by choosing \"optimal\" times\n    start = trig_time - time_window\n    end = trig_time + time_window\n    # Then if data_segments is available, check against that, and move if\n    # needed\n    if data_segments is not None:\n        # Assumes coalesced, so trig_time can only be within one segment\n        for seg in data_segments:\n            if trig_time in seg:\n                data_seg = seg\n                break\n            elif trig_time == -1.0:\n                node.add_opt('--gps-start-time', int(trig_time))\n                node.add_opt('--gps-end-time', int(trig_time))\n                node.add_opt('--center-time', trig_time)\n\n                if veto_file:\n                    node.add_input_opt('--veto-file', veto_file)\n\n                node.add_opt('--detector', single.ifo)\n                node.new_output_file_opt(workflow.analysis_time, '.png', '--output-file')\n                workflow += node\n                return node.output_files\n        else:\n            err_msg = \"Trig time {} \".format(trig_time)\n            err_msg += \"does not seem to lie within any data segments. \"\n            err_msg += \"This shouldn't be possible, please ask for help!\"\n            raise ValueError(err_msg)\n        # Check for pad-data\n        if curr_exe.has_opt('pad-data'):\n            pad_data = int(curr_exe.get_opt('pad-data'))\n        else:\n            pad_data = 0\n        if abs(data_seg) < (2 * time_window + 2 * pad_data):\n            tl = 2 * time_window + 2 * pad_data\n            err_msg = \"I was asked to use {} seconds of data \".format(tl)\n            err_msg += \"to run a plot_singles_timefreq job. However, I have \"\n            err_msg += \"only {} seconds available.\".format(abs(data_seg))\n            raise ValueError(err_msg)\n        if data_seg[0] > (start - pad_data):\n            start = data_seg[0] + pad_data\n            end = start + 2 * time_window\n        if data_seg[1] < (end + pad_data):\n            end = data_seg[1] - pad_data\n            start = end - 2 * time_window\n        # Sanity check, shouldn't get here!\n        if data_seg[0] > (start - pad_data):\n            err_msg = \"I shouldn't be here! Go ask Ian what he broke.\"\n            raise ValueError(err_msg)\n\n    node.add_opt('--gps-start-time', int(start))\n    node.add_opt('--gps-end-time', int(end))\n    node.add_opt('--center-time', trig_time)\n\n    if veto_file:\n        node.add_input_opt('--veto-file', veto_file)\n\n    node.add_opt('--detector', single.ifo)\n    node.new_output_file_opt(workflow.analysis_time, '.png', '--output-file')\n    workflow += node\n    return node.output_files"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a noop node that can be added to a DAX doing nothing.", "response": "def create_noop_node():\n    \"\"\"\n    Creates a noop node that can be added to a DAX doing nothing. The reason\n    for using this is if a minifollowups dax contains no triggers currently\n    the dax will contain no jobs and be invalid. By adding a noop node we\n    ensure that such daxes will actually run if one adds one such noop node.\n    Adding such a noop node into a workflow *more than once* will cause a\n    failure.\n    \"\"\"\n    exe = wdax.Executable('NOOP')\n    pfn = distutils.spawn.find_executable('true')\n    exe.add_pfn(pfn)\n    node = wdax.Node(exe)\n    return node"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\napplying each distributions s boundary conditions to the given list of parameters returning a new list with the conditions applied.", "response": "def apply_boundary_conditions(self, **params):\n        \"\"\"Applies each distributions' boundary conditions to the given list\n        of parameters, returning a new list with the conditions applied.\n\n        Parameters\n        ----------\n        **params :\n            Keyword arguments should give the parameters to apply the\n            conditions to.\n\n        Returns\n        -------\n        dict\n            A dictionary of the parameters after each distribution's\n            `apply_boundary_conditions` function has been applied.\n        \"\"\"\n        for dist in self.distributions:\n            params.update(dist.apply_boundary_conditions(**params))\n        return params"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _check_lal_pars(p):\n    lal_pars = lal.CreateDict()\n    #nonGRparams can be straightforwardly added if needed, however they have to\n    # be invoked one by one\n    if p['phase_order']!=-1:\n        lalsimulation.SimInspiralWaveformParamsInsertPNPhaseOrder(lal_pars,int(p['phase_order']))\n    if p['amplitude_order']!=-1:\n        lalsimulation.SimInspiralWaveformParamsInsertPNAmplitudeOrder(lal_pars,int(p['amplitude_order']))\n    if p['spin_order']!=-1:\n        lalsimulation.SimInspiralWaveformParamsInsertPNSpinOrder(lal_pars,int(p['spin_order']))\n    if p['tidal_order']!=-1:\n        lalsimulation.SimInspiralWaveformParamsInsertPNTidalOrder(lal_pars, p['tidal_order'])\n    if p['eccentricity_order']!=-1:\n        lalsimulation.SimInspiralWaveformParamsInsertPNEccentricityOrder(lal_pars, p['eccentricity_order'])\n    if p['lambda1'] is not None:\n        lalsimulation.SimInspiralWaveformParamsInsertTidalLambda1(lal_pars, p['lambda1'])\n    if p['lambda2'] is not None:\n        lalsimulation.SimInspiralWaveformParamsInsertTidalLambda2(lal_pars, p['lambda2'])\n    if p['lambda_octu1'] is not None:\n        lalsimulation.SimInspiralWaveformParamsInsertTidalOctupolarLambda1(lal_pars, p['lambda_octu1'])\n    if p['lambda_octu2'] is not None:\n        lalsimulation.SimInspiralWaveformParamsInsertTidalOctupolarLambda2(lal_pars, p['lambda_octu2'])\n    if p['quadfmode1'] is not None:\n        lalsimulation.SimInspiralWaveformParamsInsertTidalQuadrupolarFMode1(lal_pars, p['quadfmode1'])\n    if p['quadfmode2'] is not None:\n        lalsimulation.SimInspiralWaveformParamsInsertTidalQuadrupolarFMode2(lal_pars, p['quadfmode2'])\n    if p['octufmode1'] is not None:\n        lalsimulation.SimInspiralWaveformParamsInsertTidalOctupolarFMode1(lal_pars, p['octufmode1'])\n    if p['octufmode2'] is not None:\n        lalsimulation.SimInspiralWaveformParamsInsertTidalOctupolarFMode2(lal_pars, p['octufmode2'])\n    if p['dquad_mon1'] is not None:\n        lalsimulation.SimInspiralWaveformParamsInsertdQuadMon1(lal_pars, p['dquad_mon1'])\n    if p['dquad_mon2'] is not None:\n        lalsimulation.SimInspiralWaveformParamsInsertdQuadMon2(lal_pars, p['dquad_mon2'])\n    if p['numrel_data']:\n        lalsimulation.SimInspiralWaveformParamsInsertNumRelData(lal_pars, str(p['numrel_data']))\n    if p['modes_choice']:\n        lalsimulation.SimInspiralWaveformParamsInsertModesChoice(lal_pars, p['modes_choice'])\n    if p['frame_axis']:\n        lalsimulation.SimInspiralWaveformParamsInsertFrameAxis(lal_pars, p['frame_axis'])\n    if p['side_bands']:\n        lalsimulation.SimInspiralWaveformParamsInsertSideband(lal_pars, p['side_bands'])\n    if p['mode_array'] is not None:\n        ma = lalsimulation.SimInspiralCreateModeArray()\n        for l,m in p['mode_array']:\n            lalsimulation.SimInspiralModeArrayActivateMode(ma, l, m)\n        lalsimulation.SimInspiralWaveformParamsInsertModeArray(lal_pars, ma)\n\n    return lal_pars", "response": "Create a LalDict object from the dictionary of waveform parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _spintaylor_aligned_prec_swapper(**p):\n    orig_approximant = p['approximant']\n    if p['spin2x'] == 0 and p['spin2y'] == 0 and p['spin1x'] == 0 and \\\n                                                              p['spin1y'] == 0:\n        p['approximant'] = 'TaylorF2'\n    else:\n        p['approximant'] = 'SpinTaylorF2'\n    hp, hc = _lalsim_fd_waveform(**p)\n    p['approximant'] = orig_approximant\n    return hp, hc", "response": "This function is used to create a precessing SpinTaylorF2 model for a single spin - aligned SpinTaylorF2 model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a dictionary built from the attributes of the given object.", "response": "def get_obj_attrs(obj):\n    \"\"\" Return a dictionary built from the attributes of the given object.\n    \"\"\"\n    pr = {}\n    if obj is not None:\n        if isinstance(obj, numpy.core.records.record):\n            for name in obj.dtype.names:\n                pr[name] = getattr(obj, name)\n        elif hasattr(obj, '__dict__') and obj.__dict__:\n            pr = obj.__dict__\n        elif hasattr(obj, '__slots__'):\n            for slot in obj.__slots__:\n                if hasattr(obj, slot):\n                    pr[slot] = getattr(obj, slot)\n        elif isinstance(obj, dict):\n            pr = obj.copy()\n        else:\n            for name in dir(obj):\n                try:\n                    value = getattr(obj, name)\n                    if not name.startswith('__') and not inspect.ismethod(value):\n                        pr[name] = value\n                except:\n                    continue\n\n    return pr"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef props(obj, required_args=None, **kwargs):\n    pr = get_obj_attrs(obj)\n    pr.update(kwargs)\n\n    if required_args is None:\n        required_args = []\n\n    # check that required args are given\n    missing = set(required_args) - set(pr.keys())\n    if any(missing):\n        raise ValueError(\"Please provide {}\".format(', '.join(missing)))\n\n    # Get the parameters to generate the waveform\n    # Note that keyword arguments override values in the template object\n    input_params = default_args.copy()\n    input_params.update(pr)\n\n    return input_params", "response": "Returns a dictionary built from the combination of defaults kwargs and the attributes of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_fd_waveform_sequence(template=None, **kwds):\n    kwds['delta_f'] = -1\n    kwds['f_lower'] = -1\n    p = props(template, required_args=fd_required_args, **kwds)\n    lal_pars = _check_lal_pars(p)\n\n    hp, hc = lalsimulation.SimInspiralChooseFDWaveformSequence(float(p['coa_phase']),\n               float(pnutils.solar_mass_to_kg(p['mass1'])),\n               float(pnutils.solar_mass_to_kg(p['mass2'])),\n               float(p['spin1x']), float(p['spin1y']), float(p['spin1z']),\n               float(p['spin2x']), float(p['spin2y']), float(p['spin2z']),\n               float(p['f_ref']),\n               pnutils.megaparsecs_to_meters(float(p['distance'])),\n               float(p['inclination']),\n               lal_pars,\n               _lalsim_enum[p['approximant']],\n               p['sample_points'].lal())\n    return Array(hp.data.data), Array(hc.data.data)", "response": "Returns the values of the waveform evaluated at the sequence of frequency\n    points."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_td_waveform(template=None, **kwargs):\n    input_params = props(template, required_args=td_required_args, **kwargs)\n    wav_gen = td_wav[type(_scheme.mgr.state)]\n    if input_params['approximant'] not in wav_gen:\n        raise ValueError(\"Approximant %s not available\" %\n                            (input_params['approximant']))\n    return wav_gen[input_params['approximant']](**input_params)", "response": "Returns the plus and cross polarizations of a time domain waveform."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_fd_waveform(template=None, **kwargs):\n\n    input_params = props(template, required_args=fd_required_args, **kwargs)\n    wav_gen = fd_wav[type(_scheme.mgr.state)]\n    if input_params['approximant'] not in wav_gen:\n        raise ValueError(\"Approximant %s not available\" %\n                            (input_params['approximant']))\n    try:\n        ffunc = input_params.pop('f_final_func')\n        if ffunc != '':\n            # convert the frequency function to a value\n            input_params['f_final'] = pnutils.named_frequency_cutoffs[ffunc](\n                input_params)\n            # if the f_final is < f_lower, raise a NoWaveformError\n            if 'f_final' in input_params and \\\n                    (input_params['f_lower']+input_params['delta_f'] >=\n                     input_params['f_final']):\n                raise NoWaveformError(\"cannot generate waveform: f_lower >= f_final\")\n    except KeyError:\n        pass\n\n    return wav_gen[input_params['approximant']](**input_params)", "response": "Returns a frequency domain gravitational waveform."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_fd_waveform_from_td(**params):\n\n    # determine the duration to use\n    full_duration = duration = get_waveform_filter_length_in_time(**params)\n    nparams = params.copy()\n\n    while full_duration < duration * 1.5:\n        full_duration = get_waveform_filter_length_in_time(**nparams)\n        nparams['f_lower'] -= 1\n\n    if 'f_fref' not in nparams:\n        nparams['f_ref'] = params['f_lower']\n\n    # We'll try to do the right thing and figure out what the frequency\n    # end is. Otherwise, we'll just assume 2048 Hz.\n    # (consider removing as we hopefully have better estimates for more\n    # approximants\n    try:\n        f_end = get_waveform_end_frequency(**params)\n        delta_t = (0.5 / pnutils.nearest_larger_binary_number(f_end))\n    except:\n        delta_t = 1.0 / 2048\n\n    nparams['delta_t'] = delta_t\n    hp, hc = get_td_waveform(**nparams)\n\n    # Resize to the right duration\n    tsamples = int(1.0 / params['delta_f'] / delta_t)\n\n    if tsamples < len(hp):\n        raise ValueError(\"The frequency spacing (df = {}) is too low to \"\n                         \"generate the {} approximant from the time \"\n                         \"domain\".format(params['delta_f'], params['approximant']))\n\n    hp.resize(tsamples)\n    hc.resize(tsamples)\n\n    # apply the tapering, we will use a safety factor here to allow for\n    # somewhat innacurate duration difference estimation.\n    window = (full_duration - duration) * 0.8\n    hp = wfutils.td_taper(hp, hp.start_time, hp.start_time + window)\n    hc = wfutils.td_taper(hc, hc.start_time, hc.start_time + window)\n\n    # avoid wraparound\n    hp = hp.to_frequencyseries().cyclic_time_shift(hp.start_time)\n    hc = hc.to_frequencyseries().cyclic_time_shift(hc.start_time)\n    return hp, hc", "response": "Returns a frequency domain version of the fourier domain approximant from the time domain."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_td_waveform_from_fd(rwrap=0.2, **params):\n\n    # determine the duration to use\n    full_duration = duration = get_waveform_filter_length_in_time(**params)\n    nparams = params.copy()\n\n    while full_duration < duration * 1.5:\n        full_duration = get_waveform_filter_length_in_time(**nparams)\n        nparams['f_lower'] -= 1\n\n    if 'f_fref' not in nparams:\n        nparams['f_ref'] = params['f_lower']\n\n    # factor to ensure the vectors are all large enough. We don't need to\n    # completely trust our duration estimator in this case, at a small\n    # increase in computational cost\n    fudge_duration = (max(0, full_duration) + .1 + rwrap) * 1.5\n    fsamples = int(fudge_duration / params['delta_t'])\n    N = pnutils.nearest_larger_binary_number(fsamples)\n    fudge_duration = N * params['delta_t']\n\n    nparams['delta_f'] = 1.0 / fudge_duration\n    hp, hc = get_fd_waveform(**nparams)\n\n    # Resize to the right sample rate\n    tsize = int(1.0 / params['delta_t'] /  nparams['delta_f'])\n    fsize = tsize // 2 + 1\n    hp.resize(fsize)\n    hc.resize(fsize)\n\n    # avoid wraparound\n    hp = hp.cyclic_time_shift(-rwrap)\n    hc = hc.cyclic_time_shift(-rwrap)\n\n    hp = wfutils.fd_to_td(hp, left_window=(nparams['f_lower'],\n                                           params['f_lower']))\n    hc = wfutils.fd_to_td(hc, left_window=(nparams['f_lower'],\n                                           params['f_lower']))\n    return hp, hc", "response": "Return a time domain version of fourier domain approximant with the given parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_interpolated_fd_waveform(dtype=numpy.complex64, return_hc=True,\n                                 **params):\n    \"\"\" Return a fourier domain waveform approximant, using interpolation\n    \"\"\"\n\n    def rulog2(val):\n        return 2.0 ** numpy.ceil(numpy.log2(float(val)))\n\n    orig_approx = params['approximant']\n    params['approximant'] = params['approximant'].replace('_INTERP', '')\n    df = params['delta_f']\n\n    if 'duration' not in params:\n        duration = get_waveform_filter_length_in_time(**params)\n    elif params['duration'] > 0:\n        duration = params['duration']\n    else:\n        err_msg = \"Waveform duration must be greater than 0.\"\n        raise ValueError(err_msg)\n\n    #FIXME We should try to get this length directly somehow\n    # I think this number should be conservative\n    ringdown_padding = 0.5\n\n    df_min = 1.0 / rulog2(duration + ringdown_padding)\n    # FIXME: I don't understand this, but waveforms with df_min < 0.5 will chop\n    #        off the inspiral when using ringdown_padding - 0.5.\n    #        Also, if ringdown_padding is set to a very small\n    #        value we can see cases where the ringdown is chopped.\n    if df_min > 0.5:\n        df_min = 0.5\n    params['delta_f'] = df_min\n    hp, hc = get_fd_waveform(**params)\n    hp = hp.astype(dtype)\n    if return_hc:\n        hc = hc.astype(dtype)\n    else:\n        hc = None\n\n    f_end = get_waveform_end_frequency(**params)\n    if f_end is None:\n        f_end = (len(hp) - 1) * hp.delta_f\n    if 'f_final' in params and params['f_final'] > 0:\n        f_end_params = params['f_final']\n        if f_end is not None:\n            f_end = min(f_end_params, f_end)\n\n    n_min = int(rulog2(f_end / df_min)) + 1\n    if n_min < len(hp):\n        hp = hp[:n_min]\n        if hc is not None:\n            hc = hc[:n_min]\n\n    offset = int(ringdown_padding * (len(hp)-1)*2 * hp.delta_f)\n\n    hp = interpolate_complex_frequency(hp, df, zeros_offset=offset, side='left')\n    if hc is not None:\n        hc = interpolate_complex_frequency(hc, df, zeros_offset=offset,\n                                           side='left')\n    params['approximant'] = orig_approx\n    return hp, hc", "response": "Return a fourier domain waveform approximant using interpolation\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the plus and cross polarizations of a time domain sine - Gaussian burst waveform.", "response": "def get_sgburst_waveform(template=None, **kwargs):\n    \"\"\"Return the plus and cross polarizations of a time domain\n    sine-Gaussian burst waveform.\n\n    Parameters\n    ----------\n    template: object\n        An object that has attached properties. This can be used to subsitute\n        for keyword arguments. A common example would be a row in an xml table.\n    approximant : string\n        A string that indicates the chosen approximant. See `td_approximants`\n        for available options.\n    q : float\n        The quality factor of a sine-Gaussian burst\n    frequency : float\n        The centre-frequency of a sine-Gaussian burst\n    delta_t : float\n        The time step used to generate the waveform\n    hrss : float\n        The strain rss\n    amplitude: float\n        The strain amplitude\n\n    Returns\n    -------\n    hplus: TimeSeries\n        The plus polarization of the waveform.\n    hcross: TimeSeries\n        The cross polarization of the waveform.\n    \"\"\"\n    input_params = props_sgburst(template,**kwargs)\n\n    for arg in sgburst_required_args:\n        if arg not in input_params:\n            raise ValueError(\"Please provide \" + str(arg))\n\n    return _lalsim_sgburst_waveform(**input_params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_imr_length(approx, **kwds):\n    m1 = float(kwds['mass1'])\n    m2 = float(kwds['mass2'])\n    s1z = float(kwds['spin1z'])\n    s2z = float(kwds['spin2z'])\n    f_low = float(kwds['f_lower'])\n    # 10% margin of error is incorporated in the pnutils function\n    return pnutils.get_imr_duration(m1, m2, s1z, s2z, f_low, approximant=approx)", "response": "Get the length of IMR waveform in the chemical space."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a frequency domain waveform filter for the specified approximant.", "response": "def get_waveform_filter(out, template=None, **kwargs):\n    \"\"\"Return a frequency domain waveform filter for the specified approximant\n    \"\"\"\n    n = len(out)\n\n    input_params = props(template, **kwargs)\n\n    if input_params['approximant'] in filter_approximants(_scheme.mgr.state):\n        wav_gen = filter_wav[type(_scheme.mgr.state)]\n        htilde = wav_gen[input_params['approximant']](out=out, **input_params)\n        htilde.resize(n)\n        htilde.chirp_length = get_waveform_filter_length_in_time(**input_params)\n        htilde.length_in_time = htilde.chirp_length\n        return htilde\n\n    if input_params['approximant'] in fd_approximants(_scheme.mgr.state):\n        wav_gen = fd_wav[type(_scheme.mgr.state)]\n\n        duration = get_waveform_filter_length_in_time(**input_params)\n        hp, _ = wav_gen[input_params['approximant']](duration=duration,\n                                               return_hc=False, **input_params)\n\n        hp.resize(n)\n        out[0:len(hp)] = hp[:]\n        hp = FrequencySeries(out, delta_f=hp.delta_f, copy=False)\n\n        hp.length_in_time = hp.chirp_length = duration\n        return hp\n\n    elif input_params['approximant'] in td_approximants(_scheme.mgr.state):\n        wav_gen = td_wav[type(_scheme.mgr.state)]\n        hp, _ = wav_gen[input_params['approximant']](**input_params)\n        # taper the time series hp if required\n        if 'taper' in input_params.keys() and \\\n                input_params['taper'] is not None:\n            hp = wfutils.taper_timeseries(hp, input_params['taper'],\n                                          return_lal=False)\n        return td_waveform_to_fd_waveform(hp, out=out)\n\n    else:\n        raise ValueError(\"Approximant %s not available\" %\n                            (input_params['approximant']))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef td_waveform_to_fd_waveform(waveform, out=None, length=None,\n                               buffer_length=100):\n    \"\"\" Convert a time domain into a frequency domain waveform by FFT.\n        As a waveform is assumed to \"wrap\" in the time domain one must be\n        careful to ensure the waveform goes to 0 at both \"boundaries\". To\n        ensure this is done correctly the waveform must have the epoch set such\n        the merger time is at t=0 and the length of the waveform should be\n        shorter than the desired length of the FrequencySeries (times 2 - 1)\n        so that zeroes can be suitably pre- and post-pended before FFTing.\n        If given, out is a memory array to be used as the output of the FFT.\n        If not given memory is allocated internally.\n        If present the length of the returned FrequencySeries is determined\n        from the length out. If out is not given the length can be provided\n        expicitly, or it will be chosen as the nearest power of 2. If choosing\n        length explicitly the waveform length + buffer_length is used when\n        choosing the nearest binary number so that some zero padding is always\n        added.\n    \"\"\"\n    # Figure out lengths and set out if needed\n    if out is None:\n        if length is None:\n            N = pnutils.nearest_larger_binary_number(len(waveform) + \\\n                                                     buffer_length)\n            n = int(N//2) + 1\n        else:\n            n = length\n            N = (n-1)*2\n        out = zeros(n, dtype=complex_same_precision_as(waveform))\n    else:\n        n = len(out)\n        N = (n-1)*2\n    delta_f =  1. / (N * waveform.delta_t)\n\n    # total duration of the waveform\n    tmplt_length = len(waveform) * waveform.delta_t\n    if len(waveform) > N:\n        err_msg = \"The time domain template is longer than the intended \"\n        err_msg += \"duration in the frequency domain. This situation is \"\n        err_msg += \"not supported in this function. Please shorten the \"\n        err_msg += \"waveform appropriately before calling this function or \"\n        err_msg += \"increase the allowed waveform length. \"\n        err_msg += \"Waveform length (in samples): {}\".format(len(waveform))\n        err_msg += \". Intended length: {}.\".format(N)\n        raise ValueError(err_msg)\n    # for IMR templates the zero of time is at max amplitude (merger)\n    # thus the start time is minus the duration of the template from\n    # lower frequency cutoff to merger, i.e. minus the 'chirp time'\n    tChirp = - float( waveform.start_time )  # conversion from LIGOTimeGPS\n    waveform.resize(N)\n    k_zero = int(waveform.start_time / waveform.delta_t)\n    waveform.roll(k_zero)\n    htilde = FrequencySeries(out, delta_f=delta_f, copy=False)\n    fft(waveform.astype(real_same_precision_as(htilde)), htilde)\n    htilde.length_in_time = tmplt_length\n    htilde.chirp_length = tChirp\n    return htilde", "response": "Convert a time domain waveform into a frequency domain waveform by FFT."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a frequency domain waveform filter for the specified approximant.", "response": "def get_two_pol_waveform_filter(outplus, outcross, template, **kwargs):\n    \"\"\"Return a frequency domain waveform filter for the specified approximant.\n    Unlike get_waveform_filter this function returns both h_plus and h_cross\n    components of the waveform, which are needed for searches where h_plus\n    and h_cross are not related by a simple phase shift.\n    \"\"\"\n    n = len(outplus)\n\n    # If we don't have an inclination column alpha3 might be used\n    if not hasattr(template, 'inclination') and 'inclination' not in kwargs:\n        if hasattr(template, 'alpha3'):\n            kwargs['inclination'] = template.alpha3\n\n    input_params = props(template, **kwargs)\n\n    if input_params['approximant'] in fd_approximants(_scheme.mgr.state):\n        wav_gen = fd_wav[type(_scheme.mgr.state)]\n        hp, hc = wav_gen[input_params['approximant']](**input_params)\n        hp.resize(n)\n        hc.resize(n)\n        outplus[0:len(hp)] = hp[:]\n        hp = FrequencySeries(outplus, delta_f=hp.delta_f, copy=False)\n        outcross[0:len(hc)] = hc[:]\n        hc = FrequencySeries(outcross, delta_f=hc.delta_f, copy=False)\n        hp.chirp_length = get_waveform_filter_length_in_time(**input_params)\n        hp.length_in_time = hp.chirp_length\n        hc.chirp_length = hp.chirp_length\n        hc.length_in_time = hp.length_in_time\n        return hp, hc\n    elif input_params['approximant'] in td_approximants(_scheme.mgr.state):\n        # N: number of time samples required\n        N = (n-1)*2\n        delta_f = 1.0 / (N * input_params['delta_t'])\n        wav_gen = td_wav[type(_scheme.mgr.state)]\n        hp, hc = wav_gen[input_params['approximant']](**input_params)\n        # taper the time series hp if required\n        if 'taper' in input_params.keys() and \\\n                input_params['taper'] is not None:\n            hp = wfutils.taper_timeseries(hp, input_params['taper'],\n                                          return_lal=False)\n            hc = wfutils.taper_timeseries(hc, input_params['taper'],\n                                          return_lal=False)\n        # total duration of the waveform\n        tmplt_length = len(hp) * hp.delta_t\n        # for IMR templates the zero of time is at max amplitude (merger)\n        # thus the start time is minus the duration of the template from\n        # lower frequency cutoff to merger, i.e. minus the 'chirp time'\n        tChirp = - float( hp.start_time )  # conversion from LIGOTimeGPS\n        hp.resize(N)\n        hc.resize(N)\n        k_zero = int(hp.start_time / hp.delta_t)\n        hp.roll(k_zero)\n        hc.roll(k_zero)\n        hp_tilde = FrequencySeries(outplus, delta_f=delta_f, copy=False)\n        hc_tilde = FrequencySeries(outcross, delta_f=delta_f, copy=False)\n        fft(hp.astype(real_same_precision_as(hp_tilde)), hp_tilde)\n        fft(hc.astype(real_same_precision_as(hc_tilde)), hc_tilde)\n        hp_tilde.length_in_time = tmplt_length\n        hp_tilde.chirp_length = tChirp\n        hc_tilde.length_in_time = tmplt_length\n        hc_tilde.chirp_length = tChirp\n        return hp_tilde, hc_tilde\n    else:\n        raise ValueError(\"Approximant %s not available\" %\n                            (input_params['approximant']))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_template_amplitude_norm(template=None, **kwargs):\n    input_params = props(template,**kwargs)\n    approximant = kwargs['approximant']\n\n    if approximant in _template_amplitude_norms:\n        return _template_amplitude_norms[approximant](**input_params)\n    else:\n        return None", "response": "Return additional constant template normalization. This only affects\n        the effective distance calculation. Returns None if no additional constant template normalization is available. Returns None if no additional constant template normalization is available. Returns None if no additional constant template normalization is available."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_waveform_filter_precondition(approximant, length, delta_f):\n    if approximant in _filter_preconditions:\n        return _filter_preconditions[approximant](length, delta_f)\n    else:\n        return None", "response": "Return the data preconditioning factor for this approximant."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the normalization vector for the approximant", "response": "def get_waveform_filter_norm(approximant, psd, length, delta_f, f_lower):\n    \"\"\" Return the normalization vector for the approximant\n    \"\"\"\n    if approximant in _filter_norms:\n        return _filter_norms[approximant](psd, length, delta_f, f_lower)\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_waveform_end_frequency(template=None, **kwargs):\n    input_params = props(template,**kwargs)\n    approximant = kwargs['approximant']\n\n    if approximant in _filter_ends:\n        return _filter_ends[approximant](**input_params)\n    else:\n        return None", "response": "Return the stop frequency of a template"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _loglr(self):\n        params = self.current_params\n        try:\n            wfs = self._waveform_generator.generate(**params)\n        except NoWaveformError:\n            return self._nowaveform_loglr()\n        hh = 0.\n        hd = 0j\n        for det, h in wfs.items():\n            # the kmax of the waveforms may be different than internal kmax\n            kmax = min(len(h), self._kmax)\n            if self._kmin >= kmax:\n                # if the waveform terminates before the filtering low frequency\n                # cutoff, then the loglr is just 0 for this detector\n                hh_i = 0.\n                hd_i = 0j\n            else:\n                # whiten the waveform\n                h[self._kmin:kmax] *= self._weight[det][self._kmin:kmax]\n                # calculate inner products\n                hh_i = h[self._kmin:kmax].inner(h[self._kmin:kmax]).real\n                hd_i = self.data[det][self._kmin:kmax].inner(\n                    h[self._kmin:kmax])\n            # store\n            setattr(self._current_stats, '{}_optimal_snrsq'.format(det), hh_i)\n            hh += hh_i\n            hd += hd_i\n        hd = abs(hd)\n        self._current_stats.maxl_phase = numpy.angle(hd)\n        return numpy.log(special.i0e(hd)) + hd - 0.5*hh", "response": "r Returns the log likelihood ratio evaluated at the given point."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds loglr optimal_snrsq and matched filter snrsq in each detector to the default stats.", "response": "def _extra_stats(self):\n        \"\"\"Adds ``loglr``, ``optimal_snrsq`` and matched filter snrsq in each\n        detector to the default stats.\"\"\"\n        return ['loglr'] + \\\n               ['{}_optimal_snrsq'.format(det) for det in self._data] + \\\n               ['{}_matchedfilter_snrsq'.format(det) for det in self._data]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninitialize an instance of this class from the given config file.", "response": "def from_config(cls, cp, data=None, delta_f=None, delta_t=None,\n                    gates=None, recalibration=None, **kwargs):\n        \"\"\"Initializes an instance of this class from the given config file.\n\n        Parameters\n        ----------\n        cp : WorkflowConfigParser\n            Config file parser to read.\n        data : dict\n            A dictionary of data, in which the keys are the detector names and\n            the values are the data. This is not retrieved from the config\n            file, and so must be provided.\n        delta_f : float\n            The frequency spacing of the data; needed for waveform generation.\n        delta_t : float\n            The time spacing of the data; needed for time-domain waveform\n            generators.\n        recalibration : dict of pycbc.calibration.Recalibrate, optional\n            Dictionary of detectors -> recalibration class instances for\n            recalibrating data.\n        gates : dict of tuples, optional\n            Dictionary of detectors -> tuples of specifying gate times. The\n            sort of thing returned by `pycbc.gate.gates_from_cli`.\n        \\**kwargs :\n            All additional keyword arguments are passed to the class. Any\n            provided keyword will over ride what is in the config file.\n        \"\"\"\n        prior_section = \"marginalized_prior\"\n        args = cls._init_args_from_config(cp)\n        marg_prior = read_distributions_from_config(cp, prior_section)\n        if len(marg_prior) == 0:\n            raise AttributeError(\"No priors are specified for the \"\n                                 \"marginalization. Please specify this in a \"\n                                 \"section in the config file with heading \"\n                                 \"{}-variable\".format(prior_section))\n        params = [i.params[0] for i in marg_prior]\n        marg_args = [k for k, v in args.items() if \"_marginalization\" in k]\n        if len(marg_args) != len(params):\n            raise ValueError(\"There is not a prior for each keyword argument\")\n        kwargs['marg_prior'] = marg_prior\n        for i in params:\n            kwargs[i+\"_marginalization\"] = True\n        args.update(kwargs)\n        variable_params = args['variable_params']\n        args[\"data\"] = data\n        try:\n            static_params = args['static_params']\n        except KeyError:\n            static_params = {}\n        # set up waveform generator\n        try:\n            approximant = static_params['approximant']\n        except KeyError:\n            raise ValueError(\"no approximant provided in the static args\")\n        generator_function = generator.select_waveform_generator(approximant)\n        waveform_generator = generator.FDomainDetFrameGenerator(\n            generator_function, epoch=data.values()[0].start_time,\n            variable_args=variable_params, detectors=data.keys(),\n            delta_f=delta_f, delta_t=delta_t,\n            recalib=recalibration, gates=gates,\n            **static_params)\n        args['waveform_generator'] = waveform_generator\n        args[\"f_lower\"] = static_params[\"f_lower\"]\n        return cls(**args)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset up the prior for time and distance and phase which is used for the marginalization.", "response": "def _setup_prior(self):\n        \"\"\"Sets up the prior for time and/or distance and/or phase which is\n        used for the likelihood marginalization.\n        \"\"\"\n        if len(self._marg_prior) == 0:\n            raise ValueError(\"A prior must be specified for the parameters \"\n                             \"that you wish to marginalize the likelihood \"\n                             \"over\")\n        marg_number = len([i for i in self._args if i != 0])\n        if len(self._marg_prior) != marg_number:\n            raise AttributeError(\"There is not a prior for each keyword \"\n                                 \"argument\")\n        if self._margdist:\n            bounds = self._marg_prior[\"distance\"].bounds\n            self._dist_array = numpy.linspace(bounds[\"distance\"].min,\n                                              bounds[\"distance\"].max, 10**4)\n            self._deltad = self._dist_array[1] - self._dist_array[0]\n            self.dist_prior = numpy.array(\n                                  [self._marg_prior[\"distance\"].pdf(distance=i)\n                                   for i in self._dist_array])\n        if self._margtime:\n            bounds = self._marg_prior[\"time\"].bounds\n            self._time_array = numpy.linspace(bounds[\"time\"].min,\n                                              bounds[\"time\"].min, 10**4)\n            self.time_prior = numpy.array(\n                                  [self._marg_prior[\"time\"].pdf(time=i) for\n                                   i in self._time_array])\n        if self._margphase:\n            bounds = self._marg_prior[\"phase\"].bounds\n            self._phase_array = numpy.linspace(bounds[\"phase\"].min,\n                                               bounds[\"phase\"].max, 10**4)\n            self._deltap = self._phase_array[1] - self._phase_array[0]\n            self.phase_prior = numpy.array(\n                                   [self._marg_prior[\"phase\"].pdf(phase=i) for\n                                    i in self._phase_array])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a time series for the matched filter SNR assuming that the template and data have both been normalised and whitened.", "response": "def _margtime_mfsnr(template, data):\n        \"\"\"Returns a time series for the matched filter SNR assuming that the\n        template and data have both been normalised and whitened.\n        \"\"\"\n        snr = matched_filter_core(template, data, h_norm=1, psd=None)\n        hd_i = snr[0].numpy().real\n        return hd_i"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _margtimedist_loglr(self, mf_snr, opt_snr):\n        logl = special.logsumexp(mf_snr, b=self._deltat)\n        logl_marg = logl/self._dist_array\n        opt_snr_marg = opt_snr/self._dist_array**2\n        return special.logsumexp(logl_marg - 0.5*opt_snr_marg,\n                                 b=self._deltad*self.dist_prior)", "response": "Returns the log likelihood ratio marginalized over time and\n        distance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the marginalized over time and phase.", "response": "def _margtimephase_loglr(self, mf_snr, opt_snr):\n        \"\"\"Returns the log likelihood ratio marginalized over time and phase.\n        \"\"\"\n        return special.logsumexp(numpy.log(special.i0(mf_snr)),\n                                 b=self._deltat) - 0.5*opt_snr"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the log likelihood ratio marginalized over distance and phase.", "response": "def _margdistphase_loglr(self, mf_snr, opt_snr):\n        \"\"\"Returns the log likelihood ratio marginalized over distance and\n        phase.\n        \"\"\"\n        logl = numpy.log(special.i0(mf_snr))\n        logl_marg = logl/self._dist_array\n        opt_snr_marg = opt_snr/self._dist_array**2\n        return special.logsumexp(logl_marg - 0.5*opt_snr_marg,\n                                 b=self._deltad*self.dist_prior)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the log likelihood ratio marginalized over distance.", "response": "def _margdist_loglr(self, mf_snr, opt_snr):\n        \"\"\"Returns the log likelihood ratio marginalized over distance.\n        \"\"\"\n        mf_snr_marg = mf_snr/self._dist_array\n        opt_snr_marg = opt_snr/self._dist_array**2\n        return special.logsumexp(mf_snr_marg - 0.5*opt_snr_marg,\n                                 b=self._deltad*self.dist_prior)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _margtime_loglr(self, mf_snr, opt_snr):\n        return special.logsumexp(mf_snr, b=self._deltat) - 0.5*opt_snr", "response": "Returns the marginalized log likelihood ratio over time."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef istext(s, text_characters=None, threshold=0.3):\n    text_characters = \"\".join(map(chr, range(32, 127))) + \"\\n\\r\\t\\b\"\n    _null_trans = string.maketrans(\"\", \"\")\n    # if s contains any null, it's not text:\n    if \"\\0\" in s:\n        return False\n    # an \"empty\" string is \"text\" (arbitrary but reasonable choice):\n    if not s:\n        return True\n    # Get the substring of s made up of non-text characters\n    t = s.translate(_null_trans, text_characters)\n    # s is 'text' if less than 30% of its characters are non-text ones:\n    return len(t)/float(len(s)) <= threshold", "response": "Determines if a string is a set of text or a text file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nresolving a URL to a local file and returns the path to that file.", "response": "def resolve_url(url, directory=None, permissions=None):\n    \"\"\"\n    Resolves a URL to a local file, and returns the path to\n    that file.\n    \"\"\"\n\n    u = urlparse(url)\n\n    # create the name of the destination file\n    if directory is None:\n        directory = os.getcwd()\n    filename = os.path.join(directory,os.path.basename(u.path))\n\n    if u.scheme == '' or u.scheme == 'file':\n        # for regular files, make a direct copy\n        if os.path.isfile(u.path):\n            if os.path.isfile(filename):\n                # check to see if src and dest are the same file\n                src_inode = os.stat(u.path)[stat.ST_INO]\n                dst_inode = os.stat(filename)[stat.ST_INO]\n                if src_inode != dst_inode:\n                    shutil.copy(u.path, filename)\n            else:\n                shutil.copy(u.path, filename)\n        else:\n            errmsg  = \"Cannot open file %s from URL %s\" % (u.path, url)\n            raise ValueError(errmsg)\n\n    elif u.scheme == 'http' or u.scheme == 'https':\n        s = requests.Session()\n        s.mount(str(u.scheme)+'://',\n            requests.adapters.HTTPAdapter(max_retries=5))\n\n        # look for an ecp cookie file and load the cookies\n        cookie_dict = {}\n        ecp_file = '/tmp/ecpcookie.u%d' % os.getuid()\n        if os.path.isfile(ecp_file):\n            cj = cookielib.MozillaCookieJar()\n            cj.load(ecp_file, ignore_discard=True, ignore_expires=True)\n        else:\n            cj = []\n\n        for c in cj:\n            if c.domain == u.netloc:\n                # load cookies for this server\n                cookie_dict[c.name] = c.value\n            elif u.netloc == \"code.pycbc.phy.syr.edu\" and \\\n              c.domain == \"git.ligo.org\":\n                # handle the redirect for code.pycbc to git.ligo.org\n                cookie_dict[c.name] = c.value\n\n        r = s.get(url, cookies=cookie_dict, allow_redirects=True)\n        if r.status_code != 200:\n            errmsg = \"Unable to download %s\\nError code = %d\" % (url,\n                r.status_code)\n            raise ValueError(errmsg)\n\n        # if we are downloading from git.ligo.org, check that we\n        # did not get redirected to the sign-in page\n        if u.netloc == 'git.ligo.org' or u.netloc == 'code.pycbc.phy.syr.edu':\n            # Check if we have downloaded a binary file.\n            if istext(r.content):\n                soup = BeautifulSoup(r.content, 'html.parser')\n                desc = soup.findAll(attrs={\"property\":\"og:url\"})\n                if len(desc) and \\\n                  desc[0]['content'] == 'https://git.ligo.org/users/sign_in':\n                    raise ValueError(ecp_cookie_error.format(url))\n\n        output_fp = open(filename, 'w')\n        output_fp.write(r.content)\n        output_fp.close()\n\n    else:\n        # TODO: We could support other schemes such as gsiftp by\n        # calling out to globus-url-copy\n        errmsg  = \"Unknown URL scheme: %s\\n\" % (u.scheme)\n        errmsg += \"Currently supported are: file, http, and https.\"\n        raise ValueError(errmsg)\n\n    if not os.path.isfile(filename):\n        errmsg = \"Error trying to create file %s from %s\" % (filename,url)\n        raise ValueError(errmsg)\n\n    if permissions:\n        if os.access(filename, os.W_OK):\n            os.chmod(filename, permissions)\n        else:\n            # check that the file has at least the permissions requested\n            s = os.stat(filename)[stat.ST_MODE]\n            if (s & permissions) != permissions:\n                errmsg = \"Could not change permissions on %s (read-only)\" % url\n                raise ValueError(errmsg)\n\n    return filename"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_args(cls, args):\n        # Identify the config files\n        confFiles = []\n\n        # files and URLs to resolve\n        if args.config_files:\n            confFiles += args.config_files\n\n        # Identify the deletes\n        confDeletes = args.config_delete or []\n        # and parse them\n        parsedDeletes = []\n        for delete in confDeletes:\n            splitDelete = delete.split(\":\")\n            if len(splitDelete) > 2:\n                raise ValueError(\n                    \"Deletes must be of format section:option \"\n                    \"or section. Cannot parse %s.\" % str(delete))\n            else:\n                parsedDeletes.append(tuple(splitDelete))\n\n        # Identify the overrides\n        confOverrides = args.config_overrides or []\n        # and parse them\n        parsedOverrides = []\n        for override in confOverrides:\n            splitOverride = override.split(\":\")\n            if len(splitOverride) == 3:\n                parsedOverrides.append(tuple(splitOverride))\n            elif len(splitOverride) == 2:\n                parsedOverrides.append(tuple(splitOverride + [\"\"]))\n            elif len(splitOverride) > 3:\n                # Cannot have colons in either section name or variable name\n                # but the value may contain colons\n                rec_value = ':'.join(splitOverride[2:])\n                parsedOverrides.append(tuple(splitOverride[:2] + [rec_value]))\n            else:\n                raise ValueError(\n                    \"Overrides must be of format section:option:value \"\n                    \"or section:option. Cannot parse %s.\" % str(override))\n\n        return cls(confFiles, parsedOverrides, None, parsedDeletes)", "response": "Initialize a WorkflowConfigParser instance from the command line arguments parsed by argparse."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef perform_exe_expansion(self):\n        # Only works on executables section\n        if self.has_section('executables'):\n            for option, value in self.items('executables'):\n                # Check the value\n                newStr = self.interpolate_exe(value)\n                if newStr != value:\n                    self.set('executables', option, newStr)", "response": "Perform expansion of the executables section of the cfg file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreplacing testString with a path to an executable based on the format of which.", "response": "def interpolate_exe(self, testString):\n        \"\"\"\n        Replace testString with a path to an executable based on the format.\n\n        If this looks like\n\n        ${which:lalapps_tmpltbank}\n\n        it will return the equivalent of which(lalapps_tmpltbank)\n\n        Otherwise it will return an unchanged string.\n\n        Parameters\n        -----------\n        testString : string\n            The input string\n\n        Returns\n        --------\n        newString : string\n            The output string.\n        \"\"\"\n        # First check if any interpolation is needed and abort if not\n        testString = testString.strip()\n        if not (testString.startswith('${') and testString.endswith('}')):\n            return testString\n\n        # This may not be an exe interpolation, so even if it has ${XXX} form\n        # I may not have to do anything\n        newString = testString\n\n        # Strip the ${ and }\n        testString = testString[2:-1]\n\n        testList = testString.split(':')\n\n        # Maybe we can add a few different possibilities for substitution\n        if len(testList) == 2:\n            if testList[0] == 'which':\n                newString = distutils.spawn.find_executable(testList[1])\n                if not newString:\n                    errmsg = \"Cannot find exe %s in your path \" %(testList[1])\n                    errmsg += \"and you specified ${which:%s}.\" %(testList[1])\n                    raise ValueError(errmsg)\n\n        return newString"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_subsections(self, section_name):\n        # Keep only subsection names\n        subsections = [sec[len(section_name)+1:] for sec in self.sections()\\\n                       if sec.startswith(section_name + '-')]\n\n        for sec in subsections:\n            sp = sec.split('-')\n            # This is unusual, but a format [section-subsection-tag] is okay. Just\n            # check that [section-subsection] section exists. If not it is possible\n            # the user is trying to use an subsection name with '-' in it\n            if (len(sp) > 1) and not self.has_section('%s-%s' % (section_name,\n                                                                 sp[0])):\n                raise ValueError( \"Workflow uses the '-' as a delimiter so \"\n                    \"this is interpreted as section-subsection-tag. \"\n                    \"While checking section %s, no section with \"\n                    \"name %s-%s was found. \"\n                    \"If you did not intend to use tags in an \"\n                    \"'advanced user' manner, or do not understand what \"\n                    \"this means, don't use dashes in section \"\n                    \"names. So [injection-nsbhinj] is good. \"\n                    \"[injection-nsbh-inj] is not.\" % (sec, sp[0], sp[1]))\n\n        if len(subsections) > 0:\n            return [sec.split('-')[0] for sec in subsections]\n        elif self.has_section(section_name):\n            return ['']\n        else:\n            return []", "response": "Return a list of subsections for the given section name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nperforming interpolation of the config values of the current locale.", "response": "def perform_extended_interpolation(self):\n        \"\"\"\n        Filter through an ini file and replace all examples of\n        ExtendedInterpolation formatting with the exact value. For values like\n        ${example} this is replaced with the value that corresponds to the\n        option called example ***in the same section***\n\n        For values like ${common|example} this is replaced with the value that\n        corresponds to the option example in the section [common]. Note that\n        in the python3 config parser this is ${common:example} but python2.7\n        interprets the : the same as a = and this breaks things\n\n        Nested interpolation is not supported here.\n        \"\"\"\n\n        # Do not allow any interpolation of the section names\n        for section in self.sections():\n            for option,value in self.items(section):\n                # Check the option name\n                newStr = self.interpolate_string(option, section)\n                if newStr != option:\n                    self.set(section,newStr,value)\n                    self.remove_option(section,option)\n                # Check the value\n                newStr = self.interpolate_string(value, section)\n                if newStr != value:\n                    self.set(section,option,newStr)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef interpolate_string(self, testString, section):\n\n        # First check if any interpolation is needed and abort if not\n        reObj = re.search(r\"\\$\\{.*?\\}\", testString)\n        while reObj:\n            # Not really sure how this works, but this will obtain the first\n            # instance of a string contained within ${....}\n            repString = (reObj).group(0)[2:-1]\n            # Need to test which of the two formats we have\n            splitString = repString.split('|')\n            if len(splitString) == 1:\n                try:\n                    testString = testString.replace('${'+repString+'}',\\\n                                            self.get(section,splitString[0]))\n                except ConfigParser.NoOptionError:\n                    print(\"Substitution failed\")\n                    raise\n            if len(splitString) == 2:\n                try:\n                    testString = testString.replace('${'+repString+'}',\\\n                                       self.get(splitString[0],splitString[1]))\n                except ConfigParser.NoOptionError:\n                    print(\"Substitution failed\")\n                    raise\n            reObj = re.search(r\"\\$\\{.*?\\}\", testString)\n\n        return testString", "response": "Take a string and replace all example of ExtendedInterpolation\n            formatting within the string with the exact value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsplitting the workflow config into multiple sections.", "response": "def split_multi_sections(self):\n        \"\"\"\n        Parse through the WorkflowConfigParser instance and splits any sections\n        labelled with an \"&\" sign (for e.g. [inspiral&tmpltbank]) into\n        [inspiral] and [tmpltbank] sections. If these individual sections\n        already exist they  will be appended to. If an option exists in both the\n        [inspiral] and [inspiral&tmpltbank] sections an error will be thrown\n        \"\"\"\n        # Begin by looping over all sections\n        for section in self.sections():\n            # Only continue if section needs splitting\n            if '&' not in section:\n                continue\n            # Get list of section names to add these options to\n            splitSections = section.split('&')\n            for newSec in splitSections:\n                # Add sections if they don't already exist\n                if not self.has_section(newSec):\n                    self.add_section(newSec)\n                self.add_options_to_section(newSec, self.items(section))\n            self.remove_section(section)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the [ sharedoptions ] section of the ini file and populate the internal dictionary with the values.", "response": "def populate_shared_sections(self):\n        \"\"\"Parse the [sharedoptions] section of the ini file.\n\n        That section should contain entries according to:\n\n          * massparams = inspiral, tmpltbank\n          * dataparams = tmpltbank\n\n        This will result in all options in [sharedoptions-massparams] being\n        copied into the [inspiral] and [tmpltbank] sections and the options\n        in [sharedoptions-dataparams] being copited into [tmpltbank].\n        In the case of duplicates an error will be raised.\n        \"\"\"\n        if not self.has_section('sharedoptions'):\n            # No sharedoptions, exit\n            return\n        for key, value in self.items('sharedoptions'):\n            assert(self.has_section('sharedoptions-%s' %(key)))\n            # Comma separated\n            values = value.split(',')\n            common_options = self.items('sharedoptions-%s' %(key))\n            for section in values:\n                if not self.has_section(section):\n                    self.add_section(section)\n                for arg, val in common_options:\n                    if arg in self.options(section):\n                        raise ValueError('Option exists in both original ' + \\\n                               'ConfigParser section [%s] and ' %(section,) + \\\n                               'sharedoptions section: %s %s' \\\n                               %(arg,'sharedoptions-%s' %(key)))\n                    self.set(section, arg, val)\n            self.remove_section('sharedoptions-%s' %(key))\n        self.remove_section('sharedoptions')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a set of options and values to a section of a ConfigParser object.", "response": "def add_options_to_section(self ,section, items, overwrite_options=False):\n        \"\"\"\n        Add a set of options and values to a section of a ConfigParser object.\n        Will throw an error if any of the options being added already exist,\n        this behaviour can be overridden if desired\n\n        Parameters\n        ----------\n        section : string\n            The name of the section to add options+values to\n        items : list of tuples\n            Each tuple contains (at [0]) the option and (at [1]) the value to\n            add to the section of the ini file\n        overwrite_options : Boolean, optional\n            By default this function will throw a ValueError if an option exists\n            in both the original section in the ConfigParser *and* in the\n            provided items.\n            This will override so that the options+values given in items\n            will replace the original values if the value is set to True.\n            Default = True\n        \"\"\"\n        # Sanity checking\n        if not self.has_section(section):\n            raise ValueError('Section %s not present in ConfigParser.' \\\n                             %(section,))\n\n        # Check for duplicate options first\n        for option,value in items:\n            if not overwrite_options:\n                if option in self.options(section):\n                    raise ValueError('Option exists in both original ' + \\\n                                  'ConfigParser section [%s] and ' %(section,) + \\\n                                  'input list: %s' %(option,))\n            self.set(section,option,value)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking for duplicate options in two sections section1 and section2.", "response": "def check_duplicate_options(self, section1, section2, raise_error=False):\n        \"\"\"\n        Check for duplicate options in two sections, section1 and section2.\n        Will return a list of the duplicate options.\n\n        Parameters\n        ----------\n        section1 : string\n            The name of the first section to compare\n        section2 : string\n            The name of the second section to compare\n        raise_error : Boolean, optional (default=False)\n            If True, raise an error if duplicates are present.\n\n        Returns\n        ----------\n        duplicates : List\n            List of duplicate options\n        \"\"\"\n        # Sanity checking\n        if not self.has_section(section1):\n            raise ValueError('Section %s not present in ConfigParser.'\\\n                             %(section1,) )\n        if not self.has_section(section2):\n            raise ValueError('Section %s not present in ConfigParser.'\\\n                             %(section2,) )\n\n        items1 = self.options(section1)\n        items2 = self.options(section2)\n\n        # The list comprehension here creates a list of all duplicate items\n        duplicates = [x for x in items1 if x in items2]\n\n        if duplicates and raise_error:\n            raise ValueError('The following options appear in both section ' +\\\n                             '%s and %s: %s' \\\n                             %(section1,section2,' '.join(duplicates)))\n\n        return duplicates"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsupplements to ConfigParser.ConfigParser.get(). This will search for an option in [section] and if it doesn't find it will also try in [section-tag] for every value of tag in tags. Will raise a ConfigParser.Error if it cannot find a value. Parameters ----------- self : ConfigParser object The ConfigParser object (automatically passed when this is appended to the ConfigParser class) section : string The section of the ConfigParser object to read option : string The ConfigParser option to look for tags : list of strings The name of subsections to look in, if not found in [section] Returns -------- string The value of the options being searched for", "response": "def get_opt_tags(self, section, option, tags):\n        \"\"\"\n        Supplement to ConfigParser.ConfigParser.get(). This will search for an\n        option in [section] and if it doesn't find it will also try in\n        [section-tag] for every value of tag in tags.\n        Will raise a ConfigParser.Error if it cannot find a value.\n\n        Parameters\n        -----------\n        self : ConfigParser object\n            The ConfigParser object (automatically passed when this is appended\n            to the ConfigParser class)\n        section : string\n            The section of the ConfigParser object to read\n        option : string\n            The ConfigParser option to look for\n        tags : list of strings\n            The name of subsections to look in, if not found in [section]\n\n        Returns\n        --------\n        string\n            The value of the options being searched for\n        \"\"\"\n        # Need lower case tag name; also exclude cases with tag=None\n        if tags:\n            tags = [tag.lower() for tag in tags if tag is not None]\n\n        try:\n            return self.get(section, option)\n        except ConfigParser.Error:\n            err_string = \"No option '%s' in section [%s] \" %(option,section)\n            if not tags:\n                raise ConfigParser.Error(err_string + \".\")\n            return_vals = []\n            sub_section_list = []\n            for sec_len in range(1, len(tags)+1):\n                for tag_permutation in itertools.permutations(tags, sec_len):\n                    joined_name = '-'.join(tag_permutation)\n                    sub_section_list.append(joined_name)\n            section_list = [\"%s-%s\" %(section, sb) for sb in sub_section_list]\n            err_section_list = []\n            for sub in sub_section_list:\n                if self.has_section('%s-%s' %(section, sub)):\n                    if self.has_option('%s-%s' %(section, sub), option):\n                        err_section_list.append(\"%s-%s\" %(section, sub))\n                        return_vals.append(self.get('%s-%s' %(section, sub),\n                                                    option))\n\n            # We also want to recursively go into sections\n\n            if not return_vals:\n                err_string += \"or in sections [%s].\" \\\n                               %(\"] [\".join(section_list))\n                raise ConfigParser.Error(err_string)\n            if len(return_vals) > 1:\n                err_string += \"and multiple entries found in sections [%s].\"\\\n                              %(\"] [\".join(err_section_list))\n                raise ConfigParser.Error(err_string)\n            return return_vals[0]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns True if the option is present in the section or section - tag.", "response": "def has_option_tag(self, section, option, tag):\n        \"\"\"\n        Convenience function accessing has_option_tags() for a single tag: see\n        documentation for that function.\n        NB calling has_option_tags() directly is preferred for simplicity.\n\n        Parameters\n        -----------\n        self : ConfigParser object\n            The ConfigParser object (automatically passed when this is appended\n            to the ConfigParser class)\n        section : string\n            The section of the ConfigParser object to read\n        option : string\n            The ConfigParser option to look for\n        tag : string\n            The name of the subsection to look in, if not found in [section]\n\n        Returns\n        --------\n        Boolean\n            Is the option in the section or [section-tag]\n        \"\"\"\n        return self.has_option_tags(section, option, [tag])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef has_option_tags(self, section, option, tags):\n        try:\n            self.get_opt_tags(section, option, tags)\n            return True\n        except ConfigParser.Error:\n            return False", "response": "Returns True if the option exists in the section and has any of the tags in tags False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_config_opts_to_parser(parser):\n        parser.add_argument(\"--config-files\", type=str, nargs=\"+\",\n                            required=True,\n                            help=\"A file parsable by \"\n                                 \"pycbc.workflow.WorkflowConfigParser.\")\n        parser.add_argument(\"--config-overrides\", type=str, nargs=\"+\",\n                            default=None, metavar=\"SECTION:OPTION:VALUE\",\n                            help=\"List of section:option:value combinations \"\n                                 \"to add into the configuration file.\")", "response": "Adds options for configuration files to the given parser."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_cli(cls, opts):\n        # read configuration file\n        logging.info(\"Reading configuration file\")\n        if opts.config_overrides is not None:\n            overrides = [override.split(\":\")\n                         for override in opts.config_overrides]\n        else:\n            overrides = None\n        if opts.config_delete is not None:\n            deletes = [delete.split(\":\") for delete in opts.config_delete]\n        else:\n            deletes = None\n        return cls(opts.config_files, overrides, deleteTuples=deletes)", "response": "Loads a config file from the given options with overrides and\nAttributeNames deletes applied."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nperform a fourier FFT from invec to outvec.", "response": "def fft(invec, outvec):\n    \"\"\" Fourier transform from invec to outvec.\n\n    Perform a fourier transform. The type of transform is determined\n    by the dtype of invec and outvec.\n\n    Parameters\n    ----------\n    invec : TimeSeries or FrequencySeries\n        The input vector.\n    outvec : TimeSeries or FrequencySeries\n        The output.\n    \"\"\"\n    prec, itype, otype = _check_fft_args(invec, outvec)\n    _check_fwd_args(invec, itype, outvec, otype, 1, None)\n\n    # The following line is where all the work is done:\n    backend = get_backend()\n    backend.fft(invec, outvec, prec, itype, otype)\n    # For a forward FFT, the length of the *input* vector is the length\n    # we should divide by, whether C2C or R2HC transform\n    if isinstance(invec, _TimeSeries):\n        outvec._epoch = invec._epoch\n        outvec._delta_f = 1.0/(invec._delta_t * len(invec))\n        outvec *= invec._delta_t\n    elif isinstance(invec, _FrequencySeries):\n        outvec._epoch = invec._epoch\n        outvec._delta_t = 1.0/(invec._delta_f * len(invec))\n        outvec *= invec._delta_f"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninverses fourier transform from invec to outvec.", "response": "def ifft(invec, outvec):\n    \"\"\" Inverse fourier transform from invec to outvec.\n\n    Perform an inverse fourier transform. The type of transform is determined\n    by the dtype of invec and outvec.\n\n    Parameters\n    ----------\n    invec : TimeSeries or FrequencySeries\n        The input vector.\n    outvec : TimeSeries or FrequencySeries\n        The output.\n    \"\"\"\n    prec, itype, otype = _check_fft_args(invec, outvec)\n    _check_inv_args(invec, itype, outvec, otype, 1, None)\n\n    # The following line is where all the work is done:\n    backend = get_backend()\n    backend.ifft(invec, outvec, prec, itype, otype)\n    # For an inverse FFT, the length of the *output* vector is the length\n    # we should divide by, whether C2C or HC2R transform\n    if isinstance(invec, _TimeSeries):\n        outvec._epoch = invec._epoch\n        outvec._delta_f = 1.0/(invec._delta_t * len(outvec))\n        outvec *= invec._delta_t\n    elif isinstance(invec,_FrequencySeries):\n        outvec._epoch = invec._epoch\n        outvec._delta_t = 1.0/(invec._delta_f * len(outvec))\n        outvec *= invec._delta_f"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef render_workflow_html_template(filename, subtemplate, filelists, **kwargs):\n\n    dirnam = os.path.dirname(filename)\n    makedir(dirnam)\n\n    try:\n        filenames = [f.name for filelist in filelists for f in filelist if f is not None]\n    except TypeError:\n        filenames = []\n\n    # render subtemplate\n    subtemplate_dir = pycbc.results.__path__[0] + '/templates/wells'\n    env = Environment(loader=FileSystemLoader(subtemplate_dir))\n    env.globals.update(get_embedded_config=get_embedded_config)\n    env.globals.update(path_exists=os.path.exists)\n    env.globals.update(len=len)\n    subtemplate = env.get_template(subtemplate)\n    context = {'filelists' : filelists,\n               'dir' : dirnam}\n    context.update(kwargs)\n    output = subtemplate.render(context)\n\n    # save as html page\n    kwds = {'render-function' : 'render_tmplt',\n            'filenames' : ','.join(filenames)}\n    save_html_with_metadata(str(output), filename, None, kwds)", "response": "Writes a template given inputs from the workflow generator. Takes a list of tuples. Each tuple is a pycbc File object. The name of the file to render and the filename of the output."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_embedded_config(filename):\n    def check_option(self, section, name):\n        return (self.has_section(section) and\n               (self.has_option(section, name) or (name in self.defaults())))\n\n    try:\n        cp = pycbc.results.load_metadata_from_file(filename)\n    except TypeError:\n        cp = ConfigParser()\n\n    cp.check_option = types.MethodType(check_option, cp)\n\n    return cp", "response": "Attempt to load config data attached to file\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef render_glitchgram(path, cp):\n\n    # define filename and slug from path\n    filename = os.path.basename(path)\n    slug = filename.replace('.', '_')\n\n    # render template\n    template_dir = pycbc.results.__path__[0] + '/templates/files'\n    env = Environment(loader=FileSystemLoader(template_dir))\n    env.globals.update(abs=abs)\n    template = env.get_template(cp.get(filename, 'template'))\n    context = {'filename' : filename,\n               'slug'     : slug,\n               'cp'       : cp}\n    output = template.render(context)\n\n    return output", "response": "Render a glitchgram file template."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef render_text(path, cp):\n\n    # define filename and slug from path\n    filename = os.path.basename(path)\n    slug = filename.replace('.', '_')\n\n    # initializations\n    content = None\n\n    # read file as a string\n    with codecs.open(path, 'r', encoding='utf-8', errors='replace') as fp:\n        content = fp.read()\n\n    # replace all the escaped characters\n    content = unescape(content, unescape_table)\n\n    # render template\n    template_dir = pycbc.results.__path__[0] + '/templates/files'\n    env = Environment(loader=FileSystemLoader(template_dir))\n    env.globals.update(abs=abs)\n    env.globals.update(path_exists=os.path.exists)\n    template = env.get_template('file_pre.html')\n    context = {'filename' : filename,\n               'slug'     : slug,\n               'cp'       : cp,\n               'content'  : content}\n    output = template.render(context)\n\n    return output", "response": "Render a file as text."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrendering a file as text.", "response": "def render_tmplt(path, cp):\n    \"\"\" Render a file as text.\n    \"\"\"\n\n    # define filename and slug from path\n    filename = os.path.basename(path)\n    slug = filename.replace('.', '_')\n\n    # initializations\n    content = None\n\n    # read file as a string\n    with open(path, 'rb') as fp:\n        content = fp.read()\n\n    # replace all the escaped characters\n    content = unescape(content, unescape_table)\n\n    # render template\n    template_dir = '/'.join(path.split('/')[:-1])\n    env = Environment(loader=FileSystemLoader(template_dir))\n    env.globals.update(setup_template_render=setup_template_render)\n    env.globals.update(get_embedded_config=get_embedded_config)\n    env.globals.update(path_exists=os.path.exists)\n    template = env.get_template(filename)\n    context = {'filename' : filename,\n               'slug'     : slug,\n               'cp'       : cp,\n               'content'  : content}\n    output = template.render(context)\n\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of detectors that are available in the currently sourced lalsuite.", "response": "def get_available_detectors():\n    \"\"\"Return list of detectors known in the currently sourced lalsuite.\n\n    This function will query lalsuite about which detectors are known to\n    lalsuite. Detectors are identified by a two character string e.g. 'K1',\n    but also by a longer, and clearer name, e.g. KAGRA. This function returns\n    both. As LAL doesn't really expose this functionality we have to make some\n    assumptions about how this information is stored in LAL. Therefore while\n    we hope this function will work correctly, it's possible it will need\n    updating in the future. Better if lal would expose this information\n    properly.\n    \"\"\"\n    ld = lal.__dict__\n    known_lal_names = [j for j in ld.keys() if \"DETECTOR_PREFIX\" in j]\n    known_prefixes = [ld[k] for k in known_lal_names]\n    known_names = [ld[k.replace('PREFIX', 'NAME')] for k in known_lal_names]\n    return zip(known_prefixes, known_names)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef overhead_antenna_pattern(right_ascension, declination, polarization):\n    # convert from declination coordinate to polar (angle dropped from north axis)\n    theta = np.pi / 2.0 - declination\n\n    f_plus  = - (1.0/2.0) * (1.0 + cos(theta)*cos(theta)) * \\\n                cos (2.0 * right_ascension) * cos (2.0 * polarization) - \\\n                cos(theta) * sin(2.0*right_ascension) * sin (2.0 * polarization)\n\n    f_cross =   (1.0/2.0) * (1.0 + cos(theta)*cos(theta)) * \\\n                cos (2.0 * right_ascension) * sin (2.0* polarization) - \\\n                cos(theta) * sin(2.0*right_ascension) * cos (2.0 * polarization)\n\n    return f_plus, f_cross", "response": "Return the antenna pattern factors F + and Fx as a function of sky\n    location and polarization angle for a hypothetical interferometer located at the north pole."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef light_travel_time_to_detector(self, det):\n        d = self.location - det.location\n        return float(d.dot(d)**0.5 / constants.c.value)", "response": "Return the light travel time from this detector to the other detector."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the detector response.", "response": "def antenna_pattern(self, right_ascension, declination, polarization, t_gps):\n        \"\"\"Return the detector response.\n\n        Parameters\n        ----------\n        right_ascension: float or numpy.ndarray\n            The right ascension of the source\n        declination: float or numpy.ndarray\n            The declination of the source\n        polarization: float or numpy.ndarray\n            The polarization angle of the source\n\n        Returns\n        -------\n        fplus: float or numpy.ndarray\n            The plus polarization factor for this sky location / orientation\n        fcross: float or numpy.ndarray\n            The cross polarization factor for this sky location / orientation\n        \"\"\"\n        gha = self.gmst_estimate(t_gps) - right_ascension\n\n        cosgha = cos(gha)\n        singha = sin(gha)\n        cosdec = cos(declination)\n        sindec = sin(declination)\n        cospsi = cos(polarization)\n        sinpsi = sin(polarization)\n\n        x0 = -cospsi * singha - sinpsi * cosgha * sindec\n        x1 = -cospsi * cosgha + sinpsi * singha * sindec\n        x2 =  sinpsi * cosdec\n        x = np.array([x0, x1, x2])\n\n        dx = self.response.dot(x)\n\n        y0 =  sinpsi * singha - cospsi * cosgha * sindec\n        y1 =  sinpsi * cosgha + cospsi * singha * sindec\n        y2 =  cospsi * cosdec\n        y = np.array([y0, y1, y2])\n        dy = self.response.dot(y)\n\n        if hasattr(dx, 'shape'):\n            fplus = (x * dx - y * dy).sum(axis=0)\n            fcross = (x * dy + y * dx).sum(axis=0)\n        else:\n            fplus = (x * dx - y * dy).sum()\n            fcross = (x * dy + y * dx).sum()\n\n        return fplus, fcross"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef time_delay_from_earth_center(self, right_ascension, declination, t_gps):\n        return self.time_delay_from_location(np.array([0, 0, 0]),\n                                             right_ascension,\n                                             declination,\n                                             t_gps)", "response": "Return the time delay from the earth center"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef time_delay_from_location(self, other_location, right_ascension,\n                                 declination, t_gps):\n        \"\"\"Return the time delay from the given location to detector for\n        a signal with the given sky location\n\n        In other words return `t1 - t2` where `t1` is the\n        arrival time in this detector and `t2` is the arrival time in the\n        other location.\n\n        Parameters\n        ----------\n        other_location : numpy.ndarray of coordinates\n            A detector instance.\n        right_ascension : float\n            The right ascension (in rad) of the signal.\n        declination : float\n            The declination (in rad) of the signal.\n        t_gps : float\n            The GPS time (in s) of the signal.\n\n        Returns\n        -------\n        float\n            The arrival time difference between the detectors.\n        \"\"\"\n        ra_angle = self.gmst_estimate(t_gps) - right_ascension\n        cosd = cos(declination)\n\n        e0 = cosd * cos(ra_angle)\n        e1 = cosd * -sin(ra_angle)\n        e2 = sin(declination)\n\n        ehat = np.array([e0, e1, e2])\n        dx = other_location - self.location\n        return dx.dot(ehat) / constants.c.value", "response": "Return the time delay from the given location to detector for the given sky location."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef time_delay_from_detector(self, other_detector, right_ascension,\n                                 declination, t_gps):\n        \"\"\"Return the time delay from the given to detector for a signal with\n        the given sky location; i.e. return `t1 - t2` where `t1` is the\n        arrival time in this detector and `t2` is the arrival time in the\n        other detector. Note that this would return the same value as\n        `time_delay_from_earth_center` if `other_detector` was geocentric.\n\n        Parameters\n        ----------\n        other_detector : detector.Detector\n            A detector instance.\n        right_ascension : float\n            The right ascension (in rad) of the signal.\n        declination : float\n            The declination (in rad) of the signal.\n        t_gps : float\n            The GPS time (in s) of the signal.\n\n        Returns\n        -------\n        float\n            The arrival time difference between the detectors.\n        \"\"\"\n        return self.time_delay_from_location(other_detector.location,\n                                             right_ascension,\n                                             declination,\n                                             t_gps)", "response": "Return the time delay from the given detector to the given location ; i. e. return t1 - t2 where t1 - t2 is the arrival time in the other detector."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef project_wave(self, hp, hc, longitude, latitude, polarization):\n        h_lal = lalsimulation.SimDetectorStrainREAL8TimeSeries(\n                hp.astype(np.float64).lal(), hc.astype(np.float64).lal(),\n                longitude, latitude, polarization, self.frDetector)\n        return TimeSeries(\n                h_lal.data.data, delta_t=h_lal.deltaT, epoch=h_lal.epoch,\n                dtype=np.float64, copy=False)", "response": "Project a waveform from the given detector to the given location."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef optimal_orientation(self, t_gps):\n        ra = self.longitude + (self.gmst_estimate(t_gps) % (2.0*np.pi))\n        dec = self.latitude\n        return ra, dec", "response": "Return the optimal orientation in right ascension and declination for a given GPS time."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef spintaylorf2(**kwds):\n    #####Pull out the input arguments#####\n    f_lower = double(kwds['f_lower'])\n    delta_f = double(kwds['delta_f'])\n    distance = double(kwds['distance'])\n    mass1 = double(kwds['mass1'])\n    mass2 = double(kwds['mass2'])\n    spin1x = double(kwds['spin1x'])\n    spin1y = double(kwds['spin1y'])\n    spin1z = double(kwds['spin1z'])\n    phi0 = double(kwds['coa_phase'])               #Orbital Phase at coalescence\n    phase_order = int(kwds['phase_order'])\n    amplitude_order = int(kwds['amplitude_order'])\n    inclination = double(kwds['inclination'])\n    lnhatx = sin(inclination)\n    lnhaty = 0.\n    lnhatz = cos(inclination)\n    psi = 0.\n\n    tC= -1.0 / delta_f\n    M = mass1 + mass2\n    eta = mass1 * mass2 / (M * M)\n    m_sec = M * lal.MTSUN_SI\n    piM = lal.PI * m_sec\n\n    vISCO = 1. / sqrt(6.)\n    fISCO = vISCO * vISCO * vISCO / piM\n    f_max = ceilpow2(fISCO)\n    n = int(f_max / delta_f + 1)\n    kmax = int(fISCO / delta_f)\n    kmin = int(numpy.ceil(f_lower / delta_f))\n    kmax = kmax if (kmax<n) else n\n\n    #####Calculate the Orientation#####\n    v0 = pow(piM *  kmin * delta_f,1./3)\n    chi = sqrt(spin1x**2+spin1y**2+spin1z**2)\n    kappa = (lnhatx*spin1x+lnhaty*spin1y+lnhatz*spin1z)/chi if (chi > 0.)  else 1.\n    Jx0 = mass1*mass2*lnhatx/v0 + mass1*mass1*spin1x\n    Jy0 = mass1*mass2*lnhaty/v0 + mass1*mass1*spin1y\n    Jz0 = mass1*mass2*lnhatz/v0 + mass1*mass1*spin1z\n    thetaJ = acos(Jz0 / sqrt(Jx0**2+Jy0**2+Jz0**2))\n    psiJ = atan2(Jy0, -Jx0) # FIXME: check that Jy0 and Jx0 are not both 0\n    # Rotate Lnhat back to frame where J is along z, to figure out initial alpha\n    rotLx = lnhatx*cos(thetaJ)*cos(psiJ) - lnhaty*cos(thetaJ)*sin(psiJ) + lnhatz*sin(thetaJ)\n    rotLy = lnhatx*sin(psiJ) + lnhaty*cos(psiJ)\n    alpha0 = atan2(rotLy, rotLx) # FIXME: check that rotLy and rotLx are not both 0\n    psiJ_P =psiJ + psi\n    psiJ_C =psiJ + psi + lal.PI/4.\n\n    #####Calculate the Coefficients#####\n    #quadparam = 1.\n    gamma0 = mass1*chi/mass2\n    #Calculate the spin corrections\n    # FIXME should use pycbc's function, but sigma has different expression\n    # in Andy's code, double check\n    # pn_beta, pn_sigma, pn_gamma = pycbc.pnutils.mass1_mass2_spin1z_spin2z_to_beta_sigma_gamma(\n    #                               mass1, mass2, chi*kappa, 0) # FIXME: spin2 is taken to be 0\n    pn_beta = (113.*mass1/(12.*M) - 19.*eta/6.)*chi*kappa\n    pn_sigma = (  (5.*(3.*kappa*kappa-1.)/2.) + (7. - kappa*kappa)/96.  ) * (mass1*mass1*chi*chi/M/M)\n    pn_gamma = (5.*(146597. + 7056.*eta)*mass1/(2268.*M) - 10.*eta*(1276. + 153.*eta)/81.)*chi*kappa\n    prec_fac0 = 5.*(4. + 3.*mass2/mass1)/64.\n    dtdv2 = 743./336. + 11.*eta/4.\n    dtdv3 = -4.*lal.PI + pn_beta\n    dtdv4 = 3058673./1016064. + 5429.*eta/1008. + 617.*eta*eta/144. - pn_sigma\n    dtdv5 = (-7729./672.+13.*eta/8.)*lal.PI + 9.*pn_gamma/40.\n\n    #####Calculate the Initial Euler Angles alpha_ref, beta_ref=0 and zeta_ref#####\n    gam = gamma0*v0\n    sqrtfac = sqrt(1. + 2.*kappa*gam + gam*gam)\n    logv0 = log(v0)\n    logfac1 = log(1. + kappa*gam + sqrtfac)\n    logfac2 = log(kappa + gam + sqrtfac)\n    v02 = v0 * v0\n    v03 = v0 * v02\n    kappa2 = kappa * kappa\n    kappa3 = kappa2 * kappa\n    gamma02 = gamma0 * gamma0\n    gamma03 = gamma02 *gamma0\n\n    alpha_ref = prec_fac0*(  logfac2 *( dtdv2*gamma0 + dtdv3*kappa - dtdv5*kappa/(2.*gamma02) + dtdv4/(2.*gamma0) - dtdv4*kappa2/(2.*gamma0) + (dtdv5*kappa3)/(2.*gamma02) )  +  logfac1*( - dtdv2*gamma0*kappa - dtdv3 + kappa*gamma03/2. - gamma03*kappa3/2. ) + logv0 *( dtdv2*gamma0*kappa + dtdv3 - kappa*gamma03/2. + gamma03*kappa3/2. ) + sqrtfac *( dtdv3 + dtdv4*v0/2. + dtdv5/gamma02/3. + dtdv4*kappa/(2.*gamma0) + dtdv5*kappa*v0/(6.*gamma0) - dtdv5*kappa2/(2.*gamma02) - 1/(3.*v03) - gamma0*kappa/(6.*v02) - dtdv2/v0 - gamma02/(3.*v0) + gamma02*kappa2/(2.*v0) + dtdv5*v02/3. ))  - alpha0\n\n    zeta_ref = prec_fac0*( dtdv3*gamma0*kappa*v0 + dtdv4*v0 + logfac2 *(-dtdv2*gamma0 - dtdv3*kappa + dtdv5*kappa/(2.*gamma02) - dtdv4/(2.*gamma0) + dtdv4*kappa2/(2.*gamma0) - dtdv5*kappa3/(2.*gamma02) ) + logv0 *( kappa*gamma03/2. - gamma03*kappa3/2. ) + logfac1 *( dtdv2*gamma0*kappa + dtdv3 - kappa*gamma03/2. + gamma03*kappa3/2. ) - 1/(3.*v03) - gamma0*kappa/(2.*v02) - dtdv2/v0 + dtdv4*gamma0*kappa*v02/2. + dtdv5*v02/2. + sqrtfac *( -dtdv3 - dtdv4*v0/2. - dtdv5/(3.*gamma02) - dtdv4*kappa/(2.*gamma0) - dtdv5*kappa*v0/(6.*gamma0) + dtdv5*kappa2/(2.*gamma02) + 1/(3.*v03) + gamma0*kappa/(6.*v02) + dtdv2/v0 + gamma02/(3.*v0) - gamma02*kappa2/(2.*v0) - dtdv5*v02/3. ) + dtdv5*gamma0*kappa*v03/3. )\n\n    #####Calculate the Complex sideband factors, mm=2 is first entry#####\n    RE_SBfac0= (1.+cos(thetaJ)**2)/2.\n    RE_SBfac1= sin(2.*thetaJ)\n    RE_SBfac2= 3.*sin(thetaJ)**2\n    RE_SBfac3= -sin(2.*thetaJ)\n    RE_SBfac4= (1.+cos(thetaJ)**2)/2.\n    IM_SBfac0= -cos(thetaJ)\n    IM_SBfac1= -2.*sin(thetaJ)\n    IM_SBfac2= 0.\n    IM_SBfac3= -2.*sin(thetaJ)\n    IM_SBfac4= cos(thetaJ)\n\n    #####Calculate the PN terms # FIXME replace with functions in lalsimulation #####\n    theta = -11831./9240.\n    lambdaa = -1987./3080.0\n    pfaN = 3.0/(128.0 * eta)\n    pfa2 = 5.0*(743.0/84 + 11.0 * eta)/9.0\n    pfa3 = -16.0*lal.PI + 4.0*pn_beta\n    pfa4 = 5.0*(3058.673/7.056 + 5429.0/7.0 * eta + 617.0 * eta*eta)/72.0 - \\\n            10.0*pn_sigma\n    pfa5 = 5.0/9.0 * (7729.0/84.0 - 13.0 * eta) * lal.PI - pn_gamma\n    pfl5 = 5.0/3.0 * (7729.0/84.0 - 13.0 * eta) * lal.PI - pn_gamma * 3\n    pfa6 = (11583.231236531/4.694215680 - 640.0/3.0 * lal.PI * lal.PI- \\\n            6848.0/21.0*lal.GAMMA) + \\\n            eta * (-15335.597827/3.048192 + 2255./12. * lal.PI * \\\n            lal.PI - 1760./3.*theta +12320./9.*lambdaa) + \\\n            eta*eta * 76055.0/1728.0 - \\\n            eta*eta*eta*  127825.0/1296.0\n    pfl6 = -6848.0/21.0\n    pfa7 = lal.PI * 5.0/756.0 * ( 15419335.0/336.0 + 75703.0/2.0 * eta - \\\n            14809.0 * eta*eta)\n\n    FTaN = 32.0 * eta*eta / 5.0\n    FTa2 = -(12.47/3.36 + 3.5/1.2 * eta)\n    FTa3 = 4.0 * lal.PI\n    FTa4 = -(44.711/9.072 - 92.71/5.04 * eta - 6.5/1.8 * eta*eta)\n    FTa5 = -(81.91/6.72 + 58.3/2.4 * eta) * lal.PI\n    FTa6 = (664.3739519/6.9854400 + 16.0/3.0 * lal.PI*lal.PI -\n            17.12/1.05 * lal.GAMMA +\n         (4.1/4.8 * lal.PI*lal.PI - 134.543/7.776) * eta -\n         94.403/3.024 * eta*eta - 7.75/3.24 * eta*eta*eta)\n    FTl6 = -8.56/1.05\n    FTa7 = -(162.85/5.04 - 214.745/1.728 * eta - 193.385/3.024 * eta*eta) \\\n            * lal.PI\n\n    dETaN = 2 * -eta/2.0\n    dETa1 = 2 * -(3.0/4.0 + 1.0/12.0 * eta)\n    dETa2 = 3 * -(27.0/8.0 - 19.0/8.0 * eta + 1./24.0 * eta*eta)\n    dETa3 = 4 * -(67.5/6.4 - (344.45/5.76 - 20.5/9.6 * lal.PI*lal.PI) *\n                             eta + 15.5/9.6 * eta*eta + 3.5/518.4 * eta*eta*eta)\n\n    amp0 = -4. * mass1 * mass2 / (1.0e+06 * distance * lal.PC_SI ) * \\\n                    lal.MRSUN_SI * lal.MTSUN_SI * sqrt(lal.PI/12.0)\n\n    htildeP = FrequencySeries(zeros(n,dtype=complex128), delta_f=delta_f, copy=False)\n    htildeC = FrequencySeries(zeros(n,dtype=complex128), delta_f=delta_f, copy=False)\n    spintaylorf2_kernel(htildeP.data[kmin:kmax], htildeC.data[kmin:kmax],\n                        kmin, phase_order, amplitude_order, delta_f, piM, pfaN,\n                        pfa2, pfa3, pfa4, pfa5, pfl5,\n                        pfa6, pfl6, pfa7, FTaN, FTa2,\n                        FTa3, FTa4, FTa5, FTa6,\n                        FTl6, FTa7, dETaN, dETa1, dETa2,  dETa3,\n                        amp0, tC, phi0,\n                        kappa, prec_fac0, alpha_ref, zeta_ref,\n                        dtdv2, dtdv3, dtdv4, dtdv5,\n                        RE_SBfac0, RE_SBfac1, RE_SBfac2, RE_SBfac3, RE_SBfac4,\n                        IM_SBfac0, IM_SBfac1, IM_SBfac2, IM_SBfac3, IM_SBfac4,\n                        psiJ_P, psiJ_C, gamma0)\n\n    return htildeP, htildeC", "response": "Return a SpinTaylorF2 waveform using CUDA"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread a single channel from a stream.", "response": "def _read_channel(channel, stream, start, duration):\n    \"\"\" Get channel using lalframe \"\"\"\n    channel_type = lalframe.FrStreamGetTimeSeriesType(channel, stream)\n    read_func = _fr_type_map[channel_type][0]\n    d_type = _fr_type_map[channel_type][1]\n    data = read_func(stream, channel, start, duration, 0)\n    return TimeSeries(data.data.data, delta_t=data.deltaT, epoch=start,\n                      dtype=d_type)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _is_gwf(file_path):\n    try:\n        with open(file_path, 'rb') as f:\n            if f.read(4) == b'IGWD':\n                return True\n    except IOError:\n        pass\n    return False", "response": "Test if a file is a GWF file by checking if its contents begins with\n            the magic string IGWD."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a cumulative cache file from a list of locations.", "response": "def locations_to_cache(locations, latest=False):\n    \"\"\" Return a cumulative cache file build from the list of locations\n\n    Parameters\n    ----------\n    locations : list\n        A list of strings containing files, globs, or cache files used to build\n    a combined lal cache file object.\n    latest : Optional, {False, Boolean}\n        Only return a cache with the most recent frame in the locations.\n        If false, all results are returned.\n\n    Returns\n    -------\n    cache : lal.Cache\n        A cumulative lal cache object containing the files derived from the\n    list of locations\n    \"\"\"\n    cum_cache = lal.Cache()\n    for source in locations:\n        flist = glob.glob(source)\n        if latest:\n            def relaxed_getctime(fn):\n                # when building a cache from a directory of temporary\n                # low-latency frames, files might disappear between\n                # the glob() and getctime() calls\n                try:\n                    return os.path.getctime(fn)\n                except OSError:\n                    return 0\n            flist = [max(flist, key=relaxed_getctime)]\n\n        for file_path in flist:\n            dir_name, file_name = os.path.split(file_path)\n            _, file_extension = os.path.splitext(file_name)\n\n            if file_extension in [\".lcf\", \".cache\"]:\n                cache = lal.CacheImport(file_path)\n            elif file_extension == \".gwf\" or _is_gwf(file_path):\n                cache = lalframe.FrOpen(str(dir_name), str(file_name)).cache\n            else:\n                raise TypeError(\"Invalid location name\")\n\n            cum_cache = lal.CacheMerge(cum_cache, cache)\n    return cum_cache"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads time series from a given location.", "response": "def read_frame(location, channels, start_time=None,\n               end_time=None, duration=None, check_integrity=True,\n               sieve=None):\n    \"\"\"Read time series from frame data.\n\n    Using the `location`, which can either be a frame file \".gwf\" or a\n    frame cache \".gwf\", read in the data for the given channel(s) and output\n    as a TimeSeries or list of TimeSeries.\n\n    Parameters\n    ----------\n    location : string\n        A source of gravitational wave frames. Either a frame filename\n        (can include pattern), a list of frame files, or frame cache file.\n    channels : string or list of strings\n        Either a string that contains the channel name or a list of channel\n        name strings.\n    start_time : {None, LIGOTimeGPS}, optional\n        The gps start time of the time series. Defaults to reading from the\n        beginning of the available frame(s).\n    end_time : {None, LIGOTimeGPS}, optional\n        The gps end time of the time series. Defaults to the end of the frame.\n        Note, this argument is incompatible with `duration`.\n    duration : {None, float}, optional\n        The amount of data to read in seconds. Note, this argument is\n        incompatible with `end`.\n    check_integrity : {True, bool}, optional\n        Test the frame files for internal integrity.\n    sieve : string, optional\n        Selects only frames where the frame URL matches the regular\n        expression sieve\n\n    Returns\n    -------\n    Frame Data: TimeSeries or list of TimeSeries\n        A TimeSeries or a list of TimeSeries, corresponding to the data from\n        the frame file/cache for a given channel or channels.\n    \"\"\"\n\n    if end_time and duration:\n        raise ValueError(\"end time and duration are mutually exclusive\")\n\n    if type(location) is list:\n        locations = location\n    else:\n        locations = [location]\n\n    cum_cache = locations_to_cache(locations)\n    if sieve:\n        logging.info(\"Using frames that match regexp: %s\", sieve)\n        lal.CacheSieve(cum_cache, 0, 0, None, None, sieve)\n\n    stream = lalframe.FrStreamCacheOpen(cum_cache)\n    stream.mode = lalframe.FR_STREAM_VERBOSE_MODE\n\n    if check_integrity:\n        stream.mode = (stream.mode | lalframe.FR_STREAM_CHECKSUM_MODE)\n\n    lalframe.FrSetMode(stream.mode, stream)\n\n    # determine duration of data\n    if type(channels) is list:\n        first_channel = channels[0]\n    else:\n        first_channel = channels\n\n    data_length = lalframe.FrStreamGetVectorLength(first_channel, stream)\n    channel_type = lalframe.FrStreamGetTimeSeriesType(first_channel, stream)\n    create_series_func = _fr_type_map[channel_type][2]\n    get_series_metadata_func = _fr_type_map[channel_type][3]\n    series = create_series_func(first_channel, stream.epoch, 0, 0,\n                                lal.ADCCountUnit, 0)\n    get_series_metadata_func(series, stream)\n    data_duration = data_length * series.deltaT\n\n    if start_time is None:\n        start_time = stream.epoch*1\n    if end_time is None:\n        end_time = start_time + data_duration\n\n    if type(start_time) is not lal.LIGOTimeGPS:\n        start_time = lal.LIGOTimeGPS(start_time)\n    if type(end_time) is not lal.LIGOTimeGPS:\n        end_time = lal.LIGOTimeGPS(end_time)\n\n    if duration is None:\n        duration = float(end_time - start_time)\n    else:\n        duration = float(duration)\n\n    # lalframe behaves dangerously with invalid duration so catch it here\n    if duration <= 0:\n        raise ValueError(\"Negative or null duration\")\n    #if duration > data_duration:\n    #    raise ValueError(\"Requested duration longer than available data\")\n\n    if type(channels) is list:\n        all_data = []\n        for channel in channels:\n            channel_data = _read_channel(channel, stream, start_time, duration)\n            lalframe.FrStreamSeek(stream, start_time)\n            all_data.append(channel_data)\n        return all_data\n    else:\n        return _read_channel(channels, stream, start_time, duration)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a connection to the datafind server.", "response": "def datafind_connection(server=None):\n    \"\"\" Return a connection to the datafind server\n\n    Parameters\n    -----------\n    server : {SERVER:PORT, string}, optional\n       A string representation of the server and port.\n       The port may be ommitted.\n\n    Returns\n    --------\n    connection\n        The open connection to the datafind server.\n    \"\"\"\n\n    if server:\n        datafind_server = server\n    else:\n        # Get the server name from the environment\n        if 'LIGO_DATAFIND_SERVER' in os.environ:\n            datafind_server = os.environ[\"LIGO_DATAFIND_SERVER\"]\n        else:\n            err = \"Trying to obtain the ligo datafind server url from \"\n            err += \"the environment, ${LIGO_DATAFIND_SERVER}, but that \"\n            err += \"variable is not populated.\"\n            raise ValueError(err)\n\n    # verify authentication options\n    if not datafind_server.endswith(\"80\"):\n        cert_file, key_file = glue.datafind.find_credential()\n    else:\n        cert_file, key_file = None, None\n\n    # Is a port specified in the server URL\n    dfs_fields = datafind_server.split(':', 1)\n    server = dfs_fields[0]\n    port = int(dfs_fields[1]) if len(dfs_fields) == 2 else None\n\n    # Open connection to the datafind server\n    if cert_file and key_file:\n        connection = glue.datafind.GWDataFindHTTPSConnection(\n                host=server, port=port, cert_file=cert_file, key_file=key_file)\n    else:\n        connection = glue.datafind.GWDataFindHTTPConnection(\n                host=server, port=port)\n    return connection"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the list of paths to a span of frame files.", "response": "def frame_paths(frame_type, start_time, end_time, server=None, url_type='file'):\n    \"\"\"Return the paths to a span of frame files\n\n    Parameters\n    ----------\n    frame_type : string\n        The string representation of the frame type (ex. 'H1_ER_C00_L1')\n    start_time : int\n        The start time that we need the frames to span.\n    end_time : int\n        The end time that we need the frames to span.\n    server : {None, SERVER:PORT string}, optional\n        Optional string to specify the datafind server to use. By default an\n        attempt is made to use a local datafind server.\n    url_type : string\n        Returns only frame URLs with a particular scheme or head such\n        as \"file\" or \"gsiftp\". Default is \"file\", which queries locally\n        stored frames. Option can be disabled if set to None.\n    Returns\n    -------\n    paths : list of paths\n        The list of paths to the frame files.\n\n    Examples\n    --------\n    >>> paths = frame_paths('H1_LDAS_C02_L2', 968995968, 968995968+2048)\n    \"\"\"\n    site = frame_type[0]\n    connection = datafind_connection(server)\n    connection.find_times(site, frame_type,\n                          gpsstart=start_time, gpsend=end_time)\n    cache = connection.find_frame_urls(site, frame_type, start_time, end_time,urltype=url_type)\n    paths = [entry.path for entry in cache]\n    return paths"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nqueries for the locatin of physical frames matching the frame type and return the locatin of the physical frames that match the frame type.", "response": "def query_and_read_frame(frame_type, channels, start_time, end_time,\n                         sieve=None, check_integrity=False):\n    \"\"\"Read time series from frame data.\n\n    Query for the locatin of physical frames matching the frame type. Return\n    a time series containing the channel between the given start and end times.\n\n    Parameters\n    ----------\n    frame_type : string\n        The type of frame file that we are looking for.\n    channels : string or list of strings\n        Either a string that contains the channel name or a list of channel\n        name strings.\n    start_time : LIGOTimeGPS or int\n        The gps start time of the time series. Defaults to reading from the\n        beginning of the available frame(s).\n    end_time : LIGOTimeGPS or int\n        The gps end time of the time series. Defaults to the end of the frame.\n    sieve : string, optional\n        Selects only frames where the frame URL matches the regular\n        expression sieve\n    check_integrity : boolean\n        Do an expensive checksum of the file before returning.\n\n    Returns\n    -------\n    Frame Data: TimeSeries or list of TimeSeries\n        A TimeSeries or a list of TimeSeries, corresponding to the data from\n        the frame file/cache for a given channel or channels.\n\n    Examples\n    --------\n    >>> ts = query_and_read_frame('H1_LDAS_C02_L2', 'H1:LDAS-STRAIN',\n    >>>                               968995968, 968995968+2048)\n    \"\"\"\n    # Allows compatibility with our standard tools\n    # We may want to place this into a higher level frame getting tool\n    if frame_type == 'LOSC':\n        from pycbc.frame.losc import read_frame_losc\n        return read_frame_losc(channels, start_time, end_time)\n\n    logging.info('querying datafind server')\n    paths = frame_paths(frame_type, int(start_time), int(numpy.ceil(end_time)))\n    logging.info('found files: %s' % (' '.join(paths)))\n    return read_frame(paths, channels,\n                      start_time=start_time,\n                      end_time=end_time,\n                      sieve=sieve,\n                      check_integrity=check_integrity)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_frame(location, channels, timeseries):\n    # check if a single channel or a list of channels\n    if type(channels) is list and type(timeseries) is list:\n        channels = channels\n        timeseries = timeseries\n    else:\n        channels = [channels]\n        timeseries = [timeseries]\n\n    # check that timeseries have the same start and end time\n    gps_start_times = {series.start_time for series in timeseries}\n    gps_end_times = {series.end_time for series in timeseries}\n    if len(gps_start_times) != 1 or len(gps_end_times) != 1:\n        raise ValueError(\"Start and end times of TimeSeries must be identical.\")\n\n    # check that start, end time, and duration are integers\n    gps_start_time = gps_start_times.pop()\n    gps_end_time = gps_end_times.pop()\n    duration = int(gps_end_time - gps_start_time)\n    if gps_start_time % 1 or gps_end_time % 1:\n        raise ValueError(\"Start and end times of TimeSeries must be integer seconds.\")\n\n    # create frame\n    frame = lalframe.FrameNew(epoch=gps_start_time, duration=duration,\n                              project='', run=1, frnum=1,\n                              detectorFlags=lal.LALDETECTORTYPE_ABSENT)\n\n    for i,tseries in enumerate(timeseries):\n        # get data type\n        for seriestype in _fr_type_map.keys():\n            if _fr_type_map[seriestype][1] == tseries.dtype:\n                create_series_func = _fr_type_map[seriestype][2]\n                create_sequence_func = _fr_type_map[seriestype][4]\n                add_series_func = _fr_type_map[seriestype][5]\n                break\n\n        # add time series to frame\n        series = create_series_func(channels[i], tseries.start_time,\n                       0, tseries.delta_t, lal.ADCCountUnit,\n                       len(tseries.numpy()))\n        series.data = create_sequence_func(len(tseries.numpy()))\n        series.data.data = tseries.numpy()\n        add_series_func(frame, series)\n\n    # write frame\n    lalframe.FrameWrite(frame, location)", "response": "Write a list of time series to a single frame file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_cache(self):\n        cache = locations_to_cache(self.frame_src, latest=True)\n        stream = lalframe.FrStreamCacheOpen(cache)\n        self.stream = stream", "response": "Reset the lal cache."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _retrieve_metadata(stream, channel_name):\n        lalframe.FrStreamGetVectorLength(channel_name, stream)\n        channel_type = lalframe.FrStreamGetTimeSeriesType(channel_name, stream)\n        create_series_func = _fr_type_map[channel_type][2]\n        get_series_metadata_func = _fr_type_map[channel_type][3]\n        series = create_series_func(channel_name, stream.epoch, 0, 0,\n                            lal.ADCCountUnit, 0)\n        get_series_metadata_func(series, stream)\n        return channel_type, int(1.0/series.deltaT)", "response": "Retrieve basic metadata from the object store."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _read_frame(self, blocksize):\n        try:\n            read_func = _fr_type_map[self.channel_type][0]\n            dtype = _fr_type_map[self.channel_type][1]\n            data = read_func(self.stream, self.channel_name,\n                             self.read_pos, int(blocksize), 0)\n            return TimeSeries(data.data.data, delta_t=data.deltaT,\n                              epoch=self.read_pos,\n                              dtype=dtype)\n        except Exception:\n            raise RuntimeError('Cannot read {0} frame data'.format(self.channel_name))", "response": "Try to read the block of data from the channel and return a TimeSeries object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadvancing and insert zeros in the raw_buffer.", "response": "def null_advance(self, blocksize):\n        \"\"\"Advance and insert zeros\n\n        Parameters\n        ----------\n        blocksize: int\n            The number of seconds to attempt to read from the channel\n        \"\"\"\n        self.raw_buffer.roll(-int(blocksize * self.raw_sample_rate))\n        self.read_pos += blocksize\n        self.raw_buffer.start_time += blocksize"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef advance(self, blocksize):\n        ts = self._read_frame(blocksize)\n\n        self.raw_buffer.roll(-len(ts))\n        self.raw_buffer[-len(ts):] = ts[:]\n        self.read_pos += blocksize\n        self.raw_buffer.start_time += blocksize\n        return ts", "response": "Advance the time in the buffer by blocksize seconds."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating the internal cache by incrementing the next frame file name and incrementing the next frame file name.", "response": "def update_cache_by_increment(self, blocksize):\n        \"\"\"Update the internal cache by starting from the first frame\n        and incrementing.\n\n        Guess the next frame file name by incrementing from the first found\n        one. This allows a pattern to be used for the GPS folder of the file,\n        which is indicated by `GPSX` where x is the number of digits to use.\n\n        Parameters\n        ----------\n        blocksize: int\n            Number of seconds to increment the next frame file.\n        \"\"\"\n        start = float(self.raw_buffer.end_time)\n        end = float(start + blocksize)\n\n        if not hasattr(self, 'dur'):\n            fname = glob.glob(self.frame_src[0])[0]\n            fname = os.path.splitext(os.path.basename(fname))[0].split('-')\n\n            self.beg = '-'.join([fname[0], fname[1]])\n            self.ref = int(fname[2])\n            self.dur = int(fname[3])\n\n        fstart = int(self.ref + numpy.floor((start - self.ref) / float(self.dur)) * self.dur)\n        starts = numpy.arange(fstart, end, self.dur).astype(numpy.int)\n\n        keys = []\n        for s in starts:\n            pattern = self.increment_update_cache\n            if 'GPS' in pattern:\n                n = int(pattern[int(pattern.index('GPS') + 3)])\n                pattern = pattern.replace('GPS%s' % n, str(s)[0:n])\n\n            name = '%s/%s-%s-%s.gwf' % (pattern, self.beg, s, self.dur)\n            # check that file actually exists, else abort now\n            if not os.path.exists(name):\n                logging.info(\"%s does not seem to exist yet\" % name)\n                raise RuntimeError\n\n            keys.append(name)\n        cache = locations_to_cache(keys)\n        stream = lalframe.FrStreamCacheOpen(cache)\n        self.stream = stream\n        self.channel_type, self.raw_sample_rate = \\\n            self._retrieve_metadata(self.stream, self.channel_name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nattempt to advance the frame buffer. Retry upon failure.", "response": "def attempt_advance(self, blocksize, timeout=10):\n        \"\"\" Attempt to advance the frame buffer. Retry upon failure, except\n        if the frame file is beyond the timeout limit.\n\n        Parameters\n        ----------\n        blocksize: int\n            The number of seconds to attempt to read from the channel\n        timeout: {int, 10}, Optional\n            Number of seconds before giving up on reading a frame\n\n        Returns\n        -------\n        data: TimeSeries\n            TimeSeries containg 'blocksize' seconds of frame data\n        \"\"\"\n        if self.force_update_cache:\n            self.update_cache()\n\n        try:\n            if self.increment_update_cache:\n                self.update_cache_by_increment(blocksize)\n\n            return DataBuffer.advance(self, blocksize)\n\n        except RuntimeError:\n            if lal.GPSTimeNow() > timeout + self.raw_buffer.end_time:\n                # The frame is not there and it should be by now, so we give up\n                # and treat it as zeros\n                DataBuffer.null_advance(self, blocksize)\n                return None\n            else:\n                # I am too early to give up on this frame, so we should try again\n                time.sleep(1)\n                return self.attempt_advance(blocksize, timeout=timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if the data contains any non - valid status information.", "response": "def check_valid(self, values, flag=None):\n        \"\"\"Check if the data contains any non-valid status information\n\n        Parameters\n        ----------\n        values: pycbc.types.Array\n            Array of status information\n        flag: str, optional\n            Override the default valid mask with a user defined mask.\n\n        Returns\n        -------\n        status: boolean\n            Returns True if all of the status information if valid,\n             False if any is not.\n        \"\"\"\n        if self.valid_on_zero:\n            valid = values.numpy() == 0\n        else:\n            if flag is None:\n                flag = self.valid_mask\n            valid = numpy.bitwise_and(values.numpy(), flag) == flag\n        return bool(numpy.all(valid))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_extent_valid(self, start_time, duration, flag=None):\n        sr = self.raw_buffer.sample_rate\n        s = int((start_time - self.raw_buffer.start_time) * sr)\n        e = s + int(duration * sr) + 1\n        data = self.raw_buffer[s:e]\n        return self.check_valid(data, flag=flag)", "response": "Check if the duration contains any non - valid frames."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the indices of the times lying in the flagged region.", "response": "def indices_of_flag(self, start_time, duration, times, padding=0):\n        \"\"\" Return the indices of the times lying in the flagged region\n\n        Parameters\n        ----------\n        start_time: int\n            Beginning time to request for\n        duration: int\n            Number of seconds to check.\n        padding: float\n            Number of seconds to add around flag inactive times to be considered\n        inactive as well.\n\n        Returns\n        -------\n        indices: numpy.ndarray\n            Array of indices marking the location of triggers within valid\n        time.\n        \"\"\"\n        from pycbc.events.veto import indices_outside_times\n        sr = self.raw_buffer.sample_rate\n        s = int((start_time - self.raw_buffer.start_time - padding) * sr) - 1\n        e = s + int((duration + padding) * sr) + 1\n        data = self.raw_buffer[s:e]\n        stamps = data.sample_times.numpy()\n\n        if self.valid_on_zero:\n            invalid = data.numpy() != 0\n        else:\n            invalid = numpy.bitwise_and(data.numpy(), self.valid_mask) \\\n                    != self.valid_mask\n\n        starts = stamps[invalid] - padding\n        ends = starts + 1.0 / sr + padding * 2.0\n        idx = indices_outside_times(times, starts, ends)\n        return idx"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef advance(self, blocksize):\n        try:\n            if self.increment_update_cache:\n                self.update_cache_by_increment(blocksize)\n            ts = DataBuffer.advance(self, blocksize)\n            return self.check_valid(ts)\n        except RuntimeError:\n            self.null_advance(blocksize)\n            return False", "response": "Advance the amount of time to the buffer and check if all of the status information is valid."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsaves an SNR time series into an XML document in a format compatible with BAYESTAR.", "response": "def snr_series_to_xml(snr_series, document, sngl_inspiral_id):\n    \"\"\"Save an SNR time series into an XML document, in a format compatible\n    with BAYESTAR.\n    \"\"\"\n    snr_lal = snr_series.lal()\n    snr_lal.name = 'snr'\n    snr_lal.sampleUnits = ''\n    snr_xml = _build_series(snr_lal, (u'Time', u'Time,Real,Imaginary'), None,\n                            'deltaT', 's')\n    snr_node = document.childNodes[-1].appendChild(snr_xml)\n    eid_param = ligolw_param.Param.build(u'event_id', u'ilwd:char',\n                                         sngl_inspiral_id)\n    snr_node.appendChild(eid_param)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a set of PSDs to a LIGO_LW XML document.", "response": "def make_psd_xmldoc(psddict, xmldoc=None):\n    \"\"\"Add a set of PSDs to a LIGOLW XML document. If the document is not\n    given, a new one is created first.\n    \"\"\"\n    xmldoc = ligolw.Document() if xmldoc is None else xmldoc.childNodes[0]\n\n    # the PSDs must be children of a LIGO_LW with name \"psd\"\n    root_name = u\"psd\"\n    Attributes = ligolw.sax.xmlreader.AttributesImpl\n    lw = xmldoc.appendChild(\n        ligolw.LIGO_LW(Attributes({u\"Name\": root_name})))\n\n    for instrument, psd in psddict.items():\n        xmlseries = _build_series(psd, (u\"Frequency,Real\", u\"Frequency\"),\n                                  None, 'deltaF', 's^-1')\n        fs = lw.appendChild(xmlseries)\n        fs.appendChild(ligolw_param.Param.from_pyvalue(u\"instrument\",\n                                                       instrument))\n    return xmldoc"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save(self, filename):\n        gz = filename.endswith('.gz')\n        ligolw_utils.write_filename(self.outdoc, filename, gz=gz)", "response": "Write this trigger to gracedb compatible xml format"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef upload(self, fname, gracedb_server=None, testing=True,\n               extra_strings=None):\n        \"\"\"Upload this trigger to gracedb\n\n        Parameters\n        ----------\n        fname: str\n            The name to give the xml file associated with this trigger\n        gracedb_server: string, optional\n            URL to the GraceDB web API service for uploading the event.\n            If omitted, the default will be used.\n        testing: bool\n            Switch to determine if the upload should be sent to gracedb as a\n            test trigger (True) or a production trigger (False).\n        \"\"\"\n        from ligo.gracedb.rest import GraceDb\n        import matplotlib\n        matplotlib.use('Agg')\n        import pylab\n\n        # first of all, make sure the event is saved on disk\n        # as GraceDB operations can fail later\n        self.save(fname)\n\n        if self.snr_series is not None:\n            if fname.endswith('.xml.gz'):\n                snr_series_fname = fname.replace('.xml.gz', '.hdf')\n            else:\n                snr_series_fname = fname.replace('.xml', '.hdf')\n            snr_series_plot_fname = snr_series_fname.replace('.hdf',\n                                                             '_snr.png')\n            psd_series_plot_fname = snr_series_fname.replace('.hdf',\n                                                             '_psd.png')\n            pylab.figure()\n            for ifo in self.snr_series:\n                curr_snrs = self.snr_series[ifo]\n                curr_snrs.save(snr_series_fname, group='%s/snr' % ifo)\n                pylab.plot(curr_snrs.sample_times, abs(curr_snrs),\n                           c=ifo_color(ifo), label=ifo)\n                if ifo in self.ifos:\n                    snr = self.coinc_results['foreground/%s/%s' %\n                                             (ifo, 'snr')]\n                    endt = self.coinc_results['foreground/%s/%s' %\n                                              (ifo, 'end_time')]\n                    pylab.plot([endt], [snr], c=ifo_color(ifo), marker='x')\n\n            pylab.legend()\n            pylab.xlabel('GPS time (s)')\n            pylab.ylabel('SNR')\n            pylab.savefig(snr_series_plot_fname)\n            pylab.close()\n\n            pylab.figure()\n            for ifo in self.snr_series:\n                # Undo dynamic range factor\n                curr_psd = self.psds[ifo].astype(numpy.float64)\n                curr_psd /= pycbc.DYN_RANGE_FAC ** 2.0\n                curr_psd.save(snr_series_fname, group='%s/psd' % ifo)\n                # Can't plot log(0) so start from point 1\n                pylab.loglog(curr_psd.sample_frequencies[1:],\n                             curr_psd[1:]**0.5, c=ifo_color(ifo), label=ifo)\n            pylab.legend()\n            pylab.xlim([20, 2000])\n            pylab.ylim([1E-24, 1E-21])\n            pylab.xlabel('Frequency (Hz)')\n            pylab.ylabel('ASD')\n            pylab.savefig(psd_series_plot_fname)\n\n        gid = None\n        try:\n            # try connecting to GraceDB\n            gracedb = GraceDb(gracedb_server) \\\n                    if gracedb_server is not None else GraceDb()\n\n            # create GraceDB event\n            group = 'Test' if testing else 'CBC'\n            r = gracedb.createEvent(group, \"pycbc\", fname, \"AllSky\").json()\n            gid = r[\"graceid\"]\n            logging.info(\"Uploaded event %s\", gid)\n\n            if self.is_hardware_injection:\n                gracedb.writeLabel(gid, 'INJ')\n                logging.info(\"Tagging event %s as an injection\", gid)\n\n            # upload PSDs. Note that the PSDs are already stored in the\n            # original event file and we just upload a copy of that same file\n            # here. This keeps things as they were in O2 and can be removed\n            # after updating the follow-up infrastructure\n            psd_fname = 'psd.xml.gz' if fname.endswith('.gz') else 'psd.xml'\n            gracedb.writeLog(gid, \"PyCBC PSD estimate from the time of event\",\n                             psd_fname, open(fname, \"rb\").read(), \"psd\")\n            logging.info(\"Uploaded PSDs for event %s\", gid)\n\n            # add other tags and comments\n            gracedb.writeLog(\n                    gid, \"Using PyCBC code hash %s\" % pycbc_version.git_hash)\n\n            extra_strings = [] if extra_strings is None else extra_strings\n            for text in extra_strings:\n                gracedb.writeLog(gid, text)\n\n            # upload SNR series in HDF format and plots\n            if self.snr_series is not None:\n                gracedb.writeLog(gid, 'SNR timeseries HDF file upload',\n                                 filename=snr_series_fname)\n                gracedb.writeLog(gid, 'SNR timeseries plot upload',\n                                 filename=snr_series_plot_fname,\n                                 tag_name=['background'],\n                                 displayName=['SNR timeseries'])\n                gracedb.writeLog(gid, 'PSD plot upload',\n                                 filename=psd_series_plot_fname,\n                                 tag_name=['psd'], displayName=['PSDs'])\n\n        except Exception as exc:\n            logging.error('Something failed during the upload/annotation of '\n                          'event %s on GraceDB. The event may not have been '\n                          'uploaded!', fname)\n            logging.error(str(exc))\n\n        return gid", "response": "Uploads this trigger to the GraceDB server."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef z_at_value(func, fval, unit, zmax=1000., **kwargs):\n    fval, input_is_array = ensurearray(fval)\n    # make sure fval is atleast 1D\n    if fval.size == 1 and fval.ndim == 0:\n        fval = fval.reshape(1)\n    zs = numpy.zeros(fval.shape, dtype=float)  # the output array\n    for (ii, val) in enumerate(fval):\n        try:\n            zs[ii] = astropy.cosmology.z_at_value(func, val*unit, zmax=zmax,\n                                                  **kwargs)\n        except CosmologyError:\n            # we'll get this if the z was larger than zmax; in that case we'll\n            # try bumping up zmax later to get a value\n            zs[ii] = numpy.inf\n    # check if there were any zs > zmax\n    replacemask = numpy.isinf(zs)\n    # try bumping up zmax to get a result\n    if replacemask.any():\n        # we'll keep bumping up the maxz until we can get a result\n        counter = 0  # to prevent running forever\n        while replacemask.any():\n            kwargs['zmin'] = zmax\n            zmax = 10 * zmax\n            idx = numpy.where(replacemask)\n            for ii in idx:\n                val = fval[ii]\n                try:\n                    zs[ii] = astropy.cosmology.z_at_value(\n                        func, val*unit, zmax=zmax, **kwargs)\n                    replacemask[ii] = False\n                except CosmologyError:\n                    # didn't work, try on next loop\n                    pass\n            counter += 1\n            if counter == 5:\n                # give up and warn the user\n                logging.warning(\"One or more values correspond to a \"\n                                \"redshift > {0:.1e}. The redshift for these \"\n                                \"have been set to inf. If you would like \"\n                                \"better precision, call God.\".format(zmax))\n                break\n    return formatreturn(zs, input_is_array)", "response": "r Wrapper around astropy. cosmology. z_at_value to handle numpy arrays."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _redshift(distance, **kwargs):\n    cosmology = get_cosmology(**kwargs)\n    return z_at_value(cosmology.luminosity_distance, distance, units.Mpc)", "response": "r Uses astropy to get redshift from the given luminosity distance in Mpc."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef redshift(distance, **kwargs):\n    cosmology = get_cosmology(**kwargs)\n    try:\n        z = _d2zs[cosmology.name](distance)\n    except KeyError:\n        # not a standard cosmology, call the redshift function\n        z = _redshift(distance, cosmology=cosmology)\n    return z", "response": "r Returns the redshift associated with the given luminosity distance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cosmological_quantity_from_redshift(z, quantity, strip_unit=True,\n                                        **kwargs):\n    r\"\"\"Returns the value of a cosmological quantity (e.g., age) at a redshift.\n\n    Parameters\n    ----------\n    z : float\n        The redshift.\n    quantity : str\n        The name of the quantity to get. The name may be any attribute of\n        :py:class:`astropy.cosmology.FlatLambdaCDM`.\n    strip_unit : bool, optional\n        Just return the value of the quantity, sans units. Default is True.\n    \\**kwargs :\n        All other keyword args are passed to :py:func:`get_cosmology` to\n        select a cosmology. If none provided, will use\n        :py:attr:`DEFAULT_COSMOLOGY`.\n\n    Returns\n    -------\n    float or astropy.units.quantity :\n        The value of the quantity at the requested value. If ``strip_unit`` is\n        ``True``, will return the value. Otherwise, will return the value with\n        units.\n    \"\"\"\n    cosmology = get_cosmology(**kwargs)\n    val = getattr(cosmology, quantity)(z)\n    if strip_unit:\n        val = val.value\n    return val", "response": "r Returns the value of a cosmological quantity at a redshift."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef setup_interpolant(self):\n        # for computing nearby (z < 1) redshifts\n        zs = numpy.linspace(0., 1., num=self.numpoints)\n        ds = self.cosmology.luminosity_distance(zs).value\n        self.nearby_d2z = interpolate.interp1d(ds, zs, kind='linear',\n                                                bounds_error=False)\n        # for computing far away (z > 1) redshifts\n        zs = numpy.logspace(0, numpy.log10(self.default_maxz),\n                            num=self.numpoints)\n        ds = self.cosmology.luminosity_distance(zs).value\n        self.faraway_d2z = interpolate.interp1d(ds, zs, kind='linear',\n                                                 bounds_error=False)\n        # store the default maximum distance\n        self.default_maxdist = ds.max()", "response": "Initializes the z ( d ) interpolation."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_redshift(self, dist):\n        dist, input_is_array = ensurearray(dist)\n        try:\n            zs = self.nearby_d2z(dist)\n        except TypeError:\n            # interpolant hasn't been setup yet\n            self.setup_interpolant()\n            zs = self.nearby_d2z(dist)\n        # if any points had red shifts beyond the nearby, will have nans;\n        # replace using the faraway interpolation\n        replacemask = numpy.isnan(zs)\n        if replacemask.any():\n            zs[replacemask] = self.faraway_d2z(dist[replacemask])\n            replacemask = numpy.isnan(zs)\n        # if we still have nans, means that some distances are beyond our\n        # furthest default; fall back to using astropy\n        if replacemask.any():\n            # well... check that the distance is positive and finite first\n            if not (dist > 0.).all() and numpy.isfinite(dist).all():\n                raise ValueError(\"distance must be finite and > 0\")\n            zs[replacemask] = _redshift(dist[replacemask],\n                                        cosmology=self.cosmology)\n        return formatreturn(zs, input_is_array)", "response": "Returns the redshift for the given distance."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read_acceptance_fraction(self, walkers=None):\n        group = self.sampler_group + '/acceptance_fraction'\n        if walkers is None:\n            wmask = numpy.ones(self.nwalkers, dtype=bool)\n        else:\n            wmask = numpy.zeros(self.nwalkers, dtype=bool)\n            wmask[walkers] = True\n        return self[group][wmask]", "response": "Reads the acceptance fraction of the specified walkers."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite the data to the file.", "response": "def write_acceptance_fraction(self, acceptance_fraction):\n        \"\"\"Write acceptance_fraction data to file. Results are written to\n        the ``[sampler_group]/acceptance_fraction``.\n\n        Parameters\n        -----------\n        acceptance_fraction : numpy.ndarray\n            Array of acceptance fractions to write.\n        \"\"\"\n        group = self.sampler_group + '/acceptance_fraction'\n        try:\n            self[group][:] = acceptance_fraction\n        except KeyError:\n            # dataset doesn't exist yet, create it\n            self[group] = acceptance_fraction"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_posterior(self, filename, **kwargs):\n        f = h5py.File(filename, 'w')\n\n        # Preserve top-level metadata\n        for key in self.attrs:\n            f.attrs[key] = self.attrs[key]\n\n        f.attrs['filetype'] = PosteriorFile.name\n        s = f.create_group('samples')\n        fields = self[self.samples_group].keys()\n\n        # Copy and squash fields into one dimensional arrays\n        for field_name in fields:\n            fvalue = self[self.samples_group][field_name][:]\n            thin = fvalue[:,self.thin_start:self.thin_end:self.thin_interval]\n            s[field_name] = thin.flatten()", "response": "Write posterior only file"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef geweke(x, seg_length, seg_stride, end_idx, ref_start,\n           ref_end=None, seg_start=0):\n    \"\"\" Calculates Geweke conervergence statistic for a chain of data.\n    This function will advance along the chain and calculate the\n    statistic for each step.\n\n    Parameters\n    ----------\n    x : numpy.array\n        A one-dimensional array of data.\n    seg_length : int\n        Number of samples to use for each Geweke calculation.\n    seg_stride : int\n        Number of samples to advance before next Geweke calculation.\n    end_idx : int\n        Index of last start.\n    ref_start : int\n        Index of beginning of end reference segment.\n    ref_end : int\n        Index of end of end reference segment. Default is None which\n        will go to the end of the data array.\n    seg_start : int\n        What index to start computing the statistic. Default is 0 which\n        will go to the beginning of the data array.\n\n    Returns\n    -------\n    starts : numpy.array\n        The start index of the first segment in the chain.\n    ends : numpy.array\n        The end index of the first segment in the chain.\n    stats : numpy.array\n        The Geweke convergence diagnostic statistic for the segment.\n    \"\"\"\n\n    # lists to hold statistic and end index\n    stats = []\n    ends = []\n\n    # get the beginning of all segments\n    starts = numpy.arange(seg_start, end_idx, seg_stride)\n\n    # get second segment of data at the end to compare\n    x_end = x[ref_start:ref_end]\n\n    # loop over all segments\n    for start in starts:\n\n        # find the end of the first segment\n        x_start_end = int(start + seg_length)\n\n        # get first segment\n        x_start = x[start:x_start_end]\n\n        # compute statistic\n        stats.append((x_start.mean() - x_end.mean()) / numpy.sqrt(\n            x_start.var() + x_end.var()))\n\n        # store end of first segment\n        ends.append(x_start_end)\n\n    return numpy.array(starts), numpy.array(ends), numpy.array(stats)", "response": "Calculates the convergence diagnostic statistic for a single chain of data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding the options used to select an FFT backend and controlling its performance.", "response": "def insert_fft_option_group(parser):\n    \"\"\"\n    Adds the options used to choose an FFT backend. This should be used\n    if your program supports the ability to select the FFT backend; otherwise\n    you may simply call the fft and ifft functions and rely on default\n    choices.  This function will also attempt to add any options exported\n    by available backends through a function called insert_fft_options.\n    These submodule functions should take the fft_group object as argument.\n\n    Parameters\n    ----------\n    parser : object\n        OptionParser instance\n    \"\"\"\n    fft_group = parser.add_argument_group(\"Options for selecting the\"\n                                          \" FFT backend and controlling its performance\"\n                                          \" in this program.\")\n    # We have one argument to specify the backends.  This becomes the default list used\n    # if none is specified for a particular call of fft() of ifft().  Note that this\n    # argument expects a *list* of inputs, as indicated by the nargs='*'.\n    fft_group.add_argument(\"--fft-backends\",\n                      help=\"Preference list of the FFT backends. \"\n                           \"Choices are: \\n\" + str(get_backend_names()),\n                      nargs='*', default=[])\n\n    for backend in get_backend_modules():\n        try:\n            backend.insert_fft_options(fft_group)\n        except AttributeError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef verify_fft_options(opt, parser):\n\n    if len(opt.fft_backends) > 0:\n        _all_backends = get_backend_names()\n        for backend in opt.fft_backends:\n            if backend not in _all_backends:\n                parser.error(\"Backend {0} is not available\".format(backend))\n\n    for backend in get_backend_modules():\n        try:\n            backend.verify_fft_options(opt, parser)\n        except AttributeError:\n            pass", "response": "Parses the FFT options and verifies that they are are\n       reasonable."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_cli(opt):\n\n    set_backend(opt.fft_backends)\n\n    # Eventually, we need to be able to parse command lines\n    # from more than just the current scheme's preference. But\n    # the big problem is that calling from_cli for more than one\n    # backend could cause interference; apparently, FFTW and MKL\n    # don't play nice unless FFTW has been compiled and linked\n    # with icc (and possibly numpy, scipy, and/or Python as well?)\n\n    backend = get_backend()\n    try:\n        backend.from_cli(opt)\n    except AttributeError:\n        pass", "response": "Parses the command line options and sets the FFT backend\n    for each available scheme."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfunction to update analysis boundaries as workflow is generated", "response": "def set_grb_start_end(cp, start, end):\n    \"\"\"\n    Function to update analysis boundaries as workflow is generated\n\n    Parameters\n    ----------\n    cp : pycbc.workflow.configuration.WorkflowConfigParser object\n    The parsed configuration options of a pycbc.workflow.core.Workflow.\n\n    start : int\n    The start of the workflow analysis time.\n\n    end : int\n    The end of the workflow analysis time.\n\n    Returns\n    --------\n    cp : pycbc.workflow.configuration.WorkflowConfigParser object\n    The modified WorkflowConfigParser object.\n\n    \"\"\"\n    cp.set(\"workflow\", \"start-time\", str(start))\n    cp.set(\"workflow\", \"end-time\", str(end))\n\n    return cp"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the list of files needed to run coh_PTF jobs within a PyGRB workflow.", "response": "def get_coh_PTF_files(cp, ifos, run_dir, bank_veto=False, summary_files=False):\n    \"\"\"\n    Retrieve files needed to run coh_PTF jobs within a PyGRB workflow\n\n    Parameters\n    ----------\n    cp : pycbc.workflow.configuration.WorkflowConfigParser object\n    The parsed configuration options of a pycbc.workflow.core.Workflow.\n\n    ifos : str\n    String containing the analysis interferometer IDs.\n\n    run_dir : str\n    The run directory, destination for retrieved files.\n\n    bank_veto : Boolean\n    If true, will retrieve the bank_veto_bank.xml file.\n\n    summary_files : Boolean\n    If true, will retrieve the summary page style files.\n\n    Returns\n    -------\n    file_list : pycbc.workflow.FileList object\n    A FileList containing the retrieved files.\n    \"\"\"\n    if os.getenv(\"LAL_SRC\") is None:\n        raise ValueError(\"The environment variable LAL_SRC must be set to a \"\n                         \"location containing the file lalsuite.git\")\n    else:\n        lalDir = os.getenv(\"LAL_SRC\")\n        sci_seg = segments.segment(int(cp.get(\"workflow\", \"start-time\")),\n                                   int(cp.get(\"workflow\", \"end-time\")))\n        file_list = FileList([])\n\n        # Bank veto\n        if bank_veto:\n            shutil.copy(\"%s/lalapps/src/ring/coh_PTF_config_files/\" \\\n                        \"bank_veto_bank.xml\" % lalDir, \"%s\" % run_dir)\n            bank_veto_url = \"file://localhost%s/bank_veto_bank.xml\" % run_dir\n            bank_veto = File(ifos, \"bank_veto_bank\", sci_seg,\n                             file_url=bank_veto_url)\n            bank_veto.PFN(bank_veto.cache_entry.path, site=\"local\")\n            file_list.extend(FileList([bank_veto]))\n\n        if summary_files:\n            # summary.js file\n            shutil.copy(\"%s/lalapps/src/ring/coh_PTF_config_files/\" \\\n                        \"coh_PTF_html_summary.js\" % lalDir, \"%s\" % run_dir)\n            summary_js_url = \"file://localhost%s/coh_PTF_html_summary.js\" \\\n                             % run_dir\n            summary_js = File(ifos, \"coh_PTF_html_summary_js\", sci_seg,\n                              file_url=summary_js_url)\n            summary_js.PFN(summary_js.cache_entry.path, site=\"local\")\n            file_list.extend(FileList([summary_js]))\n\n            # summary.css file\n            shutil.copy(\"%s/lalapps/src/ring/coh_PTF_config_files/\" \\\n                        \"coh_PTF_html_summary.css\" % lalDir, \"%s\" % run_dir)\n            summary_css_url = \"file://localhost%s/coh_PTF_html_summary.css\" \\\n                              % run_dir\n            summary_css = File(ifos, \"coh_PTF_html_summary_css\", sci_seg,\n                               file_url=summary_css_url)\n            summary_css.PFN(summary_css.cache_entry.path, site=\"local\")\n            file_list.extend(FileList([summary_css]))\n\n        return file_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_exttrig_file(cp, ifos, sci_seg, out_dir):\n    '''\n    Make an ExtTrig xml file containing information on the external trigger\n\n    Parameters\n    ----------\n    cp : pycbc.workflow.configuration.WorkflowConfigParser object\n    The parsed configuration options of a pycbc.workflow.core.Workflow.\n\n    ifos : str\n    String containing the analysis interferometer IDs.\n\n    sci_seg : ligo.segments.segment\n    The science segment for the analysis run.\n\n    out_dir : str\n    The output directory, destination for xml file.\n\n    Returns\n    -------\n    xml_file : pycbc.workflow.File object\n    The xml file with external trigger information.\n\n    '''\n    # Initialise objects\n    xmldoc = ligolw.Document()\n    xmldoc.appendChild(ligolw.LIGO_LW())\n    tbl = lsctables.New(lsctables.ExtTriggersTable)\n    cols = tbl.validcolumns\n    xmldoc.childNodes[-1].appendChild(tbl)\n    row = tbl.appendRow()\n\n    # Add known attributes for this GRB\n    setattr(row, \"event_ra\", float(cp.get(\"workflow\", \"ra\")))\n    setattr(row, \"event_dec\", float(cp.get(\"workflow\", \"dec\")))\n    setattr(row, \"start_time\", int(cp.get(\"workflow\", \"trigger-time\")))\n    setattr(row, \"event_number_grb\", str(cp.get(\"workflow\", \"trigger-name\")))\n\n    # Fill in all empty rows\n    for entry in cols.keys():\n        if not hasattr(row, entry):\n            if cols[entry] in ['real_4','real_8']:\n                setattr(row,entry,0.)\n            elif cols[entry] == 'int_4s':\n                setattr(row,entry,0)\n            elif cols[entry] == 'lstring':\n                setattr(row,entry,'')\n            elif entry == 'process_id':\n                row.process_id = ilwd.ilwdchar(\"external_trigger:process_id:0\")\n            elif entry == 'event_id':\n                row.event_id = ilwd.ilwdchar(\"external_trigger:event_id:0\")\n            else:\n                print(\"Column %s not recognized\" %(entry), file=sys.stderr)\n                raise ValueError\n\n    # Save file\n    xml_file_name = \"triggerGRB%s.xml\" % str(cp.get(\"workflow\",\n                                                    \"trigger-name\"))\n    xml_file_path = os.path.join(out_dir, xml_file_name)\n    utils.write_filename(xmldoc, xml_file_path)\n    xml_file_url = urlparse.urljoin(\"file:\", urllib.pathname2url(xml_file_path))\n    xml_file = File(ifos, xml_file_name, sci_seg, file_url=xml_file_url)\n    xml_file.PFN(xml_file_url, site=\"local\")\n\n    return xml_file", "response": "This function creates an ExtTrig xml file containing information on the external trigger."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_ipn_sky_files(workflow, file_url, tags=None):\n    '''\n    Retreive the sky point files for searching over the IPN error box and\n    populating it with injections.\n\n    Parameters\n    ----------\n    workflow: pycbc.workflow.core.Workflow\n        An instanced class that manages the constructed workflow.\n    file_url : string\n        The URL of the IPN sky points file.\n    tags : list of strings\n        If given these tags are used to uniquely name and identify output files\n        that would be produced in multiple calls to this function.\n\n    Returns\n    --------\n    sky_points_file : pycbc.workflow.core.File\n        File object representing the IPN sky points file.\n    '''\n    tags = tags or []\n    ipn_sky_points = resolve_url(file_url)\n    sky_points_url = urlparse.urljoin(\"file:\",\n            urllib.pathname2url(ipn_sky_points))\n    sky_points_file = File(workflow.ifos, \"IPN_SKY_POINTS\",\n            workflow.analysis_time, file_url=sky_points_url, tags=tags)\n    sky_points_file.PFN(sky_points_url, site=\"local\")\n\n    return sky_points_file", "response": "Returns the sky point files for searching over the IPN error box and populating it with injections."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_gating_node(workflow, datafind_files, outdir=None, tags=None):\n    '''\n    Generate jobs for autogating the data for PyGRB runs.\n\n    Parameters\n    ----------\n    workflow: pycbc.workflow.core.Workflow\n        An instanced class that manages the constructed workflow.\n    datafind_files : pycbc.workflow.core.FileList\n        A FileList containing the frame files to be gated.\n    outdir : string\n        Path of the output directory\n    tags : list of strings\n        If given these tags are used to uniquely name and identify output files\n        that would be produced in multiple calls to this function.\n\n    Returns\n    --------\n    condition_strain_nodes : list\n        List containing the pycbc.workflow.core.Node objects representing the\n        autogating jobs.\n    condition_strain_outs : pycbc.workflow.core.FileList\n        FileList containing the pycbc.workflow.core.File objects representing\n        the gated frame files.\n    '''\n\n    cp = workflow.cp\n    if tags is None:\n        tags = []\n\n    condition_strain_class = select_generic_executable(workflow,\n                                                       \"condition_strain\")\n    condition_strain_nodes = []\n    condition_strain_outs = FileList([])\n    for ifo in workflow.ifos:\n        input_files = FileList([datafind_file for datafind_file in \\\n                                datafind_files if datafind_file.ifo == ifo])\n        condition_strain_jobs = condition_strain_class(cp, \"condition_strain\",\n                ifo=ifo, out_dir=outdir, tags=tags)\n        condition_strain_node, condition_strain_out = \\\n                condition_strain_jobs.create_node(input_files, tags=tags)\n        condition_strain_nodes.append(condition_strain_node)\n        condition_strain_outs.extend(FileList([condition_strain_out]))\n\n    return condition_strain_nodes, condition_strain_outs", "response": "Generate a node for autogating the data for PyGRB runs."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_waveform_generator(variable_params, data,\n                              recalibration=None, gates=None,\n                              **static_params):\n    \"\"\"Creates a waveform generator for use with a model.\n\n    Parameters\n    ----------\n    variable_params : list of str\n        The names of the parameters varied.\n    data : dict\n        Dictionary mapping detector names to either a\n        :py:class:`<pycbc.types.TimeSeries TimeSeries>` or\n        :py:class:`<pycbc.types.FrequencySeries FrequencySeries>`.\n    recalibration : dict, optional\n        Dictionary mapping detector names to\n        :py:class:`<pycbc.calibration.Recalibrate>` instances for\n        recalibrating data.\n    gates : dict of tuples, optional\n        Dictionary of detectors -> tuples of specifying gate times. The\n        sort of thing returned by :py:func:`pycbc.gate.gates_from_cli`.\n\n    Returns\n    -------\n    pycbc.waveform.FDomainDetFrameGenerator\n        A waveform generator for frequency domain generation.\n    \"\"\"\n    # figure out what generator to use based on the approximant\n    try:\n        approximant = static_params['approximant']\n    except KeyError:\n        raise ValueError(\"no approximant provided in the static args\")\n    generator_function = generator.select_waveform_generator(approximant)\n    # get data parameters; we'll just use one of the data to get the\n    # values, then check that all the others are the same\n    delta_f = None\n    for d in data.values():\n        if delta_f is None:\n            delta_f = d.delta_f\n            delta_t = d.delta_t\n            start_time = d.start_time\n        else:\n            if not all([d.delta_f == delta_f, d.delta_t == delta_t,\n                        d.start_time == start_time]):\n                raise ValueError(\"data must all have the same delta_t, \"\n                                 \"delta_f, and start_time\")\n    waveform_generator = generator.FDomainDetFrameGenerator(\n        generator_function, epoch=start_time,\n        variable_args=variable_params, detectors=list(data.keys()),\n        delta_f=delta_f, delta_t=delta_t,\n        recalib=recalibration, gates=gates,\n        **static_params)\n    return waveform_generator", "response": "Creates a waveform generator for use with a model."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef low_frequency_cutoff_from_config(cp):\n    try:\n        low_frequency_cutoff = float(\n            cp.get('model', 'low-frequency-cutoff'))\n    except (NoOptionError, NoSectionError) as e:\n        logging.warning(\"Low frequency cutoff for calculation of inner \"\n                        \"product needs to be specified in config file \"\n                        \"under section 'model'\")\n        raise e\n    except Exception as e:\n        # everything the float() can throw\n        logging.warning(\"Low frequency cutoff could not be \"\n                        \"converted to float \")\n        raise e\n    return low_frequency_cutoff", "response": "Gets the low frequency cutoff from the given config file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the high frequency cutoff from the given config file.", "response": "def high_frequency_cutoff_from_config(cp):\n    \"\"\"Gets the high frequency cutoff from the given config file.\n\n    This looks for ``high-frequency-cutoff`` in the ``[model]`` section and\n    casts it to float. If none is found, will just return ``None``.\n\n    Parameters\n    ----------\n    cp : WorkflowConfigParser\n        Config file parser to read.\n\n    Returns\n    -------\n    float or None :\n        The high frequency cutoff.\n    \"\"\"\n    if cp.has_option('model', 'high-frequency-cutoff'):\n        high_frequency_cutoff = float(\n            cp.get('model', 'high-frequency-cutoff'))\n    else:\n        high_frequency_cutoff = None\n    return high_frequency_cutoff"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the log likelihood assuming the data is noise.", "response": "def _lognl(self):\n        \"\"\"Computes the log likelihood assuming the data is noise.\n\n        Since this is a constant for Gaussian noise, this is only computed once\n        then stored.\n        \"\"\"\n        try:\n            return self.__lognl\n        except AttributeError:\n            det_lognls = {}\n            for (det, d) in self._data.items():\n                kmin = self._kmin\n                kmax = self._kmax\n                det_lognls[det] = -0.5 * d[kmin:kmax].inner(d[kmin:kmax]).real\n            self.__det_lognls = det_lognls\n            self.__lognl = sum(det_lognls.values())\n            return self.__lognl"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef det_lognl(self, det):\n        try:\n            return self.__det_lognls[det]\n        except AttributeError:\n            # hasn't been calculated yet, call lognl to calculate & store\n            self._lognl()\n            # now try returning\n            return self.__det_lognls[det]", "response": "Returns the log likelihood of the noise in the given detector."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef det_cplx_loglr(self, det):\n        # try to get it from current stats\n        try:\n            return getattr(self._current_stats, '{}_cplx_loglr'.format(det))\n        except AttributeError:\n            # hasn't been calculated yet; call loglr to do so\n            self._loglr()\n            # now try returning again\n            return getattr(self._current_stats, '{}_cplx_loglr'.format(det))", "response": "Returns the complex log likelihood ratio in the given detector."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the opitmal SNR squared in the given detector.", "response": "def det_optimal_snrsq(self, det):\n        \"\"\"Returns the opitmal SNR squared in the given detector.\n\n        Parameters\n        ----------\n        det : str\n            The name of the detector.\n\n        Returns\n        -------\n        float :\n            The opimtal SNR squared.\n        \"\"\"\n        # try to get it from current stats\n        try:\n            return getattr(self._current_stats, '{}_optimal_snrsq'.format(det))\n        except AttributeError:\n            # hasn't been calculated yet; call loglr to do so\n            self._loglr()\n            # now try returning again\n            return getattr(self._current_stats, '{}_optimal_snrsq'.format(det))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write_metadata(self, fp):\n        super(GaussianNoise, self).write_metadata(fp)\n        fp.attrs['low_frequency_cutoff'] = self.low_frequency_cutoff\n        if self.high_frequency_cutoff is not None:\n            fp.attrs['high_frequency_cutoff'] = self.high_frequency_cutoff\n        if self._psds is not None:\n            fp.write_psd(self._psds)\n        try:\n            attrs = fp[fp.samples_group].attrs\n        except KeyError:\n            # group doesn't exist, create it\n            fp.create_group(fp.samples_group)\n            attrs = fp[fp.samples_group].attrs\n        attrs['lognl'] = self.lognl\n        for det in self.detectors:\n            attrs['{}_lognl'.format(det)] = self.det_lognl(det)", "response": "Adds writing the psds and lognl to the sample group s attrs."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_file_type(filename):\n    txt_extensions = [\".txt\", \".dat\", \".csv\"]\n    hdf_extensions = [\".hdf\", \".h5\", \".bkup\", \".checkpoint\"]\n    for ext in hdf_extensions:\n        if filename.endswith(ext):\n            with _h5py.File(filename, 'r') as fp:\n                filetype = fp.attrs['filetype']\n            return filetypes[filetype]\n    for ext in txt_extensions:\n        if filename.endswith(ext):\n            return InferenceTXTFile\n    raise TypeError(\"Extension is not supported.\")", "response": "Returns the type of the file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading the given file into a new InferenceFile object.", "response": "def loadfile(path, mode=None, filetype=None, **kwargs):\n    \"\"\"Loads the given file using the appropriate InferenceFile class.\n\n    If ``filetype`` is not provided, this will try to retreive the ``filetype``\n    from the file's ``attrs``. If the file does not exist yet, an IOError will\n    be raised if ``filetype`` is not provided.\n\n    Parameters\n    ----------\n    path : str\n        The filename to load.\n    mode : str, optional\n        What mode to load the file with, e.g., 'w' for write, 'r' for read,\n        'a' for append. Default will default to h5py.File's mode, which is 'a'.\n    filetype : str, optional\n        Force the file to be loaded with the given class name. This must be\n        provided if creating a new file.\n\n    Returns\n    -------\n    filetype instance\n        An open file handler to the file. The class used for IO with the file\n        is determined by the ``filetype`` keyword (if provided) or the\n        ``filetype`` stored in the file (if not provided).\n    \"\"\"\n    if filetype is None:\n        # try to read the file to get its filetype\n        try:\n            fileclass = get_file_type(path)\n        except IOError:\n            # file doesn't exist, filetype must be provided\n            raise IOError(\"The file appears not to exist. In this case, \"\n                          \"filetype must be provided.\")\n    else:\n        fileclass = filetypes[filetype]\n    return fileclass(path, mode=mode, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_integrity(filename):\n    # check that the file exists\n    if not os.path.exists(filename):\n        raise ValueError(\"file {} does not exist\".format(filename))\n    # if the file is corrupted such that it cannot be opened, the next line\n    # will raise an IOError\n    with loadfile(filename, 'r') as fp:\n        # check that all datasets in samples have the same shape\n        parameters = fp[fp.samples_group].keys()\n        # but only do the check if parameters have been written\n        if len(parameters) > 0:\n            group = fp.samples_group + '/{}'\n            # use the first parameter as a reference shape\n            ref_shape = fp[group.format(parameters[0])].shape\n            if not all(fp[group.format(param)].shape == ref_shape\n                       for param in parameters):\n                raise IOError(\"not all datasets in the samples group have the \"\n                              \"same shape\")\n            # check that we can read the first/last sample\n            firstidx = tuple([0]*len(ref_shape))\n            lastidx = tuple([-1]*len(ref_shape))\n            for param in parameters:\n                _ = fp[group.format(param)][firstidx]\n                _ = fp[group.format(param)][lastidx]", "response": "Checks the integrity of an InferenceFile."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef validate_checkpoint_files(checkpoint_file, backup_file):\n    # check if checkpoint file exists and is valid\n    try:\n        check_integrity(checkpoint_file)\n        checkpoint_valid = True\n    except (ValueError, KeyError, IOError):\n        checkpoint_valid = False\n    # backup file\n    try:\n        check_integrity(backup_file)\n        backup_valid = True\n    except (ValueError, KeyError, IOError):\n        backup_valid = False\n    # check if there are any samples in the file; if not, we'll just start from\n    # scratch\n    if checkpoint_valid:\n        with loadfile(checkpoint_file, 'r') as fp:\n            try:\n                group = '{}/{}'.format(fp.samples_group, fp.variable_params[0])\n                nsamples = fp[group].size\n                checkpoint_valid = nsamples != 0\n            except KeyError:\n                checkpoint_valid = False\n    # check if there are any samples in the backup file\n    if backup_valid:\n        with loadfile(backup_file, 'r') as fp:\n            try:\n                group = '{}/{}'.format(fp.samples_group, fp.variable_params[0])\n                backup_nsamples = fp[group].size\n                backup_valid = backup_nsamples != 0\n            except KeyError:\n                backup_valid = False\n    # check that the checkpoint and backup have the same number of samples;\n    # if not, assume the checkpoint has the correct number\n    if checkpoint_valid and backup_valid:\n        backup_valid = nsamples == backup_nsamples\n    # decide what to do based on the files' statuses\n    if checkpoint_valid and not backup_valid:\n        # copy the checkpoint to the backup\n        logging.info(\"Backup invalid; copying checkpoint file\")\n        shutil.copy(checkpoint_file, backup_file)\n        backup_valid = True\n    elif backup_valid and not checkpoint_valid:\n        logging.info(\"Checkpoint invalid; copying backup file\")\n        # copy the backup to the checkpoint\n        shutil.copy(backup_file, checkpoint_file)\n        checkpoint_valid = True\n    return checkpoint_valid", "response": "Checks if the checkpoint and backup files are valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a list of variable params that are common across all input files.", "response": "def get_common_parameters(input_files, collection=None):\n    \"\"\"Gets a list of variable params that are common across all input files.\n\n    If no common parameters are found, a ``ValueError`` is raised.\n\n    Parameters\n    ----------\n    input_files : list of str\n        List of input files to load.\n    collection : str, optional\n        What group of parameters to load. Can be the name of a list of\n        parameters stored in the files' attrs (e.g., \"variable_params\"), or\n        \"all\". If \"all\", will load all of the parameters in the files'\n        samples group. Default is to load all.\n\n    Returns\n    -------\n    list :\n        List of the parameter names.\n    \"\"\"\n    if collection is None:\n        collection = \"all\"\n    parameters = []\n    for fn in input_files:\n        fp = loadfile(fn, 'r')\n        if collection == 'all':\n            ps = fp[fp.samples_group].keys()\n        else:\n            ps = fp.attrs[collection]\n        parameters.append(set(ps))\n        fp.close()\n    parameters = list(set.intersection(*parameters))\n    if parameters == []:\n        raise ValueError(\"no common parameters found for collection {} in \"\n                         \"files {}\".format(collection, ', '.join(input_files)))\n    return parameters"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef results_from_cli(opts, load_samples=True, **kwargs):\n\n    # lists for files and samples from all input files\n    fp_all = []\n    samples_all = []\n\n    input_files = opts.input_file\n    if isinstance(input_files, str):\n        input_files = [input_files]\n\n    # loop over all input files\n    for input_file in input_files:\n        logging.info(\"Reading input file %s\", input_file)\n\n        # read input file\n        fp = loadfile(input_file, \"r\")\n\n        # load the samples\n        if load_samples:\n            logging.info(\"Loading samples\")\n\n            # check if need extra parameters for a non-sampling parameter\n            file_parameters, ts = _transforms.get_common_cbc_transforms(\n                opts.parameters, fp.variable_params)\n\n            # read samples from file\n            samples = fp.samples_from_cli(opts, parameters=file_parameters, **kwargs)\n\n            logging.info(\"Using {} samples\".format(samples.size))\n\n            # add parameters not included in file\n            samples = _transforms.apply_transforms(samples, ts)\n\n        # else do not read samples\n        else:\n            samples = None\n\n        # add results to lists from all input files\n        if len(input_files) > 1:\n            fp_all.append(fp)\n            samples_all.append(samples)\n\n        # else only one input file then do not return lists\n        else:\n            fp_all = fp\n            samples_all = samples\n\n    return fp_all, opts.parameters, opts.parameters_labels, samples_all", "response": "Loads an inference result file along with any labels associated with it\n    from the command line options."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef injections_from_cli(opts):\n    input_files = opts.input_file\n    if isinstance(input_files, str):\n        input_files = [input_files]\n    injections = None\n    # loop over all input files getting the injection files\n    for input_file in input_files:\n        fp = loadfile(input_file, 'r')\n        these_injs = fp.read_injections()\n        if injections is None:\n            injections = these_injs\n        else:\n            injections = injections.append(these_injs)\n    return injections", "response": "Gets the injection parameters from the inference file given by opts. input_file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _unset_required(self):\n        self._required_args = [act for act in self._actions if act.required]\n        for act in self._required_args:\n            act.required = False", "response": "Convenience function to turn off required arguments for first parse."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_known_args(self, args=None, namespace=None):\n        # run parse args once to make sure the name space is populated with\n        # the input files. We'll turn off raising NoInputFileErrors on this\n        # pass\n        self.no_input_file_err = True\n        self._unset_required()\n        opts, extra_opts = super(ResultsArgumentParser, self).parse_known_args(\n            args, namespace)\n        # now do it again\n        self.no_input_file_err = False\n        self._reset_required()\n        opts, extra_opts = super(ResultsArgumentParser, self).parse_known_args(\n            args, opts)\n        # populate the parameters option if it wasn't specified\n        if opts.parameters is None:\n            parameters = get_common_parameters(opts.input_file,\n                                               collection='variable_params')\n            # now call parse parameters action to populate the namespace\n            self.actions['parameters'](self, opts, parameters)\n        # parse the sampler-specific options and check for any unknowns\n        unknown = []\n        for fn in opts.input_file:\n            fp = loadfile(fn, 'r')\n            sampler_parser, _ = fp.extra_args_parser(skip_args=self.skip_args)\n            if sampler_parser is not None:\n                opts, still_unknown = sampler_parser.parse_known_args(\n                    extra_opts, namespace=opts)\n                unknown.append(set(still_unknown))\n        # the intersection of the unknowns are options not understood by\n        # any of the files\n        if len(unknown) > 0:\n            unknown = set.intersection(*unknown)\n        return opts, list(unknown)", "response": "Parse the input - file dependent arguments and populate the namespace with the options that are not recognized by the parser."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd the options used to call pycbc. inference. io. results_from_cli function to the parser.", "response": "def add_results_option_group(self):\n        \"\"\"Adds the options used to call pycbc.inference.io.results_from_cli\n        function to the parser.\n\n        These are options releated to loading the results from a run of\n        pycbc_inference, for purposes of plotting and/or creating tables.\n\n        Any argument strings included in the ``skip_args`` attribute will not\n        be added.\n        \"\"\"\n        results_reading_group = self.add_argument_group(\n            title=\"Arguments for loading results\",\n            description=\"Additional, file-specific arguments may also be \"\n            \"provided, depending on what input-files are given. See \"\n            \"--file-help for details.\")\n        results_reading_group.add_argument(\n            \"--input-file\", type=str, required=True, nargs=\"+\",\n            action=ParseLabelArg, metavar='FILE[:LABEL]',\n            help=\"Path to input HDF file(s). A label may be specified for \"\n                 \"each input file to use for plots when multiple files are \"\n                 \"specified.\")\n        # advanced help\n        results_reading_group.add_argument(\n            \"-H\", \"--file-help\",\n            action=PrintFileParams, skip_args=self.skip_args,\n            help=\"Based on the provided input-file(s), print all available \"\n                 \"parameters that may be retrieved and all possible functions \"\n                 \"on those parameters. Also print available additional \"\n                 \"arguments that may be passed. This option is like an \"\n                 \"advanced --help: if run, the program will just print the \"\n                 \"information to screen, then exit.\")\n        results_reading_group.add_argument(\n            \"--parameters\", type=str, nargs=\"+\", metavar=\"PARAM[:LABEL]\",\n            action=ParseParametersArg,\n            help=\"Name of parameters to load. If none provided will load all \"\n                 \"of the model params in the input-file. If provided, the \"\n                 \"parameters can be any of the model params or posterior \"\n                 \"stats (loglikelihood, logprior, etc.) in the input file(s), \"\n                 \"derived parameters from them, or any function of them. If \"\n                 \"multiple files are provided, any parameter common to all \"\n                 \"files may be used. Syntax for functions is python; any math \"\n                 \"functions in the numpy libary may be used. Can optionally \"\n                 \"also specify a LABEL for each parameter. If no LABEL is \"\n                 \"provided, PARAM will used as the LABEL. If LABEL is the \"\n                 \"same as a parameter in pycbc.waveform.parameters, the label \"\n                 \"property of that parameter will be used (e.g., if LABEL \"\n                 \"were 'mchirp' then {} would be used). To see all possible \"\n                 \"parameters that may be used with the given input file(s), \"\n                 \"as well as all avaiable functions, run --file-help, along \"\n                 \"with one or more input files.\".format(\n                    _waveform.parameters.mchirp.label))\n        return results_reading_group"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets VCS info and write version info to version. py", "response": "def get_version_info():\n    \"\"\"Get VCS info and write version info to version.py\n    \"\"\"\n    from pycbc import _version_helper\n\n    class vdummy(object):\n        def __getattr__(self, attr):\n            return ''\n\n    # If this is a pycbc git repo always populate version information using GIT\n    try:\n        vinfo = _version_helper.generate_git_version_info()\n    except:\n        vinfo = vdummy()\n        vinfo.version = '1.13.dev7'\n        vinfo.release = 'False'\n\n    with open('pycbc/version.py', 'w') as f:\n        f.write(\"# coding: utf-8\\n\")\n        f.write(\"# Generated by setup.py for PyCBC on %s.\\n\\n\"\n                % vinfo.build_date)\n\n        # print general info\n        f.write('version = \\'%s\\'\\n' % vinfo.version)\n        f.write('date = \\'%s\\'\\n' % vinfo.date)\n        f.write('release = %s\\n' % vinfo.release)\n        f.write('last_release = \\'%s\\'\\n' % vinfo.last_release)\n\n        # print git info\n        f.write('\\ngit_hash = \\'%s\\'\\n' % vinfo.hash)\n        f.write('git_branch = \\'%s\\'\\n' % vinfo.branch)\n        f.write('git_tag = \\'%s\\'\\n' % vinfo.tag)\n        f.write('git_author = \\'%s\\'\\n' % vinfo.author)\n        f.write('git_committer = \\'%s\\'\\n' % vinfo.committer)\n        f.write('git_status = \\'%s\\'\\n' % vinfo.status)\n        f.write('git_builder = \\'%s\\'\\n' % vinfo.builder)\n        f.write('git_build_date = \\'%s\\'\\n' % vinfo.build_date)\n        f.write('git_verbose_msg = \"\"\"Version: %s\\n'\n                'Branch: %s\\n'\n                'Tag: %s\\n'\n                'Id: %s\\n'\n                'Builder: %s\\n'\n                'Build date: %s\\n'\n                'Repository status is %s\"\"\"\\n' %(\n                                               vinfo.version,\n                                               vinfo.branch,\n                                               vinfo.tag,\n                                               vinfo.hash,\n                                               vinfo.builder,\n                                               vinfo.build_date,\n                                               vinfo.status))\n        f.write('from pycbc._version import *\\n')\n        version = vinfo.version\n\n    from pycbc import version\n    version = version.version\n    return version"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef table(columns, names, page_size=None, format_strings=None):\n    if page_size is None:\n        page = 'disable'\n    else:\n        page = 'enable'\n\n    div_id = uuid.uuid4()\n\n    column_descriptions = []\n    for column, name in zip(columns, names):\n        if column.dtype.kind == 'S':\n            ctype = 'string'\n        else:\n            ctype = 'number'\n        column_descriptions.append((ctype, name))\n\n    data = []\n    for item in zip(*columns):\n        data.append(list(item))\n\n    return google_table_template.render(div_id=div_id,\n                                page_enable=page,\n                                column_descriptions = column_descriptions,\n                                page_size=page_size,\n                                data=data,\n                                format_strings=format_strings,\n                               )", "response": "Return an html table of this data set"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_timeseries(path, group=None):\n    ext = _os.path.splitext(path)[1]\n    if ext == '.npy':\n        data = _numpy.load(path)\n    elif ext == '.txt':\n        data = _numpy.loadtxt(path)\n    elif ext == '.hdf':\n        key = 'data' if group is None else group\n        f = h5py.File(path)\n        data = f[key][:]\n        series = TimeSeries(data, delta_t=f[key].attrs['delta_t'],\n                                  epoch=f[key].attrs['start_time'])\n        f.close()\n        return series\n    else:\n        raise ValueError('Path must end with .npy, .hdf, or .txt')\n\n    if data.ndim == 2:\n        delta_t = (data[-1][0] - data[0][0]) / (len(data)-1)\n        epoch = _lal.LIGOTimeGPS(data[0][0])\n        return TimeSeries(data[:,1], delta_t=delta_t, epoch=epoch)\n    elif data.ndim == 3:\n        delta_t = (data[-1][0] - data[0][0]) / (len(data)-1)\n        epoch = _lal.LIGOTimeGPS(data[0][0])\n        return TimeSeries(data[:,1] + 1j*data[:,2],\n                          delta_t=delta_t, epoch=epoch)\n    else:\n        raise ValueError('File has %s dimensions, cannot convert to Array, \\\n                          must be 2 (real) or 3 (complex)' % data.ndim)", "response": "Load a TimeSeries from a. hdf or. txt file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef prepend_zeros(self, num):\n        self.resize(len(self) + num)\n        self.roll(num)\n        self._epoch = self._epoch - num * self._delta_t", "response": "Prepend num zeros onto the beginning of this TimeSeries."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the slice of the time series that contains the time range in GPS seconds.", "response": "def time_slice(self, start, end):\n        \"\"\"Return the slice of the time series that contains the time range\n        in GPS seconds.\n        \"\"\"\n        if start < self.start_time:\n            raise ValueError('Time series does not contain a time as early as %s' % start)\n\n        if end > self.end_time:\n            raise ValueError('Time series does not contain a time as late as %s' % end)\n\n        start_idx = int((start - self.start_time) * self.sample_rate)\n        end_idx = int((end - self.start_time) * self.sample_rate)\n        return self[start_idx:end_idx]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn an Array containing the sample times.", "response": "def get_sample_times(self):\n        \"\"\"Return an Array containing the sample times.\n        \"\"\"\n        if self._epoch is None:\n            return Array(range(len(self))) * self._delta_t\n        else:\n            return Array(range(len(self))) * self._delta_t + float(self._epoch)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef at_time(self, time, nearest_sample=False):\n        if nearest_sample:\n            time += self.delta_t / 2.0\n        return self[int((time-self.start_time)*self.sample_rate)]", "response": "Return the value at the specified gps time"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef almost_equal_elem(self,other,tol,relative=True,dtol=0.0):\n        # Check that the delta_t tolerance is non-negative; raise an exception\n        # if needed.\n        if (dtol < 0.0):\n            raise ValueError(\"Tolerance in delta_t cannot be negative\")\n        if super(TimeSeries,self).almost_equal_elem(other,tol=tol,relative=relative):\n            if relative:\n                return (self._epoch == other._epoch and\n                        abs(self._delta_t-other._delta_t) <= dtol*self._delta_t)\n            else:\n                return (self._epoch == other._epoch and\n                        abs(self._delta_t-other._delta_t) <= dtol)\n        else:\n            return False", "response": "Compare whether two time series elements are almost equal with the given element."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nproduces a LAL time series object equivalent to self.", "response": "def lal(self):\n        \"\"\"Produces a LAL time series object equivalent to self.\n\n        Returns\n        -------\n        lal_data : {lal.*TimeSeries}\n            LAL time series object containing the same data as self.\n            The actual type depends on the sample's dtype.  If the epoch of\n            self is 'None', the epoch of the returned LAL object will be\n            LIGOTimeGPS(0,0); otherwise, the same as that of self.\n\n        Raises\n        ------\n        TypeError\n            If time series is stored in GPU memory.\n        \"\"\"\n        lal_data = None\n        if self._epoch is None:\n            ep = _lal.LIGOTimeGPS(0,0)\n        else:\n            ep = self._epoch\n\n        if self._data.dtype == _numpy.float32:\n            lal_data = _lal.CreateREAL4TimeSeries(\"\",ep,0,self.delta_t,_lal.SecondUnit,len(self))\n        elif self._data.dtype == _numpy.float64:\n            lal_data = _lal.CreateREAL8TimeSeries(\"\",ep,0,self.delta_t,_lal.SecondUnit,len(self))\n        elif self._data.dtype == _numpy.complex64:\n            lal_data = _lal.CreateCOMPLEX8TimeSeries(\"\",ep,0,self.delta_t,_lal.SecondUnit,len(self))\n        elif self._data.dtype == _numpy.complex128:\n            lal_data = _lal.CreateCOMPLEX16TimeSeries(\"\",ep,0,self.delta_t,_lal.SecondUnit,len(self))\n\n        lal_data.data.data[:] = self.numpy()\n\n        return lal_data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef crop(self, left, right):\n        if left + right > self.duration:\n            raise ValueError('Cannot crop more data than we have')\n\n        s = int(left * self.sample_rate)\n        e = len(self) - int(right * self.sample_rate)\n        return self[s:e]", "response": "Remove given seconds from either end of time series."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef save_to_wav(self, file_name):\n        scaled = _numpy.int16(self.numpy()/max(abs(self)) * 32767)\n        write_wav(file_name, self.sample_rate, scaled)", "response": "Save this time series to a wav format audio file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef psd(self, segment_duration, **kwds):\n        from pycbc.psd import welch\n        seg_len = int(segment_duration * self.sample_rate)\n        seg_stride = int(seg_len / 2)\n        return welch(self, seg_len=seg_len,\n                           seg_stride=seg_stride,\n                           **kwds)", "response": "Calculate the power spectral density of this time series."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwhitening the time - domain filter and return a new time - domain filter.", "response": "def whiten(self, segment_duration, max_filter_duration, trunc_method='hann',\n                     remove_corrupted=True, low_frequency_cutoff=None,\n                     return_psd=False, **kwds):\n        \"\"\" Return a whitened time series\n\n        Parameters\n        ----------\n        segment_duration: float\n            Duration in seconds to use for each sample of the spectrum.\n        max_filter_duration : int\n            Maximum length of the time-domain filter in seconds.\n        trunc_method : {None, 'hann'}\n            Function used for truncating the time-domain filter.\n            None produces a hard truncation at `max_filter_len`.\n        remove_corrupted : {True, boolean}\n            If True, the region of the time series corrupted by the whitening\n            is excised before returning. If false, the corrupted regions\n            are not excised and the full time series is returned.\n        low_frequency_cutoff : {None, float}\n            Low frequency cutoff to pass to the inverse spectrum truncation.\n            This should be matched to a known low frequency cutoff of the\n            data if there is one.\n        return_psd : {False, Boolean}\n            Return the estimated and conditioned PSD that was used to whiten\n            the data.\n        kwds : keywords\n            Additional keyword arguments are passed on to the `pycbc.psd.welch` method.\n\n        Returns\n        -------\n        whitened_data : TimeSeries\n            The whitened time series\n        \"\"\"\n        from pycbc.psd import inverse_spectrum_truncation, interpolate\n        # Estimate the noise spectrum\n        psd = self.psd(segment_duration, **kwds)\n        psd = interpolate(psd, self.delta_f)\n        max_filter_len = int(max_filter_duration * self.sample_rate)\n\n        # Interpolate and smooth to the desired corruption length\n        psd = inverse_spectrum_truncation(psd,\n                   max_filter_len=max_filter_len,\n                   low_frequency_cutoff=low_frequency_cutoff,\n                   trunc_method=trunc_method)\n\n        # Whiten the data by the asd\n        white = (self.to_frequencyseries() / psd**0.5).to_timeseries()\n\n        if remove_corrupted:\n            white = white[int(max_filter_len/2):int(len(self)-max_filter_len/2)]\n\n        if return_psd:\n            return white, psd\n\n        return white"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the interpolated 2d qtransform of this data.", "response": "def qtransform(self, delta_t=None, delta_f=None, logfsteps=None,\n                  frange=None, qrange=(4,64), mismatch=0.2, return_complex=False):\n        \"\"\" Return the interpolated 2d qtransform of this data\n\n        Parameters\n        ----------\n        delta_t : {self.delta_t, float}\n            The time resolution to interpolate to\n        delta_f : float, Optional\n            The frequency resolution to interpolate to\n        logfsteps : int\n            Do a log interpolation (incompatible with delta_f option) and set\n            the number of steps to take.\n        frange : {(30, nyquist*0.8), tuple of ints}\n            frequency range\n        qrange : {(4, 64), tuple}\n            q range\n        mismatch : float\n            Mismatch between frequency tiles\n        return_complex: {False, bool}\n            return the raw complex series instead of the normalized power.\n\n        Returns\n        -------\n        times : numpy.ndarray\n            The time that the qtransform is sampled.\n        freqs : numpy.ndarray\n            The frequencies that the qtransform is sampled.\n        qplane : numpy.ndarray (2d)\n            The two dimensional interpolated qtransform of this time series.\n        \"\"\"\n        from pycbc.filter.qtransform import qtiling, qplane\n        from scipy.interpolate import interp2d\n\n        if frange is None:\n            frange = (30, int(self.sample_rate / 2 * 8))\n\n        q_base = qtiling(self, qrange, frange, mismatch)\n        _, times, freqs, q_plane = qplane(q_base, self.to_frequencyseries(),\n                                          return_complex=return_complex)\n        if logfsteps and delta_f:\n            raise ValueError(\"Provide only one (or none) of delta_f and logfsteps\")\n\n        # Interpolate if requested\n        if delta_f or delta_t or logfsteps:\n            if return_complex:\n                interp_amp = interp2d(times, freqs, abs(q_plane))\n                interp_phase = interp2d(times, freqs, _numpy.angle(q_plane))\n            else:\n                interp = interp2d(times, freqs, q_plane)\n\n        if delta_t:\n            times = _numpy.arange(float(self.start_time),\n                                    float(self.end_time), delta_t)\n        if delta_f:\n            freqs = _numpy.arange(int(frange[0]), int(frange[1]), delta_f)\n        if logfsteps:\n            freqs = _numpy.logspace(_numpy.log10(frange[0]),\n                                    _numpy.log10(frange[1]),\n                                     logfsteps)\n\n        if delta_f or delta_t or logfsteps:\n            if return_complex:\n                q_plane = _numpy.exp(1.0j * interp_phase(times, freqs))\n                q_plane *= interp_amp(times, freqs)\n            else:\n                q_plane = interp(times, freqs)\n\n        return times, freqs, q_plane"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nnotching filter the time series using an FIR filtered generated from the ideal response passed through a time-domain kaiser window (beta = 5.0) The suppression of the notch filter is related to the bandwidth and the number of samples in the filter length. For a few Hz bandwidth, a length corresponding to a few seconds is typically required to create significant suppression in the notched band. Parameters ---------- Time Series: TimeSeries The time series to be notched. f1: float The start of the frequency suppression. f2: float The end of the frequency suppression. order: int Number of corrupted samples on each side of the time series beta: float Beta parameter of the kaiser window that sets the side lobe attenuation.", "response": "def notch_fir(self, f1, f2, order, beta=5.0, remove_corrupted=True):\n        \"\"\" notch filter the time series using an FIR filtered generated from\n        the ideal response passed through a time-domain kaiser\n        window (beta = 5.0)\n\n        The suppression of the notch filter is related to the bandwidth and\n        the number of samples in the filter length. For a few Hz bandwidth,\n        a length corresponding to a few seconds is typically\n        required to create significant suppression in the notched band.\n\n        Parameters\n        ----------\n        Time Series: TimeSeries\n            The time series to be notched.\n        f1: float\n            The start of the frequency suppression.\n        f2: float\n            The end of the frequency suppression.\n        order: int\n            Number of corrupted samples on each side of the time series\n        beta: float\n            Beta parameter of the kaiser window that sets the side lobe attenuation.\n        \"\"\"\n        from pycbc.filter import notch_fir\n        ts = notch_fir(self, f1, f2, order, beta=beta)\n        if remove_corrupted:\n            ts = ts[order:len(ts)-order]\n        return ts"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fir_zero_filter(self, coeff):\n        from pycbc.filter import fir_zero_filter\n        return self._return(fir_zero_filter(coeff, self))", "response": "Filter the timeseries with a set of FIR coefficients and return a new time series with the shifted regions zeroed out."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsave the time series to a Numpy hdf or text file.", "response": "def save(self, path, group = None):\n        \"\"\"\n        Save time series to a Numpy .npy, hdf, or text file. The first column\n        contains the sample times, the second contains the values.\n        In the case of a complex time series saved as text, the imaginary\n        part is written as a third column. When using hdf format, the data is stored\n        as a single vector, along with relevant attributes.\n\n        Parameters\n        ----------\n        path: string\n            Destination file path. Must end with either .hdf, .npy or .txt.\n\n        group: string\n            Additional name for internal storage use. Ex. hdf storage uses\n            this as the key value.\n\n        Raises\n        ------\n        ValueError\n            If path does not end in .npy or .txt.\n        \"\"\"\n\n        ext = _os.path.splitext(path)[1]\n        if ext == '.npy':\n            output = _numpy.vstack((self.sample_times.numpy(), self.numpy())).T\n            _numpy.save(path, output)\n        elif ext == '.txt':\n            if self.kind == 'real':\n                output = _numpy.vstack((self.sample_times.numpy(),\n                                        self.numpy())).T\n            elif self.kind == 'complex':\n                output = _numpy.vstack((self.sample_times.numpy(),\n                                        self.numpy().real,\n                                        self.numpy().imag)).T\n            _numpy.savetxt(path, output)\n        elif ext =='.hdf':\n            key = 'data' if group is None else group\n            f = h5py.File(path)\n            ds = f.create_dataset(key, data=self.numpy(), compression='gzip',\n                                  compression_opts=9, shuffle=True)\n            ds.attrs['start_time'] = float(self.start_time)\n            ds.attrs['delta_t'] = float(self.delta_t)\n        else:\n            raise ValueError('Path must end with .npy, .txt or .hdf')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_frequencyseries(self, delta_f=None):\n        from pycbc.fft import fft\n        if not delta_f:\n            delta_f = 1.0 / self.duration\n\n        # add 0.5 to round integer\n        tlen  = int(1.0 / delta_f / self.delta_t + 0.5)\n        flen = int(tlen / 2 + 1)\n\n        if tlen < len(self):\n            raise ValueError(\"The value of delta_f (%s) would be \"\n                             \"undersampled. Maximum delta_f \"\n                             \"is %s.\" % (delta_f, 1.0 / self.duration))\n        if not delta_f:\n            tmp = self\n        else:\n            tmp = TimeSeries(zeros(tlen, dtype=self.dtype),\n                             delta_t=self.delta_t, epoch=self.start_time)\n            tmp[:len(self)] = self[:]\n\n        f = FrequencySeries(zeros(flen,\n                           dtype=complex_same_precision_as(self)),\n                           delta_f=delta_f)\n        fft(tmp, f)\n        return f", "response": "Return the fourier transform of this time series."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_into(self, other):\n        # only handle equal sample rate for now.\n        if self.sample_rate != other.sample_rate:\n            raise ValueError('Sample rate must be the same')\n\n        # Other is disjoint\n        if ((other.start_time > self.end_time) or\n           (self.start_time > other.end_time)):\n            return self.copy()\n\n        other = other.copy()\n        dt = float((other.start_time - self.start_time) * self.sample_rate)\n        if not dt.is_integer():\n            diff = (dt - _numpy.floor(dt))\n            other.resize(len(other) + (len(other) + 1) % 2 + 1)\n            other = other.cyclic_time_shift(diff)\n\n        ts = self.copy()\n        start = max(other.start_time, self.start_time)\n        end = min(other.end_time, self.end_time)\n        part = ts.time_slice(start, end)\n        part += other.time_slice(start, end)\n        return ts", "response": "Return the sum of the two time series accounting for the time stamp."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the match between the two TimeSeries or FrequencySeries. Return the match between two waveforms. This is equivelant to the overlap maximized over time and phase. By default, the other vector will be resized to match self. This may remove high frequency content or the end of the vector. Parameters ---------- other : TimeSeries or FrequencySeries The input vector containing a waveform. psd : Frequency Series A power spectral density to weight the overlap. low_frequency_cutoff : {None, float}, optional The frequency to begin the match. high_frequency_cutoff : {None, float}, optional The frequency to stop the match. Returns ------- match: float index: int The number of samples to shift to get the match.", "response": "def match(self, other, psd=None,\n              low_frequency_cutoff=None, high_frequency_cutoff=None):\n        \"\"\" Return the match between the two TimeSeries or FrequencySeries.\n\n        Return the match between two waveforms. This is equivelant to the overlap\n        maximized over time and phase. By default, the other vector will be\n        resized to match self. This may remove high frequency content or the\n        end of the vector.\n\n        Parameters\n        ----------\n        other : TimeSeries or FrequencySeries\n            The input vector containing a waveform.\n        psd : Frequency Series\n            A power spectral density to weight the overlap.\n        low_frequency_cutoff : {None, float}, optional\n            The frequency to begin the match.\n        high_frequency_cutoff : {None, float}, optional\n            The frequency to stop the match.\n\n        Returns\n        -------\n        match: float\n        index: int\n            The number of samples to shift to get the match.\n        \"\"\"\n        return self.to_frequencyseries().match(other, psd=psd,\n                     low_frequency_cutoff=low_frequency_cutoff,\n                     high_frequency_cutoff=high_frequency_cutoff)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving linear trend from the data", "response": "def detrend(self, type='linear'):\n        \"\"\" Remove linear trend from the data\n\n        Remove a linear trend from the data to improve the approximation that\n        the data is circularly convolved, this helps reduce the size of filter\n        transients from a circular convolution / filter.\n\n        Parameters\n        ----------\n        type: str\n            The choice of detrending. The default ('linear') removes a linear\n        least squares fit. 'constant' removes only the mean of the data.\n        \"\"\"\n        from scipy.signal import detrend\n        return self._return(detrend(self.numpy(), type=type))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the final spin of the final BH that gets formed after merger. This is done usingn Eq 5 - 6 of arXiv : 710. 33345", "response": "def FinalSpin( Xi, eta ):\n    \"\"\"Computes the spin of the final BH that gets formed after merger. This is done usingn Eq 5-6 of arXiv:0710.3345\"\"\"\n    s4 = -0.129\n    s5 = -0.384\n    t0 = -2.686\n    t2 = -3.454\n    t3 = 2.353\n    etaXi = eta * Xi\n    eta2 = eta*eta\n    finspin = (Xi + s4*Xi*etaXi + s5*etaXi*eta + t0*etaXi + 2.*(3.**0.5)*eta + t2*eta2 + t3*eta2*eta)\n    if finspin > 1.0:\n        raise ValueError(\"Value of final spin > 1.0. Aborting\")\n    else:\n        return finspin"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the ring - down frequency for the final Kerr BH. Using Eq. 5. 5 of Main paper", "response": "def fRD( a, M):\n    \"\"\"Calculate the ring-down frequency for the final Kerr BH. Using Eq. 5.5 of Main paper\"\"\"\n    f = (lal.C_SI**3.0 / (2.0*lal.PI*lal.G_SI*M*lal.MSUN_SI)) * (1.5251 - 1.1568*(1.0-a)**0.1292)\n    return f"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef imrphenomc_tmplt(**kwds):\n    # Pull out the input arguments\n    f_min = float128(kwds['f_lower'])\n    f_max = float128(kwds['f_final'])\n    delta_f = float128(kwds['delta_f'])\n    distance = float128(kwds['distance'])\n    mass1 = float128(kwds['mass1'])\n    mass2 = float128(kwds['mass2'])\n    spin1z = float128(kwds['spin1z'])\n    spin2z = float128(kwds['spin2z'])\n\n    if 'out' in kwds:\n        out = kwds['out']\n    else:\n        out = None\n\n    # Calculate binary parameters\n    M = mass1 + mass2\n    eta = mass1 * mass2 / (M * M)\n    Xi = (mass1 * spin1z / M) + (mass2 * spin2z / M)\n    Xisum = 2.*Xi\n    Xiprod = Xi*Xi\n    Xi2 = Xi*Xi\n\n    m_sec = M * lal.MTSUN_SI;\n    piM = lal.PI * m_sec;\n\n    ## The units of distance given as input is taken to pe Mpc. Converting to SI\n    distance *= (1.0e6 * lal.PC_SI / (2. * sqrt(5. / (64.*lal.PI)) * M * lal.MRSUN_SI * M * lal.MTSUN_SI))\n\n    # Check if the value of f_max is correctly given, else replace with the fCut\n    # used in the PhenomB code in lalsimulation. The various coefficients come\n    # from Eq.(4.18) of http://arxiv.org/pdf/0710.2335 and\n    # Table I of http://arxiv.org/pdf/0712.0343\n    if not f_max:\n        f_max = (1.7086 * eta * eta - 0.26592 * eta + 0.28236) / piM\n\n    # Transform the eta, chi to Lambda parameters, using Eq 5.14, Table II of Main\n    # paper.\n    z101 = -2.417e-03\n    z102 = -1.093e-03\n    z111 = -1.917e-02\n    z110 = 7.267e-02\n    z120 = -2.504e-01\n\n    z201 = 5.962e-01\n    z202 = -5.600e-02\n    z211 = 1.520e-01\n    z210 = -2.970e+00\n    z220 = 1.312e+01\n\n    z301 = -3.283e+01\n    z302 = 8.859e+00\n    z311 = 2.931e+01\n    z310 = 7.954e+01\n    z320 = -4.349e+02\n\n    z401 = 1.619e+02\n    z402 = -4.702e+01\n    z411 = -1.751e+02\n    z410 = -3.225e+02\n    z420 = 1.587e+03\n\n    z501 = -6.320e+02\n    z502 = 2.463e+02\n    z511 = 1.048e+03\n    z510 = 3.355e+02\n    z520 = -5.115e+03\n\n    z601 = -4.809e+01\n    z602 = -3.643e+02\n    z611 = -5.215e+02\n    z610 = 1.870e+03\n    z620 = 7.354e+02\n\n    z701 = 4.149e+00\n    z702 = -4.070e+00\n    z711 = -8.752e+01\n    z710 = -4.897e+01\n    z720 = 6.665e+02\n\n    z801 = -5.472e-02\n    z802 = 2.094e-02\n    z811 = 3.554e-01\n    z810 = 1.151e-01\n    z820 = 9.640e-01\n\n    z901 = -1.235e+00\n    z902 = 3.423e-01\n    z911 = 6.062e+00\n    z910 = 5.949e+00\n    z920 = -1.069e+01\n\n    eta2 = eta*eta\n    Xi2 = Xiprod\n\n    # Calculate alphas, gamma, deltas from Table II and Eq 5.14 of Main paper\n    a1 = z101 * Xi + z102 * Xi2 + z111 * eta * Xi + z110 * eta + z120 * eta2\n    a2 = z201 * Xi + z202 * Xi2 + z211 * eta * Xi + z210 * eta + z220 * eta2\n    a3 = z301 * Xi + z302 * Xi2 + z311 * eta * Xi + z310 * eta + z320 * eta2\n    a4 = z401 * Xi + z402 * Xi2 + z411 * eta * Xi + z410 * eta + z420 * eta2\n    a5 = z501 * Xi + z502 * Xi2 + z511 * eta * Xi + z510 * eta + z520 * eta2\n    a6 = z601 * Xi + z602 * Xi2 + z611 * eta * Xi + z610 * eta + z620 * eta2\n\n    g1 = z701 * Xi + z702 * Xi2 + z711 * eta * Xi + z710 * eta + z720 * eta2\n\n    del1 = z801 * Xi + z802 * Xi2 + z811 * eta * Xi + z810 * eta + z820 * eta2\n    del2 = z901 * Xi + z902 * Xi2 + z911 * eta * Xi + z910 * eta + z920 * eta2\n\n    # Get the spin of the final BH\n    afin = FinalSpin( Xi, eta )\n    Q = Qa( abs(afin) )\n\n    # Get the fRD\n    frd = fRD( abs(afin), M)\n    Mfrd = frd * m_sec\n\n    # Define the frequencies where SPA->PM->RD\n    f1 = 0.1 * frd\n    Mf1 = m_sec * f1\n    f2 = frd\n    Mf2 = m_sec * f2\n    d1 = 0.005\n    d2 = 0.005\n    f0 = 0.98 * frd\n    Mf0 = m_sec * f0\n    d0 = 0.015\n\n    # Now use this frequency for calculation of betas\n    # calculate beta1 and beta2, that appear in Eq 5.7 in the main paper.\n    b2 = ((-5./3.)* a1 * pow(Mfrd,(-8./3.)) - a2/(Mfrd*Mfrd) - \\\n      (a3/3.)*pow(Mfrd,(-4./3.)) + (2./3.)* a5 * pow(Mfrd,(-1./3.)) + a6)/eta\n\n    psiPMrd = (a1 * pow(Mfrd,(-5./3.)) + a2/Mfrd + a3 * pow(Mfrd,(-1./3.)) + \\\n      a4 + a5 * pow(Mfrd,(2./3.)) + a6 * Mfrd)/eta\n    b1 = psiPMrd - (b2 * Mfrd)\n\n    ### Calculate the PN coefficients, Eq A3 - A5 of main paper ###\n    pfaN = 3.0/(128.0 * eta)\n    pfa2 = (3715./756.) + (55.*eta/9.0)\n    pfa3 = -16.0*lal.PI + (113./3.)*Xi - 38.*eta*Xisum/3.\n    pfa4 = (152.93365/5.08032) - 50.*Xi2 + eta*(271.45/5.04 + 1.25*Xiprod) + \\\n        3085.*eta2/72.\n    pfa5 = lal.PI*(386.45/7.56 - 65.*eta/9.) - \\\n        Xi*(735.505/2.268 + 130.*eta/9.) + Xisum*(1285.0*eta/8.1 + 170.*eta2/9.) - \\\n        10.*Xi2*Xi/3. + 10.*eta*Xi*Xiprod\n    pfa6 = 11583.231236531/4.694215680 - 640.0*lal.PI*lal.PI/3. - \\\n        6848.0*lal.GAMMA/21. - 684.8*log(64.)/6.3 + \\\n        eta*(2255.*lal.PI*lal.PI/12. - 15737.765635/3.048192) + \\\n        76.055*eta2/1.728 - (127.825*eta2*eta/1.296) + \\\n        2920.*lal.PI*Xi/3. - (175. - 1490.*eta)*Xi2/3. - \\\n        (1120.*lal.PI/3. - 1085.*Xi/3.)*eta*Xisum + \\\n        (269.45*eta/3.36 - 2365.*eta2/6.)*Xiprod\n\n    pfa6log = -6848./63.\n\n    pfa7 = lal.PI*(770.96675/2.54016 + 378.515*eta/1.512 - 740.45*eta2/7.56) - \\\n        Xi*(20373.952415/3.048192 + 1509.35*eta/2.24 - 5786.95*eta2/4.32) + \\\n        Xisum*(4862.041225*eta/1.524096 + 1189.775*eta2/1.008 - 717.05*eta2*eta/2.16 - 830.*eta*Xi2/3. + 35.*eta2*Xiprod/3.) - \\\n        560.*lal.PI*Xi2 + 20.*lal.PI*eta*Xiprod + \\\n        Xi2*Xi*(945.55/1.68 - 85.*eta) + Xi*Xiprod*(396.65*eta/1.68 + 255.*eta2)\n\n\n    xdotaN = 64.*eta/5.\n    xdota2 = -7.43/3.36 - 11.*eta/4.\n    xdota3 = 4.*lal.PI - 11.3*Xi/1.2 + 19.*eta*Xisum/6.\n    xdota4 = 3.4103/1.8144 + 5*Xi2 + eta*(13.661/2.016 - Xiprod/8.) + 5.9*eta2/1.8\n    xdota5 = -lal.PI*(41.59/6.72 + 189.*eta/8.) - Xi*(31.571/1.008 - 116.5*eta/2.4) + \\\n          Xisum*(21.863*eta/1.008 - 79.*eta2/6.) - 3*Xi*Xi2/4. + \\\n          9.*eta*Xi*Xiprod/4.\n    xdota6 = 164.47322263/1.39708800 - 17.12*lal.GAMMA/1.05 + \\\n          16.*lal.PI*lal.PI/3 - 8.56*log(16.)/1.05 + \\\n          eta*(45.1*lal.PI*lal.PI/4.8 - 561.98689/2.17728) + \\\n          5.41*eta2/8.96 - 5.605*eta*eta2/2.592 - 80.*lal.PI*Xi/3. + \\\n          eta*Xisum*(20.*lal.PI/3. - 113.5*Xi/3.6) + \\\n          Xi2*(64.153/1.008 - 45.7*eta/3.6) - \\\n          Xiprod*(7.87*eta/1.44 - 30.37*eta2/1.44)\n\n    xdota6log = -856./105.\n\n    xdota7 = -lal.PI*(4.415/4.032 - 358.675*eta/6.048 - 91.495*eta2/1.512) - \\\n          Xi*(252.9407/2.7216 - 845.827*eta/6.048 + 415.51*eta2/8.64) + \\\n          Xisum*(158.0239*eta/5.4432 - 451.597*eta2/6.048 + 20.45*eta2*eta/4.32 + 107.*eta*Xi2/6. - 5.*eta2*Xiprod/24.) + \\\n          12.*lal.PI*Xi2 - Xi2*Xi*(150.5/2.4 + eta/8.) + \\\n          Xi*Xiprod*(10.1*eta/2.4 + 3.*eta2/8.)\n\n\n    AN = 8.*eta*sqrt(lal.PI/5.)\n    A2 = (-107. + 55.*eta)/42.\n    A3 = 2.*lal.PI - 4.*Xi/3. + 2.*eta*Xisum/3.\n    A4 = -2.173/1.512 - eta*(10.69/2.16 - 2.*Xiprod) + 2.047*eta2/1.512\n    A5 = -10.7*lal.PI/2.1 + eta*(3.4*lal.PI/2.1)\n\n    A5imag = -24.*eta\n\n    A6 = 270.27409/6.46800 - 8.56*lal.GAMMA/1.05 + \\\n      2.*lal.PI*lal.PI/3. + \\\n      eta*(4.1*lal.PI*lal.PI/9.6 - 27.8185/3.3264) - \\\n      20.261*eta2/2.772 + 11.4635*eta*eta2/9.9792 - \\\n      4.28*log(16.)/1.05\n\n    A6log = -428./105.\n\n    A6imag = 4.28*lal.PI/1.05\n\n    ### Define other parameters needed by waveform generation ###\n    kmin = int(f_min / delta_f)\n    kmax = int(f_max / delta_f)\n    n = kmax + 1;\n\n    if not out:\n        htilde = FrequencySeries(zeros(n,dtype=numpy.complex128), delta_f=delta_f, copy=False)\n    else:\n        if type(out) is not Array:\n            raise TypeError(\"Output must be an instance of Array\")\n        if len(out) < kmax:\n            raise TypeError(\"Output array is too small\")\n        if out.dtype != complex64:\n            raise TypeError(\"Output array is the wrong dtype\")\n        htilde = FrequencySeries(out, delta_f=delta_f, copy=False)\n\n    phenomC_kernel(htilde.data[kmin:kmax], kmin, delta_f, eta, Xi, distance,\n                                       m_sec,  piM,  Mfrd,\n                                       pfaN,  pfa2,  pfa3,  pfa4, pfa5,  pfa6,  pfa6log,  pfa7,\n                                       a1,  a2,  a3,  a4, a5,  a6,  b1,  b2,\n                                       Mf1,  Mf2,  Mf0, d1,  d2,  d0,\n                                       xdota2,  xdota3,  xdota4, xdota5,  xdota6,  xdota6log,\n                                       xdota7,  xdotaN,  AN, A2,  A3,  A4,  A5,\n                                       A5imag,  A6,  A6log,  A6imag,\n                                       g1,  del1,  del2,  Q )\n    hp = htilde\n    hc = htilde * 1j\n    return hp, hc", "response": "This function generates the IMRPhenomC waveform using CUDA."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncache sigma calculation for use in tandem with the FilterBank class", "response": "def sigma_cached(self, psd):\n    \"\"\" Cache sigma calculate for use in tandem with the FilterBank class\n    \"\"\"\n    if not hasattr(self, '_sigmasq'):\n        from pycbc.opt import LimitedSizeDict\n        self._sigmasq = LimitedSizeDict(size_limit=2**5)\n\n    key = id(psd)\n    if not hasattr(psd, '_sigma_cached_key'):\n        psd._sigma_cached_key = {}\n\n    if key not in self._sigmasq or id(self) not in psd._sigma_cached_key:\n        psd._sigma_cached_key[id(self)] = True\n        # If possible, we precalculate the sigmasq vector for all possible waveforms\n        if pycbc.waveform.waveform_norm_exists(self.approximant):\n            if not hasattr(psd, 'sigmasq_vec'):\n                psd.sigmasq_vec = {}\n\n            if self.approximant not in psd.sigmasq_vec:\n                psd.sigmasq_vec[self.approximant] = pycbc.waveform.get_waveform_filter_norm(\n                     self.approximant, psd, len(psd), psd.delta_f, self.f_lower)\n\n            if not hasattr(self, 'sigma_scale'):\n                # Get an amplitude normalization (mass dependant constant norm)\n                amp_norm = pycbc.waveform.get_template_amplitude_norm(\n                                     self.params, approximant=self.approximant)\n                amp_norm = 1 if amp_norm is None else amp_norm\n                self.sigma_scale = (DYN_RANGE_FAC * amp_norm) ** 2.0\n\n\n            self._sigmasq[key] = self.sigma_scale * \\\n                psd.sigmasq_vec[self.approximant][self.end_idx-1]\n\n        else:\n            if not hasattr(self, 'sigma_view'):\n                from pycbc.filter.matchedfilter import get_cutoff_indices\n                N = (len(self) -1) * 2\n                kmin, kmax = get_cutoff_indices(\n                        self.min_f_lower or self.f_lower, self.end_frequency,\n                        self.delta_f, N)\n                self.sslice = slice(kmin, kmax)\n                self.sigma_view = self[self.sslice].squared_norm() * 4.0 * self.delta_f\n\n            if not hasattr(psd, 'invsqrt'):\n                psd.invsqrt = 1.0 / psd[self.sslice]\n\n            self._sigmasq[key] = self.sigma_view.inner(psd.invsqrt)\n    return self._sigmasq[key]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef boolargs_from_apprxstr(approximant_strs):\n    if not isinstance(approximant_strs, list):\n        approximant_strs = [approximant_strs]\n    return [tuple(arg.split(':')) for arg in approximant_strs]", "response": "Parses a list of strings specifying an approximant and where that\n    approximant should be used into a list of tuples that can be understood by FieldArray. parse_boolargs."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds an approximant argument to the given argument parser.", "response": "def add_approximant_arg(parser, default=None, help=None):\n    \"\"\"Adds an approximant argument to the given parser.\n\n    Parameters\n    ----------\n    parser : ArgumentParser\n        The argument parser to add the argument to.\n    default : {None, str}\n        Specify a default for the approximant argument. Defaults to None.\n    help : {None, str}\n        Provide a custom help message. If None, will use a descriptive message\n        on how to specify the approximant.\n    \"\"\"\n    if help is None:\n        help=str(\"The approximant(s) to use. Multiple approximants to use \"\n             \"in different regions may be provided. If multiple \"\n             \"approximants are provided, every one but the last must be \"\n             \"be followed by a conditional statement defining where that \"\n             \"approximant should be used. Conditionals can be any boolean \"\n             \"test understood by numpy. For example, 'Apprx:(mtotal > 4) & \"\n             \"(mchirp <= 5)' would use approximant 'Apprx' where total mass \"\n             \"is > 4 and chirp mass is <= 5. \"\n             \"Conditionals are applied in order, with each successive one \"\n             \"only applied to regions not covered by previous arguments. \"\n             \"For example, `'TaylorF2:mtotal < 4' 'IMRPhenomD:mchirp < 3'` \"\n             \"would result in IMRPhenomD being used where chirp mass is < 3 \"\n             \"and total mass is >= 4. The last approximant given may use \"\n             \"'else' as the conditional or include no conditional. In either \"\n             \"case, this will cause the last approximant to be used in any \"\n             \"remaning regions after all the previous conditionals have been \"\n             \"applied. For the full list of possible parameters to apply \"\n             \"conditionals to, see WaveformArray.default_fields(). Math \"\n             \"operations may also be used on parameters; syntax is python, \"\n             \"with any operation recognized by numpy.\")\n    parser.add_argument(\"--approximant\", nargs='+', type=str, default=default,\n                        metavar='APPRX[:COND]',\n                        help=help)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind a frequency value above the starting frequency that results in a waveform longer than max_length.", "response": "def find_variable_start_frequency(approximant, parameters, f_start, max_length,\n                                  delta_f = 1):\n    \"\"\" Find a frequency value above the starting frequency that results in a\n    waveform shorter than max_length.\n    \"\"\"\n    l = max_length + 1\n    f = f_start - delta_f\n    while l > max_length:\n        f += delta_f\n        l = pycbc.waveform.get_waveform_filter_length_in_time(approximant,\n                                                      parameters, f_lower=f)\n    return f"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ensure_hash(self):\n        fields = self.table.fieldnames\n        if 'template_hash' in fields:\n            return\n\n        # The fields to use in making a template hash\n        hash_fields = ['mass1', 'mass2', 'inclination',\n                       'spin1x', 'spin1y', 'spin1z',\n                       'spin2x', 'spin2y', 'spin2z',]\n\n        fields = [f for f in hash_fields if f in fields]\n        template_hash = np.array([hash(v) for v in zip(*[self.table[p]\n                                  for p in fields])])\n        self.table = self.table.add_fields(template_hash, 'template_hash')", "response": "Ensure that there is a correctly populated template_hash and create if it doesn t exist."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting the current object to the given hdf file.", "response": "def write_to_hdf(self, filename, start_index=None, stop_index=None,\n                     force=False, skip_fields=None,\n                     write_compressed_waveforms=True):\n        \"\"\"Writes self to the given hdf file.\n\n        Parameters\n        ----------\n        filename : str\n            The name of the file to write to. Must end in '.hdf'.\n        start_index : If a specific slice of the template bank is to be\n            written to the hdf file, this would specify the index of the\n            first template in the slice\n        stop_index : If a specific slice of the template bank is to be\n            written to the hdf file, this would specify the index of the\n            last template in the slice\n        force : {False, bool}\n            If the file already exists, it will be overwritten if True.\n            Otherwise, an OSError is raised if the file exists.\n        skip_fields : {None, (list of) strings}\n            Do not write the given fields to the hdf file. Default is None,\n            in which case all fields in self.table.fieldnames are written.\n        write_compressed_waveforms : {True, bool}\n            Write compressed waveforms to the output (hdf) file if this is\n            True, which is the default setting. If False, do not write the\n            compressed waveforms group, but only the template parameters to\n            the output file.\n\n        Returns\n        -------\n        h5py.File\n            The file handler to the output hdf file (left open).\n        \"\"\"\n        if not filename.endswith('.hdf'):\n            raise ValueError(\"Unrecoginized file extension\")\n        if os.path.exists(filename) and not force:\n            raise IOError(\"File %s already exists\" %(filename))\n        f = h5py.File(filename, 'w')\n        parameters = self.parameters\n        if skip_fields is not None:\n            if not isinstance(skip_fields, list):\n                skip_fields = [skip_fields]\n            parameters = [p for p in parameters if p not in skip_fields]\n        # save the parameters\n        f.attrs['parameters'] = parameters\n        write_tbl = self.table[start_index:stop_index]\n        for p in parameters:\n            f[p] = write_tbl[p]\n        if write_compressed_waveforms and self.has_compressed_waveforms:\n            for tmplt_hash in write_tbl.template_hash:\n                compressed_waveform = pycbc.waveform.compress.CompressedWaveform.from_hdf(\n                                        self.filehandler, tmplt_hash,\n                                        load_now=True)\n                compressed_waveform.write_to_hdf(f, tmplt_hash)\n        return f"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the end frequency of the waveform at the given index value", "response": "def end_frequency(self, index):\n        \"\"\" Return the end frequency of the waveform at the given index value\n        \"\"\"\n        from pycbc.waveform.waveform import props\n\n        return pycbc.waveform.get_waveform_end_frequency(\n                                self.table[index],\n                                approximant=self.approximant(index),\n                                **self.extra_args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef approximant(self, index):\n        if 'approximant' not in self.table.fieldnames:\n            raise ValueError(\"approximant not found in input file and no \"\n                \"approximant was specified on initialization\")\n        return self.table[\"approximant\"][index]", "response": "Return the name of the approximant ot use at the given index"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef template_thinning(self, inj_filter_rejector):\n        if not inj_filter_rejector.enabled or \\\n                inj_filter_rejector.chirp_time_window is None:\n            # Do nothing!\n            return\n\n        injection_parameters = inj_filter_rejector.injection_params.table\n        fref = inj_filter_rejector.f_lower\n        threshold = inj_filter_rejector.chirp_time_window\n        m1= self.table['mass1']\n        m2= self.table['mass2']\n        tau0_temp, _ = pycbc.pnutils.mass1_mass2_to_tau0_tau3(m1, m2, fref)\n        indices = []\n\n        for inj in injection_parameters:\n            tau0_inj, _ = \\\n                pycbc.pnutils.mass1_mass2_to_tau0_tau3(inj.mass1, inj.mass2,\n                                                       fref)\n            inj_indices = np.where(abs(tau0_temp - tau0_inj) <= threshold)[0]\n            indices.append(inj_indices)\n            indices_combined = np.concatenate(indices)\n\n        indices_unique= np.unique(indices_combined)\n        self.table = self.table[indices_unique]", "response": "Remove templates from bank that are far from all injections."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninitialize the filter bank common fields and set the minimum f_lower and template_duration fields to None.", "response": "def ensure_standard_filter_columns(self, low_frequency_cutoff=None):\n        \"\"\" Initialize FilterBank common fields\n\n        Parameters\n        ----------\n        low_frequency_cutoff: {float, None}, Optional\n            A low frequency cutoff which overrides any given within the\n            template bank file.\n        \"\"\"\n\n        # Make sure we have a template duration field\n        if not hasattr(self.table, 'template_duration'):\n            self.table = self.table.add_fields(np.zeros(len(self.table),\n                                     dtype=np.float32), 'template_duration')\n\n        # Make sure we have a f_lower field\n        if low_frequency_cutoff is not None:\n            if not hasattr(self.table, 'f_lower'):\n                vec = np.zeros(len(self.table), dtype=np.float32)\n                self.table = self.table.add_fields(vec, 'f_lower')\n            self.table['f_lower'][:] = low_frequency_cutoff\n\n        self.min_f_lower = min(self.table['f_lower'])\n        if self.f_lower is None and self.min_f_lower == 0.:\n            raise ValueError('Invalid low-frequency cutoff settings')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef round_up(self, num):\n        inc = self.increment\n        size = np.ceil(num / self.sample_rate / inc) * self.sample_rate * inc\n        return size", "response": "Determine the length of this waveform by rounding."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating the template with index t_num using custom length delta_f and max_freq.", "response": "def generate_with_delta_f_and_max_freq(self, t_num, max_freq, delta_f,\n                                           low_frequency_cutoff=None,\n                                           cached_mem=None):\n        \"\"\"Generate the template with index t_num using custom length.\"\"\"\n        approximant = self.approximant(t_num)\n        # Don't want to use INTERP waveforms in here\n        if approximant.endswith('_INTERP'):\n            approximant = approximant.replace('_INTERP', '')\n        # Using SPAtmplt here is bad as the stored cbrt and logv get\n        # recalculated as we change delta_f values. Fall back to TaylorF2\n        # in lalsimulation.\n        if approximant == 'SPAtmplt':\n            approximant = 'TaylorF2'\n        if cached_mem is None:\n            wav_len = int(max_freq / delta_f) + 1\n            cached_mem = zeros(wav_len, dtype=np.complex64)\n        if self.has_compressed_waveforms and self.enable_compressed_waveforms:\n            htilde = self.get_decompressed_waveform(cached_mem, t_num,\n                                                    f_lower=low_frequency_cutoff,\n                                                    approximant=approximant,\n                                                    df=delta_f)\n        else :\n            htilde = pycbc.waveform.get_waveform_filter(\n                cached_mem, self.table[t_num], approximant=approximant,\n                f_lower=low_frequency_cutoff, f_final=max_freq, delta_f=delta_f,\n                distance=1./DYN_RANGE_FAC, delta_t=1./(2.*max_freq))\n        return htilde"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the single - IFO generator for the approximant.", "response": "def select_waveform_generator(approximant):\n    \"\"\"Returns the single-IFO generator for the approximant.\n\n    Parameters\n    ----------\n    approximant : str\n        Name of waveform approximant. Valid names can be found using\n        ``pycbc.waveform`` methods.\n\n    Returns\n    -------\n    generator : (PyCBC generator instance)\n        A waveform generator object.\n\n    Examples\n    --------\n    Get a list of available approximants:\n    >>> from pycbc import waveform\n    >>> waveform.fd_approximants()\n    >>> waveform.td_approximants()\n    >>> from pycbc.waveform import ringdown\n    >>> ringdown.ringdown_fd_approximants.keys()\n\n    Get generator object:\n    >>> from pycbc.waveform.generator import select_waveform_generator\n    >>> select_waveform_generator(waveform.fd_approximants()[0])\n    \"\"\"\n\n    # check if frequency-domain CBC waveform\n    if approximant in waveform.fd_approximants():\n        return FDomainCBCGenerator\n\n    # check if time-domain CBC waveform\n    elif approximant in waveform.td_approximants():\n        return TDomainCBCGenerator\n\n    # check if frequency-domain ringdown waveform\n    elif approximant in ringdown.ringdown_fd_approximants:\n        if approximant == 'FdQNMfromFinalMassSpin':\n            return FDomainMassSpinRingdownGenerator\n        elif approximant == 'FdQNMfromFreqTau':\n            return FDomainFreqTauRingdownGenerator\n\n    elif approximant in ringdown.ringdown_td_approximants:\n        if approximant == 'TdQNMfromFinalMassSpin':\n            return TDomainMassSpinRingdownGenerator\n        elif approximant == 'TdQNMfromFreqTau':\n            return TDomainFreqTauRingdownGenerator\n\n    # otherwise waveform approximant is not supported\n    else:\n        raise ValueError(\"%s is not a valid approximant.\" % approximant)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef generate_from_args(self, *args):\n        if len(args) != len(self.variable_args):\n            raise ValueError(\"variable argument length mismatch\")\n        return self.generate(**dict(zip(self.variable_args, args)))", "response": "Generates a waveform from the given list of arguments."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _gdecorator(generate_func):\n        def dostuff(self):\n            for func in self._pregenerate_functions:\n                self.current_params = func(self.current_params)\n            res = generate_func(self) # pylint:disable=not-callable\n            return self._postgenerate(res)\n        return dostuff", "response": "A decorator that allows for seemless pre and post manipulation of\n        the waveform generator function."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\napplies a taper if it is in current params.", "response": "def _postgenerate(self, res):\n        \"\"\"Applies a taper if it is in current params.\n        \"\"\"\n        hp, hc = res\n        try:\n            hp = taper_timeseries(hp, tapermethod=self.current_params['taper'])\n            hc = taper_timeseries(hc, tapermethod=self.current_params['taper'])\n        except KeyError:\n            pass\n        return hp, hc"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating a waveform applies a time shift and the detector response function from the given args.", "response": "def generate_from_args(self, *args):\n        \"\"\"Generates a waveform, applies a time shift and the detector response\n        function from the given args.\n\n        The args are assumed to be in the same order as the variable args.\n        \"\"\"\n        return self.generate(**dict(zip(self.variable_args, args)))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef generate(self, **kwargs):\n        self.current_params.update(kwargs)\n        rfparams = {param: self.current_params[param]\n            for param in kwargs if param not in self.location_args}\n        hp, hc = self.rframe_generator.generate(**rfparams)\n        if isinstance(hp, TimeSeries):\n            df = self.current_params['delta_f']\n            hp = hp.to_frequencyseries(delta_f=df)\n            hc = hc.to_frequencyseries(delta_f=df)\n            # time-domain waveforms will not be shifted so that the peak amp\n            # happens at the end of the time series (as they are for f-domain),\n            # so we add an additional shift to account for it\n            tshift = 1./df - abs(hp._epoch)\n        else:\n            tshift = 0.\n        hp._epoch = hc._epoch = self._epoch\n        h = {}\n        if self.detector_names != ['RF']:\n            for detname, det in self.detectors.items():\n                # apply detector response function\n                fp, fc = det.antenna_pattern(self.current_params['ra'],\n                            self.current_params['dec'],\n                            self.current_params['polarization'],\n                            self.current_params['tc'])\n                thish = fp*hp + fc*hc\n                # apply the time shift\n                tc = self.current_params['tc'] + \\\n                    det.time_delay_from_earth_center(self.current_params['ra'],\n                         self.current_params['dec'], self.current_params['tc'])\n                h[detname] = apply_fd_time_shift(thish, tc+tshift, copy=False)\n                if self.recalib:\n                    # recalibrate with given calibration model\n                    h[detname] = \\\n                        self.recalib[detname].map_to_adjust(h[detname],\n                            **self.current_params)\n        else:\n            # no detector response, just use the + polarization\n            if 'tc' in self.current_params:\n                hp = apply_fd_time_shift(hp, self.current_params['tc']+tshift,\n                                         copy=False)\n            h['RF'] = hp\n        if self.gates is not None:\n            # resize all to nearest power of 2\n            for d in h.values():\n                d.resize(ceilpow2(len(d)-1) + 1)\n            h = strain.apply_gates_to_fd(h, self.gates)\n        return h", "response": "Generates a waveform applies a time shift and the detector response function from the given kwargs."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef required_opts(opt, parser, opt_list, required_by=None):\n    for name in opt_list:\n        attr = name[2:].replace('-', '_')\n        if not hasattr(opt, attr) or (getattr(opt, attr) is None):\n            err_str = \"%s is missing \" % name\n            if required_by is not None:\n                err_str += \", required by %s\" % required_by\n            parser.error(err_str)", "response": "Checks that all the opts in opt_list are defined in opt. parser. opt_list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck that all the opts in opt_list are defined by the given ifo.", "response": "def required_opts_multi_ifo(opt, parser, ifo, opt_list, required_by=None):\n    \"\"\"Check that all the opts are defined\n\n    Parameters\n    ----------\n    opt : object\n        Result of option parsing\n    parser : object\n        OptionParser instance.\n    ifo : string\n    opt_list : list of strings\n    required_by : string, optional\n        the option that requires these options (if applicable)\n    \"\"\"\n    for name in opt_list:\n        attr = name[2:].replace('-', '_')\n        try:\n            if getattr(opt, attr)[ifo] is None:\n                raise KeyError\n        except KeyError:\n            err_str = \"%s is missing \" % name\n            if required_by is not None:\n                err_str += \", required by %s\" % required_by\n            parser.error(err_str)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ensure_one_opt(opt, parser, opt_list):\n\n    the_one = None\n    for name in opt_list:\n        attr = name[2:].replace('-', '_')\n        if hasattr(opt, attr) and (getattr(opt, attr) is not None):\n            if the_one is None:\n                the_one = name\n            else:\n                parser.error(\"%s and %s are mutually exculsive\" \\\n                              % (the_one, name))\n\n    if the_one is None:\n        parser.error(\"you must supply one of the following %s\" \\\n                      % (', '.join(opt_list)))", "response": "Checks that one option is defined in opt_list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck that one and only one option is defined in opt_list.", "response": "def ensure_one_opt_multi_ifo(opt, parser, ifo, opt_list):\n    \"\"\"  Check that one and only one in the opt_list is defined in opt\n\n    Parameters\n    ----------\n    opt : object\n        Result of option parsing\n    parser : object\n        OptionParser instance.\n    opt_list : list of strings\n    \"\"\"\n\n    the_one = None\n    for name in opt_list:\n        attr = name[2:].replace('-', '_')\n        try:\n            if getattr(opt, attr)[ifo] is None:\n                raise KeyError\n        except KeyError:\n            pass\n        else:\n            if the_one is None:\n                the_one = name\n            else:\n                parser.error(\"%s and %s are mutually exculsive\" \\\n                              % (the_one, name))\n\n    if the_one is None:\n        parser.error(\"you must supply one of the following %s\" \\\n                      % (', '.join(opt_list)))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef copy_opts_for_single_ifo(opt, ifo):\n    opt = copy.deepcopy(opt)\n    for arg, val in vars(opt).items():\n        if isinstance(val, DictWithDefaultReturn):\n            setattr(opt, arg, getattr(opt, arg)[ifo])\n    return opt", "response": "Takes the namespace object from the multi - detector interface and returns a copy of the namespace object that is a single ifo."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntaking the namespace object from the multi - detector interface and returns a dictionary of command line options that will be handled correctly", "response": "def convert_to_process_params_dict(opt):\n    \"\"\"\n    Takes the namespace object (opt) from the multi-detector interface and\n    returns a dictionary of command line options that will be handled correctly\n    by the register_to_process_params ligolw function.\n    \"\"\"\n    opt = copy.deepcopy(opt)\n    for arg, val in vars(opt).items():\n        if isinstance(val, DictWithDefaultReturn):\n            new_val = []\n            for key in val.keys():\n                if isinstance(val[key], list):\n                    for item in val[key]:\n                        if item is not None:\n                            new_val.append(':'.join([key, str(item)]))\n                else:\n                    if val[key] is not None:\n                        new_val.append(':'.join([key, str(val[key])]))\n            setattr(opt, arg, new_val)\n    return vars(opt)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef positive_float(s):\n    err_msg = \"must be a positive number, not %r\" % s\n    try:\n        value = float(s)\n    except ValueError:\n        raise argparse.ArgumentTypeError(err_msg)\n    if value <= 0:\n        raise argparse.ArgumentTypeError(err_msg)\n    return value", "response": "Ensure argument is a positive real number and return it as float."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nensuring argument is a positive real number or zero and return it as float.", "response": "def nonnegative_float(s):\n    \"\"\"\n    Ensure argument is a positive real number or zero and return it as float.\n\n    To be used as type in argparse arguments.\n    \"\"\"\n    err_msg = \"must be either positive or zero, not %r\" % s\n    try:\n        value = float(s)\n    except ValueError:\n        raise argparse.ArgumentTypeError(err_msg)\n    if value < 0:\n        raise argparse.ArgumentTypeError(err_msg)\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse the CLI options related to the noise PSD and returns a frequency series with the corresponding PSD.", "response": "def from_cli(opt, length, delta_f, low_frequency_cutoff,\n             strain=None, dyn_range_factor=1, precision=None):\n    \"\"\"Parses the CLI options related to the noise PSD and returns a\n    FrequencySeries with the corresponding PSD. If necessary, the PSD is\n    linearly interpolated to achieve the resolution specified in the CLI.\n\n    Parameters\n    ----------\n    opt : object\n        Result of parsing the CLI with OptionParser, or any object with the\n        required attributes (psd_model, psd_file, asd_file, psd_estimation,\n        psd_segment_length, psd_segment_stride, psd_inverse_length,\n        psd_output).\n    length : int\n        The length in samples of the output PSD.\n    delta_f : float\n        The frequency step of the output PSD.\n    low_frequency_cutoff: float\n        The low frequncy cutoff to use when calculating the PSD.\n    strain : {None, TimeSeries}\n        Time series containing the data from which the PSD should be measured,\n        when psd_estimation is in use.\n    dyn_range_factor : {1, float}\n        For PSDs taken from models or text files, if `dyn_range_factor` is\n        not None, then the PSD is multiplied by `dyn_range_factor` ** 2.\n    precision : str, choices (None,'single','double')\n        If not specified, or specified as None, the precision of the returned\n        PSD will match the precision of the data, if measuring a PSD, or will\n        match the default precision of the model if using an analytical PSD.\n        If 'single' the PSD will be converted to float32, if not already in\n        that precision. If 'double' the PSD will be converted to float64, if\n        not already in that precision.\n\n    Returns\n    -------\n    psd : FrequencySeries\n        The frequency series containing the PSD.\n    \"\"\"\n    f_low = low_frequency_cutoff\n    sample_rate = int((length -1) * 2 * delta_f)\n\n    try:\n        psd_estimation = opt.psd_estimation is not None\n    except AttributeError:\n        psd_estimation = False\n\n    exclusive_opts = [opt.psd_model, opt.psd_file, opt.asd_file,\n                      psd_estimation]\n    if sum(map(bool, exclusive_opts)) != 1:\n        err_msg = \"You must specify exactly one of '--psd-file', \"\n        err_msg += \"'--psd-model', '--asd-file', '--psd-estimation'\"\n        raise ValueError(err_msg)\n\n    if (opt.psd_model or opt.psd_file or opt.asd_file):\n        # PSD from lalsimulation or file\n        if opt.psd_model:\n            psd = from_string(opt.psd_model, length, delta_f, f_low)\n        elif opt.psd_file or opt.asd_file:\n            if opt.asd_file:\n                psd_file_name = opt.asd_file\n            else:\n                psd_file_name = opt.psd_file\n            if psd_file_name.endswith(('.dat', '.txt')):\n                is_asd_file = bool(opt.asd_file)\n                psd = from_txt(psd_file_name, length,\n                               delta_f, f_low, is_asd_file=is_asd_file)\n            elif opt.asd_file:\n                err_msg = \"ASD files are only valid as ASCII files (.dat or \"\n                err_msg += \".txt). Supplied {}.\".format(psd_file_name)\n            elif psd_file_name.endswith(('.xml', '.xml.gz')):\n                psd = from_xml(psd_file_name, length, delta_f, f_low,\n                               ifo_string=opt.psd_file_xml_ifo_string,\n                               root_name=opt.psd_file_xml_root_name)\n        # Set values < flow to the value at flow\n        kmin = int(low_frequency_cutoff / psd.delta_f)\n        psd[0:kmin] = psd[kmin]\n\n        psd *= dyn_range_factor ** 2\n\n    elif psd_estimation:\n        # estimate PSD from data\n        psd = welch(strain, avg_method=opt.psd_estimation,\n                    seg_len=int(opt.psd_segment_length * sample_rate),\n                    seg_stride=int(opt.psd_segment_stride * sample_rate),\n                    num_segments=opt.psd_num_segments,\n                    require_exact_data_fit=False)\n\n        if delta_f != psd.delta_f:\n            psd = interpolate(psd, delta_f)\n    else:\n        # Shouldn't be possible to get here\n        raise ValueError(\"Shouldn't be possible to raise this!\")\n\n    if opt.psd_inverse_length:\n        psd = inverse_spectrum_truncation(psd,\n            int(opt.psd_inverse_length * sample_rate),\n            low_frequency_cutoff=f_low)\n\n    if hasattr(opt, 'psd_output') and opt.psd_output:\n        (psd.astype(float64) / (dyn_range_factor ** 2)).save(opt.psd_output)\n\n    if precision is None:\n        return psd\n    elif precision == 'single':\n        return psd.astype(float32)\n    elif precision == 'double':\n        return psd.astype(float64)\n    else:\n        err_msg = \"If provided the precision kwarg must be either 'single' \"\n        err_msg += \"or 'double'. You provided %s.\" %(precision)\n        raise ValueError(err_msg)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a PSD for a single detector from a CLI command line.", "response": "def from_cli_single_ifo(opt, length, delta_f, low_frequency_cutoff, ifo,\n             **kwargs):\n    \"\"\"\n    Get the PSD for a single ifo when using the multi-detector CLI\n    \"\"\"\n    single_det_opt = copy_opts_for_single_ifo(opt, ifo)\n    return from_cli(single_det_opt, length, delta_f, low_frequency_cutoff,\n                    **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the PSD for all ifos when using the multi - detector CLI", "response": "def from_cli_multi_ifos(opt, length_dict, delta_f_dict,\n                        low_frequency_cutoff_dict, ifos, strain_dict=None,\n                        **kwargs):\n    \"\"\"\n    Get the PSD for all ifos when using the multi-detector CLI\n    \"\"\"\n    psd = {}\n    for ifo in ifos:\n        if strain_dict is not None:\n            strain = strain_dict[ifo]\n        else:\n            strain = None\n        psd[ifo] = from_cli_single_ifo(opt, length_dict[ifo], delta_f_dict[ifo],\n                                       low_frequency_cutoff_dict[ifo], ifo,\n                                       strain=strain, **kwargs)\n    return psd"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef insert_psd_option_group(parser, output=True, include_data_options=True):\n    psd_options = parser.add_argument_group(\n                          \"Options to select the method of PSD generation\",\n                          \"The options --psd-model, --psd-file, --asd-file, \"\n                          \"and --psd-estimation are mutually exclusive.\")\n    psd_options.add_argument(\"--psd-model\",\n                             help=\"Get PSD from given analytical model. \",\n                             choices=get_psd_model_list())\n    psd_options.add_argument(\"--psd-file\",\n                             help=\"Get PSD using given PSD ASCII file\")\n    psd_options.add_argument(\"--asd-file\",\n                             help=\"Get PSD using given ASD ASCII file\")\n    psd_options.add_argument(\"--psd-inverse-length\", type=float,\n                             help=\"(Optional) The maximum length of the \"\n                             \"impulse response of the overwhitening \"\n                             \"filter (s)\")\n    # Options specific to XML PSD files\n    psd_options.add_argument(\"--psd-file-xml-ifo-string\",\n                             help=\"If using an XML PSD file, use the PSD in \"\n                                  \"the file's PSD dictionary with this \"\n                                  \"ifo string. If not given and only one \"\n                                  \"PSD present in the file return that, if \"\n                                  \"not given and multiple (or zero) PSDs \"\n                                  \"present an exception will be raised.\")\n    psd_options.add_argument(\"--psd-file-xml-root-name\", default='psd',\n                             help=\"If given use this as the root name for \"\n                                  \"the PSD XML file. If this means nothing \"\n                                  \"to you, then it is probably safe to \"\n                                  \"ignore this option.\")\n    # Options for PSD variation\n    psd_options.add_argument(\"--psdvar-short-segment\", type=float,\n                             metavar=\"SECONDS\", help=\"Length of short segment \"\n                             \"when calculating the PSD variability.\")\n    psd_options.add_argument(\"--psdvar-long-segment\", type=float,\n                             metavar=\"SECONDS\", help=\"Length of long segment \"\n                             \"when calculating the PSD variability.\")\n    psd_options.add_argument(\"--psdvar-short-psd-duration\", type=float,\n                             metavar=\"SECONDS\", help=\"Duration of short \"\n                             \"segments for PSD estimation.\")\n    psd_options.add_argument(\"--psdvar-short-psd-stride\", type=float,\n                             metavar=\"SECONDS\", help=\"Separation between PSD \"\n                             \"estimation segments.\")\n    psd_options.add_argument(\"--psdvar-low-freq\", type=float, metavar=\"HERTZ\",\n                             help=\"Minimum frequency to consider in PSD \"\n                             \"comparison.\")\n    psd_options.add_argument(\"--psdvar-high-freq\", type=float, metavar=\"HERTZ\",\n                             help=\"Maximum frequency to consider in PSD \"\n                             \"comparison.\")\n\n    if include_data_options :\n        psd_options.add_argument(\"--psd-estimation\",\n                                 help=\"Measure PSD from the data, using \"\n                                 \"given average method.\",\n                                 choices=[\"mean\", \"median\", \"median-mean\"])\n        psd_options.add_argument(\"--psd-segment-length\", type=float,\n                                 help=\"(Required for --psd-estimation) The \"\n                                 \"segment length for PSD estimation (s)\")\n        psd_options.add_argument(\"--psd-segment-stride\", type=float,\n                                 help=\"(Required for --psd-estimation) \"\n                                 \"The separation between consecutive \"\n                                 \"segments (s)\")\n        psd_options.add_argument(\"--psd-num-segments\", type=int, default=None,\n                                 help=\"(Optional, used only with \"\n                                 \"--psd-estimation). If given, PSDs will \"\n                                 \"be estimated using only this number of \"\n                                 \"segments. If more data is given than \"\n                                 \"needed to make this number of segments \"\n                                 \"then excess data will not be used in \"\n                                 \"the PSD estimate. If not enough data \"\n                                 \"is given, the code will fail.\")\n    if output:\n        psd_options.add_argument(\"--psd-output\",\n                          help=\"(Optional) Write PSD to specified file\")\n\n    return psd_options", "response": "Adds the options used to call pycbc. psd. from_cli function to an option group."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding the options used to call pycbc. psd. from_cli function to an option group that can be used to create a new object.", "response": "def insert_psd_option_group_multi_ifo(parser):\n    \"\"\"\n    Adds the options used to call the pycbc.psd.from_cli function to an\n    optparser as an OptionGroup. This should be used if you\n    want to use these options in your code.\n\n    Parameters\n    -----------\n    parser : object\n        OptionParser instance.\n    \"\"\"\n    psd_options = parser.add_argument_group(\n                          \"Options to select the method of PSD generation\",\n                          \"The options --psd-model, --psd-file, --asd-file, \"\n                          \"and --psd-estimation are mutually exclusive.\")\n    psd_options.add_argument(\"--psd-model\", nargs=\"+\",\n                          action=MultiDetOptionAction, metavar='IFO:MODEL',\n                          help=\"Get PSD from given analytical model. \"\n                          \"Choose from %s\" %(', '.join(get_psd_model_list()),))\n    psd_options.add_argument(\"--psd-file\", nargs=\"+\",\n                          action=MultiDetOptionAction, metavar='IFO:FILE',\n                          help=\"Get PSD using given PSD ASCII file\")\n    psd_options.add_argument(\"--asd-file\", nargs=\"+\",\n                          action=MultiDetOptionAction, metavar='IFO:FILE',\n                          help=\"Get PSD using given ASD ASCII file\")\n    psd_options.add_argument(\"--psd-estimation\", nargs=\"+\",\n                          action=MultiDetOptionAction, metavar='IFO:FILE',\n                          help=\"Measure PSD from the data, using given \"\n                          \"average method. Choose from \"\n                          \"mean, median or median-mean.\")\n    psd_options.add_argument(\"--psd-segment-length\", type=float, nargs=\"+\",\n                          action=MultiDetOptionAction, metavar='IFO:LENGTH',\n                          help=\"(Required for --psd-estimation) The segment \"\n                               \"length for PSD estimation (s)\")\n    psd_options.add_argument(\"--psd-segment-stride\", type=float, nargs=\"+\",\n                          action=MultiDetOptionAction, metavar='IFO:STRIDE',\n                          help=\"(Required for --psd-estimation) The separation\"\n                               \" between consecutive segments (s)\")\n    psd_options.add_argument(\"--psd-num-segments\", type=int, nargs=\"+\",\n                          default=None,\n                          action=MultiDetOptionAction, metavar='IFO:NUM',\n                          help=\"(Optional, used only with --psd-estimation). \"\n                               \"If given PSDs will be estimated using only \"\n                               \"this number of segments. If more data is \"\n                               \"given than needed to make this number of \"\n                               \"segments than excess data will not be used in \"\n                               \"the PSD estimate. If not enough data is given \"\n                               \"the code will fail.\")\n    psd_options.add_argument(\"--psd-inverse-length\", type=float, nargs=\"+\",\n                          action=MultiDetOptionAction, metavar='IFO:LENGTH',\n                          help=\"(Optional) The maximum length of the impulse\"\n                          \" response of the overwhitening filter (s)\")\n    psd_options.add_argument(\"--psd-output\", nargs=\"+\",\n                          action=MultiDetOptionAction, metavar='IFO:FILE',\n                          help=\"(Optional) Write PSD to specified file\")\n\n    # Options for PSD variation\n    psd_options.add_argument(\"--psdvar-short-segment\", type=float,\n                             metavar=\"SECONDS\", help=\"Length of short segment \"\n                             \"when calculating the PSD variability.\")\n    psd_options.add_argument(\"--psdvar-long-segment\", type=float,\n                             metavar=\"SECONDS\", help=\"Length of long segment \"\n                             \"when calculating the PSD variability.\")\n    psd_options.add_argument(\"--psdvar-short-psd-duration\", type=float,\n                             metavar=\"SECONDS\", help=\"Duration of short \"\n                             \"segments for PSD estimation.\")\n    psd_options.add_argument(\"--psdvar-short-psd-stride\", type=float,\n                             metavar=\"SECONDS\", help=\"Separation between PSD \"\n                             \"estimation segments.\")\n    psd_options.add_argument(\"--psdvar-low-freq\", type=float, metavar=\"HERTZ\",\n                             help=\"Minimum frequency to consider in PSD \"\n                             \"comparison.\")\n    psd_options.add_argument(\"--psdvar-high-freq\", type=float, metavar=\"HERTZ\",\n                             help=\"Maximum frequency to consider in PSD \"\n                             \"comparison.\")\n\n    return psd_options"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the CLI options and verifies that they are consistent and consistent with the given parser.", "response": "def verify_psd_options(opt, parser):\n    \"\"\"Parses the CLI options and verifies that they are consistent and\n    reasonable.\n\n    Parameters\n    ----------\n    opt : object\n        Result of parsing the CLI with OptionParser, or any object with the\n        required attributes (psd_model, psd_file, asd_file, psd_estimation,\n        psd_segment_length, psd_segment_stride, psd_inverse_length, psd_output).\n    parser : object\n        OptionParser instance.\n    \"\"\"\n    try:\n        psd_estimation = opt.psd_estimation is not None\n    except AttributeError:\n        psd_estimation = False\n\n    for opt_group in ensure_one_opt_groups:\n        ensure_one_opt(opt, parser, opt_group)\n\n    if psd_estimation:\n        required_opts(opt, parser,\n                      ['--psd-segment-stride', '--psd-segment-length'],\n                      required_by = \"--psd-estimation\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the CLI options and verifies that they are consistent and reasonable.", "response": "def verify_psd_options_multi_ifo(opt, parser, ifos):\n    \"\"\"Parses the CLI options and verifies that they are consistent and\n    reasonable.\n\n    Parameters\n    ----------\n    opt : object\n        Result of parsing the CLI with OptionParser, or any object with the\n        required attributes (psd_model, psd_file, asd_file, psd_estimation,\n        psd_segment_length, psd_segment_stride, psd_inverse_length, psd_output).\n    parser : object\n        OptionParser instance.\n    \"\"\"\n    for ifo in ifos:\n        for opt_group in ensure_one_opt_groups:\n            ensure_one_opt_multi_ifo(opt, parser, ifo, opt_group)\n\n        if opt.psd_estimation[ifo]:\n            required_opts_multi_ifo(opt, parser, ifo,\n                      ['--psd-segment-stride', '--psd-segment-length'],\n                          required_by = \"--psd-estimation\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a set of overlapping PSDs to cover a stretch of data.", "response": "def generate_overlapping_psds(opt, gwstrain, flen, delta_f, flow,\n                              dyn_range_factor=1., precision=None):\n    \"\"\"Generate a set of overlapping PSDs to cover a stretch of data. This\n    allows one to analyse a long stretch of data with PSD measurements that\n    change with time.\n\n    Parameters\n    -----------\n    opt : object\n        Result of parsing the CLI with OptionParser, or any object with the\n        required attributes (psd_model, psd_file, asd_file, psd_estimation,\n        psd_segment_length, psd_segment_stride, psd_inverse_length, psd_output).\n    gwstrain : Strain object\n        The timeseries of raw data on which to estimate PSDs.\n    flen : int\n        The length in samples of the output PSDs.\n    delta_f : float\n        The frequency step of the output PSDs.\n    flow: float\n        The low frequncy cutoff to use when calculating the PSD.\n    dyn_range_factor : {1, float}\n        For PSDs taken from models or text files, if `dyn_range_factor` is\n        not None, then the PSD is multiplied by `dyn_range_factor` ** 2.\n    precision : str, choices (None,'single','double')\n        If not specified, or specified as None, the precision of the returned\n        PSD will match the precision of the data, if measuring a PSD, or will\n        match the default precision of the model if using an analytical PSD.\n        If 'single' the PSD will be converted to float32, if not already in\n        that precision. If 'double' the PSD will be converted to float64, if\n        not already in that precision.\n\n    Returns\n    --------\n    psd_and_times : list of (start, end, PSD) tuples\n        This is a list of tuples containing one entry for each PSD. The first\n        and second entries (start, end) in each tuple represent the index\n        range of the gwstrain data that was used to estimate that PSD. The\n        third entry (psd) contains the PSD estimate between that interval.\n    \"\"\"\n    if not opt.psd_estimation:\n        psd = from_cli(opt, flen, delta_f, flow, strain=gwstrain,\n                       dyn_range_factor=dyn_range_factor, precision=precision)\n        psds_and_times = [ (0, len(gwstrain), psd) ]\n        return psds_and_times\n\n    # Figure out the data length used for PSD generation\n    seg_stride = int(opt.psd_segment_stride * gwstrain.sample_rate)\n    seg_len = int(opt.psd_segment_length * gwstrain.sample_rate)\n    input_data_len = len(gwstrain)\n\n    if opt.psd_num_segments is None:\n        # FIXME: Should we make --psd-num-segments mandatory?\n        #        err_msg = \"You must supply --num-segments.\"\n        #        raise ValueError(err_msg)\n        num_segments = int(input_data_len // seg_stride) - 1\n    else:\n        num_segments = int(opt.psd_num_segments)\n\n    psd_data_len = (num_segments - 1) * seg_stride + seg_len\n\n    # How many unique PSD measurements is this?\n    psds_and_times = []\n    if input_data_len < psd_data_len:\n        err_msg = \"Input data length must be longer than data length needed \"\n        err_msg += \"to estimate a PSD. You specified that a PSD should be \"\n        err_msg += \"estimated with %d seconds. \" %(psd_data_len)\n        err_msg += \"Input data length is %d seconds. \" %(input_data_len)\n        raise ValueError(err_msg)\n    elif input_data_len == psd_data_len:\n        num_psd_measurements = 1\n        psd_stride = 0\n    else:\n        num_psd_measurements = int(2 * (input_data_len-1) / psd_data_len)\n        psd_stride = int((input_data_len - psd_data_len) / num_psd_measurements)\n\n    for idx in range(num_psd_measurements):\n        if idx == (num_psd_measurements - 1):\n            start_idx = input_data_len - psd_data_len\n            end_idx = input_data_len\n        else:\n            start_idx = psd_stride * idx\n            end_idx = psd_data_len + psd_stride * idx\n        strain_part = gwstrain[start_idx:end_idx]\n        psd = from_cli(opt, flen, delta_f, flow, strain=strain_part,\n                       dyn_range_factor=dyn_range_factor, precision=precision)\n        psds_and_times.append( (start_idx, end_idx, psd) )\n    return psds_and_times"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef associate_psds_to_segments(opt, fd_segments, gwstrain, flen, delta_f, flow,\n                               dyn_range_factor=1., precision=None):\n    \"\"\"Generate a set of overlapping PSDs covering the data in GWstrain.\n    Then associate these PSDs with the appropriate segment in strain_segments.\n\n    Parameters\n    -----------\n    opt : object\n        Result of parsing the CLI with OptionParser, or any object with the\n        required attributes (psd_model, psd_file, asd_file, psd_estimation,\n        psd_segment_length, psd_segment_stride, psd_inverse_length, psd_output).\n    fd_segments : StrainSegments.fourier_segments() object\n        The fourier transforms of the various analysis segments. The psd\n        attribute of each segment is updated to point to the appropriate PSD.\n    gwstrain : Strain object\n        The timeseries of raw data on which to estimate PSDs.\n    flen : int\n        The length in samples of the output PSDs.\n    delta_f : float\n        The frequency step of the output PSDs.\n    flow: float\n        The low frequncy cutoff to use when calculating the PSD.\n    dyn_range_factor : {1, float}\n        For PSDs taken from models or text files, if `dyn_range_factor` is\n        not None, then the PSD is multiplied by `dyn_range_factor` ** 2.\n    precision : str, choices (None,'single','double')\n        If not specified, or specified as None, the precision of the returned\n        PSD will match the precision of the data, if measuring a PSD, or will\n        match the default precision of the model if using an analytical PSD.\n        If 'single' the PSD will be converted to float32, if not already in\n        that precision. If 'double' the PSD will be converted to float64, if\n        not already in that precision.\n    \"\"\"\n    psds_and_times = generate_overlapping_psds(opt, gwstrain, flen, delta_f,\n                                       flow, dyn_range_factor=dyn_range_factor,\n                                       precision=precision)\n\n    for fd_segment in fd_segments:\n        best_psd = None\n        psd_overlap = 0\n        inp_seg = segments.segment(fd_segment.seg_slice.start,\n                                   fd_segment.seg_slice.stop)\n        for start_idx, end_idx, psd in psds_and_times:\n            psd_seg = segments.segment(start_idx, end_idx)\n            if psd_seg.intersects(inp_seg):\n                curr_overlap = abs(inp_seg & psd_seg)\n                if curr_overlap > psd_overlap:\n                    psd_overlap = curr_overlap\n                    best_psd = psd\n        if best_psd is None:\n            err_msg = \"No PSDs found intersecting segment!\"\n            raise ValueError(err_msg)\n        fd_segment.psd = best_psd", "response": "This function creates a set of overlapping PSDs covering the data in GWstrain and associates them with the appropriate segment in strain_segments."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nassociate PSDs to segments for a single ifo when using multi - detector.", "response": "def associate_psds_to_single_ifo_segments(opt, fd_segments, gwstrain, flen,\n                                          delta_f, flow, ifo,\n                                          dyn_range_factor=1., precision=None):\n    \"\"\"\n    Associate PSDs to segments for a single ifo when using the multi-detector\n    CLI\n    \"\"\"\n    single_det_opt = copy_opts_for_single_ifo(opt, ifo)\n    associate_psds_to_segments(single_det_opt, fd_segments, gwstrain, flen,\n                               delta_f, flow, dyn_range_factor=dyn_range_factor,\n                               precision=precision)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef associate_psds_to_multi_ifo_segments(opt, fd_segments, gwstrain, flen,\n                                         delta_f, flow, ifos,\n                                         dyn_range_factor=1., precision=None):\n    \"\"\"\n    Associate PSDs to segments for all ifos when using the multi-detector CLI\n    \"\"\"\n    for ifo in ifos:\n        if gwstrain is not None:\n            strain = gwstrain[ifo]\n        else:\n            strain = None\n\n        if fd_segments is not None:\n            segments = fd_segments[ifo]\n        else:\n            segments = None\n\n        associate_psds_to_single_ifo_segments(opt, segments, strain, flen,\n                delta_f, flow, ifo, dyn_range_factor=dyn_range_factor,\n                precision=precision)", "response": "Associate PSDs to segments for all ifos when using the multi - detector CLI"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmap an input dictionary of sampling parameters to the adjust_strain function by filtering the dictionary of calibration parameters and then calling adjust_strain function.", "response": "def map_to_adjust(self, strain, prefix='recalib_', **params):\n        \"\"\"Map an input dictionary of sampling parameters to the\n        adjust_strain function by filtering the dictionary for the\n        calibration parameters, then calling adjust_strain.\n\n        Parameters\n        ----------\n        strain : FrequencySeries\n            The strain to be recalibrated.\n        prefix: str\n            Prefix for calibration parameter names\n        params : dict\n            Dictionary of sampling parameters which includes\n            calibration parameters.\n        Return\n        ------\n        strain_adjusted : FrequencySeries\n            The recalibrated strain.\n        \"\"\"\n\n        self.params.update({\n            key[len(prefix):]: params[key]\n            for key in params if prefix in key and self.ifo_name in key})\n\n        strain_adjusted = self.apply_calibration(strain)\n\n        return strain_adjusted"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef apply_calibration(self, strain):\n        amplitude_parameters =\\\n            [self.params['amplitude_{}_{}'.format(self.ifo_name, ii)]\n             for ii in range(self.n_points)]\n        amplitude_spline = UnivariateSpline(self.spline_points,\n                                            amplitude_parameters)\n        delta_amplitude = amplitude_spline(strain.sample_frequencies.numpy())\n\n        phase_parameters =\\\n            [self.params['phase_{}_{}'.format(self.ifo_name, ii)]\n             for ii in range(self.n_points)]\n        phase_spline = UnivariateSpline(self.spline_points, phase_parameters)\n        delta_phase = phase_spline(strain.sample_frequencies.numpy())\n\n        strain_adjusted = strain * (1.0 + delta_amplitude)\\\n            * (2.0 + 1j * delta_phase) / (2.0 - 1j * delta_phase)\n\n        return strain_adjusted", "response": "Applies cubic spline calibration to the strain."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the sensing function c for the current cavity at time t.", "response": "def update_c(self, fs=None, qinv=None, fc=None, kappa_c=1.0):\n        \"\"\" Calculate the sensing function c(f,t) given the new parameters\n        kappa_c(t), kappa_a(t), f_c(t), fs, and qinv.\n\n        Parameters\n        ----------\n        fc : float\n            Coupled-cavity (CC) pole at time t.\n        kappa_c : float\n            Scalar correction factor for sensing function at time t.\n        fs : float\n            Spring frequency for signal recycling cavity.\n        qinv : float\n            Inverse quality factor for signal recycling cavity.\n\n        Returns\n        -------\n        c : numpy.array\n            The new sensing function c(f,t).\n        \"\"\"\n        detuning_term = self.freq**2 / (self.freq**2 - 1.0j *self.freq*fs * \\\n                                        qinv + fs**2)\n        return self.c_res * kappa_c / (1 + 1.0j * self.freq/fc)*detuning_term"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_g(self, fs=None, qinv=None, fc=None, kappa_tst_re=1.0,\n                 kappa_tst_im=0.0, kappa_pu_re=1.0, kappa_pu_im=0.0,\n                 kappa_c=1.0):\n        \"\"\" Calculate the open loop gain g(f,t) given the new parameters\n        kappa_c(t), kappa_a(t), f_c(t), fs, and qinv.\n\n        Parameters\n        ----------\n        fc : float\n            Coupled-cavity (CC) pole at time t.\n        kappa_c : float\n            Scalar correction factor for sensing function c at time t.\n        kappa_tst_re : float\n            Real part of scalar correction factor for actuation function\n            a_tst0 at time t.\n        kappa_pu_re : float\n            Real part of scalar correction factor for actuation function\n            a_pu0 at time t.\n        kappa_tst_im : float\n            Imaginary part of scalar correction factor for actuation function\n            a_tst0 at time t.\n        kappa_pu_im : float\n            Imaginary part of scalar correction factor for actuation function\n            a_pu0 at time t.\n        fs : float\n            Spring frequency for signal recycling cavity.\n        qinv : float\n            Inverse quality factor for signal recycling cavity.\n\n        Returns\n        -------\n        g : numpy.array\n            The new open loop gain g(f,t).\n        \"\"\"\n        c = self.update_c(fs=fs, qinv=qinv, fc=fc, kappa_c=kappa_c)\n        a_tst = self.a_tst0 * (kappa_tst_re + 1.0j * kappa_tst_im)\n        a_pu = self.a_pu0 * (kappa_pu_re + 1.0j * kappa_pu_im)\n        return c * self.d0 * (a_tst + a_pu)", "response": "Calculates the open loop gain g for the new parameters."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the response function R for the new cavity and the new QInvF.", "response": "def update_r(self, fs=None, qinv=None, fc=None, kappa_c=1.0,\n                 kappa_tst_re=1.0, kappa_tst_im=0.0, kappa_pu_re=1.0,\n                 kappa_pu_im=0.0):\n        \"\"\" Calculate the response function R(f,t) given the new parameters\n        kappa_c(t), kappa_a(t), f_c(t), fs, and qinv.\n\n        Parameters\n        ----------\n        fc : float\n            Coupled-cavity (CC) pole at time t.\n        kappa_c : float\n            Scalar correction factor for sensing function c at time t.\n        kappa_tst_re : float\n            Real part of scalar correction factor for actuation function\n            a_tst0 at time t.\n        kappa_pu_re : float\n            Real part of scalar correction factor for actuation function\n            a_pu0 at time t.\n        kappa_tst_im : float\n            Imaginary part of scalar correction factor for actuation function\n            a_tst0 at time t.\n        kappa_pu_im : float\n            Imaginary part of scalar correction factor for actuation function\n            a_pu0 at time t.\n        fs : float\n            Spring frequency for signal recycling cavity.\n        qinv : float\n            Inverse quality factor for signal recycling cavity.\n\n        Returns\n        -------\n        r : numpy.array\n            The new response function r(f,t).\n        \"\"\"\n        c = self.update_c(fs=fs, qinv=qinv, fc=fc, kappa_c=kappa_c)\n        g = self.update_g(fs=fs, qinv=qinv, fc=fc, kappa_c=kappa_c,\n                          kappa_tst_re=kappa_tst_re,\n                          kappa_tst_im=kappa_tst_im,\n                          kappa_pu_re=kappa_pu_re, kappa_pu_im=kappa_pu_im)\n        return (1.0 + g) / c"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef adjust_strain(self, strain, delta_fs=None, delta_qinv=None,\n                      delta_fc=None, kappa_c=1.0, kappa_tst_re=1.0,\n                      kappa_tst_im=0.0, kappa_pu_re=1.0, kappa_pu_im=0.0):\n        \"\"\"Adjust the FrequencySeries strain by changing the time-dependent\n        calibration parameters kappa_c(t), kappa_a(t), f_c(t), fs, and qinv.\n\n        Parameters\n        ----------\n        strain : FrequencySeries\n            The strain data to be adjusted.\n        delta_fc : float\n            Change in coupled-cavity (CC) pole at time t.\n        kappa_c : float\n            Scalar correction factor for sensing function c0 at time t.\n        kappa_tst_re : float\n            Real part of scalar correction factor for actuation function\n            A_{tst0} at time t.\n        kappa_tst_im : float\n            Imaginary part of scalar correction factor for actuation function\n            A_tst0 at time t.\n        kappa_pu_re : float\n            Real part of scalar correction factor for actuation function\n            A_{pu0} at time t.\n        kappa_pu_im : float\n            Imaginary part of scalar correction factor for actuation function\n            A_{pu0} at time t.\n        fs : float\n            Spring frequency for signal recycling cavity.\n        qinv : float\n            Inverse quality factor for signal recycling cavity.\n\n        Returns\n        -------\n        strain_adjusted : FrequencySeries\n            The adjusted strain.\n        \"\"\"\n        fc = self.fc0 + delta_fc if delta_fc else self.fc0\n        fs = self.fs0 + delta_fs if delta_fs else self.fs0\n        qinv = self.qinv0 + delta_qinv if delta_qinv else self.qinv0\n\n        # calculate adjusted response function\n        r_adjusted = self.update_r(fs=fs, qinv=qinv, fc=fc, kappa_c=kappa_c,\n                                   kappa_tst_re=kappa_tst_re,\n                                   kappa_tst_im=kappa_tst_im,\n                                   kappa_pu_re=kappa_pu_re,\n                                   kappa_pu_im=kappa_pu_im)\n\n        # calculate error function\n        k = r_adjusted / self.r0\n\n        # decompose into amplitude and unwrapped phase\n        k_amp = np.abs(k)\n        k_phase = np.unwrap(np.angle(k))\n\n        # convert to FrequencySeries by interpolating then resampling\n        order = 1\n        k_amp_off = UnivariateSpline(self.freq, k_amp, k=order, s=0)\n        k_phase_off = UnivariateSpline(self.freq, k_phase, k=order, s=0)\n        freq_even = strain.sample_frequencies.numpy()\n        k_even_sample = k_amp_off(freq_even) * \\\n                        np.exp(1.0j * k_phase_off(freq_even))\n        strain_adjusted = FrequencySeries(strain.numpy() * \\\n                                          k_even_sample,\n                                          delta_f=strain.delta_f)\n\n        return strain_adjusted", "response": "Adjust the frequency series strain by changing the time - dependent calibration parameters kappa_c kappa_tst_re kappa_tst_im and kappa_pu_re and kappa_pu_im."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts the contents of a file with the columns freq real ( h ) and imaginary ( h ) to a numpy. array with columns freq real ( h ) and imaginary ( h ).", "response": "def tf_from_file(cls, path, delimiter=\" \"):\n        \"\"\"Convert the contents of a file with the columns\n        [freq, real(h), imag(h)] to a numpy.array with columns\n        [freq, real(h)+j*imag(h)].\n\n        Parameters\n        ----------\n        path : string\n        delimiter : {\" \", string}\n\n        Return\n        ------\n        numpy.array\n        \"\"\"\n        data = np.loadtxt(path, delimiter=delimiter)\n        freq = data[:, 0]\n        h = data[:, 1] + 1.0j * data[:, 2]\n        return np.array([freq, h]).transpose()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_config(cls, cp, ifo, section):\n        # read transfer functions\n        tfs = []\n        tf_names = [\"a-tst\", \"a-pu\", \"c\", \"d\"]\n        for tag in ['-'.join([ifo, \"transfer-function\", name])\n                    for name in tf_names]:\n            tf_path = cp.get_opt_tag(section, tag, None)\n            tfs.append(cls.tf_from_file(tf_path))\n        a_tst0 = tfs[0][:, 1]\n        a_pu0 = tfs[1][:, 1]\n        c0 = tfs[2][:, 1]\n        d0 = tfs[3][:, 1]\n        freq = tfs[0][:, 0]\n\n        # if upper stage actuation is included, read that in and add it\n        # to a_pu0\n        uim_tag = '-'.join([ifo, 'transfer-function-a-uim'])\n        if cp.has_option(section, uim_tag):\n            tf_path = cp.get_opt_tag(section, uim_tag, None)\n            a_pu0 += cls.tf_from_file(tf_path)[:, 1]\n\n        # read fc0, fs0, and qinv0\n        fc0 = cp.get_opt_tag(section, '-'.join([ifo, \"fc0\"]), None)\n        fs0 = cp.get_opt_tag(section, '-'.join([ifo, \"fs0\"]), None)\n        qinv0 = cp.get_opt_tag(section, '-'.join([ifo, \"qinv0\"]), None)\n\n        return cls(freq=freq, fc0=fc0, c0=c0, d0=d0, a_tst0=a_tst0,\n                   a_pu0=a_pu0, fs0=fs0, qinv0=qinv0)", "response": "Read a config file to get the calibration options and transfer functions which will be used to intialize the model."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef map_to_adjust(self, strain, **params):\n        # calibration param names\n        arg_names = ['delta_fs', 'delta_fc', 'delta_qinv', 'kappa_c',\n                     'kappa_tst_re', 'kappa_tst_im', 'kappa_pu_re',\n                     'kappa_pu_im']\n        # calibration param labels as they exist in config files\n        arg_labels = [''.join(['calib_', name]) for name in arg_names]\n        # default values for calibration params\n        default_values = [0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]\n        # make list of calibration param values\n        calib_args = []\n        for arg, val in zip(arg_labels, default_values):\n            if arg in params:\n                calib_args.append(params[arg])\n            else:\n                calib_args.append(val)\n        # adjust the strain using calibration param values\n        strain_adjusted = self.adjust_strain(strain, delta_fs=calib_args[0],\n                              delta_fc=calib_args[1], delta_qinv=calib_args[2],\n                              kappa_c=calib_args[3],\n                              kappa_tst_re=calib_args[4],\n                              kappa_tst_im=calib_args[5],\n                              kappa_pu_re=calib_args[6],\n                              kappa_pu_im=calib_args[7])\n        return strain_adjusted", "response": "Map an input dictionary of sampling parameters to the adjust_strain function by filtering the dictionary of sampling parameters and then calling adjust_strain."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsave a html output to file with metadata", "response": "def save_html_with_metadata(fig, filename, fig_kwds, kwds):\n    \"\"\" Save a html output to file with metadata \"\"\"\n    if isinstance(fig, str):\n        text = fig\n    else:\n        from mpld3 import fig_to_html\n        text = fig_to_html(fig, **fig_kwds)\n\n    f = open(filename, 'w')\n    for key, value in kwds.items():\n        value = escape(value, escape_table)\n        line = \"<div class=pycbc-meta key=\\\"%s\\\" value=\\\"%s\\\"></div>\" % (str(key), value)\n        f.write(line)\n\n    f.write(text)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_html_metadata(filename):\n    parser = MetaParser()\n    data = open(filename, 'r').read()\n\n    if 'pycbc-meta' in data:\n        print(\"LOADING HTML FILE %s\" % filename)\n    parser.feed(data)\n    cp = ConfigParser.ConfigParser(parser.metadata)\n    cp.add_section(os.path.basename(filename))\n    return cp", "response": "Load metadata from html file"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save_png_with_metadata(fig, filename, fig_kwds, kwds):\n    from PIL import Image, PngImagePlugin\n    fig.savefig(filename, **fig_kwds)\n\n    im = Image.open(filename)\n    meta = PngImagePlugin.PngInfo()\n\n    for key in kwds:\n        meta.add_text(str(key), str(kwds[key]))\n\n    im.save(filename, \"png\", pnginfo=meta)", "response": "Save a matplotlib figure to a png with metadata"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsaves a matplotlib figure to a file with metadata included.", "response": "def save_fig_with_metadata(fig, filename, fig_kwds=None, **kwds):\n    \"\"\" Save plot to file with metadata included. Kewords translate to metadata\n    that is stored directly in the plot file. Limited format types available.\n\n    Parameters\n    ----------\n    fig: matplotlib figure\n        The matplotlib figure to save to the file\n    filename: str\n        Name of file to store the plot.\n    \"\"\"\n    if fig_kwds is None:\n        fig_kwds = {}\n    try:\n        extension = os.path.splitext(filename)[1]\n        kwds['version'] = pycbc.version.git_verbose_msg\n        _metadata_saver[extension](fig, filename, fig_kwds, kwds)\n    except KeyError:\n        raise TypeError('Cannot save file %s with metadata, extension %s not '\n                        'supported at this time' % (filename, extension))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload the plot related metadata saved in a file into a ConfigParser object containing the plot related metadata.", "response": "def load_metadata_from_file(filename):\n    \"\"\" Load the plot related metadata saved in a file\n\n    Parameters\n    ----------\n    filename: str\n        Name of file load metadata from.\n\n    Returns\n    -------\n    cp: ConfigParser\n        A configparser object containing the metadata\n    \"\"\"\n    try:\n        extension = os.path.splitext(filename)[1]\n        return _metadata_loader[extension](filename)\n    except KeyError:\n        raise TypeError('Cannot read metadata from file %s, extension %s not '\n                        'supported at this time' % (filename, extension))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_code_version_numbers(cp):\n    code_version_dict = {}\n    for _, value in cp.items('executables'):\n        _, exe_name = os.path.split(value)\n        version_string = None\n        if value.startswith('gsiftp://') or value.startswith('http://'):\n            code_version_dict[exe_name] = \"Using bundle downloaded from %s\" % value\n        else:\n            try:\n                if value.startswith('file://'):\n                    value = value[7:]\n                version_string = subprocess.check_output([value, '--version'],\n                                                        stderr=subprocess.STDOUT) \n            except subprocess.CalledProcessError:\n                version_string = \"Executable fails on %s --version\" % (value)\n            except OSError:\n                version_string = \"Executable doesn't seem to exist(!)\"\n            code_version_dict[exe_name] = version_string\n    return code_version_dict", "response": "Will extract the version information from the executables listed in\n    the executable section of the supplied ConfigParser object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef initialize_page(title, style, script, header=None):\n\n    page = markup.page(mode=\"strict_html\")\n    page._escape = False\n    page.init(title=title, css=style, script=script, header=header)\n\n    return page", "response": "A function that returns a markup. py page object with the required html\n    header."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_banner(title, text='&nbsp;'):\n\n    page = markup.page(mode=\"strict_html\")\n    page._escape = False\n\n    page.div(id=\"header\")\n    page.h1()\n    page.add(title)\n    page.h1.close()\n    page.h3()\n    page.add(text)\n    page.h3.close()\n\n    page.hr(class_=\"short\")\n    page.hr(class_=\"long\")\n\n    page.div.close()\n\n    page.div(id=\"container\")\n\n    return page", "response": "Write html banner into markup. page object"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_table(page, headers, data, cl=''):\n\n    \"\"\"\n    Write table in html\n    \"\"\"\n\n    page.table(class_=cl)\n\n    # list\n    if cl=='list':\n        for i in range(len(headers)):\n\n            page.tr()\n            page.th()\n            page.add('%s' % headers[i])\n            page.th.close()\n            page.td()\n            page.add('%s' % data[i])\n            page.td.close()\n            page.tr.close()\n\n    else:\n        page.tr()\n        for n in headers:\n            page.th()\n            page.add('%s' % n)\n            page.th.close()\n        page.tr.close()\n\n        if data and not re.search('list',str(type(data[0]))):\n            data = [data]\n\n        for row in data:\n            page.tr()\n            for item in row:\n                page.td()\n                page.add('%s' % item)\n                page.td.close()\n            page.tr.close()\n\n    page.table.close()\n\n    return page", "response": "Writes a table in html"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwriting summary of information to markup. page object page", "response": "def write_summary(page, args, ifos, skyError=None, ipn=False, ipnError=False):\n\n    \"\"\"\n        Write summary of information to markup.page object page\n    \"\"\"\n    from pylal import antenna\n    from lal.gpstime import gps_to_utc, LIGOTimeGPS\n\n    gps = args.start_time\n    grbdate = gps_to_utc(LIGOTimeGPS(gps))\\\n                                .strftime(\"%B %d %Y, %H:%M:%S %ZUTC\")\n    page.h3()\n    page.add('Basic information')\n    page.h3.close()\n\n    if ipn:\n        ra = []\n        dec = []\n        td1 = []\n        td2 = []\n        td3 = []\n        timedelay = {}\n        search_file = '../../../S5IPN_GRB%s_search_180deg.txt' % args.grb_name\n        for line in open(search_file):\n            ra.append(line.split()[0])\n            dec.append(line.split()[1])\n        th1 = [ 'GPS', 'Date', 'Error Box (sq.deg.)', 'IFOs' ]\n        td1 = [ gps, grbdate, ipnError, ifos ]\n        th2 = [ 'RA', 'DEC' ]\n        th3 = ['Timedelays (ms)', '', '' ]\n        for ra_i,dec_i in zip(ra,dec):\n            td_i = [ ra_i, dec_i ]\n            td2.append(td_i)\n        ifo_list = [ ifos[i*2:(i*2)+2] for i in range(int(len(ifos)/2)) ]\n        for j in td2:\n            for p in range(0, len(ifo_list)):\n                for q in range(0, len(ifo_list)):\n                    pairs = [ifo_list[p], ifo_list[q]]\n                    ifo_pairs = \"\".join(pairs)\n                    timedelay[ifo_pairs] = antenna.timeDelay(int(gps),\n                            float(j[0]), float(j[1]), 'degree', ifo_list[p],\n                            ifo_list[q])\n                    timedelay[ifo_pairs]=\"%.4f\" % timedelay[ifo_pairs]\n            if ifos == 'H1H2L1':\n                td3.append(['H1L1: %f' % float(timedelay['H1L1'])])\n            if ifos == 'H1H2L1V1':\n                td3.append(['H1L1: %f' % float(timedelay['H1L1']),\n                            'H1V1: %f' % float(timedelay['H1V1']),\n                            'L1V1: %f' % float(timedelay['L1V1'])])\n            if ifos == 'L1V1':\n                td3.append(['L1V1: %f' % float(timedelay['L1V1'])])\n        page = write_table(page, th1, td1)\n        page = write_table(page, th2, td2)\n        page = write_table(page, th3, td3)\n\n    else:\n        ra = args.ra\n        dec = args.dec\n        if skyError:\n            th = [ 'GPS', 'Date', 'RA', 'DEC', 'Sky Error', 'IFOs' ]\n            td = [ gps, grbdate, ra, dec, skyError, ifos ]\n        else:\n            th = [ 'GPS', 'Date', 'RA', 'DEC', 'IFOs' ]\n            td = [ gps, grbdate, ra, dec, ifos ]\n\n        page = write_table(page, th, td)\n\n    return page"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_antenna(page, args, seg_plot=None, grid=False, ipn=False):\n\n    \"\"\"\n    Write antenna factors to merkup.page object page and generate John's\n    detector response plot.\n    \"\"\"\n    from pylal import antenna\n\n    page.h3()\n    page.add('Antenna factors and sky locations')\n    page.h3.close()\n\n    th = []\n    td = []\n    th2 = []\n    td2 = []\n\n    ifos = [args.ifo_tag[i:i+2] for i in range(0, len(args.ifo_tag), 2)]\n\n    if ipn:\n        antenna_ifo = {}\n        ra = []\n        dec = []\n        # FIXME: Remove hardcoding here and show this in all cases\n        search_file = open('../../../S5IPN_GRB%s_search_180deg.txt'\n                           % args.grb_name)\n        for line in search_file:\n            ra.append(line.split()[0])\n            dec.append(line.split()[1])\n        for ifo in ifos:\n            antenna_ifo[ifo] = []\n            for k, l in zip(ra, dec):\n                _, _, _, f_q = antenna.response(args.start_time, float(k),\n                                                float(l), 0.0, 0.0, 'degree',\n                                                ifo)\n                antenna_ifo[ifo].append(round(f_q,3))\n        dectKeys = antenna_ifo.keys()\n\n        for elements in range(len(antenna_ifo.values()[0])):\n            newDict={}\n            for detectors in range(len(antenna_ifo.keys())):\n                newDict[dectKeys[detectors]] = antenna_ifo[\\\n                                               dectKeys[detectors]][elements]\n            for key in newDict.keys():\n                th.append(key)\n            td.append(newDict.values())\n        page = write_table(page, list(set(th)), td)\n    for ifo in ifos:\n        _, _, _, f_q = antenna.response(args.start_time, args.ra, args.dec,\n                                        0.0, 0.0, 'degree',ifo)\n        th.append(ifo)\n        td.append(round(f_q, 3))\n\n    #FIXME: Work out a way to make these external calls safely\n    #cmmnd = 'projectedDetectorTensor --gps-sec %d --ra-deg %f --dec-deg %f' \\\n    #         % (args.start_time,args.ra, args.dec)\n    #for ifo in ifos:\n    #    if ifo == 'H1':\n    #        cmmnd += ' --display-lho'\n    #    elif ifo == 'L1':\n    #        cmmnd += ' --display-llo'\n    #    elif ifo == 'V1':\n    #        cmmnd += ' --display-virgo'\n    #status = make_external_call(cmmnd)\n\n    page = write_table(page, th, td)\n\n#    plot = markup.page()\n#    p = \"projtens.png\"\n#    plot.a(href=p, title=\"Detector response and polarization\")\n#    plot.img(src=p)\n#    plot.a.close()\n#    th2 = ['Response Diagram']\n#    td2 = [plot() ]\n\n# FIXME: Add these in!!\n#    plot = markup.page()\n#    p = \"ALL_TIMES/plots_clustered/GRB%s_search.png\"\\\n#        % args.grb_name\n#    plot.a(href=p, title=\"Error Box Search\")\n#    plot.img(src=p)\n#    plot.a.close()\n#    th2.append('Error Box Search')\n#    td2.append(plot())\n\n#    plot = markup.page()\n#    p = \"ALL_TIMES/plots_clustered/GRB%s_simulations.png\"\\\n#        % args.grb_name\n#    plot.a(href=p, title=\"Error Box Simulations\")\n#    plot.img(src=p)\n#    plot.a.close()\n#    th2.append('Error Box Simulations')\n#    td2.append(plot())\n\n    if seg_plot is not None:\n        plot = markup.page()\n        p = os.path.basename(seg_plot)\n        plot.a(href=p, title=\"Science Segments\")\n        plot.img(src=p)\n        plot.a.close()\n        th2.append('Science Segments')\n        td2.append(plot())\n\n    plot = markup.page()\n    p = \"ALL_TIMES/plots_clustered/GRB%s_sky_grid.png\"\\\n            % args.grb_name\n    plot.a(href=p, title=\"Sky Grid\")\n    plot.img(src=p)\n    plot.a.close()\n    th2.append('Sky Grid')\n    td2.append(plot())\n\n#    plot = markup.page()\n#    p = \"GRB%s_inspiral_horizon_distance.png\"\\\n#            % args.grb_name\n#    plot.a(href=p, title=\"Inspiral Horizon Distance\")\n#    plot.img(src=p)\n#    plot.a.close()\n#    th2.append('Inspiral Horizon Distance')\n#    td2.append(plot())\n\n    page = write_table(page, th2, td2)\n\n    return page", "response": "Writes antenna factors and sky locations to merkup. page object page and generate John s antenna detector response plot."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_offsource(page, args, grbtag, onsource=False):\n\n    \"\"\"\n        Write offsource SNR versus time plots to markup.page object page\n    \"\"\"\n\n    th = ['Re-weighted SNR', 'Coherent SNR']\n\n    if args.time_slides:\n        if onsource:\n            out_dir = 'ZEROLAG_ALL'\n        else:\n            out_dir = 'ZEROLAG_OFF'\n    else:\n        if onsource:\n            out_dir = 'ALL_TIMES'\n        else:\n            out_dir = 'OFFSOURCE'\n\n    plot = markup.page()\n    p = \"%s/plots_clustered/GRB%s_bestnr_vs_time_noinj.png\" % (out_dir, grbtag)\n    plot.a(href=p, title=\"Detection statistic versus time\")\n    plot.img(src=p)\n    plot.a.close()\n    td = [ plot() ]\n\n    plot = markup.page()\n    p = \"%s/plots_clustered/GRB%s_triggers_vs_time_noinj.png\" % (out_dir, grbtag)\n    plot.a(href=p, title=\"Coherent SNR versus time\")\n    plot.img(src=p)\n    plot.a.close()\n    td.append(plot())\n\n    ifos = [args.ifo_tag[i:i+2] for i in range(0, len(args.ifo_tag), 2)]\n    for ifo in ifos:\n        th.append('%s SNR' % ifo)\n        plot = markup.page()\n        p = \"%s/plots_clustered/GRB%s_%s_triggers_vs_time_noinj.png\"\\\n            % (out_dir, grbtag, ifo)\n        plot.a(href=p, title=\"%s SNR versus time\" % ifo)\n        plot.img(src=p)\n        plot.a.close()\n        td.append(plot())\n\n    page = write_table(page, th, td)\n\n    return page", "response": "Writes offsource SNR versus time plots to markup. page object page"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write_chisq(page, injList, grbtag):\n\n    \"\"\"\n        Write injection chisq plots to markup.page object page\n    \"\"\"\n\n    if injList:\n        th = ['']+injList + ['OFFSOURCE']\n    else:\n        th= ['','OFFSOURCE']\n        injList = ['OFFSOURCE']\n    td = []\n\n    plots = ['bank_veto','auto_veto','chi_square', 'mchirp']\n\n    for test in plots:\n        pTag = test.replace('_',' ').title()\n        d = [pTag]\n        for inj in injList + ['OFFSOURCE']:\n            plot = markup.page()\n            p = \"%s/plots_clustered/GRB%s_%s_vs_snr_zoom.png\" % (inj, grbtag,\n                                                                 test)\n            plot.a(href=p, title=\"%s %s versus SNR\" % (inj, pTag))\n            plot.img(src=p)\n            plot.a.close()\n            d.append(plot())\n\n        td.append(d)\n\n    page = write_table(page, th, td)\n\n    return page", "response": "Writes injection chisq plots to markup. page object page"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_inj_snrs(page, ifos, injList, grbtag):\n\n    \"\"\"\n        Write injection chisq plots to markup.page object page\n    \"\"\"\n\n    if injList:\n        th = ['']+injList + ['OFFSOURCE']\n    else:\n        th= ['','OFFSOURCE']\n        injList = ['OFFSOURCE']\n    td = []\n\n    ifos = [ifos[i:i+2] for i in range(0, len(ifos), 2)]\n    plots = ['null_stat2']+['%s_snr' % ifo for ifo in ifos]\n\n    for row in plots:\n        pTag = row.replace('_',' ').title()\n        d = [pTag]\n        for inj in injList + ['OFFSOURCE']:\n            plot = markup.page()\n            p = \"%s/plots_clustered/GRB%s_%s_vs_snr_zoom.png\" % (inj, grbtag,\n                                                                 row)\n            plot.a(href=p, title=\"%s %s versus SNR\" % (inj, pTag))\n            plot.img(src=p)\n            plot.a.close()\n            d.append(plot())\n        td.append(d)\n\n    page = write_table(page, th, td)\n\n    return page", "response": "Writes injection chisq plots to markup. page object page"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting found and missed plots to markup. page object page", "response": "def write_found_missed(page, args, injList):\n\n    \"\"\"\n        Write injection found/missed plots to markup.page object page\n    \"\"\"\n\n    th = ['']+injList\n    td = []\n\n    #FIXME: Work out a way to make externals calls safely\n    #d = ['Number of injections']\n    #for inj in injList:\n    #    cmmnd = 'lwtprint ../*' + inj + '*MISSED*xml -t sim_inspiral | wc -l'\n    #    output,status = make_external_call(cmmnd, shell=True)\n    #    numInjs = int(output)\n    #    cmmnd = 'lwtprint ../*' + inj + '*FOUND*xml -t sim_inspiral | wc -l'\n    #    output,status = make_external_call(cmmnd, shell=True)\n    #    numInjs += int(output)\n    #    d.append(str(numInjs))\n    #td.append(d)\n\n    plots = []\n    text  = {}\n    ifos = [args.ifo_tag[i:i+2] for i in range(0, len(args.ifo_tag), 2)]\n    plots.extend(['dist', 'dist_time'])\n    text['dist'] = 'Dist vs Mchirp'\n    text['dist_time'] = 'Dist vs Time'\n    for ifo in ifos:\n        plots.extend(['effdist_%s' % ifo[0].lower(),\\\n                      'effdist_time_%s' % ifo[0].lower()])\n        text['effdist_%s' % ifo[0].lower()] = 'Eff. dist. %s vs Mchirp' % ifo\n        text['effdist_time_%s' % ifo[0].lower()] = 'Eff. dist %s vs Time' % ifo\n\n    for row in plots:\n        pTag = text[row]\n        d = [pTag]\n        for inj in injList:\n            plot = markup.page()\n            p = \"%s/efficiency_OFFTRIAL_1/found_missed_injections_%s.png\"\\\n                % (inj, row)\n            plot.a(href=p, title=pTag)\n            plot.img(src=p)\n            plot.a.close()\n            d.append(plot())\n        td.append(d)\n\n    td.append(['Close injections without FAP = 0']+\\\n              ['<a href=\"%s/efficiency_OFFTRIAL_1/quiet_found_triggers.html\"> '\n               'here</a>' % inj for inj in injList])\n\n    page = write_table(page, th, td)\n\n    return page"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite injection recovery plots to markup. page object page", "response": "def write_recovery(page, injList):\n\n    \"\"\"\n        Write injection recovery plots to markup.page object page\n    \"\"\"\n\n    th = ['']+injList\n    td = []\n\n    plots = ['sky_error_time','sky_error_mchirp','sky_error_distance']\n    text = { 'sky_error_time':'Sky error vs time',\\\n                      'sky_error_mchirp':'Sky error vs mchirp',\\\n                      'sky_error_distance':'Sky error vs distance' }\n\n    for row in plots:\n        pTag = text[row]\n        d = [pTag]\n        for inj in injList:\n            plot = markup.page()\n            plot = markup.page()\n            p = \"%s/efficiency_OFFTRIAL_1/found_%s.png\" % (inj, row)\n            plot.a(href=p, title=pTag)\n            plot.img(src=p)\n            plot.a.close()\n            d.append(plot())\n        td.append(d)\n\n    page = write_table(page, th, td)\n\n    return page"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_loudest_events(page, bins, onsource=False):\n\n    \"\"\"\n        Write injection chisq plots to markup.page object page\n    \"\"\"\n\n    th = ['']+['Mchirp %s - %s' % tuple(bin) for bin in bins]\n    td = []\n\n    plots = ['BestNR','SNR']\n\n    if onsource:\n        trial = 'ONSOURCE'\n    else:\n        trial = 'OFFTRIAL_1'\n\n    for pTag in plots:\n        row = pTag.lower()\n        d = [pTag]\n        for bin in bins:\n            b = '%s_%s' % tuple(bin)\n            plot = markup.page()\n            p = \"%s/efficiency/%s_vs_fap_%s.png\" % (trial, row, b)\n            plot.a(href=p, title=\"FAP versus %s\" % pTag)\n            plot.img(src=p)\n            plot.a.close()\n            d.append(plot())\n        td.append(d)\n\n    row = 'snruncut'\n    d = ['SNR after cuts <br> have been applied']\n    for bin in bins:\n        b = '%s_%s' % tuple(bin)\n        plot = markup.page()\n        p = \"%s/efficiency/%s_vs_fap_%s.png\" % (trial, row, b)\n        plot.a(href=p, title=\"FAP versus %s\" % pTag)\n        plot.img(src=p)\n        plot.a.close()\n        d.append(plot())\n    td.append(d)\n\n    page = write_table(page, th, td)\n\n    page.add('For more details on the loudest offsource events see')\n    page.a(href='%s/efficiency/loudest_offsource_trigs.html' % (trial))\n    page.add('here.')\n    page.a.close()\n\n\n    return page", "response": "Writes the loudest offsource events to markup. page object page"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generate_anstar_3d_lattice(maxv1, minv1, maxv2, minv2, maxv3, minv3, \\\n                               mindist):\n    \"\"\"\n    This function calls into LAL routines to generate a 3-dimensional array\n    of points using the An^* lattice.\n\n    Parameters\n    -----------\n    maxv1 : float\n        Largest value in the 1st dimension to cover\n    minv1 : float\n        Smallest value in the 1st dimension to cover\n    maxv2 : float\n        Largest value in the 2nd dimension to cover\n    minv2 : float\n        Smallest value in the 2nd dimension to cover\n    maxv3 : float\n        Largest value in the 3rd dimension to cover\n    minv3 : float\n        Smallest value in the 3rd dimension to cover\n    mindist : float\n        Maximum allowed mismatch between a point in the parameter space and the\n        generated bank of points.\n\n    Returns\n    --------\n    v1s : numpy.array\n        Array of positions in the first dimension\n    v2s : numpy.array\n        Array of positions in the second dimension\n    v3s : numpy.array\n        Array of positions in the second dimension\n    \"\"\"\n    # Lalpulsar not a requirement for the rest of pycbc, so check if we have it\n    # here in this function.\n    try:\n        import lalpulsar\n    except:\n        raise ImportError(\"A SWIG-wrapped install of lalpulsar is needed to use the anstar tiling functionality.\")\n\n    tiling = lalpulsar.CreateLatticeTiling(3)\n    lalpulsar.SetLatticeTilingConstantBound(tiling, 0, minv1, maxv1)\n    lalpulsar.SetLatticeTilingConstantBound(tiling, 1, minv2, maxv2)\n    lalpulsar.SetLatticeTilingConstantBound(tiling, 2, minv3, maxv3)\n    # Make a 3x3 Euclidean lattice\n    a = lal.gsl_matrix(3,3)\n    a.data[0,0] = 1\n    a.data[1,1] = 1\n    a.data[2,2] = 1\n    try:\n        # old versions of lalpulsar used an enumeration\n        lattice = lalpulsar.TILING_LATTICE_ANSTAR\n    except AttributeError:\n        # newer versions of lalpulsar use a string\n        lattice = 'An-star'\n    lalpulsar.SetTilingLatticeAndMetric(tiling, lattice, a, mindist)\n    try:\n        iterator = lalpulsar.CreateLatticeTilingIterator(tiling, 3)\n    except TypeError:\n        # old versions of lalpulsar required the flags argument\n        # (set to 0 for defaults)\n        iterator = lalpulsar.CreateLatticeTilingIterator(tiling, 3, 0)\n\n    vs1 = []\n    vs2 = []\n    vs3 = []\n    curr_point = lal.gsl_vector(3)\n    while (lalpulsar.NextLatticeTilingPoint(iterator, curr_point) > 0):\n        vs1.append(curr_point.data[0])\n        vs2.append(curr_point.data[1])\n        vs3.append(curr_point.data[2])\n    return vs1, vs2, vs3", "response": "This function generates a 3 - dimensional LAL lattice for a set of points."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the effective SNR statistic.", "response": "def effsnr(snr, reduced_x2, fac=250.):\n    \"\"\"Calculate the effective SNR statistic. See (S5y1 paper) for definition.\n    \"\"\"\n    snr = numpy.array(snr, ndmin=1, dtype=numpy.float64)\n    rchisq = numpy.array(reduced_x2, ndmin=1, dtype=numpy.float64)\n    esnr = snr / (1 + snr ** 2 / fac) ** 0.25 / rchisq ** 0.25\n\n    # If snr input is float, return a float. Otherwise return numpy array.\n    if hasattr(snr, '__len__'):\n        return esnr\n    else:\n        return esnr[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef newsnr(snr, reduced_x2, q=6., n=2.):\n    nsnr = numpy.array(snr, ndmin=1, dtype=numpy.float64)\n    reduced_x2 = numpy.array(reduced_x2, ndmin=1, dtype=numpy.float64)\n\n    # newsnr is only different from snr if reduced chisq > 1\n    ind = numpy.where(reduced_x2 > 1.)[0]\n    nsnr[ind] *= (0.5 * (1. + reduced_x2[ind] ** (q/n))) ** (-1./q)\n\n    # If snr input is float, return a float. Otherwise return numpy array.\n    if hasattr(snr, '__len__'):\n        return nsnr\n    else:\n        return nsnr[0]", "response": "Calculate the re - weighted SNR statistic ('newSNR') from given SNR and reduced chi - squared values."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef newsnr_sgveto(snr, bchisq, sgchisq):\n    nsnr = numpy.array(newsnr(snr, bchisq), ndmin=1)\n    sgchisq = numpy.array(sgchisq, ndmin=1)\n    t = numpy.array(sgchisq > 4, ndmin=1)\n    if len(t):\n        nsnr[t] = nsnr[t] / (sgchisq[t] / 4.0) ** 0.5\n\n    # If snr input is float, return a float. Otherwise return numpy array.\n    if hasattr(snr, '__len__'):\n        return nsnr\n    else:\n        return nsnr[0]", "response": "Combined SNR derived from NewSNR and Sine - Gaussian Chisq"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncombine SNR derived from NewSNR Sine - Gaussian Chisq and PSD variation statistic", "response": "def newsnr_sgveto_psdvar(snr, bchisq, sgchisq, psd_var_val):\n    \"\"\" Combined SNR derived from NewSNR, Sine-Gaussian Chisq and PSD\n    variation statistic \"\"\"\n    nsnr = numpy.array(newsnr_sgveto(snr, bchisq, sgchisq), ndmin=1)\n    psd_var_val = numpy.array(psd_var_val, ndmin=1)\n    lgc = psd_var_val >= 1.8\n    nsnr[lgc] = nsnr[lgc] / numpy.sqrt(psd_var_val[lgc])\n\n    # If snr input is float, return a float. Otherwise return numpy array.\n    if hasattr(snr, '__len__'):\n        return nsnr\n    else:\n        return nsnr[0]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate the newsnr for a single detector trigger", "response": "def get_newsnr(trigs):\n    \"\"\"\n    Calculate newsnr ('reweighted SNR') for a trigs object\n\n    Parameters\n    ----------\n    trigs: dict of numpy.ndarrays, h5py group (or similar dict-like object)\n        Dictionary-like object holding single detector trigger information.\n        'chisq_dof', 'snr', and 'chisq' are required keys\n\n    Returns\n    -------\n    numpy.ndarray\n        Array of newsnr values\n    \"\"\"\n    dof = 2. * trigs['chisq_dof'][:] - 2.\n    nsnr = newsnr(trigs['snr'][:], trigs['chisq'][:] / dof)\n    return numpy.array(nsnr, ndmin=1, dtype=numpy.float32)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_newsnr_sgveto(trigs):\n    dof = 2. * trigs['chisq_dof'][:] - 2.\n    nsnr_sg = newsnr_sgveto(trigs['snr'][:],\n                            trigs['chisq'][:] / dof,\n                            trigs['sg_chisq'][:])\n    return numpy.array(nsnr_sg, ndmin=1, dtype=numpy.float32)", "response": "Calculate the newsnr re - weigthed by the sine - gaussian veto\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_newsnr_sgveto_psdvar(trigs):\n    dof = 2. * trigs['chisq_dof'][:] - 2.\n    nsnr_sg_psd = \\\n                 newsnr_sgveto_psdvar(trigs['snr'][:], trigs['chisq'][:] / dof,\n                                      trigs['sg_chisq'][:],\n                                      trigs['psd_var_val'][:])\n    return numpy.array(nsnr_sg_psd, ndmin=1, dtype=numpy.float32)", "response": "Calculates the newsnr re - weighted by the sine - gaussian veto and psd variation and statistic of the single detector."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndrops trailing zeros in a float that is printed.", "response": "def drop_trailing_zeros(num):\n    \"\"\"\n    Drops the trailing zeros in a float that is printed.\n    \"\"\"\n    txt = '%f' %(num)\n    txt = txt.rstrip('0')\n    if txt.endswith('.'):\n        txt = txt[:-1]\n    return txt"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_signum(val, err, max_sig=numpy.inf):\n    coeff, pwr = ('%e' % err).split('e')\n    if pwr.startswith('-'):\n        pwr = int(pwr[1:])\n        if round(float(coeff)) == 10.:\n            pwr -= 1\n        pwr = min(pwr, max_sig)\n        tmplt = '%.' + str(pwr+1) + 'f'\n        return tmplt % val\n    else:\n        pwr = int(pwr[1:])\n        if round(float(coeff)) == 10.:\n            pwr += 1\n        # if the error is large, we can sometimes get 0;\n        # adjust the round until we don't get 0 (assuming the actual\n        # value isn't 0)\n        return_val = round(val, -pwr+1)\n        if val != 0.:\n            loop_count = 0\n            max_recursion = 100\n            while return_val == 0.:\n                pwr -= 1\n                return_val = round(val, -pwr+1)\n                loop_count += 1\n                if loop_count > max_recursion:\n                    raise ValueError(\"Maximum recursion depth hit! Input \" +\\\n                        \"values are: val = %f, err = %f\" %(val, err))\n        return drop_trailing_zeros(return_val)", "response": "Given an error returns a string for val formated to the appropriate\nInsights number of significant figures."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nformatting a value into LaTeX string.", "response": "def format_value(value, error, plus_error=None, use_scientific_notation=3,\n        include_error=True, use_relative_error=False, ndecs=None):\n    \"\"\"Given a numerical value and some bound on it, formats the number into a\n    string such that the value is rounded to the nearest significant figure,\n    which is determined by the error = abs(value-bound).\n\n    Note: if either use_scientific_notation or include_error are True, the\n    returned string will include LaTeX characters.\n\n    Parameters\n    ----------\n    value : float\n        The value to format.\n    error : float\n        The uncertainty in the value. This is used to determine the\n        number of significant figures to print. If the value has no\n        uncertainty, you can just do value*1e-k, where k+1 is the number\n        of significant figures you want.\n    plus_error : {None, float}\n        The upper uncertainty on the value; i.e., what you need to add to the\n        value to get its upper bound. If provided, ``error`` is assumed to be\n        the negative; i.e., value +plus_error -error. The number of\n        significant figures printed is determined from min(error,\n        plus_error).\n    use_scientific_notation : int, optional\n        If ``abs(log10(value))`` is greater than the given, the return string\n        will be formated to \"\\%.1f \\\\times 10^{p}\", where p is the powers of 10\n        needed for the leading number in the value to be in the singles spot.\n        Otherwise will return \"\\%.(p+1)f\". Default is 3.  To turn off, set to\n        ``numpy.inf``. Note: using scientific notation assumes that the\n        returned value will be enclosed in LaTeX math mode.\n    include_error : {True, bool}\n        Include the error in the return string; the output will be formated\n        val \\\\pm err, where err is the error rounded to the same\n        power of 10 as val. Otherwise, just the formatted value will\n        be returned. If plus_error is provided then the return text will be\n        formatted as ``val^{+plus_error}_{-error}``.\n    use_relative_error : {False, bool}\n        If include_error, the error will be formatted as a percentage of the\n        the value.\n    ndecs: {None, int}\n        Number of values after the decimal point. If not provided,\n        it will default to the number of values in the error.\n\n    Returns\n    -------\n    string\n        The value (and error, if include_error is True) formatted as a string.\n\n\n    Examples\n    --------\n    Given a value and its uncertainty:\n\n    >>> val, err\n    (3.9278372067613837e-22, 2.2351435286500487e-23)\n\n    Format with error quoted:\n\n    >>> format_value(val, err)\n    '3.93 \\\\pm 0.22\\\\times 10^{-22}'\n\n    Quote error as a relative error:\n\n    >>> format_value(val, err, use_relative_error=True)\n    '3.93 \\\\times 10^{-22} \\\\pm5.6\\\\%'\n\n    Format without the error and without scientific notation:\n\n    >>> format_value(val, err, use_scientific_notation=float('inf'),\n                     include_error=False)\n    '0.000000000000000000000393'\n\n    Given an plus error:\n\n    >>> err_plus\n    8.2700310560051804e-24\n\n    Format with both bounds quoted:\n\n    >>> format_value(val, err, plus_error=err_plus)\n    '3.928^{+0.083}_{-0.224}\\\\times 10^{-22}'\n\n    Format with both bounds quoted as a relative error:\n\n    >>> format_value(val, err, plus_error=err_plus, use_relative_error=True)\n    '3.928\\\\times 10^{-22}\\\\,^{+2.1\\\\%}_{-5.7\\\\%}'\n\n    \"\"\"\n    minus_sign = '-' if value < 0. else ''\n    value = abs(value)\n    minus_err = abs(error)\n    if plus_error is None:\n        plus_err = minus_err\n    else:\n        plus_err = abs(plus_error)\n    error = min(minus_err, plus_err)\n    if value == 0. or abs(numpy.log10(value)) < use_scientific_notation:\n        conversion_factor = 0.\n    else:\n        conversion_factor = numpy.floor(numpy.log10(value))\n    value = value * 10**(-conversion_factor)\n    error = error * 10**(-conversion_factor)\n    if conversion_factor == 0.:\n        powfactor = ''\n    elif conversion_factor == 1.:\n        powfactor = r'\\times 10'\n    else:\n        powfactor = r'\\times 10^{%i}' %(int(conversion_factor))\n\n    if ndecs is not None:\n        decs = value * 10**(-ndecs)\n    else:\n        decs = error\n    # now round the the appropriate number of sig figs\n    valtxt = get_signum(value, decs)\n    valtxt = '{}{}'.format(minus_sign, valtxt)\n\n    if include_error:\n        if plus_error is None:\n            errtxt = get_signum(error, error)\n            if use_relative_error and float(valtxt) != 0.:\n                relative_err = 100.*float(errtxt)/float(valtxt)\n                # we round the relative error to the nearest 1% using\n                # get_signum; Note that if the relative error is < 1%,\n                # get_signum will automatically increase the number of values\n                # after the decimal until it gets to the first non-zero value\n                relative_err = get_signum(relative_err, 1.)\n                txt = r'%s %s \\pm%s\\%%' %(valtxt, powfactor, relative_err)\n            else:\n                txt = r'%s \\pm %s%s' %(valtxt, errtxt, powfactor)\n        else:\n            plus_err = plus_err * 10**(-conversion_factor)\n            minus_err = minus_err * 10**(-conversion_factor)\n            minus_err_txt = get_signum(minus_err, decs)\n            plus_err_txt = get_signum(plus_err, decs)\n            if use_relative_error and float(valtxt) != 0.:\n                # same as above, but with plus and minus\n                rel_plus_err = get_signum(\n                    100.*float(plus_err_txt)/float(valtxt), 1.)\n                rel_minus_err = get_signum(\n                    100.*float(minus_err_txt)/float(valtxt), 1.)\n                txt = r'%s%s\\,^{+%s\\%%}_{-%s\\%%}' %(valtxt, powfactor,\n                    rel_plus_err, rel_minus_err)\n            else:\n                txt = r'%s^{+%s}_{-%s}%s' %(valtxt, plus_err_txt,\n                    minus_err_txt, powfactor)\n    else:\n        txt = r'%s%s' %(valtxt, powfactor)\n    return txt"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_cli(opt, dyn_range_fac=1, precision='single',\n             inj_filter_rejector=None):\n    \"\"\"Parses the CLI options related to strain data reading and conditioning.\n\n    Parameters\n    ----------\n    opt : object\n        Result of parsing the CLI with OptionParser, or any object with the\n        required attributes  (gps-start-time, gps-end-time, strain-high-pass,\n        pad-data, sample-rate, (frame-cache or frame-files), channel-name,\n        fake-strain, fake-strain-seed, fake-strain-from-file, gating_file).\n    dyn_range_fac : {float, 1}, optional\n        A large constant to reduce the dynamic range of the strain.\n    precision : string\n        Precision of the returned strain ('single' or 'double').\n    inj_filter_rejector : InjFilterRejector instance; optional, default=None\n        If given send the InjFilterRejector instance to the inject module so\n        that it can store a reduced representation of injections if\n        necessary.\n\n    Returns\n    -------\n    strain : TimeSeries\n        The time series containing the conditioned strain data.\n    \"\"\"\n    gating_info = {}\n\n    if opt.frame_cache or opt.frame_files or opt.frame_type:\n        if opt.frame_cache:\n            frame_source = opt.frame_cache\n        if opt.frame_files:\n            frame_source = opt.frame_files\n\n        logging.info(\"Reading Frames\")\n\n        if hasattr(opt, 'frame_sieve') and opt.frame_sieve:\n            sieve = opt.frame_sieve\n        else:\n            sieve = None\n\n        if opt.frame_type:\n            strain = pycbc.frame.query_and_read_frame(\n                    opt.frame_type, opt.channel_name,\n                    start_time=opt.gps_start_time-opt.pad_data,\n                    end_time=opt.gps_end_time+opt.pad_data,\n                    sieve=sieve)\n        else:\n            strain = pycbc.frame.read_frame(\n                    frame_source, opt.channel_name,\n                    start_time=opt.gps_start_time-opt.pad_data,\n                    end_time=opt.gps_end_time+opt.pad_data,\n                    sieve=sieve)\n\n        if opt.zpk_z and opt.zpk_p and opt.zpk_k:\n            logging.info(\"Highpass Filtering\")\n            strain = highpass(strain, frequency=opt.strain_high_pass)\n\n            logging.info(\"Applying zpk filter\")\n            z = numpy.array(opt.zpk_z)\n            p = numpy.array(opt.zpk_p)\n            k = float(opt.zpk_k)\n            strain = filter_zpk(strain.astype(numpy.float64), z, p, k)\n\n        if opt.normalize_strain:\n            logging.info(\"Dividing strain by constant\")\n            l = opt.normalize_strain\n            strain = strain / l\n\n        if opt.injection_file:\n            logging.info(\"Applying injections\")\n            injector = InjectionSet(opt.injection_file)\n            injections = \\\n                injector.apply(strain, opt.channel_name[0:2],\n                               distance_scale=opt.injection_scale_factor,\n                               inj_filter_rejector=inj_filter_rejector)\n\n        if opt.sgburst_injection_file:\n            logging.info(\"Applying sine-Gaussian burst injections\")\n            injector = SGBurstInjectionSet(opt.sgburst_injection_file)\n            injector.apply(strain, opt.channel_name[0:2],\n                             distance_scale=opt.injection_scale_factor)\n\n        logging.info(\"Highpass Filtering\")\n        strain = highpass(strain, frequency=opt.strain_high_pass)\n\n        if precision == 'single':\n            logging.info(\"Converting to float32\")\n            strain = (strain * dyn_range_fac).astype(pycbc.types.float32)\n        elif precision == \"double\":\n            logging.info(\"Converting to float64\")\n            strain = (strain * dyn_range_fac).astype(pycbc.types.float64)\n        else:\n            raise ValueError(\"Unrecognized precision {}\".format(precision))\n\n        if opt.gating_file is not None:\n            logging.info(\"Gating times contained in gating file\")\n            gate_params = numpy.loadtxt(opt.gating_file)\n            if len(gate_params.shape) == 1:\n                gate_params = [gate_params]\n            strain = gate_data(strain, gate_params)\n            gating_info['file'] = \\\n                    [gp for gp in gate_params \\\n                     if (gp[0] + gp[1] + gp[2] >= strain.start_time) \\\n                     and (gp[0] - gp[1] - gp[2] <= strain.end_time)]\n\n        if opt.autogating_threshold is not None:\n            # the + 0 is for making a copy\n            glitch_times = detect_loud_glitches(\n                    strain + 0., threshold=opt.autogating_threshold,\n                    cluster_window=opt.autogating_cluster,\n                    low_freq_cutoff=opt.strain_high_pass,\n                    high_freq_cutoff=opt.sample_rate/2,\n                    corrupt_time=opt.pad_data+opt.autogating_pad)\n            gate_params = [[gt, opt.autogating_width, opt.autogating_taper] \\\n                           for gt in glitch_times]\n            if len(glitch_times) > 0:\n                logging.info('Autogating at %s',\n                             ', '.join(['%.3f' % gt for gt in glitch_times]))\n            strain = gate_data(strain, gate_params)\n            gating_info['auto'] = gate_params\n\n        logging.info(\"Resampling data\")\n        strain = resample_to_delta_t(strain, 1.0/opt.sample_rate, method='ldas')\n\n        logging.info(\"Highpass Filtering\")\n        strain = highpass(strain, frequency=opt.strain_high_pass)\n\n        if hasattr(opt, 'witness_frame_type') and opt.witness_frame_type:\n            stilde = strain.to_frequencyseries()\n            import h5py\n            tf_file = h5py.File(opt.witness_tf_file)\n            for key in tf_file:\n                witness = pycbc.frame.query_and_read_frame(opt.witness_frame_type, str(key),\n                       start_time=strain.start_time, end_time=strain.end_time)\n                witness = (witness * dyn_range_fac).astype(strain.dtype)\n                tf = pycbc.types.load_frequencyseries(opt.witness_tf_file, group=key)\n                tf = tf.astype(stilde.dtype)\n\n                flen = int(opt.witness_filter_length * strain.sample_rate)\n                tf = pycbc.psd.interpolate(tf, stilde.delta_f)\n\n                tf_time = tf.to_timeseries()\n                window = Array(numpy.hanning(flen*2), dtype=strain.dtype)\n                tf_time[0:flen] *= window[flen:]\n                tf_time[len(tf_time)-flen:] *= window[0:flen]\n                tf = tf_time.to_frequencyseries()\n\n                kmax = min(len(tf), len(stilde)-1)\n                stilde[:kmax] -= tf[:kmax] * witness.to_frequencyseries()[:kmax]\n\n            strain = stilde.to_timeseries()\n\n\n        logging.info(\"Remove Padding\")\n        start = opt.pad_data*opt.sample_rate\n        end = len(strain)-opt.sample_rate*opt.pad_data\n        strain = strain[start:end]\n\n    if opt.fake_strain or opt.fake_strain_from_file:\n        logging.info(\"Generating Fake Strain\")\n        if not opt.low_frequency_cutoff:\n            raise ValueError('Please provide low frequency cutoff to '\n                             'generate a fake strain')\n        duration = opt.gps_end_time - opt.gps_start_time\n        tlen = duration * opt.sample_rate\n        pdf = 1.0/128\n        plen = int(opt.sample_rate / pdf) / 2 + 1\n\n        if opt.fake_strain_from_file:\n            logging.info(\"Reading ASD from file\")\n            strain_psd = pycbc.psd.from_txt(opt.fake_strain_from_file, plen, pdf,\n                                            opt.low_frequency_cutoff, is_asd_file=True)\n        elif opt.fake_strain != 'zeroNoise':\n            logging.info(\"Making PSD for strain\")\n            strain_psd = pycbc.psd.from_string(opt.fake_strain, plen, pdf,\n                                               opt.low_frequency_cutoff)\n\n        if opt.fake_strain == 'zeroNoise':\n            logging.info(\"Making zero-noise time series\")\n            strain = TimeSeries(pycbc.types.zeros(tlen),\n                                delta_t=1.0/opt.sample_rate,\n                                epoch=opt.gps_start_time)\n        else:\n            logging.info(\"Making colored noise\")\n            from pycbc.noise.reproduceable import colored_noise\n            lowfreq = opt.low_frequency_cutoff / 2.\n            strain = colored_noise(strain_psd, opt.gps_start_time,\n                                          opt.gps_end_time,\n                                          seed=opt.fake_strain_seed,\n                                          low_frequency_cutoff=lowfreq)\n            strain = resample_to_delta_t(strain, 1.0/opt.sample_rate)\n\n        if not opt.channel_name and (opt.injection_file \\\n                                     or opt.sgburst_injection_file):\n            raise ValueError('Please provide channel names with the format '\n                             'ifo:channel (e.g. H1:CALIB-STRAIN) to inject '\n                             'simulated signals into fake strain')\n\n        if opt.injection_file:\n            logging.info(\"Applying injections\")\n            injector = InjectionSet(opt.injection_file)\n            injections = \\\n                injector.apply(strain, opt.channel_name[0:2],\n                               distance_scale=opt.injection_scale_factor,\n                               inj_filter_rejector=inj_filter_rejector)\n\n        if opt.sgburst_injection_file:\n            logging.info(\"Applying sine-Gaussian burst injections\")\n            injector =  SGBurstInjectionSet(opt.sgburst_injection_file)\n            injector.apply(strain, opt.channel_name[0:2],\n                             distance_scale=opt.injection_scale_factor)\n\n        if precision == 'single':\n            logging.info(\"Converting to float32\")\n            strain = (dyn_range_fac * strain).astype(pycbc.types.float32)\n        elif precision == 'double':\n            logging.info(\"Converting to float64\")\n            strain = (dyn_range_fac * strain).astype(pycbc.types.float64)\n        else:\n            raise ValueError(\"Unrecognized precision {}\".format(precision))\n\n    if opt.taper_data:\n        logging.info(\"Tapering data\")\n        # Use auto-gating stuff for this, a one-sided gate is a taper\n        pd_taper_window = opt.taper_data\n        gate_params = [(strain.start_time, 0., pd_taper_window)]\n        gate_params.append( (strain.end_time, 0.,\n                             pd_taper_window) )\n        gate_data(strain, gate_params)\n\n    if opt.injection_file:\n        strain.injections = injections\n    strain.gating_info = gating_info\n\n    return strain", "response": "Parses the CLI options related to strain data reading and conditioning."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_cli_single_ifo(opt, ifo, **kwargs):\n    single_det_opt = copy_opts_for_single_ifo(opt, ifo)\n    return from_cli(single_det_opt, **kwargs)", "response": "Get the strain for a single ifo when using the multi - detector CLI"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the strain for all ifos when using the multi - detector CLI", "response": "def from_cli_multi_ifos(opt, ifos, **kwargs):\n    \"\"\"\n    Get the strain for all ifos when using the multi-detector CLI\n    \"\"\"\n    strain = {}\n    for ifo in ifos:\n        strain[ifo] = from_cli_single_ifo(opt, ifo, **kwargs)\n    return strain"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef insert_strain_option_group(parser, gps_times=True):\n\n    data_reading_group = parser.add_argument_group(\"Options for obtaining h(t)\",\n                  \"These options are used for generating h(t) either by \"\n                  \"reading from a file or by generating it. This is only \"\n                  \"needed if the PSD is to be estimated from the data, ie. \"\n                  \" if the --psd-estimation option is given.\")\n\n    # Required options\n\n    if gps_times:\n        data_reading_group.add_argument(\"--gps-start-time\",\n                                help=\"The gps start time of the data \"\n                                     \"(integer seconds)\", type=int)\n        data_reading_group.add_argument(\"--gps-end-time\",\n                                help=\"The gps end time of the data \"\n                                     \" (integer seconds)\", type=int)\n\n\n    data_reading_group.add_argument(\"--strain-high-pass\", type=float,\n                            help=\"High pass frequency\")\n    data_reading_group.add_argument(\"--pad-data\",\n              help=\"Extra padding to remove highpass corruption \"\n                   \"(integer seconds)\", type=int)\n    data_reading_group.add_argument(\"--taper-data\",\n              help=\"Taper ends of data to zero using the supplied length as a \"\n                   \"window (integer seconds)\", type=int, default=0)\n    data_reading_group.add_argument(\"--sample-rate\", type=int,\n                            help=\"The sample rate to use for h(t) generation (integer Hz).\")\n    data_reading_group.add_argument(\"--channel-name\", type=str,\n                   help=\"The channel containing the gravitational strain data\")\n\n    #Read from cache file\n    data_reading_group.add_argument(\"--frame-cache\", type=str, nargs=\"+\",\n                            help=\"Cache file containing the frame locations.\")\n\n    #Read from frame files\n    data_reading_group.add_argument(\"--frame-files\",\n                            type=str, nargs=\"+\",\n                            help=\"list of frame files\")\n\n    #Use datafind to get frame files\n    data_reading_group.add_argument(\"--frame-type\",\n                            type=str,\n                            help=\"(optional), replaces frame-files. Use datafind \"\n                                 \"to get the needed frame file(s) of this type.\")\n\n    #Filter frame files by URL\n    data_reading_group.add_argument(\"--frame-sieve\",\n                            type=str,\n                            help=\"(optional), Only use frame files where the \"\n                                 \"URL matches the regular expression given.\")\n\n    #Generate gaussian noise with given psd\n    data_reading_group.add_argument(\"--fake-strain\",\n                help=\"Name of model PSD for generating fake gaussian noise.\",\n                     choices=pycbc.psd.get_lalsim_psd_list() + ['zeroNoise'])\n    data_reading_group.add_argument(\"--fake-strain-seed\", type=int, default=0,\n                help=\"Seed value for the generation of fake colored\"\n                     \" gaussian noise\")\n    data_reading_group.add_argument(\"--fake-strain-from-file\",\n                help=\"File containing ASD for generating fake noise from it.\")\n\n    #optional\n    data_reading_group.add_argument(\"--injection-file\", type=str,\n                      help=\"(optional) Injection file used to add \"\n                           \"waveforms into the strain\")\n\n    data_reading_group.add_argument(\"--sgburst-injection-file\", type=str,\n                      help=\"(optional) Injection file used to add \"\n                      \"sine-Gaussian burst waveforms into the strain\")\n\n    data_reading_group.add_argument(\"--injection-scale-factor\", type=float,\n                    default=1, help=\"Divide injections by this factor \"\n                    \"before injecting into the data.\")\n\n    data_reading_group.add_argument(\"--gating-file\", type=str,\n                    help=\"(optional) Text file of gating segments to apply.\"\n                        \" Format of each line is (all times in secs):\"\n                        \"  gps_time zeros_half_width pad_half_width\")\n\n    data_reading_group.add_argument('--autogating-threshold', type=float,\n                                    metavar='SIGMA',\n                                    help='If given, find and gate glitches '\n                                         'producing a deviation larger than '\n                                         'SIGMA in the whitened strain time '\n                                         'series.')\n    data_reading_group.add_argument('--autogating-cluster', type=float,\n                                    metavar='SECONDS', default=5.,\n                                    help='Length of clustering window for '\n                                         'detecting glitches for autogating.')\n    data_reading_group.add_argument('--autogating-width', type=float,\n                                    metavar='SECONDS', default=0.25,\n                                    help='Half-width of the gating window.')\n    data_reading_group.add_argument('--autogating-taper', type=float,\n                                    metavar='SECONDS', default=0.25,\n                                    help='Taper the strain before and after '\n                                         'each gating window over a duration '\n                                         'of SECONDS.')\n    data_reading_group.add_argument('--autogating-pad', type=float,\n                                    metavar='SECONDS', default=16,\n                                    help='Ignore the given length of whitened '\n                                         'strain at the ends of a segment, to '\n                                         'avoid filters ringing.')\n\n    data_reading_group.add_argument(\"--normalize-strain\", type=float,\n                    help=\"(optional) Divide frame data by constant.\")\n\n    data_reading_group.add_argument(\"--zpk-z\", type=float, nargs=\"+\",\n                    help=\"(optional) Zero-pole-gain (zpk) filter strain. \"\n                        \"A list of zeros for transfer function\")\n\n    data_reading_group.add_argument(\"--zpk-p\", type=float, nargs=\"+\",\n                    help=\"(optional) Zero-pole-gain (zpk) filter strain. \"\n                        \"A list of poles for transfer function\")\n\n    data_reading_group.add_argument(\"--zpk-k\", type=float,\n                    help=\"(optional) Zero-pole-gain (zpk) filter strain. \"\n                        \"Transfer function gain\")\n\n    # Options to apply to subtract noise from a witness channel and known\n    # transfer function.\n    data_reading_group.add_argument(\"--witness-frame-type\", type=str,\n                    help=\"(optional), frame type which will be use to query the\"\n                         \"witness channel data.\")\n    data_reading_group.add_argument(\"--witness-tf-file\", type=str,\n                    help=\"an hdf file containing the transfer\"\n                         \"  functions and the associated channel names\")\n    data_reading_group.add_argument(\"--witness-filter-length\", type=float,\n                    help=\"filter length in seconds for the transfer function\")\n\n    return data_reading_group", "response": "Adds strain - related options to the optparser object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding the options used to call pycbc. strain. from_cli function to an option group that can be used to generate a single strain from multiple ifos.", "response": "def insert_strain_option_group_multi_ifo(parser):\n    \"\"\"\n    Adds the options used to call the pycbc.strain.from_cli function to an\n    optparser as an OptionGroup. This should be used if you\n    want to use these options in your code.\n\n    Parameters\n    -----------\n    parser : object\n        OptionParser instance.\n    \"\"\"\n\n    data_reading_group_multi = parser.add_argument_group(\"Options for obtaining\"\n                  \" h(t)\",\n                  \"These options are used for generating h(t) either by \"\n                  \"reading from a file or by generating it. This is only \"\n                  \"needed if the PSD is to be estimated from the data, ie. \"\n                  \"if the --psd-estimation option is given. This group \"\n                  \"supports reading from multiple ifos simultaneously.\")\n\n    # Required options\n    data_reading_group_multi.add_argument(\"--gps-start-time\", nargs='+',\n                            action=MultiDetOptionAction, metavar='IFO:TIME',\n                            help=\"The gps start time of the data \"\n                                 \"(integer seconds)\", type=int)\n    data_reading_group_multi.add_argument(\"--gps-end-time\", nargs='+', type=int,\n                            action=MultiDetOptionAction, metavar='IFO:TIME',\n                            help=\"The gps end time of the data \"\n                                 \"(integer seconds)\")\n    data_reading_group_multi.add_argument(\"--strain-high-pass\", nargs='+',\n                            action=MultiDetOptionAction,\n                            type=float, metavar='IFO:FREQUENCY',\n                            help=\"High pass frequency\")\n    data_reading_group_multi.add_argument(\"--pad-data\", nargs='+',\n                            action=MultiDetOptionAction,\n                            type=int, metavar='IFO:LENGTH',\n                            help=\"Extra padding to remove highpass corruption \"\n                                \"(integer seconds)\")\n    data_reading_group_multi.add_argument(\"--taper-data\", nargs='+',\n                            action=MultiDetOptionAction,\n                            type=int, default=0, metavar='IFO:LENGTH',\n                            help=\"Taper ends of data to zero using the \"\n                                \"supplied length as a window (integer seconds)\")\n    data_reading_group_multi.add_argument(\"--sample-rate\", type=int, nargs='+',\n                            action=MultiDetOptionAction, metavar='IFO:RATE',\n                            help=\"The sample rate to use for h(t) generation \"\n                                \" (integer Hz).\")\n    data_reading_group_multi.add_argument(\"--channel-name\", type=str, nargs='+',\n                            action=MultiDetOptionActionSpecial,\n                            metavar='IFO:CHANNEL',\n                            help=\"The channel containing the gravitational \"\n                                \"strain data\")\n\n    #Read from cache file\n    data_reading_group_multi.add_argument(\"--frame-cache\", type=str, nargs=\"+\",\n                            action=MultiDetOptionAppendAction,\n                            metavar='IFO:FRAME_CACHE',\n                            help=\"Cache file containing the frame locations.\")\n\n    #Read from frame files\n    data_reading_group_multi.add_argument(\"--frame-files\", type=str, nargs=\"+\",\n                            action=MultiDetOptionAppendAction,\n                            metavar='IFO:FRAME_FILES',\n                            help=\"list of frame files\")\n\n    # Use datafind to get frame files\n    data_reading_group_multi.add_argument(\"--frame-type\", type=str, nargs=\"+\",\n                                    action=MultiDetOptionAction,\n                                    metavar='IFO:FRAME_TYPE',\n                                    help=\"(optional) Replaces frame-files. \"\n                                         \"Use datafind to get the needed frame \"\n                                         \"file(s) of this type.\")\n\n    #Filter frame files by URL\n    data_reading_group_multi.add_argument(\"--frame-sieve\", type=str, nargs=\"+\",\n                            action=MultiDetOptionAction,\n                            metavar='IFO:FRAME_SIEVE',\n                            help=\"(optional), Only use frame files where the \"\n                                 \"URL matches the regular expression given.\")\n\n    #Generate gaussian noise with given psd\n    data_reading_group_multi.add_argument(\"--fake-strain\", type=str, nargs=\"+\",\n                            action=MultiDetOptionAction, metavar='IFO:CHOICE',\n                            help=\"Name of model PSD for generating fake \"\n                            \"gaussian noise. Choose from %s or zeroNoise\" \\\n                            %((', ').join(pycbc.psd.get_lalsim_psd_list()),) )\n    data_reading_group_multi.add_argument(\"--fake-strain-seed\", type=int,\n                            default=0, nargs=\"+\", action=MultiDetOptionAction,\n                            metavar='IFO:SEED',\n                            help=\"Seed value for the generation of fake \"\n                            \"colored gaussian noise\")\n    data_reading_group_multi.add_argument(\"--fake-strain-from-file\", nargs=\"+\",\n                            action=MultiDetOptionAction, metavar='IFO:FILE',\n                            help=\"File containing ASD for generating fake \"\n                            \"noise from it.\")\n\n    #optional\n    data_reading_group_multi.add_argument(\"--injection-file\", type=str,\n                            nargs=\"+\", action=MultiDetOptionAction,\n                            metavar='IFO:FILE',\n                            help=\"(optional) Injection file used to add \"\n                            \"waveforms into the strain\")\n\n    data_reading_group_multi.add_argument(\"--sgburst-injection-file\", type=str,\n                      nargs=\"+\", action=MultiDetOptionAction,\n                      metavar='IFO:FILE',\n                      help=\"(optional) Injection file used to add \"\n                      \"sine-Gaussian burst waveforms into the strain\")\n\n    data_reading_group_multi.add_argument(\"--injection-scale-factor\",\n                    type=float, nargs=\"+\", action=MultiDetOptionAction,\n                    metavar=\"IFO:VAL\", default=1.,\n                    help=\"Multiple injections by this factor \"\n                         \"before injecting into the data.\")\n\n    data_reading_group_multi.add_argument(\"--gating-file\", type=str,\n                      nargs=\"+\", action=MultiDetOptionAction,\n                      metavar='IFO:FILE',\n                      help=\"(optional) Text file of gating segments to apply.\"\n                          \" Format of each line is (all times in secs):\"\n                          \"  gps_time zeros_half_width pad_half_width\")\n\n    data_reading_group_multi.add_argument('--autogating-threshold', type=float,\n                                    nargs=\"+\", action=MultiDetOptionAction,\n                                    metavar='IFO:SIGMA',\n                                    help='If given, find and gate glitches '\n                                         'producing a deviation larger than '\n                                         'SIGMA in the whitened strain time '\n                                         'series.')\n    data_reading_group_multi.add_argument('--autogating-cluster', type=float,\n                                    nargs=\"+\", action=MultiDetOptionAction,\n                                    metavar='IFO:SECONDS', default=5.,\n                                    help='Length of clustering window for '\n                                         'detecting glitches for autogating.')\n    data_reading_group_multi.add_argument('--autogating-width', type=float,\n                                    nargs=\"+\", action=MultiDetOptionAction,\n                                    metavar='IFO:SECONDS', default=0.25,\n                                    help='Half-width of the gating window.')\n    data_reading_group_multi.add_argument('--autogating-taper', type=float,\n                                    nargs=\"+\", action=MultiDetOptionAction,\n                                    metavar='IFO:SECONDS', default=0.25,\n                                    help='Taper the strain before and after '\n                                         'each gating window over a duration '\n                                         'of SECONDS.')\n    data_reading_group_multi.add_argument('--autogating-pad', type=float,\n                                    nargs=\"+\", action=MultiDetOptionAction,\n                                    metavar='IFO:SECONDS', default=16,\n                                    help='Ignore the given length of whitened '\n                                         'strain at the ends of a segment, to '\n                                         'avoid filters ringing.')\n\n    data_reading_group_multi.add_argument(\"--normalize-strain\", type=float,\n                     nargs=\"+\", action=MultiDetOptionAction,\n                     metavar='IFO:VALUE',\n                     help=\"(optional) Divide frame data by constant.\")\n\n    data_reading_group_multi.add_argument(\"--zpk-z\", type=float,\n                     nargs=\"+\", action=MultiDetOptionAppendAction,\n                     metavar='IFO:VALUE',\n                     help=\"(optional) Zero-pole-gain (zpk) filter strain. \"\n                         \"A list of zeros for transfer function\")\n\n    data_reading_group_multi.add_argument(\"--zpk-p\", type=float,\n                     nargs=\"+\", action=MultiDetOptionAppendAction,\n                     metavar='IFO:VALUE',\n                     help=\"(optional) Zero-pole-gain (zpk) filter strain. \"\n                         \"A list of poles for transfer function\")\n\n    data_reading_group_multi.add_argument(\"--zpk-k\", type=float,\n                     nargs=\"+\", action=MultiDetOptionAppendAction,\n                     metavar='IFO:VALUE',\n                     help=\"(optional) Zero-pole-gain (zpk) filter strain. \"\n                         \"Transfer function gain\")\n\n    return data_reading_group_multi"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef verify_strain_options(opts, parser):\n    for opt_group in ensure_one_opt_groups:\n        ensure_one_opt(opts, parser, opt_group)\n    required_opts(opts, parser, required_opts_list)", "response": "Sanity check provided strain data CLI options."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef gate_data(data, gate_params):\n    def inverted_tukey(M, n_pad):\n        midlen = M - 2*n_pad\n        if midlen < 0:\n            raise ValueError(\"No zeros left after applying padding.\")\n        padarr = 0.5*(1.+numpy.cos(numpy.pi*numpy.arange(n_pad)/n_pad))\n        return numpy.concatenate((padarr,numpy.zeros(midlen),padarr[::-1]))\n\n    sample_rate = 1./data.delta_t\n    temp = data.data\n\n    for glitch_time, glitch_width, pad_width in gate_params:\n        t_start = glitch_time - glitch_width - pad_width - data.start_time\n        t_end = glitch_time + glitch_width + pad_width - data.start_time\n        if t_start > data.duration or t_end < 0.:\n            continue # Skip gate segments that don't overlap\n        win_samples = int(2*sample_rate*(glitch_width+pad_width))\n        pad_samples = int(sample_rate*pad_width)\n        window = inverted_tukey(win_samples, pad_samples)\n        offset = int(t_start * sample_rate)\n        idx1 = max(0, -offset)\n        idx2 = min(len(window), len(data)-offset)\n        temp[idx1+offset:idx2+offset] *= window[idx1:idx2]\n\n    return data", "response": "Apply a set of gating windows to a time series."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of the FFT'd segments.", "response": "def fourier_segments(self):\n        \"\"\" Return a list of the FFT'd segments.\n        Return the list of FrequencySeries. Additional properties are\n        added that describe the strain segment. The property 'analyze'\n        is a slice corresponding to the portion of the time domain equivelant\n        of the segment to analyze for triggers. The value 'cumulative_index'\n        indexes from the beginning of the original strain series.\n        \"\"\"\n        if not self._fourier_segments:\n            self._fourier_segments = []\n            for seg_slice, ana in zip(self.segment_slices, self.analyze_slices):\n                if seg_slice.start >= 0 and seg_slice.stop <= len(self.strain):\n                    freq_seg = make_frequency_series(self.strain[seg_slice])\n                # Assume that we cannot have a case where we both zero-pad on\n                # both sides\n                elif seg_slice.start < 0:\n                    strain_chunk = self.strain[:seg_slice.stop]\n                    strain_chunk.prepend_zeros(-seg_slice.start)\n                    freq_seg = make_frequency_series(strain_chunk)\n                elif seg_slice.stop > len(self.strain):\n                    strain_chunk = self.strain[seg_slice.start:]\n                    strain_chunk.append_zeros(seg_slice.stop - len(self.strain))\n                    freq_seg = make_frequency_series(strain_chunk)\n                freq_seg.analyze = ana\n                freq_seg.cumulative_index = seg_slice.start + ana.start\n                freq_seg.seg_slice = seg_slice\n                self._fourier_segments.append(freq_seg)\n\n        return self._fourier_segments"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_cli(cls, opt, strain):\n        return cls(strain, segment_length=opt.segment_length,\n                   segment_start_pad=opt.segment_start_pad,\n                   segment_end_pad=opt.segment_end_pad,\n                   trigger_start=opt.trig_start_time,\n                   trigger_end=opt.trig_end_time,\n                   filter_inj_only=opt.filter_inj_only,\n                   injection_window=opt.injection_window,\n                   allow_zero_padding=opt.allow_zero_padding)", "response": "Calculate the segmentation of the strain data for analysis from the command line options."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the segmentation of the strain data for analysis from the command line options.", "response": "def from_cli_single_ifo(cls, opt, strain, ifo):\n        \"\"\"Calculate the segmentation of the strain data for analysis from\n        the command line options.\n        \"\"\"\n        return cls(strain, segment_length=opt.segment_length[ifo],\n                   segment_start_pad=opt.segment_start_pad[ifo],\n                   segment_end_pad=opt.segment_end_pad[ifo],\n                   trigger_start=opt.trig_start_time[ifo],\n                   trigger_end=opt.trig_end_time[ifo],\n                   filter_inj_only=opt.filter_inj_only,\n                   allow_zero_padding=opt.allow_zero_padding)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate the segmentation of the strain data for analysis from the command line options.", "response": "def from_cli_multi_ifos(cls, opt, strain_dict, ifos):\n        \"\"\"Calculate the segmentation of the strain data for analysis from\n        the command line options.\n        \"\"\"\n        strain_segments = {}\n        for ifo in ifos:\n            strain_segments[ifo] = cls.from_cli_single_ifo(opt,\n                                                         strain_dict[ifo], ifo)\n        return strain_segments"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the end time of the current valid segment of data", "response": "def end_time(self):\n        \"\"\" Return the end time of the current valid segment of data \"\"\"\n        return float(self.strain.start_time + (len(self.strain) - self.total_corruption) / self.sample_rate)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a hard countdown timer to the countdown timer so that we don t have to generate a new PSD.", "response": "def add_hard_count(self):\n        \"\"\" Reset the countdown timer, so that we don't analyze data long enough\n        to generate a new PSD.\n        \"\"\"\n        self.wait_duration = int(numpy.ceil(self.total_corruption / self.sample_rate +  self.psd_duration))\n        self.invalidate_psd()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef overwhitened_data(self, delta_f):\n        # we haven't already computed htilde for this delta_f\n        if delta_f not in self.segments:\n            buffer_length = int(1.0 / delta_f)\n            e = len(self.strain)\n            s = int(e - buffer_length * self.sample_rate - self.reduced_pad * 2)\n            fseries = make_frequency_series(self.strain[s:e])\n\n            # we haven't calculated a resample psd for this delta_f\n            if delta_f not in self.psds:\n                psdt = pycbc.psd.interpolate(self.psd, fseries.delta_f)\n                psdt = pycbc.psd.inverse_spectrum_truncation(psdt,\n                                       int(self.sample_rate * self.psd_inverse_length),\n                                       low_frequency_cutoff=self.low_frequency_cutoff)\n                psdt._delta_f = fseries.delta_f\n\n                psd = pycbc.psd.interpolate(self.psd, delta_f)\n                psd = pycbc.psd.inverse_spectrum_truncation(psd,\n                                       int(self.sample_rate * self.psd_inverse_length),\n                                       low_frequency_cutoff=self.low_frequency_cutoff)\n\n                psd.psdt = psdt\n                self.psds[delta_f] = psd\n\n            psd = self.psds[delta_f]\n            fseries /= psd.psdt\n\n            # trim ends of strain\n            if self.reduced_pad  != 0:\n                overwhite = TimeSeries(zeros(e-s, dtype=self.strain.dtype),\n                                             delta_t=self.strain.delta_t)\n                pycbc.fft.ifft(fseries, overwhite)\n                overwhite2 = overwhite[self.reduced_pad:len(overwhite)-self.reduced_pad]\n                taper_window = self.trim_padding / 2.0 / overwhite.sample_rate\n                gate_params = [(overwhite2.start_time, 0., taper_window),\n                               (overwhite2.end_time, 0., taper_window)]\n                gate_data(overwhite2, gate_params)\n                fseries_trimmed = FrequencySeries(zeros(len(overwhite2) / 2 + 1,\n                                                  dtype=fseries.dtype), delta_f=delta_f)\n                pycbc.fft.fft(overwhite2, fseries_trimmed)\n                fseries_trimmed.start_time = fseries.start_time + self.reduced_pad * self.strain.delta_t\n            else:\n                fseries_trimmed = fseries\n\n            fseries_trimmed.psd = psd\n            self.segments[delta_f] = fseries_trimmed\n\n        stilde = self.segments[delta_f]\n        return stilde", "response": "Generate overwhitened frequency domain data for a given delta_f."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking that the current set of triggers could be influenced by a hardware injection.", "response": "def near_hwinj(self):\n        \"\"\"Check that the current set of triggers could be influenced by\n        a hardware injection.\n        \"\"\"\n        if not self.state:\n            return False\n\n        if not self.state.is_extent_valid(self.start_time, self.blocksize, pycbc.frame.NO_HWINJ):\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadvancing and insert zeros at the end of the strain", "response": "def null_advance_strain(self, blocksize):\n        \"\"\" Advance and insert zeros\n\n        Parameters\n        ----------\n        blocksize: int\n            The number of seconds to attempt to read from the channel\n        \"\"\"\n        sample_step = int(blocksize * self.sample_rate)\n        csize = sample_step + self.corruption * 2\n        self.strain.roll(-sample_step)\n\n        # We should roll this off at some point too...\n        self.strain[len(self.strain) - csize + self.corruption:] = 0\n        self.strain.start_time += blocksize\n\n        # The next time we need strain will need to be tapered\n        self.taper_immediate_strain = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadvances the buffer by blocksize seconds.", "response": "def advance(self, blocksize, timeout=10):\n        \"\"\"Advanced buffer blocksize seconds.\n\n        Add blocksize seconds more to the buffer, push blocksize seconds\n        from the beginning.\n\n        Parameters\n        ----------\n        blocksize: int\n            The number of seconds to attempt to read from the channel\n\n        Returns\n        -------\n        status: boolean\n            Returns True if this block is analyzable.\n        \"\"\"\n        ts = super(StrainBuffer, self).attempt_advance(blocksize, timeout=timeout)\n        self.blocksize = blocksize\n\n        # We have given up so there is no time series\n        if ts is None:\n            logging.info(\"%s frame is late, giving up\", self.detector)\n            self.null_advance_strain(blocksize)\n            if self.state:\n                self.state.null_advance(blocksize)\n            if self.dq:\n                self.dq.null_advance(blocksize)\n            return False\n\n        # We collected some data so we are closer to being able to analyze data\n        self.wait_duration -= blocksize\n\n        # If the data we got was invalid, reset the counter on how much to collect\n        # This behavior corresponds to how we handle CAT1 vetoes\n        if self.state and self.state.advance(blocksize) is False:\n            self.add_hard_count()\n            self.null_advance_strain(blocksize)\n            if self.dq:\n                self.dq.null_advance(blocksize)\n            logging.info(\"%s time has invalid data, resetting buffer\",\n                         self.detector)\n            return False\n\n        # Also advance the dq vector in lockstep\n        if self.dq:\n            self.dq.advance(blocksize)\n\n        self.segments = {}\n\n        # only condition with the needed raw data so we can continuously add\n        # to the existing result\n\n        # Precondition\n        sample_step = int(blocksize * self.sample_rate)\n        csize = sample_step + self.corruption * 2\n        start = len(self.raw_buffer) - csize * self.factor\n        strain = self.raw_buffer[start:]\n\n        strain =  pycbc.filter.highpass_fir(strain, self.highpass_frequency,\n                                       self.highpass_samples,\n                                       beta=self.beta)\n        strain = (strain * self.dyn_range_fac).astype(numpy.float32)\n\n        strain = pycbc.filter.resample_to_delta_t(strain,\n                                           1.0/self.sample_rate, method='ldas')\n\n        # remove corruption at beginning\n        strain = strain[self.corruption:]\n\n        # taper beginning if needed\n        if self.taper_immediate_strain:\n            logging.info(\"Tapering start of %s strain block\", self.detector)\n            strain = gate_data(strain, [(strain.start_time, 0., self.autogating_pad)])\n            self.taper_immediate_strain = False\n\n        # Stitch into continuous stream\n        self.strain.roll(-sample_step)\n        self.strain[len(self.strain) - csize + self.corruption:] = strain[:]\n        self.strain.start_time += blocksize\n\n        # apply gating if need be: NOT YET IMPLEMENTED\n        if self.psd is None and self.wait_duration <=0:\n            self.recalculate_psd()\n\n        if self.wait_duration > 0:\n            return False\n        else:\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninitializing a StrainBuffer object for a particular detector.", "response": "def from_cli(cls, ifo, args, maxlen):\n        \"\"\"Initialize a StrainBuffer object (data reader) for a particular\n        detector.\n        \"\"\"\n        state_channel = analyze_flags = None\n        if args.state_channel and ifo in args.state_channel \\\n                and args.analyze_flags and ifo in args.analyze_flags:\n            state_channel = ':'.join([ifo, args.state_channel[ifo]])\n            analyze_flags = args.analyze_flags[ifo].split(',')\n\n        dq_channel = dq_flags = None\n        if args.data_quality_channel and ifo in args.data_quality_channel \\\n                and args.data_quality_flags and ifo in args.data_quality_flags:\n            dq_channel = ':'.join([ifo, args.data_quality_channel[ifo]])\n            dq_flags = args.data_quality_flags[ifo].split(',')\n\n        if args.frame_type:\n            frame_src = pycbc.frame.frame_paths(args.frame_type[ifo],\n                                                args.start_time,\n                                                args.end_time)\n        else:\n            frame_src = [args.frame_src[ifo]]\n        strain_channel = ':'.join([ifo, args.channel_name[ifo]])\n\n        return cls(frame_src, strain_channel,\n                   args.start_time, max_buffer=maxlen * 2,\n                   state_channel=state_channel,\n                   data_quality_channel=dq_channel,\n                   sample_rate=args.sample_rate,\n                   low_frequency_cutoff=args.low_frequency_cutoff,\n                   highpass_frequency=args.highpass_frequency,\n                   highpass_reduction=args.highpass_reduction,\n                   highpass_bandwidth=args.highpass_bandwidth,\n                   psd_samples=args.psd_samples,\n                   trim_padding=args.trim_padding,\n                   psd_segment_length=args.psd_segment_length,\n                   psd_inverse_length=args.psd_inverse_length,\n                   autogating_threshold=args.autogating_threshold,\n                   autogating_cluster=args.autogating_cluster,\n                   autogating_window=args.autogating_window,\n                   autogating_pad=args.autogating_pad,\n                   psd_abort_difference=args.psd_abort_difference,\n                   psd_recalculate_difference=args.psd_recalculate_difference,\n                   force_update_cache=args.force_update_cache,\n                   increment_update_cache=args.increment_update_cache[ifo],\n                   analyze_flags=analyze_flags,\n                   data_quality_flags=dq_flags,\n                   dq_padding=args.data_quality_padding)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_string(psd_name, length, delta_f, low_freq_cutoff):\n\n    # check if valid PSD model\n    if psd_name not in get_psd_model_list():\n        raise ValueError(psd_name + ' not found among analytical '\n                         'PSD functions.')\n\n    # if PSD model is in LALSimulation\n    if psd_name in get_lalsim_psd_list():\n        lalseries = lal.CreateREAL8FrequencySeries(\n            '', lal.LIGOTimeGPS(0), 0, delta_f, lal.DimensionlessUnit, length)\n        try:\n            func = lalsimulation.__dict__[\n                                        _name_prefix + psd_name + _name_suffix]\n        except KeyError:\n            func = lalsimulation.__dict__[_name_prefix + psd_name]\n            func(lalseries, low_freq_cutoff)\n        else:\n            lalsimulation.SimNoisePSD(lalseries, 0, func)\n        psd = FrequencySeries(lalseries.data.data, delta_f=delta_f)\n\n    # if PSD model is coded in PyCBC\n    else:\n        func = pycbc_analytical_psds[psd_name]\n        psd = func(length, delta_f, low_freq_cutoff)\n\n    # zero-out content below low-frequency cutoff\n    kmin = int(low_freq_cutoff / delta_f)\n    psd.data[:kmin] = 0\n\n    return psd", "response": "Generate a frequency series containing a LALSimulation PSD specified by name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a FrequencySeries containing ones above the low_frequency_cutoff.", "response": "def flat_unity(length, delta_f, low_freq_cutoff):\n    \"\"\" Returns a FrequencySeries of ones above the low_frequency_cutoff.\n\n    Parameters\n    ----------\n    length : int\n        Length of output Frequencyseries.\n    delta_f : float\n        Frequency step for output FrequencySeries.\n    low_freq_cutoff : int\n        Low-frequency cutoff for output FrequencySeries.\n\n    Returns\n    -------\n    FrequencySeries\n        Returns a FrequencySeries containing the unity PSD model.\n    \"\"\"\n    fseries = FrequencySeries(numpy.ones(length), delta_f=delta_f)\n    kmin = int(low_freq_cutoff / fseries.delta_f)\n    fseries.data[:kmin] = 0\n    return fseries"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nyield n successive chunks from l.", "response": "def chunks(l, n):\n    \"\"\" Yield n successive chunks from l.\n    \"\"\"\n    newn = int(len(l) / n)\n    for i in xrange(0, n-1):\n        yield l[i*newn:i*newn+newn]\n    yield l[n*newn-newn:]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rough_time_estimate(m1, m2, flow, fudge_length=1.1, fudge_min=0.02):\n    m = m1 + m2\n    msun = m * lal.MTSUN_SI\n    t =  5.0 / 256.0 * m * m * msun / (m1 * m2) / \\\n        (numpy.pi * msun * flow) **  (8.0 / 3.0)\n\n    # fudge factoriness\n    return .022 if t < 0 else (t + fudge_min) * fudge_length", "response": "A very rough estimate of the duration of the waveform."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the frequencies needed to compress a waveform with the given chirp mass. This is based on the estimate in rough_time_estimate. Parameters ---------- m1: float mass of first component object in solar masses m2: float mass of second component object in solar masses fmin : float The starting frequency of the compressed waveform. fmax : float The ending frequency of the compressed waveform. min_seglen : float The inverse of this gives the maximum frequency step that is used. df_multiple : {None, float} Make the compressed sampling frequencies a multiple of the given value. If None provided, the returned sample points can have any floating point value. Returns ------- array The frequencies at which to evaluate the compressed waveform.", "response": "def mchirp_compression(m1, m2, fmin, fmax, min_seglen=0.02, df_multiple=None):\n    \"\"\"Return the frequencies needed to compress a waveform with the given\n    chirp mass. This is based on the estimate in rough_time_estimate.\n\n    Parameters\n    ----------\n    m1: float\n        mass of first component object in solar masses\n    m2: float\n        mass of second component object in solar masses\n    fmin : float\n        The starting frequency of the compressed waveform.\n    fmax : float\n        The ending frequency of the compressed waveform.\n    min_seglen : float\n        The inverse of this gives the maximum frequency step that is used.\n    df_multiple : {None, float}\n        Make the compressed sampling frequencies a multiple of the given value.\n        If None provided, the returned sample points can have any floating\n        point value.\n\n    Returns\n    -------\n    array\n        The frequencies at which to evaluate the compressed waveform.\n    \"\"\"\n    sample_points = []\n    f = fmin\n    while f < fmax:\n        if df_multiple is not None:\n            f = int(f/df_multiple)*df_multiple\n        sample_points.append(f)\n        f += 1.0 / rough_time_estimate(m1, m2, f, fudge_min=min_seglen)\n    # add the last point\n    if sample_points[-1] < fmax:\n        sample_points.append(fmax)\n    return numpy.array(sample_points)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the frequencies needed to compress the given frequency domain waveform. This is done by estimating t(f) of the waveform using the stationary phase approximation. Parameters ---------- htilde : FrequencySeries The waveform to compress. fmin : float The starting frequency of the compressed waveform. fmax : float The ending frequency of the compressed waveform. min_seglen : float The inverse of this gives the maximum frequency step that is used. sample_frequencies : {None, array} The frequencies that the waveform is evaluated at. If None, will retrieve the frequencies from the waveform's sample_frequencies attribute. Returns ------- array The frequencies at which to evaluate the compressed waveform.", "response": "def spa_compression(htilde, fmin, fmax, min_seglen=0.02,\n        sample_frequencies=None):\n    \"\"\"Returns the frequencies needed to compress the given frequency domain\n    waveform. This is done by estimating t(f) of the waveform using the\n    stationary phase approximation.\n\n    Parameters\n    ----------\n    htilde : FrequencySeries\n        The waveform to compress.\n    fmin : float\n        The starting frequency of the compressed waveform.\n    fmax : float\n        The ending frequency of the compressed waveform.\n    min_seglen : float\n        The inverse of this gives the maximum frequency step that is used.\n    sample_frequencies : {None, array}\n        The frequencies that the waveform is evaluated at. If None, will\n        retrieve the frequencies from the waveform's sample_frequencies\n        attribute.\n\n    Returns\n    -------\n    array\n        The frequencies at which to evaluate the compressed waveform.\n    \"\"\"\n    if sample_frequencies is None:\n        sample_frequencies = htilde.sample_frequencies.numpy()\n    kmin = int(fmin/htilde.delta_f)\n    kmax = int(fmax/htilde.delta_f)\n    tf = abs(utils.time_from_frequencyseries(htilde,\n            sample_frequencies=sample_frequencies).data[kmin:kmax])\n    sample_frequencies = sample_frequencies[kmin:kmax]\n    sample_points = []\n    f = fmin\n    while f < fmax:\n        f = int(f/htilde.delta_f)*htilde.delta_f\n        sample_points.append(f)\n        jj = numpy.searchsorted(sample_frequencies, f)\n        f += 1./(tf[jj:].max()+min_seglen)\n    # add the last point\n    if sample_points[-1] < fmax:\n        sample_points.append(fmax)\n    return numpy.array(sample_points)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes a statistic indicating between which sample points a waveform and the interpolated waveform differ the most.", "response": "def vecdiff(htilde, hinterp, sample_points, psd=None):\n    \"\"\"Computes a statistic indicating between which sample points a waveform\n    and the interpolated waveform differ the most.\n    \"\"\"\n    vecdiffs = numpy.zeros(sample_points.size-1, dtype=float)\n    for kk,thisf in enumerate(sample_points[:-1]):\n        nextf = sample_points[kk+1]\n        vecdiffs[kk] = abs(_vecdiff(htilde, hinterp, thisf, nextf, psd=psd))\n    return vecdiffs"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef compress_waveform(htilde, sample_points, tolerance, interpolation,\n                      precision, decomp_scratch=None, psd=None):\n    \"\"\"Retrieves the amplitude and phase at the desired sample points, and adds\n    frequency points in order to ensure that the interpolated waveform\n    has a mismatch with the full waveform that is <= the desired tolerance. The\n    mismatch is computed by finding 1-overlap between `htilde` and the\n    decompressed waveform; no maximimization over phase/time is done, a\n    PSD may be used.\n\n    .. note::\n        The decompressed waveform is only garaunteed to have a true mismatch\n        <= the tolerance for the given `interpolation` and for no PSD.\n        However, since no maximization over time/phase is performed when\n        adding points, the actual mismatch between the decompressed waveform\n        and `htilde` is better than the tolerance, using no PSD. Using a PSD\n        does increase the mismatch, and can lead to mismatches > than the\n        desired tolerance, but typically by only a factor of a few worse.\n\n    Parameters\n    ----------\n    htilde : FrequencySeries\n        The waveform to compress.\n    sample_points : array\n        The frequencies at which to store the amplitude and phase. More points\n        may be added to this, depending on the desired tolerance.\n    tolerance : float\n        The maximum mismatch to allow between a decompressed waveform and\n        `htilde`.\n    interpolation : str\n        The interpolation to use for decompressing the waveform when computing\n        overlaps.\n    precision : str\n        The precision being used to generate and store the compressed waveform\n        points.\n    decomp_scratch : {None, FrequencySeries}\n        Optionally provide scratch space for decompressing the waveform. The\n        provided frequency series must have the same `delta_f` and length\n        as `htilde`.\n    psd : {None, FrequencySeries}\n        The psd to use for calculating the overlap between the decompressed\n        waveform and the original full waveform.\n\n    Returns\n    -------\n    CompressedWaveform\n        The compressed waveform data; see `CompressedWaveform` for details.\n    \"\"\"\n    fmin = sample_points.min()\n    df = htilde.delta_f\n    sample_index = (sample_points / df).astype(int)\n    amp = utils.amplitude_from_frequencyseries(htilde)\n    phase = utils.phase_from_frequencyseries(htilde)\n\n    comp_amp = amp.take(sample_index)\n    comp_phase = phase.take(sample_index)\n    if decomp_scratch is None:\n        outdf = df\n    else:\n        outdf = None\n    hdecomp = fd_decompress(comp_amp, comp_phase, sample_points,\n                            out=decomp_scratch, df=outdf, f_lower=fmin,\n                            interpolation=interpolation)\n    kmax = min(len(htilde), len(hdecomp))\n    htilde = htilde[:kmax]\n    hdecomp = hdecomp[:kmax]\n    mismatch = 1. - filter.overlap(hdecomp, htilde, psd=psd,\n                                   low_frequency_cutoff=fmin)\n    if mismatch > tolerance:\n        # we'll need the difference in the waveforms as a function of frequency\n        vecdiffs = vecdiff(htilde, hdecomp, sample_points, psd=psd)\n\n    # We will find where in the frequency series the interpolated waveform\n    # has the smallest overlap with the full waveform, add a sample point\n    # there, and re-interpolate. We repeat this until the overall mismatch\n    # is > than the desired tolerance\n    added_points = []\n    while mismatch > tolerance:\n        minpt = vecdiffs.argmax()\n        # add a point at the frequency halfway between minpt and minpt+1\n        add_freq = sample_points[[minpt, minpt+1]].mean()\n        addidx = int(round(add_freq/df))\n        # ensure that only new points are added\n        if addidx in sample_index:\n            diffidx = vecdiffs.argsort()\n            addpt = -1\n            while addidx in sample_index:\n                addpt -= 1\n                try:\n                    minpt = diffidx[addpt]\n                except IndexError:\n                    raise ValueError(\"unable to compress to desired tolerance\")\n                add_freq = sample_points[[minpt, minpt+1]].mean()\n                addidx = int(round(add_freq/df))\n        new_index = numpy.zeros(sample_index.size+1, dtype=int)\n        new_index[:minpt+1] = sample_index[:minpt+1]\n        new_index[minpt+1] = addidx\n        new_index[minpt+2:] = sample_index[minpt+1:]\n        sample_index = new_index\n        sample_points = (sample_index * df).astype(\n            real_same_precision_as(htilde))\n        # get the new compressed points\n        comp_amp = amp.take(sample_index)\n        comp_phase = phase.take(sample_index)\n        # update the vecdiffs and mismatch\n        hdecomp = fd_decompress(comp_amp, comp_phase, sample_points,\n                                out=decomp_scratch, df=outdf,\n                                f_lower=fmin, interpolation=interpolation)\n        hdecomp = hdecomp[:kmax]\n        new_vecdiffs = numpy.zeros(vecdiffs.size+1)\n        new_vecdiffs[:minpt] = vecdiffs[:minpt]\n        new_vecdiffs[minpt+2:] = vecdiffs[minpt+1:]\n        new_vecdiffs[minpt:minpt+2] = vecdiff(htilde, hdecomp,\n                                              sample_points[minpt:minpt+2],\n                                              psd=psd)\n        vecdiffs = new_vecdiffs\n        mismatch = 1. - filter.overlap(hdecomp, htilde, psd=psd,\n                                       low_frequency_cutoff=fmin)\n        added_points.append(addidx)\n    logging.info(\"mismatch: %f, N points: %i (%i added)\" %(mismatch,\n                 len(comp_amp), len(added_points)))\n\n    return CompressedWaveform(sample_points, comp_amp, comp_phase,\n                              interpolation=interpolation,\n                              tolerance=tolerance, mismatch=mismatch,\n                              precision=precision)", "response": "Returns the compressed waveform at the given frequency points."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndecompresses an FD waveform using the given amplitude phase and sample_frequencies.", "response": "def fd_decompress(amp, phase, sample_frequencies, out=None, df=None,\n                  f_lower=None, interpolation='inline_linear'):\n    \"\"\"Decompresses an FD waveform using the given amplitude, phase, and the\n    frequencies at which they are sampled at.\n\n    Parameters\n    ----------\n    amp : array\n        The amplitude of the waveform at the sample frequencies.\n    phase : array\n        The phase of the waveform at the sample frequencies.\n    sample_frequencies : array\n        The frequency (in Hz) of the waveform at the sample frequencies.\n    out : {None, FrequencySeries}\n        The output array to save the decompressed waveform to. If this contains\n        slots for frequencies > the maximum frequency in sample_frequencies,\n        the rest of the values are zeroed. If not provided, must provide a df.\n    df : {None, float}\n        The frequency step to use for the decompressed waveform. Must be\n        provided if out is None.\n    f_lower : {None, float}\n        The frequency to start the decompression at. If None, will use whatever\n        the lowest frequency is in sample_frequencies. All values at\n        frequencies less than this will be 0 in the decompressed waveform.\n    interpolation : {'inline_linear', str}\n        The interpolation to use for the amplitude and phase. Default is\n        'inline_linear'. If 'inline_linear' a custom interpolater is used.\n        Otherwise, ``scipy.interpolate.interp1d`` is used; for other options,\n        see possible values for that function's ``kind`` argument.\n\n    Returns\n    -------\n    out : FrequencySeries\n        If out was provided, writes to that array. Otherwise, a new\n        FrequencySeries with the decompressed waveform.\n    \"\"\"\n    precision = _precision_map[sample_frequencies.dtype.name]\n    if _precision_map[amp.dtype.name] != precision or \\\n            _precision_map[phase.dtype.name] != precision:\n        raise ValueError(\"amp, phase, and sample_points must all have the \"\n            \"same precision\")\n\n    if out is None:\n        if df is None:\n            raise ValueError(\"Either provide output memory or a df\")\n        hlen = int(numpy.ceil(sample_frequencies.max()/df+1))\n        out = FrequencySeries(numpy.zeros(hlen,\n            dtype=_complex_dtypes[precision]), copy=False,\n            delta_f=df)\n    else:\n        # check for precision compatibility\n        if out.precision == 'double' and precision == 'single':\n            raise ValueError(\"cannot cast single precision to double\")\n        df = out.delta_f\n        hlen = len(out)\n    if f_lower is None:\n        imin = 0 # pylint:disable=unused-variable\n        f_lower = sample_frequencies[0]\n        start_index = 0\n    else:\n        if f_lower >= sample_frequencies.max():\n            raise ValueError(\"f_lower is > than the maximum sample frequency\")\n        if f_lower < sample_frequencies.min():\n            raise ValueError(\"f_lower is < than the minimum sample frequency\")\n        imin = int(numpy.searchsorted(sample_frequencies, f_lower,\n            side='right')) - 1 # pylint:disable=unused-variable\n        start_index = int(numpy.ceil(f_lower/df))\n    if start_index >= hlen:\n        raise ValueError('requested f_lower >= largest frequency in out')\n    # interpolate the amplitude and the phase\n    if interpolation == \"inline_linear\":\n        # Call the scheme-dependent function\n        inline_linear_interp(amp, phase, sample_frequencies, out,\n                             df, f_lower, imin, start_index)\n    else:\n        # use scipy for fancier interpolation\n        sample_frequencies = numpy.array(sample_frequencies)\n        amp = numpy.array(amp)\n        phase = numpy.array(phase)\n        outfreq = out.sample_frequencies.numpy()\n        amp_interp = interpolate.interp1d(sample_frequencies, amp,\n                                          kind=interpolation,\n                                          bounds_error=False,\n                                          fill_value=0.,\n                                          assume_sorted=True)\n        phase_interp = interpolate.interp1d(sample_frequencies, phase,\n                                            kind=interpolation,\n                                            bounds_error=False,\n                                            fill_value=0.,\n                                            assume_sorted=True)\n        A = amp_interp(outfreq)\n        phi = phase_interp(outfreq)\n        out.data[:] = A*numpy.cos(phi) + (1j)*A*numpy.sin(phi)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndecompress the current waveform into a frequency series.", "response": "def decompress(self, out=None, df=None, f_lower=None, interpolation=None):\n        \"\"\"Decompress self.\n\n        Parameters\n        ----------\n        out : {None, FrequencySeries}\n            Write the decompressed waveform to the given frequency series. The\n            decompressed waveform will have the same `delta_f` as `out`.\n            Either this or `df` must be provided.\n        df : {None, float}\n            Decompress the waveform such that its `delta_f` has the given\n            value. Either this or `out` must be provided.\n        f_lower : {None, float}\n            The starting frequency at which to decompress the waveform. Cannot\n            be less than the minimum frequency in `sample_points`. If `None`\n            provided, will default to the minimum frequency in `sample_points`.\n        interpolation : {None, str}\n            The interpolation to use for decompressing the waveform. If `None`\n            provided, will default to `self.interpolation`.\n\n        Returns\n        -------\n        FrequencySeries\n            The decompressed waveform.\n        \"\"\"\n        if f_lower is None:\n            # use the minimum of the samlpe points\n            f_lower = self.sample_points.min()\n        if interpolation is None:\n            interpolation = self.interpolation\n        return fd_decompress(self.amplitude, self.phase, self.sample_points,\n                             out=out, df=df, f_lower=f_lower,\n                             interpolation=interpolation)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write_to_hdf(self, fp, template_hash, root=None, precision=None):\n        if root is None:\n            root = ''\n        else:\n            root = '%s/'%(root)\n        if precision is None:\n            precision = self.precision\n        elif precision == 'double' and self.precision == 'single':\n            raise ValueError(\"cannot cast single precision to double\")\n        outdtype = _real_dtypes[precision]\n        group = '%scompressed_waveforms/%s' %(root, str(template_hash))\n        for param in ['amplitude', 'phase', 'sample_points']:\n            fp['%s/%s' %(group, param)] = self._get(param).astype(outdtype)\n        fp_group = fp[group]\n        fp_group.attrs['mismatch'] = self.mismatch\n        fp_group.attrs['interpolation'] = self.interpolation\n        fp_group.attrs['tolerance'] = self.tolerance\n        fp_group.attrs['precision'] = precision", "response": "Write the compressed waveform to the given hdf file handler."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_hdf(cls, fp, template_hash, root=None, load_to_memory=True,\n                 load_now=False):\n        \"\"\"Load a compressed waveform from the given hdf file handler.\n\n        The waveform is retrieved from:\n        `fp['[{root}/]compressed_waveforms/{template_hash}/{param}']`,\n        where `param` is the `sample_points`, `amplitude`, and `phase`.\n\n        Parameters\n        ----------\n        fp : h5py.File\n            An open hdf file to write the compressed waveform to.\n        template_hash : {hash, int, str}\n            The id of the waveform.\n        root : {None, str}\n            Retrieve the `compressed_waveforms` group from the given string.\n            If `None`, `compressed_waveforms` will be assumed to be in the\n            top level.\n        load_to_memory : {True, bool}\n            Set the `load_to_memory` attribute to the given value in the\n            returned instance.\n        load_now : {False, bool}\n            Immediately load the `sample_points`/`amplitude`/`phase` to memory.\n\n\n        Returns\n        -------\n        CompressedWaveform\n            An instance of this class with parameters loaded from the hdf file.\n        \"\"\"\n        if root is None:\n            root = ''\n        else:\n            root = '%s/'%(root)\n        group = '%scompressed_waveforms/%s' %(root, str(template_hash))\n        fp_group = fp[group]\n        sample_points = fp_group['sample_points']\n        amp = fp_group['amplitude']\n        phase = fp_group['phase']\n        if load_now:\n            sample_points = sample_points[:]\n            amp = amp[:]\n            phase = phase[:]\n        return cls(sample_points, amp, phase,\n            interpolation=fp_group.attrs['interpolation'],\n            tolerance=fp_group.attrs['tolerance'],\n            mismatch=fp_group.attrs['mismatch'],\n            precision=fp_group.attrs['precision'],\n            load_to_memory=load_to_memory)", "response": "Load a compressed waveform from the given hdf file handler."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute correlation (two sided) between template and data and compares with autocorrelation of the template: C(t) = IFFT(A*A/S(f)) Parameters ---------- sn: Array[complex] normalized (!) array of complex snr for the template that produced the trigger(s) being tested corr_sn : Array[complex] normalized (!) array of complex snr for the template that you want to produce a correlation chisq test for. In the [common] case that sn and corr_sn are the same, you are computing auto-correlation chisq. hautocorr: Array[complex] time domain autocorrelation for the template indices: Array[int] compute correlation chisquare at the points specified in this array, num_points: [int, optional; default=None] Number of points used for autochisq on each side, if None all points are used. stride: [int, optional; default = 1] stride for points selection for autochisq total length <= 2*num_points*stride oneside: [str, optional; default=None] whether to use one or two sided autochisquare. If None (or not provided) twosided chi-squared will be used. If given, options are 'left' or 'right', to do one-sided chi-squared on the left or right. twophase: Boolean, optional; default=True If True calculate the auto-chisq using both phases of the filter. If False only use the phase of the obtained trigger(s). maxvalued: Boolean, optional; default=False Return the largest auto-chisq at any of the points tested if True. If False, return the sum of auto-chisq at all points tested. Returns ------- autochisq: [tuple] returns autochisq values and snr corresponding to the instances of time defined by indices", "response": "def autochisq_from_precomputed(sn, corr_sn, hautocorr, indices,\n                       stride=1, num_points=None, oneside=None,\n                       twophase=True, maxvalued=False):\n    \"\"\"\n    Compute correlation (two sided) between template and data\n    and compares with autocorrelation of the template: C(t) = IFFT(A*A/S(f))\n\n    Parameters\n    ----------\n    sn: Array[complex]\n        normalized (!) array of complex snr for the template that produced the\n        trigger(s) being tested\n    corr_sn : Array[complex]\n        normalized (!) array of complex snr for the template that you want to\n        produce a correlation chisq test for. In the [common] case that sn and\n        corr_sn are the same, you are computing auto-correlation chisq.\n    hautocorr: Array[complex]\n        time domain autocorrelation for the template\n    indices: Array[int]\n        compute correlation chisquare at the points specified in this array,\n    num_points: [int, optional; default=None]\n        Number of points used for autochisq on each side, if None all points\n        are used.\n    stride: [int, optional; default = 1]\n        stride for points selection for autochisq\n        total length <= 2*num_points*stride\n    oneside: [str, optional; default=None]\n        whether to use one or two sided autochisquare. If None (or not\n        provided) twosided chi-squared will be used. If given, options are\n        'left' or 'right', to do one-sided chi-squared on the left or right.\n    twophase: Boolean, optional; default=True\n        If True calculate the auto-chisq using both phases of the filter.\n        If False only use the phase of the obtained trigger(s).\n    maxvalued: Boolean, optional; default=False\n        Return the largest auto-chisq at any of the points tested if True.\n        If False, return the sum of auto-chisq at all points tested.\n\n    Returns\n    -------\n    autochisq: [tuple]\n        returns autochisq values and snr corresponding to the instances\n        of time defined by indices\n    \"\"\"\n    Nsnr = len(sn)\n\n    achisq = np.zeros(len(indices))\n    num_points_all = int(Nsnr/stride)\n    if num_points is None:\n        num_points = num_points_all\n    if (num_points > num_points_all):\n        num_points = num_points_all\n\n    snrabs = np.abs(sn[indices])\n    cphi_array = (sn[indices]).real / snrabs\n    sphi_array = (sn[indices]).imag / snrabs\n\n    start_point = - stride*num_points\n    end_point = stride*num_points+1\n    if oneside == 'left':\n        achisq_idx_list = np.arange(start_point, 0, stride)\n    elif oneside == 'right':\n        achisq_idx_list = np.arange(stride, end_point, stride)\n    else:\n        achisq_idx_list_pt1 = np.arange(start_point, 0, stride)\n        achisq_idx_list_pt2 = np.arange(stride, end_point, stride)\n        achisq_idx_list = np.append(achisq_idx_list_pt1,\n                                    achisq_idx_list_pt2)\n\n    hauto_corr_vec = hautocorr[achisq_idx_list]\n    hauto_norm = hauto_corr_vec.real*hauto_corr_vec.real\n    # REMOVE THIS LINE TO REPRODUCE OLD RESULTS\n    hauto_norm += hauto_corr_vec.imag*hauto_corr_vec.imag\n    chisq_norm = 1.0 - hauto_norm\n\n    for ip,ind in enumerate(indices):\n        curr_achisq_idx_list = achisq_idx_list + ind\n\n        cphi = cphi_array[ip]\n        sphi = sphi_array[ip]\n        # By construction, the other \"phase\" of the SNR is 0\n        snr_ind =  sn[ind].real*cphi + sn[ind].imag*sphi\n\n        # Wrap index if needed (maybe should fail in this case?)\n        if curr_achisq_idx_list[0] < 0:\n            curr_achisq_idx_list[curr_achisq_idx_list < 0] += Nsnr\n        if curr_achisq_idx_list[-1] > (Nsnr - 1):\n            curr_achisq_idx_list[curr_achisq_idx_list > (Nsnr-1)] -= Nsnr\n\n        z = corr_sn[curr_achisq_idx_list].real*cphi + \\\n             corr_sn[curr_achisq_idx_list].imag*sphi\n        dz = z - hauto_corr_vec.real*snr_ind\n        curr_achisq_list = dz*dz/chisq_norm\n\n        if twophase:\n            chisq_norm = 1.0 - hauto_norm\n            z = -corr_sn[curr_achisq_idx_list].real*sphi + \\\n                 corr_sn[curr_achisq_idx_list].imag*cphi\n            dz = z - hauto_corr_vec.imag*snr_ind\n            curr_achisq_list += dz*dz/chisq_norm\n\n        if maxvalued:\n            achisq[ip] = curr_achisq_list.max()\n        else:\n            achisq[ip] = curr_achisq_list.sum()\n\n    dof = num_points\n    if oneside is None:\n        dof = dof * 2\n    if twophase:\n        dof = dof * 2\n\n    return dof, achisq, indices"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef values(self, sn, indices, template, psd, norm, stilde=None,\n               low_frequency_cutoff=None, high_frequency_cutoff=None):\n        \"\"\"\n        Calculate the auto-chisq at the specified indices.\n\n        Parameters\n        -----------\n        sn : Array[complex]\n            SNR time series of the template for which auto-chisq is being\n            computed. Provided unnormalized.\n        indices : Array[int]\n            List of points at which to calculate auto-chisq\n        template : Pycbc template object\n            The template for which we are calculating auto-chisq\n        psd : Pycbc psd object\n            The PSD of the data being analysed\n        norm : float\n            The normalization factor to apply to sn\n        stilde : Pycbc data object, needed if using reverse-template\n            The data being analysed. Only needed if using reverse-template,\n            otherwise ignored\n        low_frequency_cutoff : float\n            The lower frequency to consider in matched-filters\n        high_frequency_cutoff : float\n            The upper frequency to consider in matched-filters\n        \"\"\"\n        if self.do and (len(indices) > 0):\n            htilde = make_frequency_series(template)\n\n            # Check if we need to recompute the autocorrelation\n            key = (id(template), id(psd))\n            if key != self._autocor_id:\n                logging.info(\"Calculating autocorrelation\")\n\n                if not self.reverse_template:\n                    Pt, _, P_norm = matched_filter_core(htilde,\n                              htilde, psd=psd,\n                              low_frequency_cutoff=low_frequency_cutoff,\n                              high_frequency_cutoff=high_frequency_cutoff)\n                    Pt = Pt * (1./ Pt[0])\n                    self._autocor = Array(Pt, copy=True)\n                else:\n                    Pt, _, P_norm = matched_filter_core(htilde.conj(),\n                              htilde, psd=psd,\n                              low_frequency_cutoff=low_frequency_cutoff,\n                              high_frequency_cutoff=high_frequency_cutoff)\n\n                    # T-reversed template has same norm as forward template\n                    # so we can normalize using that\n                    # FIXME: Here sigmasq has to be cast to a float or the\n                    #        code is really slow ... why??\n                    norm_fac = P_norm / float(((template.sigmasq(psd))**0.5))\n                    Pt *= norm_fac\n                    self._autocor = Array(Pt, copy=True)\n                self._autocor_id = key\n\n            logging.info(\"...Calculating autochisquare\")\n            sn = sn*norm\n            if self.reverse_template:\n                assert(stilde is not None)\n                asn, _, ahnrm = matched_filter_core(htilde.conj(), stilde,\n                                 low_frequency_cutoff=low_frequency_cutoff,\n                                 high_frequency_cutoff=high_frequency_cutoff,\n                                 h_norm=template.sigmasq(psd))\n                correlation_snr = asn * ahnrm\n            else:\n                correlation_snr = sn\n\n            achi_list = np.array([])\n            index_list = np.array(indices)\n            dof, achi_list, _ = autochisq_from_precomputed(sn, correlation_snr,\n                               self._autocor, index_list, stride=self.stride,\n                               num_points=self.num_points,\n                               oneside=self.one_sided, twophase=self.two_phase,\n                               maxvalued=self.take_maximum_value)\n            self.dof = dof\n            return achi_list", "response": "Calculates the auto - chisq at the specified indices."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the bounds for the given parameter in a section in a config file.", "response": "def get_param_bounds_from_config(cp, section, tag, param):\n    \"\"\"Gets bounds for the given parameter from a section in a config file.\n\n    Minimum and maximum values for bounds are specified by adding\n    `min-{param}` and `max-{param}` options, where `{param}` is the name of\n    the parameter. The types of boundary (open, closed, or reflected) to create\n    may also be specified by adding options `btype-min-{param}` and\n    `btype-max-{param}`. Cyclic conditions can be adding option\n    `cyclic-{param}`. If no `btype` arguments are provided, the\n    left bound will be closed and the right open.\n\n    For example, the following will create right-open bounds for parameter\n    `foo`:\n\n    .. code-block:: ini\n\n        [{section}-{tag}]\n        min-foo = -1\n        max-foo = 1\n\n    This would make the boundaries cyclic:\n\n    .. code-block:: ini\n\n        [{section}-{tag}]\n        min-foo = -1\n        max-foo = 1\n        cyclic-foo =\n\n    For more details on boundary types and their meaning, see\n    `boundaries.Bounds`.\n\n    If the parameter is not found in the section will just return None (in\n    this case, all `btype` and `cyclic` arguments are ignored for that\n    parameter).  If bounds are specified, both a minimum and maximum must be\n    provided, else a Value or Type Error will be raised.\n\n    Parameters\n    ----------\n    cp : ConfigParser instance\n        The config file.\n    section : str\n        The name of the section.\n    tag : str\n        Any tag in the section name. The full section name searched for in\n        the config file is `{section}(-{tag})`.\n    param : str\n        The name of the parameter to retrieve bounds for.\n\n    Returns\n    -------\n    bounds : {Bounds instance | None}\n        If bounds were provided, a `boundaries.Bounds` instance\n        representing the bounds. Otherwise, `None`.\n    \"\"\"\n    try:\n        minbnd = float(cp.get_opt_tag(section, 'min-'+param, tag))\n    except Error:\n        minbnd = None\n    try:\n        maxbnd = float(cp.get_opt_tag(section, 'max-'+param, tag))\n    except Error:\n        maxbnd = None\n    if minbnd is None and maxbnd is None:\n        bnds = None\n    elif minbnd is None or maxbnd is None:\n        raise ValueError(\"if specifying bounds for %s, \" %(param) +\n            \"you must provide both a minimum and a maximum\")\n    else:\n        bndargs = {'min_bound': minbnd, 'max_bound': maxbnd}\n        # try to get  any other conditions, if provided\n        try:\n            minbtype = cp.get_opt_tag(section, 'btype-min-{}'.format(param),\n                                      tag)\n        except Error:\n            minbtype = 'closed'\n        try:\n            maxbtype = cp.get_opt_tag(section, 'btype-max-{}'.format(param),\n                                      tag)\n        except Error:\n            maxbtype = 'open'\n        bndargs.update({'btype_min': minbtype, 'btype_max': maxbtype})\n        cyclic = cp.has_option_tag(section, 'cyclic-{}'.format(param), tag)\n        bndargs.update({'cyclic': cyclic})\n        bnds = boundaries.Bounds(**bndargs)\n    return bnds"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef bounded_from_config(cls, cp, section, variable_args,\n        bounds_required=False, additional_opts=None):\n    \"\"\"Returns a bounded distribution based on a configuration file. The\n    parameters for the distribution are retrieved from the section titled\n    \"[`section`-`variable_args`]\" in the config file.\n\n    Parameters\n    ----------\n    cls : pycbc.prior class\n        The class to initialize with.\n    cp : pycbc.workflow.WorkflowConfigParser\n        A parsed configuration file that contains the distribution\n        options.\n    section : str\n        Name of the section in the configuration file.\n    variable_args : str\n        The names of the parameters for this distribution, separated by\n        `prior.VARARGS_DELIM`. These must appear in the \"tag\" part\n        of the section header.\n    bounds_required : {False, bool}\n       If True, raise a ValueError if a min and max are not provided for\n       every parameter. Otherwise, the prior will be initialized with the\n       parameter set to None. Even if bounds are not required, a\n       ValueError will be raised if only one bound is provided; i.e.,\n       either both bounds need to provided or no bounds.\n    additional_opts : {None, dict}\n        Provide additional options to be passed to the distribution class;\n        should be a dictionary specifying option -> value. If an option is\n        provided that also exists in the config file, the value provided will\n        be used instead of being read from the file.\n\n    Returns\n    -------\n    cls\n        An instance of the given class.\n    \"\"\"\n    tag = variable_args\n    variable_args = variable_args.split(VARARGS_DELIM)\n\n    if additional_opts is None:\n        additional_opts = {}\n\n    # list of args that are used to construct distribution\n    special_args = [\"name\"] + \\\n        ['min-{}'.format(arg) for arg in variable_args] + \\\n        ['max-{}'.format(arg) for arg in variable_args] + \\\n        ['btype-min-{}'.format(arg) for arg in variable_args] + \\\n        ['btype-max-{}'.format(arg) for arg in variable_args] + \\\n        ['cyclic-{}'.format(arg) for arg in variable_args] + \\\n        list(additional_opts.keys())\n\n    # get a dict with bounds as value\n    dist_args = {}\n    for param in variable_args:\n        bounds = get_param_bounds_from_config(cp, section, tag, param)\n        if bounds_required and bounds is None:\n            raise ValueError(\"min and/or max missing for parameter %s\"%(\n                param))\n        dist_args[param] = bounds\n\n    # add any additional options that user put in that section\n    for key in cp.options(\"-\".join([section, tag])):\n\n        # ignore options that are already included\n        if key in special_args:\n            continue\n\n        # check if option can be cast as a float\n        val = cp.get_opt_tag(section, key, tag)\n        try:\n            val = float(val)\n        except ValueError:\n            pass\n\n        # add option\n        dist_args.update({key:val})\n\n    dist_args.update(additional_opts)\n\n    # construction distribution and add to list\n    return cls(**dist_args)", "response": "Returns a bounded version of the given class based on a configuration file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\napplies any boundary conditions to the given values.", "response": "def apply_boundary_conditions(self, **kwargs):\n        \"\"\"Applies any boundary conditions to the given values (e.g., applying\n        cyclic conditions, and/or reflecting values off of boundaries). This\n        is done by running `apply_conditions` of each bounds in self on the\n        corresponding value. See `boundaries.Bounds.apply_conditions` for\n        details.\n\n        Parameters\n        ----------\n        \\**kwargs :\n            The keyword args should be the name of a parameter and value to\n            apply its boundary conditions to. The arguments need not include\n            all of the parameters in self. Any unrecognized arguments are\n            ignored.\n\n        Returns\n        -------\n        dict\n            A dictionary of the parameter names and the conditioned values.\n        \"\"\"\n        return dict([[p, self._bounds[p].apply_conditions(val)]\n                     for p,val in kwargs.items() if p in self._bounds])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_status(status):\n    if status:\n        msg = lib.DftiErrorMessage(status)\n        msg = ctypes.c_char_p(msg).value\n        raise RuntimeError(msg)", "response": "Check the status of a mkl functions and raise a python exeption if there is an error."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd an argument to the set of arguments.", "response": "def add_arg(self, arg):\n        \"\"\" Add an argument\n        \"\"\"\n        if not isinstance(arg, File):\n            arg = str(arg)\n\n        self._args += [arg]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd an argument to the command line of this job.", "response": "def add_raw_arg(self, arg):\n        \"\"\" Add an argument to the command line of this job, but do *NOT* add\n            white space between arguments. This can be added manually by adding\n            ' ' if needed\n        \"\"\"\n        if not isinstance(arg, File):\n            arg = str(arg)\n\n        self._raw_options += [arg]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_opt(self, opt, value=None):\n        if value is not None:\n            if not isinstance(value, File):\n                value = str(value)\n            self._options += [opt, value]\n        else:\n            self._options += [opt]", "response": "Add an option to the list of available options."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds output to self. _outputs", "response": "def _add_output(self, out):\n        \"\"\" Add as destination of output data\n        \"\"\"\n        self._outputs += [out]\n        out.node = self\n        out._set_as_output_of(self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_input_opt(self, opt, inp):\n        self.add_opt(opt, inp._dax_repr())\n        self._add_input(inp)", "response": "Add an option that determines an input\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd an option that determines an output", "response": "def add_output_opt(self, opt, out):\n        \"\"\" Add an option that determines an output\n        \"\"\"\n        self.add_opt(opt, out._dax_repr())\n        self._add_output(out)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_output_list_opt(self, opt, outputs):\n        self.add_opt(opt)\n        for out in outputs:\n            self.add_opt(out)\n            self._add_output(out)", "response": "Add an option that determines a list of outputs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_input_list_opt(self, opt, inputs):\n        self.add_opt(opt)\n        for inp in inputs:\n            self.add_opt(inp)\n            self._add_input(inp)", "response": "Add an option that determines a list of inputs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_list_opt(self, opt, values):\n        self.add_opt(opt)\n        for val in values:\n            self.add_opt(val)", "response": "Add an option with a list of non - file parameters."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding an input as an argument to the log.", "response": "def add_input_arg(self, inp):\n        \"\"\" Add an input as an argument\n        \"\"\"\n        self.add_arg(inp._dax_repr())\n        self._add_input(inp)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding an output as an argument", "response": "def add_output_arg(self, out):\n        \"\"\" Add an output as an argument\n        \"\"\"\n        self.add_arg(out._dax_repr())\n        self._add_output(out)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd an option and return a new file handle", "response": "def new_output_file_opt(self, opt, name):\n        \"\"\" Add an option and return a new file handle\n        \"\"\"\n        fil = File(name)\n        self.add_output_opt(opt, fil)\n        return fil"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a profile to this node at the DAX level.", "response": "def add_profile(self, namespace, key, value, force=False):\n        \"\"\" Add profile information to this node at the DAX level\n        \"\"\"\n        try:\n            entry = dax.Profile(namespace, key, value)\n            self._dax_node.addProfile(entry)\n        except dax.DuplicateError:\n            if force:\n                # Replace with the new key\n                self._dax_node.removeProfile(entry)\n                self._dax_node.addProfile(entry)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_workflow(self, workflow):\n        workflow.in_workflow = self\n        self.sub_workflows += [workflow]\n\n        node = workflow.as_job\n        self._adag.addJob(node)\n\n        node.file.PFN(os.path.join(os.getcwd(), node.file.name), site='local')\n        self._adag.addFile(node.file)\n\n        for inp in self._external_workflow_inputs:\n            workflow._make_root_dependency(inp.node)\n\n        return self", "response": "Add a sub - workflow to this workflow."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a node to the workflow.", "response": "def add_node(self, node):\n        \"\"\" Add a node to this workflow\n\n        This function adds nodes to the workflow. It also determines\n        parent/child relations from the DataStorage inputs to this job.\n\n        Parameters\n        ----------\n        node : pycbc.workflow.pegasus_workflow.Node\n            A node that should be executed as part of this workflow.\n        \"\"\"\n        node._finalize()\n        node.in_workflow = self\n        self._adag.addJob(node._dax_node)\n\n        # Determine the parent child relationships based on the inputs that\n        # this node requires.\n        added_nodes = []\n        for inp in node._inputs:\n            if inp.node is not None and inp.node.in_workflow == self:\n                if inp.node not in added_nodes:\n                    parent = inp.node._dax_node\n                    child = node._dax_node\n                    dep = dax.Dependency(parent=parent, child=child)\n                    self._adag.addDependency(dep)\n                    added_nodes.append(inp.node)\n\n            elif inp.node is not None and not inp.node.in_workflow:\n                raise ValueError('Parents of this node must be added to the '\n                                 'workflow first.')\n\n            elif inp.node is None and not inp.workflow_input:\n                self._inputs += [inp]\n                inp.workflow_input = True\n\n            elif inp.node is not None and inp.node.in_workflow != self and inp not in self._inputs:\n                self._inputs += [inp]\n                self._external_workflow_inputs += [inp]\n\n\n        # Record the outputs that this node generates\n        self._outputs += node._outputs\n\n        # Record the executable that this node uses\n        if not node.executable.in_workflow:\n            node.executable.in_workflow = True\n            self._executables += [node.executable]\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting this workflow to the DAX file.", "response": "def save(self, filename=None, tc=None):\n        \"\"\" Write this workflow to DAX file\n        \"\"\"\n        if filename is None:\n            filename = self.filename\n\n        for sub in self.sub_workflows:\n            sub.save()\n\n        # FIXME this is ugly as pegasus 4.9.0 does not support the full\n        # transformation catalog in the DAX. I have asked Karan to fix this so\n        # that executables and containers can be specified in the DAX itself.\n        # Karan says that XML is going away in Pegasus 5.x and so this code\n        # will need to be re-written anyway.\n        #\n        # the transformation catalog is written in the same directory as the\n        # DAX.  pycbc_submit_dax needs to know this so that the right\n        # transformation catalog is used when the DAX is planned.\n        if tc is None:\n            tc = '{}.tc.txt'.format(filename)\n        p = os.path.dirname(tc)\n        f = os.path.basename(tc)\n        if not p:\n            p = '.'\n\n        tc = TransformationCatalog(p, f)\n\n        for e in self._adag.executables.copy():\n            tc.add(e)\n            try:\n                tc.add_container(e.container)\n            except:\n                pass\n            self._adag.removeExecutable(e)\n\n        f = open(filename, \"w\")\n        self._adag.writeXML(f)\n        tc.write()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef has_pfn(self, url, site=None):\n        curr_pfn = dax.PFN(url, site)\n        return self.hasPFN(curr_pfn)", "response": "Wrapper of the pegasus hasPFN function that allows to call hasPFN outside of specific pegasus functions."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntake a path and returns a File object with the path as the PFN.", "response": "def from_path(cls, path):\n        \"\"\"Takes a path and returns a File object with the path as the PFN.\"\"\"\n        urlparts = urlparse.urlsplit(path)\n        site = 'nonlocal'\n        if (urlparts.scheme == '' or urlparts.scheme == 'file'):\n            if os.path.isfile(urlparts.path):\n                path = os.path.abspath(urlparts.path)\n                path = urlparse.urljoin('file:',\n                                        urllib.pathname2url(path)) \n                site = 'local'\n\n        fil = File(os.path.basename(path))\n        fil.PFN(path, site)\n        return fil"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninitializes a model from the given config file.", "response": "def read_from_config(cp, **kwargs):\n    \"\"\"Initializes a model from the given config file.\n\n    The section must have a ``name`` argument. The name argument corresponds to\n    the name of the class to initialize.\n\n    Parameters\n    ----------\n    cp : WorkflowConfigParser\n        Config file parser to read.\n    \\**kwargs :\n        All other keyword arguments are passed to the ``from_config`` method\n        of the class specified by the name argument.\n\n    Returns\n    -------\n    cls\n        The initialized model.\n    \"\"\"\n    # use the name to get the distribution\n    name = cp.get(\"model\", \"name\")\n    return models[name].from_config(cp, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _loglr(self):\n        # calculate <d-h|d-h> = <h|h> - 2<h|d> + <d|d> up to a constant\n        p = self.current_params.copy()\n        p.update(self.static_params)\n\n        if self.time is None:\n            self.time = p['tc']\n\n        shloglr = hhloglr = 0\n        for ifo in self.sh:\n            fp, fc = self.det[ifo].antenna_pattern(p['ra'], p['dec'],\n                                                   p['polarization'],\n                                                   self.time)\n            dt = self.det[ifo].time_delay_from_earth_center(p['ra'],\n                                                            p['dec'],\n                                                            self.time)\n            ip = numpy.cos(p['inclination'])\n            ic = 0.5 * (1.0 + ip * ip)\n            htf = (fp * ip + 1.0j * fc * ic) / p['distance']\n\n            sh = self.sh[ifo].at_time(p['tc'] + dt) * htf\n            shloglr += sh\n            hhloglr += self.hh[ifo] * abs(htf) ** 2.0\n\n        vloglr = numpy.log(scipy.special.i0e(abs(shloglr)))\n        vloglr += abs(shloglr) + hhloglr\n\n        return float(vloglr)", "response": "r Calculates the log likelihood ratio of the current object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of PyCBC distribution instances for a section in the given configuration file.", "response": "def read_distributions_from_config(cp, section=\"prior\"):\n    \"\"\"Returns a list of PyCBC distribution instances for a section in the\n    given configuration file.\n\n    Parameters\n    ----------\n    cp : WorflowConfigParser\n        An open config file to read.\n    section : {\"prior\", string}\n        Prefix on section names from which to retrieve the distributions.\n\n    Returns\n    -------\n    list\n        A list of the parsed distributions.\n    \"\"\"\n    dists = []\n    variable_args = []\n    for subsection in cp.get_subsections(section):\n        name = cp.get_opt_tag(section, \"name\", subsection)\n        dist = distribs[name].from_config(cp, section, subsection)\n        if set(dist.params).isdisjoint(variable_args):\n            dists.append(dist)\n            variable_args += dist.params\n        else:\n            raise ValueError(\"Same parameter in more than one distribution.\")\n    return dists"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _convert_liststring_to_list(lstring):\n    if lstring[0]=='[' and lstring[-1]==']':\n        lstring = [str(lstring[1:-1].split(',')[n].strip().strip(\"'\"))\n                      for n in range(len(lstring[1:-1].split(',')))]\n    return lstring", "response": "Checks if an argument of the configuration file is a string of a list\n    and returns the corresponding list of strings."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_params_from_config(cp, prior_section='prior',\n                            vargs_section='variable_args',\n                            sargs_section='static_args'):\n    \"\"\"Loads static and variable parameters from a configuration file.\n\n    Parameters\n    ----------\n    cp : WorkflowConfigParser\n        An open config parser to read from.\n    prior_section : str, optional\n        Check that priors exist in the given section. Default is 'prior.'\n    vargs_section : str, optional\n        The section to get the parameters that will be varied/need priors\n        defined for them. Default is 'variable_args'.\n    sargs_section : str, optional\n        The section to get the parameters that will remain fixed. Default is\n        'static_args'.\n\n    Returns\n    -------\n    variable_args : list\n        The names of the parameters to vary in the PE run.\n    static_args : dict\n        Dictionary of names -> values giving the parameters to keep fixed.\n    \"\"\"\n    # sanity check that each parameter in [variable_args] has a priors section\n    variable_args = cp.options(vargs_section)\n    subsections = cp.get_subsections(prior_section)\n    tags = set([p for tag in subsections for p in tag.split('+')])\n    missing_prior = set(variable_args) - tags\n    if any(missing_prior):\n        raise KeyError(\"You are missing a priors section in the config file \"\n                       \"for parameter(s): {}\".format(', '.join(missing_prior)))\n    # get static args\n    try:\n        static_args = dict([(key, cp.get_opt_tags(sargs_section, key, []))\n                           for key in cp.options(sargs_section)])\n    except _ConfigParser.NoSectionError:\n        static_args = {}\n    # try converting values to float\n    for key in static_args:\n        val = static_args[key]\n        try:\n            # the following will raise a ValueError if it cannot be cast to\n            # float (as we would expect for string arguments)\n            static_args[key] = float(val)\n        except ValueError:\n            # try converting to a list of strings; this function will just\n            # return val if it does not begin (end) with [ (])\n            static_args[key] = _convert_liststring_to_list(val)\n    return variable_args, static_args", "response": "Loads static and variable parameters from a workflow config file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload parameter constraints from a workflow config file.", "response": "def read_constraints_from_config(cp, transforms=None,\n                                 constraint_section='constraint'):\n    \"\"\"Loads parameter constraints from a configuration file.\n\n    Parameters\n    ----------\n    cp : WorkflowConfigParser\n        An open config parser to read from.\n    transforms : list, optional\n        List of transforms to apply to parameters before applying constraints.\n    constraint_section : str, optional\n        The section to get the constraints from. Default is 'constraint'.\n\n    Returns\n    -------\n    list\n        List of ``Constraint`` objects. Empty if no constraints were provided.\n    \"\"\"\n    cons = []\n    for subsection in cp.get_subsections(constraint_section):\n        name = cp.get_opt_tag(constraint_section, \"name\", subsection)\n        constraint_arg = cp.get_opt_tag(\n            constraint_section, \"constraint_arg\", subsection)\n        # get any other keyword arguments\n        kwargs = {}\n        section = constraint_section + \"-\" + subsection\n        extra_opts = [key for key in cp.options(section)\n                      if key not in [\"name\", \"constraint_arg\"]]\n        for key in extra_opts:\n            val = cp.get(section, key)\n            if key == \"required_parameters\":\n                val = val.split(_VARARGS_DELIM)\n            else:\n                try:\n                    val = float(val)\n                except ValueError:\n                    pass\n            kwargs[key] = val\n        cons.append(constraints.constraints[name](constraint_arg,\n                                                  transforms=transforms,\n                                                  **kwargs))\n\n    return cons"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding options for injfilterrejector to the specified parser.", "response": "def insert_injfilterrejector_option_group(parser):\n    \"\"\"Add options for injfilterrejector to executable.\"\"\"\n    injfilterrejector_group = \\\n        parser.add_argument_group(_injfilterrejector_group_help)\n    curr_arg = \"--injection-filter-rejector-chirp-time-window\"\n    injfilterrejector_group.add_argument(curr_arg, type=float, default=None,\n                                         help=_injfilterer_cthresh_help)\n    curr_arg = \"--injection-filter-rejector-match-threshold\"\n    injfilterrejector_group.add_argument(curr_arg, type=float, default=None,\n                                         help=_injfilterer_mthresh_help)\n    curr_arg = \"--injection-filter-rejector-coarsematch-deltaf\"\n    injfilterrejector_group.add_argument(curr_arg, type=float, default=1.,\n                                         help=_injfilterer_deltaf_help)\n    curr_arg = \"--injection-filter-rejector-coarsematch-fmax\"\n    injfilterrejector_group.add_argument(curr_arg, type=float, default=256.,\n                                         help=_injfilterer_fmax_help)\n    curr_arg = \"--injection-filter-rejector-seg-buffer\"\n    injfilterrejector_group.add_argument(curr_arg, type=int, default=10,\n                                         help=_injfilterer_buffer_help)\n    curr_arg = \"--injection-filter-rejector-f-lower\"\n    injfilterrejector_group.add_argument(curr_arg, type=int, default=None,\n                                         help=_injfilterer_flower_help)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_cli(cls, opt):\n        injection_file = opt.injection_file\n        chirp_time_window = \\\n            opt.injection_filter_rejector_chirp_time_window\n        match_threshold = opt.injection_filter_rejector_match_threshold\n        coarsematch_deltaf = opt.injection_filter_rejector_coarsematch_deltaf\n        coarsematch_fmax = opt.injection_filter_rejector_coarsematch_fmax\n        seg_buffer = opt.injection_filter_rejector_seg_buffer\n        if opt.injection_filter_rejector_f_lower is not None:\n            f_lower = opt.injection_filter_rejector_f_lower\n        else:\n            # NOTE: Uses main low-frequency cutoff as default option. This may\n            #       need some editing if using this in multi_inspiral, which I\n            #       leave for future work, or if this is being used in another\n            #       code which doesn't have --low-frequency-cutoff\n            f_lower = opt.low_frequency_cutoff\n        return cls(injection_file, chirp_time_window, match_threshold,\n                   f_lower, coarsematch_deltaf=coarsematch_deltaf,\n                   coarsematch_fmax=coarsematch_fmax,\n                   seg_buffer=seg_buffer)", "response": "Create an InjFilterRejector instance from command - line options."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generate_short_inj_from_inj(self, inj_waveform, simulation_id):\n        if not self.enabled:\n            # Do nothing!\n            return\n        if simulation_id in self.short_injections:\n            err_msg = \"An injection with simulation id \"\n            err_msg += str(simulation_id)\n            err_msg += \" has already been added. This suggests \"\n            err_msg += \"that your injection file contains injections with \"\n            err_msg += \"duplicate simulation_ids. This is not allowed.\"\n            raise ValueError(err_msg)\n        curr_length = len(inj_waveform)\n        new_length = int(nearest_larger_binary_number(curr_length))\n        # Don't want length less than 1/delta_f\n        while new_length * inj_waveform.delta_t < 1./self.coarsematch_deltaf:\n            new_length = new_length * 2\n        inj_waveform.resize(new_length)\n        inj_tilde = inj_waveform.to_frequencyseries()\n        # Dynamic range is important here!\n        inj_tilde_np = inj_tilde.numpy() * DYN_RANGE_FAC\n        delta_f = inj_tilde.get_delta_f()\n        new_freq_len = int(self.coarsematch_fmax / delta_f + 1)\n        # This shouldn't be a problem if injections are generated at\n        # 16384 Hz ... It is only a problem of injection sample rate\n        # gives a lower Nyquist than the trunc_f_max. If this error is\n        # ever raised one could consider zero-padding the injection.\n        assert(new_freq_len <= len(inj_tilde))\n        df_ratio = int(self.coarsematch_deltaf/delta_f)\n        inj_tilde_np = inj_tilde_np[:new_freq_len:df_ratio]\n        new_inj = FrequencySeries(inj_tilde_np, dtype=np.complex64,\n                                  delta_f=self.coarsematch_deltaf)\n        self.short_injections[simulation_id] = new_inj", "response": "Generate and a store a truncated representation of inj_waveform."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntests if injections in that template are worth filtering with template.", "response": "def template_segment_checker(self, bank, t_num, segment, start_time):\n        \"\"\"Test if injections in segment are worth filtering with template.\n\n        Using the current template, current segment, and injections within that\n        segment. Test if the injections and sufficiently \"similar\" to any of\n        the injections to justify actually performing a matched-filter call.\n        Ther are two parts to this test: First we check if the chirp time of\n        the template is within a provided window of any of the injections. If\n        not then stop here, it is not worth filtering this template, segment\n        combination for this injection set. If this check passes we compute a\n        match between a coarse representation of the template and a coarse\n        representation of each of the injections. If that match is above a\n        user-provided value for any of the injections then filtering can\n        proceed. This is currently only available if using frequency-domain\n        templates.\n\n        Parameters\n        -----------\n        FIXME\n\n        Returns\n        --------\n        FIXME\n        \"\"\"\n        if not self.enabled:\n            # If disabled, always filter (ie. return True)\n            return True\n\n        # Get times covered by segment analyze\n        sample_rate = 2. * (len(segment) - 1) * segment.delta_f\n        cum_ind = segment.cumulative_index\n        diff = segment.analyze.stop - segment.analyze.start\n        seg_start_time = cum_ind / sample_rate + start_time\n        seg_end_time = (cum_ind + diff) / sample_rate + start_time\n        # And add buffer\n        seg_start_time = seg_start_time - self.seg_buffer\n        seg_end_time = seg_end_time + self.seg_buffer\n\n        # Chirp time test\n        if self.chirp_time_window is not None:\n            m1 = bank.table[t_num]['mass1']\n            m2 = bank.table[t_num]['mass2']\n            tau0_temp, _ = mass1_mass2_to_tau0_tau3(m1, m2, self.f_lower)\n            for inj in self.injection_params.table:\n                end_time = inj.geocent_end_time + \\\n                    1E-9 * inj.geocent_end_time_ns\n                if not(seg_start_time <= end_time <= seg_end_time):\n                    continue\n                tau0_inj, _ = \\\n                    mass1_mass2_to_tau0_tau3(inj.mass1, inj.mass2,\n                                             self.f_lower)\n                tau_diff = abs(tau0_temp - tau0_inj)\n                if tau_diff <= self.chirp_time_window:\n                    break\n            else:\n                # Get's here if all injections are outside chirp-time window\n                return False\n\n        # Coarse match test\n        if self.match_threshold:\n            if self._short_template_mem is None:\n                # Set the memory for the short templates\n                wav_len = 1 + int(self.coarsematch_fmax /\n                                  self.coarsematch_deltaf)\n                self._short_template_mem = zeros(wav_len, dtype=np.complex64)\n\n            # Set the current short PSD to red_psd\n            try:\n                red_psd = self._short_psd_storage[id(segment.psd)]\n            except KeyError:\n                # PSD doesn't exist yet, so make it!\n                curr_psd = segment.psd.numpy()\n                step_size = int(self.coarsematch_deltaf / segment.psd.delta_f)\n                max_idx = int(self.coarsematch_fmax / segment.psd.delta_f) + 1\n                red_psd_data = curr_psd[:max_idx:step_size]\n                red_psd = FrequencySeries(red_psd_data, #copy=False,\n                                          delta_f=self.coarsematch_deltaf)\n                self._short_psd_storage[id(curr_psd)] = red_psd\n\n            # Set htilde to be the current short template\n            if not t_num == self._short_template_id:\n                # Set the memory for the short templates if unset\n                if self._short_template_mem is None:\n                    wav_len = 1 + int(self.coarsematch_fmax /\n                                      self.coarsematch_deltaf)\n                    self._short_template_mem = zeros(wav_len,\n                                                     dtype=np.complex64)\n                # Generate short waveform\n                htilde = bank.generate_with_delta_f_and_max_freq(\n                    t_num, self.coarsematch_fmax, self.coarsematch_deltaf,\n                    low_frequency_cutoff=bank.table[t_num].f_lower,\n                    cached_mem=self._short_template_mem)\n                self._short_template_id = t_num\n                self._short_template_wav = htilde\n            else:\n                htilde = self._short_template_wav\n\n            for inj in self.injection_params.table:\n                end_time = inj.geocent_end_time + \\\n                    1E-9 * inj.geocent_end_time_ns\n                if not(seg_start_time < end_time < seg_end_time):\n                    continue\n                curr_inj = self.short_injections[inj.simulation_id]\n                o, _ = match(htilde, curr_inj, psd=red_psd,\n                             low_frequency_cutoff=self.f_lower)\n                if o > self.match_threshold:\n                    break\n            else:\n                # Get's here if all injections are outside match threshold\n                return False\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfitting a distribution of discrete values above a given threshold.", "response": "def fit_above_thresh(distr, vals, thresh=None):\n    \"\"\"\n    Maximum likelihood fit for the coefficient alpha\n\n    Fitting a distribution of discrete values above a given threshold.\n    Exponential  p(x) = alpha exp(-alpha (x-x_t))\n    Rayleigh     p(x) = alpha x exp(-alpha (x**2-x_t**2)/2)\n    Power        p(x) = ((alpha-1)/x_t) (x/x_t)**-alpha\n    Values below threshold will be discarded.\n    If no threshold is specified the minimum sample value will be used.\n\n    Parameters\n    ----------\n    distr : {'exponential', 'rayleigh', 'power'}\n        Name of distribution\n    vals : sequence of floats\n        Values to fit\n    thresh : float\n        Threshold to apply before fitting; if None, use min(vals)\n\n    Returns\n    -------\n    alpha : float\n        Fitted value\n    sigma_alpha : float\n        Standard error in fitted value\n    \"\"\"\n    vals = numpy.array(vals)\n    if thresh is None:\n        thresh = min(vals)\n    else:\n        vals = vals[vals >= thresh]\n    alpha = fitalpha_dict[distr](vals, thresh)\n    return alpha, fitstd_dict[distr](vals, alpha)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the fitted function at the requested xvals", "response": "def fit_fn(distr, xvals, alpha, thresh):\n    \"\"\"\n    The fitted function normalized to 1 above threshold\n\n    To normalize to a given total count multiply by the count.\n\n    Parameters\n    ----------\n    xvals : sequence of floats\n        Values where the function is to be evaluated\n    alpha : float\n        The fitted parameter\n    thresh : float\n        Threshold value applied to fitted values\n\n    Returns\n    -------\n    fit : array of floats\n        Fitted function at the requested xvals\n    \"\"\"\n    xvals = numpy.array(xvals)\n    fit = fitfn_dict[distr](xvals, alpha, thresh)\n    # set fitted values below threshold to 0\n    numpy.putmask(fit, xvals < thresh, 0.)\n    return fit"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cum_fit(distr, xvals, alpha, thresh):\n    xvals = numpy.array(xvals)\n    cum_fit = cum_fndict[distr](xvals, alpha, thresh)\n    # set fitted values below threshold to 0\n    numpy.putmask(cum_fit, xvals < thresh, 0.)\n    return cum_fit", "response": "Returns the cumulative CDF of the fitted function at a given value at a given distance."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef tail_threshold(vals, N=1000):\n    vals = numpy.array(vals)\n    if len(vals) < N:\n        raise RuntimeError('Not enough input values to determine threshold')\n    vals.sort()\n    return min(vals[-N:])", "response": "Determine a threshold above which there are N louder values"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef which_bin(par, minpar, maxpar, nbins, log=False):\n    assert (par >= minpar and par <= maxpar)\n    if log:\n        par, minpar, maxpar = numpy.log(par), numpy.log(minpar), numpy.log(maxpar)\n    # par lies some fraction of the way between min and max\n    frac = float(par - minpar) / float(maxpar - minpar)\n    # binind then lies between 0 and nbins - 1\n    binind = int(frac * nbins)\n    # corner case\n    if par == maxpar:\n        binind = nbins - 1\n    return binind", "response": "Helper function to find the index of a parameter value in a set of bins."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef compute_acf(cls, filename, start_index=None, end_index=None,\n                    per_walker=False, walkers=None, parameters=None,\n                    temps=None):\n        \"\"\"Computes the autocorrleation function of the model params in the\n        given file.\n\n        By default, parameter values are averaged over all walkers at each\n        iteration. The ACF is then calculated over the averaged chain for each\n        temperature. An ACF per-walker will be returned instead if\n        ``per_walker=True``.\n\n        Parameters\n        -----------\n        filename : str\n            Name of a samples file to compute ACFs for.\n        start_index : {None, int}\n            The start index to compute the acl from. If None, will try to use\n            the number of burn-in iterations in the file; otherwise, will start\n            at the first sample.\n        end_index : {None, int}\n            The end index to compute the acl to. If None, will go to the end\n            of the current iteration.\n        per_walker : optional, bool\n            Return the ACF for each walker separately. Default is False.\n        walkers : optional, int or array\n            Calculate the ACF using only the given walkers. If None (the\n            default) all walkers will be used.\n        parameters : optional, str or array\n            Calculate the ACF for only the given parameters. If None (the\n            default) will calculate the ACF for all of the model params.\n        temps : optional, (list of) int or 'all'\n            The temperature index (or list of indices) to retrieve. If None\n            (the default), the ACF will only be computed for the coldest (= 0)\n            temperature chain. To compute an ACF for all temperates pass 'all',\n            or a list of all of the temperatures.\n\n        Returns\n        -------\n        dict :\n            Dictionary of arrays giving the ACFs for each parameter. If\n            ``per-walker`` is True, the arrays will have shape\n            ``ntemps x nwalkers x niterations``. Otherwise, the returned array\n            will have shape ``ntemps x niterations``.\n        \"\"\"\n        acfs = {}\n        with cls._io(filename, 'r') as fp:\n            if parameters is None:\n                parameters = fp.variable_params\n            if isinstance(parameters, str) or isinstance(parameters, unicode):\n                parameters = [parameters]\n            if isinstance(temps, int):\n                temps = [temps]\n            elif temps == 'all':\n                temps = numpy.arange(fp.ntemps)\n            elif temps is None:\n                temps = [0]\n            for param in parameters:\n                subacfs = []\n                for tk in temps:\n                    if per_walker:\n                        # just call myself with a single walker\n                        if walkers is None:\n                            walkers = numpy.arange(fp.nwalkers)\n                        arrays = [cls.compute_acfs(filename,\n                                                   start_index=start_index,\n                                                   end_index=end_index,\n                                                   per_walker=False,\n                                                   walkers=ii,\n                                                   parameters=param,\n                                                   temps=tk)[param][0, :]\n                                  for ii in walkers]\n                        # we'll stack all of the walker arrays to make a single\n                        # nwalkers x niterations array; when these are stacked\n                        # below, we'll get a ntemps x nwalkers x niterations\n                        # array\n                        subacfs.append(numpy.vstack(arrays))\n                    else:\n                        samples = fp.read_raw_samples(\n                            param, thin_start=start_index,\n                            thin_interval=1, thin_end=end_index,\n                            walkers=walkers, temps=tk, flatten=False)[param]\n                        # contract the walker dimension using the mean, and\n                        # flatten the (length 1) temp dimension\n                        samples = samples.mean(axis=1)[0, :]\n                        thisacf = autocorrelation.calculate_acf(\n                            samples).numpy()\n                        subacfs.append(thisacf)\n                # stack the temperatures\n                acfs[param] = numpy.stack(subacfs)\n        return acfs", "response": "This method calculates the autocorrelation function of the model parameters in the given file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the autocorrelation length for all model parameters and temperatures in the given file.", "response": "def compute_acl(cls, filename, start_index=None, end_index=None,\n                    min_nsamples=10):\n        \"\"\"Computes the autocorrleation length for all model params and\n        temperatures in the given file.\n\n        Parameter values are averaged over all walkers at each iteration and\n        temperature.  The ACL is then calculated over the averaged chain.\n\n        Parameters\n        -----------\n        filename : str\n            Name of a samples file to compute ACLs for.\n        start_index : {None, int}\n            The start index to compute the acl from. If None, will try to use\n            the number of burn-in iterations in the file; otherwise, will start\n            at the first sample.\n        end_index : {None, int}\n            The end index to compute the acl to. If None, will go to the end\n            of the current iteration.\n        min_nsamples : int, optional\n            Require a minimum number of samples to compute an ACL. If the\n            number of samples per walker is less than this, will just set to\n            ``inf``. Default is 10.\n\n        Returns\n        -------\n        dict\n            A dictionary of ntemps-long arrays of the ACLs of each parameter.\n        \"\"\"\n        acls = {}\n        with cls._io(filename, 'r') as fp:\n            if end_index is None:\n                end_index = fp.niterations\n            tidx = numpy.arange(fp.ntemps)\n            for param in fp.variable_params:\n                these_acls = numpy.zeros(fp.ntemps)\n                for tk in tidx:\n                    samples = fp.read_raw_samples(\n                        param, thin_start=start_index, thin_interval=1,\n                        thin_end=end_index, temps=tk, flatten=False)[param]\n                    # contract the walker dimension using the mean, and flatten\n                    # the (length 1) temp dimension\n                    samples = samples.mean(axis=1)[0, :]\n                    if samples.size < min_nsamples:\n                        acl = numpy.inf\n                    else:\n                        acl = autocorrelation.calculate_acl(samples)\n                    if acl <= 0:\n                        acl = numpy.inf\n                    these_acls[tk] = acl\n                acls[param] = these_acls\n        return acls"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pycbc_compile_function(code,arg_names,local_dict,global_dict,\n                     module_dir,\n                     compiler='',\n                     verbose=1,\n                     support_code=None,\n                     headers=None,\n                     customize=None,\n                     type_converters=None,\n                     auto_downcast=1,\n                     **kw):\n    \"\"\" Dummy wrapper around scipy weave compile to implement file locking\n    \"\"\"\n    headers = [] if headers is None else headers\n    lockfile_dir = os.environ['PYTHONCOMPILED']\n    lockfile_name = os.path.join(lockfile_dir, 'code_lockfile')\n    logging.info(\"attempting to aquire lock '%s' for \"\n                 \"compiling code\" % lockfile_name)\n    if not os.path.exists(lockfile_dir):\n        os.makedirs(lockfile_dir)\n    lockfile = open(lockfile_name, 'w')\n    fcntl.lockf(lockfile, fcntl.LOCK_EX)\n    logging.info(\"we have aquired the lock\")\n\n    func = _compile_function(code,arg_names, local_dict, global_dict,\n                     module_dir, compiler, verbose,\n                     support_code, headers, customize,\n                     type_converters,\n                     auto_downcast, **kw)\n\n    fcntl.lockf(lockfile, fcntl.LOCK_UN)\n    logging.info(\"the lock has been released\")\n\n    return func", "response": "Wrapper around scipy weave compile function to implement file locking"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef insert_weave_option_group(parser):\n    optimization_group = parser.add_argument_group(\"Options for controlling \"\n                                   \"weave\")\n\n    optimization_group.add_argument(\"--per-process-weave-cache\",\n                    action=\"store_true\",\n                    default=False,\n                    help=\"\"\"If given, each process will use a separate directory\n                         for weave compilation.  This is slower, but safer if\n                         several instances may be starting on the same machine at\n                         the same time.\"\"\")\n\n    optimization_group.add_argument(\"--clear-weave-cache-at-start\",\n                    action=\"store_true\",\n                    default=False,\n                    help=\"If given, delete the contents of the weave cache \"\n                         \"when the process starts\")\n\n    optimization_group.add_argument(\"--clear-weave-cache-at-end\",\n                    action=\"store_true\",\n                    default=False,\n                    help=\"If given, delete the contents of the weave cache \"\n                         \"when the process exits\")\n\n    optimization_group.add_argument(\"--fixed-weave-cache\",\n                    action=\"store_true\",\n                    default=False,\n                    help=\"If given, use fixed directory PWD/pycbc_inspiral for \"\n                         \" the weave cache\")", "response": "Adds the options used to specify weave options."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _clear_weave_cache():\n\n    cache_dir = os.environ['PYTHONCOMPILED']\n    if os.path.exists(cache_dir):\n        shutil.rmtree(cache_dir)\n    logging.info(\"Cleared weave cache %s\", cache_dir)", "response": "Deletes the weave cache specified in the environment variable PYTHONCOMPILED"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef convert_bank_to_hdf(workflow, xmlbank, out_dir, tags=None):\n    if tags is None:\n        tags = []\n    #FIXME, make me not needed\n    if len(xmlbank) > 1:\n        raise ValueError('Can only convert a single template bank')\n\n    logging.info('convert template bank to HDF')\n    make_analysis_dir(out_dir)\n    bank2hdf_exe = PyCBCBank2HDFExecutable(workflow.cp, 'bank2hdf',\n                                            ifos=workflow.ifos,\n                                            out_dir=out_dir, tags=tags)\n    bank2hdf_node = bank2hdf_exe.create_node(xmlbank[0])\n    workflow.add_node(bank2hdf_node)\n    return bank2hdf_node.output_files", "response": "Convert a template bank to HDF format"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a single inspiral trigger file to hdf5.", "response": "def convert_trig_to_hdf(workflow, hdfbank, xml_trigger_files, out_dir, tags=None):\n    \"\"\"Return the list of hdf5 trigger files outputs\"\"\"\n    if tags is None:\n        tags = []\n    #FIXME, make me not needed\n    logging.info('convert single inspiral trigger files to hdf5')\n    make_analysis_dir(out_dir)\n\n    trig_files = FileList()\n    for ifo, insp_group in zip(*xml_trigger_files.categorize_by_attr('ifo')):\n        trig2hdf_exe = PyCBCTrig2HDFExecutable(workflow.cp, 'trig2hdf',\n                                       ifos=ifo, out_dir=out_dir, tags=tags)\n        _, insp_bundles = insp_group.categorize_by_attr('segment')\n        for insps in insp_bundles:\n            trig2hdf_node =  trig2hdf_exe.create_node(insps, hdfbank[0])\n            workflow.add_node(trig2hdf_node)\n            trig_files += trig2hdf_node.output_files\n    return trig_files"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setup_interval_coinc(workflow, hdfbank, trig_files, stat_files,\n                         veto_files, veto_names, out_dir, tags=None):\n    \"\"\"\n    This function sets up exact match coincidence and background estimation\n\n    using a folded interval technique.\n    \"\"\"\n    if tags is None:\n        tags = []\n    make_analysis_dir(out_dir)\n    logging.info('Setting up coincidence')\n\n    if len(hdfbank) != 1:\n        raise ValueError('Must use exactly 1 bank file for this coincidence '\n                         'method, I got %i !' % len(hdfbank))\n    hdfbank = hdfbank[0]\n\n    if len(workflow.ifos) > 2:\n        raise ValueError('This coincidence method only supports two-ifo searches')\n\n    findcoinc_exe = PyCBCFindCoincExecutable(workflow.cp, 'coinc',\n                                             ifos=workflow.ifos,\n                                             tags=tags, out_dir=out_dir)\n\n    # Wall time knob and memory knob\n    factor = int(workflow.cp.get_opt_tags('workflow-coincidence', 'parallelization-factor', tags))\n\n    statmap_files = []\n    for veto_file, veto_name in zip(veto_files, veto_names):\n        bg_files = FileList()\n        for i in range(factor):\n            group_str = '%s/%s' % (i, factor)\n            coinc_node = findcoinc_exe.create_node(trig_files, hdfbank,\n                                                   stat_files,\n                                                   veto_file, veto_name,\n                                                   group_str,\n                                                   tags=[veto_name, str(i)])\n            bg_files += coinc_node.output_files\n            workflow.add_node(coinc_node)\n\n        statmap_files += [setup_statmap(workflow, bg_files, hdfbank, out_dir, tags=tags + [veto_name])]\n\n    logging.info('...leaving coincidence ')\n    return statmap_files", "response": "This function sets up exact match coincidence and background estimation for the specified set of veto files."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setup_multiifo_interval_coinc_inj(workflow, hdfbank, full_data_trig_files, inj_trig_files,\n                                      stat_files, background_file, veto_file, veto_name,\n                                      out_dir, pivot_ifo, fixed_ifo, tags=None):\n    \"\"\"\n    This function sets up exact match multiifo coincidence for injections\n    \"\"\"\n    if tags is None:\n        tags = []\n    make_analysis_dir(out_dir)\n    logging.info('Setting up coincidence for injections')\n\n    if len(hdfbank) != 1:\n        raise ValueError('Must use exactly 1 bank file for this coincidence '\n                         'method, I got %i !' % len(hdfbank))\n    hdfbank = hdfbank[0]\n\n    # Wall time knob and memory knob\n    factor = int(workflow.cp.get_opt_tags('workflow-coincidence', 'parallelization-factor', tags))\n\n    ffiles = {}\n    ifiles = {}\n    for ifo, ffi in zip(*full_data_trig_files.categorize_by_attr('ifo')):\n        ffiles[ifo] = ffi[0]\n    for ifo, ifi in zip(*inj_trig_files.categorize_by_attr('ifo')):\n        ifiles[ifo] = ifi[0]\n\n    injinj_files = FileList()\n    injfull_files = FileList()\n    fullinj_files = FileList()\n    # For the injfull and fullinj separation we take the pivot_ifo on one side,\n    # and the rest that are attached to the fixed_ifo on the other side\n    for ifo in ifiles:  # ifiles is keyed on ifo\n        if ifo == pivot_ifo:\n            injinj_files.append(ifiles[ifo])\n            injfull_files.append(ifiles[ifo])\n            fullinj_files.append(ffiles[ifo])\n        else:\n            injinj_files.append(ifiles[ifo])\n            injfull_files.append(ffiles[ifo])\n            fullinj_files.append(ifiles[ifo])\n\n    combo = [(injinj_files, \"injinj\"),\n             (injfull_files, \"injfull\"),\n             (fullinj_files, \"fullinj\"),\n            ]\n    bg_files = {'injinj':[], 'injfull':[], 'fullinj':[]}\n\n    for trig_files, ctag in combo:\n        findcoinc_exe = PyCBCFindMultiifoCoincExecutable(workflow.cp,\n                                                         'multiifo_coinc',\n                                                         ifos=ifiles.keys(),\n                                                         tags=tags + [ctag],\n                                                         out_dir=out_dir)\n        for i in range(factor):\n            group_str = '%s/%s' % (i, factor)\n            coinc_node = findcoinc_exe.create_node(trig_files, hdfbank,\n                                                   stat_files,\n                                                   veto_file, veto_name,\n                                                   group_str,\n                                                   pivot_ifo,\n                                                   fixed_ifo,\n                                                   tags=[veto_name, str(i)])\n\n            bg_files[ctag] += coinc_node.output_files\n            workflow.add_node(coinc_node)\n\n    logging.info('...leaving coincidence for injections')\n\n    return setup_multiifo_statmap_inj(workflow, ifiles.keys(), bg_files, background_file, out_dir, tags=tags + [veto_name])", "response": "This function sets up the multiifo injections for injections."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef select_files_by_ifo_combination(ifocomb, insps):\n    inspcomb = FileList()\n    for ifo, ifile in zip(*insps.categorize_by_attr('ifo')):\n        if ifo in ifocomb:\n            inspcomb += ifile\n\n    return inspcomb", "response": "This function selects files for a given ifo combination"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncombines the multiifo statmap files into one background file", "response": "def setup_multiifo_combine_statmap(workflow, final_bg_file_list, out_dir, tags):\n    \"\"\"\n    Combine the multiifo statmap files into one background file\n    \"\"\"\n    if tags is None:\n        tags = []\n    make_analysis_dir(out_dir)\n    logging.info('Setting up multiifo combine statmap')\n\n    cstat_exe = PyCBCMultiifoCombineStatmap(workflow.cp,\n                                            'combine_statmap',\n                                            ifos=workflow.ifos,\n                                            tags=tags,\n                                            out_dir=out_dir)\n\n    ifolist = ' '.join(workflow.ifos)\n    cluster_window = float(workflow.cp.get_opt_tags('combine_statmap',\n                                                    'cluster-window',\n                                                    tags))\n    combine_statmap_node = cstat_exe.create_node(final_bg_file_list,\n                                                 ifolist,\n                                                 cluster_window,\n                                                 tags)\n    workflow.add_node(combine_statmap_node)\n    return combine_statmap_node.output_file"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a plan for performing the FFTW transpose.", "response": "def plan_transpose(N1, N2):\n    \"\"\"\n    Create a plan for transposing internally to the pruned_FFT calculation.\n    (Alex to provide a write up with more details.)\n\n    Parameters\n    -----------\n    N1 : int\n        Number of rows.\n    N2 : int\n        Number of columns.\n\n    Returns\n    --------\n    plan : FFTWF plan\n        The plan for performing the FFTW transpose.\n    \"\"\"\n\n    rows = N1\n    cols = N2\n\n    iodim = numpy.zeros(6, dtype=numpy.int32)\n    iodim[0] = rows\n    iodim[1] = 1\n    iodim[2] = cols\n    iodim[3] = cols\n    iodim[4] = rows\n    iodim[5] = 1\n\n    N = N1*N2\n    vin = pycbc.types.zeros(N, dtype=numpy.complex64)\n    vout = pycbc.types.zeros(N, dtype=numpy.complex64)\n\n    f = float_lib.fftwf_plan_guru_dft\n    f.argtypes = [ctypes.c_int, ctypes.c_void_p, ctypes.c_int,\n                  ctypes.c_void_p, ctypes.c_void_p,\n                  ctypes.c_void_p, ctypes.c_void_p,\n                  ctypes.c_int]\n    f.restype = ctypes.c_void_p\n    return f(0, None, 2, iodim.ctypes.data, vin.ptr, vout.ptr, None, FFTW_MEASURE)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef plan_first_phase(N1, N2):\n    N = N1*N2\n    vin = pycbc.types.zeros(N, dtype=numpy.complex64)\n    vout = pycbc.types.zeros(N, dtype=numpy.complex64)\n    f = float_lib.fftwf_plan_many_dft\n    f.argtypes = [ctypes.c_int, ctypes.c_void_p, ctypes.c_int,\n                  ctypes.c_void_p, ctypes.c_void_p,\n                  ctypes.c_int, ctypes.c_int,\n                  ctypes.c_void_p, ctypes.c_void_p,\n                  ctypes.c_int, ctypes.c_int,\n                  ctypes.c_int, ctypes.c_int]\n    f.restype = ctypes.c_void_p\n    return f(1, ctypes.byref(ctypes.c_int(N2)), N1,\n             vin.ptr, None, 1, N2,\n             vout.ptr, None, 1, N2, FFTW_BACKWARD, FFTW_MEASURE)", "response": "Create a plan for the first phase of the pruned FFT operation."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef first_phase(invec, outvec, N1, N2):\n    global _theplan\n    if _theplan is None:\n        _theplan = plan_first_phase(N1, N2)\n    fexecute(_theplan, invec.ptr, outvec.ptr)", "response": "This function implements the first phase of the FFT decomposition."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef second_phase(invec, indices, N1, N2):\n    invec = numpy.array(invec.data, copy=False)\n    NI = len(indices) # pylint:disable=unused-variable\n    N1=int(N1)\n    N2=int(N2)\n    out = numpy.zeros(len(indices), dtype=numpy.complex64)\n    code = \"\"\"\n        float pi = 3.14159265359;\n        for(int i=0; i<NI; i++){\n            std::complex<double> val= (0, 0);\n            unsigned int k = indices[i];\n            int N = N1*N2;\n            float k2 = k % N2;\n            float phase_inc = 2 * pi * float(k) / float(N);\n            float sp, cp;\n\n            for (float n1=0; n1<N1; n1+=1){\n                sincosf(phase_inc * n1, &sp, &cp);\n                val += std::complex<float>(cp, sp) * invec[int(k2 + N2*n1)];\n            }\n            out[i] = val;\n        }\n    \"\"\"\n    weave.inline(code, ['N1', 'N2', 'NI', 'indices', 'out', 'invec'],\n                      )\n    return out", "response": "This function performs the second phase of the FFT decomposition that actually performs the second phase of the FFT decomposition."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fft_transpose_fftw(vec):\n    global _thetransposeplan\n    outvec = pycbc.types.zeros(len(vec), dtype=vec.dtype)\n    if _theplan is None:\n        N1, N2 = splay(vec)\n        _thetransposeplan = plan_transpose(N1, N2)\n    ftexecute(_thetransposeplan, vec.ptr, outvec.ptr)\n    return  outvec", "response": "Perform an FFT transpose from vec into outvec."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nperform a numpy transpose from vec into outvec.", "response": "def fft_transpose_numpy(vec):\n    \"\"\"\n    Perform a numpy transpose from vec into outvec.\n    (Alex to provide more details in a write-up.)\n\n    Parameters\n    -----------\n    vec : array\n        Input array.\n\n    Returns\n    --------\n    outvec : array\n        Transposed output array.\n    \"\"\"\n    N1, N2 = splay(vec)\n    return pycbc.types.Array(vec.data.copy().reshape(N2, N1).transpose().reshape(len(vec)).copy())"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef splay(vec):\n    N2 = 2 ** int(numpy.log2( len(vec) ) / 2)\n    N1 = len(vec) / N2\n    return N1, N2", "response": "Determine two lengths to split stride the input vector by\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nperforms a pruned iFFT decomposition on the given vectors and returns the final result.", "response": "def pruned_c2cifft(invec, outvec, indices, pretransposed=False):\n    \"\"\"\n    Perform a pruned iFFT, only valid for power of 2 iffts as the\n    decomposition is easier to choose. This is not a strict requirement of the\n    functions, but it is unlikely to the optimal to use anything but power\n    of 2. (Alex to provide more details in write up.\n\n    Parameters\n    -----------\n    invec : array\n        The input vector. This should be the correlation between the data and\n        the template at full sample rate. Ideally this is pre-transposed, but\n        if not this will be transposed in this function.\n    outvec : array\n        The output of the first phase of the pruned FFT.\n    indices : array of ints\n        The indexes at which to calculate the full sample-rate SNR.\n    pretransposed : boolean, default=False\n        Used to indicate whether or not invec is pretransposed.\n\n    Returns\n    --------\n    SNRs : array\n        The complex SNRs at the indexes given by indices.\n    \"\"\"\n    N1, N2 = splay(invec)\n\n    if not pretransposed:\n        invec = fft_transpose(invec)\n    first_phase(invec, outvec, N1=N1, N2=N2)\n    out = fast_second_phase(outvec, indices, N1=N1, N2=N2)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates a Fourier domain sine - Gaussian from the given parameters.", "response": "def fd_sine_gaussian(amp, quality, central_frequency, fmin, fmax, delta_f):\n    \"\"\" Generate a Fourier domain sine-Gaussian\n\n    Parameters\n    ----------\n    amp: float\n        Amplitude of the sine-Gaussian\n    quality: float\n        The quality factor\n    central_frequency: float\n        The central frequency of the sine-Gaussian\n    fmin: float\n        The minimum frequency to generate the sine-Gaussian. This determines\n        the length of the output vector.\n    fmax: float\n        The maximum frequency to generate the sine-Gaussian\n    delta_f: float\n        The size of the frequency step\n\n    Returns\n    -------\n    sg: pycbc.types.Frequencyseries\n        A Fourier domain sine-Gaussian\n    \"\"\"\n    kmin = int(round(fmin / delta_f))\n    kmax = int(round(fmax / delta_f))\n    f = numpy.arange(kmin, kmax) * delta_f\n    tau = quality / 2 / numpy.pi / central_frequency\n    A = amp * numpy.pi ** 0.5 / 2 * tau\n    d = A * numpy.exp(-(numpy.pi  * tau  * (f - central_frequency))**2.0)\n    d *= (1 + numpy.exp(-quality ** 2.0 * f / central_frequency))\n    v = numpy.zeros(kmax, dtype=numpy.complex128)\n    v[kmin:kmax] = d[:]\n    return pycbc.types.FrequencySeries(v, delta_f=delta_f)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef columns_from_file_list(file_list, columns, ifo, start, end):\n    file_list = file_list.find_output_with_ifo(ifo)\n    file_list = file_list.find_all_output_in_range(ifo, segment(start, end))\n\n    trig_dict = {}\n    for trig_file in file_list:\n        f = h5py.File(trig_file.storage_path, 'r')\n\n        time = f['end_time'][:]\n        pick = numpy.logical_and(time < end, time > start)\n        pick_loc = numpy.where(pick)[0]\n\n        for col in columns:\n            if col not in trig_dict:\n                trig_dict[col] = []\n            trig_dict[col] = numpy.concatenate([trig_dict[col], f[col][:][pick_loc]])\n\n    return trig_dict", "response": "Return columns of information stored in single detector trigger\n    files."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninterpolate n PSDs from the given numpy arrays of frequency and data.", "response": "def from_numpy_arrays(freq_data, noise_data, length, delta_f, low_freq_cutoff):\n    \"\"\"Interpolate n PSD (as two 1-dimensional arrays of frequency and data)\n    to the desired length, delta_f and low frequency cutoff.\n\n    Parameters\n    ----------\n    freq_data : array\n        Array of frequencies.\n    noise_data : array\n        PSD values corresponding to frequencies in freq_arr.\n    length : int\n        Length of the frequency series in samples.\n    delta_f : float\n        Frequency resolution of the frequency series in Herz.\n    low_freq_cutoff : float\n        Frequencies below this value are set to zero.\n\n    Returns\n    -------\n    psd : FrequencySeries\n        The generated frequency series.\n    \"\"\"\n    # Only include points above the low frequency cutoff\n    if freq_data[0] > low_freq_cutoff:\n        raise ValueError('Lowest frequency in input data '\n          ' is higher than requested low-frequency cutoff ' + str(low_freq_cutoff))\n\n    kmin = int(low_freq_cutoff / delta_f)\n    flow = kmin * delta_f\n\n    data_start = (0 if freq_data[0]==low_freq_cutoff else numpy.searchsorted(freq_data, flow) - 1)\n\n    # If the cutoff is exactly in the file, start there\n    if freq_data[data_start+1] == low_freq_cutoff:\n        data_start += 1\n\n    freq_data = freq_data[data_start:]\n    noise_data = noise_data[data_start:]\n\n    flog = numpy.log(freq_data)\n    slog = numpy.log(noise_data)\n\n    psd_interp = scipy.interpolate.interp1d(flog, slog)\n\n    kmin = int(low_freq_cutoff / delta_f)\n    psd = numpy.zeros(length, dtype=numpy.float64)\n\n    vals = numpy.log(numpy.arange(kmin, length) * delta_f)\n    psd[kmin:] =  numpy.exp(psd_interp(vals))\n\n    return FrequencySeries(psd, delta_f=delta_f)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_txt(filename, length, delta_f, low_freq_cutoff, is_asd_file=True):\n    file_data = numpy.loadtxt(filename)\n    if (file_data < 0).any() or \\\n                            numpy.logical_not(numpy.isfinite(file_data)).any():\n        raise ValueError('Invalid data in ' + filename)\n\n    freq_data = file_data[:, 0]\n    noise_data = file_data[:, 1]\n    if is_asd_file:\n        noise_data = noise_data ** 2\n\n    return from_numpy_arrays(freq_data, noise_data, length, delta_f,\n                             low_freq_cutoff)", "response": "Read an ASCII file containing one - sided ASD or PSD data and generate a frequency series."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread an ASCII file containing one - sided ASD or PSD data and generates a frequency series with the corresponding PSD.", "response": "def from_xml(filename, length, delta_f, low_freq_cutoff, ifo_string=None,\n             root_name='psd'):\n    \"\"\"Read an ASCII file containing one-sided ASD or PSD  data and generate\n    a frequency series with the corresponding PSD. The ASD or PSD data is\n    interpolated in order to match the desired resolution of the\n    generated frequency series.\n\n    Parameters\n    ----------\n    filename : string\n        Path to a two-column ASCII file. The first column must contain\n        the frequency (positive frequencies only) and the second column\n        must contain the amplitude density OR power spectral density.\n    length : int\n        Length of the frequency series in samples.\n    delta_f : float\n        Frequency resolution of the frequency series in Herz.\n    low_freq_cutoff : float\n        Frequencies below this value are set to zero.\n    ifo_string : string\n        Use the PSD in the file's PSD dictionary with this ifo string.\n        If not given and only one PSD present in the file return that, if not\n        given and multiple (or zero) PSDs present an exception will be raised.\n    root_name : string (default='psd')\n        If given use this as the root name for the PSD XML file. If this means\n        nothing to you, then it is probably safe to ignore this option.\n\n    Returns\n    -------\n    psd : FrequencySeries\n        The generated frequency series.\n\n    \"\"\"\n    import lal.series\n    from glue.ligolw import utils as ligolw_utils\n    fp = open(filename, 'r')\n    ct_handler = lal.series.PSDContentHandler\n    fileobj, _ = ligolw_utils.load_fileobj(fp, contenthandler=ct_handler)\n    psd_dict = lal.series.read_psd_xmldoc(fileobj, root_name=root_name)\n\n    if ifo_string is not None:\n        psd_freq_series = psd_dict[ifo_string]\n    else:\n        if len(psd_dict.keys()) == 1:\n            psd_freq_series = psd_dict[psd_dict.keys()[0]]\n        else:\n            err_msg = \"No ifo string given and input XML file contains not \"\n            err_msg += \"exactly one PSD. Specify which PSD you want to use.\"\n            raise ValueError(err_msg)\n\n    noise_data = psd_freq_series.data.data[:]\n    freq_data = numpy.arange(len(noise_data)) * psd_freq_series.deltaF\n\n    return from_numpy_arrays(freq_data, noise_data, length, delta_f,\n                             low_freq_cutoff)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npadding a TimeSeries with a length of zeros greater than its length and that the total length is the closest power of 2.", "response": "def make_padded_frequency_series(vec,filter_N=None):\n    \"\"\"Pad a TimeSeries with a length of zeros greater than its length, such\n    that the total length is the closest power of 2. This prevents the effects\n    of wraparound.\n    \"\"\"\n    if filter_N is None:\n        power = ceil(log(len(vec),2))+1\n        N = 2 ** power\n    else:\n        N = filter_N\n    n = N/2+1\n\n\n    if isinstance(vec,FrequencySeries):\n        vectilde = FrequencySeries(zeros(n, dtype=complex_same_precision_as(vec)),\n                                   delta_f=1.0,copy=False)\n\tif len(vectilde) < len(vec):\n\t    cplen = len(vectilde)\n        else:\n            cplen = len(vec)\n        vectilde[0:cplen] = vec[0:cplen]\n        delta_f = vec.delta_f\n\n\n    if isinstance(vec,TimeSeries):\n        vec_pad = TimeSeries(zeros(N),delta_t=vec.delta_t,\n                         dtype=real_same_precision_as(vec))\n        vec_pad[0:len(vec)] = vec\n        delta_f = 1.0/(vec.delta_t*N)\n        vectilde = FrequencySeries(zeros(n),delta_f=1.0,\n                               dtype=complex_same_precision_as(vec))\n        fft(vec_pad,vectilde)\n\n    vectilde = FrequencySeries(vectilde * DYN_RANGE_FAC,delta_f=delta_f,dtype=complex64)\n    return vectilde"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef insert_processing_option_group(parser):\n    processing_group = parser.add_argument_group(\"Options for selecting the\"\n                                   \" processing scheme in this program.\")\n    processing_group.add_argument(\"--processing-scheme\",\n                      help=\"The choice of processing scheme. \"\n                           \"Choices are \" + str(list(set(scheme_prefix.values()))) +\n                           \". (optional for CPU scheme) The number of \"\n                           \"execution threads \"\n                           \"can be indicated by cpu:NUM_THREADS, \"\n                           \"where NUM_THREADS \"\n                           \"is an integer. The default is a single thread. \"\n                           \"If the scheme is provided as cpu:env, the number \"\n                           \"of threads can be provided by the PYCBC_NUM_THREADS \"\n                           \"environment variable. If the environment variable \"\n                           \"is not set, the number of threads matches the number \"\n                           \"of logical cores. \",\n                      default=\"cpu\")\n\n    processing_group.add_argument(\"--processing-device-id\",\n                      help=\"(optional) ID of GPU to use for accelerated \"\n                           \"processing\",\n                      default=0, type=int)", "response": "Adds the options used to choose a processing scheme in this object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the command line options and returns a precessing scheme.", "response": "def from_cli(opt):\n    \"\"\"Parses the command line options and returns a precessing scheme.\n\n    Parameters\n    ----------\n    opt: object\n        Result of parsing the CLI with OptionParser, or any object with\n        the required attributes.\n\n    Returns\n    -------\n    ctx: Scheme\n        Returns the requested processing scheme.\n    \"\"\"\n    scheme_str = opt.processing_scheme.split(':')\n    name = scheme_str[0]\n\n    if name == \"cuda\":\n        logging.info(\"Running with CUDA support\")\n        ctx = CUDAScheme(opt.processing_device_id)\n    elif name == \"mkl\":\n        if len(scheme_str) > 1:\n            numt = scheme_str[1]\n            if numt.isdigit():\n                numt = int(numt)\n            ctx = MKLScheme(num_threads=numt)\n        else:\n            ctx = MKLScheme()\n        logging.info(\"Running with MKL support: %s threads\" % ctx.num_threads)\n    else:\n        if len(scheme_str) > 1:\n            numt = scheme_str[1]\n            if numt.isdigit():\n                numt = int(numt)\n            ctx = CPUScheme(num_threads=numt)\n        else:\n            ctx = CPUScheme()\n        logging.info(\"Running with CPU support: %s threads\" % ctx.num_threads)\n    return ctx"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef verify_processing_options(opt, parser):\n    scheme_types = scheme_prefix.values()\n    if opt.processing_scheme.split(':')[0] not in scheme_types:\n        parser.error(\"(%s) is not a valid scheme type.\")", "response": "Parses the processing scheme options and verifies that they are reasonable."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfunctioning to create an empty SnglInspiral object with all columns set to 0.", "response": "def return_empty_sngl(nones=False):\n    \"\"\"\n    Function to create a SnglInspiral object where all columns are populated\n    but all are set to values that test False (ie. strings to '', floats/ints\n    to 0, ...). This avoids errors when you try to create a table containing\n    columns you don't care about, but which still need populating. NOTE: This\n    will also produce a process_id and event_id with 0 values. For most\n    applications these should be set to their correct values.\n\n    Parameters\n    ----------\n    nones : bool (False)\n        If True, just set all columns to None.\n\n    Returns\n    --------\n    lsctables.SnglInspiral\n        The \"empty\" SnglInspiral object.\n    \"\"\"\n\n    sngl = lsctables.SnglInspiral()\n    cols = lsctables.SnglInspiralTable.validcolumns\n    if nones:\n        for entry in cols:\n            setattr(sngl, entry, None)\n    else:\n        for entry in cols.keys():\n            if cols[entry] in ['real_4','real_8']:\n                setattr(sngl,entry,0.)\n            elif cols[entry] == 'int_4s':\n                setattr(sngl,entry,0)\n            elif cols[entry] == 'lstring':\n                setattr(sngl,entry,'')\n            elif entry == 'process_id':\n                sngl.process_id = ilwd.ilwdchar(\"process:process_id:0\")\n            elif entry == 'event_id':\n                sngl.event_id = ilwd.ilwdchar(\"sngl_inspiral:event_id:0\")\n            else:\n                raise ValueError(\"Column %s not recognized\" %(entry) )\n    return sngl"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfunction to create a SearchSummary object that contains all the required columns and values set to 0.", "response": "def return_search_summary(start_time=0, end_time=0, nevents=0,\n                          ifos=None, **kwargs):\n    \"\"\"\n    Function to create a SearchSummary object where all columns are populated\n    but all are set to values that test False (ie. strings to '', floats/ints\n    to 0, ...). This avoids errors when you try to create a table containing\n    columns you don't care about, but which still need populating. NOTE: This\n    will also produce a process_id with 0 values. For most applications these\n    should be set to their correct values.\n\n    It then populates columns if given them as options.\n\n    Returns\n    --------\n    lsctables.SeachSummary\n        The \"empty\" SearchSummary object.\n    \"\"\"\n    if ifos is None:\n        ifos = []\n\n    # create an empty search summary\n    search_summary = lsctables.SearchSummary()\n    cols = lsctables.SearchSummaryTable.validcolumns\n    for entry in cols.keys():\n        if cols[entry] in ['real_4','real_8']:\n            setattr(search_summary,entry,0.)\n        elif cols[entry] == 'int_4s':\n            setattr(search_summary,entry,0)\n        elif cols[entry] == 'lstring':\n            setattr(search_summary,entry,'')\n        elif entry == 'process_id':\n            search_summary.process_id = ilwd.ilwdchar(\"process:process_id:0\")\n        else:\n            raise ValueError(\"Column %s not recognized\" %(entry) )\n\n    # fill in columns\n    if len(ifos):\n        search_summary.ifos = ','.join(ifos)\n    if nevents:\n        search_summary.nevents = nevents\n    if start_time and end_time:\n        search_summary.in_start_time = int(start_time)\n        search_summary.in_start_time_ns = int(start_time % 1 * 1e9)\n        search_summary.in_end_time = int(end_time)\n        search_summary.in_end_time_ns = int(end_time % 1 * 1e9)\n        search_summary.out_start_time = int(start_time)\n        search_summary.out_start_time_ns = int(start_time % 1 * 1e9)\n        search_summary.out_end_time = int(end_time)\n        search_summary.out_end_time_ns = int(end_time % 1 * 1e9)\n\n    return search_summary"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef convert_to_sngl_inspiral_table(params, proc_id):\n    '''\n    Convert a list of m1,m2,spin1z,spin2z values into a basic sngl_inspiral\n    table with mass and spin parameters populated and event IDs assigned\n\n    Parameters\n    -----------\n    params : iterable\n        Each entry in the params iterable should be a sequence of\n        [mass1, mass2, spin1z, spin2z] in that order\n    proc_id : ilwd char\n        Process ID to add to each row of the sngl_inspiral table\n\n    Returns\n    ----------\n    SnglInspiralTable\n        Bank of templates in SnglInspiralTable format\n    '''\n    sngl_inspiral_table = lsctables.New(lsctables.SnglInspiralTable)\n    col_names = ['mass1','mass2','spin1z','spin2z']\n\n    for values in params:\n        tmplt = return_empty_sngl()\n\n        tmplt.process_id = proc_id\n        for colname, value in zip(col_names, values):\n            setattr(tmplt, colname, value)\n        tmplt.mtotal, tmplt.eta = pnutils.mass1_mass2_to_mtotal_eta(\n            tmplt.mass1, tmplt.mass2)\n        tmplt.mchirp, _ = pnutils.mass1_mass2_to_mchirp_eta(\n            tmplt.mass1, tmplt.mass2)\n        tmplt.template_duration = 0 # FIXME\n        tmplt.event_id = sngl_inspiral_table.get_next_id()\n        sngl_inspiral_table.append(tmplt)\n\n    return sngl_inspiral_table", "response": "Convert a list of m1 m2 spin1z spin2z values into a basic sngl_inspiral table with mass and spin parameters populated and event IDs assigned."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the Gamma components needed to use the ethinca metric. At present this outputs the standard TaylorF2 metric over the end time and chirp times \\tau_0 and \\tau_3. A desirable upgrade might be to use the \\chi coordinates [defined WHERE?] for metric distance instead of \\tau_0 and \\tau_3. The lower frequency cutoff is currently hard-coded to be the same as the bank layout options fLow and f0 (which must be the same as each other). Parameters ----------- metricParams : metricParameters instance Structure holding all the options for construction of the metric and the eigenvalues, eigenvectors and covariance matrix needed to manipulate the space. ethincaParams : ethincaParameters instance Structure holding options relevant to the ethinca metric computation. mass1 : float Mass of the heavier body in the considered template. mass2 : float Mass of the lighter body in the considered template. spin1z : float (optional, default=0) Spin of the heavier body in the considered template. spin2z : float (optional, default=0) Spin of the lighter body in the considered template. full_ethinca : boolean (optional, default=True) If True calculate the ethinca components in all 3 directions (mass1, mass2 and time). If False calculate only the time component (which is stored in Gamma0). Returns -------- fMax_theor : float Value of the upper frequency cutoff given by the template parameters and the cutoff formula requested. gammaVals : numpy_array Array holding 6 independent metric components in (end_time, tau_0, tau_3) coordinates to be stored in the Gamma0-5 slots of a SnglInspiral object.", "response": "def calculate_ethinca_metric_comps(metricParams, ethincaParams, mass1, mass2,\n                                   spin1z=0., spin2z=0., full_ethinca=True):\n    \"\"\"\n    Calculate the Gamma components needed to use the ethinca metric.\n    At present this outputs the standard TaylorF2 metric over the end time\n    and chirp times \\tau_0 and \\tau_3.\n    A desirable upgrade might be to use the \\chi coordinates [defined WHERE?]\n    for metric distance instead of \\tau_0 and \\tau_3.\n    The lower frequency cutoff is currently hard-coded to be the same as the\n    bank layout options fLow and f0 (which must be the same as each other).\n\n    Parameters\n    -----------\n    metricParams : metricParameters instance\n        Structure holding all the options for construction of the metric\n        and the eigenvalues, eigenvectors and covariance matrix\n        needed to manipulate the space.\n    ethincaParams : ethincaParameters instance\n        Structure holding options relevant to the ethinca metric computation.\n    mass1 : float\n        Mass of the heavier body in the considered template.\n    mass2 : float\n        Mass of the lighter body in the considered template.\n    spin1z : float (optional, default=0)\n        Spin of the heavier body in the considered template.\n    spin2z : float (optional, default=0)\n        Spin of the lighter body in the considered template.\n    full_ethinca : boolean (optional, default=True)\n        If True calculate the ethinca components in all 3 directions (mass1,\n        mass2 and time). If False calculate only the time component (which is\n        stored in Gamma0).\n    Returns\n    --------\n    fMax_theor : float\n        Value of the upper frequency cutoff given by the template parameters\n        and the cutoff formula requested.\n\n    gammaVals : numpy_array\n        Array holding 6 independent metric components in\n        (end_time, tau_0, tau_3) coordinates to be stored in the Gamma0-5\n        slots of a SnglInspiral object.\n    \"\"\"\n    if (float(spin1z) != 0. or float(spin2z) != 0.) and full_ethinca:\n        raise NotImplementedError(\"Ethinca cannot at present be calculated \"\n                                  \"for nonzero component spins!\")\n    f0 = metricParams.f0\n    if f0 != metricParams.fLow:\n        raise ValueError(\"If calculating ethinca the bank f0 value must be \"\n                         \"equal to f-low!\")\n    if ethincaParams.fLow is not None and (\n        ethincaParams.fLow != metricParams.fLow):\n        raise NotImplementedError(\"An ethinca metric f-low different from the\"\n                                  \" bank metric f-low is not supported!\")\n\n    twicePNOrder = ethinca_order_from_string(ethincaParams.pnOrder)\n\n    piFl = PI * f0\n    totalMass, eta = pnutils.mass1_mass2_to_mtotal_eta(mass1, mass2)\n    totalMass = totalMass * MTSUN_SI\n    v0cube = totalMass*piFl\n    v0 = v0cube**(1./3.)\n\n    # Get theoretical cutoff frequency and work out the closest\n    # frequency for which moments were calculated\n    fMax_theor = pnutils.frequency_cutoff_from_name(\n        ethincaParams.cutoff, mass1, mass2, spin1z, spin2z)\n    fMaxes = metricParams.moments['J4'].keys()\n    fMaxIdx = abs(numpy.array(fMaxes,dtype=float) - fMax_theor).argmin()\n    fMax = fMaxes[fMaxIdx]\n\n    # Set the appropriate moments\n    Js = numpy.zeros([18,3],dtype=float)\n    for i in range(18):\n        Js[i,0] = metricParams.moments['J%d'%(i)][fMax]\n        Js[i,1] = metricParams.moments['log%d'%(i)][fMax]\n        Js[i,2] = metricParams.moments['loglog%d'%(i)][fMax]\n\n    # Compute the time-dependent metric term.\n    two_pi_flower_sq = TWOPI * f0 * TWOPI * f0\n    gammaVals = numpy.zeros([6],dtype=float)\n    gammaVals[0] = 0.5 * two_pi_flower_sq * \\\n                    ( Js[(1,0)] - (Js[(4,0)]*Js[(4,0)]) )\n\n    # If mass terms not required stop here\n    if not full_ethinca:\n        return fMax_theor, gammaVals\n\n    # 3pN is a mess, so split it into pieces\n    a0 = 11583231236531/200286535680 - 5*PI*PI - 107*GAMMA/14\n    a1 = (-15737765635/130056192 + 2255*PI*PI/512)*eta\n    a2 = (76055/73728)*eta*eta\n    a3 = (-127825/55296)*eta*eta*eta\n    alog = numpy.log(4*v0) # Log terms are tricky - be careful\n\n    # Get the Psi coefficients\n    Psi = [{},{}] #Psi = numpy.zeros([2,8,2],dtype=float)\n    Psi[0][0,0] = 3/5\n    Psi[0][2,0] = (743/756 + 11*eta/3)*v0*v0\n    Psi[0][3,0] = 0.\n    Psi[0][4,0] = (-3058673/508032 + 5429*eta/504 + 617*eta*eta/24)\\\n                    *v0cube*v0\n    Psi[0][5,1] = (-7729*PI/126)*v0cube*v0*v0/3\n    Psi[0][6,0] = (128/15)*(-3*a0 - a1 + a2 + 3*a3 + 107*(1+3*alog)/14)\\\n                    *v0cube*v0cube\n    Psi[0][6,1] = (6848/35)*v0cube*v0cube/3\n    Psi[0][7,0] = (-15419335/63504 - 75703*eta/756)*PI*v0cube*v0cube*v0\n\n    Psi[1][0,0] = 0.\n    Psi[1][2,0] = (3715/12096 - 55*eta/96)/PI/v0;\n    Psi[1][3,0] = -3/2\n    Psi[1][4,0] = (15293365/4064256 - 27145*eta/16128 - 3085*eta*eta/384)\\\n                    *v0/PI\n    Psi[1][5,1] = (193225/8064)*v0*v0/3\n    Psi[1][6,0] = (4/PI)*(2*a0 + a1/3 - 4*a2/3 - 3*a3 -107*(1+6*alog)/42)\\\n                    *v0cube\n    Psi[1][6,1] = (-428/PI/7)*v0cube/3\n    Psi[1][7,0] = (77096675/1161216 + 378515*eta/24192 + 74045*eta*eta/8064)\\\n                    *v0cube*v0\n\n    # Set the appropriate moments\n    Js = numpy.zeros([18,3],dtype=float)\n    for i in range(18):\n        Js[i,0] = metricParams.moments['J%d'%(i)][fMax]\n        Js[i,1] = metricParams.moments['log%d'%(i)][fMax]\n        Js[i,2] = metricParams.moments['loglog%d'%(i)][fMax]\n\n    # Calculate the g matrix\n    PNterms = [(0,0),(2,0),(3,0),(4,0),(5,1),(6,0),(6,1),(7,0)]\n    PNterms = [term for term in PNterms if term[0] <= twicePNOrder]\n\n    # Now can compute the mass-dependent gamma values\n    for m in [0, 1]:\n        for k in PNterms:\n            gammaVals[1+m] += 0.5 * two_pi_flower_sq * Psi[m][k] * \\\n                                ( Js[(9-k[0],k[1])]\n                                - Js[(12-k[0],k[1])] * Js[(4,0)] )\n\n    g = numpy.zeros([2,2],dtype=float)\n    for (m,n) in [(0,0),(0,1),(1,1)]:\n        for k in PNterms:\n            for l in PNterms:\n                g[m,n] += Psi[m][k] * Psi[n][l] * \\\n                        ( Js[(17-k[0]-l[0], k[1]+l[1])]\n                        - Js[(12-k[0],k[1])] * Js[(12-l[0],l[1])] )\n        g[m,n] = 0.5 * two_pi_flower_sq * g[m,n]\n        g[n,m] = g[m,n]\n\n    gammaVals[3] = g[0,0]\n    gammaVals[4] = g[0,1]\n    gammaVals[5] = g[1,1]\n\n    return fMax_theor, gammaVals"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfunctions that converts the information produced by the various pyCBC banks into a valid LIGOLW xml file containing a sngl_inspiral table.", "response": "def output_sngl_inspiral_table(outputFile, tempBank, metricParams,\n                               ethincaParams, programName=\"\", optDict = None,\n                               outdoc=None, **kwargs):\n    \"\"\"\n    Function that converts the information produced by the various pyCBC bank\n    generation codes into a valid LIGOLW xml file containing a sngl_inspiral\n    table and outputs to file.\n\n    Parameters\n    -----------\n    outputFile : string\n        Name of the file that the bank will be written to\n    tempBank : iterable\n        Each entry in the tempBank iterable should be a sequence of\n        [mass1,mass2,spin1z,spin2z] in that order.\n    metricParams : metricParameters instance\n        Structure holding all the options for construction of the metric\n        and the eigenvalues, eigenvectors and covariance matrix\n        needed to manipulate the space.\n    ethincaParams: {ethincaParameters instance, None}\n        Structure holding options relevant to the ethinca metric computation\n        including the upper frequency cutoff to be used for filtering.\n        NOTE: The computation is currently only valid for non-spinning systems\n        and uses the TaylorF2 approximant.\n    programName (key-word-argument) : string\n        Name of the executable that has been run\n    optDict (key-word argument) : dictionary\n        Dictionary of the command line arguments passed to the program\n    outdoc (key-word argument) : ligolw xml document\n        If given add template bank to this representation of a xml document and\n        write to disk. If not given create a new document.\n    kwargs : key-word arguments\n        All other key word arguments will be passed directly to\n        ligolw_process.register_to_xmldoc\n    \"\"\"\n    if optDict is None:\n        optDict = {}\n    if outdoc is None:\n        outdoc = ligolw.Document()\n        outdoc.appendChild(ligolw.LIGO_LW())\n\n    # get IFO to put in search summary table\n    ifos = []\n    if 'channel_name' in optDict.keys():\n        if optDict['channel_name'] is not None:\n            ifos = [optDict['channel_name'][0:2]]\n\n    proc_id = ligolw_process.register_to_xmldoc(outdoc, programName, optDict,\n                                                ifos=ifos, **kwargs).process_id\n    sngl_inspiral_table = convert_to_sngl_inspiral_table(tempBank, proc_id)\n    # Calculate Gamma components if needed\n    if ethincaParams is not None:\n        if ethincaParams.doEthinca:\n            for sngl in sngl_inspiral_table:\n                # Set tau_0 and tau_3 values needed for the calculation of\n                # ethinca metric distances\n                (sngl.tau0,sngl.tau3) = pnutils.mass1_mass2_to_tau0_tau3(\n                    sngl.mass1, sngl.mass2, metricParams.f0)\n                fMax_theor, GammaVals = calculate_ethinca_metric_comps(\n                    metricParams, ethincaParams,\n                    sngl.mass1, sngl.mass2, spin1z=sngl.spin1z,\n                    spin2z=sngl.spin2z, full_ethinca=ethincaParams.full_ethinca)\n                # assign the upper frequency cutoff and Gamma0-5 values\n                sngl.f_final = fMax_theor\n                for i in xrange(len(GammaVals)):\n                    setattr(sngl, \"Gamma\"+str(i), GammaVals[i])\n        # If Gamma metric components are not wanted, assign f_final from an\n        # upper frequency cutoff specified in ethincaParams\n        elif ethincaParams.cutoff is not None:\n            for sngl in sngl_inspiral_table:\n                sngl.f_final = pnutils.frequency_cutoff_from_name(\n                    ethincaParams.cutoff,\n                    sngl.mass1, sngl.mass2, sngl.spin1z, sngl.spin2z)\n\n    # set per-template low-frequency cutoff\n    if 'f_low_column' in optDict and 'f_low' in optDict and \\\n            optDict['f_low_column'] is not None:\n        for sngl in sngl_inspiral_table:\n            setattr(sngl, optDict['f_low_column'], optDict['f_low'])\n\n    outdoc.childNodes[0].appendChild(sngl_inspiral_table)\n\n    # get times to put in search summary table\n    start_time = 0\n    end_time = 0\n    if 'gps_start_time' in optDict.keys() and 'gps_end_time' in optDict.keys():\n        start_time = optDict['gps_start_time']\n        end_time = optDict['gps_end_time']\n\n    # make search summary table\n    search_summary_table = lsctables.New(lsctables.SearchSummaryTable)\n    search_summary = return_search_summary(start_time, end_time,\n                               len(sngl_inspiral_table), ifos, **kwargs)\n    search_summary_table.append(search_summary)\n    outdoc.childNodes[0].appendChild(search_summary_table)\n\n    # write the xml doc to disk\n    ligolw_utils.write_filename(outdoc, outputFile,\n                                gz=outputFile.endswith('.gz'))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef spa_length_in_time(**kwds):\n    m1 = kwds['mass1']\n    m2 = kwds['mass2']\n    flow = kwds['f_lower']\n    porder = int(kwds['phase_order'])\n\n    # For now, we call the swig-wrapped function below in\n    # lalinspiral.  Eventually would be nice to replace this\n    # with a function using PN coeffs from lalsimulation.\n    return findchirp_chirptime(m1, m2, flow, porder)", "response": "Returns the length in time of the current SPA template."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the amplitude portion of the TaylorF2 approximant used to precondition the strain data.", "response": "def spa_tmplt_precondition(length, delta_f, kmin=0):\n    \"\"\"Return the amplitude portion of the TaylorF2 approximant, used to precondition\n    the strain data. The result is cached, and so should not be modified only read.\n    \"\"\"\n    global _prec\n    if _prec is None or _prec.delta_f != delta_f or len(_prec) < length:\n        v = numpy.arange(0, (kmin+length*2), 1.0) * delta_f\n        v = numpy.power(v[1:len(v)], -7.0/6.0)\n        _prec = FrequencySeries(v, delta_f=delta_f, dtype=float32)\n    return _prec[kmin:kmin + length]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef spa_distance(psd, mass1, mass2, lower_frequency_cutoff, snr=8):\n    kend = int(spa_tmplt_end(mass1=mass1, mass2=mass2) / psd.delta_f)\n    norm1 = spa_tmplt_norm(psd, len(psd), psd.delta_f, lower_frequency_cutoff)\n    norm2 = (spa_amplitude_factor(mass1=mass1, mass2=mass2)) ** 2.0\n\n    if kend >= len(psd):\n        kend = len(psd) - 1\n    return sqrt(norm1[kend] * norm2) / snr", "response": "Return the distance between two Masses in the SPA TaylorF2 template."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef spa_tmplt_engine(htilde, kmin, phase_order, delta_f, piM, pfaN,\n                     pfa2, pfa3, pfa4, pfa5, pfl5,\n                     pfa6, pfl6, pfa7, amp_factor):\n    \"\"\" Calculate the spa tmplt phase\n    \"\"\"\n    err_msg = \"This function is a stub that should be overridden using the \"\n    err_msg += \"scheme. You shouldn't be seeing this error!\"\n    raise ValueError(err_msg)", "response": "This function is a stub that is overridden by the spa tmplt engine."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef spa_tmplt(**kwds):\n    # Pull out the input arguments\n    f_lower = kwds['f_lower']\n    delta_f = kwds['delta_f']\n    distance = kwds['distance']\n    mass1 = kwds['mass1']\n    mass2 = kwds['mass2']\n    s1z = kwds['spin1z']\n    s2z = kwds['spin2z']\n    phase_order = int(kwds['phase_order'])\n    #amplitude_order = int(kwds['amplitude_order'])\n    spin_order = int(kwds['spin_order'])\n\n    if 'out' in kwds:\n        out = kwds['out']\n    else:\n        out = None\n\n    amp_factor = spa_amplitude_factor(mass1=mass1, mass2=mass2) / distance\n\n    lal_pars = lal.CreateDict()\n    if phase_order != -1:\n        lalsimulation.SimInspiralWaveformParamsInsertPNPhaseOrder(\n            lal_pars, phase_order)\n\n    if spin_order != -1:\n        lalsimulation.SimInspiralWaveformParamsInsertPNSpinOrder(\n            lal_pars, spin_order)\n\n    #Calculate the PN terms\n    phasing = lalsimulation.SimInspiralTaylorF2AlignedPhasing(\n                                    float(mass1), float(mass2),\n                                    float(s1z), float(s2z),\n                                    lal_pars)\n\n    pfaN = phasing.v[0]\n    pfa2 = phasing.v[2] / pfaN\n    pfa3 = phasing.v[3] / pfaN\n    pfa4 = phasing.v[4] / pfaN\n    pfa5 = phasing.v[5] / pfaN\n    pfa6 = (phasing.v[6] - phasing.vlogv[6] * log(4)) / pfaN\n    pfa7 = phasing.v[7] / pfaN\n\n    pfl5 = phasing.vlogv[5] / pfaN\n    pfl6 = phasing.vlogv[6] / pfaN\n\n    piM = lal.PI * (mass1 + mass2) * lal.MTSUN_SI\n\n    kmin = int(f_lower / float(delta_f))\n\n    vISCO = 1. / sqrt(6.)\n    fISCO = vISCO * vISCO * vISCO / piM\n    kmax = int(fISCO / delta_f)\n    f_max = ceilpow2(fISCO)\n    n = int(f_max / delta_f) + 1\n\n    if not out:\n        htilde = FrequencySeries(zeros(n, dtype=numpy.complex64), delta_f=delta_f, copy=False)\n    else:\n        if type(out) is not Array:\n            raise TypeError(\"Output must be an instance of Array\")\n        if len(out) < kmax:\n            kmax = len(out)\n        if out.dtype != complex64:\n            raise TypeError(\"Output array is the wrong dtype\")\n        htilde = FrequencySeries(out, delta_f=delta_f, copy=False)\n\n    spa_tmplt_engine(htilde[kmin:kmax], kmin, phase_order, delta_f, piM, pfaN,\n                     pfa2, pfa3, pfa4, pfa5, pfl5,\n                     pfa6, pfl6, pfa7, amp_factor)\n    return htilde", "response": "Generates a minimal TaylorF2 approximant with optimations for the sin and cos functions."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef save_dict_to_hdf5(dic, filename):\n    with h5py.File(filename, 'w') as h5file:\n        recursively_save_dict_contents_to_group(h5file, '/', dic)", "response": "Save dictionary to hdf5 format"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef recursively_save_dict_contents_to_group(h5file, path, dic):\n    for key, item in dic.items():\n        if isinstance(item, (np.ndarray, np.int64, np.float64, str, bytes, tuple, list)):\n            h5file[path + str(key)] = item\n        elif isinstance(item, dict):\n            recursively_save_dict_contents_to_group(h5file, path + key + '/', item)\n        else:\n            raise ValueError('Cannot save %s type' % type(item))", "response": "Recursively saves the contents of a dictionary to a group of the hdf5 file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef combine_and_copy(f, files, group):\n    f[group] = np.concatenate([fi[group][:] if group in fi else \\\n                                   np.array([], dtype=np.uint32) for fi in files])", "response": "Combine the same column from multiple files and save to a third"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef select(self, fcn, *args, **kwds):\n\n        # get references to each array\n        refs = {}\n        data = {}\n        for arg in args:\n            refs[arg] = self[arg]\n            data[arg] = []\n\n        return_indices = kwds.get('return_indices', False)\n        indices = np.array([], dtype=np.uint64)\n\n        # To conserve memory read the array in chunks\n        chunksize = kwds.get('chunksize', int(1e6))\n        size = len(refs[arg])\n\n        i = 0\n        while i < size:\n            r = i + chunksize if i + chunksize < size else size\n\n            #Read each chunks worth of data and find where it passes the function\n            partial = [refs[arg][i:r] for arg in args]\n            keep = fcn(*partial)\n            if return_indices:\n                indices = np.concatenate([indices, np.flatnonzero(keep) + i])\n\n            #store only the results that pass the function\n            for arg, part in zip(args, partial):\n                data[arg].append(part[keep])\n\n            i += chunksize\n\n        # Combine the partial results into full arrays\n        if len(args) == 1:\n            res = np.concatenate(data[args[0]])\n            if return_indices:\n                return indices, res\n            else:\n                return res\n        else:\n            res = tuple(np.concatenate(data[arg]) for arg in args)\n            if return_indices:\n                return (indices,) + res\n            else:\n                return res", "response": "Return arrays from an hdf5 file that satisfy the given function."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef select(self, idx):\n        data = {}\n        for k in self.data:\n            data[k] = self.data[k][idx]\n        return self._return(data=data)", "response": "Return a new DictArray containing only the indexed values\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove(self, idx):\n        data = {}\n        for k in self.data:\n            data[k] = np.delete(self.data[k], idx)\n        return self._return(data=data)", "response": "Return a new DictArray that does not contain the indexed values\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cluster(self, window):\n        # If no events, do nothing\n        if len(self.time1) == 0 or len(self.time2) == 0:\n            return self\n        from pycbc.events import cluster_coincs\n        interval = self.attrs['timeslide_interval']\n        cid = cluster_coincs(self.stat, self.time1, self.time2,\n                                 self.timeslide_id, interval, window)\n        return self.select(cid)", "response": "Cluster the dict array assuming it has the relevant Coinc colums time1 time2 stat and timeslide_id"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cluster(self, window):\n        # If no events, do nothing\n        pivot_ifo = self.attrs['pivot']\n        fixed_ifo = self.attrs['fixed']\n        if len(self.data['%s/time' % pivot_ifo]) == 0 or len(self.data['%s/time' % fixed_ifo]) == 0:\n            return self\n        from pycbc.events import cluster_coincs\n        interval = self.attrs['timeslide_interval']\n        cid = cluster_coincs(self.stat,\n                             self.data['%s/time' % pivot_ifo],\n                             self.data['%s/time' % fixed_ifo],\n                             self.timeslide_id,\n                             interval,\n                             window)\n        return self.select(cid)", "response": "Cluster the dict array assuming it has the relevant Coinc colums time1 time2 stat and timeslide_id"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a mask implementing the requested filter on the datasets", "response": "def mask(self):\n        \"\"\"\n        Create a mask implementing the requested filter on the datasets\n\n        Returns\n        -------\n        array of Boolean\n            True for dataset indices to be returned by the get_column method\n        \"\"\"\n        if self.filter_func is None:\n            raise RuntimeError(\"Can't get a mask without a filter function!\")\n        else:\n            # only evaluate if no previous calculation was done\n            if self._mask is None:\n                # get required columns into the namespace as numpy arrays\n                for column in self.columns:\n                    if column in self.filter_func:\n                        setattr(self, column, self.group[column][:])\n                self._mask = eval(self.filter_func)\n            return self._mask"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the values of a column in the dataset", "response": "def get_column(self, col):\n        \"\"\"\n        Parameters\n        ----------\n        col : string\n            Name of the dataset to be returned\n\n        Returns\n        -------\n        numpy array\n            Values from the dataset, filtered if requested\n        \"\"\"\n        # catch corner case with an empty file (group with no datasets)\n        if not len(self.group.keys()):\n            return np.array([])\n        vals = self.group[col]\n        if self.filter_func:\n            return vals[self.mask]\n        else:\n            return vals[:]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_column(self, col):\n        logging.info('getting %s' % col)\n        vals = []\n        for f in self.files:\n            d = FileData(f, group=self.group, columnlist=self.columns,\n                         filter_func=self.filter_func)\n            vals.append(d.get_column(col))\n            # Close each file since h5py has an upper limit on the number of\n            # open file objects (approx. 1000)\n            d.close()\n        logging.info('- got %i values' % sum(len(v) for v in vals))\n        return np.concatenate(vals)", "response": "Get the requested dataset values from each file in the dataset and return them as a numpy array."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of plottable CBC parameter variables", "response": "def get_param_names(cls):\n        \"\"\"Returns a list of plottable CBC parameter variables\"\"\"\n        return [m[0] for m in inspect.getmembers(cls) \\\n            if type(m[1]) == property]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef mask_to_n_loudest_clustered_events(self, n_loudest=10,\n                                           ranking_statistic=\"newsnr\",\n                                           cluster_window=10):\n        \"\"\"Edits the mask property of the class to point to the N loudest\n        single detector events as ranked by ranking statistic. Events are\n        clustered so that no more than 1 event within +/- cluster-window will\n        be considered.\"\"\"\n        # If this becomes memory intensive we can optimize\n        stat_instance = sngl_statistic_dict[ranking_statistic]([])\n        stat = stat_instance.single(self.trigs)[self.mask]\n\n        # Used for naming in plots ... Seems an odd place for this to live!\n        if ranking_statistic == \"newsnr\":\n            self.stat_name = \"Reweighted SNR\"\n        elif ranking_statistic == \"newsnr_sgveto\":\n            self.stat_name = \"Reweighted SNR (+sgveto)\"\n        elif ranking_statistic == \"newsnr_sgveto_psdvar\":\n            self.stat_name = \"Reweighted SNR (+sgveto+psdvar)\"\n        elif ranking_statistic == \"snr\":\n            self.stat_name = \"SNR\"\n        else:\n            self.stat_name = ranking_statistic\n\n        times = self.end_time\n        index = stat.argsort()[::-1]\n        new_times = []\n        new_index = []\n        for curr_idx in index:\n            curr_time = times[curr_idx]\n            for time in new_times:\n                if abs(curr_time - time) < cluster_window:\n                    break\n            else:\n                # Only get here if no other triggers within cluster window\n                new_index.append(curr_idx)\n                new_times.append(curr_time)\n            if len(new_index) >= n_loudest:\n                break\n        index = np.array(new_index)\n        self.stat = stat[index]\n        if self.mask.dtype == 'bool':\n            orig_indices = np.flatnonzero(self.mask)[index]\n            self.mask[:] = False\n            self.mask[orig_indices] = True\n        else:\n            self.mask = self.mask[index]", "response": "Edits the mask property of the class to point to the N loudest clustered events."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_new_output_file(sampler, filename, force=False, injection_file=None,\n                           **kwargs):\n    \"\"\"Creates a new output file.\n\n    If the output file already exists, an ``OSError`` will be raised. This can\n    be overridden by setting ``force`` to ``True``.\n\n    Parameters\n    ----------\n    sampler : sampler instance\n        Sampler\n    filename : str\n        Name of the file to create.\n    force : bool, optional\n        Create the file even if it already exists. Default is False.\n    injection_file : str, optional\n        If an injection was added to the data, write its information.\n    \\**kwargs :\n        All other keyword arguments are passed through to the file's\n        ``write_metadata`` function.\n    \"\"\"\n    if os.path.exists(filename):\n        if force:\n            os.remove(filename)\n        else:\n            raise OSError(\"output-file already exists; use force if you \"\n                          \"wish to overwrite it.\")\n    logging.info(\"Creating file {}\".format(filename))\n    with sampler.io(filename, \"w\") as fp:\n        # create the samples group and sampler info group\n        fp.create_group(fp.samples_group)\n        fp.create_group(fp.sampler_group)\n        # save the sampler's metadata\n        fp.write_sampler_metadata(sampler)\n        # save injection parameters\n        if injection_file is not None:\n            logging.info(\"Writing injection file to output\")\n            # just use the first one\n            fp.write_injections(injection_file)", "response": "Creates a new output file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef initial_dist_from_config(cp, variable_params):\n    if len(cp.get_subsections(\"initial\")):\n        logging.info(\"Using a different distribution for the starting points \"\n                     \"than the prior.\")\n        initial_dists = distributions.read_distributions_from_config(\n            cp, section=\"initial\")\n        constraints = distributions.read_constraints_from_config(\n            cp, constraint_section=\"initial_constraint\")\n        init_dist = distributions.JointDistribution(\n            variable_params, *initial_dists,\n            **{\"constraints\": constraints})\n    else:\n        init_dist = None\n    return init_dist", "response": "r Loads a distribution for the sampler start from the given config file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets up the sampler s checkpoint and output files.", "response": "def setup_output(self, output_file, force=False, injection_file=None):\n        \"\"\"Sets up the sampler's checkpoint and output files.\n\n        The checkpoint file has the same name as the output file, but with\n        ``.checkpoint`` appended to the name. A backup file will also be\n        created.\n\n        If the output file already exists, an ``OSError`` will be raised.\n        This can be overridden by setting ``force`` to ``True``.\n\n        Parameters\n        ----------\n        sampler : sampler instance\n            Sampler\n        output_file : str\n            Name of the output file.\n        force : bool, optional\n            If the output file already exists, overwrite it.\n        injection_file : str, optional\n            If an injection was added to the data, write its information.\n        \"\"\"\n        # check for backup file(s)\n        checkpoint_file = output_file + '.checkpoint'\n        backup_file = output_file + '.bkup'\n        # check if we have a good checkpoint and/or backup file\n        logging.info(\"Looking for checkpoint file\")\n        checkpoint_valid = validate_checkpoint_files(checkpoint_file,\n                                                     backup_file)\n        # Create a new file if the checkpoint doesn't exist, or if it is\n        # corrupted\n        self.new_checkpoint = False  # keeps track if this is a new file or not\n        if not checkpoint_valid:\n            logging.info(\"Checkpoint not found or not valid\")\n            create_new_output_file(self, checkpoint_file, force=force,\n                                   injection_file=injection_file)\n            # now the checkpoint is valid\n            self.new_checkpoint = True\n            # copy to backup\n            shutil.copy(checkpoint_file, backup_file)\n        # write the command line, startup\n        for fn in [checkpoint_file, backup_file]:\n            with self.io(fn, \"a\") as fp:\n                fp.write_command_line()\n                fp.write_resume_point()\n        # store\n        self.checkpoint_file = checkpoint_file\n        self.backup_file = backup_file\n        self.checkpoint_valid = checkpoint_valid"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload a FrequencySeries from a. hdf or. txt file.", "response": "def load_frequencyseries(path, group=None):\n    \"\"\"\n    Load a FrequencySeries from a .hdf, .txt or .npy file. The\n    default data types will be double precision floating point.\n\n    Parameters\n    ----------\n    path: string\n        source file path. Must end with either .npy or .txt.\n\n    group: string \n        Additional name for internal storage use. Ex. hdf storage uses\n        this as the key value.\n\n    Raises\n    ------\n    ValueError\n        If path does not end in .npy or .txt.\n    \"\"\"    \n    ext = _os.path.splitext(path)[1]\n    if ext == '.npy':\n        data = _numpy.load(path)    \n    elif ext == '.txt':\n        data = _numpy.loadtxt(path)\n    elif ext == '.hdf':\n        key = 'data' if group is None else group\n        f = h5py.File(path, 'r')\n        data = f[key][:]\n        series = FrequencySeries(data, delta_f=f[key].attrs['delta_f'],\n                                       epoch=f[key].attrs['epoch']) \n        f.close()\n        return series\n    else:\n        raise ValueError('Path must end with .npy, .hdf, or .txt')\n        \n    if data.ndim == 2:\n        delta_f = (data[-1][0] - data[0][0]) / (len(data)-1)\n        epoch = _lal.LIGOTimeGPS(data[0][0])\n        return FrequencySeries(data[:,1], delta_f=delta_f, epoch=epoch)\n    elif data.ndim == 3:\n        delta_f = (data[-1][0] - data[0][0]) / (len(data)-1)\n        epoch = _lal.LIGOTimeGPS(data[0][0])\n        return FrequencySeries(data[:,1] + 1j*data[:,2], delta_f=delta_f,\n                               epoch=epoch)\n    else:\n        raise ValueError('File has %s dimensions, cannot convert to Array, \\\n                          must be 2 (real) or 3 (complex)' % data.ndim)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncompares whether two frequency series elements are almost equal with self and other.", "response": "def almost_equal_elem(self,other,tol,relative=True,dtol=0.0):\n        \"\"\"\n        Compare whether two frequency series are almost equal, element\n        by element.\n\n        If the 'relative' parameter is 'True' (the default) then the\n        'tol' parameter (which must be positive) is interpreted as a\n        relative tolerance, and the comparison returns 'True' only if\n        abs(self[i]-other[i]) <= tol*abs(self[i])\n        for all elements of the series.\n\n        If 'relative' is 'False', then 'tol' is an absolute tolerance,\n        and the comparison is true only if\n        abs(self[i]-other[i]) <= tol\n        for all elements of the series.\n\n        The method also checks that self.delta_f is within 'dtol' of\n        other.delta_f; if 'dtol' has its default value of 0 then exact\n        equality between the two is required.\n\n        Other meta-data (type, dtype, length, and epoch) must be exactly\n        equal.  If either object's memory lives on the GPU it will be\n        copied to the CPU for the comparison, which may be slow. But the\n        original object itself will not have its memory relocated nor\n        scheme changed.\n\n        Parameters\n        ----------\n        other: another Python object, that should be tested for\n            almost-equality with 'self', element-by-element.\n        tol: a non-negative number, the tolerance, which is interpreted\n            as either a relative tolerance (the default) or an absolute\n            tolerance.\n        relative: A boolean, indicating whether 'tol' should be interpreted\n            as a relative tolerance (if True, the default if this argument\n            is omitted) or as an absolute tolerance (if tol is False).\n        dtol: a non-negative number, the tolerance for delta_f. Like 'tol',\n            it is interpreted as relative or absolute based on the value of\n            'relative'.  This parameter defaults to zero, enforcing exact\n            equality between the delta_f values of the two FrequencySeries.\n\n        Returns\n        -------\n        boolean: 'True' if the data and delta_fs agree within the tolerance,\n            as interpreted by the 'relative' keyword, and if the types,\n            lengths, dtypes, and epochs are exactly the same.\n        \"\"\"\n        # Check that the delta_f tolerance is non-negative; raise an exception\n        # if needed.\n        if (dtol < 0.0):\n            raise ValueError(\"Tolerance in delta_f cannot be negative\")\n        if super(FrequencySeries,self).almost_equal_elem(other,tol=tol,relative=relative):\n            if relative:\n                return (self._epoch == other._epoch and\n                        abs(self._delta_f-other._delta_f) <= dtol*self._delta_f)\n            else:\n                return (self._epoch == other._epoch and\n                        abs(self._delta_f-other._delta_f) <= dtol)\n        else:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef lal(self):\n\n        lal_data = None\n        if self._epoch is None:\n            ep = _lal.LIGOTimeGPS(0,0)\n        else:\n            ep = self._epoch\n\n        if self._data.dtype == _numpy.float32:\n            lal_data = _lal.CreateREAL4FrequencySeries(\"\",ep,0,self.delta_f,_lal.SecondUnit,len(self))\n        elif self._data.dtype == _numpy.float64:\n            lal_data = _lal.CreateREAL8FrequencySeries(\"\",ep,0,self.delta_f,_lal.SecondUnit,len(self))\n        elif self._data.dtype == _numpy.complex64:\n            lal_data = _lal.CreateCOMPLEX8FrequencySeries(\"\",ep,0,self.delta_f,_lal.SecondUnit,len(self))\n        elif self._data.dtype == _numpy.complex128:\n            lal_data = _lal.CreateCOMPLEX16FrequencySeries(\"\",ep,0,self.delta_f,_lal.SecondUnit,len(self))\n\n        lal_data.data.data[:] = self.numpy()\n\n        return lal_data", "response": "Produces a LAL frequency series object equivalent to self."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsave the frequency series to a Numpy hdf or text file.", "response": "def save(self, path, group=None, ifo='P1'):\n        \"\"\"\n        Save frequency series to a Numpy .npy, hdf, or text file. The first column\n        contains the sample frequencies, the second contains the values.\n        In the case of a complex frequency series saved as text, the imaginary\n        part is written as a third column.  When using hdf format, the data is stored\n        as a single vector, along with relevant attributes.\n\n        Parameters\n        ----------\n        path: string\n            Destination file path. Must end with either .hdf, .npy or .txt.\n            \n        group: string \n            Additional name for internal storage use. Ex. hdf storage uses\n            this as the key value.\n\n        Raises\n        ------\n        ValueError\n            If path does not end in .npy or .txt.\n        \"\"\"\n\n        ext = _os.path.splitext(path)[1]\n        if ext == '.npy':\n            output = _numpy.vstack((self.sample_frequencies.numpy(),\n                                    self.numpy())).T\n            _numpy.save(path, output)\n        elif ext == '.txt':\n            if self.kind == 'real':\n                output = _numpy.vstack((self.sample_frequencies.numpy(),\n                                        self.numpy())).T\n            elif self.kind == 'complex':\n                output = _numpy.vstack((self.sample_frequencies.numpy(),\n                                        self.numpy().real,\n                                        self.numpy().imag)).T\n            _numpy.savetxt(path, output)\n        elif ext == '.xml' or path.endswith('.xml.gz'):\n            from pycbc.io.live import make_psd_xmldoc\n            from glue.ligolw import utils\n\n            if self.kind != 'real':\n                raise ValueError('XML only supports real frequency series')\n            output = self.lal()\n            # When writing in this format we must *not* have the 0 values at\n            # frequencies less than flow. To resolve this we set the first\n            # non-zero value < flow.\n            data_lal = output.data.data\n            first_idx = _numpy.argmax(data_lal>0)\n            if not first_idx == 0:\n                data_lal[:first_idx] = data_lal[first_idx]\n            psddict = {ifo: output}\n            utils.write_filename(make_psd_xmldoc(psddict), path,\n                                 gz=path.endswith(\".gz\"))\n        elif ext =='.hdf':\n            key = 'data' if group is None else group\n            f = h5py.File(path)\n            ds = f.create_dataset(key, data=self.numpy(), compression='gzip',\n                                  compression_opts=9, shuffle=True)\n            ds.attrs['epoch'] = float(self.epoch)\n            ds.attrs['delta_f'] = float(self.delta_f)\n        else:\n            raise ValueError('Path must end with .npy, .txt, .xml, .xml.gz '\n                             'or .hdf')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_timeseries(self, delta_t=None):\n        from pycbc.fft import ifft\n        from pycbc.types import TimeSeries, real_same_precision_as\n        nat_delta_t =  1.0 / ((len(self)-1)*2) / self.delta_f\n        if not delta_t:\n            delta_t = nat_delta_t\n\n        # add 0.5 to round integer\n        tlen  = int(1.0 / self.delta_f / delta_t + 0.5)\n        flen = int(tlen / 2 + 1)\n        \n        if flen < len(self):\n            raise ValueError(\"The value of delta_t (%s) would be \"\n                             \"undersampled. Maximum delta_t \"\n                             \"is %s.\" % (delta_t, nat_delta_t))\n        if not delta_t:\n            tmp = self\n        else:\n            tmp = FrequencySeries(zeros(flen, dtype=self.dtype), \n                             delta_f=self.delta_f, epoch=self.epoch)\n            tmp[:len(self)] = self[:]\n        \n        f = TimeSeries(zeros(tlen, \n                           dtype=real_same_precision_as(self)),\n                           delta_t=delta_t)\n        ifft(tmp, f)\n        return f", "response": "Return the inverse fourier transform of this frequency series."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nshifts the data and timestamps by a given number of seconds.", "response": "def cyclic_time_shift(self, dt):\n        \"\"\"Shift the data and timestamps by a given number of seconds\n\n        Shift the data and timestamps in the time domain a given number of \n        seconds. To just change the time stamps, do ts.start_time += dt. \n        The time shift may be smaller than the intrinsic sample rate of the data.\n        Note that data will be cycliclly rotated, so if you shift by 2\n        seconds, the final 2 seconds of your data will now be at the \n        beginning of the data set.\n\n        Parameters\n        ----------\n        dt : float\n            Amount of time to shift the vector.\n\n        Returns\n        -------\n        data : pycbc.types.FrequencySeries\n            The time shifted frequency series.\n        \"\"\"\n        from pycbc.waveform import apply_fseries_time_shift\n        data = apply_fseries_time_shift(self, dt)\n        data.start_time = self.start_time - dt\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the match between the two TimeSeries or FrequencySeries. Return the match between two waveforms. This is equivelant to the overlap maximized over time and phase. By default, the other vector will be resized to match self. Beware, this may remove high frequency content or the end of the vector. Parameters ---------- other : TimeSeries or FrequencySeries The input vector containing a waveform. psd : Frequency Series A power spectral density to weight the overlap. low_frequency_cutoff : {None, float}, optional The frequency to begin the match. high_frequency_cutoff : {None, float}, optional The frequency to stop the match. index: int The number of samples to shift to get the match. Returns ------- match: float index: int The number of samples to shift to get the match.", "response": "def match(self, other, psd=None,\n              low_frequency_cutoff=None, high_frequency_cutoff=None):\n        \"\"\" Return the match between the two TimeSeries or FrequencySeries.\n\n        Return the match between two waveforms. This is equivelant to the overlap\n        maximized over time and phase. By default, the other vector will be\n        resized to match self. Beware, this may remove high frequency content or the\n        end of the vector.\n\n        Parameters\n        ----------\n        other : TimeSeries or FrequencySeries\n            The input vector containing a waveform.\n        psd : Frequency Series\n            A power spectral density to weight the overlap.\n        low_frequency_cutoff : {None, float}, optional\n            The frequency to begin the match.\n        high_frequency_cutoff : {None, float}, optional\n            The frequency to stop the match.\n        index: int\n            The number of samples to shift to get the match.\n\n        Returns\n        -------\n        match: float\n        index: int\n            The number of samples to shift to get the match.\n        \"\"\"\n        from pycbc.types import TimeSeries\n        from pycbc.filter import match\n\n        if isinstance(other, TimeSeries):\n            if other.duration != self.duration:\n                other = other.copy()\n                other.resize(int(other.sample_rate * self.duration))\n\n            other = other.to_frequencyseries()\n        \n        if len(other) != len(self):\n            other = other.copy()\n            other.resize(len(self))\n\n        if psd is not None and len(psd) > len(self):\n            psd = psd.copy()\n            psd.resize(len(self))\n\n        return match(self, other, psd=psd,\n                     low_frequency_cutoff=low_frequency_cutoff,\n                     high_frequency_cutoff=high_frequency_cutoff)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_physical_covaried_masses(xis, bestMasses, bestXis, req_match,\n                                 massRangeParams, metricParams, fUpper,\n                                 giveUpThresh = 5000):\n    \"\"\"\n    This function takes the position of a point in the xi parameter space and\n    iteratively finds a close point in the physical coordinate space (masses\n    and spins).\n \n    Parameters\n    -----------\n    xis : list or array\n        Desired position of the point in the xi space. If only N values are\n        provided and the xi space's dimension is larger then it is assumed that\n        *any* value in the remaining xi coordinates is acceptable.\n    bestMasses : list\n        Contains [totalMass, eta, spin1z, spin2z]. Is a physical position\n        mapped to xi coordinates in bestXis that is close to the desired point.\n        This is aimed to give the code a starting point.\n    bestXis : list\n        Contains the position of bestMasses in the xi coordinate system.\n    req_match : float\n        Desired maximum mismatch between xis and the obtained point. If a point\n        is found with mismatch < req_match immediately stop and return that\n        point. A point with this mismatch will not always be found.\n    massRangeParams : massRangeParameters instance\n        Instance holding all the details of mass ranges and spin ranges.\n    metricParams : metricParameters instance\n        Structure holding all the options for construction of the metric\n        and the eigenvalues, eigenvectors and covariance matrix\n        needed to manipulate the space.\n    fUpper : float\n        The value of fUpper that was used when obtaining the xi_i\n        coordinates. This lets us know how to rotate potential physical points\n        into the correct xi_i space. This must be a key in metricParams.evals,\n        metricParams.evecs and metricParams.evecsCV\n        (ie. we must know how to do the transformation for\n        the given value of fUpper)\n    giveUpThresh : int, optional (default = 5000)\n        The program will try this many iterations. If no close matching point\n        has been found after this it will give up.\n\n    Returns\n    --------\n    mass1 : float\n        The heavier mass of the obtained point.\n    mass2 : float\n        The smaller mass of the obtained point\n    spin1z : float\n        The heavier bodies spin of the obtained point.\n    spin2z : float\n        The smaller bodies spin of the obtained point.\n    count : int\n        How many iterations it took to find the point. For debugging.\n    mismatch : float\n        The mismatch between the obtained point and the input xis.\n    new_xis : list\n        The position of the point in the xi space\n    \"\"\"\n    # TUNABLE PARAMETERS GO HERE!\n    # This states how far apart to scatter test points in the first proposal\n    origScaleFactor = 1\n\n    # Set up\n    xi_size = len(xis)\n    scaleFactor = origScaleFactor\n    bestChirpmass = bestMasses[0] * (bestMasses[1])**(3./5.)\n    count = 0\n    unFixedCount = 0\n    currDist = 100000000000000000\n    while(1):\n        # If we are a long way away we use larger jumps\n        if count:\n            if currDist > 1 and scaleFactor == origScaleFactor:\n                scaleFactor = origScaleFactor*10\n        # Get a set of test points with mass -> xi mappings\n        totmass, eta, spin1z, spin2z, mass1, mass2, new_xis = \\\n            get_mass_distribution([bestChirpmass, bestMasses[1], bestMasses[2],\n                                   bestMasses[3]],\n                                  scaleFactor, massRangeParams, metricParams,\n                                  fUpper)\n        cDist = (new_xis[0] - xis[0])**2\n        for j in range(1,xi_size):\n            cDist += (new_xis[j] - xis[j])**2\n        if (cDist.min() < req_match):\n            idx = cDist.argmin()\n            scaleFactor = origScaleFactor\n            new_xis_list = [new_xis[ldx][idx] for ldx in range(len(new_xis))]\n            return mass1[idx], mass2[idx], spin1z[idx], spin2z[idx], count, \\\n                   cDist.min(), new_xis_list\n        if (cDist.min() < currDist):\n            idx = cDist.argmin()\n            bestMasses[0] = totmass[idx]\n            bestMasses[1] = eta[idx]\n            bestMasses[2] = spin1z[idx]\n            bestMasses[3] = spin2z[idx]\n            bestChirpmass = bestMasses[0] * (bestMasses[1])**(3./5.)\n            currDist = cDist.min()\n            unFixedCount = 0\n            scaleFactor = origScaleFactor\n        count += 1\n        unFixedCount += 1\n        if unFixedCount > giveUpThresh:\n            # Stop at this point\n            diff = (bestMasses[0]*bestMasses[0] * (1-4*bestMasses[1]))**0.5\n            mass1 = (bestMasses[0] + diff)/2.\n            mass2 = (bestMasses[0] - diff)/2.\n            new_xis_list = [new_xis[ldx][0] for ldx in range(len(new_xis))]\n            return mass1, mass2, bestMasses[2], bestMasses[3], count, \\\n                   currDist, new_xis_list\n        if not unFixedCount % 100:\n            scaleFactor *= 2\n        if scaleFactor > 64:\n            scaleFactor = 1\n    # Shouldn't be here!\n    raise RuntimeError", "response": "This function returns a list of all potential masses that are close to the given xi parameter space."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngives a set of masses, this function will create a set of points nearby in the mass space and map these to the xi space. Parameters ----------- bestMasses : list Contains [ChirpMass, eta, spin1z, spin2z]. Points will be placed around tjos scaleFactor : float This parameter describes the radius away from bestMasses that points will be placed in. massRangeParams : massRangeParameters instance Instance holding all the details of mass ranges and spin ranges. metricParams : metricParameters instance Structure holding all the options for construction of the metric and the eigenvalues, eigenvectors and covariance matrix needed to manipulate the space. fUpper : float The value of fUpper that was used when obtaining the xi_i coordinates. This lets us know how to rotate potential physical points into the correct xi_i space. This must be a key in metricParams.evals, metricParams.evecs and metricParams.evecsCV (ie. we must know how to do the transformation for the given value of fUpper) numJumpPoints : int, optional (default = 100) The number of points that will be generated every iteration chirpMassJumpFac : float, optional (default=0.0001) The jump points will be chosen with fractional variation in chirpMass up to this multiplied by scaleFactor. etaJumpFac : float, optional (default=0.01) The jump points will be chosen with fractional variation in eta up to this multiplied by scaleFactor. spin1zJumpFac : float, optional (default=0.01) The jump points will be chosen with absolute variation in spin1z up to this multiplied by scaleFactor. spin2zJumpFac : float, optional (default=0.01) The jump points will be chosen with absolute variation in spin2z up to this multiplied by scaleFactor. Returns -------- Totmass : numpy.array Total mass of the resulting points Eta : numpy.array Symmetric mass ratio of the resulting points Spin1z : numpy.array Spin of the heavier body of the resulting points Spin2z : numpy.array Spin of the smaller body of the resulting points Diff : numpy.array Mass1 - Mass2 of the resulting points Mass1 : numpy.array Mass1 (mass of heavier body) of the resulting points Mass2 : numpy.array Mass2 (mass of smaller body) of the resulting points new_xis : list of numpy.array Position of points in the xi coordinates", "response": "def get_mass_distribution(bestMasses, scaleFactor, massRangeParams,\n                          metricParams, fUpper,\n                          numJumpPoints=100, chirpMassJumpFac=0.0001,\n                          etaJumpFac=0.01, spin1zJumpFac=0.01,\n                          spin2zJumpFac=0.01):\n    \"\"\"\n    Given a set of masses, this function will create a set of points nearby\n    in the mass space and map these to the xi space.\n\n    Parameters\n    -----------\n    bestMasses : list\n        Contains [ChirpMass, eta, spin1z, spin2z]. Points will be placed around\n        tjos\n    scaleFactor : float\n        This parameter describes the radius away from bestMasses that points\n        will be placed in.\n    massRangeParams : massRangeParameters instance\n        Instance holding all the details of mass ranges and spin ranges.\n    metricParams : metricParameters instance\n        Structure holding all the options for construction of the metric\n        and the eigenvalues, eigenvectors and covariance matrix\n        needed to manipulate the space.\n    fUpper : float\n        The value of fUpper that was used when obtaining the xi_i\n        coordinates. This lets us know how to rotate potential physical points\n        into the correct xi_i space. This must be a key in metricParams.evals,\n        metricParams.evecs and metricParams.evecsCV\n        (ie. we must know how to do the transformation for\n        the given value of fUpper)\n    numJumpPoints : int, optional (default = 100)\n        The number of points that will be generated every iteration\n    chirpMassJumpFac : float, optional (default=0.0001)\n        The jump points will be chosen with fractional variation in chirpMass\n        up to this multiplied by scaleFactor.\n    etaJumpFac : float, optional (default=0.01)\n        The jump points will be chosen with fractional variation in eta\n        up to this multiplied by scaleFactor.\n    spin1zJumpFac : float, optional (default=0.01)\n        The jump points will be chosen with absolute variation in spin1z up to\n        this multiplied by scaleFactor.\n    spin2zJumpFac : float, optional (default=0.01)\n        The jump points will be chosen with absolute variation in spin2z up to\n        this multiplied by scaleFactor.\n\n    Returns \n    --------\n    Totmass : numpy.array\n        Total mass of the resulting points\n    Eta : numpy.array\n        Symmetric mass ratio of the resulting points\n    Spin1z : numpy.array\n        Spin of the heavier body of the resulting points\n    Spin2z : numpy.array\n        Spin of the smaller body of the resulting points\n    Diff : numpy.array\n        Mass1 - Mass2 of the resulting points\n    Mass1 : numpy.array\n        Mass1 (mass of heavier body) of the resulting points\n    Mass2 : numpy.array\n        Mass2 (mass of smaller body) of the resulting points\n    new_xis : list of numpy.array\n        Position of points in the xi coordinates\n    \"\"\"\n    # FIXME: It would be better if rejected values could be drawn from the \n    # full possible mass/spin distribution. However speed in this function is\n    # a major factor and must be considered.\n    bestChirpmass = bestMasses[0]\n    bestEta = bestMasses[1]\n    bestSpin1z = bestMasses[2]\n    bestSpin2z = bestMasses[3]\n\n    # Firstly choose a set of values for masses and spins\n    chirpmass = bestChirpmass * (1 - (numpy.random.random(numJumpPoints)-0.5) \\\n                                       * chirpMassJumpFac * scaleFactor )\n    etaRange = massRangeParams.maxEta - massRangeParams.minEta\n    currJumpFac = etaJumpFac * scaleFactor\n    if currJumpFac > etaRange:\n        currJumpFac = etaRange\n    eta = bestEta * ( 1 - (numpy.random.random(numJumpPoints) - 0.5) \\\n                           * currJumpFac)\n\n    maxSpinMag = max(massRangeParams.maxNSSpinMag, massRangeParams.maxBHSpinMag)\n    minSpinMag = min(massRangeParams.maxNSSpinMag, massRangeParams.maxBHSpinMag)\n    # Note that these two are cranged by spinxzFac, *not* spinxzFac/spinxz\n    currJumpFac = spin1zJumpFac * scaleFactor\n    if currJumpFac > maxSpinMag:\n        currJumpFac = maxSpinMag\n\n    # Actually set the new spin trial points\n    if massRangeParams.nsbhFlag or (maxSpinMag == minSpinMag):\n        curr_spin_1z_jump_fac = currJumpFac\n        curr_spin_2z_jump_fac = currJumpFac\n        # Check spins aren't going to be unphysical\n        if currJumpFac > massRangeParams.maxBHSpinMag:\n            curr_spin_1z_jump_fac = massRangeParams.maxBHSpinMag\n        if currJumpFac > massRangeParams.maxNSSpinMag:\n            curr_spin_2z_jump_fac = massRangeParams.maxNSSpinMag\n        spin1z = bestSpin1z + ( (numpy.random.random(numJumpPoints) - 0.5) \\\n                            * curr_spin_1z_jump_fac)\n        spin2z = bestSpin2z + ( (numpy.random.random(numJumpPoints) - 0.5) \\\n                            * curr_spin_2z_jump_fac)\n    else:\n        # If maxNSSpinMag is very low (0) and maxBHSpinMag is high we can\n        # find it hard to place any points. So mix these when\n        # masses are swapping between the NS and BH.\n        curr_spin_bh_jump_fac = currJumpFac\n        curr_spin_ns_jump_fac = currJumpFac\n        # Check spins aren't going to be unphysical\n        if currJumpFac > massRangeParams.maxBHSpinMag:\n            curr_spin_bh_jump_fac = massRangeParams.maxBHSpinMag\n        if currJumpFac > massRangeParams.maxNSSpinMag:\n            curr_spin_ns_jump_fac = massRangeParams.maxNSSpinMag\n        spin1z = numpy.zeros(numJumpPoints, dtype=float)\n        spin2z = numpy.zeros(numJumpPoints, dtype=float)\n        split_point = int(numJumpPoints/2)\n        # So set the first half to be at least within the BH range and the\n        # second half to be at least within the NS range\n        spin1z[:split_point] = bestSpin1z + \\\n                            ( (numpy.random.random(split_point) - 0.5)\\\n                              * curr_spin_bh_jump_fac)\n        spin1z[split_point:] = bestSpin1z + \\\n                      ( (numpy.random.random(numJumpPoints-split_point) - 0.5)\\\n                        * curr_spin_ns_jump_fac)\n        spin2z[:split_point] = bestSpin2z + \\\n                            ( (numpy.random.random(split_point) - 0.5)\\\n                              * curr_spin_bh_jump_fac)\n        spin2z[split_point:] = bestSpin2z + \\\n                      ( (numpy.random.random(numJumpPoints-split_point) - 0.5)\\\n                        * curr_spin_ns_jump_fac)\n\n    # Point[0] is always set to the original point\n    chirpmass[0] = bestChirpmass\n    eta[0] = bestEta\n    spin1z[0] = bestSpin1z\n    spin2z[0] = bestSpin2z\n\n    # Remove points where eta becomes unphysical\n    eta[eta > massRangeParams.maxEta] = massRangeParams.maxEta\n    if massRangeParams.minEta:\n        eta[eta < massRangeParams.minEta] = massRangeParams.minEta\n    else:\n        eta[eta < 0.0001] = 0.0001\n\n    # Total mass, masses and mass diff\n    totmass = chirpmass / (eta**(3./5.))\n    diff = (totmass*totmass * (1-4*eta))**0.5\n    mass1 = (totmass + diff)/2.\n    mass2 = (totmass - diff)/2.\n\n    # Check the validity of the spin values\n    # Do the first spin\n\n    if maxSpinMag == 0:\n        # Shortcut if non-spinning\n        pass\n    elif massRangeParams.nsbhFlag or (maxSpinMag == minSpinMag):\n        # Simple case where I don't have to worry about correlation with mass\n        numploga = abs(spin1z) > massRangeParams.maxBHSpinMag\n        spin1z[numploga] = 0\n    else:\n        # Do have to consider masses\n        boundary_mass = massRangeParams.ns_bh_boundary_mass\n        numploga1 = numpy.logical_and(mass1 >= boundary_mass,\n                                   abs(spin1z) <= massRangeParams.maxBHSpinMag)\n        numploga2 = numpy.logical_and(mass1 < boundary_mass,\n                                   abs(spin1z) <= massRangeParams.maxNSSpinMag)\n        numploga = numpy.logical_or(numploga1, numploga2)\n        numploga = numpy.logical_not(numploga)\n        spin1z[numploga] = 0\n\n    # Same for the second spin\n\n    if maxSpinMag == 0:\n        # Shortcut if non-spinning\n        pass\n    elif massRangeParams.nsbhFlag or (maxSpinMag == minSpinMag):\n        numplogb = abs(spin2z) > massRangeParams.maxNSSpinMag\n        spin2z[numplogb] = 0\n    else:\n        # Do have to consider masses\n        boundary_mass = massRangeParams.ns_bh_boundary_mass\n        numplogb1 = numpy.logical_and(mass2 >= boundary_mass,\n                                   abs(spin2z) <= massRangeParams.maxBHSpinMag)\n        numplogb2 = numpy.logical_and(mass2 < boundary_mass,\n                                   abs(spin2z) <= massRangeParams.maxNSSpinMag)\n        numplogb = numpy.logical_or(numplogb1, numplogb2)\n        numplogb = numpy.logical_not(numplogb)\n        spin2z[numplogb] = 0\n\n    if (maxSpinMag) and (numploga[0] or numplogb[0]):\n        raise ValueError(\"Cannot remove the guide point!\")\n\n    # And remove points where the individual masses are outside of the physical\n    # range. Or the total masses are.\n    # These \"removed\" points will have metric distances that will be much, much\n    # larger than any thresholds used in the functions in brute_force_utils.py\n    # and will always be rejected. An unphysical value cannot be used as it\n    # would result in unphysical metric distances and cause failures.\n    totmass[mass1 < massRangeParams.minMass1] = 0.0001\n    totmass[mass1 > massRangeParams.maxMass1] = 0.0001\n    totmass[mass2 < massRangeParams.minMass2] = 0.0001\n    totmass[mass2 > massRangeParams.maxMass2] = 0.0001\n    # There is some numerical error which can push this a bit higher. We do\n    # *not* want to reject the initial guide point. This error comes from\n    # Masses -> totmass, eta -> masses conversion, we will have points pushing\n    # onto the boudaries of the space.\n    totmass[totmass > massRangeParams.maxTotMass*1.0001] = 0.0001\n    totmass[totmass < massRangeParams.minTotMass*0.9999] = 0.0001\n    if massRangeParams.max_chirp_mass:\n        totmass[chirpmass > massRangeParams.max_chirp_mass*1.0001] = 0.0001\n    if massRangeParams.min_chirp_mass:\n        totmass[chirpmass < massRangeParams.min_chirp_mass*0.9999] = 0.0001\n\n    if totmass[0] < 0.00011:\n        raise ValueError(\"Cannot remove the guide point!\")\n\n    mass1[totmass < 0.00011] = 0.0001\n    mass2[totmass < 0.00011] = 0.0001\n\n    # Then map to xis\n    new_xis = get_cov_params(mass1, mass2, spin1z, spin2z,\n                             metricParams, fUpper)\n    return totmass, eta, spin1z, spin2z, mass1, mass2, new_xis"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef stack_xi_direction_brute(xis, bestMasses, bestXis, direction_num,\n                             req_match, massRangeParams, metricParams, fUpper,\n                             scaleFactor=0.8, numIterations=3000):\n    \"\"\"\n    This function is used to assess the depth of the xi_space in a specified\n    dimension at a specified point in the higher dimensions. It does this by\n    iteratively throwing points at the space to find maxima and minima.\n\n    Parameters\n    -----------\n\n    xis : list or array\n        Position in the xi space at which to assess the depth. This can be only\n        a subset of the higher dimensions than that being sampled.\n    bestMasses : list\n        Contains [totalMass, eta, spin1z, spin2z]. Is a physical position\n        mapped to xi coordinates in bestXis that is close to the xis point.\n        This is aimed to give the code a starting point.\n    bestXis : list\n        Contains the position of bestMasses in the xi coordinate system.\n    direction_num : int\n        The dimension that you want to assess the depth of (0 = 1, 1 = 2 ...)\n    req_match : float\n        When considering points to assess the depth with, only consider points\n        with a mismatch that is smaller than this with xis.\n    massRangeParams : massRangeParameters instance\n        Instance holding all the details of mass ranges and spin ranges.\n    metricParams : metricParameters instance\n        Structure holding all the options for construction of the metric\n        and the eigenvalues, eigenvectors and covariance matrix\n        needed to manipulate the space.\n    fUpper : float\n        The value of fUpper that was used when obtaining the xi_i\n        coordinates. This lets us know how to rotate potential physical points\n        into the correct xi_i space. This must be a key in metricParams.evals,\n        metricParams.evecs and metricParams.evecsCV\n        (ie. we must know how to do the transformation for\n        the given value of fUpper)\n    scaleFactor : float, optional (default = 0.8)\n        The value of the scale factor that is used when calling\n        pycbc.tmpltbank.get_mass_distribution.\n    numIterations : int, optional (default = 3000)\n        The number of times to make calls to get_mass_distribution when\n        assessing the maximum/minimum of this parameter space. Making this\n        smaller makes the code faster, but at the cost of accuracy.   \n \n    Returns\n    --------\n    xi_min : float\n        The minimal value of the specified dimension at the specified point in\n        parameter space.\n    xi_max : float\n       The maximal value of the specified dimension at the specified point in\n        parameter space.\n    \"\"\"\n\n    # Find minimum\n    ximin = find_xi_extrema_brute(xis, bestMasses, bestXis, direction_num, \\\n                                  req_match, massRangeParams, metricParams, \\\n                                  fUpper, find_minimum=True, \\\n                                  scaleFactor=scaleFactor, \\\n                                  numIterations=numIterations)\n \n    # Find maximum\n    ximax = find_xi_extrema_brute(xis, bestMasses, bestXis, direction_num, \\\n                                  req_match, massRangeParams, metricParams, \\\n                                  fUpper, find_minimum=False, \\\n                                  scaleFactor=scaleFactor, \\\n                                  numIterations=numIterations)\n\n    return ximin, ximax", "response": "This function is used to assess the depth of a specified xi_space in a specified xi_space."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_xi_extrema_brute(xis, bestMasses, bestXis, direction_num, req_match, \\\n                          massRangeParams, metricParams, fUpper, \\\n                          find_minimum=False, scaleFactor=0.8, \\\n                          numIterations=3000):   \n    \"\"\"\n    This function is used to find the largest or smallest value of the xi\n    space in a specified\n    dimension at a specified point in the higher dimensions. It does this by\n    iteratively throwing points at the space to find extrema.\n\n    Parameters\n    -----------\n\n    xis : list or array\n        Position in the xi space at which to assess the depth. This can be only\n        a subset of the higher dimensions than that being sampled.\n    bestMasses : list\n        Contains [totalMass, eta, spin1z, spin2z]. Is a physical position\n        mapped to xi coordinates in bestXis that is close to the xis point.\n        This is aimed to give the code a starting point.\n    bestXis : list\n        Contains the position of bestMasses in the xi coordinate system.\n    direction_num : int\n        The dimension that you want to assess the depth of (0 = 1, 1 = 2 ...)\n    req_match : float\n        When considering points to assess the depth with, only consider points\n        with a mismatch that is smaller than this with xis.\n    massRangeParams : massRangeParameters instance\n        Instance holding all the details of mass ranges and spin ranges.\n    metricParams : metricParameters instance\n        Structure holding all the options for construction of the metric\n        and the eigenvalues, eigenvectors and covariance matrix\n        needed to manipulate the space.\n    fUpper : float\n        The value of fUpper that was used when obtaining the xi_i\n        coordinates. This lets us know how to rotate potential physical points\n        into the correct xi_i space. This must be a key in metricParams.evals,\n        metricParams.evecs and metricParams.evecsCV\n        (ie. we must know how to do the transformation for\n        the given value of fUpper)\n    find_minimum : boolean, optional (default = False)\n        If True, find the minimum value of the xi direction. If False find the\n        maximum value.\n    scaleFactor : float, optional (default = 0.8)\n        The value of the scale factor that is used when calling\n        pycbc.tmpltbank.get_mass_distribution.\n    numIterations : int, optional (default = 3000)\n        The number of times to make calls to get_mass_distribution when\n        assessing the maximum/minimum of this parameter space. Making this\n        smaller makes the code faster, but at the cost of accuracy.   \n\n    Returns\n    --------\n    xi_extent : float\n        The extremal value of the specified dimension at the specified point in\n        parameter space.\n    \"\"\"\n\n    # Setup\n    xi_size = len(xis)\n    bestChirpmass = bestMasses[0] * (bestMasses[1])**(3./5.)\n    if find_minimum:\n        xiextrema = 10000000000\n    else:\n        xiextrema = -100000000000\n\n    for _ in range(numIterations):\n        # Evaluate extrema of the xi direction specified\n        totmass, eta, spin1z, spin2z, _, _, new_xis = \\\n            get_mass_distribution([bestChirpmass,bestMasses[1],bestMasses[2],\n                                   bestMasses[3]],\n                                  scaleFactor, massRangeParams, metricParams,\n                                  fUpper)\n        cDist = (new_xis[0] - xis[0])**2\n        for j in range(1, xi_size):\n            cDist += (new_xis[j] - xis[j])**2\n        redCDist = cDist[cDist < req_match]\n        if len(redCDist):\n            if not find_minimum:\n                new_xis[direction_num][cDist > req_match] = -10000000\n                currXiExtrema = (new_xis[direction_num]).max()\n                idx = (new_xis[direction_num]).argmax()\n            else:\n                new_xis[direction_num][cDist > req_match] = 10000000\n                currXiExtrema = (new_xis[direction_num]).min()\n                idx = (new_xis[direction_num]).argmin()\n            if ( ((not find_minimum) and (currXiExtrema > xiextrema)) or \\\n                         (find_minimum and (currXiExtrema < xiextrema)) ):\n                xiextrema = currXiExtrema\n                bestMasses[0] = totmass[idx]\n                bestMasses[1] = eta[idx]\n                bestMasses[2] = spin1z[idx]\n                bestMasses[3] = spin2z[idx]\n                bestChirpmass = bestMasses[0] * (bestMasses[1])**(3./5.)\n    return xiextrema", "response": "This function finds the xi_extrema space that is at a specified xi coordinate system."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef filter_zpk(timeseries, z, p, k):\n\n    # sanity check type\n    if not isinstance(timeseries, TimeSeries):\n        raise TypeError(\"Can only filter TimeSeries instances.\")\n\n    # sanity check casual filter\n    degree = len(p) - len(z)\n    if degree < 0:\n        raise TypeError(\"May not have more zeroes than poles. \\\n                         Filter is not casual.\")\n\n    # cast zeroes and poles as arrays and gain as a float\n    z = np.array(z)\n    p = np.array(p)\n    k = float(k)\n\n    # put zeroes and poles in the s-domain\n    # convert from frequency to angular frequency\n    z *= -2 * np.pi\n    p *= -2 * np.pi\n\n    # get denominator of bilinear transform\n    fs = 2.0 * timeseries.sample_rate\n\n    # zeroes in the z-domain\n    z_zd = (1 + z/fs) / (1 - z/fs)\n\n    # any zeros that were at infinity are moved to the Nyquist frequency\n    z_zd = z_zd[np.isfinite(z_zd)]\n    z_zd = np.append(z_zd, -np.ones(degree))\n\n    # poles in the z-domain\n    p_zd = (1 + p/fs) / (1 - p/fs)\n\n    # gain change in z-domain\n    k_zd = k * np.prod(fs - z) / np.prod(fs - p)\n\n    # get second-order sections\n    sos = zpk2sos(z_zd, p_zd, k_zd)\n\n    # filter\n    filtered_data = sosfilt(sos, timeseries.numpy())\n\n    return TimeSeries(filtered_data, delta_t = timeseries.delta_t,\n                      dtype=timeseries.dtype,\n                      epoch=timeseries._epoch)", "response": "Filter a time series with a zero - pole - gain filter."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if this is the main process", "response": "def is_main_process():\n    \"\"\" Check if this is the main control process and may handle one time tasks\n    \"\"\"\n    try:\n        from mpi4py import MPI\n        comm = MPI.COMM_WORLD\n        rank = comm.Get_rank()\n        return rank == 0\n    except (ImportError, ValueError, RuntimeError):\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwraps to ensure that all processes execute together", "response": "def _lockstep_fcn(values):\n    \"\"\" Wrapper to ensure that all processes execute together \"\"\"\n    numrequired, fcn, args = values\n    with _process_lock:\n        _numdone.value += 1\n    # yep this is an ugly busy loop, do something better please\n    # when we care about the performance of this call and not just the\n    # guarantee it provides (ok... maybe never)\n    while 1:\n        if _numdone.value == numrequired:\n            return fcn(args)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncall a function on every worker.", "response": "def broadcast(self, fcn, args):\n        \"\"\" Do a function call on every worker.\n\n        Parameters\n        ----------\n        fcn: funtion\n            Function to call.\n        args: tuple\n            The arguments for Pool.map\n        \"\"\"\n        results = self.map(_lockstep_fcn, [(len(self), fcn, args)] * len(self))\n        _numdone.value = 0\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndoes a function call on every worker with different arguments", "response": "def allmap(self, fcn, args):\n        \"\"\" Do a function call on every worker with different arguments\n\n        Parameters\n        ----------\n        fcn: funtion\n            Function to call.\n        args: tuple\n            The arguments for Pool.map\n        \"\"\"\n        results = self.map(_lockstep_fcn,\n                           [(len(self), fcn, arg) for arg in args])\n        _numdone.value = 0\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef map(self, func, items, chunksize=None):\n        results = self.map_async(func, items, chunksize)\n        while True:\n            try:\n                return results.get(1800)\n            except TimeoutError:\n                pass\n            except KeyboardInterrupt:\n                self.terminate()\n                self.join()\n                raise KeyboardInterrupt", "response": "This function will call a function on each item in the items list and return the result."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef lfilter(coefficients, timeseries):\n    from pycbc.filter import correlate\n\n    # If there aren't many points just use the default scipy method\n    if len(timeseries) < 2**7:\n        if hasattr(timeseries, 'numpy'):\n            timeseries = timeseries.numpy()\n        series = scipy.signal.lfilter(coefficients, 1.0, timeseries)\n        return series\n    else:\n        cseries = (Array(coefficients[::-1] * 1)).astype(timeseries.dtype)\n        cseries.resize(len(timeseries))\n        cseries.roll(len(timeseries) - len(coefficients) + 1)\n        timeseries = Array(timeseries, copy=False)\n\n        flen = len(cseries) / 2 + 1\n        ftype = complex_same_precision_as(timeseries)\n\n        cfreq = zeros(flen, dtype=ftype)\n        tfreq = zeros(flen, dtype=ftype)\n\n        fft(Array(cseries), cfreq)\n        fft(Array(timeseries), tfreq)\n\n        cout = zeros(flen, ftype)\n        out = zeros(len(timeseries), dtype=timeseries)\n\n        correlate(cfreq, tfreq, cout)\n        ifft(cout, out)\n\n        return out.numpy()  / len(out)", "response": "Apply filter coefficients to a time series returning a numpy array of the n - th entry in the time series."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfilters the timeseries with a set of FIR coefficients and return a new time series with the corresponding zeroed out regions.", "response": "def fir_zero_filter(coeff, timeseries):\n    \"\"\"Filter the timeseries with a set of FIR coefficients\n\n    Parameters\n    ----------\n    coeff: numpy.ndarray\n        FIR coefficients. Should be and odd length and symmetric.\n    timeseries: pycbc.types.TimeSeries\n        Time series to be filtered.\n\n    Returns\n    -------\n    filtered_series: pycbc.types.TimeSeries\n        Return the filtered timeseries, which has been properly shifted to account\n    for the FIR filter delay and the corrupted regions zeroed out.\n    \"\"\"\n    # apply the filter\n    series = lfilter(coeff, timeseries.numpy())\n\n    # reverse the time shift caused by the filter,\n    # corruption regions contain zeros\n    # If the number of filter coefficients is odd, the central point *should*\n    # be included in the output so we only zero out a region of len(coeff) - 1\n    data = numpy.zeros(len(timeseries))\n    data[len(coeff)//2:len(data)-len(coeff)//2] = series[(len(coeff) // 2) * 2:]\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef notch_fir(timeseries, f1, f2, order, beta=5.0):\n    k1 = f1 / float((int(1.0 / timeseries.delta_t) / 2))\n    k2 = f2 / float((int(1.0 / timeseries.delta_t) / 2))\n    coeff = scipy.signal.firwin(order * 2 + 1, [k1, k2], window=('kaiser', beta))\n    data = fir_zero_filter(coeff, timeseries)\n    return TimeSeries(data, epoch=timeseries.start_time, delta_t=timeseries.delta_t)", "response": "This function is used to filter the time series using a FIR filtered generated from a time - domain kaiser window."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef highpass_fir(timeseries, frequency, order, beta=5.0):\n    k = frequency / float((int(1.0 / timeseries.delta_t) / 2))\n    coeff = scipy.signal.firwin(order * 2 + 1, k, window=('kaiser', beta), pass_zero=False)\n    data = fir_zero_filter(coeff, timeseries)\n    return TimeSeries(data, epoch=timeseries.start_time, delta_t=timeseries.delta_t)", "response": "Highpass filter the time series using a FIR filtered generated from a kaiser window"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef highpass(timeseries, frequency, filter_order=8, attenuation=0.1):\n\n    if not isinstance(timeseries, TimeSeries):\n        raise TypeError(\"Can only resample time series\")\n\n    if timeseries.kind is not 'real':\n        raise TypeError(\"Time series must be real\")\n\n    lal_data = timeseries.lal()\n    _highpass_func[timeseries.dtype](lal_data, frequency,\n                                     1-attenuation, filter_order)\n\n    return TimeSeries(lal_data.data.data, delta_t = lal_data.deltaT,\n                      dtype=timeseries.dtype, epoch=timeseries._epoch)", "response": "Return a new time series that is high - passed above the frequency."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef interpolate_complex_frequency(series, delta_f, zeros_offset=0, side='right'):\n    new_n = int( (len(series)-1) * series.delta_f / delta_f + 1)\n    old_N = int( (len(series)-1) * 2 )\n    new_N = int( (new_n - 1) * 2 )\n    time_series = TimeSeries(zeros(old_N), delta_t =1.0/(series.delta_f*old_N),\n                             dtype=real_same_precision_as(series))\n\n    ifft(series, time_series)\n\n    time_series.roll(-zeros_offset)\n    time_series.resize(new_N)\n\n    if side == 'left':\n        time_series.roll(zeros_offset + new_N - old_N)\n    elif side == 'right':\n        time_series.roll(zeros_offset)\n\n    out_series = FrequencySeries(zeros(new_n), epoch=series.epoch,\n                           delta_f=delta_f, dtype=series.dtype)\n    fft(time_series, out_series)\n\n    return out_series", "response": "Interpolate a complex frequency series to the desired delta_f."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the expected rate of noise coincidences for multiple detectors and returns the result of this function.", "response": "def multiifo_noise_coinc_rate(rates, slop):\n    \"\"\"\n    Calculate the expected rate of noise coincidences for multiple detectors\n\n    Parameters\n    ----------\n    rates: dict\n        Dictionary keyed on ifo string\n        Value is a sequence of single-detector trigger rates, units assumed\n        to be Hz\n    slop: float\n        time added to maximum time-of-flight between detectors to account\n        for timing error\n\n    Returns\n    -------\n    expected_coinc_rates: dict\n        Dictionary keyed on the ifo combination string\n        Value is expected coincidence rate in the combination, units Hz\n    \"\"\"\n    ifos = numpy.array(sorted(rates.keys()))\n    rates_raw = list(rates[ifo] for ifo in ifos)\n    expected_coinc_rates = {}\n\n    # Calculate coincidence for all-ifo combination\n    # multiply product of trigger rates by the overlap time\n    allowed_area = multiifo_noise_coincident_area(ifos, slop)\n    rateprod = [numpy.prod(rs) for rs in zip(*rates_raw)]\n    ifostring = ' '.join(ifos)\n    expected_coinc_rates[ifostring] = allowed_area * numpy.array(rateprod)\n\n    # if more than one possible coincidence type exists,\n    # calculate coincidence for subsets through recursion\n    if len(ifos) > 2:\n        # Calculate rate for each 'miss-one-out' detector combination\n        subsets = itertools.combinations(ifos, len(ifos) - 1)\n        for subset in subsets:\n            rates_subset = {}\n            for ifo in subset:\n                rates_subset[ifo] = rates[ifo]\n            sub_coinc_rates = multiifo_noise_coinc_rate(rates_subset, slop)\n            # add these sub-coincidences to the overall dictionary\n            for sub_coinc in sub_coinc_rates:\n                expected_coinc_rates[sub_coinc] = sub_coinc_rates[sub_coinc]\n\n    return expected_coinc_rates"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate the total area of the 2d space of time offsets between two detectors and 3d space of time offsets for the multiifo coincident detectors.", "response": "def multiifo_noise_coincident_area(ifos, slop):\n    \"\"\"\n    calculate the total extent of time offset between 2 detectors,\n    or area of the 2d space of time offsets for 3 detectors, for\n    which a coincidence can be generated\n\n    Parameters\n    ----------\n    ifos: list of strings\n        list of interferometers\n    slop: float\n        extra time to add to maximum time-of-flight for timing error\n\n    Returns\n    -------\n    allowed_area: float\n        area in units of seconds^(n_ifos-1) that coincident values can fall in\n    \"\"\"\n    # set up detector objects\n    dets = {}\n    for ifo in ifos:\n        dets[ifo] = pycbc.detector.Detector(ifo)\n    n_ifos = len(ifos)\n\n    if n_ifos == 2:\n        allowed_area = 2. * \\\n            (dets[ifos[0]].light_travel_time_to_detector(dets[ifos[1]]) + slop)\n    elif n_ifos == 3:\n        tofs = numpy.zeros(n_ifos)\n        ifo2_num = []\n\n        # calculate travel time between detectors (plus extra for timing error)\n        # TO DO: allow for different timing errors between different detectors\n        for i, ifo in enumerate(ifos):\n            ifo2_num.append(int(numpy.mod(i + 1, n_ifos)))\n            det0 = dets[ifo]\n            det1 = dets[ifos[ifo2_num[i]]]\n            tofs[i] = det0.light_travel_time_to_detector(det1) + slop\n\n        # combine these to calculate allowed area\n        allowed_area = 0\n        for i, _ in enumerate(ifos):\n            allowed_area += 2 * tofs[i] * tofs[ifo2_num[i]] - tofs[i]**2\n    else:\n        raise NotImplementedError(\"Not able to deal with more than 3 ifos\")\n\n    return allowed_area"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the area in which signal time differences are physically allowed for a multi - detector object.", "response": "def multiifo_signal_coincident_area(ifos):\n    \"\"\"\n    Calculate the area in which signal time differences are physically allowed\n\n    Parameters\n    ----------\n    ifos: list of strings\n        list of interferometers\n\n    Returns\n    -------\n    allowed_area: float\n        area in units of seconds^(n_ifos-1) that coincident signals will occupy\n    \"\"\"\n    n_ifos = len(ifos)\n\n    if n_ifos == 2:\n        det0 = pycbc.detector.Detector(ifos[0])\n        det1 = pycbc.detector.Detector(ifos[1])\n        allowed_area = 2 * det0.light_travel_time_to_detector(det1)\n    elif n_ifos == 3:\n        dets = {}\n        tofs = numpy.zeros(n_ifos)\n        ifo2_num = []\n        # set up detector objects\n        for ifo in ifos:\n            dets[ifo] = pycbc.detector.Detector(ifo)\n\n        # calculate travel time between detectors\n        for i, ifo in enumerate(ifos):\n            ifo2_num.append(int(numpy.mod(i + 1, n_ifos)))\n            det0 = dets[ifo]\n            det1 = dets[ifos[ifo2_num[i]]]\n            tofs[i] = det0.light_travel_time_to_detector(det1)\n\n        # calculate allowed area\n        phi_12 = numpy.arccos((tofs[0]**2 + tofs[1]**2 - tofs[2]**2)\n                              / (2 * tofs[0] * tofs[1]))\n        allowed_area = numpy.pi * tofs[0] * tofs[1] * numpy.sin(phi_12)\n    else:\n        raise NotImplementedError(\"Not able to deal with more than 3 ifos\")\n\n    return allowed_area"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a dictionary built from the combination of defaults kwargs and the attributes of the object.", "response": "def props(obj, required, **kwargs):\n    \"\"\" Return a dictionary built from the combination of defaults, kwargs,\n    and the attributes of the given object.\n    \"\"\"\n    # Get the attributes of the template object\n    pr = get_obj_attrs(obj)\n\n    # Get the parameters to generate the waveform\n    # Note that keyword arguments override values in the template object\n    input_params = default_qnm_args.copy()\n    input_params.update(pr)\n    input_params.update(kwargs)\n\n    # Check if the required arguments are given\n    for arg in required:\n        if arg not in input_params:\n            raise ValueError('Please provide ' + str(arg))\n\n    return input_params"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntakes input_params and return dictionaries with amplitudes and phases of all overtones of a specific lm mode.", "response": "def lm_amps_phases(**kwargs):\n    \"\"\" Take input_params and return dictionaries with amplitudes and phases\n    of each overtone of a specific lm mode, checking that all of them are given.\n    \"\"\"\n    l, m = kwargs['l'], kwargs['m']\n    amps, phis = {}, {}\n    # amp220 is always required, because the amplitudes of subdominant modes\n    # are given as fractions of amp220.\n    try:\n        amps['220'] = kwargs['amp220']\n    except KeyError:\n        raise ValueError('amp220 is always required')\n\n    # Get amplitudes of subdominant modes and all phases\n    for n in range(kwargs['nmodes']):\n        # If it is the 22 mode, skip 220\n        if (l, m, n) != (2, 2, 0):\n            try:\n                amps['%d%d%d' %(l,m,n)] = kwargs['amp%d%d%d' %(l,m,n)] * amps['220']\n            except KeyError:\n                raise ValueError('amp%d%d%d is required' %(l,m,n))\n        try:\n            phis['%d%d%d' %(l,m,n)] = kwargs['phi%d%d%d' %(l,m,n)]\n        except KeyError:\n            raise ValueError('phi%d%d%d is required' %(l,m,n))\n\n    return amps, phis"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef lm_freqs_taus(**kwargs):\n    lmns = kwargs['lmns']\n    freqs, taus = {}, {}\n\n    for lmn in lmns:\n        l, m, nmodes = int(lmn[0]), int(lmn[1]), int(lmn[2])\n        for n in range(nmodes):\n            try:\n                freqs['%d%d%d' %(l,m,n)] = kwargs['f_%d%d%d' %(l,m,n)]\n            except KeyError:\n                raise ValueError('f_%d%d%d is required' %(l,m,n))\n            try:\n                taus['%d%d%d' %(l,m,n)] = kwargs['tau_%d%d%d' %(l,m,n)]\n            except KeyError:\n                raise ValueError('tau_%d%d%d is required' %(l,m,n))\n\n    return freqs, taus", "response": "Take input_params and return dictionaries with frequencies and damping\n    times of each overtone of a specific lm mode."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the frequency at which the ringdown falls to decay of the peak amplitude.", "response": "def qnm_freq_decay(f_0, tau, decay):\n    \"\"\"Return the frequency at which the amplitude of the\n    ringdown falls to decay of the peak amplitude.\n\n    Parameters\n    ----------\n    f_0 : float\n        The ringdown-frequency, which gives the peak amplitude.\n    tau : float\n        The damping time of the sinusoid.\n    decay: float\n        The fraction of the peak amplitude.\n\n    Returns\n    -------\n    f_decay: float\n        The frequency at which the amplitude of the frequency-domain\n        ringdown falls to decay of the peak amplitude.\n    \"\"\"\n\n    q_0 = pi * f_0 * tau\n    alpha = 1. / decay\n    alpha_sq = 1. / decay / decay\n\n    # Expression obtained analytically under the assumption\n    # that 1./alpha_sq, q_0^2 >> 1\n    q_sq = (alpha_sq + 4*q_0*q_0 + alpha*numpy.sqrt(alpha_sq + 16*q_0*q_0)) / 4.\n    return numpy.sqrt(q_sq) / pi / tau"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef lm_tfinal(damping_times, modes):\n\n    t_max = {}\n    for lmn in modes:\n        l, m, nmodes = int(lmn[0]), int(lmn[1]), int(lmn[2])\n        for n in range(nmodes):\n            t_max['%d%d%d' %(l,m,n)] = \\\n            qnm_time_decay(damping_times['%d%d%d' %(l,m,n)], 1./1000)\n\n    return max(t_max.values())", "response": "Return the maximum t_final of the modes given with t_final the time\n    at which the peak falls to 1 / 1000 of the peak amplitude\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef lm_ffinal(freqs, damping_times, modes):\n\n    f_max = {}\n    for lmn in modes:\n        l, m, nmodes = int(lmn[0]), int(lmn[1]), int(lmn[2])\n        for n in range(nmodes):\n            f_max['%d%d%d' %(l,m,n)] = qnm_freq_decay(freqs['%d%d%d' %(l,m,n)],\n                                      damping_times['%d%d%d' %(l,m,n)], 1./1000)\n\n    f_final = max(f_max.values())\n    if f_final > max_freq:\n        f_final = max_freq\n\n    return f_final", "response": "Return the maximum f_final of the modes given with f_final the frequency of the peak amplitude\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the minimum delta_f of all the modes given by damping_times.", "response": "def lm_deltaf(damping_times, modes):\n    \"\"\"Return the minimum delta_f of all the modes given, with delta_f given by\n    the inverse of the time at which the amplitude of the ringdown falls to\n    1/1000 of the peak amplitude.\n    \"\"\"\n\n    df = {}\n    for lmn in modes:\n        l, m, nmodes = int(lmn[0]), int(lmn[1]), int(lmn[2])\n        for n in range(nmodes):\n            df['%d%d%d' %(l,m,n)] = \\\n                1. / qnm_time_decay(damping_times['%d%d%d' %(l,m,n)], 1./1000)\n\n    return min(df.values())"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns spherical harmonic polarizations", "response": "def spher_harms(l, m, inclination):\n    \"\"\"Return spherical harmonic polarizations\n    \"\"\"\n\n    # FIXME: we are using spin -2 weighted spherical harmonics for now,\n    # when possible switch to spheroidal harmonics.\n    Y_lm = lal.SpinWeightedSphericalHarmonic(inclination, 0., -2, l, m).real\n    Y_lminusm = lal.SpinWeightedSphericalHarmonic(inclination, 0., -2, l, -m).real\n    Y_plus = Y_lm + (-1)**l * Y_lminusm\n    Y_cross = Y_lm - (-1)**l * Y_lminusm\n\n    return Y_plus, Y_cross"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef Kerr_factor(final_mass, distance):\n\n    # Convert solar masses to meters\n    mass = final_mass * lal.MSUN_SI * lal.G_SI / lal.C_SI**2\n    # Convert Mpc to meters\n    dist = distance * 1e6 * lal.PC_SI\n\n    return mass / dist", "response": "Return the factor final_mass / distance for Kerr\n    ringdowns\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding waveform shifted accordingly", "response": "def taper_shift(waveform, output):\n    \"\"\"Add waveform to output with waveform shifted accordingly (for tapering\n    multi-mode ringdowns)\n    \"\"\"\n\n    if len(waveform) == len(output):\n        output.data += waveform.data\n    else:\n        output.data[len(output)-len(waveform):] += waveform.data\n\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_td_qnm(template=None, taper=None, **kwargs):\n\n    input_params = props(template, qnm_required_args, **kwargs)\n\n    f_0 = input_params.pop('f_0')\n    tau = input_params.pop('tau')\n    amp = input_params.pop('amp')\n    phi = input_params.pop('phi')\n    # the following may not be in input_params\n    inc = input_params.pop('inclination', None)\n    l = input_params.pop('l', 2)\n    m = input_params.pop('m', 2)\n    delta_t = input_params.pop('delta_t', None)\n    t_final = input_params.pop('t_final', None)\n\n    if not delta_t:\n        delta_t = 1. / qnm_freq_decay(f_0, tau, 1./1000)\n        if delta_t < min_dt:\n            delta_t = min_dt\n    if not t_final:\n        t_final = qnm_time_decay(tau, 1./1000)\n\n    kmax = int(t_final / delta_t) + 1\n    times = numpy.arange(kmax) * delta_t\n    if inc is not None:\n        Y_plus, Y_cross = spher_harms(l, m, inc)\n    else:\n        Y_plus, Y_cross = 1, 1\n\n    hplus = amp * Y_plus * numpy.exp(-times/tau) * \\\n                                numpy.cos(two_pi*f_0*times + phi)\n    hcross = amp * Y_cross * numpy.exp(-times/tau) * \\\n                                numpy.sin(two_pi*f_0*times + phi)\n\n    if taper and delta_t < taper*tau:\n        taper_window = int(taper*tau/delta_t)\n        kmax += taper_window\n\n    outplus = TimeSeries(zeros(kmax), delta_t=delta_t)\n    outcross = TimeSeries(zeros(kmax), delta_t=delta_t)\n\n    # If size of tapering window is less than delta_t, do not apply taper.\n    if not taper or delta_t > taper*tau:\n        outplus.data[:kmax] = hplus\n        outcross.data[:kmax] = hcross\n\n        return outplus, outcross\n\n    else:\n        taper_hp, taper_hc = apply_taper(delta_t, taper, f_0, tau, amp, phi,\n                                                                    l, m, inc)\n        start = - taper * tau\n        outplus.data[:taper_window] = taper_hp\n        outplus.data[taper_window:] = hplus\n        outcross.data[:taper_window] = taper_hc\n        outcross.data[taper_window:] = hcross\n        outplus._epoch, outcross._epoch = start, start\n\n        return outplus, outcross", "response": "Return a time domain damped sinusoid."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a frequency domain damped sinusoid.", "response": "def get_fd_qnm(template=None, **kwargs):\n    \"\"\"Return a frequency domain damped sinusoid.\n\n    Parameters\n    ----------\n    template: object\n        An object that has attached properties. This can be used to substitute\n        for keyword arguments. A common example would be a row in an xml table.\n    f_0 : float\n        The ringdown-frequency.\n    tau : float\n        The damping time of the sinusoid.\n    amp : float\n        The amplitude of the ringdown (constant for now).\n    phi : float\n        The initial phase of the ringdown. Should also include the information\n        from the azimuthal angle (phi_0 + m*Phi).\n    inclination : {None, float}, optional\n        Inclination of the system in radians for the spherical harmonics.\n    l : {2, int}, optional\n        l mode for the spherical harmonics. Default is l=2.\n    m : {2, int}, optional\n        m mode for the spherical harmonics. Default is m=2.\n    t_0 :  {0, float}, optional\n        The starting time of the ringdown.\n    delta_f : {None, float}, optional\n        The frequency step used to generate the ringdown.\n        If None, it will be set to the inverse of the time at which the\n        amplitude is 1/1000 of the peak amplitude.\n    f_lower: {None, float}, optional\n        The starting frequency of the output frequency series.\n        If None, it will be set to delta_f.\n    f_final : {None, float}, optional\n        The ending frequency of the output frequency series.\n        If None, it will be set to the frequency at which the amplitude is\n        1/1000 of the peak amplitude.\n\n    Returns\n    -------\n    hplustilde: FrequencySeries\n        The plus phase of the ringdown in frequency domain.\n    hcrosstilde: FrequencySeries\n        The cross phase of the ringdown in frequency domain.\n    \"\"\"\n\n    input_params = props(template, qnm_required_args, **kwargs)\n\n    f_0 = input_params.pop('f_0')\n    tau = input_params.pop('tau')\n    amp = input_params.pop('amp')\n    phi = input_params.pop('phi')\n    # the following have defaults, and so will be populated\n    t_0 = input_params.pop('t_0')\n    # the following may not be in input_params\n    inc = input_params.pop('inclination', None)\n    l = input_params.pop('l', 2)\n    m = input_params.pop('m', 2)\n    delta_f = input_params.pop('delta_f', None)\n    f_lower = input_params.pop('f_lower', None)\n    f_final = input_params.pop('f_final', None)\n\n    if not delta_f:\n        delta_f = 1. / qnm_time_decay(tau, 1./1000)\n    if not f_lower:\n        f_lower = delta_f\n        kmin = 0\n    else:\n        kmin = int(f_lower / delta_f)\n    if not f_final:\n        f_final = qnm_freq_decay(f_0, tau, 1./1000)\n    if f_final > max_freq:\n        f_final = max_freq\n    kmax = int(f_final / delta_f) + 1\n\n    freqs = numpy.arange(kmin, kmax)*delta_f\n    if inc is not None:\n        Y_plus, Y_cross = spher_harms(l, m, inc)\n    else:\n        Y_plus, Y_cross = 1, 1\n\n    denominator = 1 + (4j * pi * freqs * tau) - (4 * pi_sq * ( freqs*freqs - f_0*f_0) * tau*tau)\n    norm = amp * tau / denominator\n    if t_0 != 0:\n        time_shift = numpy.exp(-1j * two_pi * freqs * t_0)\n        norm *= time_shift\n\n    # Analytical expression for the Fourier transform of the ringdown (damped sinusoid)\n    hp_tilde = norm * Y_plus * ( (1 + 2j * pi * freqs * tau) * numpy.cos(phi)\n                                        - two_pi * f_0 * tau * numpy.sin(phi) )\n    hc_tilde = norm * Y_cross * ( (1 + 2j * pi * freqs * tau) * numpy.sin(phi)\n                                        + two_pi * f_0 * tau * numpy.cos(phi) )\n\n    outplus = FrequencySeries(zeros(kmax, dtype=complex128), delta_f=delta_f)\n    outcross = FrequencySeries(zeros(kmax, dtype=complex128), delta_f=delta_f)\n    outplus.data[kmin:kmax] = hp_tilde\n    outcross.data[kmin:kmax] = hc_tilde\n\n    return outplus, outcross"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_td_lm(template=None, taper=None, **kwargs):\n\n    input_params = props(template, lm_required_args, **kwargs)\n\n    # Get required args\n    amps, phis = lm_amps_phases(**input_params)\n    f_0 = input_params.pop('freqs')\n    tau = input_params.pop('taus')\n    inc = input_params.pop('inclination', None)\n    l, m = input_params.pop('l'), input_params.pop('m')\n    nmodes = input_params.pop('nmodes')\n    if int(nmodes) == 0:\n        raise ValueError('Number of overtones (nmodes) must be greater '\n                         'than zero.')\n    # The following may not be in input_params\n    delta_t = input_params.pop('delta_t', None)\n    t_final = input_params.pop('t_final', None)\n\n    if not delta_t:\n        delta_t = lm_deltat(f_0, tau, ['%d%d%d' %(l,m,nmodes)])\n    if not t_final:\n        t_final = lm_tfinal(tau, ['%d%d%d' %(l, m, nmodes)])\n\n    kmax = int(t_final / delta_t) + 1\n    # Different overtones will have different tapering window-size\n    # Find maximum window size to create long enough output vector\n    if taper:\n        taper_window = int(taper*max(tau.values())/delta_t)\n        kmax += taper_window\n\n    outplus = TimeSeries(zeros(kmax, dtype=float64), delta_t=delta_t)\n    outcross = TimeSeries(zeros(kmax, dtype=float64), delta_t=delta_t)\n    if taper:\n        start = - taper * max(tau.values())\n        outplus._epoch, outcross._epoch = start, start\n\n    for n in range(nmodes):\n        hplus, hcross = get_td_qnm(template=None, taper=taper,\n                            f_0=f_0['%d%d%d' %(l,m,n)],\n                            tau=tau['%d%d%d' %(l,m,n)],\n                            phi=phis['%d%d%d' %(l,m,n)],\n                            amp=amps['%d%d%d' %(l,m,n)],\n                            inclination=inc, l=l, m=m,\n                            delta_t=delta_t, t_final=t_final)\n        if not taper:\n            outplus.data += hplus.data\n            outcross.data += hcross.data\n        else:\n            outplus = taper_shift(hplus, outplus)\n            outcross = taper_shift(hcross, outcross)\n\n    return outplus, outcross", "response": "Return the time domain lm mode with the given number of overtones."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_fd_lm(template=None, **kwargs):\n\n    input_params = props(template, lm_required_args, **kwargs)\n\n    # Get required args\n    amps, phis = lm_amps_phases(**input_params)\n    f_0 = input_params.pop('freqs')\n    tau = input_params.pop('taus')\n    l, m = input_params.pop('l'), input_params.pop('m')\n    inc = input_params.pop('inclination', None)\n    nmodes = input_params.pop('nmodes')\n    if int(nmodes) == 0:\n        raise ValueError('Number of overtones (nmodes) must be greater '\n                         'than zero.')\n    # The following may not be in input_params\n    delta_f = input_params.pop('delta_f', None)\n    f_lower = input_params.pop('f_lower', None)\n    f_final = input_params.pop('f_final', None)\n\n    if not delta_f:\n        delta_f = lm_deltaf(tau, ['%d%d%d' %(l,m,nmodes)])\n    if not f_final:\n        f_final = lm_ffinal(f_0, tau, ['%d%d%d' %(l, m, nmodes)])\n    kmax = int(f_final / delta_f) + 1\n\n    outplus = FrequencySeries(zeros(kmax, dtype=complex128), delta_f=delta_f)\n    outcross = FrequencySeries(zeros(kmax, dtype=complex128), delta_f=delta_f)\n\n    for n in range(nmodes):\n        hplus, hcross = get_fd_qnm(template=None, f_0=f_0['%d%d%d' %(l,m,n)],\n                            tau=tau['%d%d%d' %(l,m,n)],\n                            amp=amps['%d%d%d' %(l,m,n)],\n                            phi=phis['%d%d%d' %(l,m,n)],\n                            inclination=inc, l=l, m=m, delta_f=delta_f,\n                            f_lower=f_lower, f_final=f_final)\n        outplus.data += hplus.data\n        outcross.data += hcross.data\n\n    return outplus, outcross", "response": "Returns a frequency domain lm mode with a given number of overtones."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_td_from_final_mass_spin(template=None, taper=None,\n                                distance=None, **kwargs):\n    \"\"\"Return time domain ringdown with all the modes specified.\n\n    Parameters\n    ----------\n    template: object\n        An object that has attached properties. This can be used to substitute\n        for keyword arguments. A common example would be a row in an xml table.\n    taper: {None, float}, optional\n        Tapering at the beginning of the waveform with duration taper * tau.\n        This option is recommended with timescales taper=1./2 or 1. for\n        time-domain ringdown-only injections.\n        The abrupt turn on of the ringdown can cause issues on the waveform\n        when doing the fourier transform to the frequency domain. Setting\n        taper will add a rapid ringup with timescale tau/10.\n        Each mode and overtone will have a different taper depending on its tau,\n        the final taper being the superposition of all the tapers.\n    distance : {None, float}, optional\n        Luminosity distance of the system. If specified, the returned ringdown\n        will contain the factor (final_mass/distance).\n    final_mass : float\n        Mass of the final black hole.\n    final_spin : float\n        Spin of the final black hole.\n    lmns : list\n        Desired lmn modes as strings (lm modes available: 22, 21, 33, 44, 55).\n        The n specifies the number of overtones desired for the corresponding\n        lm pair (maximum n=8).\n        Example: lmns = ['223','331'] are the modes 220, 221, 222, and 330\n    amp220 : float\n        Amplitude of the fundamental 220 mode. Note that if distance is given,\n        this parameter will have a completely different order of magnitude.\n        See table II in https://arxiv.org/abs/1107.0854 for an estimate.\n    amplmn : float\n        Fraction of the amplitude of the lmn overtone relative to the\n        fundamental mode, as many as the number of subdominant modes.\n    philmn : float\n        Phase of the lmn overtone, as many as the number of modes. Should also\n        include the information from the azimuthal angle (phi + m*Phi).\n    inclination : float\n        Inclination of the system in radians.\n    delta_t : {None, float}, optional\n        The time step used to generate the ringdown.\n        If None, it will be set to the inverse of the frequency at which the\n        amplitude is 1/1000 of the peak amplitude (the minimum of all modes).\n    t_final : {None, float}, optional\n        The ending time of the output frequency series.\n        If None, it will be set to the time at which the amplitude\n        is 1/1000 of the peak amplitude (the maximum of all modes).\n\n    Returns\n    -------\n    hplus: TimeSeries\n        The plus phase of a ringdown with the lm modes specified and\n        n overtones in time domain.\n    hcross: TimeSeries\n        The cross phase of a ringdown with the lm modes specified and\n        n overtones in time domain.\n    \"\"\"\n\n    input_params = props(template, mass_spin_required_args, **kwargs)\n\n    # Get required args\n    final_mass = input_params['final_mass']\n    final_spin = input_params['final_spin']\n    lmns = input_params['lmns']\n    for lmn in lmns:\n        if int(lmn[2]) == 0:\n            raise ValueError('Number of overtones (nmodes) must be greater '\n                             'than zero.')\n    # following may not be in input_params\n    delta_t = input_params.pop('delta_t', None)\n    t_final = input_params.pop('t_final', None)\n\n    f_0, tau = get_lm_f0tau_allmodes(final_mass, final_spin, lmns)\n\n    if not delta_t:\n        delta_t = lm_deltat(f_0, tau, lmns)\n    if not t_final:\n        t_final = lm_tfinal(tau, lmns)\n\n    kmax = int(t_final / delta_t) + 1\n    # Different overtones will have different tapering window-size\n    # Find maximum window size to create long enough output vector\n    if taper:\n        taper_window = int(taper*max(tau.values())/delta_t)\n        kmax += taper_window\n\n    outplus = TimeSeries(zeros(kmax, dtype=float64), delta_t=delta_t)\n    outcross = TimeSeries(zeros(kmax, dtype=float64), delta_t=delta_t)\n    if taper:\n        start = - taper * max(tau.values())\n        outplus._epoch, outcross._epoch = start, start\n\n    for lmn in lmns:\n        l, m, nmodes = int(lmn[0]), int(lmn[1]), int(lmn[2])\n        hplus, hcross = get_td_lm(taper=taper, freqs=f_0, taus=tau,\n                             l=l, m=m, nmodes=nmodes,\n                             delta_t=delta_t, t_final=t_final,\n                             **input_params)\n        if not taper:\n            outplus.data += hplus.data\n            outcross.data += hcross.data\n        else:\n            outplus = taper_shift(hplus, outplus)\n            outcross = taper_shift(hcross, outcross)\n\n    norm = Kerr_factor(final_mass, distance) if distance else 1.\n    return norm*outplus, norm*outcross", "response": "Return a time domain ringdown with all the modes specified in the final mass and spin."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a frequency domain ringdown with all the modes specified in the final mass and spin.", "response": "def get_fd_from_final_mass_spin(template=None, distance=None, **kwargs):\n    \"\"\"Return frequency domain ringdown with all the modes specified.\n\n    Parameters\n    ----------\n    template: object\n        An object that has attached properties. This can be used to substitute\n        for keyword arguments. A common example would be a row in an xml table.\n    distance : {None, float}, optional\n        Luminosity distance of the system. If specified, the returned ringdown\n        will contain the factor (final_mass/distance).\n    final_mass : float\n        Mass of the final black hole.\n    final_spin : float\n        Spin of the final black hole.\n    lmns : list\n        Desired lmn modes as strings (lm modes available: 22, 21, 33, 44, 55).\n        The n specifies the number of overtones desired for the corresponding\n        lm pair (maximum n=8).\n        Example: lmns = ['223','331'] are the modes 220, 221, 222, and 330\n    amp220 : float\n        Amplitude of the fundamental 220 mode. Note that if distance is given,\n        this parameter will have a completely different order of magnitude.\n        See table II in https://arxiv.org/abs/1107.0854 for an estimate.\n    amplmn : float\n        Fraction of the amplitude of the lmn overtone relative to the\n        fundamental mode, as many as the number of subdominant modes.\n    philmn : float\n        Phase of the lmn overtone, as many as the number of modes. Should also\n        include the information from the azimuthal angle (phi + m*Phi).\n    inclination : float\n        Inclination of the system in radians.\n    delta_f : {None, float}, optional\n        The frequency step used to generate the ringdown.\n        If None, it will be set to the inverse of the time at which the\n        amplitude is 1/1000 of the peak amplitude (the minimum of all modes).\n    f_lower: {None, float}, optional\n        The starting frequency of the output frequency series.\n        If None, it will be set to delta_f.\n    f_final : {None, float}, optional\n        The ending frequency of the output frequency series.\n        If None, it will be set to the frequency at which the amplitude\n        is 1/1000 of the peak amplitude (the maximum of all modes).\n\n    Returns\n    -------\n    hplustilde: FrequencySeries\n        The plus phase of a ringdown with the lm modes specified and\n        n overtones in frequency domain.\n    hcrosstilde: FrequencySeries\n        The cross phase of a ringdown with the lm modes specified and\n        n overtones in frequency domain.\n    \"\"\"\n\n    input_params = props(template, mass_spin_required_args, **kwargs)\n\n    # Get required args\n    final_mass = input_params['final_mass']\n    final_spin = input_params['final_spin']\n    lmns = input_params['lmns']\n    for lmn in lmns:\n        if int(lmn[2]) == 0:\n            raise ValueError('Number of overtones (nmodes) must be greater '\n                             'than zero.')\n    # The following may not be in input_params\n    delta_f = input_params.pop('delta_f', None)\n    f_lower = input_params.pop('f_lower', None)\n    f_final = input_params.pop('f_final', None)\n\n    f_0, tau = get_lm_f0tau_allmodes(final_mass, final_spin, lmns)\n\n    if not delta_f:\n        delta_f = lm_deltaf(tau, lmns)\n    if not f_final:\n        f_final = lm_ffinal(f_0, tau, lmns)\n    if not f_lower:\n        f_lower = delta_f\n    kmax = int(f_final / delta_f) + 1\n\n    outplustilde = FrequencySeries(zeros(kmax, dtype=complex128), delta_f=delta_f)\n    outcrosstilde = FrequencySeries(zeros(kmax, dtype=complex128), delta_f=delta_f)\n    for lmn in lmns:\n        l, m, nmodes = int(lmn[0]), int(lmn[1]), int(lmn[2])\n        hplustilde, hcrosstilde = get_fd_lm(freqs=f_0, taus=tau,\n                                        l=l, m=m, nmodes=nmodes,\n                                        delta_f=delta_f, f_lower=f_lower,\n                                        f_final=f_final, **input_params)\n        outplustilde.data += hplustilde.data\n        outcrosstilde.data += hcrosstilde.data\n\n    norm = Kerr_factor(final_mass, distance) if distance else 1.\n    return norm*outplustilde, norm*outcrosstilde"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_td_from_freqtau(template=None, taper=None, **kwargs):\n\n    input_params = props(template, freqtau_required_args, **kwargs)\n\n    # Get required args\n    f_0, tau = lm_freqs_taus(**input_params)\n    lmns = input_params['lmns']\n    for lmn in lmns:\n        if int(lmn[2]) == 0:\n            raise ValueError('Number of overtones (nmodes) must be greater '\n                             'than zero.')\n    # following may not be in input_params\n    inc = input_params.pop('inclination', None)\n    delta_t = input_params.pop('delta_t', None)\n    t_final = input_params.pop('t_final', None)\n\n    if not delta_t:\n        delta_t = lm_deltat(f_0, tau, lmns)\n    if not t_final:\n        t_final = lm_tfinal(tau, lmns)\n\n    kmax = int(t_final / delta_t) + 1\n    # Different overtones will have different tapering window-size\n    # Find maximum window size to create long enough output vector\n    if taper:\n        taper_window = int(taper*max(tau.values())/delta_t)\n        kmax += taper_window\n\n    outplus = TimeSeries(zeros(kmax, dtype=float64), delta_t=delta_t)\n    outcross = TimeSeries(zeros(kmax, dtype=float64), delta_t=delta_t)\n    if taper:\n        start = - taper * max(tau.values())\n        outplus._epoch, outcross._epoch = start, start\n\n    for lmn in lmns:\n        l, m, nmodes = int(lmn[0]), int(lmn[1]), int(lmn[2])\n        hplus, hcross = get_td_lm(freqs=f_0, taus=tau, l=l, m=m, nmodes=nmodes,\n                             taper=taper, inclination=inc, delta_t=delta_t,\n                             t_final=t_final, **input_params)\n        if not taper:\n            outplus.data += hplus.data\n            outcross.data += hcross.data\n        else:\n            outplus = taper_shift(hplus, outplus)\n            outcross = taper_shift(hcross, outcross)\n\n    return outplus, outcross", "response": "Return a time domain ringdown with all the modes specified in the frequency domain."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_fd_from_freqtau(template=None, **kwargs):\n\n    input_params = props(template, freqtau_required_args, **kwargs)\n\n    # Get required args\n    f_0, tau = lm_freqs_taus(**input_params)\n    lmns = input_params['lmns']\n    for lmn in lmns:\n        if int(lmn[2]) == 0:\n            raise ValueError('Number of overtones (nmodes) must be greater '\n                             'than zero.')\n    # The following may not be in input_params\n    inc = input_params.pop('inclination', None)\n    delta_f = input_params.pop('delta_f', None)\n    f_lower = input_params.pop('f_lower', None)\n    f_final = input_params.pop('f_final', None)\n\n    if not delta_f:\n        delta_f = lm_deltaf(tau, lmns)\n    if not f_final:\n        f_final = lm_ffinal(f_0, tau, lmns)\n    if not f_lower:\n        f_lower = delta_f\n    kmax = int(f_final / delta_f) + 1\n\n    outplustilde = FrequencySeries(zeros(kmax, dtype=complex128), delta_f=delta_f)\n    outcrosstilde = FrequencySeries(zeros(kmax, dtype=complex128), delta_f=delta_f)\n    for lmn in lmns:\n        l, m, nmodes = int(lmn[0]), int(lmn[1]), int(lmn[2])\n        hplustilde, hcrosstilde = get_fd_lm(freqs=f_0, taus=tau,\n                                        l=l, m=m, nmodes=nmodes,\n                                        inclination=inc,\n                                        delta_f=delta_f, f_lower=f_lower,\n                                        f_final=f_final, **input_params)\n        outplustilde.data += hplustilde.data\n        outcrosstilde.data += hcrosstilde.data\n\n    return outplustilde, outcrosstilde", "response": "Returns a frequency domain ringdown with all the desired fundamental modes and the desired fundamental mode."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn an array of elements of the integral element of a density p.", "response": "def integral_element(mu, pdf):\n    '''\n    Returns an array of elements of the integrand dP = p(mu) dmu\n    for a density p(mu) defined at sample values mu ; samples need\n    not be equally spaced.  Uses a simple trapezium rule.\n    Number of dP elements is 1 - (number of mu samples).\n    '''\n    dmu = mu[1:] - mu[:-1]\n    bin_mean = (pdf[1:] + pdf[:-1]) / 2.\n    return dmu * bin_mean"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef normalize_pdf(mu, pofmu):\n    if min(pofmu) < 0:\n        raise ValueError(\"Probabilities cannot be negative, don't ask me to \"\n                         \"normalize a function with negative values!\")\n    if min(mu) < 0:\n        raise ValueError(\"Rates cannot be negative, don't ask me to \"\n                         \"normalize a function over a negative domain!\")\n\n    dp = integral_element(mu, pofmu)\n    return mu, pofmu/sum(dp)", "response": "Normalizes a function mu and pofmu to be a suitable pdf."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compute_upper_limit(mu_in, post, alpha=0.9):\n    if 0 < alpha < 1:\n        dp = integral_element(mu_in, post)\n        high_idx = bisect.bisect_left(dp.cumsum() / dp.sum(), alpha)\n        # if alpha is in (0,1] and post is non-negative, bisect_left\n        # will always return an index in the range of mu since\n        # post.cumsum()/post.sum() will always begin at 0 and end at 1\n        mu_high = mu_in[high_idx]\n    elif alpha == 1:\n        mu_high = numpy.max(mu_in[post > 0])\n    else:\n        raise ValueError(\"Confidence level must be in (0,1].\")\n\n    return mu_high", "response": "Compute the upper limit mu_high of the given confidence level alpha for a posterior distribution mu on the given parameter mu."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef compute_lower_limit(mu_in, post, alpha=0.9):\n    if 0 < alpha < 1:\n        dp = integral_element(mu_in, post)\n        low_idx = bisect.bisect_right(dp.cumsum() / dp.sum(), 1 - alpha)\n        # if alpha is in [0,1) and post is non-negative, bisect_right\n        # will always return an index in the range of mu since\n        # post.cumsum()/post.sum() will always begin at 0 and end at 1\n        mu_low = mu_in[low_idx]\n    elif alpha == 1:\n        mu_low = numpy.min(mu_in[post > 0])\n    else:\n        raise ValueError(\"Confidence level must be in (0,1].\")\n\n    return mu_low", "response": "Compute the lower limit mu_low of the given confidence level alpha for a\n    posterior distribution mu on the given parameter mu."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the minimal - width confidence interval mu_low mu_high for a posterior distribution mu on the parameter post.", "response": "def confidence_interval_min_width(mu, post, alpha=0.9):\n    '''\n    Returns the minimal-width confidence interval [mu_low, mu_high] of\n    confidence level alpha for a posterior distribution post on the parameter mu.\n    '''\n    if not 0 < alpha < 1:\n        raise ValueError(\"Confidence level must be in (0,1).\")\n\n    # choose a step size for the sliding confidence window\n    alpha_step = 0.01\n\n    # initialize the lower and upper limits\n    mu_low = numpy.min(mu)\n    mu_high = numpy.max(mu)\n\n    # find the smallest window (by delta-mu) stepping by dalpha\n    for ai in numpy.arange(0, 1 - alpha, alpha_step):\n        ml = compute_lower_limit(mu, post, 1 - ai)\n        mh = compute_upper_limit(mu, post, alpha + ai)\n        if mh - ml < mu_high - mu_low:\n            mu_low = ml\n            mu_high = mh\n\n    return mu_low, mu_high"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the HPD coverage of a pdf over mu taking only bins where the mean over the bin is above a given threshold.", "response": "def hpd_coverage(mu, pdf, thresh):\n    '''\n    Integrates a pdf over mu taking only bins where\n    the mean over the bin is above a given threshold\n    This gives the coverage of the HPD interval for\n    the given threshold.\n    '''\n    dp = integral_element(mu, pdf)\n    bin_mean = (pdf[1:] + pdf[:-1]) / 2.\n\n    return dp[bin_mean > thresh].sum()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding a density for a PDF post over samples mu_in find a density threshold such that the region having higher density has coverage of at least alpha and less than alpha plus a given tolerance.", "response": "def hpd_threshold(mu_in, post, alpha, tol):\n    '''\n    For a PDF post over samples mu_in, find a density\n    threshold such that the region having higher density\n    has coverage of at least alpha, and less than alpha\n    plus a given tolerance.\n    '''\n    norm_post = normalize_pdf(mu_in, post)\n    # initialize bisection search\n    p_minus = 0.0\n    p_plus = max(post)\n    while abs(hpd_coverage(mu_in, norm_post, p_minus) -\n              hpd_coverage(mu_in, norm_post, p_plus)) >= tol:\n        p_test = (p_minus + p_plus) / 2.\n        if hpd_coverage(mu_in, post, p_test) >= alpha:\n            # test value was too low or just right\n            p_minus = p_test\n        else:\n            # test value was too high\n            p_plus = p_test\n    # p_minus never goes above the required threshold and p_plus never goes below\n    # thus on exiting p_minus is at or below the required threshold and the\n    # difference in coverage is within tolerance\n    return p_minus"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef hpd_credible_interval(mu_in, post, alpha=0.9, tolerance=1e-3):\n    '''\n    Returns the minimum and maximum rate values of the HPD\n    (Highest Posterior Density) credible interval for a posterior\n    post defined at the sample values mu_in.  Samples need not be\n    uniformly spaced and posterior need not be normalized.\n\n    Will not return a correct credible interval if the posterior\n    is multimodal and the correct interval is not contiguous;\n    in this case will over-cover by including the whole range from\n    minimum to maximum mu.\n    '''\n    if alpha == 1:\n        nonzero_samples = mu_in[post > 0]\n        mu_low = numpy.min(nonzero_samples)\n        mu_high = numpy.max(nonzero_samples)\n    elif 0 < alpha < 1:\n        # determine the highest PDF for which the region with\n        # higher density has sufficient coverage\n        pthresh = hpd_threshold(mu_in, post, alpha, tol=tolerance)\n        samples_over_threshold = mu_in[post > pthresh]\n        mu_low = numpy.min(samples_over_threshold)\n        mu_high = numpy.max(samples_over_threshold)\n\n    return mu_low, mu_high", "response": "Returns the minimum and maximum rate values of the HPD\n    for a given posterior and the given number of samples."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compute_efficiency(f_dist, m_dist, dbins):\n    '''\n    Compute the efficiency as a function of distance for the given sets of found\n    and missed injection distances.\n    Note that injections that do not fit into any dbin get lost :(\n    '''\n    efficiency = numpy.zeros(len(dbins) - 1)\n    error = numpy.zeros(len(dbins) - 1)\n    for j, dlow in enumerate(dbins[:-1]):\n        dhigh = dbins[j + 1]\n        found = numpy.sum((dlow <= f_dist) * (f_dist < dhigh))\n        missed = numpy.sum((dlow <= m_dist) * (m_dist < dhigh))\n        if found+missed == 0:\n            # avoid divide by 0 in empty bins\n            missed = 1.\n        efficiency[j] = float(found) / (found + missed)\n        error[j] = numpy.sqrt(efficiency[j] * (1 - efficiency[j]) /\n                              (found + missed))\n\n    return efficiency, error", "response": "Compute the efficiency as a function of distance for the given set of found\nSSHAPY tables and the error of the found sources."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef filter_injections_by_mass(injs, mbins, bin_num, bin_type, bin_num2=None):\n    '''\n    For a given set of injections (sim_inspiral rows), return the subset\n    of injections that fall within the given mass range.\n    '''\n    if bin_type == \"Mass1_Mass2\":\n        m1bins = numpy.concatenate((mbins.lower()[0],\n                                    numpy.array([mbins.upper()[0][-1]])))\n        m1lo = m1bins[bin_num]\n        m1hi = m1bins[bin_num + 1]\n        m2bins = numpy.concatenate((mbins.lower()[1],\n                                    numpy.array([mbins.upper()[1][-1]])))\n        m2lo = m2bins[bin_num2]\n        m2hi = m2bins[bin_num2 + 1]\n        newinjs = [l for l in injs if\n                   ((m1lo <= l.mass1 < m1hi and m2lo <= l.mass2 < m2hi) or\n                    (m1lo <= l.mass2 < m1hi and m2lo <= l.mass1 < m2hi))]\n        return newinjs\n\n    mbins = numpy.concatenate((mbins.lower()[0],\n                               numpy.array([mbins.upper()[0][-1]])))\n    mlow = mbins[bin_num]\n    mhigh = mbins[bin_num + 1]\n    if bin_type == \"Chirp_Mass\":\n        newinjs = [l for l in injs if (mlow <= l.mchirp < mhigh)]\n    elif bin_type == \"Total_Mass\":\n        newinjs = [l for l in injs if (mlow <= l.mass1 + l.mass2 < mhigh)]\n    elif bin_type == \"Component_Mass\":\n        # here it is assumed that m2 is fixed\n        newinjs = [l for l in injs if (mlow <= l.mass1 < mhigh)]\n    elif bin_type == \"BNS_BBH\":\n        if bin_num in [0, 2]:\n            # BNS/BBH case\n            newinjs = [l for l in injs if\n                       (mlow <= l.mass1 < mhigh and mlow <= l.mass2 < mhigh)]\n        else:\n            # NSBH\n            newinjs = [l for l in injs if (mbins[0] <= l.mass1 < mbins[1] and\n                                           mbins[2] <= l.mass2 < mbins[3])]\n            # BHNS\n            newinjs += [l for l in injs if (mbins[0] <= l.mass2 < mbins[1] and\n                                            mbins[2] <= l.mass1 < mbins[3])]\n\n    return newinjs", "response": "Filter the set of injections by mass range."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the average luminosity an experiment was sensitive to", "response": "def compute_volume_vs_mass(found, missed, mass_bins, bin_type, dbins=None):\n    \"\"\"\n    Compute the average luminosity an experiment was sensitive to\n\n    Assumes that luminosity is uniformly distributed in space.\n    Input is the sets of found and missed injections.\n    \"\"\"\n    # mean and std estimate for luminosity\n    volArray = bin_utils.BinnedArray(mass_bins)\n    vol2Array = bin_utils.BinnedArray(mass_bins)\n\n    # found/missed stats\n    foundArray = bin_utils.BinnedArray(mass_bins)\n    missedArray = bin_utils.BinnedArray(mass_bins)\n\n    # compute the mean luminosity in each mass bin\n    effvmass = []\n    errvmass = []\n    # 2D case first\n    if bin_type == \"Mass1_Mass2\":\n        for j, mc1 in enumerate(mass_bins.centres()[0]):\n            for k, mc2 in enumerate(mass_bins.centres()[1]):\n                newfound = filter_injections_by_mass(\n                                              found, mass_bins, j, bin_type, k)\n                newmissed = filter_injections_by_mass(\n                                             missed, mass_bins, j, bin_type, k)\n\n                foundArray[(mc1, mc2)] = len(newfound)\n                missedArray[(mc1, mc2)] = len(newmissed)\n\n                # compute the volume using this injection set\n                meaneff, efferr, meanvol, volerr = mean_efficiency_volume(\n                                                    newfound, newmissed, dbins)\n                effvmass.append(meaneff)\n                errvmass.append(efferr)\n                volArray[(mc1, mc2)] = meanvol\n                vol2Array[(mc1, mc2)] = volerr\n\n        return volArray, vol2Array, foundArray, missedArray, effvmass, errvmass\n\n    for j, mc in enumerate(mass_bins.centres()[0]):\n\n        # filter out injections not in this mass bin\n        newfound = filter_injections_by_mass(found, mass_bins, j, bin_type)\n        newmissed = filter_injections_by_mass(missed, mass_bins, j, bin_type)\n\n        foundArray[(mc, )] = len(newfound)\n        missedArray[(mc, )] = len(newmissed)\n\n        # compute the volume using this injection set\n        meaneff, efferr, meanvol, volerr = mean_efficiency_volume(\n                                                    newfound, newmissed, dbins)\n        effvmass.append(meaneff)\n        errvmass.append(efferr)\n        volArray[(mc, )] = meanvol\n        vol2Array[(mc, )] = volerr\n\n    return volArray, vol2Array, foundArray, missedArray, effvmass, errvmass"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding the options used to specify optimization - specific options.", "response": "def insert_optimization_option_group(parser):\n    \"\"\"\n    Adds the options used to specify optimization-specific options.\n\n    Parameters\n    ----------\n    parser : object\n        OptionParser instance\n    \"\"\"\n    optimization_group = parser.add_argument_group(\"Options for selecting \"\n                                   \"optimization-specific settings\")\n\n    optimization_group.add_argument(\"--cpu-affinity\", help=\"\"\"\n                    A set of CPUs on which to run, specified in a format suitable\n                    to pass to taskset.\"\"\")\n    optimization_group.add_argument(\"--cpu-affinity-from-env\", help=\"\"\"\n                    The name of an enivornment variable containing a set\n                    of CPUs on which to run,  specified in a format suitable\n                    to pass to taskset.\"\"\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the CLI options and verifies that they are consistent and have the correct attributes.", "response": "def verify_optimization_options(opt, parser):\n    \"\"\"Parses the CLI options, verifies that they are consistent and\n    reasonable, and acts on them if they are\n\n    Parameters\n    ----------\n    opt : object\n        Result of parsing the CLI with OptionParser, or any object with the\n        required attributes\n    parser : object\n        OptionParser instance.\n    \"\"\"\n\n    # Pin to specified CPUs if requested\n    requested_cpus = None\n\n    if opt.cpu_affinity_from_env is not None:\n        if opt.cpu_affinity is not None:\n            logging.error(\"Both --cpu_affinity_from_env and --cpu_affinity specified\")\n            sys.exit(1)\n\n        requested_cpus = os.environ.get(opt.cpu_affinity_from_env)\n\n        if requested_cpus is None:\n            logging.error(\"CPU affinity requested from environment variable %s \"\n                          \"but this variable is not defined\" % opt.cpu_affinity_from_env)\n            sys.exit(1)\n\n        if requested_cpus == '':\n            logging.error(\"CPU affinity requested from environment variable %s \"\n                          \"but this variable is empty\" % opt.cpu_affinity_from_env)\n            sys.exit(1)\n\n    if requested_cpus is None:\n        requested_cpus = opt.cpu_affinity\n\n    if requested_cpus is not None:\n        command = 'taskset -pc %s %d' % (requested_cpus, os.getpid())\n        retcode = os.system(command)\n\n        if retcode != 0:\n            logging.error('taskset command <%s> failed with return code %d' % \\\n                          (command, retcode))\n            sys.exit(1)\n\n        logging.info(\"Pinned to CPUs %s \" % requested_cpus)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef calculate_acf(data, delta_t=1.0, unbiased=False):\n\n    # if given a TimeSeries instance then get numpy.array\n    if isinstance(data, TimeSeries):\n        y = data.numpy()\n        delta_t = data.delta_t\n    else:\n        y = data\n\n    # Zero mean\n    y = y - y.mean()\n    ny_orig = len(y)\n\n    npad = 1\n    while npad < 2*ny_orig:\n        npad = npad << 1\n    ypad = numpy.zeros(npad)\n    ypad[:ny_orig] = y\n\n    # FFT data minus the mean\n    fdata = TimeSeries(ypad, delta_t=delta_t).to_frequencyseries()\n\n    # correlate\n    # do not need to give the congjugate since correlate function does it\n    cdata = FrequencySeries(zeros(len(fdata), dtype=fdata.dtype),\n                           delta_f=fdata.delta_f, copy=False)\n    correlate(fdata, fdata, cdata)\n\n    # IFFT correlated data to get unnormalized autocovariance time series\n    acf = cdata.to_timeseries()\n    acf = acf[:ny_orig]\n\n    # normalize the autocovariance\n    # note that dividing by acf[0] is the same as ( y.var() * len(acf) )\n    if unbiased:\n        acf /= ( y.var() * numpy.arange(len(acf), 0, -1) )\n    else:\n        acf /= acf[0]\n\n    # return input datatype\n    if isinstance(data, TimeSeries):\n        return TimeSeries(acf, delta_t=delta_t)\n    else:\n        return acf", "response": "r Calculates the one - sided autocorrelation function."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds the options that affect the behavior of this backend s FFTs.", "response": "def insert_fft_options(optgroup):\n    \"\"\"\n    Inserts the options that affect the behavior of this backend\n\n    Parameters\n    ----------\n    optgroup: fft_option\n       OptionParser argument group whose options are extended\n    \"\"\"\n    optgroup.add_argument(\"--fftw-measure-level\",\n                      help=\"Determines the measure level used in planning \"\n                           \"FFTW FFTs; allowed values are: \" + str([0,1,2,3]),\n                      type=int, default=_default_measurelvl)\n    optgroup.add_argument(\"--fftw-threads-backend\",\n                      help=\"Give 'openmp', 'pthreads' or 'unthreaded' to specify which threaded FFTW to use\",\n                      default=None)\n    optgroup.add_argument(\"--fftw-input-float-wisdom-file\",\n                      help=\"Filename from which to read single-precision wisdom\",\n                      default=None)\n    optgroup.add_argument(\"--fftw-input-double-wisdom-file\",\n                      help=\"Filename from which to read double-precision wisdom\",\n                      default=None)\n    optgroup.add_argument(\"--fftw-output-float-wisdom-file\",\n                      help=\"Filename to which to write single-precision wisdom\",\n                      default=None)\n    optgroup.add_argument(\"--fftw-output-double-wisdom-file\",\n                      help=\"Filename to which to write double-precision wisdom\",\n                      default=None)\n    optgroup.add_argument(\"--fftw-import-system-wisdom\",\n                          help = \"If given, call fftw[f]_import_system_wisdom()\",\n                          action = \"store_true\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses the FFT options and verifies that they are reasonable.", "response": "def verify_fft_options(opt,parser):\n    \"\"\"Parses the FFT options and verifies that they are\n       reasonable.\n\n    Parameters\n    ----------\n    opt : object\n        Result of parsing the CLI with OptionParser, or any object with the\n        required attributes.\n    parser : object\n        OptionParser instance.\n    \"\"\"\n    if opt.fftw_measure_level not in [0,1,2,3]:\n        parser.error(\"{0} is not a valid FFTW measure level.\".format(opt.fftw_measure_level))\n\n    if opt.fftw_import_system_wisdom and ((opt.fftw_input_float_wisdom_file is not None)\n                                          or (opt.fftw_input_double_wisdom_file is not None)):\n        parser.error(\"If --fftw-import-system-wisdom is given, then you cannot give\"\n                     \" either of --fftw-input-float-wisdom-file or --fftw-input-double-wisdom-file\")\n\n    if opt.fftw_threads_backend is not None:\n        if opt.fftw_threads_backend not in ['openmp','pthreads','unthreaded']:\n            parser.error(\"Invalid threads backend; must be 'openmp', 'pthreads' or 'unthreaded'\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef kl(samples1, samples2, pdf1=False, pdf2=False,\n       bins=30, hist_min=None, hist_max=None):\n    \"\"\" Computes the Kullback-Leibler divergence for a single parameter\n    from two distributions.\n\n    Parameters\n    ----------\n    samples1 : numpy.array\n        Samples or probability density function (must also set `pdf1=True`).\n    samples2 : numpy.array\n        Samples or probability density function (must also set `pdf2=True`).\n    pdf1 : bool\n        Set to `True` if `samples1` is a probability density funtion already.\n    pdf2 : bool\n        Set to `True` if `samples2` is a probability density funtion already.\n    bins : int\n        Number of bins to use when calculating probability density function\n        from a set of samples of the distribution.\n    hist_min : numpy.float64\n        Minimum of the distributions' values to use.\n    hist_max : numpy.float64\n        Maximum of the distributions' values to use.\n\n    Returns\n    -------\n    numpy.float64\n        The Kullback-Leibler divergence value.\n    \"\"\"\n    hist_range = (hist_min, hist_max)\n    if not pdf1:\n        samples1, _ = numpy.histogram(samples1, bins=bins,\n                                      range=hist_range, normed=True)\n    if not pdf2:\n        samples2, _ = numpy.histogram(samples2, bins=bins,\n                                      range=hist_range, normed=True)\n    return stats.entropy(samples1, qk=samples2)", "response": "Computes the Kullback - Leibler divergence for a single parameter base on two distributions."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns an RST - formatted table of keys and values from a dictionary.", "response": "def rst_dict_table(dict_, key_format=str, val_format=str, header=None,\n                   sort=True):\n    \"\"\"Returns an RST-formatted table of keys and values from a `dict`\n\n    Parameters\n    ----------\n    dict_ : dict\n        data to display in table\n    key_format : callable\n        callable function with which to format keys\n    val_format : callable\n        callable function with which to format values\n    header : None, tuple of str\n        a 2-tuple of header for the two columns, or `None` to exclude\n        a header line (default)\n    sort : bool, optional\n        Sort the dictionary keys alphabetically when writing the table.\n\n    Examples\n    --------\n    >>> a = {'key1': 'value1', 'key2': 'value2'}\n    >>> print(rst_dict_table(a))\n    ====  ======\n    key1  value1\n    key2  value2\n    ====  ======\n    >>> print(rst_dict_table(a, key_format='``{}``'.format,\n    ...                      val_format=':class:`{}`'.format,\n    ...                      header=('Key', 'Value'))\n    ========  ===============\n    Key       Value\n    ========  ===============\n    ``key1``  :class:`value1`\n    ``key2``  :class:`value2`\n    ========  ===============\n    \"\"\"\n    keys, values = zip(*dict_.items())\n\n    # apply formatting\n    keys = map(key_format, keys)\n    values = map(val_format, values)\n\n    # work out longest elements in each column\n    nckey = max(map(len, keys))\n    ncval = max(map(len, values))\n    if header:\n        khead, vhead = header\n        nckey = max(nckey, len(khead))\n        ncval = max(ncval, len(vhead))\n\n    # build table header line\n    divider = \"{}  {}\".format('='*nckey, '='*ncval)\n\n    def row(key, val):\n        fmt = '{{0:{0}s}}  {{1}}'.format(nckey, ncval)\n        return fmt.format(key, val)\n\n    # build table of lines\n    lines = [divider]\n    if header:\n        lines.extend((row(*header), divider))\n    params = zip(keys, values)\n    if sort:\n        params = sorted(params)\n    for key, val in params:\n        fmt = '{{0:{0}s}}  {{1}}'.format(nckey, ncval)\n        lines.append(fmt.format(key, val))\n    lines.append(divider)\n\n    return '\\n'.join(lines)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_model_from_config(cp, ifo, section=\"calibration\"):\n    model = cp.get_opt_tag(section, \"{}_model\".format(ifo.lower()), None)\n    recalibrator = models[model].from_config(cp, ifo.lower(), section)\n\n    return recalibrator", "response": "Reads the calibration model specified in the\n    given configuration file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing the given option into something understandable by the given iif. gates.", "response": "def _gates_from_cli(opts, gate_opt):\n    \"\"\"Parses the given `gate_opt` into something understandable by\n    `strain.gate_data`.\n    \"\"\"\n    gates = {}\n    if getattr(opts, gate_opt) is None:\n        return gates\n    for gate in getattr(opts, gate_opt):\n        try:\n            ifo, central_time, half_dur, taper_dur = gate.split(':')\n            central_time = float(central_time)\n            half_dur = float(half_dur)\n            taper_dur = float(taper_dur)\n        except ValueError:\n            raise ValueError(\"--gate {} not formatted correctly; \".format(\n                gate) + \"see help\")\n        try:\n            gates[ifo].append((central_time, half_dur, taper_dur))\n        except KeyError:\n            gates[ifo] = [(central_time, half_dur, taper_dur)]\n    return gates"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\napplies the given dictionary of gates to the given dictionary of time - domain gates.", "response": "def apply_gates_to_td(strain_dict, gates):\n    \"\"\"Applies the given dictionary of gates to the given dictionary of\n    strain.\n\n    Parameters\n    ----------\n    strain_dict : dict\n        Dictionary of time-domain strain, keyed by the ifos.\n    gates : dict\n        Dictionary of gates. Keys should be the ifo to apply the data to,\n        values are a tuple giving the central time of the gate, the half\n        duration, and the taper duration.\n\n    Returns\n    -------\n    dict\n        Dictionary of time-domain strain with the gates applied.\n    \"\"\"\n    # copy data to new dictionary\n    outdict = dict(strain_dict.items())\n    for ifo in gates:\n        outdict[ifo] = strain.gate_data(outdict[ifo], gates[ifo])\n    return outdict"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\napply the given dictionary of gates to the frequency domain and then FFT - ing back to the frequency domain.", "response": "def apply_gates_to_fd(stilde_dict, gates):\n    \"\"\"Applies the given dictionary of gates to the given dictionary of\n    strain in the frequency domain.\n\n    Gates are applied by IFFT-ing the strain data to the time domain, applying\n    the gate, then FFT-ing back to the frequency domain.\n\n    Parameters\n    ----------\n    stilde_dict : dict\n        Dictionary of frequency-domain strain, keyed by the ifos.\n    gates : dict\n        Dictionary of gates. Keys should be the ifo to apply the data to,\n        values are a tuple giving the central time of the gate, the half\n        duration, and the taper duration.\n\n    Returns\n    -------\n    dict\n        Dictionary of frequency-domain strain with the gates applied.\n    \"\"\"\n    # copy data to new dictionary\n    outdict = dict(stilde_dict.items())\n    # create a time-domin strain dictionary to apply the gates to\n    strain_dict = dict([[ifo, outdict[ifo].to_timeseries()] for ifo in gates])\n    # apply gates and fft back to the frequency domain\n    for ifo,d in apply_gates_to_td(strain_dict, gates).items():\n        outdict[ifo] = d.to_frequencyseries()\n    return outdict"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_gate_option_group(parser):\n    gate_group = parser.add_argument_group(\"Options for gating data.\")\n\n    gate_group.add_argument(\"--gate\", nargs=\"+\", type=str,\n                            metavar=\"IFO:CENTRALTIME:HALFDUR:TAPERDUR\",\n                            help=\"Apply one or more gates to the data before \"\n                                 \"filtering.\")\n    gate_group.add_argument(\"--gate-overwhitened\", action=\"store_true\",\n                            help=\"Overwhiten data first, then apply the \"\n                                 \"gates specified in --gate. Overwhitening \"\n                                 \"allows for sharper tapers to be used, \"\n                                 \"since lines are not blurred.\")\n    gate_group.add_argument(\"--psd-gate\", nargs=\"+\", type=str,\n                            metavar=\"IFO:CENTRALTIME:HALFDUR:TAPERDUR\",\n                            help=\"Apply one or more gates to the data used \"\n                                 \"for computing the PSD. Gates are applied \"\n                                 \"prior to FFT-ing the data for PSD \"\n                                 \"estimation.\")\n    return gate_group", "response": "Adds the options needed to apply gates to data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef power_chisq_bins_from_sigmasq_series(sigmasq_series, num_bins, kmin, kmax):\n    sigmasq = sigmasq_series[kmax - 1]\n    edge_vec = numpy.arange(0, num_bins) * sigmasq / num_bins\n    bins = numpy.searchsorted(sigmasq_series[kmin:kmax], edge_vec, side='right')\n    bins += kmin\n    return numpy.append(bins, kmax)", "response": "Calculates the chisq bins of equal power for use with the chisq functions."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of bins of equal power for use with the chisq functions.", "response": "def power_chisq_bins(htilde, num_bins, psd, low_frequency_cutoff=None,\n                     high_frequency_cutoff=None):\n    \"\"\"Returns bins of equal power for use with the chisq functions\n\n    Parameters\n    ----------\n\n    htilde: FrequencySeries\n        A frequency series containing the template waveform\n    num_bins: int\n        The number of chisq bins to calculate.\n    psd: FrequencySeries\n        A frequency series containing the psd. Its length must be commensurate\n        with the template waveform.\n    low_frequency_cutoff: {None, float}, optional\n        The low frequency cutoff to apply\n    high_frequency_cutoff: {None, float}, optional\n        The high frequency cutoff to apply\n\n    Returns\n    -------\n\n    bins: List of ints\n        A list of the edges of the chisq bins is returned.\n    \"\"\"\n    sigma_vec = sigmasq_series(htilde, psd, low_frequency_cutoff,\n                               high_frequency_cutoff).numpy()\n    kmin, kmax = get_cutoff_indices(low_frequency_cutoff,\n                                    high_frequency_cutoff,\n                                    htilde.delta_f,\n                                    (len(htilde)-1)*2)\n    return power_chisq_bins_from_sigmasq_series(sigma_vec, num_bins, kmin, kmax)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the chisq timeseries from precomputed values for only select points.", "response": "def power_chisq_at_points_from_precomputed(corr, snr, snr_norm, bins, indices):\n    \"\"\"Calculate the chisq timeseries from precomputed values for only select points.\n\n    This function calculates the chisq at each point by explicitly time shifting\n    and summing each bin. No FFT is involved.\n\n    Parameters\n    ----------\n    corr: FrequencySeries\n        The product of the template and data in the frequency domain.\n    snr: numpy.ndarray\n        The unnormalized array of snr values at only the selected points in `indices`.\n    snr_norm: float\n        The normalization of the snr (EXPLAINME : refer to Findchirp paper?)\n    bins: List of integers\n        The edges of the equal power bins\n    indices: Array\n        The indices where we will calculate the chisq. These must be relative\n        to the given `corr` series.\n\n    Returns\n    -------\n    chisq: Array\n        An array containing only the chisq at the selected points.\n    \"\"\"\n    num_bins = len(bins) - 1\n    chisq = shift_sum(corr, indices, bins) # pylint:disable=assignment-from-no-return\n    return (chisq * num_bins - (snr.conj() * snr).real) * (snr_norm ** 2.0)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef power_chisq_from_precomputed(corr, snr, snr_norm, bins, indices=None, return_bins=False):\n    # Get workspace memory\n    global _q_l, _qtilde_l, _chisq_l\n\n    bin_snrs = []\n\n    if _q_l is None or len(_q_l) != len(snr):\n        q = zeros(len(snr), dtype=complex_same_precision_as(snr))\n        qtilde = zeros(len(snr), dtype=complex_same_precision_as(snr))\n        _q_l = q\n        _qtilde_l = qtilde\n    else:\n        q = _q_l\n        qtilde = _qtilde_l\n\n    if indices is not None:\n        snr = snr.take(indices)\n\n    if _chisq_l is None or len(_chisq_l) < len(snr):\n        chisq = zeros(len(snr), dtype=real_same_precision_as(snr))\n        _chisq_l = chisq\n    else:\n        chisq = _chisq_l[0:len(snr)]\n        chisq.clear()\n\n    num_bins = len(bins) - 1\n\n    for j in range(num_bins):\n        k_min = int(bins[j])\n        k_max = int(bins[j+1])\n\n        qtilde[k_min:k_max] = corr[k_min:k_max]\n        pycbc.fft.ifft(qtilde, q)\n        qtilde[k_min:k_max].clear()\n\n        if return_bins:\n            bin_snrs.append(TimeSeries(q  * snr_norm *  num_bins ** 0.5,\n                                      delta_t=snr.delta_t,\n                                      epoch=snr.start_time))\n\n        if indices is not None:\n            chisq_accum_bin(chisq, q.take(indices))\n        else:\n            chisq_accum_bin(chisq, q)\n\n    chisq = (chisq * num_bins - snr.squared_norm()) * (snr_norm ** 2.0)\n\n    if indices is None:\n        chisq = TimeSeries(chisq, delta_t=snr.delta_t, epoch=snr.start_time, copy=False)\n\n    if return_bins:\n        return chisq, bin_snrs\n    else:\n        return chisq", "response": "Calculate the chisq time series from precomputed values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the chisq time series for a given template and data.", "response": "def power_chisq(template, data, num_bins, psd,\n                low_frequency_cutoff=None,\n                high_frequency_cutoff=None,\n                return_bins=False):\n    \"\"\"Calculate the chisq timeseries\n\n    Parameters\n    ----------\n    template: FrequencySeries or TimeSeries\n        A time or frequency series that contains the filter template.\n    data: FrequencySeries or TimeSeries\n        A time or frequency series that contains the data to filter. The length\n        must be commensurate with the template.\n        (EXPLAINME - does this mean 'the same as' or something else?)\n    num_bins: int\n        The number of bins in the chisq. Note that the dof goes as 2*num_bins-2.\n    psd: FrequencySeries\n        The psd of the data.\n    low_frequency_cutoff: {None, float}, optional\n        The low frequency cutoff for the filter\n    high_frequency_cutoff: {None, float}, optional\n        The high frequency cutoff for the filter\n    return_bins: {boolean, False}, optional\n        Return a list of the individual chisq bins\n\n    Returns\n    -------\n    chisq: TimeSeries\n        TimeSeries containing the chisq values for all times.\n    \"\"\"\n    htilde = make_frequency_series(template)\n    stilde = make_frequency_series(data)\n\n    bins = power_chisq_bins(htilde, num_bins, psd, low_frequency_cutoff,\n                            high_frequency_cutoff)\n    corra = zeros((len(htilde)-1)*2, dtype=htilde.dtype)\n    total_snr, corr, tnorm = matched_filter_core(htilde, stilde, psd,\n                           low_frequency_cutoff, high_frequency_cutoff,\n                           corr_out=corra)\n\n    return power_chisq_from_precomputed(corr, total_snr, tnorm, bins, return_bins=return_bins)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef values(self, corr, snrv, snr_norm, psd, indices, template):\n\n        if self.do:\n            num_above = len(indices)\n            if self.snr_threshold:\n                above = abs(snrv * snr_norm) > self.snr_threshold\n                num_above = above.sum()\n                logging.info('%s above chisq activation threshold' % num_above)\n                above_indices = indices[above]\n                above_snrv = snrv[above]\n                rchisq = numpy.zeros(len(indices), dtype=numpy.float32)\n                dof = -100\n            else:\n                above_indices = indices\n                above_snrv = snrv\n\n            if num_above > 0:\n                bins = self.cached_chisq_bins(template, psd)\n                dof = (len(bins) - 1) * 2 - 2\n                chisq = power_chisq_at_points_from_precomputed(corr,\n                                     above_snrv, snr_norm, bins, above_indices)\n\n            if self.snr_threshold:\n                if num_above > 0:\n                    rchisq[above] = chisq\n            else:\n                rchisq = chisq\n\n            return rchisq, numpy.repeat(dof, len(indices))# dof * numpy.ones_like(indices)\n        else:\n            return None, None", "response": "Calculate the chisq values given by indices."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef calculate_chisq_bins(self, template, psd):\n        num_bins = int(self.parse_option(template, self.num_bins))\n        if hasattr(psd, 'sigmasq_vec') and \\\n                                       template.approximant in psd.sigmasq_vec:\n            kmin = int(template.f_lower / psd.delta_f)\n            kmax = template.end_idx\n            bins = power_chisq_bins_from_sigmasq_series(\n                   psd.sigmasq_vec[template.approximant], num_bins, kmin, kmax)\n        else:\n            bins = power_chisq_bins(template, num_bins, psd, template.f_lower)\n        return bins", "response": "Calculate the chisq bins for this template and PSD."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the chisq at points given by indices.", "response": "def values(self, corr_plus, corr_cross, snrv, psd,\n               indices, template_plus, template_cross, u_vals,\n               hplus_cross_corr, hpnorm, hcnorm):\n        \"\"\" Calculate the chisq at points given by indices.\n\n        Returns\n        -------\n        chisq: Array\n            Chisq values, one for each sample index\n\n        chisq_dof: Array\n            Number of statistical degrees of freedom for the chisq test\n            in the given template\n        \"\"\"\n        if self.do:\n            num_above = len(indices)\n            if self.snr_threshold:\n                above = abs(snrv) > self.snr_threshold\n                num_above = above.sum()\n                logging.info('%s above chisq activation threshold' % num_above)\n                above_indices = indices[above]\n                above_snrv = snrv[above]\n                rchisq = numpy.zeros(len(indices), dtype=numpy.float32)\n                dof = -100\n            else:\n                above_indices = indices\n                above_snrv = snrv\n\n            if num_above > 0:\n                chisq = []\n                curr_tmplt_mult_fac = 0.\n                curr_corr_mult_fac = 0.\n                if self.template_mem is None or \\\n                        (not len(self.template_mem) == len(template_plus)):\n                    self.template_mem = zeros(len(template_plus),\n                                dtype=complex_same_precision_as(corr_plus))\n                if self.corr_mem is None or \\\n                                (not len(self.corr_mem) == len(corr_plus)):\n                    self.corr_mem = zeros(len(corr_plus),\n                                dtype=complex_same_precision_as(corr_plus))\n\n                tmplt_data = template_cross.data\n                corr_data = corr_cross.data\n                numpy.copyto(self.template_mem.data, template_cross.data)\n                numpy.copyto(self.corr_mem.data, corr_cross.data)\n                template_cross._data = self.template_mem.data\n                corr_cross._data = self.corr_mem.data\n\n                for lidx, index in enumerate(above_indices):\n                    above_local_indices = numpy.array([index])\n                    above_local_snr = numpy.array([above_snrv[lidx]])\n                    local_u_val = u_vals[lidx]\n                    # Construct template from _plus and _cross\n                    # Note that this modifies in place, so we store that and\n                    # revert on the next pass.\n                    template = template_cross.multiply_and_add(template_plus,\n                                               local_u_val-curr_tmplt_mult_fac)\n                    curr_tmplt_mult_fac = local_u_val\n\n                    template.f_lower = template_plus.f_lower\n                    template.params = template_plus.params\n                    # Construct the corr vector\n                    norm_fac = local_u_val*local_u_val + 1\n                    norm_fac += 2 * local_u_val * hplus_cross_corr\n                    norm_fac = hcnorm / (norm_fac**0.5)\n                    hp_fac = local_u_val * hpnorm / hcnorm\n                    corr = corr_cross.multiply_and_add(corr_plus,\n                                                   hp_fac - curr_corr_mult_fac)\n                    curr_corr_mult_fac = hp_fac\n\n                    bins = self.calculate_chisq_bins(template, psd)\n                    dof = (len(bins) - 1) * 2 - 2\n                    curr_chisq = power_chisq_at_points_from_precomputed(corr,\n                                          above_local_snr/ norm_fac, norm_fac,\n                                          bins, above_local_indices)\n                    chisq.append(curr_chisq[0])\n                chisq = numpy.array(chisq)\n                # Must reset corr and template to original values!\n                template_cross._data = tmplt_data\n                corr_cross._data = corr_data\n\n            if self.snr_threshold:\n                if num_above > 0:\n                    rchisq[above] = chisq\n            else:\n                rchisq = chisq\n\n            return rchisq, numpy.repeat(dof, len(indices))# dof * numpy.ones_like(indices)\n        else:\n            return None, None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates rst files in the _include directory using the python scripts there.", "response": "def build_includes():\n    \"\"\"Creates rst files in the _include directory using the python scripts\n    there.\n\n    This will ignore any files in the _include directory that start with ``_``.\n    \"\"\"\n    print(\"Running scripts in _include:\")\n    cwd = os.getcwd()\n    os.chdir('_include')\n    pyfiles = glob.glob('*.py')\n    for fn in pyfiles:\n        if not fn.startswith('_'):\n            print(' {}'.format(fn))\n            subprocess.check_output(['python', fn])\n    os.chdir(cwd)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef apply_cyclic(value, bounds):\n    return (value - bounds._min) %(bounds._max - bounds._min) + bounds._min", "response": "Given a value applies cyclic boundary conditions between the minimum\n    and maximum bounds."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reflect_well(value, bounds):\n    while value not in bounds:\n        value = bounds._max.reflect_left(value)\n        value = bounds._min.reflect_right(value)\n    return value", "response": "Given some boundaries reflects the value until it falls within the bounds."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef apply_conditions(self, value):\n        retval = value\n        if self._cyclic:\n            retval = apply_cyclic(value, self)\n        retval = self._reflect(retval)\n        if isinstance(retval, numpy.ndarray) and retval.size == 1:\n            try:\n                retval = retval[0]\n            except IndexError:\n                retval = float(retval)\n        return retval", "response": "Applies any conditions to the given value."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing a veto definer file and return a dictionary indexed by ifo then category level and a list of veto definitions.", "response": "def parse_veto_definer(veto_def_filename):\n    \"\"\" Parse a veto definer file from the filename and return a dictionary\n    indexed by ifo and veto definer category level.\n\n    Parameters\n    ----------\n    veto_def_filename: str\n        The path to the veto definer file\n\n    Returns:\n        parsed_definition: dict\n            Returns a dictionary first indexed by ifo, then category level, and\n            finally a list of veto definitions.\n    \"\"\"\n    from glue.ligolw import table, lsctables, utils as ligolw_utils\n    from glue.ligolw.ligolw import LIGOLWContentHandler as h\n    lsctables.use_in(h)\n\n    indoc = ligolw_utils.load_filename(veto_def_filename, False,\n                                       contenthandler=h)\n    veto_table = table.get_table(indoc, 'veto_definer')\n\n    ifo = veto_table.getColumnByName('ifo')\n    name = veto_table.getColumnByName('name')\n    version = numpy.array(veto_table.getColumnByName('version'))\n    category = numpy.array(veto_table.getColumnByName('category'))\n    start = numpy.array(veto_table.getColumnByName('start_time'))\n    end = numpy.array(veto_table.getColumnByName('end_time'))\n    start_pad = numpy.array(veto_table.getColumnByName('start_pad'))\n    end_pad = numpy.array(veto_table.getColumnByName('end_pad'))\n\n    data = {}\n    for i in range(len(veto_table)):\n        if ifo[i] not in data:\n            data[ifo[i]] = {}\n\n        # The veto-definer categories are weird! Hardware injections are stored\n        # in \"3\" and numbers above that are bumped up by one (although not\n        # often used any more). So we remap 3 to H and anything above 3 to\n        # N-1. 2 and 1 correspond to 2 and 1 (YAY!)\n        if category[i] > 3:\n            curr_cat = \"CAT_{}\".format(category[i]-1)\n        elif category[i] == 3:\n            curr_cat = \"CAT_H\"\n        else:\n            curr_cat = \"CAT_{}\".format(category[i])\n\n        if curr_cat not in data[ifo[i]]:\n            data[ifo[i]][curr_cat] = []\n\n        veto_info = {'name': name[i],\n                     'version': version[i],\n                     'start': start[i],\n                     'end': end[i],\n                     'start_pad': start_pad[i],\n                     'end_pad': end_pad[i],\n                     }\n        data[ifo[i]][curr_cat].append(veto_info)\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef query_flag(ifo, name, start_time, end_time,\n               source='any', server=\"segments.ligo.org\",\n               veto_definer=None, cache=False):\n    \"\"\"Return the times where the flag is active\n\n    Parameters\n    ----------\n    ifo: string\n        The interferometer to query (H1, L1).\n    name: string\n        The status flag to query from LOSC.\n    start_time: int\n        The starting gps time to begin querying from LOSC\n    end_time: int\n        The end gps time of the query\n    source: str, Optional\n        Choice between \"GWOSC\" or \"dqsegdb\". If dqsegdb, the server option may\n        also be given. The default is to try GWOSC first then try dqsegdb.\n    server: str, Optional\n        The server path. Only used with dqsegdb atm.\n    veto_definer: str, Optional\n        The path to a veto definer to define groups of flags which\n        themselves define a set of segments.\n    cache: bool\n        If true cache the query. Default is not to cache\n\n    Returns\n    ---------\n    segments: glue.segments.segmentlist\n        List of segments\n    \"\"\"\n    info = name.split(':')\n    if len(info) == 2:\n        segment_name, version = info\n    elif len(info) == 1:\n        segment_name = info[0]\n        version = 1\n\n    flag_segments = segmentlist([])\n\n    if source in ['GWOSC', 'any']:\n        # Special cases as the LOSC convention is backwards from normal\n        # LIGO / Virgo operation!!!!\n        if (('_HW_INJ' in segment_name and 'NO' not in segment_name) or\n           'VETO' in segment_name):\n            data = query_flag(ifo, 'DATA', start_time, end_time)\n\n            if '_HW_INJ' in segment_name:\n                name = 'NO_' + segment_name\n            else:\n                name = segment_name.replace('_VETO', '')\n\n            negate = query_flag(ifo, name, start_time, end_time, cache=cache)\n            return (data - negate).coalesce()\n\n        duration = end_time - start_time\n        url = GWOSC_URL.format(get_run(start_time + duration/2),\n                               ifo, segment_name,\n                               int(start_time), int(duration))\n\n        try:\n            fname = download_file(url, cache=cache)\n            data = json.load(open(fname, 'r'))\n            if 'segments' in data:\n                flag_segments = data['segments']\n\n        except Exception as e:\n            msg = \"Unable to find segments in GWOSC, check flag name or times\"\n            print(e)\n            if source != 'any':\n                raise ValueError(msg)\n            else:\n                print(\"Tried and failed GWOSC {}, trying dqsegdb\", name)\n\n\n            return query_flag(ifo, segment_name, start_time, end_time,\n                              source='dqsegdb', server=server,\n                              veto_definer=veto_definer)\n\n    elif source == 'dqsegdb':\n        # Let's not hard require dqsegdb to be installed if we never get here.\n        try:\n            from dqsegdb.apicalls import dqsegdbQueryTimes as query\n        except ImportError:\n            raise ValueError(\"Could not query flag. Install dqsegdb\"\n                             \":'pip install dqsegdb'\")\n\n        # The veto definer will allow the use of MACRO names\n        # These directly correspond the name defined in the veto definer file.\n        if veto_definer is not None:\n            veto_def = parse_veto_definer(veto_definer)\n\n        # We treat the veto definer name as if it were its own flag and\n        # a process the flags in the veto definer\n        if veto_definer is not None and segment_name in veto_def[ifo]:\n            for flag in veto_def[ifo][segment_name]:\n                segs = query(\"https\", server, ifo, flag['name'],\n                             flag['version'], 'active',\n                             int(start_time), int(end_time))[0]['active']\n\n                # Apply padding to each segment\n                for rseg in segs:\n                    seg_start = rseg[0] + flag['start_pad']\n                    seg_end = rseg[1] + flag['end_pad']\n                    flag_segments.append(segment(seg_start, seg_end))\n\n            # Apply start / end of the veto definer segment\n            send = segmentlist([segment([veto_def['start'], veto_def['end']])])\n            flag_segments = (flag_segments.coalesce() & send)\n\n        else:  # Standard case just query directly.\n            try:\n                segs = query(\"https\", server, ifo, name, version,\n                             'active', int(start_time),\n                             int(end_time))[0]['active']\n                for rseg in segs:\n                    flag_segments.append(segment(rseg[0], rseg[1]))\n            except Exception as e:\n                print(\"Could not query flag, check name \"\n                      \" (%s) or times\" % segment_name)\n                raise e\n\n    else:\n        raise ValueError(\"Source must be dqsegdb or GWOSC.\"\n                         \" Got {}\".format(source))\n\n    return segmentlist(flag_segments).coalesce()", "response": "Query the status flag from the specified interferometer."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef query_cumulative_flags(ifo, segment_names, start_time, end_time,\n                           source='any', server=\"segments.ligo.org\",\n                           veto_definer=None,\n                           bounds=None,\n                           padding=None,\n                           override_ifos=None,\n                           cache=False):\n    \"\"\"Return the times where any flag is active\n\n    Parameters\n    ----------\n    ifo: string or dict\n        The interferometer to query (H1, L1). If a dict, an element for each\n        flag name must be provided.\n    segment_name: list of strings\n        The status flag to query from LOSC.\n    start_time: int\n        The starting gps time to begin querying from LOSC\n    end_time: int\n        The end gps time of the query\n    source: str, Optional\n        Choice between \"GWOSC\" or \"dqsegdb\". If dqsegdb, the server option may\n        also be given. The default is to try GWOSC first then try dqsegdb.\n    server: str, Optional\n        The server path. Only used with dqsegdb atm.\n    veto_definer: str, Optional\n        The path to a veto definer to define groups of flags which\n        themselves define a set of segments.\n    bounds: dict, Optional\n        Dict containing start end tuples keyed by the flag name which\n    indicated places which should have a distinct time period to be active.\n    padding: dict, Optional\n        Dict keyed by the flag name. Each element is a tuple\n    (start_pad, end_pad) which indicates how to change the segment boundaries.\n    override_ifos: dict, Optional\n        A dict keyed by flag_name to override the ifo option on a per flag\n    basis.\n\n    Returns\n    ---------\n    segments: glue.segments.segmentlist\n        List of segments\n    \"\"\"\n    total_segs = segmentlist([])\n    for flag_name in segment_names:\n        ifo_name = ifo\n        if override_ifos is not None and flag_name in override_ifos:\n            ifo_name = override_ifos[flag_name]\n\n        segs = query_flag(ifo_name, flag_name, start_time, end_time,\n                          source=source, server=server,\n                          veto_definer=veto_definer,\n                          cache=cache)\n\n        if padding and flag_name in padding:\n            s, e = padding[flag_name]\n            segs2 = segmentlist([])\n            for seg in segs:\n                segs2.append(segment(seg[0] + s, seg[1] + e))\n            segs = segs2\n\n        if bounds is not None and flag_name in bounds:\n            s, e = bounds[flag_name]\n            valid = segmentlist([segment([s, e])])\n            segs = (segs & valid).coalesce()\n\n\n        total_segs = (total_segs + segs).coalesce()\n    return total_segs", "response": "Query the cumulative set of flags for a given set of segments."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_flag_str(flag_str):\n    flags = flag_str.replace(' ', '').strip().split(',')\n\n    signs = {}\n    ifos = {}\n    bounds = {}\n    padding = {}\n    bflags = []\n\n    for flag in flags:\n        # Check if the flag should add or subtract time\n        sign = flag[0] == '+'\n        flag = flag[1:]\n\n        ifo = pad = bound = None\n\n        # Check for non-default IFO\n        if len(flag.split(':')[0]) == 2:\n            ifo = flag.split(':')[0]\n            flag = flag[3:]\n\n        # Check for padding options\n        if '<' in flag:\n            popt = flag.split('<')[1].split('>')[0]\n            spad, epad = popt.split(':')\n            pad = (float(spad), float(epad))\n            flag = flag.replace(popt, '').replace('<>', '')\n\n        # Check if there are bounds on the flag\n        if '[' in flag:\n            bopt = flag.split('[')[1].split(']')[0]\n            start, end = bopt.split(':')\n            bound = (int(start), int(end))\n            flag = flag.replace(bopt, '').replace('[]', '')\n\n        if ifo:\n            ifos[flag] = ifo\n        if pad:\n            padding[flag] = pad\n        if bound:\n            bounds[flag] = bound\n        bflags.append(flag)\n        signs[flag] = sign\n\n    return bflags, signs, ifos, bounds, padding", "response": "Parse a dq flag query string into a list of dicts."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nquery for flags based on a special str syntax", "response": "def query_str(ifo, flag_str, start_time, end_time, server=\"segments.ligo.org\",\n              veto_definer=None):\n    \"\"\" Query for flags based on a special str syntax\n\n    Parameters\n    ----------\n    ifo: str\n        The ifo to be mainly quering for. (may be overriden in syntax)\n    flag_str: str\n        Specification of how to do the query. Ex. +H1:DATA:1<-8,8>[0,100000000]\n        would return H1 time for the DATA available flag with version 1. It\n        would then apply an 8 second padding and only return times within\n        the chosen range 0,1000000000.\n    start_time: int\n        The start gps time. May be overriden for individual flags with the\n        flag str bounds syntax\n    end_time: int\n        The end gps time. May be overriden for individual flags with the\n        flag str bounds syntax\n\n    Returns\n    -------\n    segs: segmentlist\n        A list of segments corresponding to the flag query string\n    \"\"\"\n    flags, sign, ifos, bounds, padding = parse_flag_str(flag_str)\n    up = [f for f in flags if sign[f]]\n    down = [f for f in flags if not sign[f]]\n\n    if len(up) + len(down) != len(flags):\n        raise ValueError('Not all flags could be parsed, check +/- prefix')\n    segs = query_cumulative_flags(ifo, up, start_time, end_time,\n                                  server=server,\n                                  veto_definer=veto_definer,\n                                  bounds=bounds,\n                                  padding=padding,\n                                  override_ifos=ifos)\n\n    mseg = query_cumulative_flags(ifo, down, start_time, end_time,\n                                  server=server,\n                                  veto_definer=veto_definer,\n                                  bounds=bounds,\n                                  padding=padding,\n                                  override_ifos=ifos)\n\n    segs = (segs - mseg).coalesce()\n    return segs"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef data(self, data):\n        self._data = {det: d.copy() for (det, d) in data.items()}", "response": "Store a copy of the data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the log of the prior - weighted likelihood ratio at the current parameter values.", "response": "def logplr(self):\n        \"\"\"Returns the log of the prior-weighted likelihood ratio at the\n        current parameter values.\n\n        The logprior is calculated first. If the logprior returns ``-inf``\n        (possibly indicating a non-physical point), then ``loglr`` is not\n        called.\n        \"\"\"\n        logp = self.logprior\n        if logp == -numpy.inf:\n            return logp\n        else:\n            return logp + self.loglr"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding data to the metadata that s written to the inference file fp.", "response": "def write_metadata(self, fp):\n        \"\"\"Adds data to the metadata that's written.\n\n        Parameters\n        ----------\n        fp : pycbc.inference.io.BaseInferenceFile instance\n            The inference file to write to.\n        \"\"\"\n        super(BaseDataModel, self).write_metadata(fp)\n        fp.write_stilde(self.data)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_config(cls, cp, ifo, section):\n        all_params = dict(cp.items(section))\n        params = {key[len(ifo)+1:]: all_params[key]\n                  for key in all_params if ifo.lower() in key}\n        model = params.pop('model')\n        params['ifo_name'] = ifo.lower()\n\n        return all_models[model](**params)", "response": "Read a config file to get calibration options and transfer\n            functions which will be used to intialize the model."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef apply_fseries_time_shift(htilde, dt, kmin=0, copy=True):\n    if htilde.precision != 'single':\n        raise NotImplementedError(\"CUDA version of apply_fseries_time_shift only supports single precision\")\n\n    if copy:\n        out = htilde.copy()\n    else:\n        out = htilde\n\n    kmin = numpy.int32(kmin)\n    kmax = numpy.int32(len(htilde))\n    nb = int(numpy.ceil(kmax / nt_float))\n    if nb > 1024:\n        raise ValueError(\"More than 1024 blocks not supported yet\")\n\n    phi = numpy.float32(-2 * numpy.pi * dt * htilde.delta_f)\n    fseries_ts_fn.prepared_call((nb, 1), (nt, 1, 1), out.data.gpudata, phi, kmin, kmax)\n    if copy:\n        htilde = FrequencySeries(out, delta_f=htilde.delta_f, epoch=htilde.epoch,\n                                 copy=False)\n    return htilde", "response": "Shifts a frequency domain waveform in time."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rvs(self, size=1, param=None):\n        arr = super(UniformComponentMasses, self).rvs(size=size)\n        # enforce m1 > m2\n        m1 = conversions.primary_mass(arr['mass1'], arr['mass2'])\n        m2 = conversions.secondary_mass(arr['mass1'], arr['mass2'])\n        arr['mass1'][:] = m1\n        arr['mass2'][:] = m2\n        if param is not None:\n            arr = arr[param]\n        return arr", "response": "Gives a set of random values drawn from this distribution."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninitialize this class from a config file.", "response": "def from_config(cls, cp, section, variable_args):\n        \"\"\"Initialize this class from a config file.\n\n        Bounds on ``f0``, ``tau``, ``final_mass`` and ``final_spin`` should\n        be specified by providing ``min-{param}`` and ``max-{param}``. If\n        the ``f0`` or ``tau`` param should be renamed, ``rdfreq`` and\n        ``damping_time`` should be provided; these must match\n        ``variable_args``. If ``rdfreq`` and ``damping_time`` are not\n        provided, ``variable_args`` are expected to be ``f0`` and ``tau``.\n\n        Only ``min/max-f0`` and ``min/max-tau`` need to be provided.\n\n        Example:\n\n        .. code-block:: ini\n\n            [{section}-f0+tau]\n            name = uniform_f0_tau\n            min-f0 = 10\n            max-f0 = 2048\n            min-tau = 0.0001\n            max-tau = 0.010\n            min-final_mass = 10\n\n        Parameters\n        ----------\n        cp : pycbc.workflow.WorkflowConfigParser\n            WorkflowConfigParser instance to read.\n        section : str\n            The name of the section to read.\n        variable_args : str\n            The name of the variable args. These should be separated by\n            ``pycbc.VARARGS_DELIM``.\n\n        Returns\n        -------\n        UniformF0Tau :\n            This class initialized with the parameters provided in the config\n            file.\n        \"\"\"\n        tag = variable_args\n        variable_args = set(variable_args.split(pycbc.VARARGS_DELIM))\n        # get f0 and tau\n        f0 = bounded.get_param_bounds_from_config(cp, section, tag, 'f0')\n        tau = bounded.get_param_bounds_from_config(cp, section, tag, 'tau')\n        # see if f0 and tau should be renamed\n        if cp.has_option_tag(section, 'rdfreq', tag):\n            rdfreq = cp.get_opt_tag(section, 'rdfreq', tag)\n        else:\n            rdfreq = 'f0'\n        if cp.has_option_tag(section, 'damping_time', tag):\n            damping_time = cp.get_opt_tag(section, 'damping_time', tag)\n        else:\n            damping_time = 'tau'\n        # check that they match whats in the variable args\n        if not variable_args == set([rdfreq, damping_time]):\n            raise ValueError(\"variable args do not match rdfreq and \"\n                             \"damping_time names\")\n        # get the final mass and spin values, if provided\n        final_mass = bounded.get_param_bounds_from_config(\n            cp, section, tag, 'final_mass')\n        final_spin = bounded.get_param_bounds_from_config(\n            cp, section, tag, 'final_spin')\n        extra_opts = {}\n        if cp.has_option_tag(section, 'norm_tolerance', tag):\n            extra_opts['norm_tolerance'] = float(\n                cp.get_opt_tag(section, 'norm_tolerance', tag))\n        if cp.has_option_tag(section, 'norm_seed', tag):\n            extra_opts['norm_seed'] = int(\n                cp.get_opt_tag(section, 'norm_seed', tag))\n        return cls(f0=f0, tau=tau,\n                   final_mass=final_mass, final_spin=final_spin,\n                   rdfreq=rdfreq, damping_time=damping_time,\n                   **extra_opts)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\napplying additional boundary conditions to the current object.", "response": "def apply_boundary_conditions(self, **kwargs):\n        \"\"\"Maps values to be in [0, 2pi) (the domain) first, before applying\n        any additional boundary conditions.\n\n        Parameters\n        ----------\n        \\**kwargs :\n            The keyword args should be the name of a parameter and value to\n            apply its boundary conditions to. The arguments need not include\n            all of the parameters in self.\n\n        Returns\n        -------\n        dict\n            A dictionary of the parameter names and the conditioned values.\n        \"\"\"\n        # map values to be within the domain\n        kwargs = dict([[p, self._domain.apply_conditions(val)]\n                      for p,val in kwargs.items()])\n        # now apply additional conditions\n        return super(UniformAngle, self).apply_boundary_conditions(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the pdf at the given values.", "response": "def _pdf(self, **kwargs):\n        \"\"\"Returns the pdf at the given values. The keyword arguments must\n        contain all of parameters in self's params. Unrecognized arguments are\n        ignored.\n        \"\"\"\n        if kwargs not in self:\n            return 0.\n        return self._norm * \\\n            self._dfunc(numpy.array([kwargs[p] for p in self._params])).prod()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the log of the pdf at the given values.", "response": "def _logpdf(self, **kwargs):\n        \"\"\"Returns the log of the pdf at the given values. The keyword\n        arguments must contain all of parameters in self's params. Unrecognized\n        arguments are ignored.\n        \"\"\"\n        if kwargs not in self:\n            return -numpy.inf\n        return self._lognorm + \\\n            numpy.log(self._dfunc(\n                numpy.array([kwargs[p] for p in self._params]))).sum()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmapping the given values to be within the domain of the azimuthal and azimuthal angles before applying any other boundary conditions.", "response": "def apply_boundary_conditions(self, **kwargs):\n        \"\"\"Maps the given values to be within the domain of the azimuthal and\n        polar angles, before applying any other boundary conditions.\n\n        Parameters\n        ----------\n        \\**kwargs :\n            The keyword args must include values for both the azimuthal and\n            polar angle, using the names they were initilialized with. For\n            example, if `polar_angle='theta'` and `azimuthal_angle=`phi`, then\n            the keyword args must be `theta={val1}, phi={val2}`.\n\n        Returns\n        -------\n        dict\n            A dictionary of the parameter names and the conditioned values.\n        \"\"\"\n        polarval = kwargs[self._polar_angle]\n        azval = kwargs[self._azimuthal_angle]\n        # constrain each angle to its domain\n        polarval = self._polardist._domain.apply_conditions(polarval)\n        azval = self._azimuthaldist._domain.apply_conditions(azval)\n        # apply any other boundary conditions\n        polarval = self._bounds[self._polar_angle].apply_conditions(polarval)\n        azval = self._bounds[self._azimuthal_angle].apply_conditions(azval)\n        return {self._polar_angle: polarval, self._azimuthal_angle: azval}"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _pdf(self, **kwargs):\n        return self._polardist._pdf(**kwargs) * \\\n            self._azimuthaldist._pdf(**kwargs)", "response": "Returns the pdf at the given angles."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _logpdf(self, **kwargs):\n        return self._polardist._logpdf(**kwargs) +\\\n            self._azimuthaldist._logpdf(**kwargs)", "response": "Returns the logpdf at the given angles."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive a set of random values drawn from this distribution.", "response": "def rvs(self, size=1, param=None):\n        \"\"\"Gives a set of random values drawn from this distribution.\n\n        Parameters\n        ----------\n        size : {1, int}\n            The number of values to generate; default is 1.\n        param : {None, string}\n            If provided, will just return values for the given parameter.\n            Otherwise, returns random values for each parameter.\n\n        Returns\n        -------\n        structured array\n            The random values in a numpy structured array. If a param was\n            specified, the array will only have an element corresponding to the\n            given parameter. Otherwise, the array will have an element for each\n            parameter in self's params.\n        \"\"\"\n        if param is not None:\n            dtype = [(param, float)]\n        else:\n            dtype = [(p, float) for p in self.params]\n        arr = numpy.zeros(size, dtype=dtype)\n        for (p,_) in dtype:\n            if p == self._polar_angle:\n                arr[p] = self._polardist.rvs(size=size)\n            elif p == self._azimuthal_angle:\n                arr[p] = self._azimuthaldist.rvs(size=size)\n            else:\n                raise ValueError(\"unrecognized parameter %s\" %(p))\n        return arr"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a UniformSolidAngleUnivariateDistribution instance from a config file.", "response": "def from_config(cls, cp, section, variable_args):\n        \"\"\"Returns a distribution based on a configuration file.\n\n        The section must have the names of the polar and azimuthal angles in\n        the tag part of the section header. For example:\n\n        .. code-block:: ini\n\n            [prior-theta+phi]\n            name = uniform_solidangle\n\n        If nothing else is provided, the default names and bounds of the polar\n        and azimuthal angles will be used. To specify a different name for\n        each angle, set the `polar-angle` and `azimuthal-angle` attributes. For\n        example:\n\n        .. code-block:: ini\n\n            [prior-foo+bar]\n            name = uniform_solidangle\n            polar-angle = foo\n            azimuthal-angle = bar\n\n        Note that the names of the variable args in the tag part of the section\n        name must match the names of the polar and azimuthal angles.\n\n        Bounds may also be specified for each angle, as factors of pi. For\n        example:\n\n        .. code-block:: ini\n\n            [prior-theta+phi]\n            polar-angle = theta\n            azimuthal-angle = phi\n            min-theta = 0\n            max-theta = 0.5\n\n        This will return a distribution that is uniform in the upper\n        hemisphere.\n\n        By default, the domain of the azimuthal angle is `[0, 2pi)`. To make\n        this domain cyclic, add `azimuthal_cyclic_domain =`.\n\n        Parameters\n        ----------\n        cp : ConfigParser instance\n            The config file.\n        section : str\n            The name of the section.\n        variable_args : str\n            The names of the parameters for this distribution, separated by\n            ``VARARGS_DELIM``. These must appear in the \"tag\" part\n            of the section header.\n\n        Returns\n        -------\n        UniformSolidAngle\n            A distribution instance from the pycbc.inference.prior module.\n        \"\"\"\n        tag = variable_args\n        variable_args = variable_args.split(VARARGS_DELIM)\n\n        # get the variables that correspond to the polar/azimuthal angles\n        try:\n            polar_angle = cp.get_opt_tag(section, 'polar-angle', tag)\n        except Error:\n            polar_angle = cls._default_polar_angle\n        try:\n            azimuthal_angle = cp.get_opt_tag(section, 'azimuthal-angle', tag)\n        except Error:\n            azimuthal_angle = cls._default_azimuthal_angle\n\n        if polar_angle not in variable_args:\n            raise Error(\"polar-angle %s is not one of the variable args (%s)\"%(\n                polar_angle, ', '.join(variable_args)))\n        if azimuthal_angle not in variable_args:\n            raise Error(\"azimuthal-angle %s is not one of the variable args \"%(\n                azimuthal_angle) + \"(%s)\"%(', '.join(variable_args)))\n\n        # get the bounds, if provided\n        polar_bounds = bounded.get_param_bounds_from_config(\n                                                   cp, section, tag,\n                                                   polar_angle)\n        azimuthal_bounds = bounded.get_param_bounds_from_config(\n                                                   cp, section, tag,\n                                                   azimuthal_angle)\n\n        # see if the a cyclic domain is desired for the azimuthal angle\n        azimuthal_cyclic_domain = cp.has_option_tag(section,\n            'azimuthal_cyclic_domain', tag)\n\n        return cls(polar_angle=polar_angle, azimuthal_angle=azimuthal_angle,\n                   polar_bounds=polar_bounds,\n                   azimuthal_bounds=azimuthal_bounds,\n                   azimuthal_cyclic_domain=azimuthal_cyclic_domain)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_axes_grid(parameters, labels=None, height_ratios=None,\n                     width_ratios=None, no_diagonals=False):\n    \"\"\"Given a list of parameters, creates a figure with an axis for\n    every possible combination of the parameters.\n\n    Parameters\n    ----------\n    parameters : list\n        Names of the variables to be plotted.\n    labels : {None, dict}, optional\n        A dictionary of parameters -> parameter labels.\n    height_ratios : {None, list}, optional\n        Set the height ratios of the axes; see `matplotlib.gridspec.GridSpec`\n        for details.\n    width_ratios : {None, list}, optional\n        Set the width ratios of the axes; see `matplotlib.gridspec.GridSpec`\n        for details.\n    no_diagonals : {False, bool}, optional\n        Do not produce axes for the same parameter on both axes.\n\n    Returns\n    -------\n    fig : pyplot.figure\n        The figure that was created.\n    axis_dict : dict\n        A dictionary mapping the parameter combinations to the axis and their\n        location in the subplots grid; i.e., the key, values are:\n        `{('param1', 'param2'): (pyplot.axes, row index, column index)}`\n    \"\"\"\n    if labels is None:\n        labels = {p: p for p in parameters}\n    elif any(p not in labels for p in parameters):\n        raise ValueError(\"labels must be provided for all parameters\")\n    # Create figure with adequate size for number of parameters.\n    ndim = len(parameters)\n    if no_diagonals:\n        ndim -= 1\n    if ndim < 3:\n        fsize = (8, 7)\n    else:\n        fsize = (ndim*3 - 1, ndim*3 - 2)\n    fig = pyplot.figure(figsize=fsize)\n    # create the axis grid\n    gs = gridspec.GridSpec(ndim, ndim, width_ratios=width_ratios,\n                           height_ratios=height_ratios,\n                           wspace=0.05, hspace=0.05)\n    # create grid of axis numbers to easily create axes in the right locations\n    axes = numpy.arange(ndim**2).reshape((ndim, ndim))\n\n    # Select possible combinations of plots and establish rows and columns.\n    combos = list(itertools.combinations(parameters, 2))\n    # add the diagonals\n    if not no_diagonals:\n        combos += [(p, p) for p in parameters]\n\n    # create the mapping between parameter combos and axes\n    axis_dict = {}\n    # cycle over all the axes, setting thing as needed\n    for nrow in range(ndim):\n        for ncolumn in range(ndim):\n            ax = pyplot.subplot(gs[axes[nrow, ncolumn]])\n            # map to a parameter index\n            px = parameters[ncolumn]\n            if no_diagonals:\n                py = parameters[nrow+1]\n            else:\n                py = parameters[nrow]\n            if (px, py) in combos:\n                axis_dict[px, py] = (ax, nrow, ncolumn)\n                # x labels only on bottom\n                if nrow + 1 == ndim:\n                    ax.set_xlabel('{}'.format(labels[px]), fontsize=18)\n                else:\n                    pyplot.setp(ax.get_xticklabels(), visible=False)\n                # y labels only on left\n                if ncolumn == 0:\n                    ax.set_ylabel('{}'.format(labels[py]), fontsize=18)\n                else:\n                    pyplot.setp(ax.get_yticklabels(), visible=False)\n            else:\n                # make non-used axes invisible\n                ax.axis('off')\n    return fig, axis_dict", "response": "Creates a figure with an axis for each possible combination of the parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_scale_fac(fig, fiducial_width=8, fiducial_height=7):\n    width, height = fig.get_size_inches()\n    return (width*height/(fiducial_width*fiducial_height))**0.5", "response": "Gets a factor to scale fonts by for the given figure."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconstructs a KDE from the given samples.", "response": "def construct_kde(samples_array, use_kombine=False):\n    \"\"\"Constructs a KDE from the given samples.\n    \"\"\"\n    if use_kombine:\n        try:\n            import kombine\n        except ImportError:\n            raise ImportError(\"kombine is not installed.\")\n    # construct the kde\n    if use_kombine:\n        kde = kombine.clustered_kde.KDE(samples_array)\n    else:\n        kde = scipy.stats.gaussian_kde(samples_array.T)\n    return kde"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_density_plot(xparam, yparam, samples, plot_density=True,\n                        plot_contours=True, percentiles=None, cmap='viridis',\n                        contour_color=None, xmin=None, xmax=None,\n                        ymin=None, ymax=None, exclude_region=None,\n                        fig=None, ax=None, use_kombine=False):\n    \"\"\"Computes and plots posterior density and confidence intervals using the\n    given samples.\n\n    Parameters\n    ----------\n    xparam : string\n        The parameter to plot on the x-axis.\n    yparam : string\n        The parameter to plot on the y-axis.\n    samples : dict, numpy structured array, or FieldArray\n        The samples to plot.\n    plot_density : {True, bool}\n        Plot a color map of the density.\n    plot_contours : {True, bool}\n        Plot contours showing the n-th percentiles of the density.\n    percentiles : {None, float or array}\n        What percentile contours to draw. If None, will plot the 50th\n        and 90th percentiles.\n    cmap : {'viridis', string}\n        The name of the colormap to use for the density plot.\n    contour_color : {None, string}\n        What color to make the contours. Default is white for density\n        plots and black for other plots.\n    xmin : {None, float}\n        Minimum value to plot on x-axis.\n    xmax : {None, float}\n        Maximum value to plot on x-axis.\n    ymin : {None, float}\n        Minimum value to plot on y-axis.\n    ymax : {None, float}\n        Maximum value to plot on y-axis.\n    exclue_region : {None, str}\n        Exclude the specified region when plotting the density or contours.\n        Must be a string in terms of `xparam` and `yparam` that is\n        understandable by numpy's logical evaluation. For example, if\n        `xparam = m_1` and `yparam = m_2`, and you want to exclude the region\n        for which `m_2` is greater than `m_1`, then exclude region should be\n        `'m_2 > m_1'`.\n    fig : {None, pyplot.figure}\n        Add the plot to the given figure. If None and ax is None, will create\n        a new figure.\n    ax : {None, pyplot.axes}\n        Draw plot on the given axis. If None, will create a new axis from\n        `fig`.\n    use_kombine : {False, bool}\n        Use kombine's KDE to calculate density. Otherwise, will use\n        `scipy.stats.gaussian_kde.` Default is False.\n\n    Returns\n    -------\n    fig : pyplot.figure\n        The figure the plot was made on.\n    ax : pyplot.axes\n        The axes the plot was drawn on.\n    \"\"\"\n    if percentiles is None:\n        percentiles = numpy.array([50., 90.])\n    percentiles = 100. - numpy.array(percentiles)\n    percentiles.sort()\n\n    if ax is None and fig is None:\n        fig = pyplot.figure()\n    if ax is None:\n        ax = fig.add_subplot(111)\n\n    # convert samples to array and construct kde\n    xsamples = samples[xparam]\n    ysamples = samples[yparam]\n    arr = numpy.vstack((xsamples, ysamples)).T\n    kde = construct_kde(arr, use_kombine=use_kombine)\n\n    # construct grid to evaluate on\n    if xmin is None:\n        xmin = xsamples.min()\n    if xmax is None:\n        xmax = xsamples.max()\n    if ymin is None:\n        ymin = ysamples.min()\n    if ymax is None:\n        ymax = ysamples.max()\n    npts = 100\n    X, Y = numpy.mgrid[\n        xmin:xmax:complex(0, npts),  # pylint:disable=invalid-slice-index\n        ymin:ymax:complex(0, npts)]  # pylint:disable=invalid-slice-index\n    pos = numpy.vstack([X.ravel(), Y.ravel()])\n    if use_kombine:\n        Z = numpy.exp(kde(pos.T).reshape(X.shape))\n        draw = kde.draw\n    else:\n        Z = kde(pos).T.reshape(X.shape)\n        draw = kde.resample\n\n    if exclude_region is not None:\n        # convert X,Y to a single FieldArray so we can use it's ability to\n        # evaluate strings\n        farr = FieldArray.from_kwargs(**{xparam: X, yparam: Y})\n        Z[farr[exclude_region]] = 0.\n\n    if plot_density:\n        ax.imshow(numpy.rot90(Z), extent=[xmin, xmax, ymin, ymax],\n                  aspect='auto', cmap=cmap, zorder=1)\n        if contour_color is None:\n            contour_color = 'w'\n\n    if plot_contours:\n        # compute the percentile values\n        resamps = kde(draw(int(npts**2)))\n        if use_kombine:\n            resamps = numpy.exp(resamps)\n        s = numpy.percentile(resamps, percentiles)\n        if contour_color is None:\n            contour_color = 'k'\n        # make linewidths thicker if not plotting density for clarity\n        if plot_density:\n            lw = 1\n        else:\n            lw = 2\n        ct = ax.contour(X, Y, Z, s, colors=contour_color, linewidths=lw,\n                        zorder=3)\n        # label contours\n        lbls = ['{p}%'.format(p=int(p)) for p in (100. - percentiles)]\n        fmt = dict(zip(ct.levels, lbls))\n        fs = 12\n        ax.clabel(ct, ct.levels, inline=True, fmt=fmt, fontsize=fs)\n\n    return fig, ax", "response": "Creates a posterior density plot for the given samples."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_marginalized_hist(ax, values, label, percentiles=None,\n                             color='k', fillcolor='gray', linecolor='navy',\n                             linestyle='-',\n                             title=True, expected_value=None,\n                             expected_color='red', rotated=False,\n                             plot_min=None, plot_max=None):\n    \"\"\"Plots a 1D marginalized histogram of the given param from the given\n    samples.\n\n    Parameters\n    ----------\n    ax : pyplot.Axes\n        The axes on which to draw the plot.\n    values : array\n        The parameter values to plot.\n    label : str\n        A label to use for the title.\n    percentiles : {None, float or array}\n        What percentiles to draw lines at. If None, will draw lines at\n        `[5, 50, 95]` (i.e., the bounds on the upper 90th percentile and the\n        median).\n    color : {'k', string}\n        What color to make the histogram; default is black.\n    fillcolor : {'gray', string, or None}\n        What color to fill the histogram with. Set to None to not fill the\n        histogram. Default is 'gray'.\n    linestyle : str, optional\n        What line style to use for the histogram. Default is '-'.\n    linecolor : {'navy', string}\n        What color to use for the percentile lines. Default is 'navy'.\n    title : bool, optional\n        Add a title with a estimated value +/- uncertainty. The estimated value\n        is the pecentile halfway between the max/min of ``percentiles``, while\n        the uncertainty is given by the max/min of the ``percentiles``. If no\n        percentiles are specified, defaults to quoting the median +/- 95/5\n        percentiles.\n    rotated : {False, bool}\n        Plot the histogram on the y-axis instead of the x. Default is False.\n    plot_min : {None, float}\n        The minimum value to plot. If None, will default to whatever `pyplot`\n        creates.\n    plot_max : {None, float}\n        The maximum value to plot. If None, will default to whatever `pyplot`\n        creates.\n    scalefac : {1., float}\n        Factor to scale the default font sizes by. Default is 1 (no scaling).\n    \"\"\"\n    if fillcolor is None:\n        htype = 'step'\n    else:\n        htype = 'stepfilled'\n    if rotated:\n        orientation = 'horizontal'\n    else:\n        orientation = 'vertical'\n    ax.hist(values, bins=50, histtype=htype, orientation=orientation,\n            facecolor=fillcolor, edgecolor=color, ls=linestyle, lw=2,\n            density=True)\n    if percentiles is None:\n        percentiles = [5., 50., 95.]\n    if len(percentiles) > 0:\n        plotp = numpy.percentile(values, percentiles)\n    else:\n        plotp = []\n    for val in plotp:\n        if rotated:\n            ax.axhline(y=val, ls='dashed', color=linecolor, lw=2, zorder=3)\n        else:\n            ax.axvline(x=val, ls='dashed', color=linecolor, lw=2, zorder=3)\n    # plot expected\n    if expected_value is not None:\n        if rotated:\n            ax.axhline(expected_value, color=expected_color, lw=1.5, zorder=2)\n        else:\n            ax.axvline(expected_value, color=expected_color, lw=1.5, zorder=2)\n    if title:\n        if len(percentiles) > 0:\n            minp = min(percentiles)\n            maxp = max(percentiles)\n            medp = (maxp + minp) / 2.\n        else:\n            minp = 5\n            medp = 50\n            maxp = 95\n        values_min = numpy.percentile(values, minp)\n        values_med = numpy.percentile(values, medp)\n        values_max = numpy.percentile(values, maxp)\n        negerror = values_med - values_min\n        poserror = values_max - values_med\n        fmt = '${0}$'.format(str_utils.format_value(\n            values_med, negerror, plus_error=poserror))\n        if rotated:\n            ax.yaxis.set_label_position(\"right\")\n\n            # sets colored title for marginal histogram\n            set_marginal_histogram_title(ax, fmt, color,\n                                         label=label, rotated=rotated)\n\n            # Remove x-ticks\n            ax.set_xticks([])\n            # turn off x-labels\n            ax.set_xlabel('')\n            # set limits\n            ymin, ymax = ax.get_ylim()\n            if plot_min is not None:\n                ymin = plot_min\n            if plot_max is not None:\n                ymax = plot_max\n            ax.set_ylim(ymin, ymax)\n\n        else:\n\n            # sets colored title for marginal histogram\n            set_marginal_histogram_title(ax, fmt, color, label=label)\n\n            # Remove y-ticks\n            ax.set_yticks([])\n            # turn off y-label\n            ax.set_ylabel('')\n            # set limits\n            xmin, xmax = ax.get_xlim()\n            if plot_min is not None:\n                xmin = plot_min\n            if plot_max is not None:\n                xmax = plot_max\n            ax.set_xlim(xmin, xmax)", "response": "Plots a 1D marginalized histogram of the given parameter values."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the title of the marginal histograms.", "response": "def set_marginal_histogram_title(ax, fmt, color, label=None, rotated=False):\n    \"\"\" Sets the title of the marginal histograms.\n\n    Parameters\n    ----------\n    ax : Axes\n        The `Axes` instance for the plot.\n    fmt : str\n        The string to add to the title.\n    color : str\n        The color of the text to add to the title.\n    label : str\n        If title does not exist, then include label at beginning of the string.\n    rotated : bool\n        If `True` then rotate the text 270 degrees for sideways title.\n    \"\"\"\n\n    # get rotation angle of the title\n    rotation = 270 if rotated else 0\n\n    # get how much to displace title on axes\n    xscale = 1.05 if rotated else 0.0\n    if rotated:\n        yscale = 1.0\n    elif len(ax.get_figure().axes) > 1:\n        yscale = 1.15\n    else:\n        yscale = 1.05\n\n    # get class that packs text boxes vertical or horizonitally\n    packer_class = offsetbox.VPacker if rotated else offsetbox.HPacker\n\n    # if no title exists\n    if not hasattr(ax, \"title_boxes\"):\n\n        # create a text box\n        title = \"{} = {}\".format(label, fmt)\n        tbox1 = offsetbox.TextArea(\n                   title,\n                   textprops=dict(color=color, size=15, rotation=rotation,\n                                  ha='left', va='bottom'))\n\n        # save a list of text boxes as attribute for later\n        ax.title_boxes = [tbox1]\n\n        # pack text boxes\n        ybox = packer_class(children=ax.title_boxes,\n                            align=\"bottom\", pad=0, sep=5)\n\n    # else append existing title\n    else:\n\n        # delete old title\n        ax.title_anchor.remove()\n\n        # add new text box to list\n        tbox1 = offsetbox.TextArea(\n                   \" {}\".format(fmt),\n                   textprops=dict(color=color, size=15, rotation=rotation,\n                                  ha='left', va='bottom'))\n        ax.title_boxes = ax.title_boxes + [tbox1]\n\n        # pack text boxes\n        ybox = packer_class(children=ax.title_boxes,\n                            align=\"bottom\", pad=0, sep=5)\n\n    # add new title and keep reference to instance as an attribute\n    anchored_ybox = offsetbox.AnchoredOffsetbox(\n                      loc=2, child=ybox, pad=0.,\n                      frameon=False, bbox_to_anchor=(xscale, yscale),\n                      bbox_transform=ax.transAxes, borderpad=0.)\n    ax.title_anchor = ax.add_artist(anchored_ybox)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a multidim plot for the given parameters.", "response": "def create_multidim_plot(parameters, samples, labels=None,\n                         mins=None, maxs=None, expected_parameters=None,\n                         expected_parameters_color='r',\n                         plot_marginal=True, plot_scatter=True,\n                         marginal_percentiles=None, contour_percentiles=None,\n                         marginal_title=True, marginal_linestyle='-',\n                         zvals=None, show_colorbar=True, cbar_label=None,\n                         vmin=None, vmax=None, scatter_cmap='plasma',\n                         plot_density=False, plot_contours=True,\n                         density_cmap='viridis',\n                         contour_color=None, hist_color='black',\n                         line_color=None, fill_color='gray',\n                         use_kombine=False, fig=None, axis_dict=None):\n    \"\"\"Generate a figure with several plots and histograms.\n\n    Parameters\n    ----------\n    parameters: list\n        Names of the variables to be plotted.\n    samples : FieldArray\n        A field array of the samples to plot.\n    labels: dict, optional\n        A dictionary mapping parameters to labels. If none provided, will just\n        use the parameter strings as the labels.\n    mins : {None, dict}, optional\n        Minimum value for the axis of each variable in `parameters`.\n        If None, it will use the minimum of the corresponding variable in\n        `samples`.\n    maxs : {None, dict}, optional\n        Maximum value for the axis of each variable in `parameters`.\n        If None, it will use the maximum of the corresponding variable in\n        `samples`.\n    expected_parameters : {None, dict}, optional\n        Expected values of `parameters`, as a dictionary mapping parameter\n        names -> values. A cross will be plotted at the location of the\n        expected parameters on axes that plot any of the expected parameters.\n    expected_parameters_color : {'r', string}, optional\n        What color to make the expected parameters cross.\n    plot_marginal : {True, bool}\n        Plot the marginalized distribution on the diagonals. If False, the\n        diagonal axes will be turned off.\n    plot_scatter : {True, bool}\n        Plot each sample point as a scatter plot.\n    marginal_percentiles : {None, array}\n        What percentiles to draw lines at on the 1D histograms.\n        If None, will draw lines at `[5, 50, 95]` (i.e., the bounds on the\n        upper 90th percentile and the median).\n    marginal_title : bool, optional\n        Add a title over the 1D marginal plots that gives an estimated value\n        +/- uncertainty. The estimated value is the pecentile halfway between\n        the max/min of ``maginal_percentiles``, while the uncertainty is given\n        by the max/min of the ``marginal_percentiles. If no\n        ``marginal_percentiles`` are specified, the median +/- 95/5 percentiles\n        will be quoted.\n    marginal_linestyle : str, optional\n        What line style to use for the marginal histograms.\n    contour_percentiles : {None, array}\n        What percentile contours to draw on the scatter plots. If None,\n        will plot the 50th and 90th percentiles.\n    zvals : {None, array}\n        An array to use for coloring the scatter plots. If None, scatter points\n        will be the same color.\n    show_colorbar : {True, bool}\n        Show the colorbar of zvalues used for the scatter points. A ValueError\n        will be raised if zvals is None and this is True.\n    cbar_label : {None, str}\n        Specify a label to add to the colorbar.\n    vmin: {None, float}, optional\n        Minimum value for the colorbar. If None, will use the minimum of zvals.\n    vmax: {None, float}, optional\n        Maximum value for the colorbar. If None, will use the maxmimum of\n        zvals.\n    scatter_cmap : {'plasma', string}\n        The color map to use for the scatter points. Default is 'plasma'.\n    plot_density : {False, bool}\n        Plot the density of points as a color map.\n    plot_contours : {True, bool}\n        Draw contours showing the 50th and 90th percentile confidence regions.\n    density_cmap : {'viridis', string}\n        The color map to use for the density plot.\n    contour_color : {None, string}\n        The color to use for the contour lines. Defaults to white for\n        density plots, navy for scatter plots without zvals, and black\n        otherwise.\n    use_kombine : {False, bool}\n        Use kombine's KDE to calculate density. Otherwise, will use\n        `scipy.stats.gaussian_kde.` Default is False.\n\n    Returns\n    -------\n    fig : pyplot.figure\n        The figure that was created.\n    axis_dict : dict\n        A dictionary mapping the parameter combinations to the axis and their\n        location in the subplots grid; i.e., the key, values are:\n        `{('param1', 'param2'): (pyplot.axes, row index, column index)}`\n    \"\"\"\n    if labels is None:\n        labels = {p: p for p in parameters}\n    # set up the figure with a grid of axes\n    # if only plotting 2 parameters, make the marginal plots smaller\n    nparams = len(parameters)\n    if nparams == 2:\n        width_ratios = [3, 1]\n        height_ratios = [1, 3]\n    else:\n        width_ratios = height_ratios = None\n\n    # only plot scatter if more than one parameter\n    plot_scatter = plot_scatter and nparams > 1\n\n    # Sort zvals to get higher values on top in scatter plots\n    if plot_scatter:\n        if zvals is not None:\n            sort_indices = zvals.argsort()\n            zvals = zvals[sort_indices]\n            samples = samples[sort_indices]\n            if contour_color is None:\n                contour_color = 'k'\n        elif show_colorbar:\n            raise ValueError(\"must provide z values to create a colorbar\")\n        else:\n            # just make all scatter points same color\n            zvals = 'gray'\n            if plot_contours and contour_color is None:\n                contour_color = 'navy'\n\n    # convert samples to a dictionary to avoid re-computing derived parameters\n    # every time they are needed\n    samples = dict([[p, samples[p]] for p in parameters])\n\n    # values for axis bounds\n    if mins is None:\n        mins = {p: samples[p].min() for p in parameters}\n    else:\n        # copy the dict\n        mins = {p: val for p, val in mins.items()}\n    if maxs is None:\n        maxs = {p: samples[p].max() for p in parameters}\n    else:\n        # copy the dict\n        maxs = {p: val for p, val in maxs.items()}\n\n    # remove common offsets\n    for pi, param in enumerate(parameters):\n        values, offset = remove_common_offset(samples[param])\n        if offset != 0:\n            # we'll add the offset removed to the label\n            labels[param] = '{} - {:d}'.format(labels[param], offset)\n            samples[param] = values\n            mins[param] = mins[param] - float(offset)\n            maxs[param] = maxs[param] - float(offset)\n        # also remove from expected parameters, if they were provided\n        if expected_parameters is not None:\n            try:\n                expected_parameters[param] -= offset\n            except KeyError:\n                pass\n\n    # create the axis grid\n    if fig is None and axis_dict is None:\n        fig, axis_dict = create_axes_grid(\n            parameters, labels=labels,\n            width_ratios=width_ratios, height_ratios=height_ratios,\n            no_diagonals=not plot_marginal)\n\n    # Diagonals...\n    if plot_marginal:\n        for pi, param in enumerate(parameters):\n            ax, _, _ = axis_dict[param, param]\n            # if only plotting 2 parameters and on the second parameter,\n            # rotate the marginal plot\n            rotated = nparams == 2 and pi == nparams-1\n            # see if there are expected values\n            if expected_parameters is not None:\n                try:\n                    expected_value = expected_parameters[param]\n                except KeyError:\n                    expected_value = None\n            else:\n                expected_value = None\n            create_marginalized_hist(\n                ax, samples[param], label=labels[param],\n                color=hist_color, fillcolor=fill_color,\n                linestyle=marginal_linestyle, linecolor=line_color,\n                title=marginal_title, expected_value=expected_value,\n                expected_color=expected_parameters_color,\n                rotated=rotated, plot_min=mins[param], plot_max=maxs[param],\n                percentiles=marginal_percentiles)\n\n    # Off-diagonals...\n    for px, py in axis_dict:\n        if px == py:\n            continue\n        ax, _, _ = axis_dict[px, py]\n        if plot_scatter:\n            if plot_density:\n                alpha = 0.3\n            else:\n                alpha = 1.\n            plt = ax.scatter(x=samples[px], y=samples[py], c=zvals, s=5,\n                             edgecolors='none', vmin=vmin, vmax=vmax,\n                             cmap=scatter_cmap, alpha=alpha, zorder=2)\n\n        if plot_contours or plot_density:\n            # Exclude out-of-bound regions\n            # this is a bit kludgy; should probably figure out a better\n            # solution to eventually allow for more than just m_p m_s\n            if (px == 'm_p' and py == 'm_s') or (py == 'm_p' and px == 'm_s'):\n                exclude_region = 'm_s > m_p'\n            else:\n                exclude_region = None\n            create_density_plot(\n                px, py, samples, plot_density=plot_density,\n                plot_contours=plot_contours, cmap=density_cmap,\n                percentiles=contour_percentiles,\n                contour_color=contour_color, xmin=mins[px], xmax=maxs[px],\n                ymin=mins[py], ymax=maxs[py],\n                exclude_region=exclude_region, ax=ax,\n                use_kombine=use_kombine)\n\n        if expected_parameters is not None:\n            try:\n                ax.axvline(expected_parameters[px], lw=1.5,\n                           color=expected_parameters_color, zorder=5)\n            except KeyError:\n                pass\n            try:\n                ax.axhline(expected_parameters[py], lw=1.5,\n                           color=expected_parameters_color, zorder=5)\n            except KeyError:\n                pass\n\n        ax.set_xlim(mins[px], maxs[px])\n        ax.set_ylim(mins[py], maxs[py])\n\n    # adjust tick number for large number of plots\n    if len(parameters) > 3:\n        for px, py in axis_dict:\n            ax, _, _ = axis_dict[px, py]\n            ax.set_xticks(reduce_ticks(ax, 'x', maxticks=3))\n            ax.set_yticks(reduce_ticks(ax, 'y', maxticks=3))\n\n    if plot_scatter and show_colorbar:\n        # compute font size based on fig size\n        scale_fac = get_scale_fac(fig)\n        fig.subplots_adjust(right=0.85, wspace=0.03)\n        cbar_ax = fig.add_axes([0.9, 0.1, 0.03, 0.8])\n        cb = fig.colorbar(plt, cax=cbar_ax)\n        if cbar_label is not None:\n            cb.set_label(cbar_label, fontsize=12*scale_fac)\n        cb.ax.tick_params(labelsize=8*scale_fac)\n\n    return fig, axis_dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives an array of data removes a common offset > 1000 returning the array of data removed.", "response": "def remove_common_offset(arr):\n    \"\"\"Given an array of data, removes a common offset > 1000, returning the\n    removed value.\n    \"\"\"\n    offset = 0\n    isneg = (arr <= 0).all()\n    # make sure all values have the same sign\n    if isneg or (arr >= 0).all():\n        # only remove offset if the minimum and maximum values are the same\n        # order of magintude and > O(1000)\n        minpwr = numpy.log10(abs(arr).min())\n        maxpwr = numpy.log10(abs(arr).max())\n        if numpy.floor(minpwr) == numpy.floor(maxpwr) and minpwr > 3:\n            offset = numpy.floor(10**minpwr)\n            if isneg:\n                offset *= -1\n            arr = arr - offset\n    return arr, int(offset)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reduce_ticks(ax, which, maxticks=3):\n    ticks = getattr(ax, 'get_{}ticks'.format(which))()\n    if len(ticks) > maxticks:\n        # make sure the left/right value is not at the edge\n        minax, maxax = getattr(ax, 'get_{}lim'.format(which))()\n        dw = abs(maxax-minax)/10.\n        start_idx, end_idx = 0, len(ticks)\n        if ticks[0] < minax + dw:\n            start_idx += 1\n        if ticks[-1] > maxax - dw:\n            end_idx -= 1\n        # get reduction factor\n        fac = int(len(ticks) / maxticks)\n        ticks = ticks[start_idx:end_idx:fac]\n    return ticks", "response": "Resamples the axes which - axis ticks such that are at most\n    maxticks left."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef argmin(self):\n        return tuple(centres[index] for centres, index in\n                     zip(self.centres(), numpy.unravel_index(self.array.argmin(),\n                                                             self.array.shape)))", "response": "Return the co -ordinates of the bin centre containing the minimum value."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef logregularize(self, epsilon=2**-1074):\n        self.numerator.array[self.denominator.array == 0] = epsilon\n        self.denominator.array[self.denominator.array == 0] = 1\n        return self", "response": "Logregularizes the logarithm of the ratio array."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate the azimuthal angle in spherical coordinates from Cartesian coordinates.", "response": "def cartesian_to_spherical_azimuthal(x, y):\n    \"\"\" Calculates the azimuthal angle in spherical coordinates from Cartesian\n    coordinates. The azimuthal angle is in [0,2*pi].\n\n    Parameters\n    ----------\n    x : {numpy.array, float}\n        X-coordinate.\n    y : {numpy.array, float}\n        Y-coordinate.\n\n    Returns\n    -------\n    phi : {numpy.array, float}\n        The azimuthal angle.\n    \"\"\"\n    y = float(y) if isinstance(y, int) else y\n    phi = numpy.arctan2(y, x)\n    return phi % (2 * numpy.pi)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the polar angle in spherical coordinates from Cartesian coordinates.", "response": "def cartesian_to_spherical_polar(x, y, z):\n    \"\"\" Calculates the polar angle in spherical coordinates from Cartesian\n    coordinates. The polar angle is in [0,pi].\n\n    Parameters\n    ----------\n    x : {numpy.array, float}\n        X-coordinate.\n    y : {numpy.array, float}\n        Y-coordinate.\n    z : {numpy.array, float}\n        Z-coordinate.\n\n    Returns\n    -------\n    theta : {numpy.array, float}\n        The polar angle.\n    \"\"\"\n    return numpy.arccos(z / cartesian_to_spherical_rho(x, y, z))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmap cartesian coordinates x y z to spherical coordinates", "response": "def cartesian_to_spherical(x, y, z):\n    \"\"\" Maps cartesian coordinates (x,y,z) to spherical coordinates\n    (rho,phi,theta) where phi is in [0,2*pi] and theta is in [0,pi].\n\n    Parameters\n    ----------\n    x : {numpy.array, float}\n        X-coordinate.\n    y : {numpy.array, float}\n        Y-coordinate.\n    z : {numpy.array, float}\n        Z-coordinate.\n\n    Returns\n    -------\n    rho : {numpy.array, float}\n        The radial amplitude.\n    phi : {numpy.array, float}\n        The azimuthal angle.\n    theta : {numpy.array, float}\n        The polar angle.\n    \"\"\"\n    rho = cartesian_to_spherical_rho(x, y, z)\n    phi = cartesian_to_spherical_azimuthal(x, y)\n    theta = cartesian_to_spherical_polar(x, y, z)\n    return rho, phi, theta"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef spherical_to_cartesian(rho, phi, theta):\n    x = rho * numpy.cos(phi) * numpy.sin(theta)\n    y = rho * numpy.sin(phi) * numpy.sin(theta)\n    z = rho * numpy.cos(theta)\n    return x, y, z", "response": "Maps spherical coordinates to cartesian coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef rvs(self, size=1, param=None):\n\n        if param is not None:\n            dtype = [(param, float)]\n        else:\n            dtype = [(p, float) for p in self.params]\n        arr = numpy.zeros(size, dtype=dtype)\n        for (p,_) in dtype:\n            log_high = numpy.log10(self._bounds[p][0])\n            log_low = numpy.log10(self._bounds[p][1])\n            arr[p] = 10.0**(numpy.random.uniform(log_low, log_high, size=size))\n        return arr", "response": "Gives a set of random values drawn from this distribution."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the pdf at the given values.", "response": "def _pdf(self, **kwargs):\n        \"\"\"Returns the pdf at the given values. The keyword arguments must\n        contain all of parameters in self's params. Unrecognized arguments are\n        ignored.\n        \"\"\"\n        if kwargs in self:\n            vals = numpy.array([numpy.log(10) * self._norm * kwargs[param]\n                                for param in kwargs.keys()])\n            return 1.0 / numpy.prod(vals)\n        else:\n            return 0."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _logpdf(self, **kwargs):\n        if kwargs in self:\n            return numpy.log(self._pdf(**kwargs))\n        else:\n            return -numpy.inf", "response": "Returns the log of the pdf at the given values."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the bias of the median average PSD computed from n segments.", "response": "def median_bias(n):\n    \"\"\"Calculate the bias of the median average PSD computed from `n` segments.\n\n    Parameters\n    ----------\n    n : int\n        Number of segments used in PSD estimation.\n\n    Returns\n    -------\n    ans : float\n        Calculated bias.\n\n    Raises\n    ------\n    ValueError\n        For non-integer or non-positive `n`.\n\n    Notes\n    -----\n    See arXiv:gr-qc/0509116 appendix B for details.\n    \"\"\"\n    if type(n) is not int or n <= 0:\n        raise ValueError('n must be a positive integer')\n    if n >= 1000:\n        return numpy.log(2)\n    ans = 1\n    for i in range(1, int((n - 1) / 2 + 1)):\n        ans += 1.0 / (2*i + 1) - 1.0 / (2*i)\n    return ans"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef welch(timeseries, seg_len=4096, seg_stride=2048, window='hann',\n          avg_method='median', num_segments=None, require_exact_data_fit=False):\n    \"\"\"PSD estimator based on Welch's method.\n\n    Parameters\n    ----------\n    timeseries : TimeSeries\n        Time series for which the PSD is to be estimated.\n    seg_len : int\n        Segment length in samples.\n    seg_stride : int\n        Separation between consecutive segments, in samples.\n    window : {'hann', numpy.ndarray}\n        Function used to window segments before Fourier transforming, or\n        a `numpy.ndarray` that specifies the window.\n    avg_method : {'median', 'mean', 'median-mean'}\n        Method used for averaging individual segment PSDs.\n\n    Returns\n    -------\n    psd : FrequencySeries\n        Frequency series containing the estimated PSD.\n\n    Raises\n    ------\n    ValueError\n        For invalid choices of `seg_len`, `seg_stride` `window` and\n        `avg_method` and for inconsistent combinations of len(`timeseries`),\n        `seg_len` and `seg_stride`.\n\n    Notes\n    -----\n    See arXiv:gr-qc/0509116 for details.\n    \"\"\"\n    window_map = {\n        'hann': numpy.hanning\n    }\n\n    # sanity checks\n    if isinstance(window, numpy.ndarray) and window.size != seg_len:\n        raise ValueError('Invalid window: incorrect window length')\n    if not isinstance(window, numpy.ndarray) and window not in window_map:\n        raise ValueError('Invalid window: unknown window {!r}'.format(window))\n    if avg_method not in ('mean', 'median', 'median-mean'):\n        raise ValueError('Invalid averaging method')\n    if type(seg_len) is not int or type(seg_stride) is not int \\\n        or seg_len <= 0 or seg_stride <= 0:\n        raise ValueError('Segment length and stride must be positive integers')\n\n    if timeseries.precision == 'single':\n        fs_dtype = numpy.complex64\n    elif timeseries.precision == 'double':\n        fs_dtype = numpy.complex128\n\n    num_samples = len(timeseries)\n    if num_segments is None:\n        num_segments = int(num_samples // seg_stride)\n        # NOTE: Is this not always true?\n        if (num_segments - 1) * seg_stride + seg_len > num_samples:\n            num_segments -= 1\n\n    if not require_exact_data_fit:\n        data_len = (num_segments - 1) * seg_stride + seg_len\n\n        # Get the correct amount of data\n        if data_len < num_samples:\n            diff = num_samples - data_len\n            start = diff // 2\n            end = num_samples - diff // 2\n            # Want this to be integers so if diff is odd, catch it here.\n            if diff % 2:\n                start = start + 1\n\n            timeseries = timeseries[start:end]\n            num_samples = len(timeseries)\n        if data_len > num_samples:\n            err_msg = \"I was asked to estimate a PSD on %d \" %(data_len)\n            err_msg += \"data samples. However the data provided only contains \"\n            err_msg += \"%d data samples.\" %(num_samples)\n\n    if num_samples != (num_segments - 1) * seg_stride + seg_len:\n        raise ValueError('Incorrect choice of segmentation parameters')\n\n    if not isinstance(window, numpy.ndarray):\n        window = window_map[window](seg_len)\n    w = Array(window.astype(timeseries.dtype))\n\n    # calculate psd of each segment\n    delta_f = 1. / timeseries.delta_t / seg_len\n    segment_tilde = FrequencySeries(\n        numpy.zeros(int(seg_len / 2 + 1)),\n        delta_f=delta_f,\n        dtype=fs_dtype,\n    )\n\n    segment_psds = []\n    for i in range(num_segments):\n        segment_start = i * seg_stride\n        segment_end = segment_start + seg_len\n        segment = timeseries[segment_start:segment_end]\n        assert len(segment) == seg_len\n        fft(segment * w, segment_tilde)\n        seg_psd = abs(segment_tilde * segment_tilde.conj()).numpy()\n\n        #halve the DC and Nyquist components to be consistent with TO10095\n        seg_psd[0] /= 2\n        seg_psd[-1] /= 2\n\n        segment_psds.append(seg_psd)\n\n    segment_psds = numpy.array(segment_psds)\n\n    if avg_method == 'mean':\n        psd = numpy.mean(segment_psds, axis=0)\n    elif avg_method == 'median':\n        psd = numpy.median(segment_psds, axis=0) / median_bias(num_segments)\n    elif avg_method == 'median-mean':\n        odd_psds = segment_psds[::2]\n        even_psds = segment_psds[1::2]\n        odd_median = numpy.median(odd_psds, axis=0) / \\\n            median_bias(len(odd_psds))\n        even_median = numpy.median(even_psds, axis=0) / \\\n            median_bias(len(even_psds))\n        psd = (odd_median + even_median) / 2\n\n    psd *= 2 * delta_f * seg_len / (w*w).sum()\n\n    return FrequencySeries(psd, delta_f=delta_f, dtype=timeseries.dtype,\n                           epoch=timeseries.start_time)", "response": "Returns a new array of PSDs for the given time series."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmodifying a PSD such that the impulse response associated with its inverse spectrum is no longer than max_filter_len time samples.", "response": "def inverse_spectrum_truncation(psd, max_filter_len, low_frequency_cutoff=None, trunc_method=None):\n    \"\"\"Modify a PSD such that the impulse response associated with its inverse\n    square root is no longer than `max_filter_len` time samples. In practice\n    this corresponds to a coarse graining or smoothing of the PSD.\n\n    Parameters\n    ----------\n    psd : FrequencySeries\n        PSD whose inverse spectrum is to be truncated.\n    max_filter_len : int\n        Maximum length of the time-domain filter in samples.\n    low_frequency_cutoff : {None, int}\n        Frequencies below `low_frequency_cutoff` are zeroed in the output.\n    trunc_method : {None, 'hann'}\n        Function used for truncating the time-domain filter.\n        None produces a hard truncation at `max_filter_len`.\n\n    Returns\n    -------\n    psd : FrequencySeries\n        PSD whose inverse spectrum has been truncated.\n\n    Raises\n    ------\n    ValueError\n        For invalid types or values of `max_filter_len` and `low_frequency_cutoff`.\n\n    Notes\n    -----\n    See arXiv:gr-qc/0509116 for details.\n    \"\"\"\n    # sanity checks\n    if type(max_filter_len) is not int or max_filter_len <= 0:\n        raise ValueError('max_filter_len must be a positive integer')\n    if low_frequency_cutoff is not None and low_frequency_cutoff < 0 \\\n        or low_frequency_cutoff > psd.sample_frequencies[-1]:\n        raise ValueError('low_frequency_cutoff must be within the bandwidth of the PSD')\n\n    N = (len(psd)-1)*2\n\n    inv_asd = FrequencySeries((1. / psd)**0.5, delta_f=psd.delta_f, \\\n        dtype=complex_same_precision_as(psd))\n\n    inv_asd[0] = 0\n    inv_asd[N//2] = 0\n    q = TimeSeries(numpy.zeros(N), delta_t=(N / psd.delta_f), \\\n        dtype=real_same_precision_as(psd))\n\n    if low_frequency_cutoff:\n        kmin = int(low_frequency_cutoff / psd.delta_f)\n        inv_asd[0:kmin] = 0\n\n    ifft(inv_asd, q)\n\n    trunc_start = max_filter_len // 2\n    trunc_end = N - max_filter_len // 2\n    if trunc_end < trunc_start:\n        raise ValueError('Invalid value in inverse_spectrum_truncation')\n\n    if trunc_method == 'hann':\n        trunc_window = Array(numpy.hanning(max_filter_len), dtype=q.dtype)\n        q[0:trunc_start] *= trunc_window[max_filter_len//2:max_filter_len]\n        q[trunc_end:N] *= trunc_window[0:max_filter_len//2]\n\n    if trunc_start < trunc_end:\n        q[trunc_start:trunc_end] = 0\n    psd_trunc = FrequencySeries(numpy.zeros(len(psd)), delta_f=psd.delta_f, \\\n                                dtype=complex_same_precision_as(psd))\n    fft(q, psd_trunc)\n    psd_trunc *= psd_trunc.conj()\n    psd_out = 1. / abs(psd_trunc)\n\n    return psd_out"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef interpolate(series, delta_f):\n    new_n = (len(series)-1) * series.delta_f / delta_f + 1\n    samples = numpy.arange(0, numpy.rint(new_n)) * delta_f\n    interpolated_series = numpy.interp(samples, series.sample_frequencies.numpy(), series.numpy())\n    return FrequencySeries(interpolated_series, epoch=series.epoch,\n                           delta_f=delta_f, dtype=series.dtype)", "response": "Interpolate a frequency series to the desired delta_f."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bandlimited_interpolate(series, delta_f):\n    series = FrequencySeries(series, dtype=complex_same_precision_as(series), delta_f=series.delta_f)\n\n    N = (len(series) - 1) * 2\n    delta_t = 1.0 / series.delta_f / N\n\n    new_N = int(1.0 / (delta_t * delta_f))\n    new_n = new_N // 2 + 1\n\n    series_in_time = TimeSeries(zeros(N), dtype=real_same_precision_as(series), delta_t=delta_t)\n    ifft(series, series_in_time)\n\n    padded_series_in_time = TimeSeries(zeros(new_N), dtype=series_in_time.dtype, delta_t=delta_t)\n    padded_series_in_time[0:N//2] = series_in_time[0:N//2]\n    padded_series_in_time[new_N-N//2:new_N] = series_in_time[N//2:N]\n\n    interpolated_series = FrequencySeries(zeros(new_n), dtype=series.dtype, delta_f=delta_f)\n    fft(padded_series_in_time, interpolated_series)\n\n    return interpolated_series", "response": "Interpolate a frequency series to the desired delta_f."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process_full_data(fname, rhomin, mass1, mass2, lo_mchirp, hi_mchirp):\n    with h5py.File(fname, 'r') as bulk:\n\n        id_bkg = bulk['background_exc/template_id'][:]\n        id_fg = bulk['foreground/template_id'][:]\n\n        mchirp_bkg = mchirp_from_mass1_mass2(mass1[id_bkg], mass2[id_bkg])\n        bound = np.sign((mchirp_bkg - lo_mchirp) * (hi_mchirp - mchirp_bkg))\n        idx_bkg = np.where(bound == 1)\n        mchirp_fg = mchirp_from_mass1_mass2(mass1[id_fg], mass2[id_fg])\n        bound = np.sign((mchirp_fg - lo_mchirp) * (hi_mchirp - mchirp_fg))\n        idx_fg = np.where(bound == 1)\n\n        zerolagstat = bulk['foreground/stat'][:][idx_fg]\n        cstat_back_exc = bulk['background_exc/stat'][:][idx_bkg]\n        dec_factors = bulk['background_exc/decimation_factor'][:][idx_bkg]\n\n    return {'zerolagstat': zerolagstat[zerolagstat > rhomin],\n           'dec_factors': dec_factors[cstat_back_exc > rhomin],\n           'cstat_back_exc': cstat_back_exc[cstat_back_exc > rhomin]}", "response": "Read the zero - lag and time - lag triggers identified by templates in\n       a specified range of chirp mass."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading the STATMAP files to derive snr falloff for the background events and save the result to a txt file.", "response": "def save_bkg_falloff(fname_statmap, fname_bank, path, rhomin, lo_mchirp, hi_mchirp):\n    ''' Read the STATMAP files to derive snr falloff for the background events.\n        Save the output to a txt file\n        Bank file is also provided to restrict triggers to BBH templates.\n\n        Parameters\n        ----------\n        fname_statmap: string\n               STATMAP file containing trigger information\n        fname_bank: string\n               File name of the template bank\n        path: string\n               Destination where txt file is saved\n        rhomin: float\n               Minimum value of SNR threhold (will need including ifar)\n        lo_mchirp: float\n               Minimum chirp mass for the template\n        hi_mchirp: float\n               Maximum chirp mass for template\n    '''\n\n    with h5py.File(fname_bank, 'r') as bulk:\n        mass1_bank = bulk['mass1'][:]\n        mass2_bank = bulk['mass2'][:]\n        full_data = process_full_data(fname_statmap, rhomin,\n                           mass1_bank, mass2_bank, lo_mchirp, hi_mchirp)\n\n    max_bg_stat = np.max(full_data['cstat_back_exc'])\n    bg_bins = np.linspace(rhomin, max_bg_stat, 76)\n    bg_counts = np.histogram(full_data['cstat_back_exc'],\n                         weights=full_data['dec_factors'], bins=bg_bins)[0]\n\n    zerolagstat = full_data['zerolagstat']\n    coincs = zerolagstat[zerolagstat >= rhomin]\n\n    bkg = (bg_bins[:-1], bg_bins[1:], bg_counts)\n\n    return bkg, coincs"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the log of background fall - off for a set of triggers.", "response": "def log_rho_bg(trigs, bins, counts):\n    ''' Calculate the log of background fall-off\n\n        Parameters\n        ----------\n        trigs: array\n               SNR values of all the triggers\n        bins: string\n               bins for histogrammed triggers\n        path: string\n               counts for histogrammed triggers\n\n        Returns\n        -------\n        array\n    '''\n\n    trigs = np.atleast_1d(trigs)\n\n    N = sum(counts)\n\n    assert np.all(trigs >= np.min(bins)), \\\n        'Trigger SNR values cannot all be below the lowest bin limit!'\n\n    # If there are any triggers that are louder than the max bin, put one\n    # fictitious count in a bin that extends from the limits of the slide\n    # triggers out to the loudest trigger.\n\n    # If there is no counts for a foreground trigger put a fictitious count\n    # in the background bin\n    if np.any(trigs >= np.max(bins)):\n        N = N + 1\n        #log_plimit = -np.log(N) - np.log(np.max(trigs) - bins[-1]) CHECK IT\n\n    log_rhos = []\n    for t in trigs:\n        if t >= np.max(bins):\n            log_rhos.append(-log(N)-log(np.max(trigs) - bins[-1]))\n        else:\n            i = bisect.bisect(bins, t) - 1\n\n            if counts[i] == 0:\n                counts[i] = 1\n            log_rhos.append(log(counts[i]) - log(bins[i+1] - bins[i]) - log(N))\n    return np.array(log_rhos)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fgmc(log_fg_ratios, mu_log_vt, sigma_log_vt, Rf, maxfg):\n    '''\n    Function to fit the likelihood Fixme\n    '''\n\n    Lb = np.random.uniform(0., maxfg, len(Rf))\n    pquit = 0\n\n    while pquit < 0.1:\n        # quit when the posterior on Lf is very close to its prior\n\n        nsamp = len(Lb)\n        Rf_sel = np.random.choice(Rf, nsamp)\n        vt = np.random.lognormal(mu_log_vt, sigma_log_vt, len(Rf_sel))\n\n        Lf = Rf_sel * vt\n\n        log_Lf, log_Lb = log(Lf), log(Lb)\n\n        plR = 0\n        for lfr in log_fg_ratios:\n            plR += np.logaddexp(lfr + log_Lf, log_Lb)\n\n        plR -= (Lf + Lb)\n        plRn = plR - max(plR)\n\n        idx = np.exp(plRn) > np.random.random(len(plRn))\n\n        pquit = ss.stats.ks_2samp(Lb, Lb[idx])[1]\n\n        Lb = Lb[idx]\n\n    return Rf_sel[idx], Lf[idx], Lb", "response": "Function to fit the likelihood Fixme\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _optm(x, alpha, mu, sigma):\n    '''Return probability density of skew-lognormal\n       See scipy.optimize.curve_fit\n    '''\n    return ss.skewnorm.pdf(x, alpha, mu, sigma)", "response": "Return probability density of skew - lognormal\n       See scipy. optimize. curve_fit\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfitting skew - lognormal to the rate samples achived from a prior analysis", "response": "def fit(R):\n    ''' Fit skew - lognormal to the rate samples achived from a prior analysis\n        Parameters\n        ----------\n        R: array\n           Rate samples\n        Returns\n        -------\n        ff[0]: float\n            The skewness\n        ff[1]: float\n            The mean\n        ff[2]: float\n            The standard deviation\n    '''\n\n    lR = np.log(R)\n    mu_norm, sigma_norm = np.mean(lR), np.std(lR)\n\n    xs = np.linspace(min(lR), max(lR), 200)\n    kde = ss.gaussian_kde(lR)\n    pxs = kde(xs)\n\n    # Initial guess has been taken as the mean and std-dev of the data\n    # And a guess assuming small skewness\n    ff = optimize.curve_fit(_optm, xs, pxs, p0 = [0.1, mu_norm, sigma_norm])[0]\n    return ff[0], ff[1], ff[2]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef skew_lognormal_samples(alpha, mu, sigma, minrp, maxrp):\n    ''' Returns a large number of Skew lognormal samples\n        Parameters\n        ----------\n        alpha: float\n           Skewness of the distribution\n        mu: float\n           Mean of the distribution\n        sigma: float\n           Scale of the distribution\n        minrp: float\n           Minimum value for the samples\n        maxrp: float\n           Maximum value for the samples\n        Returns\n        -------\n        Rfs: array\n            Large number of samples (may need fixing)\n    '''\n\n    nsamp = 100000000\n    lRu = np.random.uniform(minrp, maxrp, nsamp)\n    plRu = ss.skewnorm.pdf(lRu, alpha, mu, sigma)\n    rndn = np.random.random(nsamp)\n    maxp = max(plRu)\n    idx = np.where(plRu/maxp > rndn)\n    log_Rf = lRu[idx]\n    Rfs = np.exp(log_Rf)\n\n    return Rfs", "response": "Returns a large number of Skew lognormal samples for a given set of parameters."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef prob_lnm(m1, m2, s1z, s2z, **kwargs):\n    ''' Return probability density for uniform in log\n        Parameters\n        ----------\n        m1: array\n            Component masses 1\n        m2: array\n            Component masses 2\n        s1z: array\n            Aligned spin 1(Not in use currently)\n        s2z:\n            Aligned spin 2(Not in use currently)\n        **kwargs: string\n            Keyword arguments as model parameters\n        Returns\n        -------\n        p_m1_m2: array\n            The probability density for m1, m2 pair\n    '''\n\n    min_mass = kwargs.get('min_mass', 5.)\n    max_mass = kwargs.get('max_mass', 95.)\n    max_mtotal = min_mass + max_mass\n    m1, m2 = np.array(m1), np.array(m2)\n\n    C_lnm = integrate.quad(lambda x: (log(max_mtotal - x) - log(min_mass))/x, min_mass, max_mass)[0]\n\n    xx = np.minimum(m1, m2)\n    m1 = np.maximum(m1, m2)\n    m2 = xx\n\n    bound = np.sign(max_mtotal - m1 - m2)\n    bound += np.sign(max_mass - m1) * np.sign(m2 - min_mass)\n    idx = np.where(bound != 2)\n\n    p_m1_m2 = (1/C_lnm)*(1./m1)*(1./m2)\n    p_m1_m2[idx] = 0\n\n    return p_m1_m2", "response": "Return probability density for uniform in log\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef prob_imf(m1, m2, s1z, s2z, **kwargs):\n    ''' Return probability density for power-law\n        Parameters\n        ----------\n        m1: array\n            Component masses 1\n        m2: array\n            Component masses 2\n        s1z: array\n            Aligned spin 1(Not in use currently)\n        s2z:\n            Aligned spin 2(Not in use currently)\n        **kwargs: string\n            Keyword arguments as model parameters\n\n        Returns\n        -------\n        p_m1_m2: array\n           the probability density for m1, m2 pair\n    '''\n\n    min_mass = kwargs.get('min_mass', 5.)\n    max_mass = kwargs.get('max_mass', 95.)\n    alpha = kwargs.get('alpha', -2.35)\n    max_mtotal = min_mass + max_mass\n    m1, m2 = np.array(m1), np.array(m2)\n\n    C_imf = max_mass**(alpha + 1)/(alpha + 1)\n    C_imf -= min_mass**(alpha + 1)/(alpha + 1)\n\n    xx = np.minimum(m1, m2)\n    m1 = np.maximum(m1, m2)\n    m2 = xx\n\n    bound = np.sign(max_mtotal - m1 - m2)\n    bound += np.sign(max_mass - m1) * np.sign(m2 - min_mass)\n    idx = np.where(bound != 2)\n\n    p_m1_m2 = np.zeros_like(m1)\n    idx = np.where(m1 <= max_mtotal/2.)\n    p_m1_m2[idx] = (1./C_imf) * m1[idx]**alpha /(m1[idx] - min_mass)\n    idx = np.where(m1 > max_mtotal/2.)\n    p_m1_m2[idx] = (1./C_imf) * m1[idx]**alpha /(max_mass - m1[idx])\n    p_m1_m2[idx] = 0\n\n    return p_m1_m2/2.", "response": "Return probability density for power - law of a set of components."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef prob_flat(m1, m2, s1z, s2z, **kwargs):\n    ''' Return probability density for uniform in component mass\n        Parameters\n        ----------\n        m1: array\n            Component masses 1\n        m2: array\n            Component masses 2\n        s1z: array\n            Aligned spin 1 (not in use currently)\n        s2z:\n            Aligned spin 2 (not in use currently)\n        **kwargs: string\n            Keyword arguments as model parameters\n\n        Returns\n        -------\n        p_m1_m2: array\n           the probability density for m1, m2 pair\n    '''\n\n    min_mass = kwargs.get('min_mass', 1.)\n    max_mass = kwargs.get('max_mass', 2.)\n\n    bound = np.sign(m1 - m2)\n    bound += np.sign(max_mass - m1) * np.sign(m2 - min_mass)\n    idx = np.where(bound != 2)\n\n    p_m1_m2 = 2. / (max_mass - min_mass)**2\n    p_m1_m2[idx] = 0\n\n    return p_m1_m2", "response": "Return probability density for uniform in component masses."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef draw_imf_samples(**kwargs):\n    ''' Draw samples for power-law model\n\n        Parameters\n        ----------\n        **kwargs: string\n           Keyword arguments as model parameters and number of samples\n\n        Returns\n        -------\n        array\n           The first mass\n        array\n           The second mass\n    '''\n\n    alpha_salpeter = kwargs.get('alpha', -2.35)\n    nsamples = kwargs.get('nsamples', 1)\n    min_mass = kwargs.get('min_mass', 5.)\n    max_mass = kwargs.get('max_mass', 95.)\n    max_mtotal = min_mass + max_mass\n\n    a = (max_mass/min_mass)**(alpha_salpeter + 1.0) - 1.0\n    beta = 1.0 / (alpha_salpeter + 1.0)\n\n    k = nsamples * int(1.5 + log(1 + 100./nsamples))\n    aa = min_mass * (1.0 + a * np.random.random(k))**beta\n    bb = np.random.uniform(min_mass, aa, k)\n\n    idx = np.where(aa + bb < max_mtotal)\n    m1, m2 = (np.maximum(aa, bb))[idx], (np.minimum(aa, bb))[idx]\n\n    return np.resize(m1, nsamples), np.resize(m2, nsamples)", "response": "Draw samples for power - law model\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndraw samples for uniform - in - log model", "response": "def draw_lnm_samples(**kwargs):\n    ''' Draw samples for uniform-in-log model\n\n        Parameters\n        ----------\n        **kwargs: string\n           Keyword arguments as model parameters and number of samples\n\n        Returns\n        -------\n        array\n           The first mass\n        array\n           The second mass\n    '''\n\n    #PDF doesnt match with sampler\n    nsamples = kwargs.get('nsamples', 1)\n    min_mass = kwargs.get('min_mass', 5.)\n    max_mass = kwargs.get('max_mass', 95.)\n    max_mtotal = min_mass + max_mass\n    lnmmin = log(min_mass)\n    lnmmax = log(max_mass)\n\n    k = nsamples * int(1.5 + log(1 + 100./nsamples))\n    aa = np.exp(np.random.uniform(lnmmin, lnmmax, k))\n    bb = np.exp(np.random.uniform(lnmmin, lnmmax, k))\n\n    idx = np.where(aa + bb < max_mtotal)\n    m1, m2 = (np.maximum(aa, bb))[idx], (np.minimum(aa, bb))[idx]\n\n    return np.resize(m1, nsamples), np.resize(m2, nsamples)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndraws samples for uniform in mass", "response": "def draw_flat_samples(**kwargs):\n    ''' Draw samples for uniform in mass\n\n        Parameters\n        ----------\n        **kwargs: string\n           Keyword arguments as model parameters and number of samples\n\n        Returns\n        -------\n        array\n           The first mass\n        array\n           The second mass\n    '''\n\n    #PDF doesnt match with sampler\n    nsamples = kwargs.get('nsamples', 1)\n    min_mass = kwargs.get('min_mass', 1.)\n    max_mass = kwargs.get('max_mass', 2.)\n\n    m1 = np.random.uniform(min_mass, max_mass, nsamples)\n    m2 = np.random.uniform(min_mass, max_mass, nsamples)\n\n    return np.maximum(m1, m2), np.minimum(m1, m2)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndraws chirp mass samples for uniform - in - log model", "response": "def mchirp_sampler_lnm(**kwargs):\n    ''' Draw chirp mass samples for uniform-in-log model\n\n        Parameters\n        ----------\n        **kwargs: string\n           Keyword arguments as model parameters and number of samples\n\n        Returns\n        -------\n        mchirp-astro: array\n           The chirp mass samples for the population\n    '''\n    m1, m2 = draw_lnm_samples(**kwargs)\n    mchirp_astro = mchirp_from_mass1_mass2(m1, m2)\n\n    return mchirp_astro"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef mchirp_sampler_imf(**kwargs):\n    ''' Draw chirp mass samples for power-law model\n\n        Parameters\n        ----------\n        **kwargs: string\n           Keyword arguments as model parameters and number of samples\n\n        Returns\n        -------\n        mchirp-astro: array\n           The chirp mass samples for the population\n    '''\n    m1, m2 = draw_imf_samples(**kwargs)\n    mchirp_astro = mchirp_from_mass1_mass2(m1, m2)\n\n    return mchirp_astro", "response": "Draw chirp mass samples for power - law model\n        Returns the chirp mass samples for the power - law model\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndraws chirp mass samples for flat in mass model Returns the chirp mass samples for the flat in mass model", "response": "def mchirp_sampler_flat(**kwargs):\n    ''' Draw chirp mass samples for flat in mass model\n\n        Parameters\n        ----------\n        **kwargs: string\n           Keyword arguments as model parameters and number of samples\n\n        Returns\n        -------\n        mchirp-astro: array\n           The chirp mass samples for the population\n    '''\n    m1, m2 = draw_flat_samples(**kwargs)\n    mchirp_astro = mchirp_from_mass1_mass2(m1, m2)\n\n    return mchirp_astro"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef frequency_noise_from_psd(psd, seed=None):\n    sigma = 0.5 * (psd / psd.delta_f) ** (0.5)\n    if seed is not None:\n        numpy.random.seed(seed)\n    sigma = sigma.numpy()\n    dtype = complex_same_precision_as(psd)\n\n    not_zero = (sigma != 0)\n\n    sigma_red = sigma[not_zero]\n    noise_re = numpy.random.normal(0, sigma_red)\n    noise_co = numpy.random.normal(0, sigma_red)\n    noise_red = noise_re + 1j * noise_co\n\n    noise = numpy.zeros(len(sigma), dtype=dtype)\n    noise[not_zero] = noise_red\n\n    return FrequencySeries(noise,\n                           delta_f=psd.delta_f,\n                           dtype=dtype)", "response": "Create a new frequency series with a given PSD."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef noise_from_psd(length, delta_t, psd, seed=None):\n    noise_ts = TimeSeries(zeros(length), delta_t=delta_t)\n\n    if seed is None:\n        seed = numpy.random.randint(2**32)\n\n    randomness = lal.gsl_rng(\"ranlux\", seed)\n\n    N = int (1.0 / delta_t / psd.delta_f)\n    n = N//2+1\n    stride = N//2\n\n    if n > len(psd):\n        raise ValueError(\"PSD not compatible with requested delta_t\")\n\n    psd = (psd[0:n]).lal()\n    psd.data.data[n-1] = 0\n\n    segment = TimeSeries(zeros(N), delta_t=delta_t).lal()\n    length_generated = 0\n\n    SimNoise(segment, 0, psd, randomness)\n    while (length_generated < length):\n        if (length_generated + stride) < length:\n            noise_ts.data[length_generated:length_generated+stride] = segment.data.data[0:stride]\n        else:\n            noise_ts.data[length_generated:length] = segment.data.data[0:length-length_generated]\n\n        length_generated += stride\n        SimNoise(segment, stride, psd, randomness)\n\n    return noise_ts", "response": "Create a new N - dimensional TimeSeries containing a gaussian noise colored by the given PSD."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef noise_from_string(psd_name, length, delta_t, seed=None, low_frequency_cutoff=10.0):\n    import pycbc.psd\n\n    # We just need enough resolution to resolve lines\n    delta_f = 1.0 / 8\n    flen = int(.5 / delta_t / delta_f) + 1\n    psd = pycbc.psd.from_string(psd_name, flen, delta_f, low_frequency_cutoff)\n    return noise_from_psd(int(length), delta_t, psd, seed=seed)", "response": "Create a new N - tuple with a given PSD and a length."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload an Array from a. hdf or. txt file.", "response": "def load_array(path, group=None):\n    \"\"\"\n    Load an Array from a .hdf, .txt or .npy file. The\n    default data types will be double precision floating point.\n\n    Parameters\n    ----------\n    path : string\n        source file path. Must end with either .npy or .txt.\n\n    group: string \n        Additional name for internal storage use. Ex. hdf storage uses\n        this as the key value.\n\n    Raises\n    ------\n    ValueError\n        If path does not end in .npy or .txt.\n    \"\"\"\n    ext = _os.path.splitext(path)[1]\n    if ext == '.npy':\n        data = _numpy.load(path)    \n    elif ext == '.txt':\n        data = _numpy.loadtxt(path)\n    elif ext == '.hdf':\n        key = 'data' if group is None else group\n        return Array(h5py.File(path)[key]) \n    else:\n        raise ValueError('Path must end with .npy, .hdf, or .txt')\n        \n    if data.ndim == 1:\n        return Array(data)\n    elif data.ndim == 2:\n        return Array(data[:,0] + 1j*data[:,1])\n    else:\n        raise ValueError('File has %s dimensions, cannot convert to Array, \\\n                          must be 1 (real) or 2 (complex)' % data.ndim)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrap the ary to return an Array type", "response": "def _return(self, ary):\n        \"\"\"Wrap the ary to return an Array type \"\"\"\n        if isinstance(ary, Array):\n            return ary\n        return Array(ary, copy=False)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _icheckother(fn, self, other):\n        self._typecheck(other) \n        if type(other) in _ALLOWED_SCALARS:\n            if self.kind == 'real' and type(other) == complex:\n                raise TypeError('dtypes are incompatible')\n            other = force_precision_to_match(other, self.precision)\n        elif isinstance(other, type(self)) or type(other) is Array:\n            if len(other) != len(self):\n                raise ValueError('lengths do not match')\n            if self.kind == 'real' and other.kind == 'complex':\n                raise TypeError('dtypes are incompatible')\n            if other.precision == self.precision:\n                _convert_to_scheme(other)\n                other = other._data\n            else:\n                raise TypeError('precisions do not match')\n        else:\n            return NotImplemented\n\n        return fn(self, other)", "response": "Checks the input to in - place operations"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompare whether two arrays are almost equal element - by - element.", "response": "def almost_equal_elem(self,other,tol,relative=True):\n        \"\"\"\n        Compare whether two array types are almost equal, element\n        by element.\n\n        If the 'relative' parameter is 'True' (the default) then the\n        'tol' parameter (which must be positive) is interpreted as a\n        relative tolerance, and the comparison returns 'True' only if\n        abs(self[i]-other[i]) <= tol*abs(self[i])\n        for all elements of the array.\n\n        If 'relative' is 'False', then 'tol' is an absolute tolerance,\n        and the comparison is true only if\n        abs(self[i]-other[i]) <= tol\n        for all elements of the array.\n\n        Other meta-data (type, dtype, and length) must be exactly equal.\n        If either object's memory lives on the GPU it will be copied to\n        the CPU for the comparison, which may be slow.  But the original\n        object itself will not have its memory relocated nor scheme\n        changed.\n\n        Parameters\n        ----------\n        other\n            Another Python object, that should be tested for\n            almost-equality with 'self', element-by-element.\n        tol\n            A non-negative number, the tolerance, which is interpreted\n            as either a relative tolerance (the default) or an absolute\n            tolerance.\n        relative\n            A boolean, indicating whether 'tol' should be interpreted\n            as a relative tolerance (if True, the default if this argument\n            is omitted) or as an absolute tolerance (if tol is False).\n\n        Returns\n        -------\n        boolean \n            'True' if the data agree within the tolerance, as\n            interpreted by the 'relative' keyword, and if the types,\n            lengths, and dtypes are exactly the same.\n        \"\"\"\n        # Check that the tolerance is non-negative and raise an\n        # exception otherwise.\n        if (tol<0):\n            raise ValueError(\"Tolerance cannot be negative\")\n        # Check that the meta-data agree; the type check is written in\n        # this way so that this method may be safely called from\n        # subclasses as well.\n        if type(other) != type(self):\n            return False\n        if self.dtype != other.dtype:\n            return False\n        if len(self) != len(other):\n            return False\n\n        # The numpy() method will move any GPU memory onto the CPU.\n        # Slow, but the user was warned.\n\n        diff = abs(self.numpy()-other.numpy())\n        if relative:\n            cmpary = tol*abs(self.numpy())\n        else:\n            cmpary = tol*ones(len(self),dtype=self.dtype)\n\n        return (diff<=cmpary).all()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncompares whether two arrays are almost equal and normwise.", "response": "def almost_equal_norm(self,other,tol,relative=True):\n        \"\"\"\n        Compare whether two array types are almost equal, normwise.\n\n        If the 'relative' parameter is 'True' (the default) then the\n        'tol' parameter (which must be positive) is interpreted as a\n        relative tolerance, and the comparison returns 'True' only if\n        abs(norm(self-other)) <= tol*abs(norm(self)).\n\n        If 'relative' is 'False', then 'tol' is an absolute tolerance,\n        and the comparison is true only if\n        abs(norm(self-other)) <= tol\n\n        Other meta-data (type, dtype, and length) must be exactly equal.\n        If either object's memory lives on the GPU it will be copied to\n        the CPU for the comparison, which may be slow.  But the original\n        object itself will not have its memory relocated nor scheme\n        changed.\n\n        Parameters\n        ----------\n        other\n            another Python object, that should be tested for\n            almost-equality with 'self', based on their norms.\n        tol \n            a non-negative number, the tolerance, which is interpreted\n            as either a relative tolerance (the default) or an absolute\n            tolerance.\n        relative\n            A boolean, indicating whether 'tol' should be interpreted\n            as a relative tolerance (if True, the default if this argument\n            is omitted) or as an absolute tolerance (if tol is False).\n\n        Returns\n        -------\n        boolean\n            'True' if the data agree within the tolerance, as\n            interpreted by the 'relative' keyword, and if the types,\n            lengths, and dtypes are exactly the same.\n        \"\"\"\n        # Check that the tolerance is non-negative and raise an\n        # exception otherwise.\n        if (tol<0):\n            raise ValueError(\"Tolerance cannot be negative\")\n        # Check that the meta-data agree; the type check is written in\n        # this way so that this method may be safely called from\n        # subclasses as well.\n        if type(other) != type(self):\n            return False\n        if self.dtype != other.dtype:\n            return False\n        if len(self) != len(other):\n            return False\n\n        # The numpy() method will move any GPU memory onto the CPU.\n        # Slow, but the user was warned.\n\n        diff = self.numpy()-other.numpy()\n        dnorm = norm(diff)\n        if relative:\n            return (dnorm <= tol*norm(self))\n        else:\n            return (dnorm <= tol)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef resize(self, new_size):\n        if new_size == len(self):\n            return\n        else:\n            self._saved = LimitedSizeDict(size_limit=2**5)\n            new_arr = zeros(new_size, dtype=self.dtype)\n            if len(self) <= new_size:\n                new_arr[0:len(self)] = self\n            else:\n                new_arr[:] = self[0:new_size]\n                \n            self._data = new_arr._data", "response": "Resize the array to new_size."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nrolls the array to a given shift", "response": "def roll(self, shift):\n        \"\"\"shift vector\n        \"\"\"\n        self._saved = LimitedSizeDict(size_limit=2**5)\n        new_arr = zeros(len(self), dtype=self.dtype)\n\n        if shift < 0:\n            shift = shift - len(self) * (shift // len(self))\n        \n        if shift == 0:\n            return\n        \n        new_arr[0:shift] = self[len(self)-shift: len(self)]\n        new_arr[shift:len(self)] = self[0:len(self)-shift]\n            \n        self._data = new_arr._data"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a LAL Object that contains this data", "response": "def lal(self):\n        \"\"\" Returns a LAL Object that contains this data \"\"\"\n\n        lal_data = None\n        if self._data.dtype == float32:\n            lal_data = _lal.CreateREAL4Vector(len(self))\n        elif self._data.dtype == float64:\n            lal_data = _lal.CreateREAL8Vector(len(self))\n        elif self._data.dtype == complex64:\n            lal_data = _lal.CreateCOMPLEX8Vector(len(self))\n        elif self._data.dtype == complex128:\n            lal_data = _lal.CreateCOMPLEX16Vector(len(self))\n\n        lal_data.data[:] = self.numpy()\n\n        return lal_data"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsaves array to a Numpy hdf or text file.", "response": "def save(self, path, group=None):\n        \"\"\"\n        Save array to a Numpy .npy, hdf, or text file. When saving a complex array as\n        text, the real and imaginary parts are saved as the first and second\n        column respectively. When using hdf format, the data is stored\n        as a single vector, along with relevant attributes.\n\n        Parameters\n        ----------\n        path: string\n            Destination file path. Must end with either .hdf, .npy or .txt.\n            \n        group: string \n            Additional name for internal storage use. Ex. hdf storage uses\n            this as the key value.\n\n        Raises\n        ------\n        ValueError\n            If path does not end in .npy or .txt.\n        \"\"\"\n\n        ext = _os.path.splitext(path)[1]\n        if ext == '.npy':\n            _numpy.save(path, self.numpy())\n        elif ext == '.txt':\n            if self.kind == 'real':\n                _numpy.savetxt(path, self.numpy())\n            elif self.kind == 'complex':\n                output = _numpy.vstack((self.numpy().real,\n                                        self.numpy().imag)).T\n                _numpy.savetxt(path, output)\n        elif ext == '.hdf':\n            key = 'data' if group is None else group\n            f = h5py.File(path)\n            f.create_dataset(key, data=self.numpy(), compression='gzip',\n                             compression_opts=9, shuffle=True)\n        else:\n            raise ValueError('Path must end with .npy, .txt, or .hdf')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef trim_zeros(self):\n        tmp = self.numpy()\n        f = len(self)-len(_numpy.trim_zeros(tmp, trim='f'))\n        b = len(self)-len(_numpy.trim_zeros(tmp, trim='b'))\n        return self[f:len(self)-b]", "response": "Remove the leading and trailing zeros."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmake a well layout in a two column format", "response": "def two_column_layout(path, cols, **kwargs):\n    \"\"\" Make a well layout in a two column format\n\n    Parameters\n    ----------\n    path: str\n        Location to make the well html file\n    cols: list of tuples\n        The format of the items on the well result section. Each tuple\n        contains the two files that are shown in the left and right hand\n        side of a row in the well.html page.\n    \"\"\"\n    path = os.path.join(os.getcwd(), path, 'well.html')\n    from pycbc.results.render import render_workflow_html_template\n    render_workflow_html_template(path, 'two_column.html', cols, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef single_layout(path, files, **kwargs):\n    two_column_layout(path, [(f,) for f in files], **kwargs)", "response": "Make a well layout in single column format"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngrouping a well layout in chunks of two from a list of files", "response": "def group_layout(path, files, **kwargs):\n    \"\"\" Make a well layout in chunks of two from a list of files\n\n    path: str\n        Location to make the well html file\n    files: list of pycbc.workflow.core.Files\n        This list of images to show in order within the well layout html file.\n        Every two are placed on the same row.\n    \"\"\"\n    if len(files) > 0:\n        two_column_layout(path, list(grouper(files, 2)), **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_seg_table(workflow, seg_files, seg_names, out_dir, tags=None,\n                  title_text=None, description=None):\n    \"\"\" Creates a node in the workflow for writing the segment summary\n    table. Returns a File instances for the output file.\n    \"\"\"\n    seg_files = list(seg_files)\n    seg_names = list(seg_names)\n    if tags is None: tags = []\n    makedir(out_dir)\n    node = PlotExecutable(workflow.cp, 'page_segtable', ifos=workflow.ifos,\n                    out_dir=out_dir, tags=tags).create_node()\n    node.add_input_list_opt('--segment-files', seg_files)\n    quoted_seg_names = []\n    for s in seg_names:\n        quoted_seg_names.append(\"'\" + s + \"'\")\n    node.add_opt('--segment-names', ' '.join(quoted_seg_names))\n    if description:\n        node.add_opt('--description', \"'\" + description + \"'\")\n    if title_text:\n        node.add_opt('--title-text', \"'\" + title_text + \"'\")\n    node.new_output_file_opt(workflow.analysis_time, '.html', '--output-file')\n    workflow += node\n    return node.output_files[0]", "response": "Creates a node in the workflow for writing the segment summary\n    table."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a node in the workflow for writing the veto_definer table.", "response": "def make_veto_table(workflow, out_dir, vetodef_file=None, tags=None):\n    \"\"\" Creates a node in the workflow for writing the veto_definer\n    table. Returns a File instances for the output file.\n    \"\"\"\n    if vetodef_file is None:\n        vetodef_file = workflow.cp.get_opt_tags(\"workflow-segments\",\n                                           \"segments-veto-definer-file\", [])\n        file_url = urlparse.urljoin('file:',\n                                    urllib.pathname2url(vetodef_file))\n        vdf_file = File(workflow.ifos, 'VETO_DEFINER',\n                        workflow.analysis_time, file_url=file_url)\n        vdf_file.PFN(file_url, site='local')\n    else:\n        vdf_file = vetodef_file\n\n    if tags is None: tags = []\n    makedir(out_dir)\n    node = PlotExecutable(workflow.cp, 'page_vetotable', ifos=workflow.ifos,\n                    out_dir=out_dir, tags=tags).create_node()\n    node.add_input_opt('--veto-definer-file', vdf_file)\n    node.new_output_file_opt(workflow.analysis_time, '.html', '--output-file')\n    workflow += node\n    return node.output_files[0]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make_ifar_plot(workflow, trigger_file, out_dir, tags=None,\n                   hierarchical_level=None):\n    \"\"\" Creates a node in the workflow for plotting cumulative histogram\n    of IFAR values.\n    \"\"\"\n\n    if hierarchical_level is not None and tags:\n        tags = [(\"HIERARCHICAL_LEVEL_{:02d}\".format(\n                hierarchical_level))] + tags\n    elif hierarchical_level is not None and not tags:\n        tags = [\"HIERARCHICAL_LEVEL_{:02d}\".format(hierarchical_level)]\n    elif hierarchical_level is None and not tags:\n        tags = []\n\n    makedir(out_dir)\n    node = PlotExecutable(workflow.cp, 'page_ifar', ifos=workflow.ifos,\n                    out_dir=out_dir, tags=tags).create_node()\n    node.add_input_opt('--trigger-file', trigger_file)\n    if hierarchical_level is not None:\n        node.add_opt('--use-hierarchical-level', hierarchical_level)\n    node.new_output_file_opt(workflow.analysis_time, '.png', '--output-file')\n    workflow += node\n    return node.output_files[0]", "response": "Creates a node in the workflow for plotting cumulative histogram of IFAR values."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds writing ntemps to file.", "response": "def write_sampler_metadata(self, sampler):\n        \"\"\"Adds writing ntemps to file.\n        \"\"\"\n        super(MultiTemperedMetadataIO, self).write_sampler_metadata(sampler)\n        self[self.sampler_group].attrs[\"ntemps\"] = sampler.ntemps"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef extra_args_parser(parser=None, skip_args=None, **kwargs):\n        if skip_args is None:\n            skip_args = []\n        parser, actions = MCMCMetadataIO.extra_args_parser(\n            parser=parser, skip_args=skip_args, **kwargs)\n        if 'temps' not in skip_args:\n            act = parser.add_argument(\n                \"--temps\", nargs=\"+\", default=0, action=ParseTempsArg,\n                help=\"Get the given temperatures. May provide either a \"\n                     \"sequence of integers specifying the temperatures to \"\n                     \"plot, or 'all' for all temperatures. Default is to only \"\n                     \"plot the coldest (= 0) temperature chain.\")\n            actions.append(act)\n        return parser, actions", "response": "Adds the temperatures to the MCMCIO parser."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef walk(chains, start, end, step):\n\n    # get number of chains, parameters, and iterations\n    chains = numpy.array(chains)\n    _, nparameters, _ = chains.shape\n\n    # get end index of blocks\n    ends = numpy.arange(start, end, step)\n    stats = numpy.zeros((nparameters, len(ends)))\n\n    # get start index of blocks\n    starts = numpy.array(len(ends) * [start])\n\n    # loop over end indexes and calculate statistic\n    for i, e in enumerate(ends):\n        tmp = chains[:, :, 0:e]\n        stats[:, i] = gelman_rubin(tmp)\n\n    return starts, ends, stats", "response": "This function walks the given set of samples and calculates the convergence statistic for each step."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef gelman_rubin(chains, auto_burn_in=True):\n\n    # remove first half of samples\n    # this will have shape (nchains, nparameters, niterations)\n    if auto_burn_in:\n        _, _, niterations = numpy.array(chains).shape\n        chains = numpy.array([chain[:, niterations / 2 + 1:]\n                              for chain in chains])\n\n    # get number of chains, parameters, and iterations\n    chains = numpy.array(chains)\n    nchains, nparameters, niterations = chains.shape\n\n    # calculate the covariance matrix for each chain\n    # this will have shape (nchains, nparameters, nparameters)\n    chains_covs = numpy.array([numpy.cov(chain) for chain in chains])\n    if nparameters == 1:\n        chains_covs = chains_covs.reshape((nchains, 1, 1))\n\n    # calculate W the within-chain variance\n    # this will have shape (nparameters, nparameters)\n    w = numpy.zeros(chains_covs[0].shape)\n    for i, row in enumerate(chains_covs[0]):\n        for j, _ in enumerate(row):\n            w[i, j] = numpy.mean(chains_covs[:, i, j])\n    if nparameters == 1:\n        w = w.reshape((1, 1))\n\n    # calculate B the between-chain variance\n    # this will have shape (nparameters, nparameters)\n    means = numpy.zeros((nparameters, nchains))\n    for i, chain in enumerate(chains):\n        means[:, i] = numpy.mean(chain, axis=1).transpose()\n    b = niterations * numpy.cov(means)\n    if nparameters == 1:\n        b = b.reshape((1, 1))\n\n    # get diagonal elements of W and B\n    # these will have shape (nparameters)\n    w_diag = numpy.diag(w)\n    b_diag = numpy.diag(b)\n\n    # get variance for each chain\n    # this will have shape (nparameters, nchains)\n    var = numpy.zeros((nparameters, nchains))\n    for i, chain_cov in enumerate(chains_covs):\n        var[:, i] = numpy.diag(chain_cov)\n\n    # get mean of means\n    # this will have shape (nparameters)\n    mu_hat = numpy.mean(means, axis=1)\n\n    # get variance of variances\n    # this will have shape (nparameters)\n    s = numpy.var(var, axis=1)\n\n    # get V the combined variance of all chains\n    # this will have shape (nparameters)\n    v = ((niterations - 1.) * w_diag / niterations +\n         (1. + 1. / nchains) * b_diag / niterations)\n\n    # get factors in variance of V calculation\n    # this will have shape (nparameters)\n    k = 2 * b_diag**2 / (nchains - 1)\n    mid_term = numpy.cov(\n        var, means**2)[nparameters:2*nparameters, 0:nparameters].T\n    end_term = numpy.cov(\n        var, means)[nparameters:2*nparameters, 0:nparameters].T\n    wb = niterations / nchains * numpy.diag(mid_term - 2 * mu_hat * end_term)\n\n    # get variance of V\n    # this will have shape (nparameters)\n    var_v = (\n        (niterations - 1.) ** 2 * s +\n        (1. + 1. / nchains) ** 2 * k +\n        2. * (niterations - 1.) * (1. + 1. / nchains) * wb\n    ) / niterations**2\n\n    # get degrees of freedom\n    # this will have shape (nparameters)\n    dof = (2. * v**2) / var_v\n\n    # more degrees of freedom factors\n    # this will have shape (nparameters)\n    df_adj = (dof + 3.) / (dof + 1.)\n\n    # estimate R\n    # this will have shape (nparameters)\n    r2_fixed = (niterations - 1.) / niterations\n    r2_random = (1. + 1. / nchains) * (1. / niterations) * (b_diag / w_diag)\n    r2_estimate = r2_fixed + r2_random\n\n    # calculate PSRF the potential scale reduction factor\n    # this will have shape (nparameters)\n    psrf = numpy.sqrt(r2_estimate * df_adj)\n\n    return psrf", "response": "Calculates the univariate Gelman - Rubin convergence statistic for a set of Markov - Chain objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _pdf(self, **kwargs):\n        for p in self._params:\n            if p not in kwargs.keys():\n                raise ValueError('Missing parameter {} to construct pdf.'\n                                 .format(p))\n        if kwargs in self:\n            # transform into the kde space\n            jacobian = 1.\n            for param, tparam in self._tparams.items():\n                t = self._transforms[tparam]\n                try:\n                    samples = t.transform({param: kwargs[param]})\n                except ValueError as e:\n                    # can get a value error if the value is exactly == to\n                    # the bounds, in which case, just return 0.\n                    if kwargs[param] in self.bounds[param]:\n                        return 0.\n                    else:\n                        raise ValueError(e)\n                kwargs[param] = samples[tparam]\n                # update the jacobian for the transform; if p is the pdf\n                # in the params frame (the one we want) and p' is the pdf\n                # in the transformed frame (the one that's calculated) then:\n                # p = J * p', where J is the Jacobian of going from p to p'\n                jacobian *= t.jacobian(samples)\n            # for scipy < 0.15.0, gaussian_kde.pdf = gaussian_kde.evaluate\n            this_pdf = jacobian * self._kde.evaluate([kwargs[p]\n                                                      for p in self._params])\n            if len(this_pdf) == 1:\n                return float(this_pdf)\n            else:\n                return this_pdf\n        else:\n            return 0.", "response": "Returns the pdf at the given values."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rvs(self, size=1, param=None):\n        if param is not None:\n            dtype = [(param, float)]\n        else:\n            dtype = [(p, float) for p in self.params]\n        size = int(size)\n        arr = numpy.zeros(size, dtype=dtype)\n        draws = self._kde.resample(size)\n        draws = {param: draws[ii,:] for ii,param in enumerate(self.params)}\n        for (param,_) in dtype:\n            try:\n                # transform back to param space\n                tparam = self._tparams[param]\n                tdraws = {tparam: draws[param]}\n                draws[param] = self._transforms[tparam].inverse_transform(\n                    tdraws)[param]\n            except KeyError:\n                pass\n            arr[param] = draws[param]\n        return arr", "response": "Gives a set of random values drawn from the kde."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads the values of one or more parameters from an hdf file and returns as a dictionary.", "response": "def get_arrays_from_file(params_file, params=None):\n        \"\"\"Reads the values of one or more parameters from an hdf file and\n        returns as a dictionary.\n\n        Parameters\n        ----------\n        params_file : str\n            The hdf file that contains the values of the parameters.\n        params : {None, list}\n            If provided, will just retrieve the given parameter names.\n\n        Returns\n        -------\n        dict\n            A dictionary of the parameters mapping `param_name -> array`.\n        \"\"\"\n        try:\n            f = h5py.File(params_file, 'r')\n        except:\n            raise ValueError('File not found.')\n        if params is not None:\n            if not isinstance(params, list):\n                params = [params]\n            for p in params:\n                if p not in f.keys():\n                    raise ValueError('Parameter {} is not in {}'\n                                     .format(p, params_file))\n        else:\n            params = [str(k) for k in f.keys()]\n        params_values = {p:f[p][:] for p in params}\n        try:\n            bandwidth = f.attrs[\"bandwidth\"]\n        except KeyError:\n            bandwidth = \"scott\"\n\n        f.close()\n        return params_values, bandwidth"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a BoundedDistanction object from a configuration file.", "response": "def from_config(cls, cp, section, variable_args):\n        \"\"\"Returns a distribution based on a configuration file.\n\n        The parameters\n        for the distribution are retrieved from the section titled\n        \"[`section`-`variable_args`]\" in the config file.\n\n        The file to construct the distribution from must be provided by setting\n        `filename`. Boundary arguments can be provided in the same way as\n        described in `get_param_bounds_from_config`.\n\n        .. code-block:: ini\n\n            [{section}-{tag}]\n            name = fromfile\n            filename = ra_prior.hdf\n            min-ra = 0\n            max-ra = 6.28\n\n        Parameters\n        ----------\n        cp : pycbc.workflow.WorkflowConfigParser\n            A parsed configuration file that contains the distribution\n            options.\n        section : str\n            Name of the section in the configuration file.\n        variable_args : str\n            The names of the parameters for this distribution, separated by\n            `prior.VARARGS_DELIM`. These must appear in the \"tag\" part\n            of the section header.\n\n        Returns\n        -------\n        BoundedDist\n            A distribution instance from the pycbc.inference.prior module.\n        \"\"\"\n        return bounded.bounded_from_config(cls, cp, section, variable_args,\n                                           bounds_required=False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef block(seed):\n    num = SAMPLE_RATE * BLOCK_SIZE\n    rng = RandomState(seed % 2**32)\n    variance = SAMPLE_RATE / 2\n    return rng.normal(size=num, scale=variance**0.5)", "response": "Returns a block of normal random numbers"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef normal(start, end, seed=0):\n    # This is reproduceable because we used fixed seeds from known values\n    s = int(start / BLOCK_SIZE)\n    e = int(end / BLOCK_SIZE)\n\n    # The data evenly divides so the last block would be superfluous\n    if end % BLOCK_SIZE == 0:\n        e -= 1\n\n    sv = RandomState(seed).randint(-2**50, 2**50)\n    data = numpy.concatenate([block(i + sv) for i in numpy.arange(s, e + 1, 1)])\n    ts = TimeSeries(data, delta_t=1.0 / SAMPLE_RATE, epoch=start)\n    return ts.time_slice(start, end)", "response": "Generate a data with a white Gaussian distribution"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef colored_noise(psd, start_time, end_time, seed=0, low_frequency_cutoff=1.0):\n    psd = psd.copy()\n\n    flen = int(SAMPLE_RATE / psd.delta_f) / 2 + 1\n    oldlen = len(psd)\n    psd.resize(flen)\n\n    # Want to avoid zeroes in PSD.\n    max_val = psd.max()\n    for i in xrange(len(psd)):\n        if i >= (oldlen-1):\n            psd.data[i] = psd[oldlen - 2]\n        if psd[i] == 0:\n            psd.data[i] = max_val\n\n    wn_dur = int(end_time - start_time) + 2*FILTER_LENGTH\n    if psd.delta_f >= 1. / (2.*FILTER_LENGTH):\n        # If the PSD is short enough, this method is less memory intensive than\n        # resizing and then calling inverse_spectrum_truncation\n        psd = pycbc.psd.interpolate(psd, 1.0 / (2.*FILTER_LENGTH))\n        # inverse_spectrum_truncation truncates the inverted PSD. To truncate\n        # the non-inverted PSD we give it the inverted PSD to truncate and then\n        # invert the output.\n        psd = 1. / pycbc.psd.inverse_spectrum_truncation(1./psd,\n                                FILTER_LENGTH * SAMPLE_RATE,\n                                low_frequency_cutoff=low_frequency_cutoff,\n                                trunc_method='hann')\n        psd = psd.astype(complex_same_precision_as(psd))\n        # Zero-pad the time-domain PSD to desired length. Zeroes must be added\n        # in the middle, so some rolling between a resize is used.\n        psd = psd.to_timeseries()\n        psd.roll(SAMPLE_RATE * FILTER_LENGTH)\n        psd.resize(wn_dur * SAMPLE_RATE)\n        psd.roll(-SAMPLE_RATE * FILTER_LENGTH)\n        # As time series is still mirrored the complex frequency components are\n        # 0. But convert to real by using abs as in inverse_spectrum_truncate\n        psd = psd.to_frequencyseries()\n    else:\n        psd = pycbc.psd.interpolate(psd, 1.0 / wn_dur)\n        psd = 1. / pycbc.psd.inverse_spectrum_truncation(1./psd,\n                                FILTER_LENGTH * SAMPLE_RATE,\n                                low_frequency_cutoff=low_frequency_cutoff,\n                                trunc_method='hann')\n\n    kmin = int(low_frequency_cutoff / psd.delta_f)\n    psd[:kmin].clear()\n    asd = (psd.real())**0.5\n    del psd\n\n    white_noise = normal(start_time - FILTER_LENGTH, end_time + FILTER_LENGTH,\n                         seed=seed)\n    white_noise = white_noise.to_frequencyseries()\n    # Here we color. Do not want to duplicate memory here though so use '*='\n    white_noise *= asd\n    del asd\n    colored = white_noise.to_timeseries()\n    del white_noise\n    return colored.time_slice(start_time, end_time)", "response": "Create a new N - tuple with the noise colored by the given PSD."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef noise_from_string(psd_name, start_time, end_time, seed=0, low_frequency_cutoff=1.0):\n    delta_f = 1.0 / FILTER_LENGTH\n    flen = int(SAMPLE_RATE / delta_f) / 2 + 1\n    psd = pycbc.psd.from_string(psd_name, flen, delta_f, low_frequency_cutoff)\n    return colored_noise(psd, start_time, end_time,\n                         seed=seed,\n                         low_frequency_cutoff=low_frequency_cutoff)", "response": "Create a new Nagios - compatible TimeSeries containing a gaussian noise from a given PSD."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_sampler_metadata(self, sampler):\n        super(EmceePTFile, self).write_sampler_metadata(sampler)\n        self[self.sampler_group].attrs[\"betas\"] = sampler.betas", "response": "Adds betas to MultiTemperedMCMCIO. betas attribute"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_acceptance_fraction(self, temps=None, walkers=None):\n        group = self.sampler_group + '/acceptance_fraction'\n        if walkers is None:\n            wmask = numpy.ones(self.nwalkers, dtype=bool)\n        else:\n            wmask = numpy.zeros(self.nwalkers, dtype=bool)\n            wmask[walkers] = True\n        if temps is None:\n            tmask = numpy.ones(self.ntemps, dtype=bool)\n        else:\n            tmask = numpy.zeros(self.ntemps, dtype=bool)\n            tmask[temps] = True\n        return self[group][:][numpy.ix_(tmask, wmask)]", "response": "Reads the acceptance fraction of a set of temperatures and walkers."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write_acceptance_fraction(self, acceptance_fraction):\n        # check\n        assert acceptance_fraction.shape == (self.ntemps, self.nwalkers), (\n            \"acceptance fraction must have shape ntemps x nwalker\")\n        group = self.sampler_group + '/acceptance_fraction'\n        try:\n            self[group][:] = acceptance_fraction\n        except KeyError:\n            # dataset doesn't exist yet, create it\n            self[group] = acceptance_fraction", "response": "Write the acceptance_fraction data to file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef findchirp_cluster_over_window(times, values, window_length):\n    assert window_length > 0, 'Clustering window length is not positive'\n\n    from weave import inline\n    indices = numpy.zeros(len(times), dtype=int)\n    tlen = len(times) # pylint:disable=unused-variable\n    k = numpy.zeros(1, dtype=int)\n    absvalues = abs(values) # pylint:disable=unused-variable\n    times = times.astype(int)\n    code = \"\"\"\n        int j = 0;\n        int curr_ind = 0;\n        for (int i=0; i < tlen; i++){\n            if ((times[i] - times[curr_ind]) > window_length){\n                j += 1;\n                indices[j] = i;\n                curr_ind = i;\n            }\n            else if (absvalues[i] > absvalues[curr_ind]){\n                indices[j] = i;\n                curr_ind = i;\n            }\n        }\n        k[0] = j;\n    \"\"\"\n    inline(code, ['times', 'absvalues', 'window_length', 'indices', 'tlen', 'k'],\n           extra_compile_args=[WEAVE_FLAGS])\n    return indices[0:k[0]+1]", "response": "This function calculates the number of events that are clustering over a window."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreduce the events by clustering over a window", "response": "def cluster_reduce(idx, snr, window_size):\n    \"\"\" Reduce the events by clustering over a window\n\n    Parameters\n    -----------\n    indices: Array\n        The list of indices of the SNR values\n    snr: Array\n        The list of SNR value\n    window_size: int\n        The size of the window in integer samples.\n\n    Returns\n    -------\n    indices: Array\n        The list of indices of the SNR values\n    snr: Array\n        The list of SNR values\n    \"\"\"\n    ind = findchirp_cluster_over_window(idx, snr, window_size)\n    return idx.take(ind), snr.take(ind)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_multi_ifo_interface(cls, opt, ifo, column, column_types, **kwds):\n        opt = copy.deepcopy(opt)\n        opt_dict = vars(opt)\n        for arg, value in opt_dict.items():\n            if isinstance(value, dict):\n                setattr(opt, arg, getattr(opt, arg)[ifo])\n        return cls(opt, column, column_types, **kwds)", "response": "Create a new object from a multi - ifo interface."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving events with newsnr smaller than given threshold", "response": "def newsnr_threshold(self, threshold):\n        \"\"\" Remove events with newsnr smaller than given threshold\n        \"\"\"\n        if not self.opt.chisq_bins:\n            raise RuntimeError('Chi-square test must be enabled in order to '\n                               'use newsnr threshold')\n\n        remove = [i for i, e in enumerate(self.events) if\n                  ranking.newsnr(abs(e['snr']), e['chisq'] / e['chisq_dof'])\n                  < threshold]\n        self.events = numpy.delete(self.events, remove)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_template_events(self, columns, vectors):\n        # initialize with zeros - since vectors can be None, look for the\n        # longest one that isn't\n        new_events = None\n        for v in vectors:\n            if v is not None:\n                new_events = numpy.zeros(len(v), dtype=self.event_dtype)\n                break\n        # they shouldn't all be None\n        assert new_events is not None\n        new_events['template_id'] = self.template_index\n        for c, v in zip(columns, vectors):\n            if v is not None:\n                if isinstance(v, Array):\n                    new_events[c] = v.numpy()\n                else:\n                    new_events[c] = v\n        self.template_events = numpy.append(self.template_events, new_events)", "response": "Add a vector indexed to the template_events attribute"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cluster_template_events(self, tcolumn, column, window_size):\n        cvec = self.template_events[column]\n        tvec = self.template_events[tcolumn]\n        if window_size == 0:\n            indices = numpy.arange(len(tvec))\n        else:\n            indices = findchirp_cluster_over_window(tvec, cvec, window_size)\n        self.template_events = numpy.take(self.template_events, indices)", "response": "Cluster the internal events over the named column"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef save_performance(self, ncores, nfilters, ntemplates, run_time,\n                         setup_time):\n        \"\"\"\n        Calls variables from pycbc_inspiral to be used in a timing calculation\n        \"\"\"\n        self.run_time = run_time\n        self.setup_time = setup_time\n        self.ncores = ncores\n        self.nfilters = nfilters\n        self.ntemplates = ntemplates\n        self.write_performance = True", "response": "Save the performance of the current process to disk."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting the found events to an inspiral table.", "response": "def write_events(self, outname):\n        \"\"\" Write the found events to a sngl inspiral table\n        \"\"\"\n        self.make_output_dir(outname)\n\n        if '.hdf' in outname:\n            self.write_to_hdf(outname)\n        else:\n            raise ValueError('Cannot write to this format')"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a vector indexed template events to an IFO", "response": "def add_template_events_to_ifo(self, ifo, columns, vectors):\n        \"\"\" Add a vector indexed \"\"\"\n        # Just call through to the standard function\n        self.template_events = self.template_event_dict[ifo]\n        self.add_template_events(columns, vectors)\n        self.template_event_dict[ifo] = self.template_events\n        self.template_events = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a vector indexed to the template_events array", "response": "def add_template_network_events(self, columns, vectors):\n        \"\"\" Add a vector indexed \"\"\"\n        # initialize with zeros - since vectors can be None, look for the\n        # longest one that isn't\n        new_events = None\n        new_events = numpy.zeros(\n            max([len(v) for v in vectors if v is not None]),\n            dtype=self.network_event_dtype\n        )\n        # they shouldn't all be None\n        assert new_events is not None\n        new_events['template_id'] = self.template_index\n        for c, v in zip(columns, vectors):\n            if v is not None:\n                if isinstance(v, Array):\n                    new_events[c] = v.numpy()\n                else:\n                    new_events[c] = v\n        self.template_events = numpy.append(self.template_events, new_events)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_template_events_to_network(self, columns, vectors):\n        # Just call through to the standard function\n        self.template_events = self.template_event_dict['network']\n        self.add_template_network_events(columns, vectors)\n        self.template_event_dict['network'] = self.template_events\n        self.template_events = None", "response": "Add a vector indexed template events to the network"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cluster_template_events_single_ifo(\n            self, tcolumn, column, window_size, ifo):\n        \"\"\" Cluster the internal events over the named column\n        \"\"\"\n        # Just call through to the standard function\n        self.template_events = self.template_event_dict[ifo]\n        self.cluster_template_events(tcolumn, column, window_size)\n        self.template_event_dict[ifo] = self.template_events\n        self.template_events = None", "response": "Cluster the internal events over the named column"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef nltides_fourier_phase_difference(f, delta_f, f0, amplitude, n, m1, m2):\n\n    kmin = int(f0/delta_f)\n    kmax = len(f)\n\n    f_ref, t_of_f_factor, phi_of_f_factor = \\\n        pycbc.conversions.nltides_coefs(amplitude, n, m1, m2)\n\n    # Fourier phase shift below f0 from \\Delta \\phi(f)\n    delta_psi_f_le_f0 = numpy.ones(kmin)\n    delta_psi_f_le_f0 *= - phi_of_f_factor * (f0/f_ref)**(n-3.)\n\n    # Fourier phase shift above f0 from \\Delta \\phi(f)\n    delta_psi_f_gt_f0 = - phi_of_f_factor * (f[kmin:kmax]/f_ref)**(n-3.)\n\n    # Fourier phase shift below f0 from 2 pi f \\Delta t(f)\n    delta_psi_f_le_f0 += 2.0 * lal.lal.PI * f[0:kmin] * t_of_f_factor * \\\n        (f0/f_ref)**(n-4.)\n\n    # Fourier phase shift above f0 from 2 pi f \\Delta t(f)\n    delta_psi_f_gt_f0 += 2.0 * lal.lal.PI * f[kmin:kmax] * t_of_f_factor * \\\n        (f[kmin:kmax]/f_ref)**(n-4.)\n\n    # Return the shift to the Fourier phase\n    return numpy.concatenate((delta_psi_f_le_f0, delta_psi_f_gt_f0), axis=0)", "response": "Calculate the change to the Fourier phase change due\n    to non - linear tides switch on."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef nonlinear_tidal_spa(**kwds):\n\n    from pycbc import waveform\n    from pycbc.types import Array\n\n    # We start with the standard TaylorF2 based waveform\n    kwds.pop('approximant')\n    hp, hc = waveform.get_fd_waveform(approximant=\"TaylorF2\", **kwds)\n\n    # Add the phasing difference from the nonlinear tides\n    f = numpy.arange(len(hp)) * hp.delta_f\n    pd =  Array(numpy.exp(-1.0j * nltides_fourier_phase_difference(f,\n               hp.delta_f,\n               kwds['f0'], kwds['amplitude'], kwds['n'],\n               kwds['mass1'], kwds['mass2'])),\n               dtype=hp.dtype)\n    hp *= pd\n    hc *= pd\n    return hp, hc", "response": "Generates a frequency - domain waveform that implements the\n    TaylorF2 + NL tide model described in https://arxiv. org."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload the sampler from the given config file.", "response": "def from_config(cls, cp, model, nprocesses=1, use_mpi=False):\n        \"\"\"\n        Loads the sampler from the given config file.\n\n        For generating the temperature ladder to be used by emcee_pt, either\n        the number of temperatures (provided by the option 'ntemps'),\n        or the path to a file storing inverse temperature values (provided\n        under a subsection inverse-temperatures-file) can be loaded from the\n        config file. If the latter, the file should be of hdf format, having\n        an attribute named 'betas' storing the list of inverse temperature\n        values to be provided to emcee_pt. If the former, emcee_pt will\n        construct the ladder with \"ntemps\" geometrically spaced temperatures.\n        \"\"\"\n        section = \"sampler\"\n        # check name\n        assert cp.get(section, \"name\") == cls.name, (\n            \"name in section [sampler] must match mine\")\n        # get the number of walkers to use\n        nwalkers = int(cp.get(section, \"nwalkers\"))\n        if cp.has_option(section, \"ntemps\") and \\\n                cp.has_option(section, \"inverse-temperatures-file\"):\n            raise ValueError(\"Must specify either ntemps or \"\n                             \"inverse-temperatures-file, not both.\")\n        if cp.has_option(section, \"inverse-temperatures-file\"):\n            # get the path of the file containing inverse temperatures values.\n            inverse_temperatures_file = cp.get(section,\n                                               \"inverse-temperatures-file\")\n            with h5py.File(inverse_temperatures_file, \"r\") as fp:\n                try:\n                    betas = numpy.array(fp.attrs['betas'])\n                    ntemps = betas.shape[0]\n                except KeyError:\n                    raise AttributeError(\"No attribute called betas\")\n        else:\n            # get the number of temperatures\n            betas = None\n            ntemps = int(cp.get(section, \"ntemps\"))\n        # get the checkpoint interval, if it's specified\n        checkpoint_interval = cls.checkpoint_from_config(cp, section)\n        checkpoint_signal = cls.ckpt_signal_from_config(cp, section)\n        # get the loglikelihood function\n        logl = get_optional_arg_from_config(cp, section, 'logl-function')\n        obj = cls(model, ntemps, nwalkers, betas=betas,\n                  checkpoint_interval=checkpoint_interval,\n                  checkpoint_signal=checkpoint_signal,\n                  loglikelihood_function=logl, nprocesses=nprocesses,\n                  use_mpi=use_mpi)\n        # set target\n        obj.set_target_from_config(cp, section)\n        # add burn-in if it's specified\n        obj.set_burn_in_from_config(cp)\n        # set prethin options\n        obj.set_thin_interval_from_config(cp, section)\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the log likelihood ratio and log prior as a dict of arrays.", "response": "def model_stats(self):\n        \"\"\"Returns the log likelihood ratio and log prior as a dict of arrays.\n\n        The returned array has shape ntemps x nwalkers x niterations.\n\n        Unfortunately, because ``emcee_pt`` does not have blob support, this\n        will only return the loglikelihood and logprior (with the logjacobian\n        set to zero) regardless of what stats the model can return.\n\n\n        .. warning::\n            Since the `logjacobian` is not saved by `emcee_pt`, the `logprior`\n            returned here is the log of the prior pdf in the sampling\n            coordinate frame rather than the variable params frame. This\n            differs from the variable params frame by the log of the Jacobian\n            of the transform from one frame to the other. If no sampling\n            transforms were used, then the `logprior` is the same.\n        \"\"\"\n        # likelihood has shape ntemps x nwalkers x niterations\n        logl = self._sampler.lnlikelihood\n        # get prior from posterior\n        logp = self._sampler.lnprobability - logl\n        logjacobian = numpy.zeros(logp.shape)\n        return {'loglikelihood': logl, 'logprior': logp,\n                'logjacobian': logjacobian}"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nclear the chain and blobs from memory.", "response": "def clear_samples(self):\n        \"\"\"Clears the chain and blobs from memory.\n        \"\"\"\n        # store the iteration that the clear is occuring on\n        self._lastclear = self.niterations\n        self._itercounter = 0\n        # now clear the chain\n        self._sampler.reset()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadvance the ensemble for a number of samples.", "response": "def run_mcmc(self, niterations):\n        \"\"\"Advance the ensemble for a number of samples.\n\n        Parameters\n        ----------\n        niterations : int\n            Number of samples to get from sampler.\n        \"\"\"\n        pos = self._pos\n        if pos is None:\n            pos = self._p0\n        res = self._sampler.run_mcmc(pos, niterations)\n        p, _, _ = res[0], res[1], res[2]\n        # update the positions\n        self._pos = p"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef calculate_logevidence(cls, filename, thin_start=None, thin_end=None,\n                              thin_interval=None):\n        \"\"\"Calculates the log evidence from the given file using ``emcee_pt``'s\n        thermodynamic integration.\n\n        Parameters\n        ----------\n        filename : str\n            Name of the file to read the samples from. Should be an\n            ``EmceePTFile``.\n        thin_start : int\n            Index of the sample to begin returning stats. Default is to read\n            stats after burn in. To start from the beginning set thin_start\n            to 0.\n        thin_interval : int\n            Interval to accept every i-th sample. Default is to use the\n            `fp.acl`. If `fp.acl` is not set, then use all stats\n            (set thin_interval to 1).\n        thin_end : int\n            Index of the last sample to read. If not given then\n            `fp.niterations` is used.\n\n        Returns\n        -------\n        lnZ : float\n            The estimate of log of the evidence.\n        dlnZ : float\n            The error on the estimate.\n        \"\"\"\n        with cls._io(filename, 'r') as fp:\n            logls = fp.read_raw_samples(['loglikelihood'],\n                                        thin_start=thin_start,\n                                        thin_interval=thin_interval,\n                                        thin_end=thin_end,\n                                        temps='all', flatten=False)\n            logls = logls['loglikelihood']\n            # we need the betas that were used\n            betas = fp.betas\n            # annoyingly, theromdynaimc integration in PTSampler is an instance\n            # method, so we'll implement a dummy one\n            ntemps = fp.ntemps\n            nwalkers = fp.nwalkers\n            ndim = len(fp.variable_params)\n        dummy_sampler = emcee.PTSampler(ntemps, nwalkers, ndim, None,\n                                        None, betas=betas)\n        return dummy_sampler.thermodynamic_integration_log_evidence(\n            logls=logls, fburnin=0.)", "response": "Calculates the log evidence from the given file using the specified thin start and thin end intervals."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef finalize(self):\n        logging.info(\"Calculating log evidence\")\n        # get the thinning settings\n        with self.io(self.checkpoint_file, 'r') as fp:\n            thin_start = fp.thin_start\n            thin_interval = fp.thin_interval\n            thin_end = fp.thin_end\n        # calculate\n        logz, dlogz = self.calculate_logevidence(\n            self.checkpoint_file, thin_start=thin_start, thin_end=thin_end,\n            thin_interval=thin_interval)\n        logging.info(\"log Z, dlog Z: {}, {}\".format(logz, dlogz))\n        # write to both the checkpoint and backup\n        for fn in [self.checkpoint_file, self.backup_file]:\n            with self.io(fn, \"a\") as fp:\n                fp.write_logevidence(logz, dlogz)", "response": "Finalizes the log evidence calculation and writes to the checkpoint file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the information about the public data files in a duration of time", "response": "def losc_frame_json(ifo, start_time, end_time):\n    \"\"\" Get the information about the public data files in a duration of time\n\n    Parameters\n    ----------\n    ifo: str\n        The name of the IFO to find the information about.\n    start_time: int\n        The gps time in GPS seconds\n    end_time: int\n        The end time in GPS seconds\n\n    Returns\n    -------\n    info: dict\n        A dictionary containing information about the files that span the\n        requested times.\n    \"\"\"\n    import urllib, json\n    run = get_run(start_time)\n    run2 = get_run(end_time)\n    if run != run2:\n        raise ValueError('Spanning multiple runs is not currently supported.'\n                         'You have requested data that uses '\n                         'both %s and %s' % (run, run2))\n\n    url = _losc_url % (run, ifo, int(start_time), int(end_time))\n\n    try:\n        return json.loads(urllib.urlopen(url).read())\n    except Exception as e:\n        print(e)\n        raise ValueError('Failed to find gwf files for '\n            'ifo=%s, run=%s, between %s-%s' % (ifo, run, start_time, end_time))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef losc_frame_urls(ifo, start_time, end_time):\n    data = losc_frame_json(ifo, start_time, end_time)['strain']\n    return [d['url'] for d in data if d['format'] == 'gwf']", "response": "Get a list of urls to losc frame files that span the requested times."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_frame_losc(channels, start_time, end_time):\n    from pycbc.frame import read_frame\n    if not isinstance(channels, list):\n        channels = [channels]\n    ifos = [c[0:2] for c in channels]\n    urls = {}\n    for ifo in ifos:\n        urls[ifo] = losc_frame_urls(ifo, start_time, end_time)\n        if len(urls[ifo]) == 0:\n            raise ValueError(\"No data found for %s so we \"\n                             \"can't produce a time series\" % ifo)\n\n    fnames = {ifo:[] for ifo in ifos}\n    for ifo in ifos:\n        for url in urls[ifo]:\n            fname = download_file(url, cache=True)\n            fnames[ifo].append(fname)\n\n    ts = [read_frame(fnames[channel[0:2]], channel,\n           start_time=start_time, end_time=end_time) for channel in channels]\n    if len(ts) == 1:\n        return ts[0]\n    else:\n        return ts", "response": "Read channels from losc data and return a TimeSeries with the requested data."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread the data from the LOSC data for a given strain.", "response": "def read_strain_losc(ifo, start_time, end_time):\n    \"\"\" Get the strain data from the LOSC data\n\n    Parameters\n    ----------\n    ifo: str\n        The name of the IFO to read data for. Ex. 'H1', 'L1', 'V1'\n    start_time: int\n        The gps time in GPS seconds\n    end_time: int\n        The end time in GPS seconds\n\n    Returns\n    -------\n    ts: TimeSeries\n        Returns a timeseries with the strain data.\n    \"\"\"\n    channel = _get_channel(start_time)\n    return read_frame_losc('%s:%s' % (ifo, channel), start_time, end_time)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the ids of each background bin in the list of strings.", "response": "def background_bin_from_string(background_bins, data):\n    \"\"\" Return template ids for each bin as defined by the format string\n\n    Parameters\n    ----------\n    bins: list of strings\n        List of strings which define how a background bin is taken from the\n        list of templates.\n    data: dict of numpy.ndarrays\n        Dict with parameter key values and numpy.ndarray values which define\n        the parameters of the template bank to bin up.\n\n    Returns\n    -------\n    bins: dict\n        Dictionary of location indices indexed by a bin name\n    \"\"\"\n    used = numpy.array([], dtype=numpy.uint32)\n    bins = {}\n    for mbin in background_bins:\n        name, bin_type, boundary = tuple(mbin.split(':'))\n\n        if boundary[0:2] == 'lt':\n            member_func = lambda vals, bd=boundary : vals < float(bd[2:])\n        elif boundary[0:2] == 'gt':\n            member_func = lambda vals, bd=boundary : vals > float(bd[2:])\n        else:\n            raise RuntimeError(\"Can't parse boundary condition! Must begin \"\n                               \"with 'lt' or 'gt'\")\n\n        if bin_type == 'component' and boundary[0:2] == 'lt':\n            # maximum component mass is less than boundary value\n            vals = numpy.maximum(data['mass1'], data['mass2'])\n        if bin_type == 'component' and boundary[0:2] == 'gt':\n            # minimum component mass is greater than bdary\n            vals = numpy.minimum(data['mass1'], data['mass2'])\n        elif bin_type == 'total':\n            vals = data['mass1'] + data['mass2']\n        elif bin_type == 'chirp':\n            vals = pycbc.pnutils.mass1_mass2_to_mchirp_eta(\n                                               data['mass1'], data['mass2'])[0]\n        elif bin_type == 'SEOBNRv2Peak':\n            vals = pycbc.pnutils.get_freq('fSEOBNRv2Peak',\n                  data['mass1'], data['mass2'], data['spin1z'], data['spin2z'])\n        elif bin_type == 'SEOBNRv4Peak':\n            vals = pycbc.pnutils.get_freq('fSEOBNRv4Peak', data['mass1'],\n                                          data['mass2'], data['spin1z'],\n                                          data['spin2z'])\n        elif bin_type == 'SEOBNRv2duration':\n            vals = pycbc.pnutils.get_imr_duration(data['mass1'], data['mass2'],\n                               data['spin1z'], data['spin2z'], data['f_lower'],\n                                                        approximant='SEOBNRv2')\n        else:\n            raise ValueError('Invalid bin type %s' % bin_type)\n\n        locs = member_func(vals)\n        del vals\n\n        # make sure we don't reuse anything from an earlier bin\n        locs = numpy.where(locs)[0]\n        locs = numpy.delete(locs, numpy.where(numpy.in1d(locs, used))[0])\n        used = numpy.concatenate([used, locs])\n        bins[name] = locs\n\n    return bins"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef calculate_n_louder(bstat, fstat, dec, skip_background=False):\n    sort = bstat.argsort()\n    bstat = bstat[sort]\n    dec = dec[sort]\n\n    # calculate cumulative number of triggers louder than the trigger in\n    # a given index. We need to subtract the decimation factor, as the cumsum\n    # includes itself in the first sum (it is inclusive of the first value)\n    n_louder = dec[::-1].cumsum()[::-1] - dec\n\n    # Determine how many values are louder than the foreground ones\n    # We need to subtract one from the index, to be consistent with the definition\n    # of n_louder, as here we do want to include the background value at the\n    # found index\n    idx = numpy.searchsorted(bstat, fstat, side='left') - 1\n\n    # If the foreground are *quieter* than the background or at the same value\n    # then the search sorted alorithm will choose position -1, which does not exist\n    # We force it back to zero.\n    if isinstance(idx, numpy.ndarray): # Handle the case where our input is an array\n        idx[idx < 0] = 0\n    else: # Handle the case where we are simply given a scalar value\n        if idx < 0:\n            idx = 0\n\n    fore_n_louder = n_louder[idx]\n\n    if not skip_background:\n        unsort = sort.argsort()\n        back_cum_num = n_louder[unsort]\n        return back_cum_num, fore_n_louder\n    else:\n        return fore_n_louder", "response": "Calculates for each foreground event the number of background events that are louder than it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the coincident time for each timeslide in the given timeslide_offsets vector.", "response": "def timeslide_durations(start1, start2, end1, end2, timeslide_offsets):\n    \"\"\" Find the coincident time for each timeslide.\n\n    Find the coincident time for each timeslide, where the first time vector\n    is slid to the right by the offset in the given timeslide_offsets vector.\n\n    Parameters\n    ----------\n    start1: numpy.ndarray\n        Array of the start of valid analyzed times for detector 1\n    start2: numpy.ndarray\n        Array of the start of valid analyzed times for detector 2\n    end1: numpy.ndarray\n        Array of the end of valid analyzed times for detector 1\n    end2: numpy.ndarray\n        Array of the end of valid analyzed times for detector 2\n    timseslide_offset: numpy.ndarray\n        Array of offsets (in seconds) for each timeslide\n\n    Returns\n    --------\n    durations: numpy.ndarray\n        Array of coincident time for each timeslide in the offset array\n    \"\"\"\n    from . import veto\n    durations = []\n    seg2 = veto.start_end_to_segments(start2, end2)\n    for offset in timeslide_offsets:\n        seg1 = veto.start_end_to_segments(start1 + offset, end1 + offset)\n        durations.append(abs((seg1 & seg2).coalesce()))\n    return numpy.array(durations)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfind coincidences by time window t1 t2 and slide_step", "response": "def time_coincidence(t1, t2, window, slide_step=0):\n    \"\"\" Find coincidences by time window\n\n    Parameters\n    ----------\n    t1 : numpy.ndarray\n        Array of trigger times from the first detector\n    t2 : numpy.ndarray\n        Array of trigger times from the second detector\n    window : float\n        The coincidence window in seconds\n    slide_step : optional, {None, float}\n        If calculating background coincidences, the interval between background\n        slides in seconds.\n\n    Returns\n    -------\n    idx1 : numpy.ndarray\n        Array of indices into the t1 array.\n    idx2 : numpy.ndarray\n        Array of indices into the t2 array.\n    slide : numpy.ndarray\n        Array of slide ids\n    \"\"\"\n    if slide_step:\n        fold1 = t1 % slide_step\n        fold2 = t2 % slide_step\n    else:\n        fold1 = t1\n        fold2 = t2\n\n    sort1 = fold1.argsort()\n    sort2 = fold2.argsort()\n    fold1 = fold1[sort1]\n    fold2 = fold2[sort2]\n\n    if slide_step:\n        fold2 = numpy.concatenate([fold2 - slide_step, fold2, fold2 + slide_step])\n        sort2 = numpy.concatenate([sort2, sort2, sort2])\n\n    left = numpy.searchsorted(fold2, fold1 - window)\n    right = numpy.searchsorted(fold2, fold1 + window)\n\n    idx1 = numpy.repeat(sort1, right-left)\n    idx2 = [sort2[l:r] for l,r in zip(left, right)]\n\n    if len(idx2) > 0:\n        idx2 = numpy.concatenate(idx2)\n    else:\n        idx2 = numpy.array([], dtype=numpy.int64)\n\n    if slide_step:\n        diff = ((t1 / slide_step)[idx1] - (t2 / slide_step)[idx2])\n        slide = numpy.rint(diff)\n    else:\n        slide = numpy.zeros(len(idx1))\n\n    return idx1.astype(numpy.uint32), idx2.astype(numpy.uint32), slide.astype(numpy.int32)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds multi detector concidences for a given set of time - sliding times.", "response": "def time_multi_coincidence(times, slide_step=0, slop=.003,\n                           pivot='H1', fixed='L1'):\n    \"\"\" Find multi detector concidences.\n\n    Parameters\n    ----------\n    times: dict of numpy.ndarrays\n        Dictionary keyed by ifo of the times of each single detector trigger.\n    slide_step: float\n        The interval between time slides\n    slop: float\n        The amount of time to add to the TOF between detectors for coincidence\n    pivot: str\n        ifo used to test coincidence against in first stage\n    fixed: str\n        the other ifo used in the first stage coincidence which we'll use\n        as a fixed time reference for coincident triggers. All other detectors\n        are time slid by being fixed to this detector.\n    \"\"\"\n    # pivots are used to determine standard coincidence triggers, we then\n    # pair off additional detectors to those.\n    def win(ifo1, ifo2):\n        d1 = Detector(ifo1)\n        d2 = Detector(ifo2)\n        return d1.light_travel_time_to_detector(d2) + slop\n\n    # Find coincs first between the two fully time-slid detectors\n    pivot_id, fix_id, slide = time_coincidence(times[pivot], times[fixed],\n                                               win(pivot, fixed),\n                                               slide_step=slide_step)\n\n    # additional detectors do not slide independently of the fixed one\n    # Each trigger in an additional detector must be concident with an\n    # existing coincident one. All times moved to 'fixed' relative time\n    fixed_time = times[fixed][fix_id]\n    pivot_time = times[pivot][pivot_id] - slide_step * slide\n\n    ctimes = {fixed: fixed_time, pivot:pivot_time}\n    ids = {fixed:fix_id, pivot:pivot_id}\n\n    dep_ifos = [ifo for ifo in times.keys() if ifo != fixed and ifo != pivot]\n    for ifo1 in dep_ifos:\n        otime = times[ifo1]\n        sort = times[ifo1].argsort()\n        time = otime[sort]\n\n        # Find coincidences between dependent ifo triggers and existing coinc.\n        for ifo2 in ids.keys():\n            # Currently assumes that additional detectors do not slide\n            # independently of the 'fixed one'\n            #\n            # To modify that assumption, the code here would be modified\n            # by adding a function that remaps the coinc time frame and unmaps\n            # it and the end of this loop.\n            # This remapping must ensure\n            #    * function of the standard slide number\n            #    * ensure all times remain within coincident segment\n            #    * unbiased distribution of triggers after mapping.\n\n            w = win(ifo1, ifo2)\n            left = numpy.searchsorted(time, ctimes[ifo2] - w)\n            right = numpy.searchsorted(time, ctimes[ifo2] + w)\n\n            # remove elements that will not form a coinc\n            # There is only at most one trigger for an existing coinc\n            # (assumes triggers spaced > slide step)\n            nz = (right - left).nonzero()\n            dep_ids = left[nz]\n\n            # The property that only one trigger can be within the window is ensured\n            # by the peak finding algorithm we use for each template.\n            # If that is modifed, this function may need to be\n            # extended.\n            if len(left) > 0 and (right - left).max() > 1:\n                raise ValueError('Somehow triggers are closer than time-delay window')\n\n            slide = slide[nz]\n            for ifo in ctimes:\n                ctimes[ifo] = ctimes[ifo][nz]\n                ids[ifo] = ids[ifo][nz]\n\n        # Add this detector now to the cumulative set and proceed to the next\n        # ifo coincidence test\n        ids[ifo1] = sort[dep_ids]\n        ctimes[ifo1] = otime[ids[ifo1]]\n\n    return ids, slide"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nclustering the coincidences over a given time window.", "response": "def cluster_coincs(stat, time1, time2, timeslide_id, slide, window, argmax=numpy.argmax):\n    \"\"\"Cluster coincident events for each timeslide separately, across\n    templates, based on the ranking statistic\n\n    Parameters\n    ----------\n    stat: numpy.ndarray\n        vector of ranking values to maximize\n    time1: numpy.ndarray\n        first time vector\n    time2: numpy.ndarray\n        second time vector\n    timeslide_id: numpy.ndarray\n        vector that determines the timeslide offset\n    slide: float\n        length of the timeslides offset interval\n    window: float\n        length to cluster over\n\n    Returns\n    -------\n    cindex: numpy.ndarray\n        The set of indices corresponding to the surviving coincidences.\n    \"\"\"\n\n    logging.info('clustering coinc triggers over %ss window' % window)\n\n    if len(time1) == 0 or len(time2) == 0:\n        logging.info('No coinc triggers in one, or both, ifos.')\n        return numpy.array([])\n\n    if numpy.isfinite(slide):\n        # for a time shifted coinc, time1 is greater than time2 by approximately timeslide_id*slide\n        # adding this quantity gives a mean coinc time located around time1\n        time = (time1 + time2 + timeslide_id * slide) / 2\n    else:\n        time = 0.5 * (time2 + time1)\n\n    tslide = timeslide_id.astype(numpy.float128)\n    time = time.astype(numpy.float128)\n\n    span = (time.max() - time.min()) + window * 10\n    time = time + span * tslide\n    cidx = cluster_over_time(stat, time, window, argmax)\n    return cidx"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cluster_coincs_multiifo(stat, time_coincs, timeslide_id, slide, window, argmax=numpy.argmax):\n    time_coinc_zip = zip(*time_coincs)\n    if len(time_coinc_zip) == 0:\n        logging.info('No coincident triggers.')\n        return numpy.array([])\n\n    time_avg_num = []\n    #find number of ifos and mean time over participating ifos for each coinc\n    for tc in time_coinc_zip:\n        time_avg_num.append(mean_if_greater_than_zero(tc))\n\n    time_avg, num_ifos = zip(*time_avg_num)\n\n    time_avg = numpy.array(time_avg)\n    num_ifos = numpy.array(num_ifos)\n\n    # shift all but the pivot ifo by (num_ifos-1) * timeslide_id * slide\n    # this leads to a mean coinc time located around pivot time\n    if numpy.isfinite(slide):\n        nifos_minusone = (num_ifos - numpy.ones_like(num_ifos))\n        time_avg = time_avg + (nifos_minusone * timeslide_id * slide)/num_ifos\n\n    tslide = timeslide_id.astype(numpy.float128)\n    time_avg = time_avg.astype(numpy.float128)\n\n    span = (time_avg.max() - time_avg.min()) + window * 10\n    time_avg = time_avg + span * tslide\n    cidx = cluster_over_time(stat, time_avg, window, argmax)\n\n    return cidx", "response": "Cluster the coincidences for each timeslide separately across the multiifo coincidences."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the mean over numerical values ignoring values less than zero.", "response": "def mean_if_greater_than_zero(vals):\n    \"\"\" Calculate mean over numerical values, ignoring values less than zero.\n    E.g. used for mean time over coincident triggers when timestamps are set\n    to -1 for ifos not included in the coincidence.\n\n    Parameters\n    ----------\n    vals: iterator of numerical values\n        values to be mean averaged\n\n    Returns\n    -------\n    mean: float\n        The mean of the values in the original vector which are\n        greater than zero\n    num_above_zero: int\n        The number of entries in the vector which are above zero\n    \"\"\"\n    vals = numpy.array(vals)\n    above_zero = vals > 0\n    return vals[above_zero].mean(), above_zero.sum()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cluster_over_time(stat, time, window, argmax=numpy.argmax):\n    logging.info('Clustering events over %s s window', window)\n\n    indices = []\n    time_sorting = time.argsort()\n    stat = stat[time_sorting]\n    time = time[time_sorting]\n\n    left = numpy.searchsorted(time, time - window)\n    right = numpy.searchsorted(time, time + window)\n    indices = numpy.zeros(len(left), dtype=numpy.uint32)\n\n    # i is the index we are inspecting, j is the next one to save\n    i = 0\n    j = 0\n    while i < len(left):\n        l = left[i]\n        r = right[i]\n\n        # If there are no other points to compare it is obviously the max\n        if (r - l) == 1:\n            indices[j] = i\n            j += 1\n            i += 1\n            continue\n\n        # Find the location of the maximum within the time interval around i\n        max_loc = argmax(stat[l:r]) + l\n\n        # If this point is the max, we can skip to the right boundary\n        if max_loc == i:\n            indices[j] = i\n            i = r\n            j += 1\n\n        # If the max is later than i, we can skip to it\n        elif max_loc > i:\n            i = max_loc\n\n        elif max_loc < i:\n            i += 1\n\n    indices = indices[:j]\n\n    logging.info('%d triggers remaining', len(indices))\n    return time_sorting[indices]", "response": "Cluster generalized transient events over time via maximum stat over a symmetric sliding window."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef discard_last(self, indices):\n        for i in indices:\n            self.buffer_expire[i] = self.buffer_expire[i][:-1]\n            self.buffer[i] = self.buffer[i][:-1]", "response": "Discard the triggers added in the latest update"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding triggers in values to the buffers indicated by indices.", "response": "def add(self, indices, values):\n        \"\"\"Add triggers in 'values' to the buffers indicated by the indices\n        \"\"\"\n        for i, v in zip(indices, values):\n            self.buffer[i] = numpy.append(self.buffer[i], v)\n            self.buffer_expire[i] = numpy.append(self.buffer_expire[i], self.time)\n        self.advance_time()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the data vector for a given ring buffer", "response": "def data(self, buffer_index):\n        \"\"\"Return the data vector for a given ring buffer\"\"\"\n        # Check for expired elements and discard if they exist\n        expired = self.time - self.max_time     \n        exp = self.buffer_expire[buffer_index]\n        j = 0\n        while j < len(exp):\n            # Everything before this j must be expired\n            if exp[j] >= expired:\n                self.buffer_expire[buffer_index] = exp[j:].copy()\n                self.buffer[buffer_index] = self.buffer[buffer_index][j:].copy()\n                break\n            j += 1\n       \n        return self.buffer[buffer_index]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add(self, values, times, ifos):\n\n        for ifo in ifos:\n            self.time[ifo] += 1\n\n        # Resize the internal buffer if we need more space\n        if self.index + len(values) >= len(self.buffer):\n            newlen = len(self.buffer) * 2\n            for ifo in self.ifos:\n                self.timer[ifo].resize(newlen)\n            self.buffer.resize(newlen)\n\n        self.buffer[self.index:self.index+len(values)] = values\n        if len(values) > 0:\n            for ifo in self.ifos:\n                self.timer[ifo][self.index:self.index+len(values)] = times[ifo]\n\n            self.index += len(values)\n\n        # Remove the expired old elements\n        keep = None\n        for ifo in ifos:\n            kt = self.timer[ifo][:self.index] >= self.time[ifo] - self.expiration\n            keep = numpy.logical_and(keep, kt) if keep is not None else kt\n\n        self.buffer[:keep.sum()] = self.buffer[:self.index][keep]\n        for ifo in self.ifos:\n            self.timer[ifo][:keep.sum()] = self.timer[ifo][:self.index][keep]\n        self.index = keep.sum()", "response": "Add values to the internal buffer."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pick_best_coinc(cls, coinc_results):\n        mstat = 0\n        mifar = 0\n        mresult = None\n\n        # record the trials factor from the possible coincs we could\n        # maximize over\n        trials = 0\n        for result in coinc_results:\n            # Check that a coinc was possible. See the 'add_singles' method\n            # to see where this flag was added into the results dict\n            if 'coinc_possible' in result:\n                trials += 1\n\n                # Check that a coinc exists\n                if 'foreground/ifar' in result:\n                    ifar = result['foreground/ifar']\n                    stat = result['foreground/stat']\n                    if ifar > mifar or (ifar == mifar and stat > mstat):\n                        mifar = ifar\n                        mstat = stat\n                        mresult = result\n\n        # apply trials factor for the best coinc\n        if mresult:\n            mresult['foreground/ifar'] = mifar / float(trials)\n            logging.info('Found %s coinc with ifar %s',\n                         mresult['foreground/type'],\n                         mresult['foreground/ifar'])\n            return mresult\n        # If no coinc, just return one of the results dictionaries. They will\n        # all contain the same results (i.e. single triggers) in this case.\n        else:\n            return coinc_results[0]", "response": "This function picks the best two -ifo coinc from the list of coinc_results dictionary by ifar then statistic if needed."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef background_time(self):\n        time = 1.0 / self.timeslide_interval\n        for ifo in self.singles:\n            time *= self.singles[ifo].filled_time * self.analysis_block\n        return time", "response": "Return the amount of background time that the buffers contain"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the far that would be associated with the coincident given.", "response": "def ifar(self, coinc_stat):\n        \"\"\"Return the far that would be associated with the coincident given.\n        \"\"\"\n        n = self.coincs.num_greater(coinc_stat)\n        return self.background_time / lal.YRJUL_SI / (n + 1)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_singles_buffer(self, results):\n        # Determine the dtype from a sample of the data.\n        self.singles_dtype = []\n        data = False\n        for ifo in self.ifos:\n            if ifo in results and results[ifo] is not False:\n                data = results[ifo]\n                break\n\n        if data is False:\n            return\n\n        for key in data:\n            self.singles_dtype.append((key, data[key].dtype))\n\n        if 'stat' not in data:\n            self.singles_dtype.append(('stat', self.stat_calculator.single_dtype))\n\n        # Create a ring buffer for each template ifo combination\n        for ifo in self.ifos:\n            self.singles[ifo] = MultiRingBuffer(self.num_templates,\n                                            self.buffer_size,\n                                            self.singles_dtype)", "response": "Create the singles buffer for each template in the results."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _add_singles_to_buffer(self, results, ifos):\n        if len(self.singles.keys()) == 0:\n            self.set_singles_buffer(results)\n\n        # convert to single detector trigger values\n        # FIXME Currently configured to use pycbc live output\n        # where chisq is the reduced chisq and chisq_dof is the actual DOF\n        logging.info(\"adding singles to the background estimate...\")\n        updated_indices = {}\n        for ifo in ifos:\n            trigs = results[ifo]\n\n            if len(trigs['snr'] > 0):\n                trigsc = copy.copy(trigs)\n                trigsc['chisq'] = trigs['chisq'] * trigs['chisq_dof']\n                trigsc['chisq_dof'] = (trigs['chisq_dof'] + 2) / 2\n                single_stat = self.stat_calculator.single(trigsc)\n            else:\n                single_stat = numpy.array([], ndmin=1,\n                              dtype=self.stat_calculator.single_dtype)\n            trigs['stat'] = single_stat\n\n            # add each single detector trigger to the and advance the buffer\n            data = numpy.zeros(len(single_stat), dtype=self.singles_dtype)\n            for key, value in trigs.items():\n                data[key] = value\n\n            self.singles[ifo].add(trigs['template_id'], data)\n            updated_indices[ifo] = trigs['template_id']\n        return updated_indices", "response": "Add single detector triggers to the internal buffer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind all coincidences for the single detector triggers in the set of ifos.", "response": "def _find_coincs(self, results, ifos):\n        \"\"\"Look for coincs within the set of single triggers\n\n        Parameters\n        ----------\n        results: dict of arrays\n            Dictionary of dictionaries indexed by ifo and keys such as 'snr',\n            'chisq', etc. The specific format it determined by the\n            LiveBatchMatchedFilter class.\n\n        Returns\n        -------\n        coinc_results: dict of arrays\n            A dictionary of arrays containing the coincident results.\n        \"\"\"\n        # for each single detector trigger find the allowed coincidences\n        # Record which template and the index of the single trigger\n        # that forms each coincident trigger\n        cstat = [[]]\n        offsets = []\n        ctimes = {self.ifos[0]:[], self.ifos[1]:[]}\n        single_expire = {self.ifos[0]:[], self.ifos[1]:[]}\n        template_ids = [[]]\n        trigger_ids = {self.ifos[0]:[[]], self.ifos[1]:[[]]}\n\n        # Calculate all the permutations of coincident triggers for each\n        # new single detector trigger collected\n        for ifo in ifos:\n            trigs = results[ifo]\n\n            oifo = self.ifos[1] if self.ifos[0] == ifo else self.ifos[0]\n\n            for i in range(len(trigs['end_time'])):\n                trig_stat = trigs['stat'][i]\n                trig_time = trigs['end_time'][i]\n                template = trigs['template_id'][i]\n\n                times = self.singles[oifo].data(template)['end_time']\n                stats = self.singles[oifo].data(template)['stat']\n\n                i1, _, slide = time_coincidence(times,\n                                 numpy.array(trig_time, ndmin=1,\n                                 dtype=numpy.float64),\n                                 self.time_window,\n                                 self.timeslide_interval)\n                trig_stat = numpy.resize(trig_stat, len(i1))\n                c = self.stat_calculator.coinc(stats[i1], trig_stat,\n                                               slide, self.timeslide_interval)\n                offsets.append(slide)\n                cstat.append(c)\n                ctimes[oifo].append(times[i1])\n                ctimes[ifo].append(numpy.zeros(len(c), dtype=numpy.float64))\n                ctimes[ifo][-1].fill(trig_time)\n\n                single_expire[oifo].append(self.singles[oifo].expire_vector(template)[i1])\n                single_expire[ifo].append(numpy.zeros(len(c),\n                                          dtype=numpy.int32))\n                single_expire[ifo][-1].fill(self.singles[ifo].time - 1)\n\n                # save the template and trigger ids to keep association\n                # to singles. The trigger was just added so it must be in\n                # the last position we mark this with -1 so the\n                # slicing picks the right point\n                template_ids.append(numpy.zeros(len(c)) + template)\n                trigger_ids[oifo].append(i1)\n                trigger_ids[ifo].append(numpy.zeros(len(c)) - 1)\n\n        cstat = numpy.concatenate(cstat)\n        template_ids = numpy.concatenate(template_ids).astype(numpy.int32)\n        for ifo in ifos:\n            trigger_ids[ifo] = numpy.concatenate(trigger_ids[ifo]).astype(numpy.int32)\n\n        # cluster the triggers we've found\n        # (both zerolag and non handled together)\n        num_zerolag = 0\n        num_background = 0\n\n        logging.info('%s background and zerolag coincs', len(cstat))\n        if len(cstat) > 0:\n            offsets = numpy.concatenate(offsets)\n            ctime0 = numpy.concatenate(ctimes[self.ifos[0]]).astype(numpy.float64)\n            ctime1 = numpy.concatenate(ctimes[self.ifos[1]]).astype(numpy.float64)\n\n            cidx = cluster_coincs(cstat, ctime0, ctime1, offsets,\n                                      self.timeslide_interval,\n                                      self.analysis_block)\n            offsets = offsets[cidx]\n            zerolag_idx = (offsets == 0)\n            bkg_idx = (offsets != 0)\n\n            for ifo in self.ifos:\n                single_expire[ifo] = numpy.concatenate(single_expire[ifo])\n                single_expire[ifo] = single_expire[ifo][cidx][bkg_idx]\n\n            self.coincs.add(cstat[cidx][bkg_idx], single_expire, ifos)\n            num_zerolag = zerolag_idx.sum()\n            num_background = bkg_idx.sum()\n        elif len(ifos) > 0:\n            self.coincs.increment(ifos)\n\n        ####################################Collect coinc results for saving\n        coinc_results = {}\n        # Save information about zerolag triggers\n        if num_zerolag > 0:\n            zerolag_results = {}\n            idx = cidx[zerolag_idx][0]\n            zerolag_cstat = cstat[cidx][zerolag_idx]\n            zerolag_results['foreground/ifar'] = self.ifar(zerolag_cstat)\n            zerolag_results['foreground/stat'] = zerolag_cstat\n            template = template_ids[idx]\n            for ifo in self.ifos:\n                trig_id = trigger_ids[ifo][idx]\n                single_data = self.singles[ifo].data(template)[trig_id]\n                for key in single_data.dtype.names:\n                    path = 'foreground/%s/%s' % (ifo, key)\n                    zerolag_results[path] = single_data[key]\n\n            zerolag_results['foreground/type'] = '-'.join(self.ifos)\n\n            coinc_results.update(zerolag_results)\n\n        # Save some summary statistics about the background\n        coinc_results['background/time'] = numpy.array([self.background_time])\n        coinc_results['background/count'] = len(self.coincs.data)\n\n        # Save all the background triggers\n        if self.return_background:\n            coinc_results['background/stat'] = self.coincs.data\n        return num_background, coinc_results"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef backout_last(self, updated_singles, num_coincs):\n        for ifo in updated_singles:\n            self.singles[ifo].discard_last(updated_singles[ifo])\n        self.coincs.remove(num_coincs)", "response": "Remove the recently added singles and coincs from the internal buffer."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds singles to the bacckground estimate and find candidates for the given set of results.", "response": "def add_singles(self, results):\n        \"\"\"Add singles to the bacckground estimate and find candidates\n\n        Parameters\n        ----------\n        results: dict of arrays\n            Dictionary of dictionaries indexed by ifo and keys such as 'snr',\n            'chisq', etc. The specific format it determined by the\n            LiveBatchMatchedFilter class.\n\n        Returns\n        -------\n        coinc_results: dict of arrays\n            A dictionary of arrays containing the coincident results.\n        \"\"\"\n        # Let's see how large everything is\n        logging.info('BKG Coincs %s stored %s bytes',\n                     len(self.coincs), self.coincs.nbytes)\n\n        # If there are no results just return\n        valid_ifos = [k for k in results.keys() if results[k] and k in self.ifos]\n        if len(valid_ifos) == 0: return {}\n\n        # Add single triggers to the internal buffer\n        self._add_singles_to_buffer(results, ifos=valid_ifos)\n\n        # Calculate zerolag and background coincidences\n        _, coinc_results = self._find_coincs(results, ifos=valid_ifos)\n\n        # record if a coinc is possible in this chunk\n        if len(valid_ifos) == 2:\n            coinc_results['coinc_possible'] = True\n\n        return coinc_results"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\napplying the physical constraints to the given parameter values.", "response": "def _constraints(self, values):\n        \"\"\"Applies physical constraints to the given parameter values.\n\n        Parameters\n        ----------\n        values : {arr or dict}\n            A dictionary or structured array giving the values.\n\n        Returns\n        -------\n        bool\n            Whether or not the values satisfy physical\n        \"\"\"\n        mass1, mass2, phi_a, phi_s, chi_eff, chi_a, xi1, xi2, _ = \\\n            conversions.ensurearray(values['mass1'], values['mass2'],\n                                    values['phi_a'], values['phi_s'],\n                                    values['chi_eff'], values['chi_a'],\n                                    values['xi1'], values['xi2'])\n        s1x = conversions.spin1x_from_xi1_phi_a_phi_s(xi1, phi_a, phi_s)\n        s2x = conversions.spin2x_from_mass1_mass2_xi2_phi_a_phi_s(mass1, mass2,\n            xi2, phi_a, phi_s)\n        s1y = conversions.spin1y_from_xi1_phi_a_phi_s(xi1, phi_a, phi_s)\n        s2y = conversions.spin2y_from_mass1_mass2_xi2_phi_a_phi_s(mass1, mass2,\n            xi2, phi_a, phi_s)\n        s1z = conversions.spin1z_from_mass1_mass2_chi_eff_chi_a(mass1, mass2,\n            chi_eff, chi_a)\n        s2z = conversions.spin2z_from_mass1_mass2_chi_eff_chi_a(mass1, mass2,\n            chi_eff, chi_a)\n        test = ((s1x**2. + s1y**2. + s1z**2.) < 1.) & \\\n               ((s2x**2. + s2y**2. + s2z**2.) < 1.)\n        return test"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndraw random samples without applying physical constrains.", "response": "def _draw(self, size=1, **kwargs):\n        \"\"\"Draws random samples without applying physical constrains.\n        \"\"\"\n        # draw masses\n        try:\n            mass1 = kwargs['mass1']\n        except KeyError:\n            mass1 = self.mass1_distr.rvs(size=size)['mass1']\n        try:\n            mass2 = kwargs['mass2']\n        except KeyError:\n            mass2 = self.mass2_distr.rvs(size=size)['mass2']\n        # draw angles\n        try:\n            phi_a = kwargs['phi_a']\n        except KeyError:\n            phi_a = self.phia_distr.rvs(size=size)['phi_a']\n        try:\n            phi_s = kwargs['phi_s']\n        except KeyError:\n            phi_s = self.phis_distr.rvs(size=size)['phi_s']\n        # draw chi_eff, chi_a\n        try:\n            chi_eff = kwargs['chi_eff']\n        except KeyError:\n            chi_eff = self.chieff_distr.rvs(size=size)['chi_eff']\n        try:\n            chi_a = kwargs['chi_a']\n        except KeyError:\n            chi_a = self.chia_distr.rvs(size=size)['chi_a']\n        # draw xis\n        try:\n            xi1 = kwargs['xi1']\n        except KeyError:\n            xi1 = self.xi1_distr.rvs(size=size)['xi1']\n        try:\n            xi2 = kwargs['xi2']\n        except KeyError:\n            xi2 = self.xi2_distr.rvs(size=size)['xi2']\n        dtype = [(p, float) for p in self.params]\n        arr = numpy.zeros(size, dtype=dtype)\n        arr['mass1'] = mass1\n        arr['mass2'] = mass2\n        arr['phi_a'] = phi_a\n        arr['phi_s'] = phi_s\n        arr['chi_eff'] = chi_eff\n        arr['chi_a'] = chi_a\n        arr['xi1'] = xi1\n        arr['xi2'] = xi2\n        return arr"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn random values for all of the parameters.", "response": "def rvs(self, size=1, **kwargs):\n        \"\"\"Returns random values for all of the parameters.\n        \"\"\"\n        size = int(size)\n        dtype = [(p, float) for p in self.params]\n        arr = numpy.zeros(size, dtype=dtype)\n        remaining = size\n        keepidx = 0\n        while remaining:\n            draws = self._draw(size=remaining, **kwargs)\n            mask = self._constraints(draws)\n            addpts = mask.sum()\n            arr[keepidx:keepidx+addpts] = draws[mask]\n            keepidx += addpts\n            remaining = size - keepidx\n        return arr"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_config(cls, cp, section, variable_args):\n        tag = variable_args\n        variable_args = variable_args.split(VARARGS_DELIM)\n        if not set(variable_args) == set(cls._params):\n            raise ValueError(\"Not all parameters used by this distribution \"\n                             \"included in tag portion of section name\")\n        # get the bounds for the setable parameters\n        mass1 = get_param_bounds_from_config(cp, section, tag, 'mass1')\n        mass2 = get_param_bounds_from_config(cp, section, tag, 'mass2')\n        chi_eff = get_param_bounds_from_config(cp, section, tag, 'chi_eff')\n        chi_a = get_param_bounds_from_config(cp, section, tag, 'chi_a')\n        xi_bounds = get_param_bounds_from_config(cp, section, tag, 'xi_bounds')\n        if cp.has_option('-'.join([section, tag]), 'nsamples'):\n            nsamples = int(cp.get('-'.join([section, tag]), 'nsamples'))\n        else:\n            nsamples = None\n        return cls(mass1=mass1, mass2=mass2, chi_eff=chi_eff, chi_a=chi_a,\n                   xi_bounds=xi_bounds, nsamples=nsamples)", "response": "Returns a new IndependentChiPChiEffUnityUnityUnityUnityUnityUnityUnityUnityUnityUnityUnityUnityUnityUnityUnityUnityUnityUnityUnityUnityUnityUnityUnityUnityUnityUnityUnityUnityUnityUnityUnityUnityUnityUnityUnityUnityUnityUnityUnityUnityUnityUnityUnity"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the source data for a particular GW catalog", "response": "def get_source(source):\n    \"\"\"Get the source data for a particular GW catalog\n    \"\"\"\n    if source == 'gwtc-1':\n        fname = download_file(gwtc1_url, cache=True)\n        data = json.load(open(fname, 'r'))\n    else:\n        raise ValueError('Unkown catalog source {}'.format(source))\n    return data['data']"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef init_logging(verbose=False, format='%(asctime)s %(message)s'):\n    def sig_handler(signum, frame):\n        logger = logging.getLogger()\n        log_level = logger.level\n        if log_level == logging.DEBUG:\n            log_level = logging.WARN\n        else:\n            log_level = logging.DEBUG\n        logging.warn('Got signal %d, setting log level to %d',\n                     signum, log_level)\n        logger.setLevel(log_level)\n\n    signal.signal(signal.SIGUSR1, sig_handler)\n\n    if verbose:\n        initial_level = logging.DEBUG\n    else:\n        initial_level = logging.WARN\n    logging.getLogger().setLevel(initial_level)\n    logging.basicConfig(format=format, level=initial_level)", "response": "Initializes the logging system."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef makedir(path):\n    if path is not None and not os.path.exists(path):\n        os.makedirs(path)", "response": "Make the analysis directory path and any parent directories that don t exist. Will do nothing."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef max_posterior(lnps_per_walker, dim):\n    if len(lnps_per_walker.shape) != 2:\n        raise ValueError(\"lnps_per_walker must have shape \"\n                         \"nwalkers x niterations\")\n    # find the value to compare against\n    max_p = lnps_per_walker.max()\n    criteria = max_p - dim/2.\n    nwalkers, _ = lnps_per_walker.shape\n    burn_in_idx = numpy.empty(nwalkers, dtype=int)\n    is_burned_in = numpy.empty(nwalkers, dtype=bool)\n    # find the first iteration in each chain where the logpost has exceeded\n    # max_p - dim/2\n    for ii in range(nwalkers):\n        chain = lnps_per_walker[ii, :]\n        passedidx = numpy.where(chain >= criteria)[0]\n        is_burned_in[ii] = passedidx.size > 0\n        if is_burned_in[ii]:\n            burn_in_idx[ii] = passedidx[0]\n        else:\n            burn_in_idx[ii] = NOT_BURNED_IN_ITER\n    return burn_in_idx, is_burned_in", "response": "Burn in based on samples being within dim of maximum posterior."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef posterior_step(logposts, dim):\n    if logposts.ndim > 1:\n        raise ValueError(\"logposts must be a 1D array\")\n    criteria = dim/2.\n    dp = numpy.diff(logposts)\n    indices = numpy.where(dp >= criteria)[0]\n    if indices.size > 0:\n        idx = indices[-1] + 1\n    else:\n        idx = 0\n    return idx", "response": "Finds the index of the last time a logpost made a jump > dim."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_external_call(cmdList, out_dir=None, out_basename='external_call',\n                       shell=False, fail_on_error=True):\n    \"\"\"\n    Use this to make an external call using the python subprocess module.\n    See the subprocess documentation for more details of how this works.\n    http://docs.python.org/2/library/subprocess.html\n\n    Parameters\n    -----------\n    cmdList : list of strings\n        This list of strings contains the command to be run. See the subprocess\n        documentation for more details.\n    out_dir : string\n        If given the stdout and stderr will be redirected to\n        os.path.join(out_dir,out_basename+[\".err\",\".out])\n        If not given the stdout and stderr will not be recorded\n    out_basename : string\n        The value of out_basename used to construct the file names used to\n        store stderr and stdout. See out_dir for more information.\n    shell : boolean, default=False\n        This value will be given as the shell kwarg to the subprocess call.\n        **WARNING** See the subprocess documentation for details on this\n        Kwarg including a warning about a serious security exploit. Do not\n        use this unless you are sure it is necessary **and** safe.\n    fail_on_error : boolean, default=True\n        If set to true an exception will be raised if the external command does\n        not return a code of 0. If set to false such failures will be ignored.\n        Stderr and Stdout can be stored in either case using the out_dir\n        and out_basename options.\n\n    Returns\n    --------\n    exitCode : int\n        The code returned by the process.\n    \"\"\"\n    if out_dir:\n        outBase = os.path.join(out_dir,out_basename)\n        errFile = outBase + '.err'\n        errFP = open(errFile, 'w')\n        outFile = outBase + '.out'\n        outFP = open(outFile, 'w')\n        cmdFile = outBase + '.sh'\n        cmdFP = open(cmdFile, 'w')\n        cmdFP.write(' '.join(cmdList))\n        cmdFP.close()\n    else:\n        errFile = None\n        outFile = None\n        cmdFile = None\n        errFP = None\n        outFP = None\n\n    msg = \"Making external call %s\" %(' '.join(cmdList))\n    logging.debug(msg)\n    errCode = subprocess.call(cmdList, stderr=errFP, stdout=outFP,\\\n                              shell=shell)\n    if errFP:\n        errFP.close()\n    if outFP:\n        outFP.close()\n\n    if errCode and fail_on_error:\n        raise CalledProcessErrorMod(errCode, ' '.join(cmdList),\n                errFile=errFile, outFile=outFile, cmdFile=cmdFile)\n    logging.debug(\"Call successful, or error checking disabled.\")", "response": "This function will make an external call to the internal command list."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_full_analysis_chunk(science_segs):\n    extents = [science_segs[ifo].extent() for ifo in science_segs.keys()]\n    min, max = extents[0]\n    for lo, hi in extents:\n        if min > lo:\n            min = lo\n        if max < hi:\n            max = hi\n    fullSegment = segments.segment(min, max)\n    return fullSegment", "response": "Function to find the first and last time point contained in the science segments\n    and return a single segment spanning that full time."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a random label string to use when clustering jobs.", "response": "def get_random_label():\n    \"\"\"\n    Get a random label string to use when clustering jobs.\n    \"\"\"\n    return ''.join(random.choice(string.ascii_uppercase + string.digits) \\\n                   for _ in range(15))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ifo(self):\n        if self.ifo_list and len(self.ifo_list) == 1:\n            return self.ifo_list[0]\n        else:\n            errMsg = \"self.ifoList must contain only one ifo to access the \"\n            errMsg += \"ifo property. %s.\" %(str(self.ifo_list),)\n            raise TypeError(errMsg)", "response": "Return the ifo.\n\n        If only one ifo in the ifo list this will be that ifo. Otherwise an\n        error is raised."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_ini_profile(self, cp, sec):\n        for opt in cp.options(sec):\n            namespace = opt.split('|')[0]\n            if namespace == 'pycbc' or namespace == 'container':\n                continue\n\n            value = string.strip(cp.get(sec, opt))\n            key = opt.split('|')[1]\n            self.add_profile(namespace, key, value, force=True)\n\n            # Remove if Pegasus can apply this hint in the TC\n            if namespace == 'hints' and key == 'execution.site':\n                self.execution_site = value", "response": "Add profile from configuration file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_ini_opts(self, cp, sec):\n        for opt in cp.options(sec):\n            value = string.strip(cp.get(sec, opt))\n            opt = '--%s' %(opt,)\n            if opt in self.file_input_options:\n                # This now expects the option to be a file\n                # Check is we have a list of files\n                values = [path for path in value.split(' ') if path]\n\n                self.common_raw_options.append(opt)\n                self.common_raw_options.append(' ')\n\n                # Get LFN and PFN\n                for path in values:\n                    # Here I decide if the path is URL or\n                    # IFO:/path/to/file or IFO:url://path/to/file\n                    # That's somewhat tricksy as we used : as delimiter\n                    split_path = path.split(':', 1)\n                    if len(split_path) == 1:\n                        ifo = None\n                        path = path\n                    else:\n                        # Have I split a URL or not?\n                        if split_path[1].startswith('//'):\n                            # URL\n                            ifo = None\n                            path = path\n                        else:\n                            #IFO:path or IFO:URL\n                            ifo = split_path[0]\n                            path = split_path[1]\n\n                    curr_lfn = os.path.basename(path)\n\n                    # If the file exists make sure to use the\n                    # fill path as a file:// URL\n                    if os.path.isfile(path):\n                        curr_pfn = urlparse.urljoin('file:',\n                                    urllib.pathname2url(\n                                    os.path.abspath(path)))\n                    else:\n                        curr_pfn = path\n\n                    if curr_lfn in file_input_from_config_dict.keys():\n                        file_pfn = file_input_from_config_dict[curr_lfn][2]\n                        assert(file_pfn == curr_pfn)\n                        curr_file = file_input_from_config_dict[curr_lfn][1]\n                    else:\n                        local_file_path = resolve_url(curr_pfn)\n                        curr_file = File.from_path(local_file_path)\n                        tuple_val = (local_file_path, curr_file, curr_pfn)\n                        file_input_from_config_dict[curr_lfn] = tuple_val\n                    self.common_input_files.append(curr_file)\n                    if ifo:\n                        self.common_raw_options.append(ifo + ':')\n                        self.common_raw_options.append(curr_file.dax_repr)\n                    else:\n                        self.common_raw_options.append(curr_file.dax_repr)\n                    self.common_raw_options.append(' ')\n            else:\n                self.common_options += [opt, value]", "response": "Add options from the configuration file to the internal list of options that are set in the config file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding option to job.", "response": "def add_opt(self, opt, value=None):\n        \"\"\"Add option to job.\n\n        Parameters\n        -----------\n        opt : string\n            Name of option (e.g. --output-file-format)\n        value : string, (default=None)\n            The value for the option (no value if set to None).\n        \"\"\"\n        if value is None:\n            self.common_options += [opt]\n        else:\n            self.common_options += [opt, value]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets value of option from configuration file", "response": "def get_opt(self, opt):\n        \"\"\"Get value of option from configuration file\n\n        Parameters\n        -----------\n        opt : string\n            Name of option (e.g. output-file-format)\n\n        Returns\n        --------\n        value : string\n            The value for the option. Returns None if option not present.\n        \"\"\"\n        for sec in self.sections:\n            try:\n                key = self.cp.get(sec, opt)\n                if key:\n                    return key\n            except ConfigParser.NoOptionError:\n                pass\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if option is present in configuration file", "response": "def has_opt(self, opt):\n        \"\"\"Check if option is present in configuration file\n\n        Parameters\n        -----------\n        opt : string\n            Name of option (e.g. output-file-format)\n        \"\"\"\n        for sec in self.sections:\n            val = self.cp.has_option(sec, opt)\n            if val:\n                return val\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating the value of self. current_retention_level for an updated value of the current retention level.", "response": "def update_current_retention_level(self, value):\n        \"\"\"Set a new value for the current retention level.\n\n        This updates the value of self.retain_files for an updated value of the\n        retention level.\n\n        Parameters\n        -----------\n        value : int\n            The new value to use for the retention level.\n        \"\"\"\n        # Determine the level at which output files should be kept\n        self.current_retention_level = value\n        try:\n            global_retention_level = \\\n                self.cp.get_opt_tags(\"workflow\", \"file-retention-level\",\n                                   self.tags+[self.name])\n        except ConfigParser.Error:\n            msg=\"Cannot find file-retention-level in [workflow] section \"\n            msg+=\"of the configuration file. Setting a default value of \"\n            msg+=\"retain all files.\"\n            logging.warn(msg)\n            self.retain_files = True\n            self.global_retention_threshold = 1\n            self.cp.set(\"workflow\", \"file-retention-level\", \"all_files\")\n        else:\n            # FIXME: Are these names suitably descriptive?\n            retention_choices = {\n                                 'all_files' : 1,\n                                 'all_triggers' : 2,\n                                 'merged_triggers' : 3,\n                                 'results' : 4\n                                }\n            try:\n                self.global_retention_threshold = \\\n                      retention_choices[global_retention_level]\n            except KeyError:\n                err_msg = \"Cannot recognize the file-retention-level in the \"\n                err_msg += \"[workflow] section of the ini file. \"\n                err_msg += \"Got : {0}.\".format(global_retention_level)\n                err_msg += \"Valid options are: 'all_files', 'all_triggers',\"\n                err_msg += \"'merged_triggers' or 'results' \"\n                raise ValueError(err_msg)\n            if self.current_retention_level == 5:\n                self.retain_files = True\n                if type(self).__name__ in Executable._warned_classes_list:\n                    pass\n                else:\n                    warn_msg = \"Attribute current_retention_level has not \"\n                    warn_msg += \"been set in class {0}. \".format(type(self))\n                    warn_msg += \"This value should be set explicitly. \"\n                    warn_msg += \"All output from this class will be stored.\"\n                    logging.warn(warn_msg)\n                    Executable._warned_classes_list.append(type(self).__name__)\n            elif self.global_retention_threshold > self.current_retention_level:\n                self.retain_files = False\n            else:\n                self.retain_files = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_current_tags(self, tags):\n        if tags is None:\n            tags = []\n        tags = [tag.upper() for tag in tags]\n        self.tags = tags\n\n        if len(tags) > 6:\n            warn_msg = \"This job has way too many tags. \"\n            warn_msg += \"Current tags are {}. \".format(' '.join(tags))\n            warn_msg += \"Current executable {}.\".format(self.name)\n            logging.info(warn_msg)\n\n        if len(tags) != 0:\n            self.tagged_name = \"{0}-{1}\".format(self.name, '_'.join(tags))\n        else:\n            self.tagged_name = self.name\n        if self.ifo_string is not None:\n            self.tagged_name = \"{0}-{1}\".format(self.tagged_name,\n                                                self.ifo_string)\n\n\n        # Determine the sections from the ini file that will configure\n        # this executable\n        sections = [self.name]\n        if self.ifo_list is not None:\n            if len(self.ifo_list) > 1:\n                sec_tags = tags + self.ifo_list + [self.ifo_string]\n            else:\n                sec_tags = tags + self.ifo_list\n        else:\n            sec_tags = tags\n        for sec_len in range(1, len(sec_tags)+1):\n            for tag_permutation in permutations(sec_tags, sec_len):\n                joined_name = '-'.join(tag_permutation)\n                section = '{0}-{1}'.format(self.name, joined_name.lower())\n                if self.cp.has_section(section):\n                    sections.append(section)\n\n        self.sections = sections\n\n        # Do some basic sanity checking on the options\n        for sec1, sec2 in combinations(sections, 2):\n            self.cp.check_duplicate_options(sec1, sec2, raise_error=True)\n\n        # collect the options and profile information\n        # from the ini file section(s)\n        self.common_options = []\n        self.common_raw_options = []\n        self.common_input_files = []\n        for sec in sections:\n            if self.cp.has_section(sec):\n                self.add_ini_opts(self.cp, sec)\n            else:\n                warn_string = \"warning: config file is missing section \"\n                warn_string += \"[{0}]\".format(sec)\n                logging.warn(warn_string)", "response": "Update the set of tags that this executable will use. This will update the set of tags that this executable will use."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_output_directory(self, out_dir=None):\n        # Determine the output directory\n        if out_dir is not None:\n            self.out_dir = out_dir\n        elif len(self.tags) == 0:\n            self.out_dir = self.name\n        else:\n            self.out_dir = self.tagged_name\n        if not os.path.isabs(self.out_dir):\n            self.out_dir = os.path.join(os.getcwd(), self.out_dir)", "response": "Update the output directory for the output files."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the pegasus - profile options for this Executable.", "response": "def _set_pegasus_profile_options(self):\n        \"\"\"Set the pegasus-profile settings for this Executable.\n\n        These are a property of the Executable and not of nodes that it will\n        spawn. Therefore it *cannot* be updated without also changing values\n        for nodes that might already have been created. Therefore this is\n        only called once in __init__. Second calls to this will fail.\n        \"\"\"\n        # Add executable non-specific profile information\n        if self.cp.has_section('pegasus_profile'):\n            self.add_ini_profile(self.cp, 'pegasus_profile')\n\n        # Executable- and tag-specific profile information\n        for sec in self.sections:\n            if self.cp.has_section('pegasus_profile-{0}'.format(sec)):\n                self.add_ini_profile(self.cp,\n                                     'pegasus_profile-{0}'.format(sec))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexecute this node immediately on the local machine.", "response": "def execute_node(self, node, verbatim_exe = False):\n        \"\"\" Execute this node immediately on the local machine\n        \"\"\"\n        node.executed = True\n\n        # Check that the PFN is for a file or path\n        if node.executable.needs_fetching:\n            try:\n                # The pfn may have been marked local...\n                pfn = node.executable.get_pfn()\n            except:\n                # or it may have been marked nonlocal.  That's\n                # fine, we'll resolve the URL and make a local\n                # entry.\n                pfn = node.executable.get_pfn('nonlocal')\n\n            resolved = resolve_url(pfn, permissions=stat.S_IRUSR | stat.S_IWUSR | stat.S_IXUSR)\n            node.executable.clear_pfns()\n            node.executable.add_pfn(urlparse.urljoin('file:',\n                                    urllib.pathname2url(\n                                    resolved)), site='local')\n\n        cmd_list = node.get_command_line()\n\n        # Must execute in output directory.\n        curr_dir = os.getcwd()\n        out_dir = node.executable.out_dir\n        os.chdir(out_dir)\n\n        # Make call\n        make_external_call(cmd_list, out_dir=os.path.join(out_dir, 'logs'),\n                                     out_basename=node.executable.name)\n        # Change back\n        os.chdir(curr_dir)\n\n        for fil in node._outputs:\n            fil.node = None\n            fil.PFN(urlparse.urljoin('file:',\n                    urllib.pathname2url(fil.storage_path)),\n                    site='local')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef save_config(self, fname, output_dir, cp=None):\n        cp = self.cp if cp is None else cp\n        ini_file_path = os.path.join(output_dir, fname)\n        with open(ini_file_path, \"wb\") as fp:\n            cp.write(fp)\n        ini_file = FileList([File(self.ifos, \"\",\n                                  self.analysis_time,\n                                  file_url=\"file://\" + ini_file_path)])\n        return ini_file", "response": "Writes the configuration file to disk and returns a FileList object with the configuration file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef new_output_file_opt(self, valid_seg, extension, option_name, tags=None,\n                            store_file=None, use_tmp_subdirs=False):\n        \"\"\"\n        This function will create a workflow.File object corresponding to the given\n        information and then add that file as output of this node.\n\n        Parameters\n        -----------\n        valid_seg : ligo.segments.segment\n            The time span over which the job is valid for.\n        extension : string\n            The extension to be used at the end of the filename.\n            E.g. '.xml' or '.sqlite'.\n        option_name : string\n            The option that is used when setting this job as output. For e.g.\n            'output-name' or 'output-file', whatever is appropriate for the\n            current executable.\n        tags : list of strings, (optional, default=[])\n            These tags will be added to the list of tags already associated with\n            the job. They can be used to uniquely identify this output file.\n        store_file : Boolean, (optional, default=True)\n            This file is to be added to the output mapper and will be stored\n            in the specified output location if True. If false file will be\n            removed when no longer needed in the workflow.\n        \"\"\"\n        if tags is None:\n            tags = []\n\n        # Changing this from set(tags) to enforce order. It might make sense\n        # for all jobs to have file names with tags in the same order.\n        all_tags = copy.deepcopy(self.executable.tags)\n        for tag in tags:\n            if tag not in all_tags:\n                all_tags.append(tag)\n\n        store_file = store_file if store_file is not None else self.executable.retain_files\n\n        fil = File(self.executable.ifo_list, self.executable.name,\n                   valid_seg, extension=extension, store_file=store_file,\n                   directory=self.executable.out_dir, tags=all_tags,\n                   use_tmp_subdirs=use_tmp_subdirs)\n        self.add_output_opt(option_name, fil)\n        return fil", "response": "This function will create a new output file object corresponding to the given option_name and add it to the workflow. File object corresponding to the given valid_seg extension and option_name."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_multiifo_input_list_opt(self, opt, inputs):\n        # NOTE: Here we have to use the raw arguments functionality as the\n        #       file and ifo are not space separated.\n        self.add_raw_arg(opt)\n        self.add_raw_arg(' ')\n        for infile in inputs:\n            self.add_raw_arg(infile.ifo)\n            self.add_raw_arg(':')\n            self.add_raw_arg(infile.name)\n            self.add_raw_arg(' ')\n            self._add_input(infile)", "response": "Add an option that determines a list of inputs from multiple IFOs. Files will be supplied as - opt ifo1 - input1 ifo2 - input2....."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding an option that determines a list of outputs from multiple IFOs. Files will be supplied as - opt ifo1 - input1 ifo2 - input2.....", "response": "def add_multiifo_output_list_opt(self, opt, outputs):\n        \"\"\" Add an option that determines a list of outputs from multiple\n            detectors. Files will be supplied as --opt ifo1:input1 ifo2:input2\n            .....\n        \"\"\"\n        # NOTE: Here we have to use the raw arguments functionality as the\n        #       file and ifo are not space separated.\n        self.add_raw_arg(opt)\n        self.add_raw_arg(' ')\n        for outfile in outputs:\n            self.add_raw_arg(outfile.ifo)\n            self.add_raw_arg(':')\n            self.add_raw_arg(outfile.name)\n            self.add_raw_arg(' ')\n            self._add_output(outfile)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef new_multiifo_output_list_opt(self, opt, ifos, analysis_time, extension,\n                                     tags=None, store_file=None,\n                                     use_tmp_subdirs=False):\n        \"\"\" Add an option that determines a list of outputs from multiple\n            detectors. Files will be supplied as --opt ifo1:input1 ifo2:input2\n            .....\n            File names are created internally from the provided extension and\n            analysis time.\n        \"\"\"\n        if tags is None:\n            tags = []\n        all_tags = copy.deepcopy(self.executable.tags)\n        for tag in tags:\n            if tag not in all_tags:\n                all_tags.append(tag)\n\n        output_files = FileList([])\n        store_file = store_file if store_file is not None \\\n                                              else self.executable.retain_files\n\n        for ifo in ifos:\n            curr_file = File(ifo, self.executable.name, analysis_time,\n                             extension=extension, store_file=store_file,\n                             directory=self.executable.out_dir, tags=all_tags,\n                             use_tmp_subdirs=use_tmp_subdirs)\n            output_files.append(curr_file)\n        self.add_multiifo_output_list_opt(opt, output_files)", "response": "Add an option that determines a list of outputs from multiple IFOs. Files will be created internally from the provided extension and analysis time."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef output_file(self):\n        out_files = self.output_files\n        if len(out_files) != 1:\n            err_msg = \"output_file property is only valid if there is a single\"\n            err_msg += \" output file. Here there are \"\n            err_msg += \"%d output files.\" %(len(out_files))\n            raise ValueError(err_msg)\n        return out_files[0]", "response": "Return the name of the output file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the ifo in the ifo_list.", "response": "def ifo(self):\n        \"\"\"\n        If only one ifo in the ifo_list this will be that ifo. Otherwise an\n        error is raised.\n        \"\"\"\n        if len(self.ifo_list) == 1:\n            return self.ifo_list[0]\n        else:\n            err = \"self.ifo_list must contain only one ifo to access the \"\n            err += \"ifo property. %s.\" %(str(self.ifo_list),)\n            raise TypeError(err)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the segment property of the current object.", "response": "def segment(self):\n        \"\"\"\n        If only one segment in the segmentlist this will be that segment.\n        Otherwise an error is raised.\n        \"\"\"\n        if len(self.segment_list) == 1:\n            return self.segment_list[0]\n        else:\n            err = \"self.segment_list must only contain one segment to access\"\n            err += \" the segment property. %s.\" %(str(self.segment_list),)\n            raise TypeError(err)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a CacheEntry instance for File.", "response": "def cache_entry(self):\n        \"\"\"\n        Returns a CacheEntry instance for File.\n        \"\"\"\n        if self.storage_path is None:\n            raise ValueError('This file is temporary and so a lal '\n                             'cache entry cannot be made')\n\n        file_url = urlparse.urlunparse(['file', 'localhost', self.storage_path, None,\n                                            None, None])\n        cache_entry = lal.utils.CacheEntry(self.ifo_string,\n                   self.tagged_description, self.segment_list.extent(), file_url)\n        cache_entry.workflow_file = self\n        return cache_entry"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _filename(self, ifo, description, extension, segment):\n        if extension.startswith('.'):\n            extension = extension[1:]\n\n        # Follow the frame convention of using integer filenames,\n        # but stretching to cover partially covered seconds.\n        start = int(segment[0])\n        end = int(math.ceil(segment[1]))\n        duration = str(end-start)\n        start = str(start)\n\n        return \"%s-%s-%s-%s.%s\" % (ifo, description.upper(), start,\n                                   duration, extension)", "response": "Construct the standard output filename."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfunction to categorize a FileList by a File object attribute.", "response": "def categorize_by_attr(self, attribute):\n        '''\n        Function to categorize a FileList by a File object\n        attribute (eg. 'segment', 'ifo', 'description').\n\n        Parameters\n        -----------\n        attribute : string\n           File object attribute to categorize FileList\n\n        Returns\n        --------\n        keys : list\n           A list of values for an attribute\n        groups : list\n           A list of FileLists\n        '''\n\n        # need to sort FileList otherwise using groupby without sorting does\n        # 'AAABBBCCDDAABB' -> ['AAA','BBB','CC','DD','AA','BB']\n        # and using groupby with sorting does\n        # 'AAABBBCCDDAABB' -> ['AAAAA','BBBBB','CC','DD']\n        flist = sorted(self, key=attrgetter(attribute), reverse=True)\n\n        # use groupby to create lists\n        groups = []\n        keys = []\n        for k, g in groupby(flist, attrgetter(attribute)):\n            groups.append(FileList(g))\n            keys.append(k)\n\n        return keys, groups"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning one File that covers the given time or is most appropriate at the given time range.", "response": "def find_output(self, ifo, time):\n        '''Returns one File most appropriate at the given time/time range.\n\n        Return one File that covers the given time, or is most\n        appropriate for the supplied time range.\n\n        Parameters\n        -----------\n        ifo : string\n           Name of the ifo (or ifos) that the file should be valid for.\n        time : int/float/LIGOGPStime or tuple containing two values\n           If int/float/LIGOGPStime (or similar may of specifying one time) is\n           given, return the File corresponding to the time. This calls\n           self.find_output_at_time(ifo,time).\n           If a tuple of two values is given, return the File that is\n           **most appropriate** for the time range given. This calls\n           self.find_output_in_range\n\n        Returns\n        --------\n        pycbc_file : pycbc.workflow.File instance\n           The File that corresponds to the time or time range\n        '''\n        # Determine whether I have a specific time, or a range of times\n        try:\n            lenTime = len(time)\n        except TypeError:\n            # This is if I have a single time\n            outFile = self.find_output_at_time(ifo,time)\n        else:\n            # This is if I have a range of times\n            if lenTime == 2:\n                outFile = self.find_output_in_range(ifo,time[0],time[1])\n            # This is if I got a list that had more (or less) than 2 entries\n            if len(time) != 2:\n                raise TypeError(\"I do not understand the input variable time\")\n        return outFile"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the list of output files that overlap the given time.", "response": "def find_output_at_time(self, ifo, time):\n        '''\n        Return File that covers the given time.\n\n        Parameters\n        -----------\n        ifo : string\n           Name of the ifo (or ifos) that the File should correspond to\n        time : int/float/LIGOGPStime\n           Return the Files that covers the supplied time. If no\n           File covers the time this will return None.\n\n        Returns\n        --------\n        list of File classes\n           The Files that corresponds to the time.\n         '''\n        # Get list of Files that overlap time, for given ifo\n        outFiles = [i for i in self if ifo in i.ifo_list and time in i.segment_list]\n        if len(outFiles) == 0:\n            # No OutFile at this time\n            return None\n        elif len(outFiles) == 1:\n            # 1 OutFile at this time (good!)\n            return outFiles\n        else:\n            # Multiple output files. Currently this is valid, but we may want\n            # to demand exclusivity later, or in certain cases. Hence the\n            # separation.\n            return outFiles"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_outputs_in_range(self, ifo, current_segment, useSplitLists=False):\n        currsegment_list = segments.segmentlist([current_segment])\n\n        # Get all files overlapping the window\n        overlap_files = self.find_all_output_in_range(ifo, current_segment,\n                                                    useSplitLists=useSplitLists)\n\n        # By how much do they overlap?\n        overlap_windows = [abs(i.segment_list & currsegment_list) for i in overlap_files]\n\n        if not overlap_windows:\n            return []\n\n        # Return the File with the biggest overlap\n        # Note if two File have identical overlap, the first is used\n        # to define the valid segment\n        overlap_windows = numpy.array(overlap_windows, dtype = int)\n        segmentLst = overlap_files[overlap_windows.argmax()].segment_list\n\n        # Get all output files with the exact same segment definition\n        output_files = [f for f in overlap_files if f.segment_list==segmentLst]\n        return output_files", "response": "Find all files that are in the specified time range."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_all_output_in_range(self, ifo, currSeg, useSplitLists=False):\n        if not useSplitLists:\n            # Slower, but simpler method\n            outFiles = [i for i in self if ifo in i.ifo_list]\n            outFiles = [i for i in outFiles \\\n                                      if i.segment_list.intersects_segment(currSeg)]\n        else:\n            # Faster, but more complicated\n            # Basically only check if a subset of files intersects_segment by\n            # using a presorted list. Sorting only happens once.\n            if not self._check_split_list_validity():\n                # FIXME: DO NOT hard code this.\n                self._temporal_split_list(100)\n            startIdx = int( (currSeg[0] - self._splitListsStart) / \\\n                                                          self._splitListsStep )\n            # Add some small rounding here\n            endIdx = (currSeg[1] - self._splitListsStart) / self._splitListsStep\n            endIdx = int(endIdx - 0.000001)\n\n            outFiles = []\n            for idx in range(startIdx, endIdx + 1):\n                if idx < 0 or idx >= self._splitListsNum:\n                    continue\n                outFilesTemp = [i for i in self._splitLists[idx] \\\n                                                            if ifo in i.ifo_list]\n                outFiles.extend([i for i in outFilesTemp \\\n                                      if i.segment_list.intersects_segment(currSeg)])\n                # Remove duplicates\n                outFiles = list(set(outFiles))\n\n        return self.__class__(outFiles)", "response": "Return all output files that overlap the specified segment."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding all files who have a given tag", "response": "def find_output_with_tag(self, tag):\n        \"\"\"\n        Find all files who have tag in self.tags\n        \"\"\"\n        # Enforce upper case\n        tag = tag.upper()\n        return FileList([i for i in self if tag in i.tags])"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds all files who do not have a given tag", "response": "def find_output_without_tag(self, tag):\n        \"\"\"\n        Find all files who do not have tag in self.tags\n        \"\"\"\n        # Enforce upper case\n        tag = tag.upper()\n        return FileList([i for i in self if tag not in i.tags])"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds all files who have ifo", "response": "def find_output_with_ifo(self, ifo):\n        \"\"\"\n        Find all files who have ifo = ifo\n        \"\"\"\n        # Enforce upper case\n        ifo = ifo.upper()\n        return FileList([i for i in self if ifo in i.ifo_list])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind the coalesced intersection of all files in the file list.", "response": "def get_times_covered_by_files(self):\n        \"\"\"\n        Find the coalesced intersection of the segments of all files in the\n        list.\n        \"\"\"\n        times = segments.segmentlist([])\n        for entry in self:\n            times.extend(entry.segment_list)\n        times.coalesce()\n        return times"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef convert_to_lal_cache(self):\n        lal_cache = gluelal.Cache([])\n        for entry in self:\n            try:\n                lal_cache.append(entry.cache_entry)\n            except ValueError:\n                pass\n        return lal_cache", "response": "Convert this object to a gluelal. lal. Cache object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _check_split_list_validity(self):\n        # FIXME: Currently very primitive, but needs to be fast\n        if not (hasattr(self,\"_splitListsSet\") and (self._splitListsSet)):\n            return False\n        elif len(self) != self._splitListsLength:\n            return False\n        else:\n            return True", "response": "Checks if the current cluster split lists are still valid."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dump(self, filename):\n        f = open(filename, 'w')\n        cPickle.dump(self, f)", "response": "Dump this AhopeFileList to a pickle file"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndump to a pickle file and return an AhopeFile object reference of this list .", "response": "def to_file_object(self, name, out_dir):\n        \"\"\"Dump to a pickle file and return an File object reference of this list\n\n        Parameters\n        ----------\n        name : str\n            An identifier of this file. Needs to be unique.\n        out_dir : path\n            path to place this file\n\n        Returns\n        -------\n        file : AhopeFile\n        \"\"\"\n        make_analysis_dir(out_dir)\n\n        file_ref = File('ALL', name, self.get_times_covered_by_files(),\n                             extension='.pkl', directory=out_dir)\n        self.dump(file_ref.storage_path)\n        return file_ref"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninitialize a SegFile object from a segmentlist.", "response": "def from_segment_list(cls, description, segmentlist, name, ifo,\n                          seg_summ_list=None, **kwargs):\n        \"\"\" Initialize a SegFile object from a segmentlist.\n\n        Parameters\n        ------------\n        description : string (required)\n            See File.__init__\n        segmentlist : ligo.segments.segmentslist\n            The segment list that will be stored in this file.\n        name : str\n            The name of the segment lists to be stored in the file.\n        ifo : str\n            The ifo of the segment lists to be stored in this file.\n        seg_summ_list : ligo.segments.segmentslist (OPTIONAL)\n            Specify the segment_summary segmentlist that goes along with the\n            segmentlist. Default=None, in this case segment_summary is taken\n            from the valid_segment of the SegFile class.\n        \"\"\"\n        seglistdict = segments.segmentlistdict()\n        seglistdict[ifo + ':' + name] = segmentlist\n        if seg_summ_list is not None:\n            seg_summ_dict = segments.segmentlistdict()\n            seg_summ_dict[ifo + ':' + name] = seg_summ_list\n        else:\n            seg_summ_dict = None\n        return cls.from_segment_list_dict(description, seglistdict,\n                                          seg_summ_dict=None, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_multi_segment_list(cls, description, segmentlists, names, ifos,\n                                seg_summ_lists=None, **kwargs):\n        \"\"\" Initialize a SegFile object from a list of segmentlists.\n\n        Parameters\n        ------------\n        description : string (required)\n            See File.__init__\n        segmentlists : List of ligo.segments.segmentslist\n            List of segment lists that will be stored in this file.\n        names : List of str\n            List of names of the segment lists to be stored in the file.\n        ifos : str\n            List of ifos of the segment lists to be stored in this file.\n        seg_summ_lists : ligo.segments.segmentslist (OPTIONAL)\n            Specify the segment_summary segmentlists that go along with the\n            segmentlists. Default=None, in this case segment_summary is taken\n            from the valid_segment of the SegFile class.\n        \"\"\"\n        seglistdict = segments.segmentlistdict()\n        for name, ifo, segmentlist in zip(names, ifos, segmentlists):\n            seglistdict[ifo + ':' + name] = segmentlist\n        if seg_summ_lists is not None:\n            seg_summ_dict = segments.segmentlistdict()\n            for name, ifo, seg_summ_list in zip(names, ifos, seg_summ_lists):\n                seg_summ_dict[ifo + ':' + name] = seg_summ_list\n        else:\n            seg_summ_dict = None\n\n        return cls.from_segment_list_dict(description, seglistdict,\n                                         seg_summ_dict=seg_summ_dict, **kwargs)", "response": "Initialize a SegFile object from a list of segmentlists."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninitialize a SegFile object from a segmentlistdict.", "response": "def from_segment_list_dict(cls, description, segmentlistdict,\n                               ifo_list=None, valid_segment=None,\n                               file_exists=False, seg_summ_dict=None,\n                               **kwargs):\n        \"\"\" Initialize a SegFile object from a segmentlistdict.\n\n        Parameters\n        ------------\n        description : string (required)\n            See File.__init__\n        segmentlistdict : ligo.segments.segmentslistdict\n            See SegFile.__init__\n        ifo_list : string or list (optional)\n            See File.__init__, if not given a list of all ifos in the\n            segmentlistdict object will be used\n        valid_segment : ligo.segments.segment or ligo.segments.segmentlist\n            See File.__init__, if not given the extent of all segments in the\n            segmentlistdict is used.\n        file_exists : boolean (default = False)\n            If provided and set to True it is assumed that this file already\n            exists on disk and so there is no need to write again.\n        seg_summ_dict : ligo.segments.segmentslistdict\n            Optional. See SegFile.__init__.\n        \"\"\"\n        if ifo_list is None:\n            ifo_set = set([i.split(':')[0] for i in segmentlistdict.keys()])\n            ifo_list = list(ifo_set)\n            ifo_list.sort()\n        if valid_segment is None:\n            if seg_summ_dict and \\\n                    numpy.any([len(v) for _, v in seg_summ_dict.items()]):\n                # Only come here if seg_summ_dict is supplied and it is\n                # not empty.\n                valid_segment = seg_summ_dict.extent_all()\n            else:\n                try:\n                    valid_segment = segmentlistdict.extent_all()\n                except:\n                    # Numpty probably didn't supply a glue.segmentlistdict\n                    segmentlistdict=segments.segmentlistdict(segmentlistdict)\n                    try:\n                        valid_segment = segmentlistdict.extent_all()\n                    except ValueError:\n                        # No segment_summary and segment list is empty\n                        # Setting valid segment now is hard!\n                        warn_msg = \"No information with which to set valid \"\n                        warn_msg += \"segment.\"\n                        logging.warn(warn_msg)\n                        valid_segment = segments.segment([0,1])\n        instnc = cls(ifo_list, description, valid_segment,\n                     segment_dict=segmentlistdict, seg_summ_dict=seg_summ_dict,\n                     **kwargs)\n        if not file_exists:\n            instnc.to_segment_xml()\n        else:\n            instnc.PFN(urlparse.urljoin('file:',\n                       urllib.pathname2url(\n                       instnc.storage_path)), site='local')\n        return instnc"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_segment_xml(cls, xml_file, **kwargs):\n        # load xmldocument and SegmentDefTable and SegmentTables\n        fp = open(xml_file, 'r')\n        xmldoc, _ = ligolw_utils.load_fileobj(fp,\n                                              gz=xml_file.endswith(\".gz\"),\n                                              contenthandler=ContentHandler)\n\n        seg_def_table = table.get_table(xmldoc,\n                                        lsctables.SegmentDefTable.tableName)\n        seg_table = table.get_table(xmldoc, lsctables.SegmentTable.tableName)\n        seg_sum_table = table.get_table(xmldoc,\n                                        lsctables.SegmentSumTable.tableName)\n\n        segs = segments.segmentlistdict()\n        seg_summ = segments.segmentlistdict()\n\n        seg_id = {}\n        for seg_def in seg_def_table:\n            # Here we want to encode ifo and segment name\n            full_channel_name = ':'.join([str(seg_def.ifos),\n                                          str(seg_def.name)])\n            seg_id[int(seg_def.segment_def_id)] = full_channel_name\n            segs[full_channel_name] = segments.segmentlist()\n            seg_summ[full_channel_name] = segments.segmentlist()\n\n        for seg in seg_table:\n            seg_obj = segments.segment(\n                    lal.LIGOTimeGPS(seg.start_time, seg.start_time_ns),\n                    lal.LIGOTimeGPS(seg.end_time, seg.end_time_ns))\n            segs[seg_id[int(seg.segment_def_id)]].append(seg_obj)\n\n        for seg in seg_sum_table:\n            seg_obj = segments.segment(\n                    lal.LIGOTimeGPS(seg.start_time, seg.start_time_ns),\n                    lal.LIGOTimeGPS(seg.end_time, seg.end_time_ns))\n            seg_summ[seg_id[int(seg.segment_def_id)]].append(seg_obj)\n\n        for seg_name in seg_id.values():\n            segs[seg_name] = segs[seg_name].coalesce()\n\n        xmldoc.unlink()\n        fp.close()\n        curr_url = urlparse.urlunparse(['file', 'localhost', xml_file, None,\n                                        None, None])\n\n        return cls.from_segment_list_dict('SEGMENTS', segs, file_url=curr_url,\n                                          file_exists=True,\n                                          seg_summ_dict=seg_summ, **kwargs)", "response": "Reads a ligo. segments. segmentlist object from the specified segment xml file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfunctions to remove all science segments shorter than a specific length.", "response": "def remove_short_sci_segs(self, minSegLength):\n        \"\"\"\n        Function to remove all science segments\n        shorter than a specific length. Also updates the file on disk to remove\n        these segments.\n\n        Parameters\n        -----------\n        minSegLength : int\n            Maximum length of science segments. Segments shorter than this will\n            be removed.\n        \"\"\"\n        newsegment_list = segments.segmentlist()\n        for key, seglist in self.segment_dict.items():\n            newsegment_list = segments.segmentlist()\n            for seg in seglist:\n                if abs(seg) > minSegLength:\n                    newsegment_list.append(seg)\n            newsegment_list.coalesce()\n            self.segment_dict[key] = newsegment_list\n        self.to_segment_xml(override_file_if_exists=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the segdict key.", "response": "def parse_segdict_key(self, key):\n        \"\"\"\n        Return ifo and name from the segdict key.\n        \"\"\"\n        splt = key.split(':')\n        if len(splt) == 2:\n            return splt[0], splt[1]\n        else:\n            err_msg = \"Key should be of the format 'ifo:name', got %s.\" %(key,)\n            raise ValueError(err_msg)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting the segment list in self. segmentList to self. storage_path.", "response": "def to_segment_xml(self, override_file_if_exists=False):\n        \"\"\"\n        Write the segment list in self.segmentList to self.storage_path.\n        \"\"\"\n        # create XML doc and add process table\n        outdoc = ligolw.Document()\n        outdoc.appendChild(ligolw.LIGO_LW())\n        process = ligolw_process.register_to_xmldoc(outdoc, sys.argv[0], {})\n\n        for key, seglist in self.segment_dict.items():\n            ifo, name = self.parse_segdict_key(key)\n            # Ensure we have LIGOTimeGPS\n            fsegs = [(lal.LIGOTimeGPS(seg[0]),\n                      lal.LIGOTimeGPS(seg[1])) for seg in seglist]\n\n            if self.seg_summ_dict is None:\n                vsegs = [(lal.LIGOTimeGPS(seg[0]),\n                          lal.LIGOTimeGPS(seg[1])) \\\n                         for seg in self.valid_segments]\n            else:\n                vsegs = [(lal.LIGOTimeGPS(seg[0]),\n                          lal.LIGOTimeGPS(seg[1])) \\\n                         for seg in self.seg_summ_dict[key]]\n\n            # Add using glue library to set all segment tables\n            with ligolw_segments.LigolwSegments(outdoc, process) as x:\n                x.add(ligolw_segments.LigolwSegmentList(active=fsegs,\n                                    instruments=set([ifo]), name=name,\n                                    version=1, valid=vsegs))\n\n        # write file\n        url = urlparse.urljoin('file:', urllib.pathname2url(self.storage_path))\n        if not override_file_if_exists or not self.has_pfn(url, site='local'):\n            self.PFN(url, site='local')\n        ligolw_utils.write_filename(outdoc, self.storage_path)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef complex_median(complex_list):\n    median_real = numpy.median([complex_number.real\n                     for complex_number in complex_list])\n    median_imag = numpy.median([complex_number.imag\n                     for complex_number in complex_list])\n    return median_real + 1.j*median_imag", "response": "Calculates the median value of a list of complex numbers."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the time - domain inner product of data1 and data2 averaged over bins.", "response": "def avg_inner_product(data1, data2, bin_size):\n    \"\"\" Calculate the time-domain inner product averaged over bins.\n\n    Parameters\n    ----------\n    data1: pycbc.types.TimeSeries\n        First data set.\n    data2: pycbc.types.TimeSeries\n        Second data set, with same duration and sample rate as data1.\n    bin_size: float\n        Duration of the bins the data will be divided into to calculate\n        the inner product.\n\n    Returns\n    -------\n    inner_prod: list\n        The (complex) inner product of data1 and data2 obtained in each bin.\n    amp: float\n        The absolute value of the median of the inner product.\n    phi: float\n        The angle of the median of the inner product.\n    \"\"\"\n    assert data1.duration == data2.duration\n    assert data1.sample_rate == data2.sample_rate\n    seglen = int(bin_size * data1.sample_rate)\n    inner_prod = []\n    for idx in range(int(data1.duration / bin_size)):\n        start, end = idx * seglen, (idx+1) * seglen\n        norm = len(data1[start:end])\n        bin_prod = 2 * sum(data1.data[start:end].real *\n                           numpy.conjugate(data2.data[start:end])) / norm\n        inner_prod.append(bin_prod)\n\n    # Get the median over all bins to avoid outliers due to the presence\n    # of a signal in a particular bin.\n    inner_median = complex_median(inner_prod)\n    return inner_prod, numpy.abs(inner_median), numpy.angle(inner_median)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef line_model(freq, data, tref, amp=1, phi=0):\n    freq_line = TimeSeries(zeros(len(data)), delta_t=data.delta_t,\n                           epoch=data.start_time)\n\n    times = data.sample_times - float(tref)\n    alpha = 2 * numpy.pi * freq * times + phi\n    freq_line.data = amp * numpy.exp(1.j * alpha)\n\n    return freq_line", "response": "Simple time - domain model for a frequency line."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding the parameter of the line with frequency freq in the data.", "response": "def matching_line(freq, data, tref, bin_size=1):\n    \"\"\" Find the parameter of the line with frequency 'freq' in the data.\n\n    Parameters\n    ----------\n    freq: float\n        Frequency of the line to find in the data.\n    data: pycbc.types.TimeSeries\n        Data from which the line wants to be measured.\n    tref: float\n        Reference time for the frequency line.\n    bin_size: {1, float}, optional\n        Duration of the bins the data will be divided into for averaging.\n\n    Returns\n    -------\n    line_model: pycbc.types.TimeSeries\n        A timeseries containing the frequency line with the amplitude\n        and phase measured from the data.\n    \"\"\"\n    template_line = line_model(freq, data, tref=tref)\n    # Measure amplitude and phase of the line in the data\n    _, amp, phi = avg_inner_product(data, template_line,\n                                    bin_size=bin_size)\n    return line_model(freq, data, tref=tref, amp=amp, phi=phi)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nextract the calibration lines from strain data.", "response": "def calibration_lines(freqs, data, tref=None):\n    \"\"\" Extract the calibration lines from strain data.\n\n    Parameters\n    ----------\n    freqs: list\n        List containing the frequencies of the calibration lines.\n    data: pycbc.types.TimeSeries\n        Strain data to extract the calibration lines from.\n    tref: {None, float}, optional\n        Reference time for the line. If None, will use data.start_time.\n\n    Returns\n    -------\n    data: pycbc.types.TimeSeries\n        The strain data with the calibration lines removed.\n    \"\"\"\n    if tref is None:\n        tref = float(data.start_time)\n    for freq in freqs:\n        measured_line = matching_line(freq, data, tref,\n                                      bin_size=data.duration)\n        data -= measured_line.data.real\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nextract time-varying (wandering) lines from strain data. Parameters ---------- freqs: list List containing the frequencies of the wandering lines. data: pycbc.types.TimeSeries Strain data to extract the wandering lines from. chunk: float Duration of the chunks the data will be divided into to account for the time variation of the wandering lines. Should be smaller than data.duration, and allow for at least a few chunks. avg_bin: float Duration of the bins each chunk will be divided into for averaging the inner product when measuring the parameters of the line. Should be smaller than chunk. Returns ------- data: pycbc.types.TimeSeries The strain data with the wandering lines removed.", "response": "def clean_data(freqs, data, chunk, avg_bin):\n    \"\"\" Extract time-varying (wandering) lines from strain data.\n\n    Parameters\n    ----------\n    freqs: list\n        List containing the frequencies of the wandering lines.\n    data: pycbc.types.TimeSeries\n        Strain data to extract the wandering lines from.\n    chunk: float\n        Duration of the chunks the data will be divided into to account\n        for the time variation of the wandering lines. Should be smaller\n        than data.duration, and allow for at least a few chunks.\n    avg_bin: float\n        Duration of the bins each chunk will be divided into for averaging\n        the inner product when measuring the parameters of the line. Should\n        be smaller than chunk.\n\n    Returns\n    -------\n    data: pycbc.types.TimeSeries\n        The strain data with the wandering lines removed.\n    \"\"\"\n    if avg_bin >= chunk:\n        raise ValueError('The bin size for averaging the inner product '\n                         'must be less than the chunk size.')\n    if chunk >= data.duration:\n        raise ValueError('The chunk size must be less than the '\n                         'data duration.')\n    steps = numpy.arange(0, int(data.duration/chunk)-0.5, 0.5)\n    seglen = chunk * data.sample_rate\n\n    tref = float(data.start_time)\n    for freq in freqs:\n        for step in steps:\n            start, end = int(step*seglen), int((step+1)*seglen)\n            chunk_line = matching_line(freq, data[start:end],\n                                       tref, bin_size=avg_bin)\n\n            # Apply hann window on sides of chunk_line to smooth boundaries\n            # and avoid discontinuities\n            hann_window = numpy.hanning(len(chunk_line))\n            apply_hann = TimeSeries(numpy.ones(len(chunk_line)),\n                                    delta_t=chunk_line.delta_t,\n                                    epoch=chunk_line.start_time)\n            if step == 0:\n                apply_hann.data[len(hann_window)/2:] *= \\\n                                hann_window[len(hann_window)/2:]\n            elif step == steps[-1]:\n                apply_hann.data[:len(hann_window)/2] *= \\\n                                hann_window[:len(hann_window)/2]\n            else:\n                apply_hann.data *= hann_window\n            chunk_line.data *= apply_hann.data\n            data.data[start:end] -= chunk_line.data.real\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compute_inj_optimal_snr(workflow, inj_file, precalc_psd_files, out_dir,\n                            tags=None):\n    \"Set up a job for computing optimal SNRs of a sim_inspiral file.\"\n    if tags is None:\n        tags = []\n\n    node = Executable(workflow.cp, 'optimal_snr', ifos=workflow.ifos,\n                      out_dir=out_dir, tags=tags).create_node()\n    node.add_input_opt('--input-file', inj_file)\n    node.add_input_list_opt('--time-varying-psds', precalc_psd_files)\n    node.new_output_file_opt(workflow.analysis_time, '.xml', '--output-file')\n    workflow += node\n    return node.output_files[0]", "response": "Set up a job for computing optimal SNRs of a sim_inspiral file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cut_distant_injections(workflow, inj_file, out_dir, tags=None):\n    \"Set up a job for removing injections that are too distant to be seen\"\n    if tags is None:\n        tags = []\n\n    node = Executable(workflow.cp, 'inj_cut', ifos=workflow.ifos,\n                      out_dir=out_dir, tags=tags).create_node()\n    node.add_input_opt('--input', inj_file)\n    node.new_output_file_opt(workflow.analysis_time, '.xml', '--output-file')\n    workflow += node\n    return node.output_files[0]", "response": "Set up a job for removing injections that are too distant to be seen"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read_sampling_params_from_config(cp, section_group=None,\n                                     section='sampling_params'):\n    \"\"\"Reads sampling parameters from the given config file.\n\n    Parameters are read from the `[({section_group}_){section}]` section.\n    The options should list the variable args to transform; the parameters they\n    point to should list the parameters they are to be transformed to for\n    sampling. If a multiple parameters are transformed together, they should\n    be comma separated. Example:\n\n    .. code-block:: ini\n\n        [sampling_params]\n        mass1, mass2 = mchirp, logitq\n        spin1_a = logitspin1_a\n\n    Note that only the final sampling parameters should be listed, even if\n    multiple intermediate transforms are needed. (In the above example, a\n    transform is needed to go from mass1, mass2 to mchirp, q, then another one\n    needed to go from q to logitq.) These transforms should be specified\n    in separate sections; see ``transforms.read_transforms_from_config`` for\n    details.\n\n    Parameters\n    ----------\n    cp : WorkflowConfigParser\n        An open config parser to read from.\n    section_group : str, optional\n        Append `{section_group}_` to the section name. Default is None.\n    section : str, optional\n        The name of the section. Default is 'sampling_params'.\n\n    Returns\n    -------\n    sampling_params : list\n        The list of sampling parameters to use instead.\n    replaced_params : list\n        The list of variable args to replace in the sampler.\n    \"\"\"\n    if section_group is not None:\n        section_prefix = '{}_'.format(section_group)\n    else:\n        section_prefix = ''\n    section = section_prefix + section\n    replaced_params = set()\n    sampling_params = set()\n    for args in cp.options(section):\n        map_args = cp.get(section, args)\n        sampling_params.update(set(map(str.strip, map_args.split(','))))\n        replaced_params.update(set(map(str.strip, args.split(','))))\n    return list(sampling_params), list(replaced_params)", "response": "Reads sampling parameters from the given config file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getstats(self, names, default=numpy.nan):\n        return tuple(getattr(self, n, default) for n in names)", "response": "Get the requested stats as a tuple."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getstatsdict(self, names, default=numpy.nan):\n        return dict(zip(names, self.getstats(names, default=default)))", "response": "Get the requested stats as a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef logjacobian(self, **params):\n        return numpy.log(abs(transforms.compute_jacobian(\n            params, self.sampling_transforms, inverse=True)))", "response": "r Returns the log of the jacobian needed to transform pdfs in the base set of variables and sampling parameters."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\napply the sampling transforms to the given samples.", "response": "def apply(self, samples, inverse=False):\n        \"\"\"Applies the sampling transforms to the given samples.\n\n        Parameters\n        ----------\n        samples : dict or FieldArray\n            The samples to apply the transforms to.\n        inverse : bool, optional\n            Whether to apply the inverse transforms (i.e., go from the sampling\n            args to the ``variable_params``). Default is False.\n\n        Returns\n        -------\n        dict or FieldArray\n            The transformed samples, along with the original samples.\n        \"\"\"\n        return transforms.apply_transforms(samples, self.sampling_transforms,\n                                           inverse=inverse)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading the sampling parameters and sampling transforms from a config file.", "response": "def from_config(cls, cp, variable_params):\n        \"\"\"Gets sampling transforms specified in a config file.\n\n        Sampling parameters and the parameters they replace are read from the\n        ``sampling_params`` section, if it exists. Sampling transforms are\n        read from the ``sampling_transforms`` section(s), using\n        ``transforms.read_transforms_from_config``.\n\n        An ``AssertionError`` is raised if no ``sampling_params`` section\n        exists in the config file.\n\n        Parameters\n        ----------\n        cp : WorkflowConfigParser\n            Config file parser to read.\n        variable_params : list\n            List of parameter names of the original variable params.\n\n        Returns\n        -------\n        SamplingTransforms\n            A sampling transforms class.\n        \"\"\"\n        if not cp.has_section('sampling_params'):\n            raise ValueError(\"no sampling_params section found in config file\")\n        # get sampling transformations\n        sampling_params, replace_parameters = \\\n            read_sampling_params_from_config(cp)\n        sampling_transforms = transforms.read_transforms_from_config(\n            cp, 'sampling_transforms')\n        logging.info(\"Sampling in {} in place of {}\".format(\n            ', '.join(sampling_params), ', '.join(replace_parameters)))\n        return cls(variable_params, sampling_params,\n                   replace_parameters, sampling_transforms)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the sampling parameters.", "response": "def sampling_params(self):\n        \"\"\"Returns the sampling parameters.\n\n        If ``sampling_transforms`` is None, this is the same as the\n        ``variable_params``.\n        \"\"\"\n        if self.sampling_transforms is None:\n            sampling_params = self.variable_params\n        else:\n            sampling_params = self.sampling_transforms.sampling_params\n        return sampling_params"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update(self, **params):\n        self._current_params = self._transform_params(**params)\n        self._current_stats = ModelStats()", "response": "Updates the current parameter positions and resets the stats."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_current_stats(self, names=None):\n        if names is None:\n            names = self.default_stats\n        return self._current_stats.getstats(names)", "response": "Return one or more of the current stats as a tuple."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the logjacobian of the current parameters.", "response": "def _logjacobian(self):\n        \"\"\"Calculates the logjacobian of the current parameters.\"\"\"\n        if self.sampling_transforms is None:\n            logj = 0.\n        else:\n            logj = self.sampling_transforms.logjacobian(\n                **self.current_params)\n        return logj"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _logprior(self):\n        logj = self.logjacobian\n        logp = self.prior_distribution(**self.current_params) + logj\n        if numpy.isnan(logp):\n            logp = -numpy.inf\n        return logp", "response": "Calculates the log prior at the current parameters."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef logposterior(self):\n        logp = self.logprior\n        if logp == -numpy.inf:\n            return logp\n        else:\n            return logp + self.loglikelihood", "response": "Returns the log of the posterior of the current parameter values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef prior_rvs(self, size=1, prior=None):\n        # draw values from the prior\n        if prior is None:\n            prior = self.prior_distribution\n        p0 = prior.rvs(size=size)\n        # transform if necessary\n        if self.sampling_transforms is not None:\n            ptrans = self.sampling_transforms.apply(p0)\n            # pull out the sampling args\n            p0 = FieldArray.from_arrays([ptrans[arg]\n                                         for arg in self.sampling_params],\n                                        names=self.sampling_params)\n        return p0", "response": "Returns random variates drawn from the prior."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\napplies all transforms to the given params.", "response": "def _transform_params(self, **params):\n        \"\"\"Applies all transforms to the given params.\n\n        Parameters\n        ----------\n        \\**params :\n            Key, value pairs of parameters to apply the transforms to.\n\n        Returns\n        -------\n        dict\n            A dictionary of the transformed parameters.\n        \"\"\"\n        # apply inverse transforms to go from sampling parameters to\n        # variable args\n        if self.sampling_transforms is not None:\n            params = self.sampling_transforms.apply(params, inverse=True)\n        # apply waveform transforms\n        if self.waveform_transforms is not None:\n            params = transforms.apply_transforms(params,\n                                                 self.waveform_transforms,\n                                                 inverse=False)\n        # apply boundary conditions\n        params = self.prior_distribution.apply_boundary_conditions(**params)\n        return params"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets any additional keyword arguments from the given config file.", "response": "def extra_args_from_config(cp, section, skip_args=None, dtypes=None):\n        \"\"\"Gets any additional keyword in the given config file.\n\n        Parameters\n        ----------\n        cp : WorkflowConfigParser\n            Config file parser to read.\n        section : str\n            The name of the section to read.\n        skip_args : list of str, optional\n            Names of arguments to skip.\n        dtypes : dict, optional\n            A dictionary of arguments -> data types. If an argument is found\n            in the dict, it will be cast to the given datatype. Otherwise, the\n            argument's value will just be read from the config file (and thus\n            be a string).\n\n        Returns\n        -------\n        dict\n            Dictionary of keyword arguments read from the config file.\n        \"\"\"\n        kwargs = {}\n        if dtypes is None:\n            dtypes = {}\n        if skip_args is None:\n            skip_args = []\n        read_args = [opt for opt in cp.options(section)\n                     if opt not in skip_args]\n        for opt in read_args:\n            val = cp.get(section, opt)\n            # try to cast the value if a datatype was specified for this opt\n            try:\n                val = dtypes[opt](val)\n            except KeyError:\n                pass\n            kwargs[opt] = val\n        return kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef prior_from_config(cp, variable_params, prior_section,\n                          constraint_section):\n        \"\"\"Gets arguments and keyword arguments from a config file.\n\n        Parameters\n        ----------\n        cp : WorkflowConfigParser\n            Config file parser to read.\n        variable_params : list\n            List of of model parameter names.\n        prior_section : str\n            Section to read prior(s) from.\n        constraint_section : str\n            Section to read constraint(s) from.\n\n        Returns\n        -------\n        pycbc.distributions.JointDistribution\n            The prior.\n        \"\"\"\n        # get prior distribution for each variable parameter\n        logging.info(\"Setting up priors for each parameter\")\n        dists = distributions.read_distributions_from_config(cp, prior_section)\n        constraints = distributions.read_constraints_from_config(\n            cp, constraint_section)\n        return distributions.JointDistribution(variable_params, *dists,\n                                               constraints=constraints)", "response": "Reads the arguments and keyword arguments from a config file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninitializing an instance of this class from the given config file.", "response": "def from_config(cls, cp, **kwargs):\n        \"\"\"Initializes an instance of this class from the given config file.\n\n        Parameters\n        ----------\n        cp : WorkflowConfigParser\n            Config file parser to read.\n        \\**kwargs :\n            All additional keyword arguments are passed to the class. Any\n            provided keyword will over ride what is in the config file.\n        \"\"\"\n        args = cls._init_args_from_config(cp)\n        # get any other keyword arguments provided in the model section\n        args.update(cls.extra_args_from_config(cp, \"model\",\n                                               skip_args=['name']))\n        args.update(kwargs)\n        return cls(**args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_metadata(self, fp):\n        fp.attrs['model'] = self.name\n        fp.attrs['variable_params'] = list(self.variable_params)\n        fp.attrs['sampling_params'] = list(self.sampling_params)\n        fp.write_kwargs_to_attrs(fp.attrs, static_params=self.static_params)", "response": "Writes the metadata to the given file handler."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_cov_params(mass1, mass2, spin1z, spin2z, metricParams, fUpper,\n                   lambda1=None, lambda2=None, quadparam1=None,\n                   quadparam2=None):\n    \"\"\"\n    Function to convert between masses and spins and locations in the xi\n    parameter space. Xi = Cartesian metric and rotated to principal components.\n\n    Parameters\n    -----------\n    mass1 : float\n        Mass of heavier body.\n    mass2 : float\n        Mass of lighter body.\n    spin1z : float\n        Spin of body 1.\n    spin2z : float\n        Spin of body 2.\n    metricParams : metricParameters instance\n        Structure holding all the options for construction of the metric\n        and the eigenvalues, eigenvectors and covariance matrix\n        needed to manipulate the space.\n    fUpper : float\n        The value of fUpper to use when getting the mu coordinates from the\n        lambda coordinates. This must be a key in metricParams.evals,\n        metricParams.evecs and metricParams.evecsCV\n        (ie. we must know how to do the transformation for\n        the given value of fUpper)\n\n    Returns\n    --------\n    xis : list of floats or numpy.arrays\n        Position of the system(s) in the xi coordinate system\n    \"\"\"\n\n    # Do this by doing masses - > lambdas -> mus\n    mus = get_conv_params(mass1, mass2, spin1z, spin2z, metricParams, fUpper,\n                          lambda1=lambda1, lambda2=lambda2,\n                          quadparam1=quadparam1, quadparam2=quadparam2)\n    # and then mus -> xis\n    xis = get_covaried_params(mus, metricParams.evecsCV[fUpper])\n    return xis", "response": "Function to convert between masses and spins and locations in the xi coordinate system."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_conv_params(mass1, mass2, spin1z, spin2z, metricParams, fUpper,\n                    lambda1=None, lambda2=None, quadparam1=None,\n                    quadparam2=None):\n    \"\"\"\n    Function to convert between masses and spins and locations in the mu\n    parameter space. Mu = Cartesian metric, but not principal components.\n\n    Parameters\n    -----------\n    mass1 : float\n        Mass of heavier body.\n    mass2 : float\n        Mass of lighter body.\n    spin1z : float\n        Spin of body 1.\n    spin2z : float\n        Spin of body 2.\n    metricParams : metricParameters instance\n        Structure holding all the options for construction of the metric\n        and the eigenvalues, eigenvectors and covariance matrix\n        needed to manipulate the space.\n    fUpper : float\n        The value of fUpper to use when getting the mu coordinates from the\n        lambda coordinates. This must be a key in metricParams.evals and\n        metricParams.evecs (ie. we must know how to do the transformation for\n        the given value of fUpper)\n\n    Returns\n    --------\n    mus : list of floats or numpy.arrays\n        Position of the system(s) in the mu coordinate system\n    \"\"\"\n\n    # Do this by masses -> lambdas\n    lambdas = get_chirp_params(mass1, mass2, spin1z, spin2z,\n                               metricParams.f0, metricParams.pnOrder,\n                               lambda1=lambda1, lambda2=lambda2,\n                               quadparam1=quadparam1, quadparam2=quadparam2)\n    # and lambdas -> mus\n    mus = get_mu_params(lambdas, metricParams, fUpper)\n    return mus", "response": "Function to convert between masses and spins and locations in the mu parameter space."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_mu_params(lambdas, metricParams, fUpper):\n    lambdas = numpy.array(lambdas, copy=False)\n    # If original inputs were floats we need to make this a 2D array\n    if len(lambdas.shape) == 1:\n        resize_needed = True\n        lambdas = lambdas[:,None]\n    else:\n        resize_needed = False\n\n    evecs = metricParams.evecs[fUpper]\n    evals = metricParams.evals[fUpper]\n\n    evecs = numpy.array(evecs, copy=False)\n\n    mus = ((lambdas.T).dot(evecs)).T\n    mus = mus * numpy.sqrt(evals)[:,None]\n\n    if resize_needed:\n        mus = numpy.ndarray.flatten(mus)\n\n    return mus", "response": "Function to rotate from the lambda coefficients into the position of the mu coordinate system."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_covaried_params(mus, evecsCV):\n    mus = numpy.array(mus, copy=False)\n    # If original inputs were floats we need to make this a 2D array\n    if len(mus.shape) == 1:\n        resize_needed = True\n        mus = mus[:,None]\n    else:\n        resize_needed = False\n\n    xis = ((mus.T).dot(evecsCV)).T\n\n    if resize_needed:\n        xis = numpy.ndarray.flatten(xis)\n\n    return xis", "response": "Function to rotate from position in the mu_i coordinate system into the xi_i coordinate system"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rotate_vector(evecs, old_vector, rescale_factor, index):\n    temp = 0\n    for i in range(len(evecs)):\n        temp += (evecs[i,index] * rescale_factor) * old_vector[i]\n    return temp", "response": "Function to find the position of the system in one of the xi_i or mu_i or mu_i structures."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_point_distance(point1, point2, metricParams, fUpper):\n    aMass1 = point1[0]\n    aMass2 = point1[1]\n    aSpin1 = point1[2]\n    aSpin2 = point1[3]\n\n    bMass1 = point2[0]\n    bMass2 = point2[1]\n    bSpin1 = point2[2]\n    bSpin2 = point2[3]\n\n    aXis = get_cov_params(aMass1, aMass2, aSpin1, aSpin2, metricParams, fUpper)\n\n    bXis = get_cov_params(bMass1, bMass2, bSpin1, bSpin2, metricParams, fUpper)\n\n    dist = (aXis[0] - bXis[0])**2\n    for i in range(1,len(aXis)):\n        dist += (aXis[i] - bXis[i])**2\n\n    return dist, aXis, bXis", "response": "Function to calculate the mismatch between two points in terms\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef calc_point_dist(vsA, entryA):\n    chi_diffs = vsA - entryA\n    val = ((chi_diffs)*(chi_diffs)).sum()\n    return val", "response": "This function is used to determine the distance between two points."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfunctioning to determine if two points with differing upper frequency cutoffs have a mismatch < MMdistA for both upper frequency cutoffs.", "response": "def calc_point_dist_vary(mus1, fUpper1, mus2, fUpper2, fMap, norm_map, MMdistA):\n    \"\"\"\n    Function to determine if two points, with differing upper frequency cutoffs\n    have a mismatch < MMdistA for *both* upper frequency cutoffs.\n\n    Parameters\n    ----------\n    mus1 : List of numpy arrays\n        mus1[i] will give the array of point 1's position in the \\chi_j\n        coordinate system. The i element corresponds to varying values of the\n        upper frequency cutoff. fMap is used to map between i and actual\n        frequencies\n    fUpper1 : float\n        The upper frequency cutoff of point 1.\n    mus2 : List of numpy arrays\n        mus2[i] will give the array of point 2's position in the \\chi_j\n        coordinate system. The i element corresponds to varying values of the\n        upper frequency cutoff. fMap is used to map between i and actual\n        frequencies\n    fUpper2 : float\n        The upper frequency cutoff of point 2.\n    fMap : dictionary\n        fMap[fUpper] will give the index needed to get the \\chi_j coordinates\n        in the two sets of mus\n    norm_map : dictionary\n        norm_map[fUpper] will give the relative frequency domain template\n        amplitude (sigma) at the given value of fUpper.\n    MMdistA\n        The minimal mismatch allowed between the points\n\n    Returns\n    --------\n    Boolean\n        True if the points have a mismatch < MMdistA\n        False if the points have a mismatch > MMdistA\n    \"\"\"\n    f_upper = min(fUpper1, fUpper2)\n    f_other = max(fUpper1, fUpper2)\n    idx = fMap[f_upper]\n    vecs1 = mus1[idx]\n    vecs2 = mus2[idx]\n    val = ((vecs1 - vecs2)*(vecs1 - vecs2)).sum()\n    if (val > MMdistA):\n        return False\n    # Reduce match to account for normalization.\n    norm_fac = norm_map[f_upper] / norm_map[f_other]\n    val = 1 - (1 - val)*norm_fac\n    return (val < MMdistA)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_max_and_min_frequencies(name, mass_range_params, freqs):\n\n    cutoff_fns = pnutils.named_frequency_cutoffs\n    if name not in cutoff_fns.keys():\n        err_msg = \"%s not recognized as a valid cutoff frequency choice.\" %name\n        err_msg += \"Recognized choices: \" + \" \".join(cutoff_fns.keys())\n        raise ValueError(err_msg)\n\n    # Can I do this quickly?\n    total_mass_approxs = {\n        \"SchwarzISCO\": pnutils.f_SchwarzISCO,\n        \"LightRing\"  : pnutils.f_LightRing,\n        \"ERD\"        : pnutils.f_ERD\n    }\n    \n    if name in total_mass_approxs.keys():\n        # This can be done quickly if the cutoff only depends on total mass\n        # Assumes that lower total mass = higher cutoff frequency\n        upper_f_cutoff = total_mass_approxs[name](mass_range_params.minTotMass)\n        lower_f_cutoff = total_mass_approxs[name](mass_range_params.maxTotMass)\n    else:\n        # Do this numerically\n        # FIXME: Is 1000000 the right choice? I think so, but just highlighting\n        mass1, mass2, spin1z, spin2z = \\\n                get_random_mass(1000000, mass_range_params)\n        mass_dict = {}\n        mass_dict['mass1'] = mass1\n        mass_dict['mass2'] = mass2\n        mass_dict['spin1z'] = spin1z\n        mass_dict['spin2z'] = spin2z\n        tmp_freqs = cutoff_fns[name](mass_dict)\n        upper_f_cutoff = tmp_freqs.max()\n        lower_f_cutoff = tmp_freqs.min()\n\n    cutoffs = numpy.array([lower_f_cutoff,upper_f_cutoff])\n    if lower_f_cutoff < freqs.min():\n        warn_msg = \"WARNING: \"\n        warn_msg += \"Lowest frequency cutoff is %s Hz \" %(lower_f_cutoff,)\n        warn_msg += \"which is lower than the lowest frequency calculated \"\n        warn_msg += \"for the metric: %s Hz. \" %(freqs.min())\n        warn_msg += \"Distances for these waveforms will be calculated at \"\n        warn_msg += \"the lowest available metric frequency.\"\n        logging.warn(warn_msg)\n    if upper_f_cutoff > freqs.max():\n        warn_msg = \"WARNING: \"\n        warn_msg += \"Highest frequency cutoff is %s Hz \" %(upper_f_cutoff,)\n        warn_msg += \"which is larger than the highest frequency calculated \"\n        warn_msg += \"for the metric: %s Hz. \" %(freqs.max())\n        warn_msg += \"Distances for these waveforms will be calculated at \"\n        warn_msg += \"the largest available metric frequency.\"\n        logging.warn(warn_msg)\n    return find_closest_calculated_frequencies(cutoffs, freqs)", "response": "Find the maximum and minimum frequencies for a given frequency range."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef return_nearest_cutoff(name, mass_dict, freqs):\n    # A bypass for the redundant case\n    if len(freqs) == 1:\n        return numpy.zeros(len(mass_dict['m1']), dtype=float) + freqs[0]\n    cutoff_fns = pnutils.named_frequency_cutoffs\n    if name not in cutoff_fns.keys():\n        err_msg = \"%s not recognized as a valid cutoff frequency choice.\" %name\n        err_msg += \"Recognized choices: \" + \" \".join(cutoff_fns.keys())\n        raise ValueError(err_msg)\n    f_cutoff = cutoff_fns[name](mass_dict)\n    return find_closest_calculated_frequencies(f_cutoff, freqs)", "response": "This function calculates the specified cutoff formula for each base class and returns the nearest frequency to each cutoff from the input list of frequencies."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive a value or array of input frequencies find the closest value in the list of calculated frequencies in the metric. This function is used to find the closest value in the input_freqs in the metric.", "response": "def find_closest_calculated_frequencies(input_freqs, metric_freqs):\n    \"\"\"\n    Given a value (or array) of input frequencies find the closest values in\n    the list of frequencies calculated in the metric.\n\n    Parameters\n    -----------\n    input_freqs : numpy.array or float\n        The frequency(ies) that you want to find the closest value in\n        metric_freqs\n    metric_freqs : numpy.array\n        The list of frequencies calculated by the metric\n\n    Returns\n    --------\n    output_freqs : numpy.array or float\n        The list of closest values to input_freqs for which the metric was\n        computed\n    \"\"\"\n    try:\n        refEv = numpy.zeros(len(input_freqs),dtype=float)\n    except TypeError:\n        refEv = numpy.zeros(1, dtype=float)\n        input_freqs = numpy.array([input_freqs])\n\n    if len(metric_freqs) == 1:\n        refEv[:] = metric_freqs[0]\n        return refEv\n\n    # FIXME: This seems complicated for what is a simple operation. Is there\n    #        a simpler *and* faster way of doing this?\n    # NOTE: This function assumes a sorted list of frequencies\n    # NOTE: totmass and f_cutoff are both numpy arrays as this function is\n    #       designed so that the cutoff can be calculated for many systems\n    #       simulataneously\n    for i in range(len(metric_freqs)):\n        if i == 0:\n            # If frequency is lower than halfway between the first two entries\n            # use the first (lowest) value\n            logicArr = input_freqs < ((metric_freqs[0] + metric_freqs[1])/2.)\n        elif i == (len(metric_freqs)-1):\n            # If frequency is larger than halfway between the last two entries\n            # use the last (highest) value\n            logicArr = input_freqs > ((metric_freqs[-2] + metric_freqs[-1])/2.)\n        else:\n            # For frequencies within the range in freqs, check which points\n            # should use the frequency corresponding to index i.\n            logicArrA = input_freqs > ((metric_freqs[i-1] + metric_freqs[i])/2.)\n            logicArrB = input_freqs < ((metric_freqs[i] + metric_freqs[i+1])/2.)\n            logicArr = numpy.logical_and(logicArrA,logicArrB)\n        if logicArr.any():\n            refEv[logicArr] = metric_freqs[i]\n    return refEv"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef outspiral_loop(N):\n    # Create a 2D lattice of all points\n    X,Y = numpy.meshgrid(numpy.arange(-N,N+1), numpy.arange(-N,N+1))\n\n    # Flatten it\n    X = numpy.ndarray.flatten(X)\n    Y = numpy.ndarray.flatten(Y)\n\n    # Force to an integer\n    X = numpy.array(X, dtype=int)\n    Y = numpy.array(Y, dtype=int)\n   \n    # Calculate distances\n    G = numpy.sqrt(X**2+Y**2)\n\n    # Combine back into an array\n    out_arr = numpy.array([X,Y,G])\n   \n    # And order correctly\n    sorted_out_arr = out_arr[:,out_arr[2].argsort()]\n\n    return sorted_out_arr[:2,:].T", "response": "This function loops outwards in a 2D lattice in terms\n    of distance from a central point."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads all the injections from the files in the provided folder and returns a dictionary containing the injections for each entry in the folder.", "response": "def read_injections(sim_files, m_dist, s_dist, d_dist):\n    ''' Read all the injections from the files in the provided folder.\n        The files must belong to individual set i.e. no files that combine\n        all the injections in a run.\n        Identify injection strategies and finds parameter boundaries.\n        Collect injection according to GPS.\n\n        Parameters\n        ----------\n        sim_files: list\n           List containign names of the simulation files\n        m_dist: list\n           The mass distribution used in the simulation runs\n        s_dist: list\n           The spin distribution used in the simulation runs\n        d_dist: list\n           The distance distribution used in the simulation runs\n\n        Returns\n        -------\n        injections: dictionary\n           Contains the organized information about the injections\n    '''\n\n    injections = {}\n    min_d, max_d = 1e12, 0\n    nf = len(sim_files)\n    for i in range(nf):\n\n        key = str(i)\n        injections[key] = process_injections(sim_files[i])\n        injections[key]['file_name'] = sim_files[i]\n        injections[key]['m_dist'] = m_dist[i]\n        injections[key]['s_dist'] = s_dist[i]\n        injections[key]['d_dist'] = d_dist[i]\n\n        mass1, mass2 = injections[key]['mass1'], injections[key]['mass2']\n        distance = injections[key]['distance']\n\n        mchirp = m1m2tomch(mass1, mass2)\n        injections[key]['chirp_mass'] = mchirp\n        injections[key]['total_mass'] = mass1 + mass2\n\n        injections[key]['mtot_range'] = [min(mass1 + mass2), max(mass1 + mass2)]\n        injections[key]['m1_range'] = [min(mass1), max(mass1)]\n        injections[key]['m2_range'] = [min(mass2), max(mass2)]\n        injections[key]['d_range'] = [min(distance), max(distance)]\n\n        min_d, max_d = min(min_d, min(distance)), max(max_d, max(distance))\n\n    injections['z_range'] = [dlum_to_z(min_d), dlum_to_z(max_d)]\n\n    return injections"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nestimating the injected volume for a given set of injections and a given model.", "response": "def estimate_vt(injections, mchirp_sampler, model_pdf, **kwargs):\n    #Try including ifar threshold\n    '''Based on injection strategy and the desired astro model estimate the injected volume.\n       Scale injections and estimate sensitive volume.\n\n       Parameters\n       ----------\n       injections: dictionary\n            Dictionary obtained after reading injections from read_injections\n       mchirp_sampler: function\n            Sampler for producing chirp mass samples for the astro model.\n       model_pdf: function\n            The PDF for astro model in mass1-mass2-spin1z-spin2z space.\n            This is easily extendible to include precession\n       kwargs: key words\n            Inputs for thresholds and astrophysical models\n\n       Returns\n       -------\n       injection_chunks: dictionary\n        The input dictionary with VT and VT error included with the injections\n    '''\n\n    thr_var = kwargs.get('thr_var')\n    thr_val = kwargs.get('thr_val')\n\n    nsamples = 1000000 #Used to calculate injected astro volume\n    injections = copy.deepcopy(injections)\n    min_z, max_z = injections['z_range']\n    V = quad(contracted_dVdc, 0., max_z)[0]\n\n    z_astro = astro_redshifts(min_z, max_z, nsamples)\n    astro_lum_dist = cosmo.luminosity_distance(z_astro).value\n\n    mch_astro = np.array(mchirp_sampler(nsamples = nsamples, **kwargs))\n    mch_astro_det = mch_astro * (1. + z_astro)\n    idx_within = np.zeros(nsamples)\n\n    for key in injections.keys():\n\n        if key == 'z_range':\n            # This is repeated down again and is so\n            continue\n\n        mchirp = injections[key]['chirp_mass']\n        min_mchirp, max_mchirp = min(mchirp),  max(mchirp)\n        distance = injections[key]['distance']\n\n        if injections[key]['d_dist'] == 'uniform':\n            d_min, d_max = min(distance), max(distance)\n        elif injections[key]['d_dist'] == 'dchirp':\n            d_fid_min = min(distance / (mchirp/_mch_BNS)**(5/6.))\n            d_fid_max = max(distance / (mchirp/_mch_BNS)**(5/6.))\n\n            d_min = d_fid_min * (mch_astro_det/_mch_BNS)**(5/6.)\n            d_max = d_fid_max * (mch_astro_det/_mch_BNS)**(5/6.)\n\n        bound = np.sign((max_mchirp-mch_astro_det)*(mch_astro_det-min_mchirp))\n        bound += np.sign((d_max - astro_lum_dist)*(astro_lum_dist - d_min))\n\n        idx = np.where(bound == 2)\n        idx_within[idx] = 1\n\n    inj_V0 = 4*np.pi*V*len(idx_within[idx_within == 1])/float(nsamples)\n    injections['inj_astro_vol'] = inj_V0\n\n    # Estimate the sensitive volume\n    z_range = injections['z_range']\n    V_min = quad(contracted_dVdc, 0., z_range[0])[0]\n    V_max = quad(contracted_dVdc, 0., z_range[1])[0]\n\n    thr_falloff, i_inj, i_det, i_det_sq = [], 0, 0, 0\n    gps_min, gps_max = 1e15, 0\n    keys = injections.keys()\n    for key in keys:\n\n        if key == 'z_range' or key == 'inj_astro_vol':\n            continue\n\n        data = injections[key]\n        distance = data['distance']\n        mass1, mass2 = data['mass1'], data['mass2']\n        spin1z, spin2z = data['spin1z'], data['spin2z']\n        mchirp = data['chirp_mass']\n        gps_min = min(gps_min, min(data['end_time']))\n        gps_max = max(gps_max, max(data['end_time']))\n\n        z_inj = dlum_to_z(distance)\n        m1_sc, m2_sc = mass1/(1 + z_inj), mass2/(1 + z_inj)\n        p_out = model_pdf(m1_sc, m2_sc, spin1z, spin2z)\n        p_out *= pdf_z_astro(z_inj, V_min, V_max)\n\n        p_in = 0\n        J = cosmo.luminosity_distance(z_inj + 0.0005).value\n        J -= cosmo.luminosity_distance(z_inj - 0.0005).value\n        J = abs(J)/0.001 # A quick way to get dD_l/dz\n\n        # Sum probability of injections from j-th set for all the strategies\n        for key2 in keys:\n\n            if key2 == 'z_range' or key2 == 'inj_astro_vol':\n                continue\n\n            dt_j = injections[key2]\n            dist_j = dt_j['distance']\n            m1_j, m2_j = dt_j['mass1'], dt_j['mass2']\n            s1x_2, s2x_2 = dt_j['spin1x'], dt_j['spin2x']\n            s1y_2, s2y_2 = dt_j['spin1y'], dt_j['spin2y']\n            s1z_2, s2z_2 = dt_j['spin1z'], dt_j['spin2z']\n            s1 = np.sqrt(s1x_2**2 + s1y_2**2 + s1z_2**2)\n            s2 = np.sqrt(s2x_2**2 + s2y_2**2 + s2z_2**2)\n            mch_j = dt_j['chirp_mass']\n\n        #Get probability density for injections in mass-distance space\n            if dt_j['m_dist'] == 'totalMass':\n                lomass, himass = min(min(m1_j), min(m2_j), max(max(m1_j), max(m2_j)))\n                lomass_2, himass_2 = lomass, himass\n            elif dt_j['m_dist'] == 'componentMass' or dt_j['m_dist'] == 'log':\n                lomass, himass = min(m1_j), max(m1_j)\n                lomass_2, himass_2 = min(m2_j), max(m2_j)\n\n            if dt_j['d_dist'] == 'dchirp':\n                l_dist = min(dist_j / (mch_j/_mch_BNS)**(5/6.))\n                h_dist = max(dist_j / (mch_j/_mch_BNS)**(5/6.))\n            elif dt_j['d_dist'] == 'uniform':\n                l_dist, h_dist = min(dist_j), max(dist_j)\n\n            mdist = dt_j['m_dist']\n            prob_mass = inj_mass_pdf(mdist, mass1, mass2,\n                                          lomass, himass, lomass_2, himass_2)\n\n            ddist = dt_j['d_dist']\n            prob_dist = inj_distance_pdf(ddist, distance, l_dist,\n                                                              h_dist, mchirp)\n\n            hspin1, hspin2 = max(s1), max(s2)\n            prob_spin = inj_spin_pdf(dt_j['s_dist'], hspin1, spin1z)\n            prob_spin *= inj_spin_pdf(dt_j['s_dist'], hspin2, spin2z)\n\n            p_in += prob_mass * prob_dist * prob_spin * J * (1 + z_inj)**2\n\n        p_in[p_in == 0] = 1e12\n        p_out_in = p_out/p_in\n\n        i_inj += np.sum(p_out_in)\n        i_det += np.sum((p_out_in)[data[thr_var] > thr_val])\n        i_det_sq += np.sum((p_out_in)[data[thr_var] > thr_val]**2)\n\n        idx_thr = np.where(data[thr_var] > thr_val)\n        thrs = data[thr_var][idx_thr]\n        ratios = p_out_in[idx_thr]/max(p_out_in[idx_thr])\n        rndn = np.random.uniform(0, 1, len(ratios))\n        idx_ratio = np.where(ratios > rndn)\n        thr_falloff.append(thrs[idx_ratio])\n\n    inj_V0 = injections['inj_astro_vol']\n    injections['ninj'] = i_inj\n    injections['ndet'] = i_det\n    injections['ndetsq'] = i_det_sq\n    injections['VT'] = ((inj_V0*i_det/i_inj) * (gps_max - gps_min)/31557600)\n    injections['VT_err'] = injections['VT'] * np.sqrt(i_det_sq)/i_det\n    injections['thr_falloff'] = np.hstack(np.array(thr_falloff).flat)\n\n    return injections"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfunctioning to read in the injection file and extract the found injections and all injections", "response": "def process_injections(hdffile):\n    \"\"\"Function to read in the injection file and\n       extract the found injections and all injections\n\n       Parameters\n       ----------\n       hdffile: hdf file\n           File for which injections are to be processed\n\n       Returns\n       -------\n       data: dictionary\n           Dictionary containing injection read from the input file\n    \"\"\"\n    data = {}\n\n    with h5py.File(hdffile, 'r') as inp:\n        found_index = inp['found_after_vetoes/injection_index'][:]\n\n        for param in _save_params:\n            data[param] = inp['injections/'+param][:]\n\n        ifar = np.zeros_like(data[_save_params[0]])\n        ifar[found_index] = inp['found_after_vetoes/ifar'][:]\n\n        data['ifar'] = ifar\n\n        stat = np.zeros_like(data[_save_params[0]])\n        stat[found_index] = inp['found_after_vetoes/stat'][:]\n\n        data['stat'] = stat\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsampling the redshifts for sources with redshift independent rate using standard cosmology.", "response": "def astro_redshifts(min_z, max_z, nsamples):\n    '''Sample the redshifts for sources, with redshift\n            independent rate, using standard cosmology\n\n       Parameters\n       ----------\n       min_z: float\n            Minimum redshift\n       max_z: float\n            Maximum redshift\n       nsamples: int\n            Number of samples\n\n       Returns\n       -------\n       z_astro: array\n            nsamples of redshift, between min_z, max_z, by standard cosmology\n    '''\n\n    dz, fac = 0.001, 3.0\n    # use interpolation instead of directly estimating all the pdfz for rndz\n    V = quad(contracted_dVdc, 0., max_z)[0]\n    zbins = np.arange(min_z, max_z + dz/2., dz)\n    zcenter = (zbins[:-1] + zbins[1:]) / 2\n    pdfz = cosmo.differential_comoving_volume(zcenter).value/(1+zcenter)/V\n\n    int_pdf = interp1d(zcenter, pdfz, bounds_error=False, fill_value=0)\n\n    rndz = np.random.uniform(min_z, max_z, int(fac*nsamples))\n    pdf_zs = int_pdf(rndz)\n    maxpdf = max(pdf_zs)\n    rndn = np.random.uniform(0, 1, int(fac*nsamples)) * maxpdf\n    diff = pdf_zs - rndn\n    idx = np.where(diff > 0)\n    z_astro = rndz[idx]\n\n    np.random.shuffle(z_astro)\n    z_astro.resize(nsamples)\n\n    return z_astro"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef inj_mass_pdf(key, mass1, mass2, lomass, himass, lomass_2 = 0, himass_2 = 0):\n\n    '''Estimate the probability density based on the injection strategy\n\n       Parameters\n       ----------\n       key: string\n          Injection strategy\n       mass1: array\n          First mass of the injections\n       mass2: array\n          Second mass of the injections\n       lomass: float\n          Lower value of the mass distributions\n       himass: float\n          higher value of the mass distribution\n\n       Returns\n       -------\n       pdf: array\n          Probability density of the injections\n    '''\n\n    mass1, mass2 = np.array(mass1), np.array(mass2)\n\n    if key == 'totalMass':\n        # Returns the PDF of mass when total mass is uniformly distributed.\n        # Both the component masses have the same distribution for this case.\n\n        # Parameters\n        # ----------\n        # lomass: lower component mass\n        # himass: higher component mass\n\n        bound = np.sign((lomass + himass) - (mass1 + mass2))\n        bound += np.sign((himass - mass1)*(mass1 - lomass))\n        bound += np.sign((himass - mass2)*(mass2 - lomass))\n        idx = np.where(bound != 3)\n        pdf = 1./(himass - lomass)/(mass1 + mass2 - 2 * lomass)\n        pdf[idx] = 0\n\n        return pdf\n\n    if key == 'componentMass':\n        # Returns the PDF of mass when component mass is uniformly\n        # distributed. Component masses are independent for this case.\n\n        # Parameters\n        # ----------\n        # lomass: lower component mass\n        # himass: higher component mass\n\n        bound = np.sign((himass - mass1)*(mass1 - lomass))\n        bound += np.sign((himass_2 - mass2)*(mass2 - lomass_2))\n        idx = np.where(bound != 2)\n        pdf = np.ones_like(mass1) / (himass - lomass) / (himass_2 - lomass_2)\n        pdf[idx] = 0\n\n        return pdf\n\n    if key == 'log':\n        # Returns the PDF of mass when component mass is uniform in log.\n        # Component masses are independent for this case.\n\n        # Parameters\n        # ----------\n        # lomass: lower component mass\n        # himass: higher component mass\n\n        bound = np.sign((himass - mass1)*(mass1 - lomass))\n        bound += np.sign((himass_2 - mass2)*(mass2 - lomass_2))\n        idx = np.where(bound != 2)\n        pdf = 1 / (log(himass) - log(lomass)) / (log(himass_2) - log(lomass_2))\n        pdf /= (mass1 * mass2)\n        pdf[idx] = 0\n\n        return pdf", "response": "Estimate the probability density based on the injection strategy inj_mass_pdf."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef inj_spin_pdf(key, high_spin, spinz):\n    ''' Estimate the probability density of the\n           injections for the spin distribution.\n\n        Parameters\n        ----------\n        key: string\n          Injections strategy\n        high_spin: float\n          Maximum spin used in the strategy\n        spinz: array\n          Spin of the injections (for one component)\n    '''\n\n    # If the data comes from disable_spin simulation\n    if spinz[0] == 0:\n        return np.ones_like(spinz)\n\n    spinz = np.array(spinz)\n\n    bound = np.sign(np.absolute(high_spin) - np.absolute(spinz))\n    bound += np.sign(1 - np.absolute(spinz))\n\n    if key == 'precessing':\n        # Returns the PDF of spins when total spin is\n        # isotropically distributed. Both the component\n        # masses have the same distribution for this case.\n\n        pdf = (np.log(high_spin - np.log(abs(spinz)))/high_spin/2)\n        idx = np.where(bound != 2)\n        pdf[idx] = 0\n\n        return pdf\n\n    if key == 'aligned':\n        # Returns the PDF of mass when spins are aligned and uniformly\n        # distributed. Component spins are independent for this case.\n\n        pdf = (np.ones_like(spinz) / 2 / high_spin)\n        idx = np.where(bound != 2)\n        pdf[idx] = 0\n\n        return pdf\n\n    if key == 'disable_spin':\n        # Returns unit array\n\n        pdf = np.ones_like(spinz)\n\n        return pdf", "response": "Estimate the probability density of the injections for the spin distribution."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef inj_distance_pdf(key, distance, low_dist, high_dist, mchirp = 1):\n    ''' Estimate the probability density of the\n        injections for the distance distribution.\n\n        Parameters\n        ----------\n        key: string\n          Injections strategy\n        distance: array\n          Array of distances\n        low_dist: float\n          Lower value of distance used in the injection strategy\n        high_dist: float\n          Higher value of distance used in the injection strategy\n    '''\n\n    distance = np.array(distance)\n\n    if key == 'uniform':\n        # Returns the PDF at a distance when\n        # distance is uniformly distributed.\n\n        pdf = np.ones_like(distance)/(high_dist - low_dist)\n        bound = np.sign((high_dist - distance)*(distance - low_dist))\n        idx = np.where(bound != 1)\n        pdf[idx] = 0\n        return pdf\n\n    if key == 'dchirp':\n        # Returns the PDF at a distance when distance is uniformly\n        # distributed but scaled by the chirp mass\n\n        weight = (mchirp/_mch_BNS)**(5./6)\n        pdf = np.ones_like(distance) / weight / (high_dist - low_dist)\n        bound = np.sign((weight*high_dist - distance)*(distance - weight*low_dist))\n        idx = np.where(bound != 1)\n        pdf[idx] = 0\n        return pdf", "response": "Estimate the probability density of the injections for the distance distribution."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndetermine if any additional parameters from the InferenceFile are needed to get derived parameters that user has asked for. First it will try to add any base parameters that are required to calculate the derived parameters. Then it will add any sampling parameters that are required to calculate the base parameters needed. Parameters ---------- requested_params : list List of parameters that user wants. variable_args : list List of parameters that InferenceFile has. valid_params : list List of parameters that can be accepted. Returns ------- requested_params : list Updated list of parameters that user wants. all_c : list List of BaseTransforms to apply.", "response": "def get_common_cbc_transforms(requested_params, variable_args,\n                              valid_params=None):\n    \"\"\"Determines if any additional parameters from the InferenceFile are\n    needed to get derived parameters that user has asked for.\n\n    First it will try to add any base parameters that are required to calculate\n    the derived parameters. Then it will add any sampling parameters that are\n    required to calculate the base parameters needed.\n\n    Parameters\n    ----------\n    requested_params : list\n        List of parameters that user wants.\n    variable_args : list\n        List of parameters that InferenceFile has.\n    valid_params : list\n        List of parameters that can be accepted.\n\n    Returns\n    -------\n    requested_params : list\n        Updated list of parameters that user wants.\n    all_c : list\n        List of BaseTransforms to apply.\n    \"\"\"\n    variable_args = set(variable_args) if not isinstance(variable_args, set) \\\n                                    else variable_args\n\n    # try to parse any equations by putting all strings together\n    # this will get some garbage but ensures all alphanumeric/underscored\n    # parameter names are added\n    new_params = []\n    for opt in requested_params:\n        s = \"\"\n        for ch in opt:\n            s += ch if ch.isalnum() or ch == \"_\" else \" \"\n        new_params += s.split(\" \")\n    requested_params = set(list(requested_params) + list(new_params))\n\n    # can pass a list of valid parameters to remove garbage from parsing above\n    if valid_params:\n        valid_params = set(valid_params)\n        requested_params = requested_params.intersection(valid_params)\n\n    # find all the transforms for the requested derived parameters\n    # calculated from base parameters\n    from_base_c = []\n    for converter in common_cbc_inverse_transforms:\n        if (converter.outputs.issubset(variable_args) or\n                converter.outputs.isdisjoint(requested_params)):\n            continue\n        intersect = converter.outputs.intersection(requested_params)\n        if (not intersect or intersect.issubset(converter.inputs) or\n                intersect.issubset(variable_args)):\n            continue\n        requested_params.update(converter.inputs)\n        from_base_c.append(converter)\n\n    # find all the tranforms for the required base parameters\n    # calculated from sampling parameters\n    to_base_c = []\n    for converter in common_cbc_forward_transforms:\n        if (converter.inputs.issubset(variable_args) and\n                len(converter.outputs.intersection(requested_params)) > 0):\n            requested_params.update(converter.inputs)\n            to_base_c.append(converter)\n            variable_args.update(converter.outputs)\n\n    # get list of transforms that converts sampling parameters to the base\n    # parameters and then converts base parameters to the derived parameters\n    all_c = to_base_c + from_base_c\n\n    return list(requested_params), all_c"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\napply a list of BaseTransform instances on a mapping object.", "response": "def apply_transforms(samples, transforms, inverse=False):\n    \"\"\"Applies a list of BaseTransform instances on a mapping object.\n\n    Parameters\n    ----------\n    samples : {FieldArray, dict}\n        Mapping object to apply transforms to.\n    transforms : list\n        List of BaseTransform instances to apply. Nested transforms are assumed\n        to be in order for forward transforms.\n    inverse : bool, optional\n        Apply inverse transforms. In this case transforms will be applied in\n        the opposite order. Default is False.\n\n    Returns\n    -------\n    samples : {FieldArray, dict}\n        Mapping object with transforms applied. Same type as input.\n    \"\"\"\n    if inverse:\n        transforms = transforms[::-1]\n    for t in transforms:\n        try:\n            if inverse:\n                samples = t.inverse_transform(samples)\n            else:\n                samples = t.transform(samples)\n        except NotImplementedError:\n            continue\n    return samples"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the jacobian of the list of base transforms at the given samples.", "response": "def compute_jacobian(samples, transforms, inverse=False):\n    \"\"\"Computes the jacobian of the list of transforms at the given sample\n    points.\n\n    Parameters\n    ----------\n    samples : {FieldArray, dict}\n        Mapping object specifying points at which to compute jacobians.\n    transforms : list\n        List of BaseTransform instances to apply. Nested transforms are assumed\n        to be in order for forward transforms.\n    inverse : bool, optional\n        Compute inverse jacobians. Default is False.\n\n    Returns\n    -------\n    float :\n        The product of the jacobians of all fo the transforms.\n    \"\"\"\n    j = 1.\n    if inverse:\n        for t in transforms:\n            j *= t.inverse_jacobian(samples)\n    else:\n        for t in transforms:\n            j *= t.jacobian(samples)\n    return j"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\norders the list of transforms to ensure proper chaining.", "response": "def order_transforms(transforms):\n    \"\"\"Orders transforms to ensure proper chaining.\n\n    For example, if `transforms = [B, A, C]`, and `A` produces outputs needed\n    by `B`, the transforms will be re-rorderd to `[A, B, C]`.\n\n    Parameters\n    ----------\n    transforms : list\n        List of transform instances to order.\n\n    Outputs\n    -------\n    list :\n        List of transformed ordered such that forward transforms can be carried\n        out without error.\n    \"\"\"\n    # get a set of all inputs and all outputs\n    outputs = set().union(*[t.outputs for t in transforms])\n    out = []\n    remaining = [t for t in transforms]\n    while remaining:\n        # pull out transforms that have no inputs in the set of outputs\n        leftover = []\n        for t in remaining:\n            if t.inputs.isdisjoint(outputs):\n                out.append(t)\n                outputs -= t.outputs\n            else:\n                leftover.append(t)\n        remaining = leftover\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread the transforms from a configuration file.", "response": "def read_transforms_from_config(cp, section=\"transforms\"):\n    \"\"\"Returns a list of PyCBC transform instances for a section in the\n    given configuration file.\n\n    If the transforms are nested (i.e., the output of one transform is the\n    input of another), the returned list will be sorted by the order of the\n    nests.\n\n    Parameters\n    ----------\n    cp : WorflowConfigParser\n        An open config file to read.\n    section : {\"transforms\", string}\n        Prefix on section names from which to retrieve the transforms.\n\n    Returns\n    -------\n    list\n        A list of the parsed transforms.\n    \"\"\"\n    trans = []\n    for subsection in cp.get_subsections(section):\n        name = cp.get_opt_tag(section, \"name\", subsection)\n        t = transforms[name].from_config(cp, section, subsection)\n        trans.append(t)\n    return order_transforms(trans)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_config(cls, cp, section, outputs, skip_opts=None,\n                    additional_opts=None):\n        \"\"\"Initializes a transform from the given section.\n\n        Parameters\n        ----------\n        cp : pycbc.workflow.WorkflowConfigParser\n            A parsed configuration file that contains the transform options.\n        section : str\n            Name of the section in the configuration file.\n        outputs : str\n            The names of the parameters that are output by this transformation,\n            separated by `VARARGS_DELIM`. These must appear in the \"tag\" part\n            of the section header.\n        skip_opts : list, optional\n            Do not read options in the given list.\n        additional_opts : dict, optional\n            Any additional arguments to pass to the class. If an option is\n            provided that also exists in the config file, the value provided\n            will be used instead of being read from the file.\n\n        Returns\n        -------\n        cls\n            An instance of the class.\n        \"\"\"\n        tag = outputs\n        if skip_opts is None:\n            skip_opts = []\n        if additional_opts is None:\n            additional_opts = {}\n        else:\n            additional_opts = additional_opts.copy()\n        outputs = set(outputs.split(VARARGS_DELIM))\n        special_args = ['name'] + skip_opts + additional_opts.keys()\n        # get any extra arguments to pass to init\n        extra_args = {}\n        for opt in cp.options(\"-\".join([section, tag])):\n            if opt in special_args:\n                continue\n            # check if option can be cast as a float\n            val = cp.get_opt_tag(section, opt, tag)\n            try:\n                val = float(val)\n            except ValueError:\n                pass\n            # add option\n            extra_args.update({opt:val})\n        extra_args.update(additional_opts)\n        out = cls(**extra_args)\n        # check that the outputs matches\n        if outputs-out.outputs != set() or out.outputs-outputs != set():\n            raise ValueError(\"outputs of class do not match outputs specified \"\n                             \"in section\")\n        return out", "response": "Initializes a new instance of the class from the given section and outputs."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a scratch FieldArray to use for transforms.", "response": "def _createscratch(self, shape=1):\n        \"\"\"Creates a scratch FieldArray to use for transforms.\"\"\"\n        self._scratch = record.FieldArray(shape, dtype=[(p, float)\n            for p in self.inputs])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _copytoscratch(self, maps):\n        try:\n            for p in self.inputs:\n                self._scratch[p][:] = maps[p]\n        except ValueError:\n            # we'll get a ValueError if the scratch space isn't the same size\n            # as the maps; in that case, re-create the scratch space with the\n            # appropriate size and try again\n            invals = maps[list(self.inputs)[0]]\n            if isinstance(invals, numpy.ndarray):\n                shape = invals.shape\n            else:\n                shape = len(invals)\n            self._createscratch(shape)\n            for p in self.inputs:\n                self._scratch[p][:] = maps[p]", "response": "Copies the data in maps to the scratch space."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _getslice(self, maps):\n        invals =  maps[list(self.inputs)[0]]\n        if not isinstance(invals, (numpy.ndarray, list)):\n            getslice = 0\n        else:\n            getslice = slice(None, None)\n        return getslice", "response": "Determines how to slice the scratch for returning values."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef transform(self, maps):\n        if self.transform_functions is None:\n            raise NotImplementedError(\"no transform function(s) provided\")\n        # copy values to scratch\n        self._copytoscratch(maps)\n        # ensure that we return the same data type in each dict\n        getslice = self._getslice(maps)\n        # evaluate the functions\n        out = {p: self._scratch[func][getslice]\n               for p,func in self.transform_functions.items()}\n        return self.format_output(maps, out)", "response": "Applies the transform functions to the given maps object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading a CustomTransform from the given config file.", "response": "def from_config(cls, cp, section, outputs):\n        \"\"\"Loads a CustomTransform from the given config file.\n\n        Example section:\n\n        .. code-block:: ini\n\n            [{section}-outvar1+outvar2]\n            name = custom\n            inputs = inputvar1, inputvar2\n            outvar1 = func1(inputs)\n            outvar2 = func2(inputs)\n            jacobian = func(inputs)\n        \"\"\"\n        tag = outputs\n        outputs = set(outputs.split(VARARGS_DELIM))\n        inputs = map(str.strip,\n                     cp.get_opt_tag(section, 'inputs', tag).split(','))\n        # get the functions for each output\n        transform_functions = {}\n        for var in outputs:\n            # check if option can be cast as a float\n            func = cp.get_opt_tag(section, var, tag)\n            transform_functions[var] = func\n        s = '-'.join([section, tag])\n        if cp.has_option(s, 'jacobian'):\n            jacobian = cp.get_opt_tag(section, 'jacobian', tag)\n        else:\n            jacobian = None\n        return cls(inputs, outputs, transform_functions, jacobian=jacobian)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the Jacobian for transforming mchirp and q to mass1 and mass2.", "response": "def jacobian(self, maps):\n        \"\"\"Returns the Jacobian for transforming mchirp and q to mass1 and\n        mass2.\n        \"\"\"\n        mchirp = maps[parameters.mchirp]\n        q = maps[parameters.q]\n        return mchirp * ((1.+q)/q**3.)**(2./5)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef inverse_jacobian(self, maps):\n        m1 = maps[parameters.mass1]\n        m2 = maps[parameters.mass2]\n        return conversions.mchirp_from_mass1_mass2(m1, m2)/m2**2.", "response": "Returns the Jacobian for transforming mass1 and mass2 to\n        mchirp and q."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef jacobian(self, maps):\n        mchirp = maps[parameters.mchirp]\n        eta = maps[parameters.eta]\n        m1 = conversions.mass1_from_mchirp_eta(mchirp, eta)\n        m2 = conversions.mass2_from_mchirp_eta(mchirp, eta)\n        return mchirp * (m1 - m2) / (m1 + m2)**3", "response": "Returns the Jacobian for transforming mchirp and eta to mass1 and mass2."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef inverse_jacobian(self, maps):\n        m1 = maps[parameters.mass1]\n        m2 = maps[parameters.mass2]\n        mchirp = conversions.mchirp_from_mass1_mass2(m1, m2)\n        eta = conversions.eta_from_mass1_mass2(m1, m2)\n        return -1. * mchirp / eta**(6./5)", "response": "Returns the Jacobian for transforming mass1 and mass2 to\n        mchirp and eta."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef transform(self, maps):\n        out = {}\n        out[parameters.distance] = \\\n                conversions.distance_from_chirp_distance_mchirp(\n                                                    maps[parameters.chirp_distance],\n                                                    maps[parameters.mchirp],\n                                                    ref_mass=self.ref_mass)\n        return self.format_output(maps, out)", "response": "This function transforms from chirp distance to luminosity distance given the chirp mass."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef inverse_transform(self, maps):\n        out = {}\n        out[parameters.chirp_distance] = \\\n                conversions.chirp_distance(maps[parameters.distance],\n                                            maps[parameters.mchirp], ref_mass=self.ref_mass)\n        return self.format_output(maps, out)", "response": "This function transforms from luminosity distance to chirp distance given the chirp mass."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef transform(self, maps):\n        a, az, po = self._inputs\n        data = coordinates.spherical_to_cartesian(maps[a], maps[az], maps[po])\n        out = {param : val for param, val in zip(self._outputs, data)}\n        return self.format_output(maps, out)", "response": "This function transforms from spherical to cartesian spins."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef transform(self, maps):\n        out = {parameters.redshift : cosmology.redshift(\n                                                    maps[parameters.distance])}\n        return self.format_output(maps, out)", "response": "This function transforms from distance to redshift."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef inverse_transform(self, maps):\n        mass1 = maps[parameters.mass1]\n        spin1z = maps[parameters.spin1z]\n        mass2 = maps[parameters.mass2]\n        spin2z = maps[parameters.spin2z]\n        out = {\n            parameters.chi_eff : conversions.chi_eff(mass1, mass2,\n                                                     spin1z, spin2z),\n            \"chi_a\" : conversions.chi_a(mass1, mass2, spin1z, spin2z),\n        }\n        return self.format_output(maps, out)", "response": "This function transforms from component masses and cartesian spins\n            to mass - weighted spin parameters aligned with the angular momentum."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef transform(self, maps):\n\n        # find primary and secondary masses\n        # since functions in conversions.py map to primary/secondary masses\n        m_p = conversions.primary_mass(maps[\"mass1\"], maps[\"mass2\"])\n        m_s = conversions.secondary_mass(maps[\"mass1\"], maps[\"mass2\"])\n\n        # find primary and secondary xi\n        # can re-purpose spin functions for just a generic variable\n        xi_p = conversions.primary_spin(maps[\"mass1\"], maps[\"mass2\"],\n                                        maps[\"xi1\"], maps[\"xi2\"])\n        xi_s = conversions.secondary_spin(maps[\"mass1\"], maps[\"mass2\"],\n                                          maps[\"xi1\"], maps[\"xi2\"])\n\n        # convert using convention of conversions.py that is mass1 > mass2\n        spinx_p = conversions.spin1x_from_xi1_phi_a_phi_s(\n                           xi_p, maps[\"phi_a\"], maps[\"phi_s\"])\n        spiny_p = conversions.spin1y_from_xi1_phi_a_phi_s(\n                           xi_p, maps[\"phi_a\"], maps[\"phi_s\"])\n        spinx_s = conversions.spin2x_from_mass1_mass2_xi2_phi_a_phi_s(\n                           m_p, m_s, xi_s, maps[\"phi_a\"], maps[\"phi_s\"])\n        spiny_s = conversions.spin2y_from_mass1_mass2_xi2_phi_a_phi_s(\n                           m_p, m_s, xi_s, maps[\"phi_a\"], maps[\"phi_s\"])\n\n        # map parameters from primary/secondary to indices\n        out = {}\n        if isinstance(m_p, numpy.ndarray):\n            mass1, mass2 = map(numpy.array, [maps[\"mass1\"], maps[\"mass2\"]])\n            mask_mass1_gte_mass2 = mass1 >= mass2\n            mask_mass1_lt_mass2 = mass1 < mass2\n            out[parameters.spin1x] = numpy.concatenate((\n                                        spinx_p[mask_mass1_gte_mass2],\n                                        spinx_s[mask_mass1_lt_mass2]))\n            out[parameters.spin1y] = numpy.concatenate((\n                                        spiny_p[mask_mass1_gte_mass2],\n                                        spiny_s[mask_mass1_lt_mass2]))\n            out[parameters.spin2x] = numpy.concatenate((\n                                        spinx_p[mask_mass1_lt_mass2],\n                                        spinx_s[mask_mass1_gte_mass2]))\n            out[parameters.spin2y] = numpy.concatenate((\n                                        spinx_p[mask_mass1_lt_mass2],\n                                        spinx_s[mask_mass1_gte_mass2]))\n        elif maps[\"mass1\"] > maps[\"mass2\"]:\n            out[parameters.spin1x] = spinx_p\n            out[parameters.spin1y] = spiny_p\n            out[parameters.spin2x] = spinx_s\n            out[parameters.spin2y] = spiny_s\n        else:\n            out[parameters.spin1x] = spinx_s\n            out[parameters.spin1y] = spiny_s\n            out[parameters.spin2x] = spinx_p\n            out[parameters.spin2y] = spiny_p\n\n        return self.format_output(maps, out)", "response": "This function transforms from mass - weighted spins to caretsian spins."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef inverse_transform(self, maps):\n\n        # convert\n        out = {}\n        xi1 = conversions.primary_xi(\n                             maps[parameters.mass1], maps[parameters.mass2],\n                             maps[parameters.spin1x], maps[parameters.spin1y],\n                             maps[parameters.spin2x], maps[parameters.spin2y])\n        xi2 = conversions.secondary_xi(\n                             maps[parameters.mass1], maps[parameters.mass2],\n                             maps[parameters.spin1x], maps[parameters.spin1y],\n                             maps[parameters.spin2x], maps[parameters.spin2y])\n        out[\"phi_a\"] = conversions.phi_a(\n                             maps[parameters.mass1], maps[parameters.mass2],\n                             maps[parameters.spin1x], maps[parameters.spin1y],\n                             maps[parameters.spin2x], maps[parameters.spin2y])\n        out[\"phi_s\"] = conversions.phi_s(\n                             maps[parameters.spin1x], maps[parameters.spin1y],\n                             maps[parameters.spin2x], maps[parameters.spin2y])\n\n        # map parameters from primary/secondary to indices\n        if isinstance(xi1, numpy.ndarray):\n            mass1, mass2 = map(numpy.array, [maps[parameters.mass1],\n                                             maps[parameters.mass2]])\n            mask_mass1_gte_mass2 = mass1 >= mass2\n            mask_mass1_lt_mass2 = mass1 < mass2\n            out[\"xi1\"] = numpy.concatenate((\n                                        xi1[mask_mass1_gte_mass2],\n                                        xi2[mask_mass1_lt_mass2]))\n            out[\"xi2\"] = numpy.concatenate((\n                                        xi1[mask_mass1_gte_mass2],\n                                        xi2[mask_mass1_lt_mass2]))\n        elif maps[\"mass1\"] > maps[\"mass2\"]:\n            out[\"xi1\"] = xi1\n            out[\"xi2\"] = xi2\n        else:\n            out[\"xi1\"] = xi2\n            out[\"xi2\"] = xi1\n\n        return self.format_output(maps, out)", "response": "This function transforms from component masses and cartesian spins perpendicular with the angular momentum."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef transform(self, maps):\n        out = {}\n        out[\"chi_p\"] = conversions.chi_p(\n                             maps[parameters.mass1], maps[parameters.mass2],\n                             maps[parameters.spin1x], maps[parameters.spin1y],\n                             maps[parameters.spin2x], maps[parameters.spin2y])\n        return self.format_output(maps, out)", "response": "This function transforms from component masses and caretsian spins\n        to chi_p."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef logit(x, a=0., b=1.):\n        return numpy.log(x-a) - numpy.log(b-x)", "response": "r Returns the logit function with domain a and b."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef logistic(x, a=0., b=1.):\n        expx = numpy.exp(x)\n        return (a + b*expx)/(1. + expx)", "response": "r Computes the logistic function with range a and b."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef transform(self, maps):\n        x = maps[self._inputvar]\n        # check that x is in bounds\n        isin = self._bounds.__contains__(x)\n        if isinstance(isin, numpy.ndarray):\n            isin = isin.all()\n        if not isin:\n            raise ValueError(\"one or more values are not in bounds\")\n        out = {self._outputvar : self.logit(x, self._a, self._b)}\n        return self.format_output(maps, out)", "response": "r Returns a dictionary of the result of this transform."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef jacobian(self, maps):\n        x = maps[self._inputvar]\n        # check that x is in bounds\n        isin = self._bounds.__contains__(x)\n        if isinstance(isin, numpy.ndarray) and not isin.all():\n            raise ValueError(\"one or more values are not in bounds\")\n        elif not isin:\n            raise ValueError(\"{} is not in bounds\".format(x))\n        return (self._b - self._a)/((x - self._a)*(self._b - x))", "response": "r Returns the Jacobian of the variable at the given point."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef inverse_jacobian(self, maps):\n        x = maps[self._outputvar]\n        expx = numpy.exp(x)\n        return expx * (self._b - self._a) / (1. + expx)**2.", "response": "r Returns the inverse Jacobian of the logistic transform of the logistic function at the given point."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_config(cls, cp, section, outputs, skip_opts=None,\n                    additional_opts=None):\n        \"\"\"Initializes a Logistic transform from the given section.\n\n        The section must specify an input and output variable name. The\n        codomain of the output may be specified using `min-{output}`,\n        `max-{output}`. Example:\n\n        .. code-block:: ini\n\n            [{section}-q]\n            name = logistic\n            inputvar = logitq\n            outputvar = q\n            min-q = 1\n            max-q = 8\n\n        Parameters\n        ----------\n        cp : pycbc.workflow.WorkflowConfigParser\n            A parsed configuration file that contains the transform options.\n        section : str\n            Name of the section in the configuration file.\n        outputs : str\n            The names of the parameters that are output by this transformation,\n            separated by `VARARGS_DELIM`. These must appear in the \"tag\" part\n            of the section header.\n        skip_opts : list, optional\n            Do not read options in the given list.\n        additional_opts : dict, optional\n            Any additional arguments to pass to the class. If an option is\n            provided that also exists in the config file, the value provided\n            will be used instead of being read from the file.\n\n        Returns\n        -------\n        cls\n            An instance of the class.\n        \"\"\"\n        # pull out the minimum, maximum values of the output variable\n        outputvar = cp.get_opt_tag(section, 'output', outputs)\n        if skip_opts is None:\n            skip_opts = []\n        if additional_opts is None:\n            additional_opts = {}\n        else:\n            additional_opts = additional_opts.copy()\n        s = '-'.join([section, outputs])\n        opt = 'min-{}'.format(outputvar)\n        if cp.has_option(s, opt):\n            a = cp.get_opt_tag(section, opt, outputs)\n            skip_opts.append(opt)\n        else:\n            a = None\n        opt = 'max-{}'.format(outputvar)\n        if cp.has_option(s, opt):\n            b = cp.get_opt_tag(section, opt, outputs)\n            skip_opts.append(opt)\n        else:\n            b = None\n        if a is None and b is not None or b is None and a is not None:\n            raise ValueError(\"if providing a min(max)-{}, must also provide \"\n                             \"a max(min)-{}\".format(outputvar, outputvar))\n        elif a is not None:\n            additional_opts.update({'codomain': (float(a), float(b))})\n        return super(Logistic, cls).from_config(cp, section, outputs,\n                                                skip_opts, additional_opts)", "response": "Initializes a Logistic transform from the given section and outputs."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _logpdf(self, **kwargs):\n        if kwargs in self:\n            return sum([self._lognorm[p] +\n                        self._expnorm[p]*(kwargs[p]-self._mean[p])**2.\n                        for p in self._params])\n        else:\n            return -numpy.inf", "response": "Returns the log of the pdf at the given values."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rvs(self, size=1, param=None):\n        if param is not None:\n            dtype = [(param, float)]\n        else:\n            dtype = [(p, float) for p in self.params]\n        arr = numpy.zeros(size, dtype=dtype)\n        for (p,_) in dtype:\n            sigma = numpy.sqrt(self._var[p])\n            mu = self._mean[p]\n            a,b = self._bounds[p]\n            arr[p][:] = scipy.stats.truncnorm.rvs((a-mu)/sigma, (b-mu)/sigma,\n                loc=self._mean[p], scale=sigma, size=size)\n        return arr", "response": "Gives a set of random values drawn from this distribution."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_options_from_group(option_group):\n    option_list = option_group._group_actions\n    command_lines = []\n    for option in option_list:\n        option_strings = option.option_strings\n        for string in option_strings:\n            if string.startswith('--'):\n                command_lines.append(string)\n    return command_lines", "response": "Take an option group and return all the options that are defined in that option group."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef insert_base_bank_options(parser):\n\n    def match_type(s):\n        err_msg = \"must be a number between 0 and 1 excluded, not %r\" % s\n        try:\n            value = float(s)\n        except ValueError:\n            raise argparse.ArgumentTypeError(err_msg)\n        if value <= 0 or value >= 1:\n            raise argparse.ArgumentTypeError(err_msg)\n        return value\n\n    parser.add_argument(\n            '-m', '--min-match', type=match_type, required=True,\n            help=\"Generate bank with specified minimum match. Required.\")\n    parser.add_argument(\n            '-O', '--output-file', required=True,\n            help=\"Output file name. Required.\")\n    parser.add_argument('--f-low-column', type=str, metavar='NAME',\n                        help='If given, store the lower frequency cutoff into '\n                             'column NAME of the single-inspiral table.')", "response": "Adds essential common options for template bank generation to an ArgumentParser instance."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef insert_metric_calculation_options(parser):\n    metricOpts = parser.add_argument_group(\n                \"Options related to calculating the parameter space metric\")\n    metricOpts.add_argument(\"--pn-order\", action=\"store\", type=str,\n                required=True,\n                help=\"Determines the PN order to use.  For a bank of \"\n                     \"non-spinning templates, spin-related terms in the \"\n                     \"metric will be zero.  REQUIRED. \"\n                     \"Choices: %s\" %(pycbcValidOrdersHelpDescriptions))\n    metricOpts.add_argument(\"--f0\", action=\"store\", type=positive_float,\n                default=70.,\\\n                help=\"f0 is used as a dynamic scaling factor when \"\n                     \"calculating integrals used in metric construction.  \"\n                     \"I.e. instead of integrating F(f) we integrate F(f/f0) \"\n                     \"then rescale by powers of f0.  The default value 70Hz \"\n                     \"should be fine for most applications.  OPTIONAL. \"\n                     \"UNITS=Hz. **WARNING: If the ethinca metric is to be \"\n                     \"calculated, f0 must be set equal to f-low**\")\n    metricOpts.add_argument(\"--f-low\", action=\"store\", type=positive_float,\n                required=True,\n                help=\"Lower frequency cutoff used in computing the \"\n                     \"parameter space metric.  REQUIRED. UNITS=Hz\")\n    metricOpts.add_argument(\"--f-upper\", action=\"store\", type=positive_float,\n                required=True,\n                help=\"Upper frequency cutoff used in computing the \"\n                     \"parameter space metric.  REQUIRED. UNITS=Hz\")\n    metricOpts.add_argument(\"--delta-f\", action=\"store\", type=positive_float,\n                required=True,\n                help=\"Frequency spacing used in computing the parameter \"\n                     \"space metric:  integrals of the form \\int F(f) df \"\n                     \"are approximated as \\sum F(f) delta_f.  REQUIRED. \"\n                     \"UNITS=Hz\")\n    metricOpts.add_argument(\"--write-metric\", action=\"store_true\",\n                default=False, help=\"If given write the metric components \"\n                     \"to disk as they are calculated.\")\n    return metricOpts", "response": "Adds the options used to obtain a metric in the bank generation codes for the current order of the bank generation codes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding the options related to mass and spin for bank generation codes to an object parser.", "response": "def insert_mass_range_option_group(parser,nonSpin=False):\n    \"\"\"\n    Adds the options used to specify mass ranges in the bank generation codes\n    to an argparser as an OptionGroup. This should be used if you\n    want to use these options in your code.\n \n    Parameters\n    -----------\n    parser : object\n        OptionParser instance.\n    nonSpin : boolean, optional (default=False)\n        If this is provided the spin-related options will not be added.\n    \"\"\"\n    massOpts = parser.add_argument_group(\"Options related to mass and spin \"\n                  \"limits for bank generation\")\n    massOpts.add_argument(\"--min-mass1\", action=\"store\", type=positive_float,\n                  required=True, \n                  help=\"Minimum mass1: must be >= min-mass2. \"\n                       \"REQUIRED. UNITS=Solar mass\")\n    massOpts.add_argument(\"--max-mass1\", action=\"store\", type=positive_float,\n                  required=True,\n                  help=\"Maximum mass1: must be >= max-mass2. \"\n                       \"REQUIRED. UNITS=Solar mass\")\n    massOpts.add_argument(\"--min-mass2\", action=\"store\", type=positive_float,\n                  required=True,\n                  help=\"Minimum mass2. REQUIRED. UNITS=Solar mass\")\n    massOpts.add_argument(\"--max-mass2\", action=\"store\", type=positive_float,\n                  required=True,\n                  help=\"Maximum mass2. REQUIRED. UNITS=Solar mass\")\n    massOpts.add_argument(\"--max-total-mass\", action=\"store\",\n                          type=positive_float, default=None, \n                          help=\"Maximum total mass. OPTIONAL, if not provided \"\n                          \"the max total mass is determined by the component \"\n                          \"masses. UNITS=Solar mass\")\n    massOpts.add_argument(\"--min-total-mass\", action=\"store\",\n                          type=positive_float, default=None, \n                  help=\"Minimum total mass. OPTIONAL, if not provided the \"\n                       \"min total mass is determined by the component masses.\"\n                       \" UNITS=Solar mass\")\n    massOpts.add_argument(\"--max-chirp-mass\", action=\"store\",\n                          type=positive_float, default=None,\n                  help=\"Maximum chirp mass. OPTIONAL, if not provided the \"\n                       \"max chirp mass is determined by the component masses.\"\n                       \" UNITS=Solar mass\")\n    massOpts.add_argument(\"--min-chirp-mass\", action=\"store\",\n                          type=positive_float, default=None,\n                  help=\"Minimum total mass. OPTIONAL, if not provided the \"\n                       \"min chirp mass is determined by the component masses.\"\n                       \" UNITS=Solar mass\")\n    massOpts.add_argument(\"--max-eta\", action=\"store\", type=positive_float,\n                  default=0.25,\n                  help=\"Maximum symmetric mass ratio. OPTIONAL, no upper bound\"\n                       \" on eta will be imposed if not provided. \"\n                       \"UNITS=Solar mass.\")\n    massOpts.add_argument(\"--min-eta\", action=\"store\", type=nonnegative_float,\n                  default=0.,\n                  help=\"Minimum symmetric mass ratio. OPTIONAL, no lower bound\"\n                       \" on eta will be imposed if not provided. \"\n                       \"UNITS=Solar mass.\")\n    massOpts.add_argument(\"--ns-eos\", action=\"store\",\n                  default=None,\n                  help=\"Select the EOS to be used for the NS when calculating \"\n                       \"the remnant disk mass. Only 2H is currently supported. \"\n                       \"OPTIONAL\")\n    massOpts.add_argument(\"--remnant-mass-threshold\", action=\"store\",\n                          type=nonnegative_float, default=None,\n                  help=\"Setting this filters EM dim NS-BH binaries: if the \"\n                       \"remnant disk mass does not exceed this value, the NS-BH \"\n                       \"binary is dropped from the target parameter space. \"\n                       \"When it is set to None (default value) the EM dim \"\n                       \"filter is not activated. OPTIONAL\")\n    massOpts.add_argument(\"--use-eos-max-ns-mass\", action=\"store_true\", default=False,\n                  help=\"Cut the mass range of the smaller object to the maximum \"\n                       \"mass allowed by EOS. \"\n                       \"OPTIONAL\")\n    massOpts.add_argument(\"--delta-bh-spin\", action=\"store\",\n                          type=positive_float, default=None,\n                  help=\"Grid spacing used for the BH spin z component when \"\n                       \"generating the surface of the minumum minimum symmetric \"\n                       \"mass ratio as a function of BH spin and NS mass required \"\n                       \"to produce a remnant disk mass that exceeds the threshold \"\n                       \"specificed in --remnant-mass-threshold. \"\n                       \"OPTIONAL (0.1 by default) \")\n    massOpts.add_argument(\"--delta-ns-mass\", action=\"store\",\n                          type=positive_float, default=None,\n                  help=\"Grid spacing used for the NS mass when generating the \"\n                       \"surface of the minumum minimum symmetric mass ratio as \"\n                       \"a function of BH spin and NS mass required to produce \"\n                       \"a remnant disk mass that exceeds the thrsehold specified \"\n                       \"in --remnant-mass-threshold. \"\n                       \"OPTIONAL (0.1 by default) \")\n    if nonSpin:\n        parser.add_argument_group(massOpts)\n        return massOpts\n\n    massOpts.add_argument(\"--max-ns-spin-mag\", action=\"store\",\n                          type=nonnegative_float, default=None,\n                  help=\"Maximum neutron star spin magnitude.  Neutron stars \"\n                       \"are defined as components lighter than the NS-BH \"\n                       \"boundary (3 Msun by default). REQUIRED if min-mass2 \"\n                       \"< ns-bh-boundary-mass\")\n    massOpts.add_argument(\"--max-bh-spin-mag\", action=\"store\",\n                          type=nonnegative_float, default=None,\n                  help=\"Maximum black hole spin magnitude.  Black holes are \"\n                       \"defined as components at or above the NS-BH boundary \"\n                       \"(3 Msun by default). REQUIRED if max-mass1 >= \"\n                       \"ns-bh-boundary-mass\")\n    # Mutually exclusive group prevents both options being set on command line\n    # If --nsbh-flag is True then spinning bank generation must ignore the\n    # default value of ns-bh-boundary-mass.\n    action = massOpts.add_mutually_exclusive_group(required=False)\n    action.add_argument(\"--ns-bh-boundary-mass\", action='store',\n                        type=positive_float,\n                  help=\"Mass boundary between neutron stars and black holes. \"\n                       \"Components below this mass are considered neutron \"\n                       \"stars and are subject to the neutron star spin limits. \"\n                       \"Components at/above are subject to the black hole spin \"\n                       \"limits.  OPTIONAL, default=%f.  UNITS=Solar mass\" \\\n                       % massRangeParameters.default_nsbh_boundary_mass)\n    action.add_argument(\"--nsbh-flag\", action=\"store_true\", default=False,\n                  help=\"Set this flag if generating a bank that contains only \"\n                       \"systems with 1 black hole and 1 neutron star. With \"\n                       \"this flag set the heavier body will always be subject \"\n                       \"to the black hole spin restriction and the lighter \"\n                       \"to the neutron star spin restriction, regardless of \"\n                       \"mass.  OPTIONAL.  If set, the value of \"\n                       \"--ns-bh-boundary-mass will be ignored.\")\n    return massOpts"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nverifying that the given options are correct.", "response": "def verify_mass_range_options(opts, parser, nonSpin=False):\n    \"\"\"\n    Parses the metric calculation options given and verifies that they are\n    correct.\n\n    Parameters\n    ----------\n    opts : argparse.Values instance\n        Result of parsing the input options with OptionParser\n    parser : object\n        The OptionParser instance.\n    nonSpin : boolean, optional (default=False)\n        If this is provided the spin-related options will not be checked.\n    \"\"\"\n    # Mass1 must be the heavier!\n    if opts.min_mass1 < opts.min_mass2:\n        parser.error(\"min-mass1 cannot be less than min-mass2!\")\n    if opts.max_mass1 < opts.max_mass2:\n        parser.error(\"max-mass1 cannot be less than max-mass2!\")\n    # If given are min/max total mass/chirp mass possible?\n    if opts.min_total_mass \\\n            and (opts.min_total_mass > opts.max_mass1 + opts.max_mass2):\n        err_msg = \"Supplied minimum total mass %f \" %(opts.min_total_mass,)\n        err_msg += \"greater than the sum of the two max component masses \"\n        err_msg += \" %f and %f.\" %(opts.max_mass1,opts.max_mass2)\n        parser.error(err_msg)\n    if opts.max_total_mass \\\n            and (opts.max_total_mass < opts.min_mass1 + opts.min_mass2):\n        err_msg = \"Supplied maximum total mass %f \" %(opts.max_total_mass,)\n        err_msg += \"smaller than the sum of the two min component masses \"\n        err_msg += \" %f and %f.\" %(opts.min_mass1,opts.min_mass2)\n        parser.error(err_msg)\n    if opts.max_total_mass and opts.min_total_mass \\\n            and (opts.max_total_mass < opts.min_total_mass):\n        parser.error(\"Min total mass must be larger than max total mass.\")\n    # Warn the user that his/her setup is such that EM dim NS-BH binaries\n    # will not be targeted by the template bank that is being built.  Also\n    # inform him/her about the caveats involved in this.\n    if hasattr(opts, 'remnant_mass_threshold') \\\n            and opts.remnant_mass_threshold is not None:\n        logging.info(\"\"\"You have asked to exclude EM dim NS-BH systems from the\n                        target parameter space. The script will assume that m1 is\n                        the BH and m2 is the NS: make sure that your settings\n                        respect this convention. The script will also treat the\n                        NS as non-spinning: use NS spins in the template bank\n                        at your own risk!\"\"\")\n        if opts.use_eos_max_ns_mass:\n            logging.info(\"\"\"You have asked to take into account the maximum NS\n                        mass value for the EOS in use.\"\"\")\n        # Find out if the EM constraint surface data already exists or not\n        # and inform user whether this will be read from file or generated.\n        # This is the minumum eta as a function of BH spin and NS mass\n        # required to produce an EM counterpart\n        if os.path.isfile('constraint_em_bright.npz'):\n            logging.info(\"\"\"The constraint surface for EM bright binaries\n                        will be read in from constraint_em_bright.npz.\"\"\")\n\n    # Assign min/max total mass from mass1, mass2 if not specified\n    if (not opts.min_total_mass) or \\\n            ((opts.min_mass1 + opts.min_mass2) > opts.min_total_mass):\n        opts.min_total_mass = opts.min_mass1 + opts.min_mass2\n    if (not opts.max_total_mass) or \\\n            ((opts.max_mass1 + opts.max_mass2) < opts.max_total_mass):\n        opts.max_total_mass = opts.max_mass1 + opts.max_mass2\n\n    # It is vital that min and max total mass be set correctly.\n    # This is becasue the heavily-used function get_random_mass will place\n    # points first in total mass (to some power), and then in eta. If the total\n    # mass limits are not well known ahead of time it will place unphysical\n    # points and fail.\n    # This test is a bit convoluted as we identify the maximum and minimum\n    # possible total mass from chirp mass and/or eta restrictions.\n    if opts.min_chirp_mass is not None:\n        # Need to get the smallest possible min_tot_mass from this chirp mass\n        # There are 4 possibilities for where the min_tot_mass is found on the\n        # line of min_chirp_mass that interacts with the component mass limits.\n        # Either it is found at max_m2, or at min_m1, or it starts on the equal\n        # mass line within the parameter space, or it doesn't intersect\n        # at all.\n        # First let's get the masses at both of these possible points\n        m1_at_max_m2 = pnutils.mchirp_mass1_to_mass2(opts.min_chirp_mass,\n                                                     opts.max_mass2)\n        if m1_at_max_m2 < opts.max_mass2:\n            # Unphysical, remove\n            m1_at_max_m2 = -1\n        m2_at_min_m1 = pnutils.mchirp_mass1_to_mass2(opts.min_chirp_mass,\n                                                      opts.min_mass1)\n        if m2_at_min_m1 > opts.min_mass1:\n            # Unphysical, remove\n            m2_at_min_m1 = -1\n        # Get the values on the equal mass line\n        m1_at_equal_mass, m2_at_equal_mass = pnutils.mchirp_eta_to_mass1_mass2(\n                                                     opts.min_chirp_mass, 0.25)\n\n        # Are any of these possible?\n        if m1_at_max_m2 <= opts.max_mass1 and m1_at_max_m2 >= opts.min_mass1:\n            min_tot_mass = opts.max_mass2 + m1_at_max_m2\n        elif m2_at_min_m1 <= opts.max_mass2 and m2_at_min_m1 >= opts.min_mass2:\n            min_tot_mass = opts.min_mass1 + m2_at_min_m1\n        elif m1_at_equal_mass <= opts.max_mass1 and \\\n                 m1_at_equal_mass >= opts.min_mass1 and \\\n                 m2_at_equal_mass <= opts.max_mass2 and \\\n                 m2_at_equal_mass >= opts.min_mass2:\n            min_tot_mass = m1_at_equal_mass + m2_at_equal_mass\n        # So either the restriction is low enough to be redundant, or is\n        # removing all the parameter space\n        elif m2_at_min_m1 < opts.min_mass2:\n            # This is the redundant case, ignore\n            min_tot_mass = opts.min_total_mass\n        else:\n            # And this is the bad case\n            err_msg = \"The minimum chirp mass provided is not possible given \"\n            err_msg += \"restrictions on component masses.\"\n            raise ValueError(err_msg)\n        # Is there also an eta restriction?\n        if opts.max_eta:\n            # Get the value of m1,m2 at max_eta, min_chirp_mass\n            max_eta_m1, max_eta_m2 = pnutils.mchirp_eta_to_mass1_mass2(\n                                         opts.min_chirp_mass, opts.max_eta)\n            max_eta_min_tot_mass = max_eta_m1 + max_eta_m2\n            if max_eta_min_tot_mass > min_tot_mass:\n                # Okay, eta does restrict this further. Still physical?\n                min_tot_mass = max_eta_min_tot_mass\n                if max_eta_m1 > opts.max_mass1:\n                    err_msg = \"The combination of component mass, chirp \"\n                    err_msg += \"mass, eta and (possibly) total mass limits \"\n                    err_msg += \"have precluded all systems.\"\n                    raise ValueError(err_msg)\n        # Update min_tot_mass if needed\n        if min_tot_mass > opts.min_total_mass:\n            opts.min_total_mass = float(min_tot_mass)\n\n    # Then need to do max_chirp_mass and min_eta\n    if opts.max_chirp_mass is not None:\n        # Need to get the largest possible maxn_tot_mass from this chirp mass\n        # There are 3 possibilities for where the max_tot_mass is found on the\n        # line of max_chirp_mass that interacts with the component mass limits.\n        # Either it is found at min_m2, or at max_m1, or it doesn't intersect\n        # at all.\n        # First let's get the masses at both of these possible points\n        m1_at_min_m2 = pnutils.mchirp_mass1_to_mass2(opts.max_chirp_mass,\n                                                     opts.min_mass2)\n        m2_at_max_m1 = pnutils.mchirp_mass1_to_mass2(opts.max_chirp_mass,\n                                                      opts.max_mass1)\n        # Are either of these possible?\n        if m1_at_min_m2 <= opts.max_mass1 and m1_at_min_m2 >= opts.min_mass1:\n            max_tot_mass = opts.min_mass2 + m1_at_min_m2\n        elif m2_at_max_m1 <= opts.max_mass2 and m2_at_max_m1 >= opts.min_mass2:\n            max_tot_mass = opts.max_mass1 + m2_at_max_m1\n        # So either the restriction is low enough to be redundant, or is\n        # removing all the paramter space\n        elif m2_at_max_m1 > opts.max_mass2:\n            # This is the redundant case, ignore\n            max_tot_mass = opts.max_total_mass\n        else:\n            # And this is the bad case\n            err_msg = \"The maximum chirp mass provided is not possible given \"\n            err_msg += \"restrictions on component masses.\"\n            raise ValueError(err_msg)\n        # Is there also an eta restriction?\n        if opts.min_eta:\n            # Get the value of m1,m2 at max_eta, min_chirp_mass\n            min_eta_m1, min_eta_m2 = pnutils.mchirp_eta_to_mass1_mass2(\n                                         opts.max_chirp_mass, opts.min_eta)\n            min_eta_max_tot_mass = min_eta_m1 + min_eta_m2\n            if min_eta_max_tot_mass < max_tot_mass:\n                # Okay, eta does restrict this further. Still physical?\n                max_tot_mass = min_eta_max_tot_mass\n                if min_eta_m1 < opts.min_mass1:\n                    err_msg = \"The combination of component mass, chirp \"\n                    err_msg += \"mass, eta and (possibly) total mass limits \"\n                    err_msg += \"have precluded all systems.\"\n                    raise ValueError(err_msg)\n        # Update min_tot_mass if needed\n        if max_tot_mass < opts.max_total_mass:\n            opts.max_total_mass = float(max_tot_mass)\n\n    # Need to check max_eta alone for minimum and maximum mass\n    if opts.max_eta:\n        # Similar to above except this can affect both the minimum and maximum\n        # total mass. Need to identify where the line of max_eta intersects\n        # the parameter space, and if it affects mass restrictions.\n        m1_at_min_m2 = pnutils.eta_mass1_to_mass2(opts.max_eta, opts.min_mass2,\n                                                      return_mass_heavier=True)\n        m2_at_min_m1 = pnutils.eta_mass1_to_mass2(opts.max_eta, opts.min_mass1,\n                                                     return_mass_heavier=False)\n        m1_at_max_m2 = pnutils.eta_mass1_to_mass2(opts.max_eta, opts.max_mass2,\n                                                      return_mass_heavier=True)\n        m2_at_max_m1 = pnutils.eta_mass1_to_mass2(opts.max_eta, opts.max_mass1,\n                                                      return_mass_heavier=False)\n        # Check for restrictions on the minimum total mass\n        # Are either of these possible?\n        if m1_at_min_m2 <= opts.max_mass1 and m1_at_min_m2 >= opts.min_mass1:\n            min_tot_mass = opts.min_mass2 + m1_at_min_m2\n        elif m2_at_min_m1 <= opts.max_mass2 and m2_at_min_m1 >= opts.min_mass2:\n            # This case doesn't change the minimal total mass\n            min_tot_mass = opts.min_total_mass\n        # So either the restriction is low enough to be redundant, or is\n        # removing all the paramter space\n        elif m2_at_min_m1 > opts.max_mass2:\n            # This is the redundant case, ignore\n            min_tot_mass = opts.min_total_mass\n        elif opts.max_eta == 0.25 and (m1_at_min_m2 < opts.min_mass2 or \\\n                                                m2_at_min_m1 > opts.min_mass1): \n            # This just catches potential roundoff issues in the case that\n            # max-eta is not used\n            min_tot_mass = opts.min_total_mass\n        else:\n            # And this is the bad case\n            err_msg = \"The maximum eta provided is not possible given \"\n            err_msg += \"restrictions on component masses.\"\n            print(m1_at_min_m2, m2_at_min_m1, m1_at_max_m2, m2_at_max_m1)\n            print(opts.min_mass1, opts.max_mass1, opts.min_mass2, opts.max_mass2)\n            raise ValueError(err_msg)\n        # Update min_tot_mass if needed\n        if min_tot_mass > opts.min_total_mass:\n            opts.min_total_mass = float(min_tot_mass)\n\n        # Check for restrictions on the maximum total mass\n        # Are either of these possible?\n        if m2_at_max_m1 <= opts.max_mass2 and m2_at_max_m1 >= opts.min_mass2:\n            max_tot_mass = opts.max_mass1 + m2_at_max_m1\n        elif m1_at_max_m2 <= opts.max_mass1 and m1_at_max_m2 >= opts.min_mass1:\n            # This case doesn't change the maximal total mass\n            max_tot_mass = opts.max_total_mass\n        # So either the restriction is low enough to be redundant, or is\n        # removing all the paramter space, the latter case is already tested\n        else:\n            # This is the redundant case, ignore\n            max_tot_mass = opts.max_total_mass\n        if max_tot_mass < opts.max_total_mass:\n            opts.max_total_mass = float(max_tot_mass)\n\n    # Need to check min_eta alone for maximum and minimum total mass\n    if opts.min_eta:\n        # Same as max_eta.\n        # Need to identify where the line of max_eta intersects\n        # the parameter space, and if it affects mass restrictions.\n        m1_at_min_m2 = pnutils.eta_mass1_to_mass2(opts.min_eta, opts.min_mass2,\n                                                      return_mass_heavier=True)\n        m2_at_min_m1 = pnutils.eta_mass1_to_mass2(opts.min_eta, opts.min_mass1,\n                                                     return_mass_heavier=False)\n        m1_at_max_m2 = pnutils.eta_mass1_to_mass2(opts.min_eta, opts.max_mass2,\n                                                      return_mass_heavier=True)\n        m2_at_max_m1 = pnutils.eta_mass1_to_mass2(opts.min_eta, opts.max_mass1,\n                                                      return_mass_heavier=False)\n\n        # Check for restrictions on the maximum total mass\n        # Are either of these possible?\n        if m1_at_max_m2 <= opts.max_mass1 and m1_at_max_m2 >= opts.min_mass1:\n            max_tot_mass = opts.max_mass2 + m1_at_max_m2\n\n        elif m2_at_max_m1 <= opts.max_mass2 and m2_at_max_m1 >= opts.min_mass2:\n            # This case doesn't affect the maximum total mass\n            max_tot_mass = opts.max_total_mass\n        # So either the restriction is low enough to be redundant, or is\n        # removing all the paramter space\n        elif m2_at_max_m1 < opts.min_mass2:\n            # This is the redundant case, ignore\n            max_tot_mass = opts.max_total_mass\n        else:\n            # And this is the bad case\n            err_msg = \"The minimum eta provided is not possible given \"\n            err_msg += \"restrictions on component masses.\"\n            raise ValueError(err_msg)\n        # Update min_tot_mass if needed\n        if max_tot_mass < opts.max_total_mass:\n            opts.max_total_mass = float(max_tot_mass)\n\n        # Check for restrictions on the minimum total mass\n        # Are either of these possible?\n        if m2_at_min_m1 <= opts.max_mass2 and m2_at_min_m1 >= opts.min_mass2:\n            min_tot_mass = opts.min_mass1 + m2_at_min_m1\n        elif m1_at_min_m2 <= opts.max_mass1 and m1_at_min_m2 >= opts.min_mass1:\n            # This case doesn't change the maximal total mass\n            min_tot_mass = opts.min_total_mass\n        # So either the restriction is low enough to be redundant, or is\n        # removing all the paramter space, which is tested above\n        else:\n            # This is the redundant case, ignore\n            min_tot_mass = opts.min_total_mass\n        if min_tot_mass > opts.min_total_mass:\n            opts.min_total_mass = float(min_tot_mass)\n\n    if opts.max_total_mass < opts.min_total_mass:\n        err_msg = \"After including restrictions on chirp mass, component mass, \"\n        err_msg += \"eta and total mass, no physical systems are possible.\"\n        raise ValueError(err_msg)\n\n    if opts.max_eta and opts.min_eta and (opts.max_eta < opts.min_eta):\n        parser.error(\"--max-eta must be larger than --min-eta.\")\n    if nonSpin:\n        return\n\n    if opts.max_ns_spin_mag is None:\n        if opts.nsbh_flag:\n            parser.error(\"Must supply --max_ns_spin_mag with --nsbh-flag\")\n        # Can ignore this if no NSs will be generated\n        elif opts.min_mass2 < (opts.ns_bh_boundary_mass or\n                massRangeParameters.default_nsbh_boundary_mass):\n            parser.error(\"Must supply --max-ns-spin-mag for the chosen\"\n                         \" value of --min_mass2\")\n        else:\n            opts.max_ns_spin_mag = opts.max_bh_spin_mag\n    if opts.max_bh_spin_mag is None:\n        if opts.nsbh_flag:\n            parser.error(\"Must supply --max_bh_spin_mag with --nsbh-flag\")\n        # Can ignore this if no BHs will be generated\n        if opts.max_mass1 >= (opts.ns_bh_boundary_mass or\n                massRangeParameters.default_nsbh_boundary_mass):\n            parser.error(\"Must supply --max-bh-spin-mag for the chosen\"\n                         \" value of --max_mass1\")\n        else:\n            opts.max_bh_spin_mag = opts.max_ns_spin_mag"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef insert_ethinca_metric_options(parser):\n    ethincaGroup = parser.add_argument_group(\"Ethinca metric options\",\n                    \"Options used in the calculation of Gamma metric \"\n                    \"components for the ethinca coincidence test and for \"\n                    \"assigning high-frequency cutoffs to templates.\")\n    ethinca_methods = ethincaGroup.add_mutually_exclusive_group()\n    ethinca_methods.add_argument(\"--calculate-time-metric-components\",\n                    action=\"store_true\", default=False,\n                    help=\"If given, the ethinca metric will be calculated \"\n                    \"for only the time component, and stored in the Gamma0 \"\n                    \"entry of the sngl_inspiral table. OPTIONAL, default=False\")\n    ethinca_methods.add_argument(\"--calculate-ethinca-metric\",\n                    action=\"store_true\", default=False, \n                    help=\"If given, the ethinca metric will be calculated \"\n                    \"and stored in the Gamma entries of the sngl_inspiral \"\n                    \"table. OPTIONAL, default=False\")\n    ethincaGroup.add_argument(\"--ethinca-pn-order\",\n                    default=None, choices=get_ethinca_orders(), \n                    help=\"Specify a PN order to be used in calculating the \"\n                    \"ethinca metric. OPTIONAL: if not specified, the same \"\n                    \"order will be used as for the bank metric.\")\n    ethincaGroup.add_argument(\"--filter-cutoff\",\n                    default=None, \n                    choices=pnutils.named_frequency_cutoffs.keys(),\n                    help=\"Specify an upper frequency cutoff formula for the \"\n                    \"ethinca metric calculation, and for the values of f_final\"\n                    \" assigned to the templates.  REQUIRED if the \"\n                    \"calculate-ethinca-metric option is given.\")\n    ethincaGroup.add_argument(\"--ethinca-frequency-step\", action=\"store\",\n                    type=float, default=10.,\n                    help=\"Control the precision of the upper frequency cutoff.\"\n                    \" For speed, the metric is calculated only for discrete \"\n                    \"f_max values with a spacing given by this option. Each \"\n                    \"template is assigned the metric for the f_max closest to \"\n                    \"its analytical cutoff formula. OPTIONAL, default=10. \"\n                    \"UNITS=Hz\")\n\n    return ethincaGroup", "response": "Adds the options used to calculate the ethinca metric."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef verify_ethinca_metric_options(opts, parser):\n    if opts.filter_cutoff is not None and not (opts.filter_cutoff in\n              pnutils.named_frequency_cutoffs.keys()):\n        parser.error(\"Need a valid cutoff formula to calculate ethinca or \"\n                     \"assign filter f_final values! Possible values are \"\n                     +str(pnutils.named_frequency_cutoffs.keys()))\n    if (opts.calculate_ethinca_metric or opts.calculate_time_metric_components)\\\n                                           and not opts.ethinca_frequency_step:\n        parser.error(\"Need to specify a cutoff frequency step to calculate \"\n                     \"ethinca!\")\n    if not (opts.calculate_ethinca_metric or\\\n              opts.calculate_time_metric_components) and opts.ethinca_pn_order:\n        parser.error(\"Can't specify an ethinca PN order if not \"\n                     \"calculating ethinca metric!\")", "response": "Checks that the necessary options are given for the ethinca metric\n    calculation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncrossing - check the ethinca and bank layout metric calculation parameters and set the PN order equal to the bank PN order if not already set.", "response": "def check_ethinca_against_bank_params(ethincaParams, metricParams):\n    \"\"\"\n    Cross-check the ethinca and bank layout metric calculation parameters\n    and set the ethinca metric PN order equal to the bank PN order if not\n    previously set.\n\n    Parameters\n    ----------\n    ethincaParams: instance of ethincaParameters\n    metricParams: instance of metricParameters\n    \"\"\"\n    if ethincaParams.doEthinca:\n        if metricParams.f0 != metricParams.fLow:\n            raise ValueError(\"If calculating ethinca metric, f0 and f-low \"\n                             \"must be equal!\")\n        if ethincaParams.fLow is not None and (\n                ethincaParams.fLow != metricParams.fLow):\n            raise ValueError(\"Ethinca metric calculation does not currently \"\n                             \"support a f-low value different from the bank \"\n                             \"metric!\")\n        if ethincaParams.pnOrder is None:\n            ethincaParams.pnOrder = metricParams.pnOrder\n    else: pass"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef format_description(self, description):\n        if not description: return \"\"\n        desc_width = self.width - self.current_indent\n        indent = \" \"*self.current_indent\n        # the above is still the same\n        bits = description.split('\\n')\n        formatted_bits = [\n            textwrap.fill(bit,\n                desc_width,\n                initial_indent=indent,\n                subsequent_indent=indent)\n            for bit in bits]\n        result = \"\\n\".join(formatted_bits) + \"\\n\"\n        return result", "response": "Format the description of the log entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nformats the option string for the log entry.", "response": "def format_option(self, option):\n        \"\"\"\n        No documentation\n        \"\"\"\n        # The help for each option consists of two parts:\n        #   * the opt strings and metavars\n        #   eg. (\"-x\", or \"-fFILENAME, --file=FILENAME\")\n        #   * the user-supplied help string\n        #   eg. (\"turn on expert mode\", \"read data from FILENAME\")\n        #\n        # If possible, we write both of these on the same line:\n        #   -x    turn on expert mode\n        #\n        # But if the opt string list is too long, we put the help\n        # string on a second line, indented to the same column it would\n        # start in if it fit on the first line.\n        #   -fFILENAME, --file=FILENAME\n        #       read data from FILENAME\n        result = []\n        opts = self.option_strings[option]\n        opt_width = self.help_position - self.current_indent - 2\n        if len(opts) > opt_width:\n            opts = \"%*s%s\\n\" % (self.current_indent, \"\", opts)\n            indent_first = self.help_position\n        else: # start help on same line as opts\n            opts = \"%*s%-*s  \" % (self.current_indent, \"\", opt_width, opts)\n            indent_first = 0\n        result.append(opts)\n        if option.help:\n            help_text = self.expand_default(option)\n            # Everything is the same up through here\n            help_lines = []\n            for para in help_text.split(\"\\n\"):\n                help_lines.extend(textwrap.wrap(para, self.help_width))\n            # Everything is the same after here\n            result.append(\"%*s%s\\n\" % (\n                indent_first, \"\", help_lines[0]))\n            result.extend([\"%*s%s\\n\" % (self.help_position, \"\", line)\n                for line in help_lines[1:]])\n        elif opts[-1] != \"\\n\":\n            result.append(\"\\n\")\n        return \"\".join(result)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninitialize an instance of the metricParameters class from an argparse. OptionParser instance.", "response": "def from_argparse(cls, opts):\n        \"\"\"\n        Initialize an instance of the metricParameters class from an\n        argparse.OptionParser instance. This assumes that\n        insert_metric_calculation_options\n        and\n        verify_metric_calculation_options\n        have already been called before initializing the class.\n        \"\"\"\n        return cls(opts.pn_order, opts.f_low, opts.f_upper, opts.delta_f,\\\n                   f0=opts.f0, write_metric=opts.write_metric)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the PSD used in the metric calculation.", "response": "def psd(self):\n        \"\"\"\n        A pyCBC FrequencySeries holding the appropriate PSD.\n        Return the PSD used in the metric calculation.\n        \"\"\"\n        if not self._psd:\n            errMsg = \"The PSD has not been set in the metricParameters \"\n            errMsg += \"instance.\"\n            raise ValueError(errMsg)\n        return self._psd"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef evals(self):\n        if self._evals is None:\n            errMsg = \"The metric eigenvalues have not been set in the \"\n            errMsg += \"metricParameters instance.\"\n            raise ValueError(errMsg)\n        return self._evals", "response": "The eigenvalues of the parameter space."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef metric(self):\n        if self._metric is None:\n            errMsg = \"The metric eigenvectors have not been set in the \"\n            errMsg += \"metricParameters instance.\"\n            raise ValueError(errMsg)\n        return self._metric", "response": "Returns the metric of the parameter space."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the time unprojected metric of the parameter space with the time dimension unprojected.", "response": "def time_unprojected_metric(self):\n        \"\"\"\n        The metric of the parameter space with the time dimension unprojected.\n        This is a Dictionary of numpy.matrix\n        Each entry in the dictionary is as described under evals.\n        Each numpy.matrix contains the metric of the parameter space in the\n        Lambda_i, t coordinate system. The time components are always in the\n        last [-1] position in the matrix.\n        \"\"\"\n        if self._time_unprojected_metric is None:\n            err_msg = \"The time unprojected metric has not been set in the \"\n            err_msg += \"metricParameters instance.\"\n            raise ValueError(err_msg)\n        return self._time_unprojected_metric"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_argparse(cls, opts, nonSpin=False):\n        if nonSpin:\n            return cls(opts.min_mass1, opts.max_mass1, opts.min_mass2,\n                       opts.max_mass2, maxTotMass=opts.max_total_mass,\n                       minTotMass=opts.min_total_mass, maxEta=opts.max_eta,\n                       minEta=opts.min_eta, max_chirp_mass=opts.max_chirp_mass,\n                       min_chirp_mass=opts.min_chirp_mass,\n                       remnant_mass_threshold=opts.remnant_mass_threshold,\n                       ns_eos=opts.ns_eos, use_eos_max_ns_mass=opts.use_eos_max_ns_mass,\n                       delta_bh_spin=opts.delta_bh_spin, delta_ns_mass=opts.delta_ns_mass)\n        else:\n            return cls(opts.min_mass1, opts.max_mass1, opts.min_mass2,\n                       opts.max_mass2, maxTotMass=opts.max_total_mass,\n                       minTotMass=opts.min_total_mass, maxEta=opts.max_eta,\n                       minEta=opts.min_eta, maxNSSpinMag=opts.max_ns_spin_mag,\n                       maxBHSpinMag=opts.max_bh_spin_mag,\n                       nsbhFlag=opts.nsbh_flag,\n                       max_chirp_mass=opts.max_chirp_mass,\n                       min_chirp_mass=opts.min_chirp_mass,\n                       ns_bh_boundary_mass=opts.ns_bh_boundary_mass,\n                       remnant_mass_threshold=opts.remnant_mass_threshold,\n                       ns_eos=opts.ns_eos, use_eos_max_ns_mass=opts.use_eos_max_ns_mass,\n                       delta_bh_spin=opts.delta_bh_spin, delta_ns_mass=opts.delta_ns_mass)", "response": "Initialize an instance of massRangeParameters from an argparse. OptionParser instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntest if a given location in mass1 mass2 spin1z spin2z is within the range of parameters allowed by the massParams object.", "response": "def is_outside_range(self, mass1, mass2, spin1z, spin2z):\n        \"\"\"\n        Test if a given location in mass1, mass2, spin1z, spin2z is within the\n        range of parameters allowed by the massParams object.\n        \"\"\"\n        # Mass1 test\n        if mass1 * 1.001 < self.minMass1:\n            return 1\n        if mass1 > self.maxMass1 * 1.001:\n            return 1\n        # Mass2 test\n        if mass2 * 1.001 < self.minMass2:\n            return 1\n        if mass2 > self.maxMass2 * 1.001:\n            return 1\n        # Spin1 test\n        if self.nsbhFlag:\n            if (abs(spin1z) > self.maxBHSpinMag * 1.001):\n                return 1\n        else:\n            spin1zM = abs(spin1z)\n            if not( (mass1 * 1.001 > self.ns_bh_boundary_mass \\\n                     and spin1zM <= self.maxBHSpinMag * 1.001) \\\n                 or (mass1 < self.ns_bh_boundary_mass * 1.001 \\\n                     and spin1zM <= self.maxNSSpinMag * 1.001)):\n                return 1\n        # Spin2 test\n        if self.nsbhFlag:\n            if (abs(spin2z) > self.maxNSSpinMag * 1.001):\n                return 1\n        else:\n            spin2zM = abs(spin2z)\n            if not( (mass2 * 1.001 > self.ns_bh_boundary_mass \\\n                     and spin2zM <= self.maxBHSpinMag * 1.001) \\\n                 or (mass2 < self.ns_bh_boundary_mass * 1.001 and \\\n                     spin2zM <= self.maxNSSpinMag * 1.001)):\n                return 1\n        # Total mass test\n        mTot = mass1 + mass2\n        if mTot > self.maxTotMass * 1.001:\n            return 1\n        if mTot * 1.001 < self.minTotMass:\n            return 1\n\n        # Eta test\n        eta = mass1 * mass2 / (mTot * mTot)\n        if eta > self.maxEta * 1.001:\n            return 1\n        if eta * 1.001 < self.minEta:\n            return 1\n\n        # Chirp mass test\n        chirp_mass = mTot * eta**(3./5.)\n        if self.min_chirp_mass is not None \\\n                and chirp_mass * 1.001 < self.min_chirp_mass:\n            return 1\n        if self.max_chirp_mass is not None \\\n                and chirp_mass > self.max_chirp_mass * 1.001:\n            return 1\n\n        return 0"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_argparse(cls, opts):\n        return cls(opts.ethinca_pn_order, opts.filter_cutoff,\n            opts.ethinca_frequency_step, fLow=None,\n            full_ethinca=opts.calculate_ethinca_metric,\n            time_ethinca=opts.calculate_time_metric_components)", "response": "Initialize an instance of the ethincaParameters class from an argparse. OptionParser instance."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_sim_data(inj, field, data):\n    try:\n        sim_field = sim_inspiral_map[field]\n    except KeyError:\n        sim_field = field\n    # for tc, map to geocentric times\n    if sim_field == 'tc':\n        inj.geocent_end_time = int(data)\n        inj.geocent_end_time_ns = int(1e9*(data % 1))\n    else:\n        setattr(inj, sim_field, data)", "response": "Sets data of a SimInspiral instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef legacy_approximant_name(apx):\n    apx = str(apx)\n    try:\n        order = sim.GetOrderFromString(apx)\n    except:\n        print(\"Warning: Could not read phase order from string, using default\")\n        order = -1\n    name = sim.GetStringFromApproximant(sim.GetApproximantFromString(apx))\n    return name, order", "response": "Convert the old style xml approximant name to a name\n    and phase_order. Alex is I hate this function."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the HDFInjectionSet class to use with the given file.", "response": "def get_hdf_injtype(sim_file):\n    \"\"\"Gets the HDFInjectionSet class to use with the given file.\n\n    This looks for the ``injtype`` in the given file's top level ``attrs``. If\n    that attribute isn't set, will default to :py:class:`CBCHDFInjectionSet`.\n\n    Parameters\n    ----------\n    sim_file : str\n        Name of the file. The file must already exist.\n\n    Returns\n    -------\n    HDFInjectionSet :\n        The type of HDFInjectionSet to use.\n    \"\"\"\n    with h5py.File(sim_file, 'r') as fp:\n        try:\n            ftype = fp.attrs['injtype']\n        except KeyError:\n            ftype = CBCHDFInjectionSet.injtype\n    return hdfinjtypes[ftype]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef hdf_injtype_from_approximant(approximant):\n    retcls = None\n    for cls in hdfinjtypes.values():\n        if approximant in cls.supported_approximants():\n            retcls = cls\n    if retcls is None:\n        # none were found, raise an error\n        raise ValueError(\"Injection file type unknown for approximant {}\"\n                         .format(approximant))\n    return retcls", "response": "Gets the HDFInjectionSet class to use with the given approximant."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding injections to a time series.", "response": "def apply(self, strain, detector_name, f_lower=None, distance_scale=1,\n              simulation_ids=None, inj_filter_rejector=None):\n        \"\"\"Add injections (as seen by a particular detector) to a time series.\n\n        Parameters\n        ----------\n        strain : TimeSeries\n            Time series to inject signals into, of type float32 or float64.\n        detector_name : string\n            Name of the detector used for projecting injections.\n        f_lower : {None, float}, optional\n            Low-frequency cutoff for injected signals. If None, use value\n            provided by each injection.\n        distance_scale: {1, float}, optional\n            Factor to scale the distance of an injection with. The default is\n            no scaling.\n        simulation_ids: iterable, optional\n            If given, only inject signals with the given simulation IDs.\n        inj_filter_rejector: InjFilterRejector instance; optional, default=None\n            If given send each injected waveform to the InjFilterRejector\n            instance so that it can store a reduced representation of that\n            injection if necessary.\n\n        Returns\n        -------\n        None\n\n        Raises\n        ------\n        TypeError\n            For invalid types of `strain`.\n        \"\"\"\n        if strain.dtype not in (float32, float64):\n            raise TypeError(\"Strain dtype must be float32 or float64, not \" \\\n                    + str(strain.dtype))\n\n        lalstrain = strain.lal()\n        earth_travel_time = lal.REARTH_SI / lal.C_SI\n        t0 = float(strain.start_time) - earth_travel_time\n        t1 = float(strain.end_time) + earth_travel_time\n\n        # pick lalsimulation injection function\n        add_injection = injection_func_map[strain.dtype]\n\n        injections = self.table\n        if simulation_ids:\n            injections = [inj for inj in injections \\\n                          if inj.simulation_id in simulation_ids]\n        injection_parameters = []\n        for inj in injections:\n            if f_lower is None:\n                f_l = inj.f_lower\n            else:\n                f_l = f_lower\n            # roughly estimate if the injection may overlap with the segment\n            # Add 2s to end_time to account for ringdown and light-travel delay\n            end_time = inj.get_time_geocent() + 2\n            inj_length = sim.SimInspiralTaylorLength(\n                strain.delta_t, inj.mass1 * lal.MSUN_SI,\n                inj.mass2 * lal.MSUN_SI, f_l, 0)\n            # Start time is taken as twice approx waveform length with a 1s\n            # safety buffer\n            start_time = inj.get_time_geocent() - 2 * (inj_length+1)\n            if end_time < t0 or start_time > t1:\n                continue\n            signal = self.make_strain_from_inj_object(inj, strain.delta_t,\n                     detector_name, f_lower=f_l, distance_scale=distance_scale)\n            if float(signal.start_time) > t1:\n                continue\n\n            signal = signal.astype(strain.dtype)\n            signal_lal = signal.lal()\n            add_injection(lalstrain, signal_lal, None)\n            injection_parameters.append(inj)\n            if inj_filter_rejector is not None:\n                sid = inj.simulation_id\n                inj_filter_rejector.generate_short_inj_from_inj(signal, sid)\n\n        strain.data[:] = lalstrain.data.data[:]\n\n        injected = copy.copy(self)\n        injected.table = lsctables.SimInspiralTable()\n        injected.table += injection_parameters\n        if inj_filter_rejector is not None:\n            inj_filter_rejector.injection_params = injected\n        return injected"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_strain_from_inj_object(self, inj, delta_t, detector_name,\n                                    f_lower=None, distance_scale=1):\n        \"\"\"Make a h(t) strain time-series from an injection object as read from\n        a sim_inspiral table, for example.\n\n        Parameters\n        -----------\n        inj : injection object\n            The injection object to turn into a strain h(t).\n        delta_t : float\n            Sample rate to make injection at.\n        detector_name : string\n            Name of the detector used for projecting injections.\n        f_lower : {None, float}, optional\n            Low-frequency cutoff for injected signals. If None, use value\n            provided by each injection.\n        distance_scale: {1, float}, optional\n            Factor to scale the distance of an injection with. The default is\n            no scaling.\n\n        Returns\n        --------\n        signal : float\n            h(t) corresponding to the injection.\n        \"\"\"\n        detector = Detector(detector_name)\n        if f_lower is None:\n            f_l = inj.f_lower\n        else:\n            f_l = f_lower\n\n        name, phase_order = legacy_approximant_name(inj.waveform)\n\n        # compute the waveform time series\n        hp, hc = get_td_waveform(\n            inj, approximant=name, delta_t=delta_t,\n            phase_order=phase_order,\n            f_lower=f_l, distance=inj.distance,\n            **self.extra_args)\n\n        hp /= distance_scale\n        hc /= distance_scale\n\n        hp._epoch += inj.get_time_geocent()\n        hc._epoch += inj.get_time_geocent()\n\n        # taper the polarizations\n        hp_tapered = wfutils.taper_timeseries(hp, inj.taper)\n        hc_tapered = wfutils.taper_timeseries(hc, inj.taper)\n\n        # compute the detector response and add it to the strain\n        signal = detector.project_wave(hp_tapered, hc_tapered,\n                             inj.longitude, inj.latitude, inj.polarization)\n\n        return signal", "response": "This function takes an injection object and creates a strain time - series from it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting the injection samples to the given xml file.", "response": "def write(filename, samples, write_params=None, static_args=None):\n        \"\"\"Writes the injection samples to the given xml.\n\n        Parameters\n        ----------\n        filename : str\n            The name of the file to write to.\n        samples : io.FieldArray\n            FieldArray of parameters.\n        write_params : list, optional\n            Only write the given parameter names. All given names must be keys\n            in ``samples``. Default is to write all parameters in ``samples``.\n        static_args : dict, optional\n            Dictionary mapping static parameter names to values. These are\n            written to the ``attrs``.\n        \"\"\"\n        xmldoc = ligolw.Document()\n        xmldoc.appendChild(ligolw.LIGO_LW())\n        simtable = lsctables.New(lsctables.SimInspiralTable)\n        xmldoc.childNodes[0].appendChild(simtable)\n        if static_args is None:\n            static_args = {}\n        if write_params is None:\n            write_params = samples.fieldnames\n        for ii in range(samples.size):\n            sim = lsctables.SimInspiral()\n            # initialize all elements to None\n            for col in sim.__slots__:\n                setattr(sim, col, None)\n            for field in write_params:\n                data = samples[ii][field]\n                set_sim_data(sim, field, data)\n            # set any static args\n            for (field, value) in static_args.items():\n                set_sim_data(sim, field, value)\n            simtable.append(sim)\n        ligolw_utils.write_filename(xmldoc, filename,\n                                    gz=filename.endswith('gz'))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write(cls, filename, samples, write_params=None, static_args=None,\n              **metadata):\n        \"\"\"Writes the injection samples to the given hdf file.\n\n        Parameters\n        ----------\n        filename : str\n            The name of the file to write to.\n        samples : io.FieldArray\n            FieldArray of parameters.\n        write_params : list, optional\n            Only write the given parameter names. All given names must be keys\n            in ``samples``. Default is to write all parameters in ``samples``.\n        static_args : dict, optional\n            Dictionary mapping static parameter names to values. These are\n            written to the ``attrs``.\n        \\**metadata :\n            All other keyword arguments will be written to the file's attrs.\n        \"\"\"\n        with h5py.File(filename, 'w') as fp:\n            # write metadata\n            if static_args is None:\n                static_args = {}\n            fp.attrs[\"static_args\"] = static_args.keys()\n            fp.attrs['injtype'] = cls.injtype\n            for key, val in metadata.items():\n                fp.attrs[key] = val\n            if write_params is None:\n                write_params = samples.fieldnames\n            for arg, val in static_args.items():\n                fp.attrs[arg] = val\n            for field in write_params:\n                fp[field] = samples[field]", "response": "Writes the injection samples to the given hdf file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef apply(self, strain, detector_name, f_lower=None, distance_scale=1,\n              simulation_ids=None, inj_filter_rejector=None):\n        \"\"\"Add injections (as seen by a particular detector) to a time series.\n\n        Parameters\n        ----------\n        strain : TimeSeries\n            Time series to inject signals into, of type float32 or float64.\n        detector_name : string\n            Name of the detector used for projecting injections.\n        f_lower : {None, float}, optional\n            Low-frequency cutoff for injected signals. If None, use value\n            provided by each injection.\n        distance_scale: {1, float}, optional\n            Factor to scale the distance of an injection with. The default is\n            no scaling.\n        simulation_ids: iterable, optional\n            If given, only inject signals with the given simulation IDs.\n        inj_filter_rejector: InjFilterRejector instance; optional, default=None\n            If given send each injected waveform to the InjFilterRejector\n            instance so that it can store a reduced representation of that\n            injection if necessary.\n\n        Returns\n        -------\n        None\n\n        Raises\n        ------\n        TypeError\n            For invalid types of `strain`.\n        \"\"\"\n        if strain.dtype not in (float32, float64):\n            raise TypeError(\"Strain dtype must be float32 or float64, not \" \\\n                    + str(strain.dtype))\n\n        lalstrain = strain.lal()\n        earth_travel_time = lal.REARTH_SI / lal.C_SI\n        t0 = float(strain.start_time) - earth_travel_time\n        t1 = float(strain.end_time) + earth_travel_time\n\n        # pick lalsimulation injection function\n        add_injection = injection_func_map[strain.dtype]\n\n        injections = self.table\n        if simulation_ids:\n            injections = injections[list(simulation_ids)]\n        for ii in range(injections.size):\n            inj = injections[ii]\n            if f_lower is None:\n                f_l = inj.f_lower\n            else:\n                f_l = f_lower\n            # roughly estimate if the injection may overlap with the segment\n            # Add 2s to end_time to account for ringdown and light-travel delay\n            end_time = inj.tc + 2\n            inj_length = sim.SimInspiralTaylorLength(\n                strain.delta_t, inj.mass1 * lal.MSUN_SI,\n                inj.mass2 * lal.MSUN_SI, f_l, 0)\n            # Start time is taken as twice approx waveform length with a 1s\n            # safety buffer\n            start_time = inj.tc - 2 * (inj_length+1)\n            if end_time < t0 or start_time > t1:\n                continue\n            signal = self.make_strain_from_inj_object(inj, strain.delta_t,\n                     detector_name, f_lower=f_l, distance_scale=distance_scale)\n            if float(signal.start_time) > t1:\n                continue\n\n            signal = signal.astype(strain.dtype)\n            signal_lal = signal.lal()\n            add_injection(lalstrain, signal_lal, None)\n            if inj_filter_rejector is not None:\n                inj_filter_rejector.generate_short_inj_from_inj(signal, ii)\n\n        strain.data[:] = lalstrain.data.data[:]\n\n        injected = copy.copy(self)\n        injected.table = injections\n        if inj_filter_rejector is not None:\n            inj_filter_rejector.injection_params = injected\n        return injected", "response": "Adds injections to a time series."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmake a h(t) strain time-series from an injection object. Parameters ----------- inj : injection object The injection object to turn into a strain h(t). Can be any object which has waveform parameters as attributes, such as an element in a ``WaveformArray``. delta_t : float Sample rate to make injection at. detector_name : string Name of the detector used for projecting injections. f_lower : {None, float}, optional Low-frequency cutoff for injected signals. If None, use value provided by each injection. distance_scale: {1, float}, optional Factor to scale the distance of an injection with. The default is no scaling. Returns -------- signal : float h(t) corresponding to the injection.", "response": "def make_strain_from_inj_object(self, inj, delta_t, detector_name,\n                                    f_lower=None, distance_scale=1):\n        \"\"\"Make a h(t) strain time-series from an injection object.\n\n        Parameters\n        -----------\n        inj : injection object\n            The injection object to turn into a strain h(t). Can be any\n            object which has waveform parameters as attributes, such as an\n            element in a ``WaveformArray``.\n        delta_t : float\n            Sample rate to make injection at.\n        detector_name : string\n            Name of the detector used for projecting injections.\n        f_lower : {None, float}, optional\n            Low-frequency cutoff for injected signals. If None, use value\n            provided by each injection.\n        distance_scale: {1, float}, optional\n            Factor to scale the distance of an injection with. The default is\n            no scaling.\n\n        Returns\n        --------\n        signal : float\n            h(t) corresponding to the injection.\n        \"\"\"\n        detector = Detector(detector_name)\n        if f_lower is None:\n            f_l = inj.f_lower\n        else:\n            f_l = f_lower\n\n        # compute the waveform time series\n        hp, hc = get_td_waveform(inj, delta_t=delta_t, f_lower=f_l,\n                                 **self.extra_args)\n\n        hp /= distance_scale\n        hc /= distance_scale\n\n        hp._epoch += inj.tc\n        hc._epoch += inj.tc\n\n        # taper the polarizations\n        try:\n            hp_tapered = wfutils.taper_timeseries(hp, inj.taper)\n            hc_tapered = wfutils.taper_timeseries(hc, inj.taper)\n        except AttributeError:\n            hp_tapered = hp\n            hc_tapered = hc\n\n        # compute the detector response and add it to the strain\n        signal = detector.project_wave(hp_tapered, hc_tapered,\n                             inj.ra, inj.dec, inj.polarization)\n\n        return signal"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding injections to a time series.", "response": "def apply(self, strain, detector_name, distance_scale=1,\n              simulation_ids=None, inj_filter_rejector=None):\n        \"\"\"Add injection (as seen by a particular detector) to a time series.\n\n        Parameters\n        ----------\n        strain : TimeSeries\n            Time series to inject signals into, of type float32 or float64.\n        detector_name : string\n            Name of the detector used for projecting injections.\n        distance_scale: float, optional\n            Factor to scale the distance of an injection with. The default (=1)\n            is no scaling.\n        simulation_ids: iterable, optional\n            If given, only inject signals with the given simulation IDs.\n        inj_filter_rejector: InjFilterRejector instance, optional\n            Not implemented. If not ``None``, a ``NotImplementedError`` will\n            be raised.\n\n        Returns\n        -------\n        None\n\n        Raises\n        ------\n        NotImplementedError\n            If an ``inj_filter_rejector`` is provided.\n        TypeError\n            For invalid types of `strain`.\n        \"\"\"\n        if inj_filter_rejector is not None:\n            raise NotImplementedError(\"Ringdown injections do not support \"\n                                      \"inj_filter_rejector\")\n        if strain.dtype not in (float32, float64):\n            raise TypeError(\"Strain dtype must be float32 or float64, not \" \\\n                    + str(strain.dtype))\n\n        lalstrain = strain.lal()\n\n        # pick lalsimulation injection function\n        add_injection = injection_func_map[strain.dtype]\n\n        injections = self.table\n        if simulation_ids:\n            injections = injections[list(simulation_ids)]\n        for ii in range(injections.size):\n            injection = injections[ii]\n            signal = self.make_strain_from_inj_object(\n                injection, strain.delta_t, detector_name,\n                distance_scale=distance_scale)\n            signal = signal.astype(strain.dtype)\n            signal_lal = signal.lal()\n            add_injection(lalstrain, signal_lal, None)\n\n            strain.data[:] = lalstrain.data.data[:]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_strain_from_inj_object(self, inj, delta_t, detector_name,\n                                    distance_scale=1):\n        \"\"\"Make a h(t) strain time-series from an injection object as read from\n        an hdf file.\n\n        Parameters\n        -----------\n        inj : injection object\n            The injection object to turn into a strain h(t).\n        delta_t : float\n            Sample rate to make injection at.\n        detector_name : string\n            Name of the detector used for projecting injections.\n        distance_scale: float, optional\n            Factor to scale the distance of an injection with. The default (=1)\n            is no scaling.\n\n        Returns\n        --------\n        signal : float\n            h(t) corresponding to the injection.\n        \"\"\"\n        detector = Detector(detector_name)\n\n        # compute the waveform time series\n        hp, hc = ringdown_td_approximants[inj['approximant']](\n            inj, delta_t=delta_t, **self.extra_args)\n\n        hp._epoch += inj['tc']\n        hc._epoch += inj['tc']\n\n        if distance_scale != 1:\n            hp /= distance_scale\n            hc /= distance_scale\n\n        # compute the detector response and add it to the strain\n        signal = detector.project_wave(hp, hc,\n                             inj['ra'], inj['dec'], inj['polarization'])\n\n        return signal", "response": "This function creates a h ( t ) strain time - series from an injection object as read from an hdf file and returns the signal corresponding to the injection object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write(filename, samples, write_params=None, static_args=None,\n              injtype=None, **metadata):\n        \"\"\"Writes the injection samples to the given hdf file.\n\n        Parameters\n        ----------\n        filename : str\n            The name of the file to write to.\n        samples : io.FieldArray\n            FieldArray of parameters.\n        write_params : list, optional\n            Only write the given parameter names. All given names must be keys\n            in ``samples``. Default is to write all parameters in ``samples``.\n        static_args : dict, optional\n            Dictionary mapping static parameter names to values. These are\n            written to the ``attrs``.\n        injtype : str, optional\n            Specify which `HDFInjectionSet` class to use for writing. If not\n            provided, will try to determine it by looking for an approximant in\n            the ``static_args``, followed by the ``samples``.\n        \\**metadata :\n            All other keyword arguments will be written to the file's attrs.\n        \"\"\"\n        # DELETE the following \"if\" once xml is dropped\n        ext = os.path.basename(filename)\n        if ext.endswith(('.xml', '.xml.gz', '.xmlgz')):\n            _XMLInjectionSet.write(filename, samples, write_params,\n                                   static_args)\n        else:\n            # try determine the injtype if it isn't given\n            if injtype is None:\n                if static_args is not None and 'approximant' in static_args:\n                    injcls = hdf_injtype_from_approximant(\n                        static_args['approximant'])\n                elif 'approximant' in samples.fieldnames:\n                    apprxs = np.unique(samples['approximant'])\n                    # make sure they all correspond to the same injection type\n                    injcls = [hdf_injtype_from_approximant(a) for a in apprxs]\n                    if not all(c == injcls[0] for c in injcls):\n                        raise ValueError(\"injections must all be of the same \"\n                                         \"type\")\n                    injcls = injcls[0]\n                else:\n                    raise ValueError(\"Could not find an approximant in the \"\n                                     \"static args or samples to determine the \"\n                                     \"injection type. Please specify an \"\n                                     \"injtype instead.\")\n            else:\n                injcls = hdfinjtypes[injtype]\n            injcls.write(filename, samples, write_params, static_args,\n                         **metadata)", "response": "Writes the injection samples to the given hdf file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds injections to a time series.", "response": "def apply(self, strain, detector_name, f_lower=None, distance_scale=1):\n        \"\"\"Add injections (as seen by a particular detector) to a time series.\n\n        Parameters\n        ----------\n        strain : TimeSeries\n            Time series to inject signals into, of type float32 or float64.\n        detector_name : string\n            Name of the detector used for projecting injections.\n        f_lower : {None, float}, optional\n            Low-frequency cutoff for injected signals. If None, use value\n            provided by each injection.\n        distance_scale: {1, foat}, optional\n            Factor to scale the distance of an injection with. The default is\n            no scaling.\n\n        Returns\n        -------\n        None\n\n        Raises\n        ------\n        TypeError\n            For invalid types of `strain`.\n        \"\"\"\n        if strain.dtype not in (float32, float64):\n            raise TypeError(\"Strain dtype must be float32 or float64, not \" \\\n                    + str(strain.dtype))\n\n        lalstrain = strain.lal()\n        #detector = Detector(detector_name)\n        earth_travel_time = lal.REARTH_SI / lal.C_SI\n        t0 = float(strain.start_time) - earth_travel_time\n        t1 = float(strain.end_time) + earth_travel_time\n\n        # pick lalsimulation injection function\n        add_injection = injection_func_map[strain.dtype]\n\n        for inj in self.table:\n            # roughly estimate if the injection may overlap with the segment\n            end_time = inj.get_time_geocent()\n            #CHECK: This is a hack (10.0s); replace with an accurate estimate\n            inj_length = 10.0\n            eccentricity = 0.0\n            polarization = 0.0\n            start_time = end_time - 2 * inj_length\n            if end_time < t0 or start_time > t1:\n                continue\n\n            # compute the waveform time series\n            hp, hc = sim.SimBurstSineGaussian(float(inj.q),\n                float(inj.frequency),float(inj.hrss),float(eccentricity),\n                float(polarization),float(strain.delta_t))\n            hp = TimeSeries(hp.data.data[:], delta_t=hp.deltaT, epoch=hp.epoch)\n            hc = TimeSeries(hc.data.data[:], delta_t=hc.deltaT, epoch=hc.epoch)\n            hp._epoch += float(end_time)\n            hc._epoch += float(end_time)\n            if float(hp.start_time) > t1:\n                continue\n\n            # compute the detector response, taper it if requested\n            # and add it to the strain\n            strain = wfutils.taper_timeseries(strain, inj.taper)\n            signal_lal = hp.astype(strain.dtype).lal()\n            add_injection(lalstrain, signal_lal, None)\n\n        strain.data[:] = lalstrain.data.data[:]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the spa tmplt phase in the order specified by the given HTLde object.", "response": "def spa_tmplt_engine(htilde,  kmin,  phase_order,\n                    delta_f,  piM,  pfaN,\n                    pfa2,  pfa3,  pfa4,  pfa5,  pfl5,\n                    pfa6,  pfl6,  pfa7, amp_factor):\n    \"\"\" Calculate the spa tmplt phase\n    \"\"\"\n    taylorf2_kernel(htilde.data,  kmin,  phase_order,\n                    delta_f,  piM,  pfaN,\n                    pfa2,  pfa3,  pfa4,  pfa5,  pfl5,\n                    pfa6,  pfl6,  pfa7, amp_factor)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_point_from_bins_and_idx(self, chi1_bin, chi2_bin, idx):\n        mass1 = self.massbank[chi1_bin][chi2_bin]['mass1s'][idx]\n        mass2 = self.massbank[chi1_bin][chi2_bin]['mass2s'][idx]\n        spin1z = self.massbank[chi1_bin][chi2_bin]['spin1s'][idx]\n        spin2z = self.massbank[chi1_bin][chi2_bin]['spin2s'][idx]\n        return mass1, mass2, spin1z, spin2z", "response": "Given the bin numbers and index return the masses and spins of the point at that index. Will fail if no point exists."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_freq_map_and_normalizations(self, frequency_list,\n                                        upper_freq_formula):\n        \"\"\"\n        If using the --vary-fupper capability we need to store the mapping\n        between index and frequencies in the list. We also precalculate the\n        normalization factor at every frequency, which is used when estimating\n        overlaps to account for abrupt changes in termination frequency.\n\n        Parameters\n        -----------\n        frequency_list : array of floats\n            The frequencies for which the metric has been computed and lie\n            within the parameter space being considered.\n        upper_freq_formula : string\n        \"\"\"\n        self.frequency_map = {}\n        self.normalization_map = {}\n        self.upper_freq_formula = upper_freq_formula\n        # FIXME: Must this be sorted on input\n        frequency_list.sort()\n\n        for idx, frequency in enumerate(frequency_list):\n            self.frequency_map[frequency] = idx\n            self.normalization_map[frequency] = \\\n                             (self.metric_params.moments['I7'][frequency])**0.5", "response": "This function is used to get the frequency map and normalization factors for the cluster entries."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_point_bin(self, chi_coords):\n        # Identify bin\n        chi1_bin = int((chi_coords[0] - self.chi1_min) // self.bin_spacing)\n        chi2_bin = int((chi_coords[1] - self.chi2_min) // self.bin_spacing)\n        self.check_bin_existence(chi1_bin, chi2_bin)\n        return chi1_bin, chi2_bin", "response": "Given a set of coordinates in the chi parameter space find the bin indices that the point occurs in."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if the bin in the object exists in the object. If not add it.", "response": "def check_bin_existence(self, chi1_bin, chi2_bin):\n        \"\"\"\n        Given indices for bins in chi1 and chi2 space check that the bin\n        exists in the object. If not add it. Also check for the existence of\n        all bins within +/- self.bin_range_check and add if not present.\n\n        Parameters\n        -----------\n        chi1_bin : int\n            The index of the chi1_bin to check\n        chi2_bin : int\n            The index of the chi2_bin to check\n        \"\"\"\n        bin_range_check = self.bin_range_check\n        # Check if this bin actually exists. If not add it\n        if ( (chi1_bin < self.min_chi1_bin+bin_range_check) or\n             (chi1_bin > self.max_chi1_bin-bin_range_check) or\n             (chi2_bin < self.min_chi2_bin+bin_range_check) or\n             (chi2_bin > self.max_chi2_bin-bin_range_check) ):\n            for temp_chi1 in xrange(chi1_bin-bin_range_check,\n                                                   chi1_bin+bin_range_check+1):\n                if temp_chi1 not in self.massbank:\n                    self.massbank[temp_chi1] = {}\n                    self.bank[temp_chi1] = {}\n                for temp_chi2 in xrange(chi2_bin-bin_range_check,\n                                                   chi2_bin+bin_range_check+1):\n                    if temp_chi2 not in self.massbank[temp_chi1]:\n                        self.massbank[temp_chi1][temp_chi2] = {}\n                        self.massbank[temp_chi1][temp_chi2]['mass1s'] =\\\n                                                                numpy.array([])\n                        self.bank[temp_chi1][temp_chi2] = []"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the distance between the point and the bank. Return the closest distance between the point and the bank.", "response": "def calc_point_distance(self, chi_coords):\n        \"\"\"\n        Calculate distance between point and the bank. Return the closest\n        distance.\n\n        Parameters\n        -----------\n        chi_coords : numpy.array\n            The position of the point in the chi coordinates.\n\n        Returns\n        --------\n        min_dist : float\n            The smallest **SQUARED** metric distance between the test point and\n            the bank.\n        indexes : The chi1_bin, chi2_bin and position within that bin at which\n            the closest matching point lies.\n        \"\"\"\n        chi1_bin, chi2_bin = self.find_point_bin(chi_coords)\n        min_dist = 1000000000\n        indexes = None\n        for chi1_bin_offset, chi2_bin_offset in self.bin_loop_order:\n            curr_chi1_bin = chi1_bin + chi1_bin_offset\n            curr_chi2_bin = chi2_bin + chi2_bin_offset\n            for idx, bank_chis in \\\n                            enumerate(self.bank[curr_chi1_bin][curr_chi2_bin]):\n                dist = coord_utils.calc_point_dist(chi_coords, bank_chis)\n                if dist < min_dist:\n                    min_dist = dist\n                    indexes = (curr_chi1_bin, curr_chi2_bin, idx)\n        return min_dist, indexes"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the distance between a point and the bank allowing the metric to be vary based on varying upper frequency cutoff.", "response": "def calc_point_distance_vary(self, chi_coords, point_fupper, mus):\n        \"\"\"\n        Calculate distance between point and the bank allowing the metric to\n        vary based on varying upper frequency cutoff. Slower than\n        calc_point_distance, but more reliable when upper frequency cutoff can\n        change a lot.\n\n        Parameters\n        -----------\n        chi_coords : numpy.array\n            The position of the point in the chi coordinates.\n        point_fupper : float\n            The upper frequency cutoff to use for this point. This value must\n            be one of the ones already calculated in the metric.\n        mus : numpy.array\n            A 2D array where idx 0 holds the upper frequency cutoff and idx 1\n            holds the coordinates in the [not covaried] mu parameter space for\n            each value of the upper frequency cutoff.\n\n        Returns\n        --------\n        min_dist : float\n            The smallest **SQUARED** metric distance between the test point and\n            the bank.\n        indexes : The chi1_bin, chi2_bin and position within that bin at which\n            the closest matching point lies.\n        \"\"\"\n        chi1_bin, chi2_bin = self.find_point_bin(chi_coords)\n        min_dist = 1000000000\n        indexes = None\n        for chi1_bin_offset, chi2_bin_offset in self.bin_loop_order:\n            curr_chi1_bin = chi1_bin + chi1_bin_offset\n            curr_chi2_bin = chi2_bin + chi2_bin_offset\n            # No points = Next iteration\n            curr_bank = self.massbank[curr_chi1_bin][curr_chi2_bin]\n            if not curr_bank['mass1s'].size:\n                continue\n\n            # *NOT* the same of .min and .max\n            f_upper = numpy.minimum(point_fupper, curr_bank['freqcuts'])\n            f_other = numpy.maximum(point_fupper, curr_bank['freqcuts'])\n            # NOTE: freq_idxes is a vector!\n            freq_idxes = numpy.array([self.frequency_map[f] for f in f_upper])\n            # vecs1 gives a 2x2 vector: idx0 = stored index, idx1 = mu index\n            vecs1 = mus[freq_idxes, :]\n            # vecs2 gives a 2x2 vector: idx0 = stored index, idx1 = mu index\n            range_idxes = numpy.arange(len(freq_idxes))\n            vecs2 = curr_bank['mus'][range_idxes, freq_idxes, :]\n\n            # Now do the sums\n            dists = (vecs1 - vecs2)*(vecs1 - vecs2)\n            # This reduces to 1D: idx = stored index\n            dists = numpy.sum(dists, axis=1)\n            norm_upper = numpy.array([self.normalization_map[f] \\\n                                      for f in f_upper])\n            norm_other = numpy.array([self.normalization_map[f] \\\n                                      for f in f_other])\n            norm_fac = norm_upper / norm_other\n            renormed_dists = 1 - (1 - dists)*norm_fac\n            curr_min_dist = renormed_dists.min()\n            if curr_min_dist < min_dist:\n                min_dist = curr_min_dist\n                indexes = curr_chi1_bin, curr_chi2_bin, renormed_dists.argmin()\n\n        return min_dist, indexes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a point to the partitioned template bank. The point_fupper and mus kwargs must be provided for all templates if the vary fupper capability is desired. This requires that the chi_coords, as well as mus and point_fupper if needed, to be precalculated. If you just have the masses and don't want to worry about translations see add_point_by_masses, which will do translations and then call this. Parameters ----------- chi_coords : numpy.array The position of the point in the chi coordinates. mass1 : float The heavier mass of the point to add. mass2 : float The lighter mass of the point to add. spin1z: float The [aligned] spin on the heavier body. spin2z: float The [aligned] spin on the lighter body. The upper frequency cutoff to use for this point. This value must be one of the ones already calculated in the metric. mus : numpy.array A 2D array where idx 0 holds the upper frequency cutoff and idx 1 holds the coordinates in the [not covaried] mu parameter space for each value of the upper frequency cutoff.", "response": "def add_point_by_chi_coords(self, chi_coords, mass1, mass2, spin1z, spin2z,\n                          point_fupper=None, mus=None):\n        \"\"\"\n        Add a point to the partitioned template bank. The point_fupper and mus\n        kwargs must be provided for all templates if the vary fupper capability\n        is desired. This requires that the chi_coords, as well as mus and\n        point_fupper if needed, to be precalculated. If you just have the\n        masses and don't want to worry about translations see\n        add_point_by_masses, which will do translations and then call this.\n\n        Parameters\n        -----------\n        chi_coords : numpy.array\n            The position of the point in the chi coordinates.\n        mass1 : float\n            The heavier mass of the point to add.\n        mass2 : float\n            The lighter mass of the point to add.\n        spin1z: float\n            The [aligned] spin on the heavier body.\n        spin2z: float\n            The [aligned] spin on the lighter body.\n            The upper frequency cutoff to use for this point. This value must\n            be one of the ones already calculated in the metric.\n        mus : numpy.array\n            A 2D array where idx 0 holds the upper frequency cutoff and idx 1\n            holds the coordinates in the [not covaried] mu parameter space for\n            each value of the upper frequency cutoff.\n        \"\"\"\n        chi1_bin, chi2_bin = self.find_point_bin(chi_coords)\n        self.bank[chi1_bin][chi2_bin].append(copy.deepcopy(chi_coords))\n        curr_bank = self.massbank[chi1_bin][chi2_bin]\n\n        if curr_bank['mass1s'].size:\n            curr_bank['mass1s'] = numpy.append(curr_bank['mass1s'],\n                                               numpy.array([mass1]))\n            curr_bank['mass2s'] = numpy.append(curr_bank['mass2s'],\n                                               numpy.array([mass2]))\n            curr_bank['spin1s'] = numpy.append(curr_bank['spin1s'],\n                                               numpy.array([spin1z]))\n            curr_bank['spin2s'] = numpy.append(curr_bank['spin2s'],\n                                               numpy.array([spin2z]))\n            if point_fupper is not None:\n                curr_bank['freqcuts'] = numpy.append(curr_bank['freqcuts'],\n                                                 numpy.array([point_fupper]))\n            # Mus needs to append onto axis 0. See below for contents of\n            # the mus variable\n            if mus is not None:\n                curr_bank['mus'] = numpy.append(curr_bank['mus'],\n                                            numpy.array([mus[:,:]]), axis=0)\n        else:\n            curr_bank['mass1s'] = numpy.array([mass1])\n            curr_bank['mass2s'] = numpy.array([mass2])\n            curr_bank['spin1s'] = numpy.array([spin1z])\n            curr_bank['spin2s'] = numpy.array([spin2z])\n            if point_fupper is not None:\n                curr_bank['freqcuts'] = numpy.array([point_fupper])\n            # curr_bank['mus'] is a 3D array\n            # NOTE: mu relates to the non-covaried Cartesian coordinate system\n            # Axis 0: Template index\n            # Axis 1: Frequency cutoff index\n            # Axis 2: Mu coordinate index\n            if mus is not None:\n                curr_bank['mus'] = numpy.array([mus[:,:]])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_point_by_masses(self, mass1, mass2, spin1z, spin2z,\n                            vary_fupper=False):\n        \"\"\"\n        Add a point to the template bank. This differs from add point to bank\n        as it assumes that the chi coordinates and the products needed to use\n        vary_fupper have not already been calculated. This function calculates\n        these products and then calls add_point_by_chi_coords.\n        This function also\n        carries out a number of sanity checks (eg. is the point within the\n        ranges given by mass_range_params) that add_point_by_chi_coords does\n        not do for speed concerns.\n\n        Parameters\n        -----------\n        mass1 : float\n            Mass of the heavier body\n        mass2 : float\n            Mass of the lighter body\n        spin1z : float\n            Spin of the heavier body\n        spin2z : float\n            Spin of the lighter body\n        \"\"\"\n        # Test that masses are the expected way around (ie. mass1 > mass2)\n        if mass2 > mass1:\n            if not self.spin_warning_given:\n                warn_msg = \"Am adding a template where mass2 > mass1. The \"\n                warn_msg += \"convention is that mass1 > mass2. Swapping mass1 \"\n                warn_msg += \"and mass2 and adding point to bank. This message \"\n                warn_msg += \"will not be repeated.\"\n                logging.warn(warn_msg)\n                self.spin_warning_given = True\n\n        # These that masses obey the restrictions of mass_range_params\n        if self.mass_range_params.is_outside_range(mass1, mass2, spin1z,\n                                                                       spin2z):\n            err_msg = \"Point with masses given by \"\n            err_msg += \"%f %f %f %f \" %(mass1, mass2, spin1z, spin2z)\n            err_msg += \"(mass1, mass2, spin1z, spin2z) is not consistent \"\n            err_msg += \"with the provided command-line restrictions on masses \"\n            err_msg += \"and spins.\"\n            raise ValueError(err_msg)\n\n        # Get chi coordinates\n        chi_coords = coord_utils.get_cov_params(mass1, mass2, spin1z, spin2z,\n                                                self.metric_params,\n                                                self.ref_freq)\n\n        # Get mus and best fupper for this point, if needed\n        if vary_fupper:\n            mass_dict = {}\n            mass_dict['m1'] = numpy.array([mass1])\n            mass_dict['m2'] = numpy.array([mass2])\n            mass_dict['s1z'] = numpy.array([spin1z])\n            mass_dict['s2z'] = numpy.array([spin2z])\n            freqs = numpy.array([self.frequency_map.keys()], dtype=float)\n            freq_cutoff = coord_utils.return_nearest_cutoff(\\\n                                     self.upper_freq_formula, mass_dict, freqs)\n            freq_cutoff = freq_cutoff[0]\n            lambdas = coord_utils.get_chirp_params\\\n                (mass1, mass2, spin1z, spin2z, self.metric_params.f0,\n                 self.metric_params.pnOrder)\n            mus = []\n            for freq in self.frequency_map:\n                mus.append(coord_utils.get_mu_params(lambdas,\n                                                    self.metric_params, freq) )\n            mus = numpy.array(mus)\n        else:\n            freq_cutoff=None\n            mus=None\n\n        self.add_point_by_chi_coords(chi_coords, mass1, mass2, spin1z, spin2z,\n                               point_fupper=freq_cutoff, mus=mus)", "response": "This function adds a point to the template bank. This function checks that the chi coordinates of the template bank and the masses given by mass_range_params and then checks that the spin1z and spin2z are consistent with the command - line restrictions."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef output_all_points(self):\n        mass1 = []\n        mass2 = []\n        spin1z = []\n        spin2z = []\n        for i in self.massbank.keys():\n            for j in self.massbank[i].keys():\n                for k in xrange(len(self.massbank[i][j]['mass1s'])):\n                    curr_bank = self.massbank[i][j]\n                    mass1.append(curr_bank['mass1s'][k])\n                    mass2.append(curr_bank['mass2s'][k])\n                    spin1z.append(curr_bank['spin1s'][k])\n                    spin2z.append(curr_bank['spin2s'][k])\n\n        return mass1, mass2, spin1z, spin2z", "response": "Return all points in the bank as lists of m1 m2 spin1z spin2z."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef eta_mass1_to_mass2(eta, mass1, return_mass_heavier=False, force_real=True):\n    return conversions.mass_from_knownmass_eta(mass1, eta,\n        known_is_secondary=return_mass_heavier, force_real=force_real)", "response": "This function takes values for eta and one component mass and returns the second component mass."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef mchirp_q_to_mass1_mass2(mchirp, q):\n    eta = conversions.eta_from_q(q)\n    mass1 = conversions.mass1_from_mchirp_eta(mchirp, eta)\n    mass2 = conversions.mass2_from_mchirp_eta(mchirp, eta)\n    return mass1, mass2", "response": "This function takes a value of mchirp and the mass ratio\n    mass1 and mass2 and returns the two component masses."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_beta_sigma_from_aligned_spins(eta, spin1z, spin2z):\n    chiS = 0.5 * (spin1z + spin2z)\n    chiA = 0.5 * (spin1z - spin2z)\n    delta = (1 - 4 * eta) ** 0.5\n    spinspin = spin1z * spin2z\n    beta = (113. / 12. - 19. / 3. * eta) * chiS\n    beta += 113. / 12. * delta * chiA\n    sigma = eta / 48. * (474 * spinspin)\n    sigma += (1 - 2 * eta) * (81. / 16. * (chiS * chiS + chiA * chiA))\n    sigma += delta * (81. / 8. * (chiS * chiA))\n    gamma = (732985. / 2268. - 24260. / 81. * eta - \\\n            340. / 9. * eta * eta) * chiS\n    gamma += (732985. / 2268. + 140. / 9. * eta) * delta * chiA\n    return beta, sigma, gamma", "response": "Calculates the various PN spin combinations from the masses and spins."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the final spin Mass ratio dependent ISCO derived from estimates of the final spin of a merged black hole.", "response": "def f_BKLISCO(m1, m2):\n    \"\"\"\n    Mass ratio dependent ISCO derived from estimates of the final spin\n    of a merged black hole in a paper by Buonanno, Kidder, Lehner\n    (arXiv:0709.3839).  See also arxiv:0801.4297v2 eq.(5)\n\n    Parameters\n    ----------\n    m1 : float or numpy.array\n        First component mass in solar mass units\n    m2 : float or numpy.array\n        Second component mass in solar mass units\n\n    Returns\n    -------\n    f : float or numpy.array\n        Frequency in Hz\n    \"\"\"\n    # q is defined to be in [0,1] for this formula\n    q = numpy.minimum(m1/m2, m2/m1)\n    return f_SchwarzISCO(m1+m2) * ( 1 + 2.8*q - 2.6*q*q + 0.8*q*q*q )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef f_FRD(m1, m2):\n    m_total, eta = mass1_mass2_to_mtotal_eta(m1, m2)\n    tmp = ( (1. - 0.63*(1. - 3.4641016*eta + 2.9*eta**2)**(0.3)) /\n    (1. - 0.057191*eta - 0.498*eta**2) )\n    return tmp / (2.*lal.PI * m_total*lal.MTSUN_SI)", "response": "Calculates Fundamental RingDown frequency using the Berti Cardoso and the Hz of the omega_220 QNM."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwraps of the LALSimulation function returning the frequency in Hz for a given frequency function and template parameters.", "response": "def _get_freq(freqfunc, m1, m2, s1z, s2z):\n    \"\"\"\n    Wrapper of the LALSimulation function returning the frequency\n    for a given frequency function and template parameters.\n\n    Parameters\n    ----------\n    freqfunc : lalsimulation FrequencyFunction wrapped object e.g.\n        lalsimulation.fEOBNRv2RD\n    m1 : float-ish, i.e. castable to float\n        First component mass in solar masses\n    m2 : float-ish\n        Second component mass in solar masses\n    s1z : float-ish\n        First component dimensionless spin S_1/m_1^2 projected onto L\n    s2z : float-ish\n        Second component dimensionless spin S_2/m_2^2 projected onto L\n\n    Returns\n    -------\n    f : float\n        Frequency in Hz\n    \"\"\"\n    # Convert to SI units for lalsimulation\n    m1kg = float(m1) * lal.MSUN_SI\n    m2kg = float(m2) * lal.MSUN_SI\n    return lalsimulation.SimInspiralGetFrequency(\n        m1kg, m2kg, 0, 0, float(s1z), 0, 0, float(s2z), int(freqfunc))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_freq(freqfunc, m1, m2, s1z, s2z):\n    lalsim_ffunc = getattr(lalsimulation, freqfunc)\n    return _vec_get_freq(lalsim_ffunc, m1, m2, s1z, s2z)", "response": "Returns the frequency function which evaluates the frequency\n    for the given frequency function and template parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_final_freq(approx, m1, m2, s1z, s2z):\n    # Convert to SI units for lalsimulation\n    m1kg = float(m1) * lal.MSUN_SI\n    m2kg = float(m2) * lal.MSUN_SI\n    return lalsimulation.SimInspiralGetFinalFreq(\n        m1kg, m2kg, 0, 0, float(s1z), 0, 0, float(s2z), int(approx))", "response": "Wrapper of the LALSimulation function returning the final frequency for a given approximant"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_final_freq(approx, m1, m2, s1z, s2z):\n    lalsim_approx = lalsimulation.GetApproximantFromString(approx)\n    return _vec_get_final_freq(lalsim_approx, m1, m2, s1z, s2z)", "response": "Returns the final frequency for a given approximant"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the frequency cutoff function that evaluates the frequency cutoff specified by name on a template with given parameters.", "response": "def frequency_cutoff_from_name(name, m1, m2, s1z, s2z):\n    \"\"\"\n    Returns the result of evaluating the frequency cutoff function\n    specified by 'name' on a template with given parameters.\n\n    Parameters\n    ----------\n    name : string\n        Name of the cutoff function\n    m1 : float or numpy.array\n        First component mass in solar masses\n    m2 : float or numpy.array\n        Second component mass in solar masses\n    s1z : float or numpy.array\n        First component dimensionless spin S_1/m_1^2 projected onto L\n    s2z : float or numpy.array\n        Second component dimensionless spin S_2/m_2^2 projected onto L\n\n    Returns\n    -------\n    f : float or numpy.array\n        Frequency in Hz\n    \"\"\"\n    params = {\"mass1\":m1, \"mass2\":m2, \"spin1z\":s1z, \"spin2z\":s2z}\n    return named_frequency_cutoffs[name](params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_imr_duration(m1, m2, s1z, s2z, f_low, approximant=\"SEOBNRv4\"):\n    m1, m2, s1z, s2z, f_low = float(m1), float(m2), float(s1z), float(s2z),\\\n                              float(f_low)\n    if approximant == \"SEOBNRv2\":\n        chi = lalsimulation.SimIMRPhenomBComputeChi(m1, m2, s1z, s2z)\n        time_length = lalsimulation.SimIMRSEOBNRv2ChirpTimeSingleSpin(\n                                m1 * lal.MSUN_SI, m2 * lal.MSUN_SI, chi, f_low)\n    elif approximant == \"IMRPhenomD\":\n        time_length = lalsimulation.SimIMRPhenomDChirpTime(\n                           m1 * lal.MSUN_SI, m2 * lal.MSUN_SI, s1z, s2z, f_low)\n    elif approximant == \"SEOBNRv4\":\n        # NB for no clear reason this function has f_low as first argument\n        time_length = lalsimulation.SimIMRSEOBNRv4ROMTimeOfFrequency(\n                           f_low, m1 * lal.MSUN_SI, m2 * lal.MSUN_SI, s1z, s2z)\n    else:\n        raise RuntimeError(\"I can't calculate a duration for %s\" % approximant)\n    # FIXME Add an extra factor of 1.1 for 'safety' since the duration\n    # functions are approximate\n    return time_length * 1.1", "response": "This function calculates the duration of the IMR model."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_inspiral_tf(tc, mass1, mass2, spin1, spin2, f_low, n_points=100,\n        pn_2order=7, approximant='TaylorF2'):\n    \"\"\"Compute the time-frequency evolution of an inspiral signal.\n\n    Return a tuple of time and frequency vectors tracking the evolution of an\n    inspiral signal in the time-frequency plane.\n    \"\"\"\n    # handle param-dependent approximant specification\n    class Params:\n        pass\n    params = Params()\n    params.mass1 = mass1\n    params.mass2 = mass2\n    params.spin1z = spin1\n    params.spin2z = spin2\n    try:\n        approximant = eval(approximant, {'__builtins__': None},\n                           dict(params=params))\n    except NameError:\n        pass\n\n    if approximant in ['TaylorF2', 'SPAtmplt']:\n        from pycbc.waveform.spa_tmplt import findchirp_chirptime\n\n        # FIXME spins are not taken into account\n        f_high = f_SchwarzISCO(mass1 + mass2)\n        track_f = numpy.logspace(numpy.log10(f_low), numpy.log10(f_high),\n                                 n_points)\n        track_t = numpy.array([findchirp_chirptime(float(mass1), float(mass2),\n                                        float(f), pn_2order) for f in track_f])\n    elif approximant in ['SEOBNRv2', 'SEOBNRv2_ROM_DoubleSpin',\n                         'SEOBNRv2_ROM_DoubleSpin_HI']:\n        f_high = get_final_freq('SEOBNRv2', mass1, mass2, spin1, spin2)\n        track_f = numpy.logspace(numpy.log10(f_low), numpy.log10(f_high),\n                                 n_points)\n        # use HI function as it has wider freq range validity\n        track_t = numpy.array([\n                lalsimulation.SimIMRSEOBNRv2ROMDoubleSpinHITimeOfFrequency(f,\n                    solar_mass_to_kg(mass1), solar_mass_to_kg(mass2),\n                    float(spin1), float(spin2)) for f in track_f])\n    elif approximant in ['SEOBNRv4', 'SEOBNRv4_ROM']:\n        f_high = get_final_freq('SEOBNRv4', mass1, mass2, spin1, spin2)\n        # use frequency below final freq in case of rounding error\n        track_f = numpy.logspace(numpy.log10(f_low), numpy.log10(0.999*f_high),\n                                 n_points)\n        track_t = numpy.array([\n                lalsimulation.SimIMRSEOBNRv4ROMTimeOfFrequency(\n                        f, solar_mass_to_kg(mass1), solar_mass_to_kg(mass2),\n                        float(spin1), float(spin2)) for f in track_f])\n    else:\n        raise ValueError('Approximant ' + approximant + ' not supported')\n    return (tc - track_t, track_f)", "response": "Compute the time - frequency evolution of an inspiral signal in the time - frequency plane."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the energy coefficients of the center - of - mass and spin.", "response": "def _energy_coeffs(m1, m2, chi1, chi2):\n    \"\"\" Return the center-of-mass energy coefficients up to 3.0pN (2.5pN spin)\n    \"\"\"\n    mtot = m1 + m2\n    eta = m1*m2 / (mtot*mtot)\n    chi = (m1*chi1 + m2*chi2) / mtot\n    chisym = (chi1 + chi2) / 2.\n    beta = (113.*chi - 76.*eta*chisym)/12.\n    sigma12 = 79.*eta*chi1*chi2/8.\n    sigmaqm = 81.*m1*m1*chi1*chi1/(16.*mtot*mtot) \\\n            + 81.*m2*m2*chi2*chi2/(16.*mtot*mtot)\n\n    energy0 = -0.5*eta\n    energy2 = -0.75 - eta/12.\n    energy3 = 0.\n    energy4 = -3.375 + (19*eta)/8. - pow(eta,2)/24.\n    energy5 = 0.\n    energy6 = -10.546875 - (155*pow(eta,2))/96. - (35*pow(eta,3))/5184. \\\n                + eta*(59.80034722222222 - (205*pow(lal.PI,2))/96.)\n\n    energy3 += (32*beta)/113. + (52*chisym*eta)/113.\n\n    energy4 += (-16*sigma12)/79. - (16*sigmaqm)/81.\n    energy5 += (96*beta)/113. + ((-124*beta)/339. - (522*chisym)/113.)*eta \\\n                - (710*chisym*pow(eta,2))/339.\n\n    return (energy0, energy2, energy3, energy4, energy5, energy6)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the velocity of the minimum energy cutoff for 3. 5pN spin.", "response": "def meco_velocity(m1, m2, chi1, chi2):\n    \"\"\"\n    Returns the velocity of the minimum energy cutoff for 3.5pN (2.5pN spin)\n\n    Parameters\n    ----------\n    m1 : float\n        First component mass in solar masses\n    m2 : float\n        Second component mass in solar masses\n    chi1 : float\n        First component dimensionless spin S_1/m_1^2 projected onto L\n    chi2 : float\n        Second component dimensionless spin S_2/m_2^2 projected onto L\n\n    Returns\n    -------\n    v : float\n        Velocity (dimensionless)\n    \"\"\"\n    _, energy2, energy3, energy4, energy5, energy6 = \\\n        _energy_coeffs(m1, m2, chi1, chi2)\n    def eprime(v):\n        return 2. + v * v * (4.*energy2 + v * (5.*energy3 \\\n                + v * (6.*energy4\n                + v * (7.*energy5 + 8.*energy6 * v))))\n    return bisect(eprime, 0.05, 1.0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the frequency of the minimum energy cutoff for 3. 5pN spin.", "response": "def _meco_frequency(m1, m2, chi1, chi2):\n    \"\"\"Returns the frequency of the minimum energy cutoff for 3.5pN (2.5pN spin)\n    \"\"\"\n    return velocity_to_frequency(meco_velocity(m1, m2, chi1, chi2), m1+m2)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _dtdv_coeffs(m1, m2, chi1, chi2):\n    mtot = m1 + m2\n    eta = m1*m2 / (mtot*mtot)\n    chi = (m1*chi1 + m2*chi2) / mtot\n    chisym = (chi1 + chi2) / 2.\n    beta = (113.*chi - 76.*eta*chisym)/12.\n    sigma12 = 79.*eta*chi1*chi2/8.\n    sigmaqm = 81.*m1*m1*chi1*chi1/(16.*mtot*mtot) \\\n            + 81.*m2*m2*chi2*chi2/(16.*mtot*mtot)\n\n    dtdv0 = 1. # FIXME: Wrong but doesn't matter for now.\n    dtdv2 = (1./336.) * (743. + 924.*eta)\n    dtdv3 = -4. * lal.PI + beta\n    dtdv4 = (3058673. + 5472432.*eta + 4353552.*eta*eta)/1016064. - sigma12 - sigmaqm\n    dtdv5 = (1./672.) * lal.PI * (-7729. + 1092.*eta) + (146597.*beta/18984. + 42.*beta*eta/113. - 417307.*chisym*eta/18984. - 1389.*chisym*eta*eta/226.)\n    dtdv6 = 22.065 + 165.416*eta - 2.20067*eta*eta + 4.93152*eta*eta*eta\n    dtdv6log = 1712./315.\n    dtdv7 = (lal.PI/1016064.) * (-15419335. - 12718104.*eta + 4975824.*eta*eta)\n\n    return (dtdv0, dtdv2, dtdv3, dtdv4, dtdv5, dtdv6, dtdv6log, dtdv7)", "response": "Returns the dtv coefficients up to 3. 5pN spin"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef energy_coefficients(m1, m2, s1z=0, s2z=0, phase_order=-1, spin_order=-1):\n    implemented_phase_order = 7\n    implemented_spin_order = 7\n    if phase_order > implemented_phase_order:\n        raise ValueError(\"pN coeffiecients of that order have not been implemented\")\n    elif phase_order == -1:\n        phase_order = implemented_phase_order\n\n    if spin_order > implemented_spin_order:\n        raise ValueError(\"pN coeffiecients of that order have not been implemented\")\n    elif spin_order == -1:\n        spin_order = implemented_spin_order\n\n    qmdef1 = 1.0\n    qmdef2 = 1.0\n\n    M = m1 + m2\n    dm = (m1-m2)/M\n    m1M = m1 / M\n    m2M = m2 / M\n\n    s1z = s1z * m1M * m1M\n    s2z = s2z * m2M * m2M\n\n    _, eta = mass1_mass2_to_mchirp_eta(m1, m2)\n\n    ecof = numpy.zeros(phase_order+1)\n    # Orbital terms\n    if phase_order >= 0:\n        ecof[0] = 1.0\n    if phase_order >= 1:\n        ecof[1] = 0\n    if phase_order >= 2:\n        ecof[2] = -(1.0/12.0) * (9.0 + eta)\n    if phase_order >= 3:\n        ecof[3] = 0\n    if phase_order >= 4:\n        ecof[4] = (-81.0 + 57.0*eta - eta*eta) / 24.0\n    if phase_order >= 5:\n        ecof[5] = 0\n    if phase_order >= 6:\n        ecof[6] = - 675.0/64.0 + ( 34445.0/576.0    \\\n              - 205.0/96.0 * lal.PI * lal.PI ) * eta  \\\n              - (155.0/96.0) *eta * eta - 35.0/5184.0 * eta * eta\n    # Spin terms\n\n    ESO15s1 = 8.0/3.0 + 2.0*m2/m1\n    ESO15s2 = 8.0/3.0 + 2.0*m1/m2\n\n    ESS2 = 1.0 / eta\n    EQM2s1 = qmdef1/2.0/m1M/m1M\n    EQM2s1L = -qmdef1*3.0/2.0/m1M/m1M\n    #EQM2s2 = qmdef2/2.0/m2M/m2M\n    EQM2s2L = -qmdef2*3.0/2.0/m2M/m2M\n\n    ESO25s1 = 11.0 - 61.0*eta/9.0 + (dm/m1M) * (-3.0 + 10.*eta/3.0)\n    ESO25s2 = 11.0 - 61.0*eta/9.0 + (dm/m2M) * (3.0 - 10.*eta/3.0)\n\n    ESO35s1 = 135.0/4.0 - 367.0*eta/4.0 + 29.0*eta*eta/12.0 + (dm/m1M) * (-27.0/4.0 + 39.0*eta - 5.0*eta*eta/4.0)\n    ESO35s2 = 135.0/4.0 - 367.0*eta/4.0 + 29.0*eta*eta/12.0 - (dm/m2M) * (-27.0/4.0 + 39.0*eta - 5.0*eta*eta/4.0)\n\n    if spin_order >=3:\n        ecof[3] += ESO15s1 * s1z + ESO15s2 * s2z\n    if spin_order >=4:\n        ecof[4] += ESS2 * (s1z*s2z - 3.0*s1z*s2z)\n        ecof[4] += EQM2s1*s1z*s1z + EQM2s1*s2z*s2z + EQM2s1L*s1z*s1z + EQM2s2L*s2z*s2z\n    if spin_order >=5:\n        ecof[5] = ESO25s1*s1z + ESO25s2*s2z\n    if spin_order >=7:\n        ecof[7] += ESO35s1*s1z + ESO35s2*s2z\n\n    return ecof", "response": "Return the energy coefficients of a single system."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef kerr_lightring_velocity(chi):\n    # If chi > 0.9996, the algorithm cannot solve the function\n    if chi >= 0.9996:\n        return brentq(kerr_lightring, 0, 0.8, args=(0.9996))\n    else:\n        return brentq(kerr_lightring, 0, 0.8, args=(chi))", "response": "Return the velocity at the Kerr light ring"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef hybridEnergy(v, m1, m2, chi1, chi2, qm1, qm2):\n    pi_sq = numpy.pi**2\n    v2, v3, v4, v5, v6, v7 = v**2, v**3, v**4, v**5, v**6, v**7\n    chi1_sq, chi2_sq = chi1**2, chi2**2\n    m1, m2 = float(m1), float(m2)\n    M = float(m1 + m2)\n    M_2, M_4 = M**2, M**4\n    eta = m1 * m2 / M_2\n    eta2, eta3 = eta**2, eta**3\n    m1_2, m1_4 = m1**2, m1**4\n    m2_2, m2_4 = m2**2, m2**4\n\n    chi = (chi1 * m1 + chi2 * m2) / M\n    Kerr = -1. + (1. - 2. * v2 * (1. - chi * v3)**(1./3.)) / \\\n        numpy.sqrt((1. - chi * v3) * (1. + chi * v3 - 3. * v2 * (1 - chi * v3)**(1./3.)))\n\n    h_E = Kerr - \\\n        (v2 / 2.) * \\\n        (\n        - eta * v2 / 12. - 2 * (chi1 + chi2) * eta * v3 / 3. +\n        (19. * eta / 8. - eta2 / 24. + chi1_sq * m1_2 * (1 - qm1) / M_2 +\n            chi2_sq * m2_2 * (1 - qm2) / M_2) * v4\n        - 1. / 9. * (120. * (chi1 + chi2) * eta2 +\n            (76. * chi1 + 45. * chi2) * m1_2 * eta / M_2 +\n            (45. * chi1 + 76. * chi2) * m2_2 * eta / M_2) * v5\n        + (34445. * eta / 576. - 205. * pi_sq * eta / 96. - 155. * eta2 / 96. -\n            35. * eta3 / 5184. +\n            5. / 18. * (21. * chi1_sq * (1. - qm1) * m1_4 / M_4 +\n            21. * chi2_sq * (1. - qm2) * m2_4 / M_4 +\n            (chi1_sq * (56. - 27. * qm1) + 20. * chi1 * chi2) * eta * m1_2 / M_2 +\n            (chi2_sq * (56. - 27. * qm2) + 20. * chi1 * chi2) * eta * m2_2 / M_2 +\n            (chi1_sq * (31. - 9. * qm1) + 38. * chi1 * chi2 +\n            chi2_sq * (31. - 9. * qm2)) * eta2)) * v6\n        - eta / 12. * (3. * (292. * chi1 + 81. * chi2) * m1_4 / M_4 +\n            3. * (81. * chi1 + 292. * chi2) * m2_4 / M_4 +\n            4. * (673. * chi1 + 360. * chi2) * eta * m1_2 / M_2 +\n            4. * (360. * chi1 + 673. * chi2) * eta * m2_2 / M_2 +\n            3012. * eta2 * (chi1 + chi2)) * v7\n        )\n\n    return h_E", "response": "Return the hybrid MECO energy for a given set of solar masses."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef hybrid_meco_velocity(m1, m2, chi1, chi2, qm1=None, qm2=None):\n\n    if qm1 is None:\n        qm1 = 1\n    if qm2 is None:\n        qm2 = 1\n\n    # Set bounds at 0.1 to skip v=0 and at the lightring velocity\n    chi = (chi1 * m1 + chi2 * m2) / (m1 + m2)\n    vmax = kerr_lightring_velocity(chi) - 0.01\n\n    return minimize(hybridEnergy, 0.2, args=(m1, m2, chi1, chi2, qm1, qm2),\n                    bounds=[(0.1, vmax)]).x.item()", "response": "Return the velocity of the hybrid MECO."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef hybrid_meco_frequency(m1, m2, chi1, chi2, qm1=None, qm2=None):\n    if qm1 is None:\n        qm1 = 1\n    if qm2 is None:\n        qm2 = 1\n\n    return velocity_to_frequency(hybrid_meco_velocity(m1, m2, chi1, chi2, qm1, qm2), m1 + m2)", "response": "Return the frequency of the hybrid MECO."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_parameters(self, parameters, array_class=None):\n        # get the type of array class to use\n        if array_class is None:\n            array_class = FieldArray\n        # get the names of fields needed for the given parameters\n        possible_fields = self[self.samples_group].keys()\n        return array_class.parse_parameters(parameters, possible_fields)", "response": "Parses a list of strings containing the parameters of the any\n            field."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread samples for the given parameter list.", "response": "def read_samples(self, parameters, array_class=None, **kwargs):\n        \"\"\"Reads samples for the given parameter(s).\n\n        The ``parameters`` can be the name of any dataset in ``samples_group``,\n        a virtual field or method of ``FieldArray`` (as long as the file\n        contains the necessary fields to derive the virtual field or method),\n        and/or any numpy function of these.\n\n        The ``parameters`` are parsed to figure out what datasets are needed.\n        Only those datasets will be loaded, and will be the base-level fields\n        of the returned ``FieldArray``.\n\n        The ``static_params`` are also added as attributes of the returned\n        ``FieldArray``.\n\n        Parameters\n        -----------\n        fp : InferenceFile\n            An open file handler to read the samples from.\n        parameters : (list of) strings\n            The parameter(s) to retrieve.\n        array_class : FieldArray-like class, optional\n            The type of array to return. The class must have ``from_kwargs``\n            and ``parse_parameters`` methods. If None, will return a\n            ``FieldArray``.\n        \\**kwargs :\n            All other keyword arguments are passed to ``read_raw_samples``.\n\n        Returns\n        -------\n        FieldArray :\n            The samples as a ``FieldArray``.\n        \"\"\"\n        # get the type of array class to use\n        if array_class is None:\n            array_class = FieldArray\n        # get the names of fields needed for the given parameters\n        possible_fields = self[self.samples_group].keys()\n        loadfields = array_class.parse_parameters(parameters, possible_fields)\n        samples = self.read_raw_samples(loadfields, **kwargs)\n        # convert to FieldArray\n        samples = array_class.from_kwargs(**samples)\n        # add the static params and attributes\n        addatrs = (self.static_params.items() +\n                   self[self.samples_group].attrs.items())\n        for (p, val) in addatrs:\n            setattr(samples, p, val)\n        return samples"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_optional_args(args, opts, err_on_missing=False, **kwargs):\n        parsed = {}\n        for arg in args:\n            try:\n                parsed[arg] = getattr(opts, arg)\n            except AttributeError as e:\n                if err_on_missing:\n                    raise AttributeError(e)\n                else:\n                    continue\n        parsed.update(kwargs)\n        return parsed", "response": "Convenience function to retrieve optional arguments from an argparse. Namespace object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef samples_from_cli(self, opts, parameters=None, **kwargs):\n        if parameters is None and opts.parameters is None:\n            parameters = self.variable_args\n        elif parameters is None:\n            parameters = opts.parameters\n        # parse optional arguments\n        _, extra_actions = self.extra_args_parser()\n        extra_args = [act.dest for act in extra_actions]\n        kwargs = self._get_optional_args(extra_args, opts, **kwargs)\n        return self.read_samples(parameters, **kwargs)", "response": "Reads samples from the given command - line options."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the last saved command line.", "response": "def cmd(self):\n        \"\"\"Returns the (last) saved command line.\n\n        If the file was created from a run that resumed from a checkpoint, only\n        the last command line used is returned.\n\n        Returns\n        -------\n        cmd : string\n            The command line that created this InferenceFile.\n        \"\"\"\n        cmd = self.attrs[\"cmd\"]\n        if isinstance(cmd, numpy.ndarray):\n            cmd = cmd[-1]\n        return cmd"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write_logevidence(self, lnz, dlnz):\n        self.attrs['log_evidence'] = lnz\n        self.attrs['dlog_evidence'] = dlnz", "response": "Writes the given log evidence and its error in the estimate of the log evidence."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write_random_state(self, group=None, state=None):\n        group = self.sampler_group if group is None else group\n        dataset_name = \"/\".join([group, \"random_state\"])\n        if state is None:\n            state = numpy.random.get_state()\n        s, arr, pos, has_gauss, cached_gauss = state\n        if dataset_name in self:\n            self[dataset_name][:] = arr\n        else:\n            self.create_dataset(dataset_name, arr.shape, fletcher32=True,\n                                dtype=arr.dtype)\n            self[dataset_name][:] = arr\n        self[dataset_name].attrs[\"s\"] = s\n        self[dataset_name].attrs[\"pos\"] = pos\n        self[dataset_name].attrs[\"has_gauss\"] = has_gauss\n        self[dataset_name].attrs[\"cached_gauss\"] = cached_gauss", "response": "Writes the random state of the random number generator from the file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading the state of the random number generator from the file.", "response": "def read_random_state(self, group=None):\n        \"\"\"Reads the state of the random number generator from the file.\n\n        Parameters\n        ----------\n        group : str\n            Name of group to read random state from.\n\n        Returns\n        -------\n        tuple\n            A tuple with 5 elements that can be passed to numpy.set_state.\n        \"\"\"\n        group = self.sampler_group if group is None else group\n        dataset_name = \"/\".join([group, \"random_state\"])\n        arr = self[dataset_name][:]\n        s = self[dataset_name].attrs[\"s\"]\n        pos = self[dataset_name].attrs[\"pos\"]\n        has_gauss = self[dataset_name].attrs[\"has_gauss\"]\n        cached_gauss = self[dataset_name].attrs[\"cached_gauss\"]\n        return s, arr, pos, has_gauss, cached_gauss"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write_strain(self, strain_dict, group=None):\n        subgroup = self.data_group + \"/{ifo}/strain\"\n        if group is None:\n            group = subgroup\n        else:\n            group = '/'.join([group, subgroup])\n        for ifo, strain in strain_dict.items():\n            self[group.format(ifo=ifo)] = strain\n            self[group.format(ifo=ifo)].attrs['delta_t'] = strain.delta_t\n            self[group.format(ifo=ifo)].attrs['start_time'] = \\\n                float(strain.start_time)", "response": "Writes the strain for each IFO to file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_stilde(self, stilde_dict, group=None):\n        subgroup = self.data_group + \"/{ifo}/stilde\"\n        if group is None:\n            group = subgroup\n        else:\n            group = '/'.join([group, subgroup])\n        for ifo, stilde in stilde_dict.items():\n            self[group.format(ifo=ifo)] = stilde\n            self[group.format(ifo=ifo)].attrs['delta_f'] = stilde.delta_f\n            self[group.format(ifo=ifo)].attrs['epoch'] = float(stilde.epoch)", "response": "Writes the stilde for each IFO to file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_psd(self, psds, group=None):\n        subgroup = self.data_group + \"/{ifo}/psds/0\"\n        if group is None:\n            group = subgroup\n        else:\n            group = '/'.join([group, subgroup])\n        for ifo in psds:\n            self[group.format(ifo=ifo)] = psds[ifo]\n            self[group.format(ifo=ifo)].attrs['delta_f'] = psds[ifo].delta_f", "response": "Writes the PSD for each IFO to file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_injections(self, injection_file):\n        try:\n            with h5py.File(injection_file, \"r\") as fp:\n                super(BaseInferenceFile, self).copy(fp, self.injections_group)\n        except IOError:\n            logging.warn(\"Could not read %s as an HDF file\", injection_file)", "response": "Writes injection parameters from the given injection file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread the injections from the file.", "response": "def read_injections(self):\n        \"\"\"Gets injection parameters.\n\n        Returns\n        -------\n        FieldArray\n            Array of the injection parameters.\n        \"\"\"\n        injset = InjectionSet(self.filename, hdf_group=self.injections_group)\n        injections = injset.table.view(FieldArray)\n        # close the new open filehandler to self\n        injset._injhandler.filehandler.close()\n        return injections"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write_command_line(self):\n        cmd = [\" \".join(sys.argv)]\n        try:\n            previous = self.attrs[\"cmd\"]\n            if isinstance(previous, str):\n                # convert to list\n                previous = [previous]\n            elif isinstance(previous, numpy.ndarray):\n                previous = previous.tolist()\n        except KeyError:\n            previous = []\n        self.attrs[\"cmd\"] = cmd + previous", "response": "Writes the command line to the attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_slice(self, thin_start=None, thin_interval=None, thin_end=None):\n        if thin_start is None:\n            thin_start = int(self.thin_start)\n        else:\n            thin_start = int(thin_start)\n        if thin_interval is None:\n            thin_interval = self.thin_interval\n        else:\n            thin_interval = int(numpy.ceil(thin_interval))\n        if thin_end is None:\n            thin_end = self.thin_end\n        else:\n            thin_end = int(thin_end)\n        return slice(thin_start, thin_end, thin_interval)", "response": "Formats a slice using the given arguments that can be used to retrieve a thinned array from an InferenceFile."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef copy_metadata(self, other):\n        logging.info(\"Copying metadata\")\n        # copy attributes\n        for key in self.attrs.keys():\n            other.attrs[key] = self.attrs[key]", "response": "Copies all metadata from this file to the other file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncopy info from this file to the other.", "response": "def copy_info(self, other, ignore=None):\n        \"\"\"Copies \"info\" from this file to the other.\n\n        \"Info\" is defined all groups that are not the samples group.\n\n        Parameters\n        ----------\n        other : output file\n            The output file. Must be an hdf file.\n        ignore : (list of) str\n            Don't copy the given groups.\n        \"\"\"\n        logging.info(\"Copying info\")\n        # copy non-samples/stats data\n        if ignore is None:\n            ignore = []\n        if isinstance(ignore, (str, unicode)):\n            ignore = [ignore]\n        ignore = set(ignore + [self.samples_group])\n        copy_groups = set(self.keys()) - ignore\n        for key in copy_groups:\n            super(BaseInferenceFile, self).copy(key, other)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef copy_samples(self, other, parameters=None, parameter_names=None,\n                     read_args=None, write_args=None):\n        \"\"\"Should copy samples to the other files.\n\n        Parameters\n        ----------\n        other : InferenceFile\n            An open inference file to write to.\n        parameters : list of str, optional\n            List of parameters to copy. If None, will copy all parameters.\n        parameter_names : dict, optional\n            Rename one or more parameters to the given name. The dictionary\n            should map parameter -> parameter name. If None, will just use the\n            original parameter names.\n        read_args : dict, optional\n            Arguments to pass to ``read_samples``.\n        write_args : dict, optional\n            Arguments to pass to ``write_samples``.\n        \"\"\"\n        # select the samples to copy\n        logging.info(\"Reading samples to copy\")\n        if parameters is None:\n            parameters = self.variable_params\n        # if list of desired parameters is different, rename\n        if set(parameters) != set(self.variable_params):\n            other.attrs['variable_params'] = parameters\n        samples = self.read_samples(parameters, **read_args)\n        logging.info(\"Copying {} samples\".format(samples.size))\n        # if different parameter names are desired, get them from the samples\n        if parameter_names:\n            arrs = {pname: samples[p] for p, pname in parameter_names.items()}\n            arrs.update({p: samples[p] for p in parameters if\n                         p not in parameter_names})\n            samples = FieldArray.from_kwargs(**arrs)\n            other.attrs['variable_params'] = samples.fieldnames\n        logging.info(\"Writing samples\")\n        other.write_samples(other, samples, **write_args)", "response": "Copy samples from this InferenceFile to another InferenceFile."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef copy(self, other, ignore=None, parameters=None, parameter_names=None,\n             read_args=None, write_args=None):\n        \"\"\"Copies metadata, info, and samples in this file to another file.\n\n        Parameters\n        ----------\n        other : str or InferenceFile\n            The file to write to. May be either a string giving a filename,\n            or an open hdf file. If the former, the file will be opened with\n            the write attribute (note that if a file already exists with that\n            name, it will be deleted).\n        ignore : (list of) strings\n            Don't copy the given groups. If the samples group is included, no\n            samples will be copied.\n        parameters : list of str, optional\n            List of parameters in the samples group to copy. If None, will copy\n            all parameters.\n        parameter_names : dict, optional\n            Rename one or more parameters to the given name. The dictionary\n            should map parameter -> parameter name. If None, will just use the\n            original parameter names.\n        read_args : dict, optional\n            Arguments to pass to ``read_samples``.\n        write_args : dict, optional\n            Arguments to pass to ``write_samples``.\n\n        Returns\n        -------\n        InferenceFile\n            The open file handler to other.\n        \"\"\"\n        if not isinstance(other, h5py.File):\n            # check that we're not trying to overwrite this file\n            if other == self.name:\n                raise IOError(\"destination is the same as this file\")\n            other = self.__class__(other, 'w')\n        # metadata\n        self.copy_metadata(other)\n        # info\n        if ignore is None:\n            ignore = []\n        if isinstance(ignore, (str, unicode)):\n            ignore = [ignore]\n        self.copy_info(other, ignore=ignore)\n        # samples\n        if self.samples_group not in ignore:\n            self.copy_samples(other, parameters=parameters,\n                              parameter_names=parameter_names,\n                              read_args=read_args,\n                              write_args=write_args)\n            # if any down selection was done, re-set the default\n            # thin-start/interval/end\n            p = self[self.samples_group].keys()[0]\n            my_shape = self[self.samples_group][p].shape\n            p = other[other.samples_group].keys()[0]\n            other_shape = other[other.samples_group][p].shape\n            if my_shape != other_shape:\n                other.attrs['thin_start'] = 0\n                other.attrs['thin_interval'] = 1\n                other.attrs['thin_end'] = None\n        return other", "response": "Copies metadata info and samples in this file to another file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write_kwargs_to_attrs(cls, attrs, **kwargs):\n        for arg, val in kwargs.items():\n            if val is None:\n                val = str(None)\n            if isinstance(val, dict):\n                attrs[arg] = val.keys()\n                # just call self again with the dict as kwargs\n                cls.write_kwargs_to_attrs(attrs, **val)\n            else:\n                attrs[arg] = val", "response": "Writes the given keywords to the given attrs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef coalign_waveforms(h1, h2, psd=None,\n                      low_frequency_cutoff=None,\n                      high_frequency_cutoff=None,\n                      resize=True):\n    \"\"\" Return two time series which are aligned in time and phase.\n\n    The alignment is only to the nearest sample point and all changes to the\n    phase are made to the first input waveform. Waveforms should not be split\n    accross the vector boundary. If it is, please use roll or cyclic time shift\n    to ensure that the entire signal is contiguous in the time series.\n\n    Parameters\n    ----------\n    h1: pycbc.types.TimeSeries\n        The first waveform to align.\n    h2: pycbc.types.TimeSeries\n        The second waveform to align.\n    psd: {None, pycbc.types.FrequencySeries}\n        A psd to weight the alignment\n    low_frequency_cutoff: {None, float}\n        The low frequency cutoff to weight the matching in Hz.\n    high_frequency_cutoff: {None, float}\n        The high frequency cutoff to weight the matching in Hz.\n    resize: Optional, {True, boolean}\n        If true, the vectors will be resized to match each other. If false,\n        they must be the same length and even in length\n\n    Returns\n    -------\n    h1: pycbc.types.TimeSeries\n        The shifted waveform to align with h2\n    h2: pycbc.type.TimeSeries\n        The resized (if necessary) waveform to align with h1.\n    \"\"\"\n    from pycbc.filter import matched_filter\n    mlen = ceilpow2(max(len(h1), len(h2)))\n\n    h1 = h1.copy()\n    h2 = h2.copy()\n\n    if resize:\n        h1.resize(mlen)\n        h2.resize(mlen)\n    elif len(h1) != len(h2) or len(h2) % 2 != 0:\n        raise ValueError(\"Time series must be the same size and even if you do \"\n                         \"not allow resizing\")\n\n    snr = matched_filter(h1, h2, psd=psd,\n                         low_frequency_cutoff=low_frequency_cutoff,\n                         high_frequency_cutoff=high_frequency_cutoff)\n\n    _, l =  snr.abs_max_loc()\n    rotation =  snr[l] / abs(snr[l])\n    h1 = (h1.to_frequencyseries() * rotation).to_timeseries()\n    h1.roll(l)\n\n    h1 = TimeSeries(h1, delta_t=h2.delta_t, epoch=h2.start_time)\n    return h1, h2", "response": "Return two time series which are aligned in time and phase."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef phase_from_frequencyseries(htilde, remove_start_phase=True):\n    p = numpy.unwrap(numpy.angle(htilde.data)).astype(\n            real_same_precision_as(htilde))\n    if remove_start_phase:\n        p += -p[0]\n    return FrequencySeries(p, delta_f=htilde.delta_f, epoch=htilde.epoch,\n        copy=False)", "response": "Returns the phase of the given frequency - domain waveform."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef amplitude_from_frequencyseries(htilde):\n    amp = abs(htilde.data).astype(real_same_precision_as(htilde))\n    return FrequencySeries(amp, delta_f=htilde.delta_f, epoch=htilde.epoch,\n        copy=False)", "response": "Returns the amplitude of the given frequency - domain waveform as a FrequencySeries."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the time evolution of a frequency - domain waveform.", "response": "def time_from_frequencyseries(htilde, sample_frequencies=None,\n        discont_threshold=0.99*numpy.pi):\n    \"\"\"Computes time as a function of frequency from the given\n    frequency-domain waveform. This assumes the stationary phase\n    approximation. Any frequencies lower than the first non-zero value in\n    htilde are assigned the time at the first non-zero value. Times for any\n    frequencies above the next-to-last non-zero value in htilde will be\n    assigned the time of the next-to-last non-zero value.\n\n    .. note::\n        Some waveform models (e.g., `SEOBNRv2_ROM_DoubleSpin`) can have\n        discontinuities in the phase towards the end of the waveform due to\n        numerical error. We therefore exclude any points that occur after a\n        discontinuity in the phase, as the time estimate becomes untrustworthy\n        beyond that point. What determines a discontinuity in the phase is set\n        by the `discont_threshold`. To turn this feature off, just set\n        `discont_threshold` to a value larger than pi (due to the unwrapping\n        of the phase, no two points can have a difference > pi).\n\n    Parameters\n    ----------\n    htilde : FrequencySeries\n        The waveform to get the time evolution of; must be complex.\n    sample_frequencies : {None, array}\n        The frequencies at which the waveform is sampled. If None, will\n        retrieve from ``htilde.sample_frequencies``.\n    discont_threshold : {0.99*pi, float}\n        If the difference in the phase changes by more than this threshold,\n        it is considered to be a discontinuity. Default is 0.99*pi.\n\n    Returns\n    -------\n    FrequencySeries\n        The time evolution of the waveform as a function of frequency.\n    \"\"\"\n    if sample_frequencies is None:\n        sample_frequencies = htilde.sample_frequencies.numpy()\n    phase = phase_from_frequencyseries(htilde).data\n    dphi = numpy.diff(phase)\n    time = -dphi / (2.*numpy.pi*numpy.diff(sample_frequencies))\n    nzidx = numpy.nonzero(abs(htilde.data))[0]\n    kmin, kmax = nzidx[0], nzidx[-2]\n    # exclude everything after a discontinuity\n    discont_idx = numpy.where(abs(dphi[kmin:]) >= discont_threshold)[0]\n    if discont_idx.size != 0:\n        kmax = min(kmax, kmin + discont_idx[0]-1)\n    time[:kmin] = time[kmin]\n    time[kmax:] = time[kmax]\n    return FrequencySeries(time.astype(real_same_precision_as(htilde)),\n        delta_f=htilde.delta_f, epoch=htilde.epoch,\n        copy=False)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the gravitational wave phase from the h_plus and h_cross polarizations of the waveform.", "response": "def phase_from_polarizations(h_plus, h_cross, remove_start_phase=True):\n    \"\"\"Return gravitational wave phase\n\n    Return the gravitation-wave phase from the h_plus and h_cross\n    polarizations of the waveform. The returned phase is always\n    positive and increasing with an initial phase of 0.\n\n    Parameters\n    ----------\n    h_plus : TimeSeries\n        An PyCBC TmeSeries vector that contains the plus polarization of the\n        gravitational waveform.\n    h_cross : TimeSeries\n        A PyCBC TmeSeries vector that contains the cross polarization of the\n        gravitational waveform.\n\n    Returns\n    -------\n    GWPhase : TimeSeries\n        A TimeSeries containing the gravitational wave phase.\n\n    Examples\n    --------s\n    >>> from pycbc.waveform import get_td_waveform, phase_from_polarizations\n    >>> hp, hc = get_td_waveform(approximant=\"TaylorT4\", mass1=10, mass2=10,\n                         f_lower=30, delta_t=1.0/4096)\n    >>> phase = phase_from_polarizations(hp, hc)\n\n    \"\"\"\n    p = numpy.unwrap(numpy.arctan2(h_cross.data, h_plus.data)).astype(\n        real_same_precision_as(h_plus))\n    if remove_start_phase:\n        p += -p[0]\n    return TimeSeries(p, delta_t=h_plus.delta_t, epoch=h_plus.start_time,\n        copy=False)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef amplitude_from_polarizations(h_plus, h_cross):\n    amp = (h_plus.squared_norm() + h_cross.squared_norm()) ** (0.5)\n    return TimeSeries(amp, delta_t=h_plus.delta_t, epoch=h_plus.start_time)", "response": "Return the gravitational wave amplitude from the h_plus and h_cross polarizations of the waveform."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the gravitational wave frequency as a function of time from the given polarizations h_plus and h_cross polarizations.", "response": "def frequency_from_polarizations(h_plus, h_cross):\n    \"\"\"Return gravitational wave frequency\n\n    Return the gravitation-wave frequency as a function of time\n    from the h_plus and h_cross polarizations of the waveform.\n    It is 1 bin shorter than the input vectors and the sample times\n    are advanced half a bin.\n\n    Parameters\n    ----------\n    h_plus : TimeSeries\n        A PyCBC TimeSeries vector that contains the plus polarization of the\n        gravitational waveform.\n    h_cross : TimeSeries\n        A PyCBC TimeSeries vector that contains the cross polarization of the\n        gravitational waveform.\n\n    Returns\n    -------\n    GWFrequency : TimeSeries\n        A TimeSeries containing the gravitational wave frequency as a function\n        of time.\n\n    Examples\n    --------\n    >>> from pycbc.waveform import get_td_waveform, phase_from_polarizations\n    >>> hp, hc = get_td_waveform(approximant=\"TaylorT4\", mass1=10, mass2=10,\n                         f_lower=30, delta_t=1.0/4096)\n    >>> freq = frequency_from_polarizations(hp, hc)\n\n    \"\"\"\n    phase = phase_from_polarizations(h_plus, h_cross)\n    freq = numpy.diff(phase) / ( 2 * lal.PI * phase.delta_t )\n    start_time = phase.start_time + phase.delta_t / 2\n    return TimeSeries(freq.astype(real_same_precision_as(h_plus)),\n        delta_t=phase.delta_t, epoch=start_time)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef taper_timeseries(tsdata, tapermethod=None, return_lal=False):\n    if tapermethod is None:\n        raise ValueError(\"Must specify a tapering method (function was called\"\n                         \"with tapermethod=None)\")\n    if tapermethod not in taper_map.keys():\n        raise ValueError(\"Unknown tapering method %s, valid methods are %s\" % \\\n                         (tapermethod, \", \".join(taper_map.keys())))\n    if tsdata.dtype not in (float32, float64):\n        raise TypeError(\"Strain dtype must be float32 or float64, not \"\n                    + str(tsdata.dtype))\n    taper_func = taper_func_map[tsdata.dtype]\n    # make a LAL TimeSeries to pass to the LALSim function\n    ts_lal = tsdata.astype(tsdata.dtype).lal()\n    if taper_map[tapermethod] is not None:\n        taper_func(ts_lal.data, taper_map[tapermethod])\n    if return_lal:\n        return ts_lal\n    else:\n        return TimeSeries(ts_lal.data.data[:], delta_t=ts_lal.deltaT,\n                          epoch=ts_lal.epoch)", "response": "Taper a time series using a wrapped LAL simulation function."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nshift a frequency domain waveform in time.", "response": "def apply_fd_time_shift(htilde, shifttime, kmin=0, fseries=None, copy=True):\n    \"\"\"Shifts a frequency domain waveform in time. The shift applied is\n    shiftime - htilde.epoch.\n\n    Parameters\n    ----------\n    htilde : FrequencySeries\n        The waveform frequency series.\n    shifttime : float\n        The time to shift the frequency series to.\n    kmin : {0, int}\n        The starting index of htilde to apply the time shift. Default is 0.\n    fseries : {None, numpy array}\n        The frequencies of each element in htilde. This is only needed if htilde is not\n        sampled at equal frequency steps.\n    copy : {True, bool}\n        Make a copy of htilde before applying the time shift. If False, the time\n        shift will be applied to htilde's data.\n\n    Returns\n    -------\n    FrequencySeries\n        A frequency series with the waveform shifted to the new time. If makecopy\n        is True, will be a new frequency series; if makecopy is False, will be\n        the same as htilde.\n    \"\"\"\n    dt = float(shifttime - htilde.epoch)\n    if dt == 0.:\n        # no shift to apply, just copy if desired\n        if copy:\n            htilde = 1. * htilde\n    elif isinstance(htilde, FrequencySeries):\n        # FrequencySeries means equally sampled in frequency, use faster shifting\n        htilde = apply_fseries_time_shift(htilde, dt, kmin=kmin, copy=copy)\n    else:\n        if fseries is None:\n            fseries = htilde.sample_frequencies.numpy()\n        shift = Array(numpy.exp(-2j*numpy.pi*dt*fseries),\n                    dtype=complex_same_precision_as(htilde))\n        if copy:\n            htilde = 1. * htilde\n        htilde *= shift\n    return htilde"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\napplies a taper to the given time series.", "response": "def td_taper(out, start, end, beta=8, side='left'):\n    \"\"\"Applies a taper to the given TimeSeries.\n\n    A half-kaiser window is used for the roll-off.\n\n    Parameters\n    ----------\n    out : TimeSeries\n        The ``TimeSeries`` to taper.\n    start : float\n        The time (in s) to start the taper window.\n\n    end : float\n        The time (in s) to end the taper window.\n    beta : int, optional\n        The beta parameter to use for the Kaiser window. See\n        ``scipy.signal.kaiser`` for details. Default is 8.\n    side : {'left', 'right'}\n        The side to apply the taper to. If ``'left'`` (``'right'``), the taper\n        will roll up (down) between ``start`` and ``end``, with all values\n        before ``start`` (after ``end``) set to zero. Default is ``'left'``.\n\n    Returns\n    -------\n    TimeSeries\n        The tapered time series.\n    \"\"\"\n    out = out.copy()\n    width = end - start\n    winlen = 2 * int(width / out.delta_t)\n    window = Array(signal.get_window(('kaiser', beta), winlen))\n    xmin = int((start - out.start_time) / out.delta_t)\n    xmax = xmin + winlen//2\n    if side == 'left':\n        out[xmin:xmax] *= window[:winlen//2]\n        if xmin > 0:\n            out[:xmin].clear()\n    elif side == 'right':\n        out[xmin:xmax] *= window[winlen//2:]\n        if xmax < len(out):\n            out[xmax:].clear()\n    else:\n        raise ValueError(\"unrecognized side argument {}\".format(side))\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fd_taper(out, start, end, beta=8, side='left'):\n    out = out.copy()\n    width = end - start\n    winlen = 2 * int(width / out.delta_f)\n    window = Array(signal.get_window(('kaiser', beta), winlen))\n    kmin = int(start / out.delta_f)\n    kmax = kmin + winlen//2\n    if side == 'left':\n        out[kmin:kmax] *= window[:winlen//2]\n        out[:kmin] *= 0.\n    elif side == 'right':\n        out[kmin:kmax] *= window[winlen//2:]\n        out[kmax:] *= 0.\n    else:\n        raise ValueError(\"unrecognized side argument {}\".format(side))\n    return out", "response": "Applies a taper to the given frequency series."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a FD waveform to TD.", "response": "def fd_to_td(htilde, delta_t=None, left_window=None, right_window=None,\n          left_beta=8, right_beta=8):\n    \"\"\"Converts a FD waveform to TD.\n\n    A window can optionally be applied using ``fd_taper`` to the left or right\n    side of the waveform before being converted to the time domain.\n\n    Parameters\n    ----------\n    htilde : FrequencySeries\n        The waveform to convert.\n    delta_t : float, optional\n        Make the returned time series have the given ``delta_t``.\n    left_window : tuple of float, optional\n        A tuple giving the start and end frequency of the FD taper to apply\n        on the left side. If None, no taper will be applied on the left.\n    right_window : tuple of float, optional\n        A tuple giving the start and end frequency of the FD taper to apply\n        on the right side. If None, no taper will be applied on the right.\n    left_beta : int, optional\n        The beta parameter to use for the left taper. See ``fd_taper`` for\n        details. Default is 8.\n    right_beta : int, optional\n        The beta parameter to use for the right taper. Default is 8.\n\n    Returns\n    -------\n    TimeSeries\n        The time-series representation of ``htilde``.\n    \"\"\"\n    if left_window is not None:\n        start, end = left_window\n        htilde = fd_taper(htilde, start, end, side='left', beta=left_beta)\n    if right_window is not None:\n        start, end = right_window\n        htilde = fd_taper(htilde, start, end, side='right', beta=right_beta)\n    return htilde.to_timeseries(delta_t=delta_t)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a string summarizing the parameter.", "response": "def docstr(self, prefix='', include_label=True):\n        \"\"\"Returns a string summarizing the parameter. Format is:\n        <prefix>``name`` : {``default``, ``dtype``}\n        <prefix>   ``description`` Label: ``label``.\n        \"\"\"\n        outstr = \"%s%s : {%s, %s}\\n\" %(prefix, self.name, str(self.default),\n            str(self.dtype).replace(\"<type '\", '').replace(\"'>\", '')) + \\\n            \"%s    %s\" %(prefix, self.description)\n        if include_label:\n            outstr += \" Label: %s\" %(self.label)\n        return outstr"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the docstr of each parameter joined together.", "response": "def docstr(self, prefix='', include_label=True):\n        \"\"\"Returns the ``docstr`` of each parameter joined together.\"\"\"\n        return '\\n'.join([x.docstr(prefix, include_label) for x in self])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmakes a histogram with an overflow bar above val_max", "response": "def hist_overflow(val, val_max, **kwds):\n    \"\"\" Make a histogram with an overflow bar above val_max \"\"\"\n    import pylab, numpy\n\n    overflow = len(val[val>=val_max])\n    pylab.hist(val[val<val_max], **kwds)\n\n    if 'color' in kwds:\n        color = kwds['color']\n    else:\n        color = None\n\n    if overflow > 0:\n        rect = pylab.bar(val_max+0.05, overflow, .5, color=color)[0]\n        pylab.text(rect.get_x(),\n                   1.10*rect.get_height(), '%s+' % val_max)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef setup_psd_workflow(workflow, science_segs, datafind_outs,\n                             output_dir=None, tags=None):\n    '''\n    Setup static psd section of CBC workflow. At present this only supports pregenerated\n    psd files, in the future these could be created within the workflow.\n\n    Parameters\n    ----------\n    workflow: pycbc.workflow.core.Workflow\n        An instanced class that manages the constructed workflow.\n    science_segs : Keyed dictionary of glue.segmentlist objects\n        scienceSegs[ifo] holds the science segments to be analysed for each\n        ifo.\n    datafind_outs : pycbc.workflow.core.FileList\n        The file list containing the datafind files.\n    output_dir : path string\n        The directory where data products will be placed.\n    tags : list of strings\n        If given these tags are used to uniquely name and identify output files\n        that would be produced in multiple calls to this function.\n\n    Returns\n    --------\n    psd_files : pycbc.workflow.core.FileList\n        The FileList holding the psd files, 0 or 1 per ifo\n    '''\n    if tags is None:\n        tags = []\n    logging.info(\"Entering static psd module.\")\n    make_analysis_dir(output_dir)\n    cp = workflow.cp\n\n    # Parse for options in ini file.\n    try:\n        psdMethod = cp.get_opt_tags(\"workflow-psd\", \"psd-method\",\n                                     tags)\n    except:\n        # Predefined PSD sare optional, just return an empty list if not\n        # provided.\n        return FileList([])\n\n    if psdMethod == \"PREGENERATED_FILE\":\n        logging.info(\"Setting psd from pre-generated file(s).\")\n        psd_files = setup_psd_pregenerated(workflow, tags=tags)\n    else:\n        errMsg = \"PSD method not recognized. Only \"\n        errMsg += \"PREGENERATED_FILE is currently supported.\"\n        raise ValueError(errMsg)\n\n    logging.info(\"Leaving psd module.\")\n    return psd_files", "response": "Setup static psd section of CBC workflow."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_mapping(order):\n    mapping = {}\n    mapping['Lambda0'] = 0\n    if order == 'zeroPN':\n        return mapping\n    mapping['Lambda2'] = 1\n    if order == 'onePN':\n        return mapping\n    mapping['Lambda3'] = 2\n    if order == 'onePointFivePN':\n        return mapping\n    mapping['Lambda4'] = 3\n    if order == 'twoPN':\n        return mapping\n    mapping['LogLambda5'] = 4\n    if order == 'twoPointFivePN':\n        return mapping\n    mapping['Lambda6'] = 5\n    mapping['LogLambda6'] = 6\n    if order == 'threePN':\n        return mapping\n    mapping['Lambda7'] = 7\n    if order == 'threePointFivePN':\n        return mapping\n    # For some as-of-yet unknown reason, the tidal terms are not giving correct\n    # match estimates when enabled. So, for now, this order is commented out.\n    #if order == 'tidalTesting':\n    #    mapping['Lambda10'] = 8\n    #    mapping['Lambda12'] = 9\n    #    return mapping\n    raise ValueError(\"Order %s is not understood.\" %(order))", "response": "This function generates a mapping between the active Lambda terms and the index in the metric and the Lambda terms."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_inverse_mapping(order):\n    mapping = generate_mapping(order)\n    inv_mapping = {}\n    for key,value in mapping.items():\n        inv_mapping[value] = key\n\n    return inv_mapping", "response": "Generate an inverse mapping between the active Lambda terms and the index in the metric."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ethinca_order_from_string(order):\n    if order in get_ethinca_orders().keys():\n        return get_ethinca_orders()[order]\n    else: raise ValueError(\"Order \"+str(order)+\" is not valid for ethinca\"\n                           \"calculation! Valid orders: \"+\n                           str(get_ethinca_orders().keys()))", "response": "Returns the integer giving twice the post - Newtonian order\n    used by the ethinca calculation. Currently valid only for TaylorF2 metric."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntake a set of masses and spins and convert to the various lambda coordinates that describe the orbital phase. Accepted PN orders are: {} Parameters ---------- mass1 : float or array Mass1 of input(s). mass2 : float or array Mass2 of input(s). spin1z : float or array Parallel spin component(s) of body 1. spin2z : float or array Parallel spin component(s) of body 2. f0 : float This is an arbitrary scaling factor introduced to avoid the potential for numerical overflow when calculating this. Generally the default value (70) is safe here. **IMPORTANT, if you want to calculate the ethinca metric components later this MUST be set equal to f_low.** This value must also be used consistently (ie. don't change its value when calling different functions!). order : string The Post-Newtonian order that is used to translate from masses and spins to the lambda_i coordinate system. Valid orders given above. Returns -------- lambdas : list of floats or numpy.arrays The lambda coordinates for the input system(s)", "response": "def get_chirp_params(mass1, mass2, spin1z, spin2z, f0, order,\n                     quadparam1=None, quadparam2=None, lambda1=None,\n                     lambda2=None):\n    \"\"\"\n    Take a set of masses and spins and convert to the various lambda\n    coordinates that describe the orbital phase. Accepted PN orders are:\n    {}\n\n    Parameters\n    ----------\n    mass1 : float or array\n        Mass1 of input(s).\n    mass2 : float or array\n        Mass2 of input(s).\n    spin1z : float or array\n        Parallel spin component(s) of body 1.\n    spin2z : float or array\n        Parallel spin component(s) of body 2.\n    f0 : float\n        This is an arbitrary scaling factor introduced to avoid the potential\n        for numerical overflow when calculating this. Generally the default\n        value (70) is safe here. **IMPORTANT, if you want to calculate the\n        ethinca metric components later this MUST be set equal to f_low.**\n        This value must also be used consistently (ie. don't change its value\n        when calling different functions!).\n    order : string\n        The Post-Newtonian order that is used to translate from masses and\n        spins to the lambda_i coordinate system. Valid orders given above.\n\n    Returns\n    --------\n    lambdas : list of floats or numpy.arrays\n        The lambda coordinates for the input system(s)\n    \"\"\"\n\n    # Determine whether array or single value input\n    sngl_inp = False\n    try:\n        num_points = len(mass1)\n    except TypeError:\n        sngl_inp = True\n        # If you care about speed, you aren't calling this function one entry\n        # at a time.\n        mass1 = numpy.array([mass1])\n        mass2 = numpy.array([mass2])\n        spin1z = numpy.array([spin1z])\n        spin2z = numpy.array([spin2z])\n        if quadparam1 is not None:\n            quadparam1 = numpy.array([quadparam1])\n        if quadparam2 is not None:\n            quadparam2 = numpy.array([quadparam2])\n        if lambda1 is not None:\n            lambda1 = numpy.array([lambda1])\n        if lambda2 is not None:\n            lambda2 = numpy.array([lambda2])\n        num_points = 1\n\n    if quadparam1 is None:\n        quadparam1 = numpy.ones(len(mass1), dtype=float)\n    if quadparam2 is None:\n        quadparam2 = numpy.ones(len(mass1), dtype=float)\n    if lambda1 is None:\n        lambda1 = numpy.zeros(len(mass1), dtype=float)\n    if lambda2 is None:\n        lambda2 = numpy.zeros(len(mass1), dtype=float)\n\n    mass1_v = CreateREAL8Vector(len(mass1))\n    mass1_v.data[:] = mass1[:]\n    mass2_v = CreateREAL8Vector(len(mass1))\n    mass2_v.data[:] = mass2[:]\n    spin1z_v = CreateREAL8Vector(len(mass1))\n    spin1z_v.data[:] = spin1z[:]\n    spin2z_v = CreateREAL8Vector(len(mass1))\n    spin2z_v.data[:] = spin2z[:]\n    lambda1_v = CreateREAL8Vector(len(mass1))\n    lambda1_v.data[:] = lambda1[:]\n    lambda2_v = CreateREAL8Vector(len(mass1))\n    lambda2_v.data[:] = lambda2[:]\n    dquadparam1_v = CreateREAL8Vector(len(mass1))\n    dquadparam1_v.data[:] = quadparam1[:] - 1.\n    dquadparam2_v = CreateREAL8Vector(len(mass1))\n    dquadparam2_v.data[:] = quadparam2[:] - 1.\n\n    phasing_arr = lalsimulation.SimInspiralTaylorF2AlignedPhasingArray\\\n        (mass1_v, mass2_v, spin1z_v, spin2z_v, lambda1_v, lambda2_v,\n         dquadparam1_v, dquadparam2_v)\n\n    vec_len = lalsimulation.PN_PHASING_SERIES_MAX_ORDER + 1;\n    phasing_vs = numpy.zeros([num_points, vec_len])\n    phasing_vlogvs = numpy.zeros([num_points, vec_len])\n    phasing_vlogvsqs = numpy.zeros([num_points, vec_len])\n\n    lng = len(mass1)\n    jmp = lng * vec_len\n    for idx in range(vec_len):\n        phasing_vs[:,idx] = phasing_arr.data[lng*idx : lng*(idx+1)]\n        phasing_vlogvs[:,idx] = \\\n            phasing_arr.data[jmp + lng*idx : jmp + lng*(idx+1)]\n        phasing_vlogvsqs[:,idx] = \\\n            phasing_arr.data[2*jmp + lng*idx : 2*jmp + lng*(idx+1)]\n\n    pim = PI * (mass1 + mass2)*MTSUN_SI\n    pmf = pim * f0\n    pmf13 = pmf**(1./3.)\n    logpim13 = numpy.log((pim)**(1./3.))\n\n    mapping = generate_inverse_mapping(order)\n    lambdas = []\n    lambda_str = '^Lambda([0-9]+)'\n    loglambda_str = '^LogLambda([0-9]+)'\n    logloglambda_str = '^LogLogLambda([0-9]+)'\n    for idx in range(len(mapping.keys())):\n        # RE magic engage!\n        rematch = re.match(lambda_str, mapping[idx])\n        if rematch:\n            pn_order = int(rematch.groups()[0])\n            term = phasing_vs[:,pn_order]\n            term = term + logpim13 * phasing_vlogvs[:,pn_order]\n            lambdas.append(term * pmf13**(-5+pn_order))\n            continue\n        rematch = re.match(loglambda_str, mapping[idx])\n        if rematch:\n            pn_order = int(rematch.groups()[0])\n            lambdas.append((phasing_vlogvs[:,pn_order]) * pmf13**(-5+pn_order))\n            continue\n        rematch = re.match(logloglambda_str, mapping[idx])\n        if rematch:\n            raise ValueError(\"LOGLOG terms are not implemented\")\n            #pn_order = int(rematch.groups()[0])\n            #lambdas.append(phasing_vlogvsqs[:,pn_order] * pmf13**(-5+pn_order))\n            #continue\n        err_msg = \"Failed to parse \" +  mapping[idx]\n        raise ValueError(err_msg)\n\n    if sngl_inp:\n        return [l[0] for l in lambdas]\n    else:\n        return lambdas"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompile the string source code into a shared object linked against the cufft object.", "response": "def compile(source, name):\n    \"\"\" Compile the string source code into a shared object linked against\n    the static version of cufft for callback support.\n    \"\"\"\n    cache = os.path.join(pycbc._cache_dir_path, name)\n    hash_file = cache + \".hash\"\n    lib_file = cache + \".so\"\n    obj_file = cache + \".o\"\n\n    try:\n        if int(open(hash_file, \"r\").read()) == hash(source):\n            return lib_file\n        raise ValueError\n    except:\n        pass\n\n    src_file = cache + \".cu\"\n    fsrc = open(src_file, \"w\")\n    fsrc.write(source)\n    fsrc.close()\n\n    cmd = [\"nvcc\", \"-ccbin\", \"g++\", \"-dc\", \"-m64\",\n           \"--compiler-options\", \"'-fPIC'\",\n           \"-o\", obj_file,\n           \"-c\", src_file]\n    print(\" \".join(cmd))\n    subprocess.check_call(cmd)\n\n    cmd = [\"nvcc\", \"-shared\", \"-ccbin\", \"g++\", \"-m64\",\n       \"-o\", lib_file, obj_file, \"-lcufft_static\", \"-lculibos\"]\n    print(\" \".join(cmd))\n\n    subprocess.check_call(cmd)\n\n    hash_file = cache + \".hash\"\n    fhash = open(hash_file, \"w\")\n    fhash.write(str(hash(source)))\n    return lib_file"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the IFFT execute and plan functions", "response": "def get_fn_plan(callback=None, out_callback=None, name='pycbc_cufft', parameters=None):\n    \"\"\" Get the IFFT execute and plan functions\n    \"\"\"\n    if parameters is None:\n        parameters = []\n    source = fftsrc.render(input_callback=callback, output_callback=out_callback, parameters=parameters)\n    path = compile(source, name)\n    lib = ctypes.cdll.LoadLibrary(path)\n    fn = lib.execute\n    fn.argtypes = [ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p, ctypes.c_void_p]\n    plan = lib.create_plan\n    plan.restype = ctypes.c_void_p\n    plan.argyptes = [ctypes.c_uint]\n    return fn, plan"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef determine_eigen_directions(metricParams, preserveMoments=False,\n                               vary_fmax=False, vary_density=None):\n    \"\"\"\n    This function will calculate the coordinate transfomations that are needed\n    to rotate from a coordinate system described by the various Lambda\n    components in the frequency expansion, to a coordinate system where the\n    metric is Cartesian.\n\n    Parameters\n    -----------\n    metricParams : metricParameters instance\n        Structure holding all the options for construction of the metric.\n    preserveMoments : boolean, optional (default False)\n        Currently only used for debugging.\n        If this is given then if the moments structure is already set\n        within metricParams then they will not be recalculated.\n    vary_fmax : boolean, optional (default False)\n        If set to False the metric and rotations are calculated once, for the\n        full range of frequency [f_low,f_upper).\n        If set to True the metric and rotations are calculated multiple times,\n        for frequency ranges [f_low,f_low + i*vary_density), where i starts at\n        1 and runs up until f_low + (i+1)*vary_density > f_upper.\n        Thus values greater than f_upper are *not* computed.\n        The calculation for the full range [f_low,f_upper) is also done.\n    vary_density : float, optional\n        If vary_fmax is True, this will be used in computing the frequency\n        ranges as described for vary_fmax.\n\n    Returns\n    --------\n    metricParams : metricParameters instance\n        Structure holding all the options for construction of the metric.\n        **THIS FUNCTION ONLY RETURNS THE CLASS**\n        The following will be **added** to this structure\n    metricParams.evals : Dictionary of numpy.array\n        Each entry in the dictionary corresponds to the different frequency\n        ranges described in vary_fmax. If vary_fmax = False, the only entry\n        will be f_upper, this corresponds to integrals in [f_low,f_upper). This\n        entry is always present. Each other entry will use floats as keys to\n        the dictionary. These floats give the upper frequency cutoff when it is\n        varying.\n        Each numpy.array contains the eigenvalues which, with the eigenvectors\n        in evecs, are needed to rotate the\n        coordinate system to one in which the metric is the identity matrix.\n    metricParams.evecs : Dictionary of numpy.matrix\n        Each entry in the dictionary is as described under evals.\n        Each numpy.matrix contains the eigenvectors which, with the eigenvalues\n        in evals, are needed to rotate the\n        coordinate system to one in which the metric is the identity matrix.\n    metricParams.metric : Dictionary of numpy.matrix\n        Each entry in the dictionary is as described under evals.\n        Each numpy.matrix contains the metric of the parameter space in the\n        Lambda_i coordinate system.\n    metricParams.moments : Moments structure\n        See the structure documentation for a description of this. This\n        contains the result of all the integrals used in computing the metrics\n        above. It can be used for the ethinca components calculation, or other\n        similar calculations.\n    \"\"\"\n\n    evals = {}\n    evecs = {}\n    metric = {}\n    unmax_metric = {}\n\n    # First step is to get the moments needed to calculate the metric\n    if not (metricParams.moments and preserveMoments):\n        get_moments(metricParams, vary_fmax=vary_fmax,\n                    vary_density=vary_density)\n\n    # What values are going to be in the moments\n    # J7 is the normalization factor so it *MUST* be present\n    list = metricParams.moments['J7'].keys()\n\n    # We start looping over every item in the list of metrics\n    for item in list:\n        # Here we convert the moments into a form easier to use here\n        Js = {}\n        for i in range(-7,18):\n            Js[i] = metricParams.moments['J%d'%(i)][item]\n\n        logJs = {}\n        for i in range(-1,18):\n            logJs[i] = metricParams.moments['log%d'%(i)][item]\n\n        loglogJs = {}\n        for i in range(-1,18):\n            loglogJs[i] = metricParams.moments['loglog%d'%(i)][item]\n\n        logloglogJs = {}\n        for i in range(-1,18):\n            logloglogJs[i] = metricParams.moments['logloglog%d'%(i)][item]\n\n        loglogloglogJs = {}\n        for i in range(-1,18):\n            loglogloglogJs[i] = metricParams.moments['loglogloglog%d'%(i)][item]\n\n        mapping = generate_mapping(metricParams.pnOrder)\n\n        # Calculate the metric\n        gs, unmax_metric_curr = calculate_metric(Js, logJs, loglogJs,\n                                          logloglogJs, loglogloglogJs, mapping)\n        metric[item] = numpy.matrix(gs)\n        unmax_metric[item] = unmax_metric_curr\n\n        # And the eigenvalues\n        evals[item],evecs[item] = numpy.linalg.eig(gs)\n\n        # Numerical error can lead to small negative eigenvalues.\n        for i in range(len(evals[item])):\n            if evals[item][i] < 0:\n                # Due to numerical imprecision the very small eigenvalues can\n                # be negative. Make these positive.\n                evals[item][i] = -evals[item][i]\n            if evecs[item][i,i] < 0:\n                # We demand a convention that all diagonal terms in the matrix\n                # of eigenvalues are positive.\n                # This is done to help visualization of the spaces (increasing\n                # mchirp always goes the same way)\n                evecs[item][:,i] = - evecs[item][:,i]\n\n    metricParams.evals = evals\n    metricParams.evecs = evecs\n    metricParams.metric = metric\n    metricParams.time_unprojected_metric = unmax_metric\n\n    return metricParams", "response": "This function calculates the coordinate transfomations needed by the Lambda\n    components in the frequency expansion system."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef interpolate_psd(psd_f, psd_amp, deltaF):\n    # In some cases this will be a no-op. I thought about removing this, but\n    # this function can take unequally sampled PSDs and it is difficult to\n    # check for this. As this function runs quickly anyway (compared to the\n    # moment calculation) I decided to always interpolate.\n\n    new_psd_f = []\n    new_psd_amp = []\n    fcurr = psd_f[0]\n\n    for i in range(len(psd_f) - 1):\n        f_low = psd_f[i]\n        f_high = psd_f[i+1]\n        amp_low = psd_amp[i]\n        amp_high = psd_amp[i+1]\n        while(1):\n            if fcurr > f_high:\n                break\n            new_psd_f.append(fcurr)\n            gradient = (amp_high - amp_low) / (f_high - f_low)\n            fDiff = fcurr - f_low\n            new_psd_amp.append(amp_low + fDiff * gradient)\n            fcurr = fcurr + deltaF\n    return numpy.asarray(new_psd_f), numpy.asarray(new_psd_amp)", "response": "Function to interpolate a PSD to a different value of deltaF. Uses linear interpolation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef calculate_moment(psd_f, psd_amp, fmin, fmax, f0, funct,\n                     norm=None, vary_fmax=False, vary_density=None):\n    \"\"\"\n    Function for calculating one of the integrals used to construct a template\n    bank placement metric. The integral calculated will be\n\n    \\int funct(x) * (psd_x)**(-7./3.) * delta_x / PSD(x)\n\n    where x = f / f0. The lower frequency cutoff is given by fmin, see\n    the parameters below for details on how the upper frequency cutoff is\n    chosen\n\n    Parameters\n    -----------\n    psd_f : numpy.array\n       numpy array holding the set of evenly spaced frequencies used in the PSD\n    psd_amp : numpy.array\n       numpy array holding the PSD values corresponding to the psd_f\n       frequencies\n    fmin : float\n        The lower frequency cutoff used in the calculation of the integrals\n        used to obtain the metric.\n    fmax : float\n        The upper frequency cutoff used in the calculation of the integrals\n        used to obtain the metric. This can be varied (see the vary_fmax\n        option below).\n    f0 : float\n        This is an arbitrary scaling factor introduced to avoid the potential\n        for numerical overflow when calculating this. Generally the default\n        value (70) is safe here. **IMPORTANT, if you want to calculate the\n        ethinca metric components later this MUST be set equal to f_low.**\n    funct : Lambda function\n        The function to use when computing the integral as described above.\n    norm : Dictionary of floats\n        If given then moment[f_cutoff] will be divided by norm[f_cutoff]\n    vary_fmax : boolean, optional (default False)\n        If set to False the metric and rotations are calculated once, for the\n        full range of frequency [f_low,f_upper).\n        If set to True the metric and rotations are calculated multiple times,\n        for frequency ranges [f_low,f_low + i*vary_density), where i starts at\n        1 and runs up until f_low + (i+1)*vary_density > f_upper.\n        Thus values greater than f_upper are *not* computed.\n        The calculation for the full range [f_low,f_upper) is also done.\n    vary_density : float, optional\n        If vary_fmax is True, this will be used in computing the frequency\n        ranges as described for vary_fmax.\n\n    Returns\n    --------\n    moment : Dictionary of floats\n        moment[f_cutoff] will store the value of the moment at the frequency\n        cutoff given by f_cutoff.\n    \"\"\"\n\n    # Must ensure deltaF in psd_f is constant\n    psd_x = psd_f / f0\n    deltax = psd_x[1] - psd_x[0]\n\n    mask = numpy.logical_and(psd_f > fmin, psd_f < fmax)\n    psdf_red = psd_f[mask]\n    comps_red = psd_x[mask] ** (-7./3.) * funct(psd_x[mask], f0) * deltax / \\\n                psd_amp[mask]\n    moment = {}\n    moment[fmax] = comps_red.sum()\n    if norm:\n        moment[fmax] = moment[fmax] / norm[fmax]\n    if vary_fmax:\n        for t_fmax in numpy.arange(fmin + vary_density, fmax, vary_density):\n            moment[t_fmax] = comps_red[psdf_red < t_fmax].sum()\n            if norm:\n                moment[t_fmax] = moment[t_fmax] / norm[t_fmax]\n    return moment", "response": "This function calculates the moment of a template based on the PSD values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the metric comprehension of the current unmax_metric and unmax_metric.", "response": "def calculate_metric_comp(gs, unmax_metric, i, j, Js, logJs, loglogJs,\n                          logloglogJs, loglogloglogJs, mapping):\n    \"\"\"\n    Used to compute part of the metric. Only call this from within\n    calculate_metric(). Please see the documentation for that function.\n    \"\"\"\n    # Time term in unmax_metric. Note that these terms are recomputed a bunch\n    # of time, but this cost is insignificant compared to computing the moments\n    unmax_metric[-1,-1] = (Js[1] - Js[4]*Js[4])\n\n    # Normal terms\n    if 'Lambda%d'%i in mapping and 'Lambda%d'%j in mapping:\n        gammaij = Js[17-i-j] - Js[12-i]*Js[12-j]\n        gamma0i = (Js[9-i] - Js[4]*Js[12-i])\n        gamma0j = (Js[9-j] - Js[4] * Js[12-j])\n        gs[mapping['Lambda%d'%i],mapping['Lambda%d'%j]] = \\\n            0.5 * (gammaij - gamma0i*gamma0j/(Js[1] - Js[4]*Js[4]))\n        unmax_metric[mapping['Lambda%d'%i], -1] = gamma0i\n        unmax_metric[-1, mapping['Lambda%d'%j]] = gamma0j\n        unmax_metric[mapping['Lambda%d'%i],mapping['Lambda%d'%j]] = gammaij\n    # Normal,log cross terms\n    if 'Lambda%d'%i in mapping and 'LogLambda%d'%j in mapping:\n        gammaij = logJs[17-i-j] - logJs[12-j] * Js[12-i]\n        gamma0i = (Js[9-i] - Js[4] * Js[12-i])\n        gamma0j = logJs[9-j] - logJs[12-j] * Js[4]\n        gs[mapping['Lambda%d'%i],mapping['LogLambda%d'%j]] = \\\n            gs[mapping['LogLambda%d'%j],mapping['Lambda%d'%i]] = \\\n            0.5 * (gammaij - gamma0i*gamma0j/(Js[1] - Js[4]*Js[4]))\n        unmax_metric[mapping['Lambda%d'%i], -1] = gamma0i\n        unmax_metric[-1, mapping['Lambda%d'%i]] = gamma0i\n        unmax_metric[-1, mapping['LogLambda%d'%j]] = gamma0j\n        unmax_metric[mapping['LogLambda%d'%j], -1] = gamma0j\n        unmax_metric[mapping['Lambda%d'%i],mapping['LogLambda%d'%j]] = gammaij\n        unmax_metric[mapping['LogLambda%d'%j],mapping['Lambda%d'%i]] = gammaij\n    # Log,log terms\n    if 'LogLambda%d'%i in mapping and 'LogLambda%d'%j in mapping:\n        gammaij = loglogJs[17-i-j] - logJs[12-j] * logJs[12-i]\n        gamma0i = (logJs[9-i] - Js[4] * logJs[12-i])\n        gamma0j = logJs[9-j] - logJs[12-j] * Js[4]\n        gs[mapping['LogLambda%d'%i],mapping['LogLambda%d'%j]] = \\\n            0.5 * (gammaij - gamma0i*gamma0j/(Js[1] - Js[4]*Js[4]))\n        unmax_metric[mapping['LogLambda%d'%i], -1] = gamma0i\n        unmax_metric[-1, mapping['LogLambda%d'%j]] = gamma0j\n        unmax_metric[mapping['LogLambda%d'%i],mapping['LogLambda%d'%j]] =\\\n            gammaij\n\n    # Normal,loglog cross terms\n    if 'Lambda%d'%i in mapping and 'LogLogLambda%d'%j in mapping:\n        gammaij = loglogJs[17-i-j] - loglogJs[12-j] * Js[12-i]\n        gamma0i = (Js[9-i] - Js[4] * Js[12-i])\n        gamma0j = loglogJs[9-j] - loglogJs[12-j] * Js[4]\n        gs[mapping['Lambda%d'%i],mapping['LogLogLambda%d'%j]] = \\\n            gs[mapping['LogLogLambda%d'%j],mapping['Lambda%d'%i]] = \\\n            0.5 * (gammaij - gamma0i*gamma0j/(Js[1] - Js[4]*Js[4]))\n        unmax_metric[mapping['Lambda%d'%i], -1] = gamma0i\n        unmax_metric[-1, mapping['Lambda%d'%i]] = gamma0i\n        unmax_metric[-1, mapping['LogLogLambda%d'%j]] = gamma0j\n        unmax_metric[mapping['LogLogLambda%d'%j], -1] = gamma0j\n        unmax_metric[mapping['Lambda%d'%i],mapping['LogLogLambda%d'%j]] = \\\n            gammaij\n        unmax_metric[mapping['LogLogLambda%d'%j],mapping['Lambda%d'%i]] = \\\n            gammaij\n\n    # log,loglog cross terms\n    if 'LogLambda%d'%i in mapping and 'LogLogLambda%d'%j in mapping:\n        gammaij = logloglogJs[17-i-j] - loglogJs[12-j] * logJs[12-i]\n        gamma0i = (logJs[9-i] - Js[4] * logJs[12-i])\n        gamma0j = loglogJs[9-j] - loglogJs[12-j] * Js[4]\n        gs[mapping['LogLambda%d'%i],mapping['LogLogLambda%d'%j]] = \\\n            gs[mapping['LogLogLambda%d'%j],mapping['LogLambda%d'%i]] = \\\n            0.5 * (gammaij - gamma0i*gamma0j/(Js[1] - Js[4]*Js[4]))\n        unmax_metric[mapping['LogLambda%d'%i], -1] = gamma0i\n        unmax_metric[-1, mapping['LogLambda%d'%i]] = gamma0i\n        unmax_metric[-1, mapping['LogLogLambda%d'%j]] = gamma0j\n        unmax_metric[mapping['LogLogLambda%d'%j], -1] = gamma0j\n        unmax_metric[mapping['LogLambda%d'%i],mapping['LogLogLambda%d'%j]] = \\\n            gammaij\n        unmax_metric[mapping['LogLogLambda%d'%j],mapping['LogLambda%d'%i]] = \\\n            gammaij\n\n    # Loglog,loglog terms\n    if 'LogLogLambda%d'%i in mapping and 'LogLogLambda%d'%j in mapping:\n        gammaij = loglogloglogJs[17-i-j] - loglogJs[12-j] * loglogJs[12-i]\n        gamma0i = (loglogJs[9-i] - Js[4] * loglogJs[12-i])\n        gamma0j = loglogJs[9-j] - loglogJs[12-j] * Js[4]\n        gs[mapping['LogLogLambda%d'%i],mapping['LogLogLambda%d'%j]] = \\\n            0.5 * (gammaij - gamma0i*gamma0j/(Js[1] - Js[4]*Js[4]))\n        unmax_metric[mapping['LogLogLambda%d'%i], -1] = gamma0i\n        unmax_metric[-1, mapping['LogLogLambda%d'%j]] = gamma0j\n        unmax_metric[mapping['LogLogLambda%d'%i],mapping['LogLogLambda%d'%j]] =\\\n            gammaij"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds options to the optparser object for selecting templates in bins.", "response": "def insert_bank_bins_option_group(parser):\n    \"\"\" Add options to the optparser object for selecting templates in bins.\n\n    Parameters\n    -----------\n    parser : object\n        OptionParser instance.\n    \"\"\"\n    bins_group = parser.add_argument_group(\n                                 \"Options for selecting templates in bins.\")\n    bins_group.add_argument(\"--bank-bins\", nargs=\"+\", default=None,\n                            help=\"Ordered list of mass bin upper boundaries. \"\n                                 \"An ordered list of type-boundary pairs, \"\n                                 \"applied sequentially. Must provide a name \"\n                                 \"(can be any unique string for tagging \"\n                                 \"purposes), the parameter to bin \"\n                                 \"on, and the membership condition via \"\n                                 \"'lt' / 'gt' operators. \"\n                                 \"Ex. name1:component:lt2 name2:total:lt15\")\n    bins_group.add_argument(\"--bank-file\", default=None,\n                            help=\"HDF format template bank file.\")\n    bins_group.add_argument(\"--f-lower\", default=None,\n                            help=\"Low frequency cutoff in Hz.\")\n    return bins_group"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses the CLI options related to binning templates in the bank file and returns the bin indices and bank data.", "response": "def bank_bins_from_cli(opts):\n    \"\"\" Parses the CLI options related to binning templates in the bank.\n\n    Parameters\n    ----------\n    opts : object\n        Result of parsing the CLI with OptionParser.\n\n    Results\n    -------\n    bins_idx : dict\n        A dict with bin names as key and an array of their indices as value.\n    bank : dict\n        A dict of the datasets from the bank file.\n    \"\"\"\n    bank = {}\n    fp = h5py.File(opts.bank_file)\n    for key in fp.keys():\n        bank[key] = fp[key][:]\n    bank[\"f_lower\"] = float(opts.f_lower) if opts.f_lower else None\n    if opts.bank_bins:\n        bins_idx = coinc.background_bin_from_string(opts.bank_bins, bank)\n    else:\n        bins_idx = {\"all\" : numpy.arange(0, len(bank[fp.keys()[0]]))}\n    fp.close()\n    return bins_idx, bank"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef insert_loudest_triggers_option_group(parser, coinc_options=True):\n    opt_group = insert_bank_bins_option_group(parser)\n    opt_group.title = \"Options for finding loudest triggers.\"\n    if coinc_options:\n        opt_group.add_argument(\"--statmap-file\", default=None,\n                               help=\"HDF format clustered coincident trigger \"\n                                    \"result file.\")\n        opt_group.add_argument(\"--statmap-group\", default=\"foreground\",\n                               help=\"Name of group in statmap file to \"\n                                    \"get triggers.\")\n    opt_group.add_argument(\"--sngl-trigger-files\", nargs=\"+\", default=None,\n                           action=types.MultiDetOptionAction,\n                           help=\"HDF format merged single detector \"\n                                \"trigger files.\")\n    opt_group.add_argument(\"--veto-file\", default=None,\n                           help=\"XML file with segment_definer and \"\n                                \"segment table.\")\n    opt_group.add_argument(\"--veto-segment-name\", default=None,\n                           help=\"Name of segment to use as veto in \"\n                                 \"XML file's segment_definer table.\")\n    opt_group.add_argument(\"--search-n-loudest\", type=int, default=None,\n                           help=\"Number of triggers to search over.\")\n    opt_group.add_argument(\"--n-loudest\", type=int, default=None,\n                           help=\"Number of triggers to return in results.\")\n    return opt_group", "response": "Adds options to the parser object for selecting loudest triggers."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef loudest_triggers_from_cli(opts, coinc_parameters=None,\n                              sngl_parameters=None, bank_parameters=None):\n    \"\"\" Parses the CLI options related to find the loudest coincident or\n    single detector triggers.\n\n    Parameters\n    ----------\n    opts : object\n        Result of parsing the CLI with OptionParser.\n    coinc_parameters : list\n        List of datasets in statmap file to retrieve.\n    sngl_parameters : list\n        List of datasets in single-detector trigger files to retrieve.\n    bank_parameters : list\n        List of datasets in template bank file to retrieve.\n\n    Results\n    -------\n    bin_names : dict\n        A list of bin names.\n    bin_results : dict\n        A list of dict holding trigger data data.\n    \"\"\"\n\n    # list to hold trigger data\n    bin_results = []\n\n    # list of IFOs\n    ifos = opts.sngl_trigger_files.keys()\n\n    # get indices of bins in template bank\n    bins_idx, bank_data = bank_bins_from_cli(opts)\n    bin_names = bins_idx.keys()\n\n    # if taking triggers from statmap file\n    if opts.statmap_file and opts.bank_file and opts.sngl_trigger_files:\n\n        # loop over each bin\n        for bin_name in bin_names:\n            data = {}\n\n            # get template has and detection statistic for coincident events\n            statmap = hdf.ForegroundTriggers(\n                                 opts.statmap_file, opts.bank_file,\n                                 sngl_files=opts.sngl_trigger_files.values(),\n                                 n_loudest=opts.search_n_loudest,\n                                 group=opts.statmap_group)\n            template_hash = statmap.get_bankfile_array(\"template_hash\")\n            stat = statmap.get_coincfile_array(\"stat\")\n\n            # get indices of triggers in bin\n            bin_idx = numpy.in1d(template_hash,\n                              bank_data[\"template_hash\"][bins_idx[bin_name]])\n\n            # get indices for sorted detection statistic in bin\n            sorting = stat[bin_idx].argsort()[::-1]\n\n            # get variables for n-th loudest triggers\n            for p in coinc_parameters:\n                arr = statmap.get_coincfile_array(p)\n                data[p] = arr[bin_idx][sorting][:opts.n_loudest]\n            for p in sngl_parameters:\n                for ifo in ifos:\n                    key = \"/\".join([ifo, p])\n                    arr = statmap.get_snglfile_array_dict(p)[ifo]\n                    data[key] = arr[bin_idx][sorting][:opts.n_loudest]\n            for p in bank_parameters:\n                arr = statmap.get_bankfile_array(p)\n                data[p] = arr[bin_idx][sorting][:opts.n_loudest]\n\n            # append results\n            bin_results.append(data)\n\n    # if taking triggers from single detector file\n    elif opts.bank_file and opts.sngl_trigger_files:\n\n        # loop over each bin\n        for bin_name in bin_names:\n            data = {}\n\n            # only use one IFO\n            if len(opts.sngl_trigger_files.keys()) == 1:\n                ifo = opts.sngl_trigger_files.keys()[0]\n            else:\n                raise ValueError(\"Too many IFOs\")\n\n            # get newSNR as statistic from single detector files\n            sngls =  hdf.SingleDetTriggers(opts.sngl_trigger_files[ifo],\n                                           opts.bank_file, opts.veto_file,\n                                           opts.veto_segment_name, None, ifo)\n\n            # cluster\n            n_loudest = opts.search_n_loudest \\\n                           if opts.search_n_loudest else len(sngls.template_id)\n            sngls.mask_to_n_loudest_clustered_events(n_loudest=n_loudest)\n            template_hash = \\\n                  sngls.bank[\"template_hash\"][:][sngls.template_id]\n\n            # get indices of triggers in bin\n            bin_idx = numpy.in1d(template_hash,\n                              bank_data[\"template_hash\"][bins_idx[bin_name]])\n\n            # sort by detection statistic\n            stats = sngls.stat\n            sorting = stats[bin_idx].argsort()[::-1]\n\n            # get indices for sorted detection statistic in bin\n            for p in sngl_parameters:\n                key = \"/\".join([ifo, p])\n                arr = sngls.get_column(p)\n                data[key] = arr[bin_idx][sorting][:opts.n_loudest]\n            for p in bank_parameters:\n                arr = sngls.bank[p][:]\n                data[p] = \\\n                      arr[sngls.template_id][bin_idx][sorting][:opts.n_loudest]\n\n            # append results\n            bin_results.append(data)\n\n    # else did not supply enough command line options\n    else:\n        raise ValueError(\"Must have --bank-file and --sngl-trigger-files\")\n\n    return bin_names, bin_results", "response": "Parses the options related to find the loudest coincident or single detector trigger files and returns a list of dictionaries containing the data for each bin in the template bank and the template has and detection statistic for each bin in the template bank."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_mass_spin(bank, tid):\n    m1 = bank['mass1'][:][tid]\n    m2 = bank['mass2'][:][tid]\n    s1z = bank['spin1z'][:][tid]\n    s2z = bank['spin2z'][:][tid]\n    return m1, m2, s1z, s2z", "response": "Returns the mass and spin of the specified entry"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfunction to calculate the parameter values for a given parameter name.", "response": "def get_param(par, args, m1, m2, s1z, s2z):\n    \"\"\"\n    Helper function\n\n    Parameters\n    ----------\n    par : string\n        Name of parameter to calculate\n    args : Namespace object returned from ArgumentParser instance\n        Calling code command line options, used for f_lower value\n    m1 : float or array of floats\n        First binary component mass (etc.)\n\n    Returns\n    -------\n    parvals : float or array of floats\n        Calculated parameter values\n    \"\"\"\n    if par == 'mchirp':\n        parvals = conversions.mchirp_from_mass1_mass2(m1, m2)\n    elif par == 'mtotal':\n        parvals = m1 + m2\n    elif par == 'eta':\n        parvals = conversions.eta_from_mass1_mass2(m1, m2)\n    elif par in ['chi_eff', 'effective_spin']:\n        parvals = conversions.chi_eff(m1, m2, s1z, s2z)\n    elif par == 'template_duration':\n        # default to SEOBNRv4 duration function\n        parvals = pnutils.get_imr_duration(m1, m2, s1z, s2z, args.f_lower,\n                                           args.approximant or \"SEOBNRv4\")\n        if args.min_duration:\n            parvals += args.min_duration\n    elif par == 'tau0':\n        parvals = conversions.tau0_from_mass1_mass2(m1, m2, args.f_lower)\n    elif par == 'tau3':\n        parvals = conversions.tau3_from_mass1_mass2(m1, m2, args.f_lower)\n    elif par in pnutils.named_frequency_cutoffs.keys():\n        parvals = pnutils.frequency_cutoff_from_name(par, m1, m2, s1z, s2z)\n    else:\n        # try asking for a LALSimulation frequency function\n        parvals = pnutils.get_freq(par, m1, m2, s1z, s2z)\n    return parvals"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntranslating some popular trigger parameters into functions that calculate them from an hdf found injection file Parameters ---------- injfile: hdf5 File object Injection file of format known to ANitz (DOCUMENTME) bankfile: hdf5 File object or None Template bank file trigfile: hdf5 File object or None Single-detector trigger file param: string Parameter to be calculated for the recovered triggers ifo: string or None Standard ifo name, ex. 'L1' args : Namespace object returned from ArgumentParser instance Calling code command line options, used for f_lower value Returns ------- [return value]: NumPy array of floats The calculated parameter values", "response": "def get_found_param(injfile, bankfile, trigfile, param, ifo, args=None):\n    \"\"\"\n    Translates some popular trigger parameters into functions that calculate\n    them from an hdf found injection file\n\n    Parameters\n    ----------\n    injfile: hdf5 File object\n        Injection file of format known to ANitz (DOCUMENTME)\n    bankfile: hdf5 File object or None\n        Template bank file\n    trigfile: hdf5 File object or None\n        Single-detector trigger file\n    param: string\n        Parameter to be calculated for the recovered triggers\n    ifo: string or None\n        Standard ifo name, ex. 'L1'\n    args : Namespace object returned from ArgumentParser instance\n        Calling code command line options, used for f_lower value\n\n    Returns\n    -------\n    [return value]: NumPy array of floats\n        The calculated parameter values\n    \"\"\"\n    foundtmp = injfile[\"found_after_vetoes/template_id\"][:]\n    if trigfile is not None:\n        # get the name of the ifo in the injection file, eg \"detector_1\"\n        # and the integer from that name\n        ifolabel = [name for name, val in injfile.attrs.items() if \\\n                    \"detector\" in name and val == ifo][0]\n        foundtrg = injfile[\"found_after_vetoes/trigger_id\" + ifolabel[-1]]\n    if bankfile is not None and param in bankfile.keys():\n        return bankfile[param][:][foundtmp]\n    elif trigfile is not None and param in trigfile[ifo].keys():\n        return trigfile[ifo][param][:][foundtrg]\n    else:\n        b = bankfile\n        return get_param(param, args, b['mass1'][:], b['mass2'][:],\n                                     b['spin1z'][:], b['spin2z'][:])[foundtmp]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntranslate some popular injection parameters into functions that calculate them from an hdf found injection file Parameters ---------- injfile: hdf5 File object Injection file of format known to ANitz (DOCUMENTME) param: string Parameter to be calculated for the injected signals ifo: string Standard detector name, ex. 'L1' args: Namespace object returned from ArgumentParser instance Calling code command line options, used for f_lower value Returns ------- [return value]: NumPy array of floats The calculated parameter values", "response": "def get_inj_param(injfile, param, ifo, args=None):\n    \"\"\"\n    Translates some popular injection parameters into functions that calculate\n    them from an hdf found injection file\n\n    Parameters\n    ----------\n    injfile: hdf5 File object\n        Injection file of format known to ANitz (DOCUMENTME)\n    param: string\n        Parameter to be calculated for the injected signals\n    ifo: string\n        Standard detector name, ex. 'L1'\n    args: Namespace object returned from ArgumentParser instance\n        Calling code command line options, used for f_lower value\n\n    Returns\n    -------\n    [return value]: NumPy array of floats\n        The calculated parameter values\n    \"\"\"\n    det = pycbc.detector.Detector(ifo)\n\n    inj = injfile[\"injections\"]\n    if param in inj.keys():\n        return inj[\"injections/\"+param]\n\n    if param == \"end_time_\"+ifo[0].lower():\n        return inj['end_time'][:] + det.time_delay_from_earth_center(\n                                        inj['longitude'][:],\n                                        inj['latitude'][:],\n                                        inj['end_time'][:])\n    else:\n        return get_param(param, args, inj['mass1'][:], inj['mass2'][:],\n                                     inj['spin1z'][:], inj['spin2z'][:])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ISSO_eq_at_pole(r, chi):\n    return r**3*(r**2*(r-6)+chi**2*(3*r+4))+chi**4*(3*r*(r-2)+chi**2)", "response": "Returns the ISSO polarization of the Kerr polar\n    at the specified radial coordinate."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef PG_ISSO_eq(r, chi, ci):\n    X=chi**2*(chi**2*(3*chi**2+4*r*(2*r-3))+r**2*(15*r*(r-4)+28))-6*r**4*(r**2-4)\n    Y=chi**4*(chi**4+r**2*(7*r*(3*r-4)+36))+6*r*(r-2)*(chi**6+2*r**3*(chi**2*(3*r+2)+3*r**2*(r-2)))\n    Z=ISCO_eq(r, chi)\n\n    return r**8*Z+chi**2*(1-ci**2)*(chi**2*(1-ci**2)*Y-2*r**4*X)", "response": "Calculates the Polynomial that enables the calculation of a generic innermost stable spherical orbit via its roots."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfunctions that determines the radius of the innermost stable tree element for a given spin and inclination angle.", "response": "def PG_ISSO_solver(chi,incl):\n    \"\"\"\n    Function that determines the radius of the innermost stable\n    spherical orbit (ISSO) for a Kerr BH and a generic inclination\n    angle between the BH spin and the orbital angular momentum.\n    This function finds the appropriat root of PG_ISSO_eq.\n\n    Parameters\n    -----------\n    chi: float\n        the BH dimensionless spin parameter\n    incl: float\n        the inclination angle between the BH spin and the orbital\n        angular momentum in radians\n\n    Returns\n    ----------\n    solution: float\n        the radius of the orbit in BH mass units\n    \"\"\"\n    # Auxiliary variables\n    ci=math.cos(incl)\n    sgnchi = np.sign(ci)*chi\n\n    # ISCO radius for the given spin magnitude\n    if sgnchi > 0.99:\n        initial_guess = 2 # Works better than 6 for chi=1\n    elif sgnchi < 0:\n        initial_guess = 9\n    else:\n        initial_guess = 5 # Works better than 6 for chi=0.5\n    rISCO_limit = scipy.optimize.fsolve(ISCO_eq, initial_guess, args=sgnchi)\n\n    # ISSO radius for an inclination of pi/2\n    if chi < 0:\n        initial_guess = 9\n    else:\n        initial_guess = 6\n    rISSO_at_pole_limit = scipy.optimize.fsolve(ISSO_eq_at_pole, initial_guess, args=chi)\n\n    # If the inclination is 0 or pi, just output the ISCO radius\n    if incl in [0, math.pi]:\n        solution = rISCO_limit\n    # If the inclination is pi/2, just output the ISSO radius at the pole(s)\n    elif incl == math.pi/2:\n        solution = rISSO_at_pole_limit\n    # Otherwise, find the ISSO radius for a generic inclination\n    else:\n        initial_guess = max(rISCO_limit,rISSO_at_pole_limit)\n        solution = scipy.optimize.fsolve(PG_ISSO_eq, initial_guess, args=(chi, ci))\n        if solution < 1 or solution > 9:\n            initial_guess = min(rISCO_limit,rISSO_at_pole_limit)\n            solution = scipy.optimize.fsolve(PG_ISSO_eq, initial_guess, args=(chi, ci))\n\n    return solution"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the root chi_eff of the effective dimensionless spin parameter of a NS - BH binary with tilted BH spin.", "response": "def pos_branch(incl, chi):\n    \"\"\"\n    Determines the effective [as defined in Stone, Loeb,\n    Berger, PRD 87, 084053 (2013)] aligned dimensionless\n    spin parameter of a NS-BH binary with tilted BH spin.\n    This means finding the root chi_eff of\n    ISCO_eq_chi_first(chi_eff, PG_ISSO_solver(chi,incl)).\n    The result returned by this function belongs to the\n    branch of the greater solutions, i.e. the greater of\n    the two possible solutions is returned.\n\n    Parameters\n    -----------\n    incl: float\n        the inclination angle between the BH spin and the orbital\n        angular momentum in radians\n    chi: float\n        the BH dimensionless spin parameter\n\n    Returns\n    ----------\n    chi_eff: float\n        the (greater) effective dimensionless spin parameter solution\n    \"\"\"\n    if incl == 0:\n        chi_eff = chi\n    else:\n        rISSO = PG_ISSO_solver(chi,incl)\n        chi_eff = scipy.optimize.fsolve(ISCO_eq_chi_first, 1.0, args=(rISSO))\n\n    return chi_eff"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef bh_effective_spin(chi,incl):\n    if incl == 0:\n        chi_eff = chi\n    else:\n        # ISSO radius for the given BH spin magnitude and inclination\n        rISSO = PG_ISSO_solver(chi,incl)\n        # Angle at which the branch of positive solutions has its minumum\n        incl_flip = scipy.optimize.fmin(pos_branch, math.pi/4, args=tuple([chi]), full_output=False, disp=False)[-1]\n        # Use incl_flip to determine the initial guess: the sign difference\n        # in the initial_guess ensures that chi_eff has the correct sign\n        if incl>incl_flip:\n            initial_guess = -1.1\n        else:\n            initial_guess = 1.0\n        chi_eff = scipy.optimize.fsolve(ISCO_eq_chi_first, initial_guess, args=(rISSO))\n\n    return chi_eff", "response": "Determines the effective dimensionless spin parameter of a NS - BH binary with tilted BH spin."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_ns_sequence(eos_name):\n    ns_sequence = []\n\n    if eos_name == '2H':\n        ns_sequence_path = os.path.join(pycbc.tmpltbank.NS_SEQUENCE_FILE_DIRECTORY, 'equil_2H.dat')\n        #ns_sequence_path = os.path.join(NS_SEQUENCE_FILE_DIRECTORY, 'equil_2H.dat')\n        ns_sequence = np.loadtxt(ns_sequence_path)\n    else:\n        print('Only the 2H EOS is currently supported!')\n        print('If you plan to use a different NS EOS, be sure not to filter')\n        print('too many templates!\\n')\n        raise Exception('Unsupported EOS!')\n\n    max_ns_g_mass = max(ns_sequence[:,0])\n\n    return (ns_sequence, max_ns_g_mass)", "response": "Loads the data of an NS non - rotating equilibrium sequence containing the most massive stable NS."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ns_g_mass_to_ns_b_mass(ns_g_mass, ns_sequence):\n    x = ns_sequence[:,0]\n    y = ns_sequence[:,1]\n    f = scipy.interpolate.interp1d(x, y)\n\n    return f(ns_g_mass)", "response": "This function takes the gravitational base mass and a sequence of NS s and returns the baryonic mass of an NS given its gravitational base mass and an equilibrium sequence."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the roots of this equation", "response": "def xi_eq(x, kappa, chi_eff, q):\n    \"\"\"\n    The roots of this equation determine the orbital radius\n    at the onset of NS tidal disruption in a nonprecessing\n    NS-BH binary [(7) in Foucart PRD 86, 124007 (2012)]\n\n    Parameters\n    -----------\n    x: float\n        orbital separation in units of the NS radius\n    kappa: float\n        the BH mass divided by the NS radius\n    chi_eff: float\n        the BH dimensionless spin parameter\n    q: float\n        the binary mass ratio (BH mass / NS mass)\n\n    Returns\n    ----------\n    float\n        x**3*(x**2-3*kappa*x+2*chi_eff*kappa*sqrt[kappa*x)\n        -3*q*(x**2-2*kappa*x+(chi_eff*kappa)**2)\n    \"\"\"\n    return x**3*(x**2-3*kappa*x+2*chi_eff*kappa*math.sqrt(kappa*x))-3*q*(x**2-2*kappa*x+(chi_eff*kappa)**2)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfunctions that determines the remnant disk mass of a given NS - BH system using the fit to numerical - relativity.", "response": "def remnant_mass(eta, ns_g_mass, ns_sequence, chi, incl, shift):\n    \"\"\"\n    Function that determines the remnant disk mass of\n    an NS-BH system using the fit to numerical-relativity\n    results discussed in Foucart PRD 86, 124007 (2012).\n\n    Parameters\n    -----------\n    eta: float\n        the symmetric mass ratio of the binary\n    ns_g_mass: float\n        NS gravitational mass (in solar masses)\n    ns_sequence: 3D-array\n        contains the sequence data in the form NS gravitational\n         mass (in solar masses), NS baryonic mass (in solar\n         masses), NS compactness (dimensionless)\n    chi: float\n        the BH dimensionless spin parameter\n    incl: float\n        the inclination angle between the BH spin and the orbital\n        angular momentum in radians\n    shift: float\n        an amount to be subtracted to the remnant mass predicted\n        by the model (in solar masses)\n\n    Returns\n    ----------\n    remnant_mass: float\n        The remnant mass in solar masses\n    \"\"\"\n    # Sanity checks\n    if not (eta>0. and eta<=0.25 and abs(chi)<=1):\n        print('The BH spin magnitude must be <=1 and eta must be between 0 and 0.25')\n        print('This script was launched with ns_mass={0}, eta={1}, chi={2}, inclination={3}\\n'.format(ns_g_mass, eta, chi, incl))\n        raise Exception('Unphysical parameters!')\n\n    # Binary mass ratio define to be > 1\n    q = (1+math.sqrt(1-4*eta)-2*eta)/eta*0.5\n\n    # NS compactness and rest mass\n    ns_compactness = ns_g_mass_to_ns_compactness(ns_g_mass, ns_sequence)\n    ns_b_mass = ns_g_mass_to_ns_b_mass(ns_g_mass, ns_sequence)\n\n    # Sanity checks\n    if not (ns_compactness>0 and q>=1):\n        print('A positive NS compactness and a mass ratio that is >1 must be obtained.')\n        print('This script was launched with ns_mass={0}, eta={1}, chi={2}, inclination={3}'.format(ns_b_mass, eta, chi, incl))\n        print('and obtained ns_compactness={0} and q={1}.'.format(ns_compactness, q))\n        print('SOMETHING WENT WRONG!!\\n')\n        raise Exception('Unphysical parameters!')\n\n    # Calculate the dimensionless parameter kappa\n    kappa = q*ns_compactness\n\n    # Effective equatorial spin parameter needed to determine the torus mass*)\n    chi_eff = bh_effective_spin(chi, incl)\n\n    #Sanity checks\n    if not abs(chi_eff)<=1:\n        print('The effective BH spin magnitude must be <=1')\n        print('This script was launched with ns_mass={0}, eta={1}, chi={2}, inclination={3}'.format(ns_b_mass, eta, chi, incl))\n        print('and obtained chi_eff={0}.'.format(chi_eff))\n        print('SOMETHING WENT WRONG!!\\n')\n        raise Exception('Unphysical parameters!')\n\n    # Taking the 1st element with full_output=1 avoids some annoying messages on stdout\n    xi = scipy.optimize.fsolve(xi_eq, 100., args=(kappa,chi_eff,q), full_output=1)[0]\n\n    # Fit parameters and tidal correction\n    alpha = 0.296 # +/- 0.011\n    beta  = 0.171 # +/- 0.008\n    # The remnant mass over the NS rest mass\n    remnant_mass = alpha*xi*(1-2*ns_compactness)-beta*kappa*PG_ISSO_solver(chi_eff,0)\n\n    # The remnant mass in the same units as the NS rest mass (presumably solar masses)\n    remnant_mass = remnant_mass*ns_b_mass - shift\n\n    return remnant_mass"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remnant_mass_ulim(eta, ns_g_mass, bh_spin_z, ns_sequence, max_ns_g_mass, shift):\n    # Sanity checks\n    if not (eta > 0. and eta <=0.25 and abs(bh_spin_z)<=1):\n        raise Exception(\"\"\"The absolute value of the BH spin z-component must be <=1.\n           Eta must be between 0 and 0.25.\n           The function remnant_mass_ulim was launched with eta={0} and chi_z={1}.\n           Unphysical parameters!\"\"\".format(eta, bh_spin_z))\n    # To maximise the remnant mass, allow for the BH spin magnitude to be maximum\n    bh_spin_magnitude = 1.\n    # Unreasonably large remnant disk mass\n    default_remnant_mass = 100.\n    if not ns_g_mass > max_ns_g_mass:\n        bh_spin_inclination = np.arccos(bh_spin_z/bh_spin_magnitude)\n        remnant_mass_upper_limit = pycbc.tmpltbank.em_progenitors.remnant_mass(eta, ns_g_mass, ns_sequence, bh_spin_magnitude, bh_spin_inclination, shift)\n    else:\n        remnant_mass_upper_limit = default_remnant_mass\n\n    return remnant_mass_upper_limit", "response": "This function determines the maximum remnant disk mass for a given NS - BH system and the given NPS and NPS."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_em_constraint_data_point(mNS, sBH, eos_name, threshold, eta_default):\n    ns_sequence, max_ns_g_mass = load_ns_sequence(eos_name)\n    if mNS > max_ns_g_mass:\n        eta_sol = eta_default\n    else:\n        eta_min = 0.01 #mBH_max*mNS/(mBH_max+mNS)**2\n        disk_mass_down = remnant_mass_ulim(eta_min, mNS, sBH, ns_sequence, max_ns_g_mass, threshold)\n        eta_max = 0.25 #mBH_min*mNS/(mBH_min+mNS)**2\n        disk_mass_up = remnant_mass_ulim(eta_max, mNS, sBH, ns_sequence, max_ns_g_mass, threshold)\n        if disk_mass_down*disk_mass_up < 0:\n            # Methods that work are (in order of performance speed): brentq, brenth, ridder, bisect\n            eta_sol = scipy.optimize.brentq(remnant_mass_ulim, eta_min, eta_max, args=(mNS, sBH, ns_sequence, max_ns_g_mass, threshold))\n        elif disk_mass_down > 0:\n            eta_sol = 0.        # EM counterpart requires eta<eta_min: penalize this point\n        elif disk_mass_up < 0:\n            eta_sol = 0.2500001 # EM counterpart is impossible\n        elif disk_mass_down == 0:\n            eta_sol = eta_min\n        else:\n            eta_sol = eta_max\n\n    return eta_sol", "response": "Function that determines the minimum symmetric mass ratio for an em - constraint data point for a given NS mass and BH spin parameter."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef generate_em_constraint_data(mNS_min, mNS_max, delta_mNS, sBH_min, sBH_max, delta_sBH, eos_name, threshold, eta_default):\n    # Build a grid of points in the mNS x sBHz space,\n    # making sure maxima and minima are included\n    mNS_nsamples = complex(0,int(np.ceil((mNS_max-mNS_min)/delta_mNS)+1))\n    sBH_nsamples = complex(0,int(np.ceil((sBH_max-sBH_min)/delta_sBH)+1))\n    mNS_vec, sBH_vec = np.mgrid[mNS_min:mNS_max:mNS_nsamples, sBH_min:sBH_max:sBH_nsamples] # pylint:disable=invalid-slice-index\n    mNS_locations = np.array(mNS_vec[:,0])\n    sBH_locations = np.array(sBH_vec[0])\n    mNS_sBH_grid = zip(mNS_vec.ravel(), sBH_vec.ravel())\n    mNS_sBH_grid = np.array(mNS_sBH_grid)\n    mNS_vec = np.array(mNS_sBH_grid[:,0])\n    sBH_vec = np.array(mNS_sBH_grid[:,1])\n\n    # Until a numpy v>=1.7 is available everywhere, we have to use a silly\n    # vectorization of find_em_constraint_data_point and pass to it a bunch of\n    # constant arguments as vectors with one entry repeated several times\n    eos_name_vec=[eos_name for _ in range(len(mNS_vec))]\n    eos_name_vec=np.array(eos_name_vec)\n    threshold_vec=np.empty(len(mNS_vec))\n    threshold_vec.fill(threshold)\n    eta_default_vec=np.empty(len(mNS_vec))\n    eta_default_vec.fill(eta_default)\n\n    # Compute the minimum etas at all point in the mNS x sBHz grid\n    eta_sol = find_em_constraint_data_points(mNS_vec, sBH_vec, eos_name_vec, threshold_vec, eta_default_vec)\n    eta_sol = eta_sol.reshape(-1,len(sBH_locations))\n    # Save the results\n    np.savez('constraint_em_bright', mNS_pts=mNS_locations, sBH_pts=sBH_locations, eta_mins=eta_sol)\n    # Cast the results in a format that is quick to plot from textfile\n    constraint_data = zip(mNS_vec.ravel(), sBH_vec.ravel(), eta_sol.ravel())\n    np.savetxt('constraint_em_bright.dat', constraint_data)", "response": "This function generates the constraint data for a given set of MNS and maxima."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfunctioning that uses the end product of generate_em_constraint_data to swipe over a set of NS-BH binaries and determine the minimum symmetric mass ratio required by each binary to yield a remnant disk mass that exceeds a certain threshold. Each binary passed to this function consists of a NS mass and a BH spin parameter component along the orbital angular momentum. Unlike find_em_constraint_data_point, which solves the problem at a given point in the paremter space and is more generic, this function interpolates the results produced by generate_em_constraint_data at the desired locations: generate_em_constraint_data must be run once prior to calling min_eta_for_em_bright. Parameters ----------- bh_spin_z: array desired values of the BH dimensionless spin parameter for the spin projection along the orbital angular momentum ns_g_mass: array desired values of the NS gravitational mass (in solar masses) mNS_pts: array NS mass values (in solar masses) from the output of generate_em_constraint_data sBH_pts: array BH dimensionless spin parameter values along the orbital angular momentum from the output of generate_em_constraint_data eta_mins: array minimum symmetric mass ratio values to exceed a given remnant disk mass threshold from the output of generate_em_constraint_data Returns ---------- eta_min: array the minimum symmetric mass ratio required by each binary in the input to yield a remnant disk mass that exceeds a certain threshold", "response": "def min_eta_for_em_bright(bh_spin_z, ns_g_mass, mNS_pts, sBH_pts, eta_mins):\n    \"\"\"\n    Function that uses the end product of generate_em_constraint_data\n    to swipe over a set of NS-BH binaries and determine the minimum\n    symmetric mass ratio required by each binary to yield a remnant\n    disk mass that exceeds a certain threshold.  Each binary passed\n    to this function consists of a NS mass and a BH spin parameter\n    component along the orbital angular momentum.  Unlike\n    find_em_constraint_data_point, which solves the problem at\n    a given point in the paremter space and is more generic, this\n    function interpolates the results produced by\n    generate_em_constraint_data at the desired locations:\n    generate_em_constraint_data must be run once prior to calling\n    min_eta_for_em_bright.\n\n    Parameters\n    -----------\n    bh_spin_z: array\n        desired values of the BH dimensionless spin parameter for the\n        spin projection along the orbital angular momentum\n    ns_g_mass: array\n        desired values of the NS gravitational mass (in solar masses)\n    mNS_pts: array\n        NS mass values (in solar masses) from the output of\n        generate_em_constraint_data\n    sBH_pts: array\n        BH dimensionless spin parameter values along the orbital\n        angular momentum from the output of generate_em_constraint_data\n    eta_mins: array\n        minimum symmetric mass ratio values to exceed a given remnant\n        disk mass threshold from the output of generate_em_constraint_data\n\n    Returns\n    ----------\n    eta_min: array\n        the minimum symmetric mass ratio required by each binary in the\n        input to yield a remnant disk mass that exceeds a certain\n        threshold\n    \"\"\"\n    f = scipy.interpolate.RectBivariateSpline(mNS_pts, sBH_pts, eta_mins, kx=1, ky=1)\n    # If bh_spin_z is a numpy array (assuming ns_g_mass has the same size)\n    if isinstance(bh_spin_z, np.ndarray):\n        eta_min = np.empty(len(bh_spin_z))\n        for i in range(len(bh_spin_z)):\n            eta_min[i] = f(ns_g_mass[i], bh_spin_z[i])\n    # Else (assuming ns_g_mass and bh_spin_z are single numbers)\n    else:\n        eta_min = f(ns_g_mass, bh_spin_z)\n\n    return eta_min"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef low_frequency_cutoff_from_cli(opts):\n    # FIXME: this just uses the same frequency cutoff for every instrument for\n    # now. We should allow for different frequency cutoffs to be used; that\n    # will require (minor) changes to the Likelihood class\n    instruments = opts.instruments if opts.instruments is not None else []\n    return {ifo: opts.low_frequency_cutoff for ifo in instruments}", "response": "Parses the low frequency cutoff from the given options."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef data_from_cli(opts):\n    # get gates to apply\n    gates = gates_from_cli(opts)\n    psd_gates = psd_gates_from_cli(opts)\n\n    # get strain time series\n    instruments = opts.instruments if opts.instruments is not None else []\n    strain_dict = strain_from_cli_multi_ifos(opts, instruments,\n                                             precision=\"double\")\n    # apply gates if not waiting to overwhiten\n    if not opts.gate_overwhitened:\n        strain_dict = apply_gates_to_td(strain_dict, gates)\n\n    # get strain time series to use for PSD estimation\n    # if user has not given the PSD time options then use same data as analysis\n    if opts.psd_start_time and opts.psd_end_time:\n        logging.info(\"Will generate a different time series for PSD \"\n                     \"estimation\")\n        psd_opts = opts\n        psd_opts.gps_start_time = psd_opts.psd_start_time\n        psd_opts.gps_end_time = psd_opts.psd_end_time\n        psd_strain_dict = strain_from_cli_multi_ifos(psd_opts,\n                                                     instruments,\n                                                     precision=\"double\")\n        # apply any gates\n        logging.info(\"Applying gates to PSD data\")\n        psd_strain_dict = apply_gates_to_td(psd_strain_dict, psd_gates)\n\n    elif opts.psd_start_time or opts.psd_end_time:\n        raise ValueError(\"Must give --psd-start-time and --psd-end-time\")\n    else:\n        psd_strain_dict = strain_dict\n\n    # FFT strain and save each of the length of the FFT, delta_f, and\n    # low frequency cutoff to a dict\n    stilde_dict = {}\n    length_dict = {}\n    delta_f_dict = {}\n    low_frequency_cutoff_dict = low_frequency_cutoff_from_cli(opts)\n    for ifo in instruments:\n        stilde_dict[ifo] = strain_dict[ifo].to_frequencyseries()\n        length_dict[ifo] = len(stilde_dict[ifo])\n        delta_f_dict[ifo] = stilde_dict[ifo].delta_f\n\n    # get PSD as frequency series\n    psd_dict = psd_from_cli_multi_ifos(\n        opts, length_dict, delta_f_dict, low_frequency_cutoff_dict,\n        instruments, strain_dict=psd_strain_dict, precision=\"double\")\n\n    # apply any gates to overwhitened data, if desired\n    if opts.gate_overwhitened and opts.gate is not None:\n        logging.info(\"Applying gates to overwhitened data\")\n        # overwhiten the data\n        for ifo in gates:\n            stilde_dict[ifo] /= psd_dict[ifo]\n        stilde_dict = apply_gates_to_fd(stilde_dict, gates)\n        # unwhiten the data for the model\n        for ifo in gates:\n            stilde_dict[ifo] *= psd_dict[ifo]\n\n    return strain_dict, stilde_dict, psd_dict", "response": "Loads the data needed for a single language model from the given command - line options."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd the options needed to configure plots of posterior results.", "response": "def add_plot_posterior_option_group(parser):\n    \"\"\"Adds the options needed to configure plots of posterior results.\n\n    Parameters\n    ----------\n    parser : object\n        ArgumentParser instance.\n    \"\"\"\n    pgroup = parser.add_argument_group(\"Options for what plots to create and \"\n                                       \"their formats.\")\n    pgroup.add_argument('--plot-marginal', action='store_true', default=False,\n                        help=\"Plot 1D marginalized distributions on the \"\n                             \"diagonal axes.\")\n    pgroup.add_argument('--marginal-percentiles', nargs='+', default=None,\n                        type=float,\n                        help=\"Percentiles to draw lines at on the 1D \"\n                             \"histograms.\")\n    pgroup.add_argument(\"--plot-scatter\", action='store_true', default=False,\n                        help=\"Plot each sample point as a scatter plot.\")\n    pgroup.add_argument(\"--plot-density\", action=\"store_true\", default=False,\n                        help=\"Plot the posterior density as a color map.\")\n    pgroup.add_argument(\"--plot-contours\", action=\"store_true\", default=False,\n                        help=\"Draw contours showing the 50th and 90th \"\n                             \"percentile confidence regions.\")\n    pgroup.add_argument('--contour-percentiles', nargs='+', default=None,\n                        type=float,\n                        help=\"Percentiles to draw contours if different \"\n                             \"than 50th and 90th.\")\n    # add mins, maxs options\n    pgroup.add_argument('--mins', nargs='+', metavar='PARAM:VAL', default=[],\n                        help=\"Specify minimum parameter values to plot. This \"\n                             \"should be done by specifying the parameter name \"\n                             \"followed by the value. Parameter names must be \"\n                             \"the same as the PARAM argument in --parameters \"\n                             \"(or, if no parameters are provided, the same as \"\n                             \"the parameter name specified in the variable \"\n                             \"args in the input file. If none provided, \"\n                             \"the smallest parameter value in the posterior \"\n                             \"will be used.\")\n    pgroup.add_argument('--maxs', nargs='+', metavar='PARAM:VAL', default=[],\n                        help=\"Same as mins, but for the maximum values to \"\n                             \"plot.\")\n    # add expected parameters options\n    pgroup.add_argument('--expected-parameters', nargs='+',\n                        metavar='PARAM:VAL',\n                        default=[],\n                        help=\"Specify expected parameter values to plot. If \"\n                             \"provided, a cross will be plotted in each axis \"\n                             \"that an expected parameter is provided. \"\n                             \"Parameter names must be \"\n                             \"the same as the PARAM argument in --parameters \"\n                             \"(or, if no parameters are provided, the same as \"\n                             \"the parameter name specified in the variable \"\n                             \"args in the input file.\")\n    pgroup.add_argument('--expected-parameters-color', default='r',\n                        help=\"What to color the expected-parameters cross. \"\n                             \"Default is red.\")\n    pgroup.add_argument('--plot-injection-parameters', action='store_true',\n                        default=False,\n                        help=\"Get the expected parameters from the injection \"\n                             \"in the input file. There must be only a single \"\n                             \"injection in the file to work. Any values \"\n                             \"specified by expected-parameters will override \"\n                             \"the values obtained for the injection.\")\n    return pgroup"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot_ranges_from_cli(opts):\n    mins = {}\n    for x in opts.mins:\n        x = x.split(':')\n        if len(x) != 2:\n            raise ValueError(\"option --mins not specified correctly; see help\")\n        mins[x[0]] = float(x[1])\n    maxs = {}\n    for x in opts.maxs:\n        x = x.split(':')\n        if len(x) != 2:\n            raise ValueError(\"option --maxs not specified correctly; see help\")\n        maxs[x[0]] = float(x[1])\n    return mins, maxs", "response": "Parses the mins and maxs arguments from the plot_posterior option\n    group."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing the expected_parameters option from the plot_posterior option group.", "response": "def expected_parameters_from_cli(opts):\n    \"\"\"Parses the --expected-parameters arguments from the `plot_posterior`\n    option group.\n\n    Parameters\n    ----------\n    opts : ArgumentParser\n        The parsed arguments from the command line.\n\n    Returns\n    -------\n    dict\n        Dictionary of parameter name -> expected value. Only parameters that\n        were specified in the --expected-parameters option will be included; if\n        no parameters were provided, will return an empty dictionary.\n    \"\"\"\n    expected = {}\n    for x in opts.expected_parameters:\n        x = x.split(':')\n        if len(x) != 2:\n            raise ValueError(\"option --expected-paramters not specified \"\n                             \"correctly; see help\")\n        expected[x[0]] = float(x[1])\n    return expected"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_scatter_option_group(parser):\n    scatter_group = parser.add_argument_group(\"Options for configuring the \"\n                                              \"scatter plot.\")\n\n    scatter_group.add_argument(\n        '--z-arg', type=str, default=None, action=ParseParametersArg,\n        help='What to color the scatter points by. Syntax is the same as the '\n             'parameters option.')\n    scatter_group.add_argument(\n        \"--vmin\", type=float, help=\"Minimum value for the colorbar.\")\n    scatter_group.add_argument(\n        \"--vmax\", type=float, help=\"Maximum value for the colorbar.\")\n    scatter_group.add_argument(\n        \"--scatter-cmap\", type=str, default='plasma',\n        help=\"Specify the colormap to use for points. Default is plasma.\")\n\n    return scatter_group", "response": "Adds the options needed to configure the scatter plot."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_density_option_group(parser):\n    density_group = parser.add_argument_group(\"Options for configuring the \"\n                                              \"contours and density color map\")\n\n    density_group.add_argument(\n        \"--density-cmap\", type=str, default='viridis',\n        help=\"Specify the colormap to use for the density. \"\n             \"Default is viridis.\")\n    density_group.add_argument(\n        \"--contour-color\", type=str, default=None,\n        help=\"Specify the color to use for the contour lines. Default is \"\n             \"white for density plots and black for scatter plots.\")\n    density_group.add_argument(\n        '--use-kombine-kde', default=False, action=\"store_true\",\n        help=\"Use kombine's KDE for determining contours. \"\n             \"Default is to use scipy's gaussian_kde.\")\n\n    return density_group", "response": "Adds the options needed to configure contours and density colour map."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads a prior distribution from the given config file.", "response": "def prior_from_config(cp, prior_section='prior'):\n    \"\"\"Loads a prior distribution from the given config file.\n\n    Parameters\n    ----------\n    cp : pycbc.workflow.WorkflowConfigParser\n        The config file to read.\n    sections : list of str, optional\n        The sections to retrieve the prior from. If ``None`` (the default),\n        will look in sections starting with 'prior'.\n\n    Returns\n    -------\n    distributions.JointDistribution\n        The prior distribution.\n    \"\"\"\n    # Read variable and static parameters from the config file\n    variable_params, _ = distributions.read_params_from_config(\n        cp, prior_section=prior_section, vargs_section='variable_params',\n        sargs_section='static_params')\n    # Read constraints to apply to priors from the config file\n    constraints = distributions.read_constraints_from_config(cp)\n    # Get PyCBC distribution instances for each variable parameter in the\n    # config file\n    dists = distributions.read_distributions_from_config(cp, prior_section)\n    # construct class that will return draws from the prior\n    return distributions.JointDistribution(variable_params, *dists,\n                                           **{\"constraints\": constraints})"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compute_search_efficiency_in_bins(\n         found, total, ndbins,\n         sim_to_bins_function=lambda sim: (sim.distance,)):\n    \"\"\"\n    Calculate search efficiency in the given ndbins.\n\n    The first dimension of ndbins must be bins over injected distance.\n    sim_to_bins_function must map an object to a tuple indexing the ndbins.\n    \"\"\"\n    bins = bin_utils.BinnedRatios(ndbins)\n\n    # increment the numerator and denominator with found / found+missed injs\n    [bins.incnumerator(sim_to_bins_function(sim)) for sim in found]\n    [bins.incdenominator(sim_to_bins_function(sim)) for sim in total]\n\n    # regularize by setting denoms to 1 to avoid nans\n    bins.regularize()\n\n    # efficiency array is the ratio\n    eff = bin_utils.BinnedArray(bin_utils.NDBins(ndbins), array=bins.ratio())\n\n    # compute binomial uncertainties in each bin\n    err_arr = numpy.sqrt(eff.array * (1-eff.array)/bins.denominator.array)\n    err = bin_utils.BinnedArray(bin_utils.NDBins(ndbins), array=err_arr)\n\n    return eff, err", "response": "Compute search efficiency in the given ndbins."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compute_search_volume_in_bins(found, total, ndbins, sim_to_bins_function):\n    eff, err = compute_search_efficiency_in_bins(\n                                    found, total, ndbins, sim_to_bins_function)\n    dx = ndbins[0].upper() - ndbins[0].lower()\n    r = ndbins[0].centres()\n\n    # volume and errors have one fewer dimension than the input NDBins\n    vol = bin_utils.BinnedArray(bin_utils.NDBins(ndbins[1:]))\n    errors = bin_utils.BinnedArray(bin_utils.NDBins(ndbins[1:]))\n\n    # integrate efficiency to obtain volume\n    vol.array = numpy.trapz(eff.array.T * 4. * numpy.pi * r**2, r, dx)\n\n    # propagate errors in eff to errors in V\n    errors.array = numpy.sqrt(\n        ((4 * numpy.pi * r**2 * err.array.T * dx)**2).sum(axis=-1)\n    )\n\n    return vol, errors", "response": "Compute search sensitive volume by integrating efficiency in distance bins and propagating errors in V\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef volume_to_distance_with_errors(vol, vol_err):\n    dist = (vol * 3.0/4.0/numpy.pi) ** (1.0/3.0)\n    ehigh = ((vol + vol_err) * 3.0/4.0/numpy.pi) ** (1.0/3.0) - dist\n    delta = numpy.where(vol >= vol_err, vol - vol_err, 0)\n    elow = dist - (delta * 3.0/4.0/numpy.pi) ** (1.0/3.0)\n    return dist, ehigh, elow", "response": "Return the distance and standard deviation upper and lower bounds of a\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes sensitive volume and standard error via direct Monte Carlo integral.", "response": "def volume_montecarlo(found_d, missed_d, found_mchirp, missed_mchirp,\n                      distribution_param, distribution, limits_param,\n                      min_param=None, max_param=None):\n    \"\"\"\n    Compute sensitive volume and standard error via direct Monte Carlo integral\n\n    Injections should be made over a range of distances such that sensitive\n    volume due to signals closer than D_min is negligible, and efficiency at\n    distances above D_max is negligible\n    TODO : Replace this function by Collin's formula given in Usman et al .. ?\n    OR get that coded as a new function?\n\n    Parameters\n    -----------\n    found_d: numpy.ndarray\n        The distances of found injections\n    missed_d: numpy.ndarray\n        The distances of missed injections\n    found_mchirp: numpy.ndarray\n        Chirp mass of found injections\n    missed_mchirp: numpy.ndarray\n        Chirp mass of missed injections\n    distribution_param: string\n        Parameter D of the injections used to generate a distribution over\n        distance, may be 'distance', 'chirp_distance'.\n    distribution: string\n        form of the distribution over the parameter, may be\n        'log' (uniform in log D)\n        'uniform' (uniform in D)\n        'distancesquared' (uniform in D**2)\n        'volume' (uniform in D***3)\n    limits_param: string\n        Parameter Dlim specifying limits inside which injections were made\n        may be 'distance', 'chirp distance'\n    min_param: float\n        minimum value of Dlim at which injections were made; only used for\n        log distribution, then if None the minimum actually injected value\n        will be used\n    max_param: float\n        maximum value of Dlim out to which injections were made; if None\n        the maximum actually injected value will be used\n\n    Returns\n    --------\n    volume: float\n        Volume estimate\n    volume_error: float\n        The standard error in the volume\n    \"\"\"\n    d_power = {\n        'log'             : 3.,\n        'uniform'         : 2.,\n        'distancesquared' : 1.,\n        'volume'          : 0.\n    }[distribution]\n    mchirp_power = {\n            'log'             : 0.,\n            'uniform'         : 5. / 6.,\n            'distancesquared' : 5. / 3.,\n            'volume'          : 15. / 6.\n    }[distribution]\n\n    # establish maximum physical distance: first for chirp distance distribution\n    if limits_param == 'chirp_distance':\n        mchirp_standard_bns = 1.4 * 2.**(-1. / 5.)\n        all_mchirp = numpy.concatenate((found_mchirp, missed_mchirp))\n        max_mchirp = all_mchirp.max()\n        if max_param is not None:\n            # use largest actually injected mchirp for conversion\n            max_distance = max_param * \\\n                                  (max_mchirp / mchirp_standard_bns)**(5. / 6.)\n        else:\n            max_distance = max(found_d.max(), missed_d.max())\n    elif limits_param == 'distance':\n        if max_param is not None:\n            max_distance = max_param\n        else:\n            # if no max distance given, use max distance actually injected\n            max_distance = max(found_d.max(), missed_d.max())\n    else:\n        raise NotImplementedError(\"%s is not a recognized parameter\"\n                                  % limits_param)\n\n    # volume of sphere\n    montecarlo_vtot = (4. / 3.) * numpy.pi * max_distance**3.\n\n    # arrays of weights for the MC integral\n    if distribution_param == 'distance':\n        found_weights = found_d ** d_power\n        missed_weights = missed_d ** d_power\n    elif distribution_param == 'chirp_distance':\n        # weight by a power of mchirp to rescale injection density to the\n        # target mass distribution\n        found_weights = found_d ** d_power * \\\n                        found_mchirp ** mchirp_power\n        missed_weights = missed_d ** d_power * \\\n                         missed_mchirp ** mchirp_power\n    else:\n        raise NotImplementedError(\"%s is not a recognized distance parameter\"\n                                  % distribution_param)\n\n    all_weights = numpy.concatenate((found_weights, missed_weights))\n\n    # measured weighted efficiency is w_i for a found inj and 0 for missed\n    # MC integral is volume of sphere * (sum of found weights)/(sum of all weights)\n    # over injections covering the sphere\n    mc_weight_samples = numpy.concatenate((found_weights, 0 * missed_weights))\n    mc_sum = sum(mc_weight_samples)\n\n    if limits_param == 'distance':\n        mc_norm = sum(all_weights)\n    elif limits_param == 'chirp_distance':\n        # if injections are made up to a maximum chirp distance, account for\n        # extra missed injections that would occur when injecting up to\n        # maximum physical distance : this works out to a 'chirp volume' factor\n        mc_norm = sum(all_weights * (max_mchirp / all_mchirp) ** (5. / 2.))\n\n    # take out a constant factor\n    mc_prefactor = montecarlo_vtot / mc_norm\n\n    # count the samples\n    if limits_param == 'distance':\n        Ninj = len(mc_weight_samples)\n    elif limits_param == 'chirp_distance':\n        # find the total expected number after extending from maximum chirp\n        # dist up to maximum physical distance\n        if distribution == 'log':\n            # only need minimum distance in this one case\n            if min_param is not None:\n                min_distance = min_param * \\\n                     (numpy.min(all_mchirp) / mchirp_standard_bns) ** (5. / 6.)\n            else:\n                min_distance = min(numpy.min(found_d), numpy.min(missed_d))\n            logrange = numpy.log(max_distance / min_distance)\n            Ninj = len(mc_weight_samples) + (5. / 6.) * \\\n                             sum(numpy.log(max_mchirp / all_mchirp) / logrange)\n        else:\n            Ninj = sum((max_mchirp / all_mchirp) ** mchirp_power)\n\n    # sample variance of efficiency: mean of the square - square of the mean\n    mc_sample_variance = sum(mc_weight_samples ** 2.) / Ninj - \\\n                                                          (mc_sum / Ninj) ** 2.\n\n    # return MC integral and its standard deviation; variance of mc_sum scales\n    # relative to sample variance by Ninj (Bienayme' rule)\n    vol = mc_prefactor * mc_sum\n    vol_err = mc_prefactor * (Ninj * mc_sample_variance) ** 0.5\n    return vol, vol_err"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the sensitive volume using a distance binned efficiency estimate", "response": "def volume_binned_pylal(f_dist, m_dist, bins=15):\n    \"\"\" Compute the sensitive volume using a distance binned efficiency estimate\n\n    Parameters\n    -----------\n    f_dist: numpy.ndarray\n        The distances of found injections\n    m_dist: numpy.ndarray\n        The distances of missed injections\n\n    Returns\n    --------\n    volume: float\n        Volume estimate\n    volume_error: float\n        The standard error in the volume\n    \"\"\"\n    def sims_to_bin(sim):\n        return (sim, 0)\n\n    total = numpy.concatenate([f_dist, m_dist])\n    ndbins = bin_utils.NDBins([bin_utils.LinearBins(min(total), max(total), bins),\n                               bin_utils.LinearBins(0., 1, 1)])\n    vol, verr = compute_search_volume_in_bins(f_dist, total, ndbins, sims_to_bin)\n    return vol.array[0], verr.array[0]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the sensitive volume using sum over spherical shells.", "response": "def volume_shell(f_dist, m_dist):\n    \"\"\" Compute the sensitive volume using sum over spherical shells.\n\n    Parameters\n    -----------\n    f_dist: numpy.ndarray\n        The distances of found injections\n    m_dist: numpy.ndarray\n        The distances of missed injections\n\n    Returns\n    --------\n    volume: float\n        Volume estimate\n    volume_error: float\n        The standard error in the volume\n    \"\"\"\n    f_dist.sort()\n    m_dist.sort()\n    distances = numpy.concatenate([f_dist, m_dist])\n    dist_sorting = distances.argsort()\n    distances = distances[dist_sorting]\n    low = 0\n    vol = 0\n    vol_err = 0\n    for i in range(len(distances)):\n        if i == len(distances) - 1:\n            break\n\n        high = (distances[i+1] + distances[i]) / 2\n        bin_width = high - low\n\n        if dist_sorting[i] < len(f_dist):\n            vol += 4 * numpy.pi * distances[i]**2.0 * bin_width\n            vol_err += (4 * numpy.pi * distances[i]**2.0 * bin_width)**2.0\n\n        low = high\n    vol_err = vol_err ** 0.5\n    return vol, vol_err"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef raw_samples_to_dict(sampler, raw_samples):\n    sampling_params = sampler.sampling_params\n    # convert to dictionary\n    samples = {param: raw_samples[..., ii] for\n               ii, param in enumerate(sampling_params)}\n    # apply boundary conditions\n    samples = sampler.model.prior_distribution.apply_boundary_conditions(\n        **samples)\n    # apply transforms to go to model's variable params space\n    if sampler.model.sampling_transforms is not None:\n        samples = sampler.model.sampling_transforms.apply(\n            samples, inverse=True)\n    return samples", "response": "Convenience function for converting raw samples to a dictionary of samples."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a list of blobs into a dictionary of model stats.", "response": "def blob_data_to_dict(stat_names, blobs):\n    \"\"\"Converts list of \"blobs\" to a dictionary of model stats.\n\n    Samplers like ``emcee`` store the extra tuple returned by ``CallModel`` to\n    a list called blobs. This is a list of lists of tuples with shape\n    niterations x nwalkers x nstats, where nstats is the number of stats\n    returned by the model's ``default_stats``. This converts that list to a\n    dictionary of arrays keyed by the stat names.\n\n    Parameters\n    ----------\n    stat_names : list of str\n        The list of the stat names.\n    blobs : list of list of tuples\n        The data to convert.\n\n    Returns\n    -------\n    dict :\n        A dictionary mapping the model's ``default_stats`` to arrays of values.\n        Each array will have shape ``nwalkers x niterations``.\n    \"\"\"\n    # get the dtypes of each of the stats; we'll just take this from the\n    # first iteration and walker\n    dtypes = [type(val) for val in blobs[0][0]]\n    assert len(stat_names) == len(dtypes), (\n        \"number of stat names must match length of tuples in the blobs\")\n    # convert to an array; to ensure that we get the dtypes correct, we'll\n    # cast to a structured array\n    raw_stats = numpy.array(blobs, dtype=zip(stat_names, dtypes))\n    # transpose so that it has shape nwalkers x niterations\n    raw_stats = raw_stats.transpose()\n    # now return as a dictionary\n    return {stat: raw_stats[stat] for stat in stat_names}"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_optional_arg_from_config(cp, section, arg, dtype=str):\n    if cp.has_option(section, arg):\n        val = dtype(cp.get(section, arg))\n    else:\n        val = None\n    return val", "response": "Convenience function to retrieve an optional argument from a configparser object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef niterations(self):\n        itercounter = self._itercounter\n        if itercounter is None:\n            itercounter = 0\n        lastclear = self._lastclear\n        if lastclear is None:\n            lastclear = 0\n        return itercounter + lastclear", "response": "The current number of iterations."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the thin interval to use for this entry.", "response": "def get_thin_interval(self):\n        \"\"\"Gets the thin interval to use.\n\n        If ``max_samples_per_chain`` is set, this will figure out what thin\n        interval is needed to satisfy that criteria. In that case, the thin\n        interval used must be a multiple of the currently used thin interval.\n        \"\"\"\n        if self.max_samples_per_chain is not None:\n            # the extra factor of 2 is to account for the fact that the thin\n            # interval will need to be at least twice as large as a previously\n            # used interval\n            thinfactor = 2 * self.niterations // self.max_samples_per_chain\n            # make the new interval is a multiple of the previous, to ensure\n            # that any samples currently on disk can be thinned accordingly\n            thin_interval = (thinfactor // self.thin_interval) * \\\n                self.thin_interval\n            # make sure it's at least 1\n            thin_interval = max(thin_interval, 1)\n        else:\n            thin_interval = self.thin_interval\n        return thin_interval"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the target niterations and eff_nsamples for the sampler. One or the other must be provided.", "response": "def set_target(self, niterations=None, eff_nsamples=None):\n        \"\"\"Sets the target niterations/nsamples for the sampler.\n\n        One or the other must be provided, not both.\n        \"\"\"\n        if niterations is None and eff_nsamples is None:\n            raise ValueError(\"Must provide a target niterations or \"\n                             \"eff_nsamples\")\n        if niterations is not None and eff_nsamples is not None:\n            raise ValueError(\"Must provide a target niterations or \"\n                             \"eff_nsamples, not both\")\n        self._target_niterations = int(niterations) \\\n            if niterations is not None else None\n        self._target_eff_nsamples = int(eff_nsamples) \\\n            if eff_nsamples is not None else None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef p0(self):\n        if self._p0 is None:\n            raise ValueError(\"initial positions not set; run set_p0\")\n        # convert to dict\n        p0 = {param: self._p0[..., k]\n              for (k, param) in enumerate(self.sampling_params)}\n        return p0", "response": "A dictionary of the initial position of the walkers. This is set by using set_p0."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_p0(self, samples_file=None, prior=None):\n        # if samples are given then use those as initial positions\n        if samples_file is not None:\n            with self.io(samples_file, 'r') as fp:\n                samples = fp.read_samples(self.variable_params,\n                                          iteration=-1, flatten=False)\n                # remove the (length 1) niterations dimension\n                samples = samples[..., 0]\n                # make sure we have the same shape\n                assert samples.shape == self.base_shape, (\n                       \"samples in file {} have shape {}, but I have shape {}\".\n                       format(samples_file, samples.shape, self.base_shape))\n            # transform to sampling parameter space\n            if self.model.sampling_transforms is not None:\n                samples = self.model.sampling_transforms.apply(samples)\n        # draw random samples if samples are not provided\n        else:\n            nsamples = numpy.prod(self.base_shape)\n            samples = self.model.prior_rvs(size=nsamples, prior=prior).reshape(\n                self.base_shape)\n        # store as ND array with shape [base_shape] x nparams\n        ndim = len(self.variable_params)\n        p0 = numpy.ones(list(self.base_shape)+[ndim])\n        for i, param in enumerate(self.sampling_params):\n            p0[..., i] = samples[param]\n        self._p0 = p0\n        return self.p0", "response": "Sets the initial position of the walkers."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_initial_conditions(self, initial_distribution=None,\n                               samples_file=None):\n        \"\"\"Sets the initial starting point for the MCMC.\n\n        If a starting samples file is provided, will also load the random\n        state from it.\n        \"\"\"\n        self.set_p0(samples_file=samples_file, prior=initial_distribution)\n        # if a samples file was provided, use it to set the state of the\n        # sampler\n        if samples_file is not None:\n            self.set_state_from_file(samples_file)", "response": "Sets the initial conditions for the MCMC."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef effective_nsamples(self):\n        try:\n            act = numpy.array(list(self.acts.values())).max()\n        except (AttributeError, TypeError):\n            act = numpy.inf\n        if self.burn_in is None:\n            nperwalker = max(int(self.niterations // act), 1)\n        elif self.burn_in.is_burned_in:\n            nperwalker = int(\n                (self.niterations - self.burn_in.burn_in_iteration) // act)\n            # after burn in, we always have atleast 1 sample per walker\n            nperwalker = max(nperwalker, 1)\n        else:\n            nperwalker = 0\n        return self.nwalkers * nperwalker", "response": "The effective number of samples post burn - in that the sampler has\n            acquired so far."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef checkpoint(self):\n        # thin and write new samples\n        for fn in [self.checkpoint_file, self.backup_file]:\n            with self.io(fn, \"a\") as fp:\n                # write the current number of iterations\n                fp.write_niterations(self.niterations)\n                thin_interval = self.get_thin_interval()\n                # thin samples on disk if it changed\n                if thin_interval > 1:\n                    # if this is the first time writing, set the file's\n                    # thinned_by\n                    if fp.last_iteration() == 0:\n                        fp.thinned_by = thin_interval\n                    else:\n                        # check if we need to thin the current samples on disk\n                        thin_by = thin_interval // fp.thinned_by\n                        if thin_by > 1:\n                            logging.info(\"Thinning samples in %s by a factor \"\n                                         \"of %i\", fn, int(thin_by))\n                            fp.thin(thin_by)\n                fp_lastiter = fp.last_iteration()\n            logging.info(\"Writing samples to %s with thin interval %i\", fn,\n                         thin_interval)\n            self.write_results(fn)\n        # see if we had anything to write after thinning; if not, don't try\n        # to compute anything\n        with self.io(self.checkpoint_file, \"r\") as fp:\n            nsamples_written = fp.last_iteration() - fp_lastiter\n        if nsamples_written == 0:\n            logging.info(\"No samples written due to thinning\")\n        else:\n            # check for burn in, compute the acls\n            self.acls = None\n            if self.burn_in is not None:\n                logging.info(\"Updating burn in\")\n                self.burn_in.evaluate(self.checkpoint_file)\n                burn_in_index = self.burn_in.burn_in_index\n                logging.info(\"Is burned in: %r\", self.burn_in.is_burned_in)\n                if self.burn_in.is_burned_in:\n                    logging.info(\"Burn-in iteration: %i\",\n                                 int(self.burn_in.burn_in_iteration))\n            else:\n                burn_in_index = 0\n            # Compute acls; the burn_in test may have calculated an acl and\n            # saved it, in which case we don't need to do it again.\n            if self.acls is None:\n                logging.info(\"Computing acls\")\n                self.acls = self.compute_acl(self.checkpoint_file,\n                                             start_index=burn_in_index)\n            logging.info(\"ACT: %s\", str(numpy.array(self.acts.values()).max()))\n            # write\n            for fn in [self.checkpoint_file, self.backup_file]:\n                with self.io(fn, \"a\") as fp:\n                    if self.burn_in is not None:\n                        fp.write_burn_in(self.burn_in)\n                    if self.acls is not None:\n                        fp.write_acls(self.acls)\n                    # write effective number of samples\n                    fp.write_effective_nsamples(self.effective_nsamples)\n        # check validity\n        logging.info(\"Validating checkpoint and backup files\")\n        checkpoint_valid = validate_checkpoint_files(\n            self.checkpoint_file, self.backup_file)\n        if not checkpoint_valid:\n            raise IOError(\"error writing to checkpoint file\")\n        elif self.checkpoint_signal:\n            # kill myself with the specified signal\n            logging.info(\"Exiting with SIG{}\".format(self.checkpoint_signal))\n            kill_cmd=\"os.kill(os.getpid(), signal.SIG{})\".format(\n                self.checkpoint_signal)\n            exec(kill_cmd)\n        # clear the in-memory chain to save memory\n        logging.info(\"Clearing samples from memory\")\n        self.clear_samples()", "response": "Dumps current samples to the checkpoint file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_target_from_config(self, cp, section):\n        if cp.has_option(section, \"niterations\"):\n            niterations = int(cp.get(section, \"niterations\"))\n        else:\n            niterations = None\n        if cp.has_option(section, \"effective-nsamples\"):\n            nsamples = int(cp.get(section, \"effective-nsamples\"))\n        else:\n            nsamples = None\n        self.set_target(niterations=niterations, eff_nsamples=nsamples)", "response": "Sets the target using the given config file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the burn in class from the given config file.", "response": "def set_burn_in_from_config(self, cp):\n        \"\"\"Sets the burn in class from the given config file.\n\n        If no burn-in section exists in the file, then this just set the\n        burn-in class to None.\n        \"\"\"\n        try:\n            bit = self.burn_in_class.from_config(cp, self)\n        except ConfigParser.Error:\n            bit = None\n        self.set_burn_in(bit)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_thin_interval_from_config(self, cp, section):\n        if cp.has_option(section, \"thin-interval\"):\n            thin_interval = int(cp.get(section, \"thin-interval\"))\n            logging.info(\"Will thin samples using interval %i\", thin_interval)\n        else:\n            thin_interval = None\n        if cp.has_option(section, \"max-samples-per-chain\"):\n            max_samps_per_chain = int(cp.get(section, \"max-samples-per-chain\"))\n            logging.info(\"Setting max samples per chain to %i\",\n                         max_samps_per_chain)\n        else:\n            max_samps_per_chain = None\n        # check for consistency\n        if thin_interval is not None and max_samps_per_chain is not None:\n            raise ValueError(\"provide either thin-interval or \"\n                             \"max-samples-per-chain, not both\")\n        # check that the thin interval is < then the checkpoint interval\n        if thin_interval is not None and self.checkpoint_interval is not None \\\n                and thin_interval >= self.checkpoint_interval:\n            raise ValueError(\"thin interval must be less than the checkpoint \"\n                             \"interval\")\n        self.thin_interval = thin_interval\n        self.max_samples_per_chain = max_samps_per_chain", "response": "Sets the thinning options from the given config file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef acts(self):\n        if self.acls is None:\n            return None\n        return {p: acl * self.get_thin_interval()\n                for (p, acl) in self.acls.items()}", "response": "The autocorrelation times of each parameter."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the autocorrleation function of the model params in the given file. By default, parameter values are averaged over all walkers at each iteration. The ACF is then calculated over the averaged chain. An ACF per-walker will be returned instead if ``per_walker=True``. Parameters ----------- filename : str Name of a samples file to compute ACFs for. start_index : {None, int} The start index to compute the acl from. If None, will try to use the number of burn-in iterations in the file; otherwise, will start at the first sample. end_index : {None, int} The end index to compute the acl to. If None, will go to the end of the current iteration. per_walker : optional, bool Return the ACF for each walker separately. Default is False. walkers : optional, int or array Calculate the ACF using only the given walkers. If None (the default) all walkers will be used. parameters : optional, str or array Calculate the ACF for only the given parameters. If None (the default) will calculate the ACF for all of the model params. Returns ------- dict : Dictionary of arrays giving the ACFs for each parameter. If ``per-walker`` is True, the arrays will have shape ``nwalkers x niterations``.", "response": "def compute_acf(cls, filename, start_index=None, end_index=None,\n                    per_walker=False, walkers=None, parameters=None):\n        \"\"\"Computes the autocorrleation function of the model params in the\n        given file.\n\n        By default, parameter values are averaged over all walkers at each\n        iteration. The ACF is then calculated over the averaged chain. An\n        ACF per-walker will be returned instead if ``per_walker=True``.\n\n        Parameters\n        -----------\n        filename : str\n            Name of a samples file to compute ACFs for.\n        start_index : {None, int}\n            The start index to compute the acl from. If None, will try to use\n            the number of burn-in iterations in the file; otherwise, will start\n            at the first sample.\n        end_index : {None, int}\n            The end index to compute the acl to. If None, will go to the end\n            of the current iteration.\n        per_walker : optional, bool\n            Return the ACF for each walker separately. Default is False.\n        walkers : optional, int or array\n            Calculate the ACF using only the given walkers. If None (the\n            default) all walkers will be used.\n        parameters : optional, str or array\n            Calculate the ACF for only the given parameters. If None (the\n            default) will calculate the ACF for all of the model params.\n\n        Returns\n        -------\n        dict :\n            Dictionary of arrays giving the ACFs for each parameter. If\n            ``per-walker`` is True, the arrays will have shape\n            ``nwalkers x niterations``.\n        \"\"\"\n        acfs = {}\n        with cls._io(filename, 'r') as fp:\n            if parameters is None:\n                parameters = fp.variable_params\n            if isinstance(parameters, str) or isinstance(parameters, unicode):\n                parameters = [parameters]\n            for param in parameters:\n                if per_walker:\n                    # just call myself with a single walker\n                    if walkers is None:\n                        walkers = numpy.arange(fp.nwalkers)\n                    arrays = [\n                        cls.compute_acf(filename, start_index=start_index,\n                                        end_index=end_index,\n                                        per_walker=False, walkers=ii,\n                                        parameters=param)[param]\n                        for ii in walkers]\n                    acfs[param] = numpy.vstack(arrays)\n                else:\n                    samples = fp.read_raw_samples(\n                        param, thin_start=start_index, thin_interval=1,\n                        thin_end=end_index, walkers=walkers,\n                        flatten=False)[param]\n                    samples = samples.mean(axis=0)\n                    acfs[param] = autocorrelation.calculate_acf(\n                        samples).numpy()\n        return acfs"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compute_acl(cls, filename, start_index=None, end_index=None,\n                    min_nsamples=10):\n        \"\"\"Computes the autocorrleation length for all model params in the\n        given file.\n\n        Parameter values are averaged over all walkers at each iteration.\n        The ACL is then calculated over the averaged chain. If an ACL cannot\n        be calculated because there are not enough samples, it will be set\n        to ``inf``.\n\n        Parameters\n        -----------\n        filename : str\n            Name of a samples file to compute ACLs for.\n        start_index : int, optional\n            The start index to compute the acl from. If None, will try to use\n            the number of burn-in iterations in the file; otherwise, will start\n            at the first sample.\n        end_index : int, optional\n            The end index to compute the acl to. If None, will go to the end\n            of the current iteration.\n        min_nsamples : int, optional\n            Require a minimum number of samples to compute an ACL. If the\n            number of samples per walker is less than this, will just set to\n            ``inf``. Default is 10.\n\n        Returns\n        -------\n        dict\n            A dictionary giving the ACL for each parameter.\n        \"\"\"\n        acls = {}\n        with cls._io(filename, 'r') as fp:\n            for param in fp.variable_params:\n                samples = fp.read_raw_samples(\n                    param, thin_start=start_index, thin_interval=1,\n                    thin_end=end_index, flatten=False)[param]\n                samples = samples.mean(axis=0)\n                # if < min number of samples, just set to inf\n                if samples.size < min_nsamples:\n                    acl = numpy.inf\n                else:\n                    acl = autocorrelation.calculate_acl(samples)\n                if acl <= 0:\n                    acl = numpy.inf\n                acls[param] = acl\n        return acls", "response": "Computes the autocorrelation length for all parameters in the given file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if a single detector trigger passes the thresholds in the current data.", "response": "def check(self, triggers, data_reader):\n        \"\"\" Look for a single detector trigger that passes the thresholds in\n        the current data.\n        \"\"\"\n        if len(triggers['snr']) == 0:\n            return None\n\n        i = triggers['snr'].argmax()\n        # This uses the pycbc live convention of chisq always meaning the\n        # reduced chisq.\n        rchisq = triggers['chisq'][i]\n        nsnr = ranking.newsnr(triggers['snr'][i], rchisq)\n        dur = triggers['template_duration'][i]\n\n        if nsnr > self.newsnr_threshold and \\\n                rchisq < self.reduced_chisq_threshold and \\\n                dur > self.duration_threshold:\n            fake_coinc = {'foreground/%s/%s' % (self.ifo, k): triggers[k][i]\n                          for k in triggers}\n            fake_coinc['foreground/stat'] = nsnr\n            fake_coinc['foreground/ifar'] = self.fixed_ifar\n            fake_coinc['HWINJ'] = data_reader.near_hwinj()\n            return fake_coinc\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rvs(self, size=1, param=None):\n        if param is not None:\n            dtype = [(param, float)]\n        else:\n            dtype = [(p, float) for p in self.params]\n        arr = numpy.zeros(size, dtype=dtype)\n        for (p,_) in dtype:\n            offset = numpy.power(self._bounds[p][0], self.dim)\n            factor = numpy.power(self._bounds[p][1], self.dim) - \\\n                                      numpy.power(self._bounds[p][0], self.dim)\n            arr[p] = numpy.random.uniform(0.0, 1.0, size=size)\n            arr[p] = numpy.power(factor * arr[p] + offset, 1.0 / self.dim)\n        return arr", "response": "Gives a set of random values drawn from this distribution."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _pdf(self, **kwargs):\n        for p in self._params:\n            if p not in kwargs.keys():\n                raise ValueError(\n                            'Missing parameter {} to construct pdf.'.format(p))\n        if kwargs in self:\n            pdf = self._norm * \\\n                  numpy.prod([(kwargs[p])**(self.dim - 1)\n                              for p in self._params])\n            return float(pdf)\n        else:\n            return 0.0", "response": "Returns the pdf at the given values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the log of the pdf at the given values.", "response": "def _logpdf(self, **kwargs):\n        \"\"\"Returns the log of the pdf at the given values. The keyword\n        arguments must contain all of parameters in self's params. Unrecognized\n        arguments are ignored.\n        \"\"\"\n        for p in self._params:\n            if p not in kwargs.keys():\n                raise ValueError(\n                            'Missing parameter {} to construct pdf.'.format(p))\n        if kwargs in self:\n            log_pdf = self._lognorm + \\\n                      (self.dim - 1) * \\\n                      numpy.log([kwargs[p] for p in self._params]).sum()\n            return log_pdf\n        else:\n            return -numpy.inf"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a UniformPowerLaw distribution based on a configuration file.", "response": "def from_config(cls, cp, section, variable_args):\n        \"\"\"Returns a distribution based on a configuration file. The parameters\n        for the distribution are retrieved from the section titled\n        \"[`section`-`variable_args`]\" in the config file.\n\n        Parameters\n        ----------\n        cp : pycbc.workflow.WorkflowConfigParser\n            A parsed configuration file that contains the distribution\n            options.\n        section : str\n            Name of the section in the configuration file.\n        variable_args : str\n            The names of the parameters for this distribution, separated by\n            `prior.VARARGS_DELIM`. These must appear in the \"tag\" part\n            of the section header.\n\n        Returns\n        -------\n        Uniform\n            A distribution instance from the pycbc.inference.prior module.\n        \"\"\"\n        return super(UniformPowerLaw, cls).from_config(cp, section,\n                                                       variable_args,\n                                                       bounds_required=True)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting a text file with samples.", "response": "def write(cls, output_file, samples, labels, delimiter=None):\n        \"\"\" Writes a text file with samples.\n\n        Parameters\n        -----------\n        output_file : str\n            The path of the file to write.\n        samples : FieldArray\n            Samples to write to file.\n        labels : list\n            A list of strings to include as header in TXT file.\n        delimiter : str\n            Delimiter to use in TXT file.\n        \"\"\"\n        delimiter = delimiter if delimiter is not None else cls.delimiter\n        header = delimiter.join(labels)\n        numpy.savetxt(output_file, samples,\n                      comments=cls.comments, header=header,\n                      delimiter=delimiter)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_topclasses(cls):\n    bases = [c for c in inspect.getmro(cls)\n             if c.__module__.startswith('pycbc') and c != cls]\n    return ', '.join(['{}.{}'.format(c.__module__, c.__name__) for c in bases])", "response": "Gets the base classes that are in pycbc."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntake an integer GPS time and converts it to a string. If a LIGOTimeGPS with nonzero decimal part is passed raises a ValueError.", "response": "def int_gps_time_to_str(t):\n    \"\"\"Takes an integer GPS time, either given as int or lal.LIGOTimeGPS, and\n    converts it to a string. If a LIGOTimeGPS with nonzero decimal part is\n    given, raises a ValueError.\"\"\"\n\n    if isinstance(t, int):\n        return str(t)\n    elif isinstance(t, float):\n        # Wouldn't this just work generically?\n        int_t = int(t)\n        if abs(t - int_t) > 0.:\n            raise ValueError('Need an integer GPS time, got %s' % str(t))\n        return str(int_t)\n    elif isinstance(t, lal.LIGOTimeGPS):\n        if t.gpsNanoSeconds == 0:\n            return str(t.gpsSeconds)\n        else:\n            raise ValueError('Need an integer GPS time, got %s' % str(t))\n    else:\n        err_msg = \"Didn't understand input type {}\".format(type(t))\n        raise ValueError(err_msg)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef select_tmpltbank_class(curr_exe):\n    exe_to_class_map = {\n        'pycbc_geom_nonspinbank'  : PyCBCTmpltbankExecutable,\n        'pycbc_aligned_stoch_bank': PyCBCTmpltbankExecutable\n    }\n    try:\n        return exe_to_class_map[curr_exe]\n    except KeyError:\n        raise NotImplementedError(\n            \"No job class exists for executable %s, exiting\" % curr_exe)", "response": "This function returns a sub - class that is appropriate for setting up the template bank jobs within workflow."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef select_matchedfilter_class(curr_exe):\n    exe_to_class_map = {\n        'pycbc_inspiral'          : PyCBCInspiralExecutable,\n        'pycbc_inspiral_skymax'   : PyCBCInspiralExecutable,\n        'pycbc_multi_inspiral'    : PyCBCMultiInspiralExecutable,\n    }\n    try:\n        return exe_to_class_map[curr_exe]\n    except KeyError:\n        # also conceivable to introduce a default class??\n        raise NotImplementedError(\n            \"No job class exists for executable %s, exiting\" % curr_exe)", "response": "This function returns a sub - class that is appropriate for setting up the matched - filtering jobs within workflow."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a sub - class that is appropriate for setting up jobs to run executables with specific tags in the workflow config.", "response": "def select_generic_executable(workflow, exe_tag):\n    \"\"\" Returns a class that is appropriate for setting up jobs to run executables\n    having specific tags in the workflow config.\n    Executables should not be \"specialized\" jobs fitting into one of the\n    select_XXX_class functions above, i.e. not a matched filter or template\n    bank job, which require extra setup.\n\n    Parameters\n    ----------\n    workflow : pycbc.workflow.core.Workflow\n        The Workflow instance.\n\n    exe_tag : string\n        The name of the config section storing options for this executable and\n        the option giving the executable path in the [executables] section.\n\n    Returns\n    --------\n    exe_class : Sub-class of pycbc.workflow.core.Executable that holds utility\n        functions appropriate for the given executable.  Instances of the class\n        ('jobs') **must** have a method job.create_node()\n    \"\"\"\n    exe_path = workflow.cp.get(\"executables\", exe_tag)\n    exe_name = os.path.basename(exe_path)\n    exe_to_class_map = {\n        'ligolw_add'               : LigolwAddExecutable,\n        'ligolw_cbc_sstinca'           : LigolwSSthincaExecutable,\n        'pycbc_sqlite_simplify'    : PycbcSqliteSimplifyExecutable,\n        'ligolw_cbc_cluster_coincs': SQLInOutExecutable,\n        'ligolw_cbc_repop_coinc'   : SQLInOutExecutable,\n        'repop_coinc_expfit'       : SQLInOutExecutable,\n        'ligolw_cbc_dbinjfind'         : SQLInOutExecutable,\n        'lalapps_inspinj'          : LalappsInspinjExecutable,\n        'pycbc_dark_vs_bright_injections' : PycbcDarkVsBrightInjectionsExecutable,\n        'pycbc_timeslides'         : PycbcTimeslidesExecutable,\n        'pycbc_compute_durations'  : ComputeDurationsExecutable,\n        'pycbc_calculate_far'      : PycbcCalculateFarExecutable,\n        \"pycbc_run_sqlite\"         : SQLInOutExecutable,\n        # FIXME: We may end up with more than one class for using ligolw_sqlite\n        #        How to deal with this?\n        \"ligolw_sqlite\"            : ExtractToXMLExecutable,\n        \"pycbc_inspinjfind\"        : InspinjfindExecutable,\n        \"pycbc_pickle_horizon_distances\" : PycbcPickleHorizonDistsExecutable,\n        \"pycbc_combine_likelihood\" : PycbcCombineLikelihoodExecutable,\n        \"pycbc_gen_ranking_data\"   : PycbcGenerateRankingDataExecutable,\n        \"pycbc_calculate_likelihood\"     : PycbcCalculateLikelihoodExecutable,\n        \"gstlal_inspiral_marginalize_likelihood\"      : GstlalMarginalizeLikelihoodExecutable,\n        \"pycbc_compute_far_from_snr_chisq_histograms\" : GstlalFarfromsnrchisqhistExecutable,\n        \"gstlal_inspiral_plot_sensitivity\"            : GstlalPlotSensitivity,\n        \"gstlal_inspiral_plot_background\" : GstlalPlotBackground,\n        \"gstlal_inspiral_plotsummary\"     : GstlalPlotSummary,\n        \"gstlal_inspiral_summary_page\"    : GstlalSummaryPage,\n        \"pycbc_condition_strain\"         : PycbcConditionStrainExecutable\n    }\n    try:\n        return exe_to_class_map[exe_name]\n    except KeyError:\n        # Should we try some sort of default class??\n        raise NotImplementedError(\n            \"No job class exists for executable %s, exiting\" % exe_name)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pick_tile_size(self, seg_size, data_lengths, valid_chunks, valid_lengths):\n\n        if len(valid_lengths) == 1:\n            return data_lengths[0], valid_chunks[0], valid_lengths[0]\n        else:\n            # Pick the tile size that is closest to 1/3 of the science segment\n            target_size = seg_size / 3\n            pick, pick_diff = 0, abs(valid_lengths[0] - target_size)\n            for i, size in enumerate(valid_lengths):\n                if abs(size - target_size) < pick_diff:\n                    pick, pick_diff  = i, abs(size - target_size)\n            return data_lengths[pick], valid_chunks[pick], valid_lengths[pick]", "response": "Pick the tile size based on the science segment length"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the times for which this job is valid.", "response": "def get_valid_times_for_job(self, num_job, allow_overlap=True):\n        \"\"\" Get the times for which this job is valid. \"\"\"\n        if self.compatibility_mode:\n            return self.get_valid_times_for_job_legacy(num_job)\n        else:\n            return self.get_valid_times_for_job_workflow(num_job,\n                                                   allow_overlap=allow_overlap)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_valid_times_for_job_workflow(self, num_job, allow_overlap=True):\n        # small factor of 0.0001 to avoid float round offs causing us to\n        # miss a second at end of segments.\n        shift_dur = self.curr_seg[0] + int(self.job_time_shift * num_job\\\n                                           + 0.0001)\n        job_valid_seg = self.valid_chunk.shift(shift_dur)\n        # If we need to recalculate the valid times to avoid overlap\n        if not allow_overlap:\n            data_per_job = (self.curr_seg_length - self.data_loss) / \\\n                           float(self.num_jobs)\n            lower_boundary = num_job*data_per_job + \\\n                                 self.valid_chunk[0] + self.curr_seg[0]\n            upper_boundary = data_per_job + lower_boundary\n            # NOTE: Convert to int after calculating both boundaries\n            # small factor of 0.0001 to avoid float round offs causing us to\n            # miss a second at end of segments.\n            lower_boundary = int(lower_boundary)\n            upper_boundary = int(upper_boundary + 0.0001)\n            if lower_boundary < job_valid_seg[0] or \\\n                    upper_boundary > job_valid_seg[1]:\n                err_msg = (\"Workflow is attempting to generate output \"\n                          \"from a job at times where it is not valid.\")\n                raise ValueError(err_msg)\n            job_valid_seg = segments.segment([lower_boundary,\n                                              upper_boundary])\n        return job_valid_seg", "response": "Get the times for which the job num_job will be valid using workflow s\nMimeType method."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_valid_times_for_job_legacy(self, num_job):\n        # All of this should be integers, so no rounding factors needed.\n        shift_dur = self.curr_seg[0] + int(self.job_time_shift * num_job)\n        job_valid_seg = self.valid_chunk.shift(shift_dur)\n\n        # If this is the last job, push the end back\n        if num_job == (self.num_jobs - 1):\n            dataPushBack = self.data_length - self.valid_chunk[1]\n            job_valid_seg = segments.segment(job_valid_seg[0],\n                                               self.curr_seg[1] - dataPushBack)\n\n        return job_valid_seg", "response": "Get the times for which the job num_job will be valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_data_times_for_job(self, num_job):\n        if self.compatibility_mode:\n            job_data_seg =  self.get_data_times_for_job_legacy(num_job)\n        else:\n            job_data_seg =  self.get_data_times_for_job_workflow(num_job)\n\n        # Sanity check that all data is used\n        if num_job == 0:\n            if job_data_seg[0] != self.curr_seg[0]:\n                err= \"Job is not using data from the start of the \"\n                err += \"science segment. It should be using all data.\"\n                raise ValueError(err)\n        if num_job == (self.num_jobs - 1):\n            if job_data_seg[1] != self.curr_seg[1]:\n                err = \"Job is not using data from the end of the \"\n                err += \"science segment. It should be using all data.\"\n                raise ValueError(err)\n\n        if hasattr(self.exe_class, 'zero_pad_data_extend'):\n            job_data_seg = self.exe_class.zero_pad_data_extend(job_data_seg,\n                                                                 self.curr_seg)\n\n        return job_data_seg", "response": "Get the data that this job will read in."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_data_times_for_job_workflow(self, num_job):\n        # small factor of 0.0001 to avoid float round offs causing us to\n        # miss a second at end of segments.\n        shift_dur = self.curr_seg[0] + int(self.job_time_shift * num_job\\\n                                           + 0.0001)\n        job_data_seg = self.data_chunk.shift(shift_dur)\n        return job_data_seg", "response": "Get the data that this job will need to read in."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_data_times_for_job_legacy(self, num_job):\n        # Should all be integers, so no rounding needed\n        shift_dur = self.curr_seg[0] + int(self.job_time_shift * num_job)\n        job_data_seg = self.data_chunk.shift(shift_dur)\n\n        # If this is the last job, push the end back\n        if num_job == (self.num_jobs - 1):\n            dataPushBack = job_data_seg[1] - self.curr_seg[1]\n            assert dataPushBack >= 0\n            job_data_seg = segments.segment(job_data_seg[0] - dataPushBack,\n                                                 self.curr_seg[1])\n            assert (abs(job_data_seg) == self.data_length)\n        return job_data_seg", "response": "Get the data that this job needs to read in."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndetermining possible dimensions of needed input and valid output and valid output.", "response": "def get_valid_times(self):\n        \"\"\" Determine possible dimensions of needed input and valid output\n        \"\"\"\n\n        if self.cp.has_option('workflow-matchedfilter',\n                                                      'min-analysis-segments'):\n            min_analysis_segs = int(self.cp.get('workflow-matchedfilter',\n                                                'min-analysis-segments'))\n        else:\n            min_analysis_segs = 0\n\n        if self.cp.has_option('workflow-matchedfilter',\n                                                      'max-analysis-segments'):\n            max_analysis_segs = int(self.cp.get('workflow-matchedfilter',\n                                                'max-analysis-segments'))\n        else:\n            # Choose ridiculously large default value\n            max_analysis_segs = 1000\n\n        if self.cp.has_option('workflow-matchedfilter', 'min-analysis-length'):\n            min_analysis_length = int(self.cp.get('workflow-matchedfilter',\n                                                  'min-analysis-length'))\n        else:\n            min_analysis_length = 0\n\n        if self.cp.has_option('workflow-matchedfilter', 'max-analysis-length'):\n            max_analysis_length = int(self.cp.get('workflow-matchedfilter',\n                                                  'max-analysis-length'))\n        else:\n            # Choose a ridiculously large default value\n            max_analysis_length = 100000\n\n        segment_length = int(self.get_opt('segment-length'))\n        pad_data = 0\n        if self.has_opt('pad-data'):\n            pad_data += int(self.get_opt('pad-data'))\n\n        # NOTE: Currently the tapered data is ignored as it is short and\n        #       will lie within the segment start/end pad. This means that\n        #       the tapered data *will* be used for PSD estimation (but this\n        #       effect should be small). It will also be in the data segments\n        #       used for SNR generation (when in the middle of a data segment\n        #       where zero-padding is not being used) but the templates should\n        #       not be long enough to use this data assuming segment start/end\n        #       pad take normal values. When using zero-padding this data will\n        #       be used for SNR generation.\n\n        #if self.has_opt('taper-data'):\n        #    pad_data += int(self.get_opt( 'taper-data' ))\n        if self.has_opt('allow-zero-padding'):\n            self.zero_padding=True\n        else:\n            self.zero_padding=False\n\n        start_pad = int(self.get_opt( 'segment-start-pad'))\n        end_pad = int(self.get_opt('segment-end-pad'))\n\n        seg_ranges = range(min_analysis_segs, max_analysis_segs + 1)\n        data_lengths = []\n        valid_regions = []\n        for nsegs in seg_ranges:\n            analysis_length = (segment_length - start_pad - end_pad) * nsegs\n            if not self.zero_padding:\n                data_length = analysis_length + pad_data * 2 \\\n                              + start_pad + end_pad\n                start = pad_data + start_pad\n                end = data_length - pad_data - end_pad\n            else:\n                data_length = analysis_length + pad_data * 2\n                start = pad_data\n                end = data_length - pad_data\n            if data_length > max_analysis_length: continue\n            if data_length < min_analysis_length: continue\n            data_lengths += [data_length]\n            valid_regions += [segments.segment(start, end)]\n        # If min_analysis_length is given, ensure that it is added as an option\n        # for job analysis length.\n        if min_analysis_length:\n            data_length = min_analysis_length\n            if not self.zero_padding:\n                start = pad_data + start_pad\n                end = data_length - pad_data - end_pad\n            else:\n                start = pad_data\n                end = data_length - pad_data\n            if end > start:\n                data_lengths += [data_length]\n                valid_regions += [segments.segment(start, end)]\n\n        return data_lengths, valid_regions"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a CondorDagmanNode class to run the job.", "response": "def create_node(self, bank, tags=None):\n        \"\"\"\n        Set up a CondorDagmanNode class to run splitbank code\n\n        Parameters\n        ----------\n        bank : pycbc.workflow.core.File\n            The File containing the template bank to be split\n\n        Returns\n        --------\n        node : pycbc.workflow.core.Node\n            The node to run the job\n        \"\"\"\n        if tags is None:\n            tags = []\n        node = Node(self)\n        node.add_input_opt('--bank-file', bank)\n\n        # Get the output (taken from inspiral.py)\n        out_files = FileList([])\n        for i in range( 0, self.num_banks):\n            curr_tag = 'bank%d' %(i)\n            # FIXME: What should the tags actually be? The job.tags values are\n            #        currently ignored.\n            curr_tags = bank.tags + [curr_tag] + tags\n            job_tag = bank.description + \"_\" + self.name.upper()\n            out_file = File(bank.ifo_list, job_tag, bank.segment,\n                            extension=self.extension, directory=self.out_dir,\n                            tags=curr_tags, store_file=self.retain_files)\n            out_files.append(out_file)\n        node.add_output_list_opt('--output-filenames', out_files)\n        return node"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_node(self, config_file=None, seed=None, tags=None):\n\n        # default for tags is empty list\n        tags = [] if tags is None else tags\n\n        # get analysis start and end time\n        start_time = self.cp.get(\"workflow\", \"start-time\")\n        end_time = self.cp.get(\"workflow\", \"end-time\")\n        analysis_time = segments.segment(int(start_time), int(end_time))\n\n        # make node for running executable\n        node = Node(self)\n        node.add_input_opt(\"--config-file\", config_file)\n        if seed:\n            node.add_opt(\"--seed\", seed)\n        injection_file = node.new_output_file_opt(analysis_time,\n                                                  \".hdf\", \"--output-file\",\n                                                  tags=tags)\n\n        return node, injection_file", "response": "Create a CondorDagmanNode class to run the injections."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_node(self, channel_names, config_file, injection_file=None,\n                    seed=None, fake_strain_seed=None, tags=None):\n        \"\"\" Set up a CondorDagmanNode class to run ``pycbc_inference``.\n\n        Parameters\n        ----------\n        channel_names : dict\n            A ``dict`` of ``str`` to use for ``--channel-name`` option.\n        config_file : pycbc.workflow.core.File\n            A ``pycbc.workflow.core.File`` for inference configuration file\n            to be used with ``--config-files`` option.\n        injection_file : pycbc.workflow.core.File\n            A ``pycbc.workflow.core.File`` for injection file to be used\n            with ``--injection-file`` option.\n        seed : int\n            An ``int`` to be used with ``--seed`` option.\n        fake_strain_seed : dict\n            An ``int`` to be used with ``--fake-strain-seed`` option.\n        tags : list\n            A list of tags to include in filenames.\n\n        Returns\n        --------\n        node : pycbc.workflow.core.Node\n            The node to run the job.\n        \"\"\"\n\n        # default for tags is empty list\n        tags = [] if tags is None else tags\n\n        # get analysis start and end time\n        start_time = self.cp.get(\"workflow\", \"start-time\")\n        end_time = self.cp.get(\"workflow\", \"end-time\")\n        analysis_time = segments.segment(int(start_time), int(end_time))\n\n        # get multi-IFO opts\n        channel_names_opt = \" \".join([\"{}:{}\".format(k, v)\n                                      for k, v in channel_names.iteritems()])\n        if fake_strain_seed is not None:\n            fake_strain_seed_opt = \" \".join([\n                                    \"{}:{}\".format(k, v)\n                                    for k, v in fake_strain_seed.iteritems()])\n\n        # make node for running executable\n        node = Node(self)\n        node.add_opt(\"--instruments\", \" \".join(self.ifo_list))\n        node.add_opt(\"--gps-start-time\", start_time)\n        node.add_opt(\"--gps-end-time\", end_time)\n        node.add_opt(\"--channel-name\", channel_names_opt)\n        node.add_input_opt(\"--config-file\", config_file)\n        if fake_strain_seed is not None:\n            node.add_opt(\"--fake-strain-seed\", fake_strain_seed_opt)\n        if injection_file:\n            node.add_input_opt(\"--injection-file\", injection_file)\n        if seed:\n            node.add_opt(\"--seed\", seed)\n        inference_file = node.new_output_file_opt(analysis_time,\n                                                  \".hdf\", \"--output-file\",\n                                                  tags=tags)\n\n        if self.cp.has_option(\"pegasus_profile-inference\",\n                              \"condor|+CheckpointSig\"):\n            ckpt_file_name = \"{}.checkpoint\".format(inference_file.name)\n            ckpt_file = dax.File(ckpt_file_name)\n            node._dax_node.uses(ckpt_file, link=dax.Link.OUTPUT,\n                                register=False, transfer=False)\n\n        return node, inference_file", "response": "Create a CondorDagmanNode class to run the pycbc_inference."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ensurearray(*args):\n    input_is_array = any(isinstance(arg, numpy.ndarray) for arg in args)\n    args = numpy.broadcast_arrays(*args)\n    args.append(input_is_array)\n    return args", "response": "Apply numpy s broadcast rules to the given arguments."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef formatreturn(arg, input_is_array=False):\n    if not input_is_array and arg.size == 1:\n        arg = arg.item()\n    return arg", "response": "Returns the value of the n - item in the given argument."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef primary_mass(mass1, mass2):\n    mass1, mass2, input_is_array = ensurearray(mass1, mass2)\n    mp = copy.copy(mass1)\n    mask = mass1 < mass2\n    mp[mask] = mass2[mask]\n    return formatreturn(mp, input_is_array)", "response": "Returns the larger of mass1 and mass2."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the smaller of mass1 and mass2.", "response": "def secondary_mass(mass1, mass2):\n    \"\"\"Returns the smaller of mass1 and mass2 (s = secondary).\"\"\"\n    mass1, mass2, input_is_array = ensurearray(mass1, mass2)\n    if mass1.shape != mass2.shape:\n        raise ValueError(\"mass1 and mass2 must have same shape\")\n    ms = copy.copy(mass2)\n    mask = mass1 < mass2\n    ms[mask] = mass1[mask]\n    return formatreturn(ms, input_is_array)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef mass1_from_mchirp_eta(mchirp, eta):\n    mtotal = mtotal_from_mchirp_eta(mchirp, eta)\n    return mass1_from_mtotal_eta(mtotal, eta)", "response": "Returns the primary mass from the chirp mass and symmetric mass ratio."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mass2_from_mchirp_eta(mchirp, eta):\n    mtotal = mtotal_from_mchirp_eta(mchirp, eta)\n    return mass2_from_mtotal_eta(mtotal, eta)", "response": "Returns the primary mass from the chirp mass and symmetric mass ratio."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _mass2_from_mchirp_mass1(mchirp, mass1):\n    a = mchirp**5 / mass1**3\n    roots = numpy.roots([1,0,-a,-a*mass1])\n    # Find the real one\n    real_root = roots[(abs(roots - roots.real)).argmin()]\n    return real_root.real", "response": "r Returns the secondary mass from the chirp mass and primary mass."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef mass2_from_mass1_eta(mass1, eta, force_real=True):\n    return mass_from_knownmass_eta(mass1, eta, known_is_secondary=False,\n                                   force_real=force_real)", "response": "Returns the secondary mass from the primary mass and symmetric mass\n    ratio."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the primary mass from the secondary mass and symmetric mass ratio.", "response": "def mass1_from_mass2_eta(mass2, eta, force_real=True):\n    \"\"\"Returns the primary mass from the secondary mass and symmetric mass\n    ratio.\n    \"\"\"\n    return mass_from_knownmass_eta(mass2, eta, known_is_secondary=True,\n                                   force_real=force_real)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef tau0_from_mtotal_eta(mtotal, eta, f_lower):\n    # convert to seconds\n    mtotal = mtotal * lal.MTSUN_SI\n    # formulae from arxiv.org:0706.4437\n    return _a0(f_lower) / (mtotal**(5./3.) * eta)", "response": "r Returns \\ tau_0 from the total mass symmetric mass ratio and\n    the given frequency."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef tau3_from_mass1_mass2(mass1, mass2, f_lower):\n    mtotal = mass1 + mass2\n    eta = eta_from_mass1_mass2(mass1, mass2)\n    return tau3_from_mtotal_eta(mtotal, eta, f_lower)", "response": "r Returns \\ tau_3 from the component masses and given frequency."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef mass2_from_tau0_tau3(tau0, tau3, f_lower):\n    mtotal = mtotal_from_tau0_tau3(tau0, tau3, f_lower)\n    eta = eta_from_tau0_tau3(tau0, tau3, f_lower)\n    return mass2_from_mtotal_eta(mtotal, eta)", "response": "r Returns the secondary mass from the given tau0 and tau3."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the effective spin from mass1 mass2 spin1z and spin2z.", "response": "def chi_eff(mass1, mass2, spin1z, spin2z):\n    \"\"\"Returns the effective spin from mass1, mass2, spin1z, and spin2z.\"\"\"\n    return (spin1z * mass1 + spin2z * mass2) / (mass1 + mass2)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the aligned mass - weighted spin difference from mass1 mass2 spin1z and spin2z.", "response": "def chi_a(mass1, mass2, spin1z, spin2z):\n    \"\"\" Returns the aligned mass-weighted spin difference from mass1, mass2,\n    spin1z, and spin2z.\n    \"\"\"\n    return (spin2z * mass2 - spin1z * mass1) / (mass2 + mass1)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the effective precession spin from mass1 mass2 spin1x and spin1y spin2x and spin2y.", "response": "def chi_p(mass1, mass2, spin1x, spin1y, spin2x, spin2y):\n    \"\"\"Returns the effective precession spin from mass1, mass2, spin1x,\n    spin1y, spin2x, and spin2y.\n    \"\"\"\n    xi1 = secondary_xi(mass1, mass2, spin1x, spin1y, spin2x, spin2y)\n    xi2 = primary_xi(mass1, mass2, spin1x, spin1y, spin2x, spin2y)\n    return chi_p_from_xi1_xi2(xi1, xi2)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the angle between the in - plane perpendicular spins.", "response": "def phi_a(mass1, mass2, spin1x, spin1y, spin2x, spin2y):\n    \"\"\" Returns the angle between the in-plane perpendicular spins.\"\"\"\n    phi1 = phi_from_spinx_spiny(primary_spin(mass1, mass2, spin1x, spin2x),\n                                primary_spin(mass1, mass2, spin1y, spin2y))\n    phi2 = phi_from_spinx_spiny(secondary_spin(mass1, mass2, spin1x, spin2x),\n                                secondary_spin(mass1, mass2, spin1y, spin2y))\n    return (phi1 - phi2) % (2 * numpy.pi)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef phi_s(spin1x, spin1y, spin2x, spin2y):\n    phi1 = phi_from_spinx_spiny(spin1x, spin1y)\n    phi2 = phi_from_spinx_spiny(spin2x, spin2y)\n    return (phi1 + phi2) % (2 * numpy.pi)", "response": "Returns the sum of the in - plane perpendicular spins."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the effective spin using spins in spherical coordinates.", "response": "def chi_eff_from_spherical(mass1, mass2, spin1_a, spin1_polar,\n                           spin2_a, spin2_polar):\n    \"\"\"Returns the effective spin using spins in spherical coordinates.\"\"\"\n    spin1z = spin1_a * numpy.cos(spin1_polar)\n    spin2z = spin2_a * numpy.cos(spin2_polar)\n    return chi_eff(mass1, mass2, spin1z, spin2z)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the effective precession spin using spins in spherical coordinates.", "response": "def chi_p_from_spherical(mass1, mass2, spin1_a, spin1_azimuthal, spin1_polar,\n                         spin2_a, spin2_azimuthal, spin2_polar):\n    \"\"\"Returns the effective precession spin using spins in spherical\n    coordinates.\n    \"\"\"\n    spin1x, spin1y, _ = _spherical_to_cartesian(\n        spin1_a, spin1_azimuthal, spin1_polar)\n    spin2x, spin2y, _ = _spherical_to_cartesian(\n        spin2_a, spin2_azimuthal, spin2_polar)\n    return chi_p(mass1, mass2, spin1x, spin1y, spin2x, spin2y)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef primary_spin(mass1, mass2, spin1, spin2):\n    mass1, mass2, spin1, spin2, input_is_array = ensurearray(\n        mass1, mass2, spin1, spin2)\n    sp = copy.copy(spin1)\n    mask = mass1 < mass2\n    sp[mask] = spin2[mask]\n    return formatreturn(sp, input_is_array)", "response": "Returns the dimensionless spin of the primary mass."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef secondary_spin(mass1, mass2, spin1, spin2):\n    mass1, mass2, spin1, spin2, input_is_array = ensurearray(\n        mass1, mass2, spin1, spin2)\n    ss = copy.copy(spin2)\n    mask = mass1 < mass2\n    ss[mask] = spin1[mask]\n    return formatreturn(ss, input_is_array)", "response": "Returns the dimensionless spin of the secondary mass."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the effective precession spin argument for the larger mass.", "response": "def primary_xi(mass1, mass2, spin1x, spin1y, spin2x, spin2y):\n    \"\"\"Returns the effective precession spin argument for the larger mass.\n    \"\"\"\n    spinx = primary_spin(mass1, mass2, spin1x, spin2x)\n    spiny = primary_spin(mass1, mass2, spin1y, spin2y)\n    return chi_perp_from_spinx_spiny(spinx, spiny)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the effective precession spin argument for the smaller mass.", "response": "def secondary_xi(mass1, mass2, spin1x, spin1y, spin2x, spin2y):\n    \"\"\"Returns the effective precession spin argument for the smaller mass.\n    \"\"\"\n    spinx = secondary_spin(mass1, mass2, spin1x, spin2x)\n    spiny = secondary_spin(mass1, mass2, spin1y, spin2y)\n    return xi2_from_mass1_mass2_spin2x_spin2y(mass1, mass2, spinx, spiny)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef xi2_from_mass1_mass2_spin2x_spin2y(mass1, mass2, spin2x, spin2y):\n    q = q_from_mass1_mass2(mass1, mass2)\n    a1 = 2 + 3 * q / 2\n    a2 = 2 + 3 / (2 * q)\n    return a1 / (q**2 * a2) * chi_perp_from_spinx_spiny(spin2x, spin2y)", "response": "Returns the effective precession spin argument for the smaller mass."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the in - plane spin from mass1 mass2 and xi2 for the secondary mass.", "response": "def chi_perp_from_mass1_mass2_xi2(mass1, mass2, xi2):\n    \"\"\"Returns the in-plane spin from mass1, mass2, and xi2 for the\n    secondary mass.\n    \"\"\"\n    q = q_from_mass1_mass2(mass1, mass2)\n    a1 = 2 + 3 * q / 2\n    a2 = 2 + 3 / (2 * q)\n    return q**2 * a2 / a1 * xi2"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef chi_p_from_xi1_xi2(xi1, xi2):\n    xi1, xi2, input_is_array = ensurearray(xi1, xi2)\n    chi_p = copy.copy(xi1)\n    mask = xi1 < xi2\n    chi_p[mask] = xi2[mask]\n    return formatreturn(chi_p, input_is_array)", "response": "Returns effective precession spin from xi1 and xi2."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef phi_from_spinx_spiny(spinx, spiny):\n    phi = numpy.arctan2(spiny, spinx)\n    return phi % (2 * numpy.pi)", "response": "Returns the angle between the x - component axis and the in - plane spin."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef spin1x_from_xi1_phi_a_phi_s(xi1, phi_a, phi_s):\n    phi1 = phi1_from_phi_a_phi_s(phi_a, phi_s)\n    return xi1 * numpy.cos(phi1)", "response": "Returns x - component spin for primary mass."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef spin1y_from_xi1_phi_a_phi_s(xi1, phi_a, phi_s):\n    phi1 = phi1_from_phi_a_phi_s(phi_s, phi_a)\n    return xi1 * numpy.sin(phi1)", "response": "Returns y - component spin for primary mass."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef spin2x_from_mass1_mass2_xi2_phi_a_phi_s(mass1, mass2, xi2, phi_a, phi_s):\n    chi_perp = chi_perp_from_mass1_mass2_xi2(mass1, mass2, xi2)\n    phi2 = phi2_from_phi_a_phi_s(phi_a, phi_s)\n    return chi_perp * numpy.cos(phi2)", "response": "Returns x - component spin for secondary mass."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef spin2y_from_mass1_mass2_xi2_phi_a_phi_s(mass1, mass2, xi2, phi_a, phi_s):\n    chi_perp = chi_perp_from_mass1_mass2_xi2(mass1, mass2, xi2)\n    phi2 = phi2_from_phi_a_phi_s(phi_a, phi_s)\n    return chi_perp * numpy.sin(phi2)", "response": "Returns y - component spin for secondary mass."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the time of a signal in a given detector.", "response": "def _det_tc(detector_name, ra, dec, tc, ref_frame='geocentric'):\n    \"\"\"Returns the coalescence time of a signal in the given detector.\n\n    Parameters\n    ----------\n    detector_name : string\n        The name of the detector, e.g., 'H1'.\n    ra : float\n        The right ascension of the signal, in radians.\n    dec : float\n        The declination of the signal, in radians.\n    tc : float\n        The GPS time of the coalescence of the signal in the `ref_frame`.\n    ref_frame : {'geocentric', string}\n        The reference frame that the given coalescence time is defined in.\n        May specify 'geocentric', or a detector name; default is 'geocentric'.\n\n    Returns\n    -------\n    float :\n        The GPS time of the coalescence in detector `detector_name`.\n    \"\"\"\n    if ref_frame == detector_name:\n        return tc\n    detector = Detector(detector_name)\n    if ref_frame == 'geocentric':\n        return tc + detector.time_delay_from_earth_center(ra, dec, tc)\n    else:\n        other = Detector(ref_frame)\n        return tc + detector.time_delay_from_detector(other, ra, dec, tc)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef snr_from_loglr(loglr):\n    singleval = isinstance(loglr, float)\n    if singleval:\n        loglr = numpy.array([loglr])\n    # temporarily quiet sqrt(-1) warnings\n    numpysettings = numpy.seterr(invalid='ignore')\n    snrs = numpy.sqrt(2*loglr)\n    numpy.seterr(**numpysettings)\n    snrs[numpy.isnan(snrs)] = 0.\n    if singleval:\n        snrs = snrs[0]\n    return snrs", "response": "Returns the SNR computed from the given log likelihood ratio. This is a special case for the log likelihood ratio."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the f0 and tau of each overtone for a given l m mode.", "response": "def get_lm_f0tau(mass, spin, l, m, nmodes):\n    \"\"\"Return the f0 and the tau of each overtone for a given l, m mode.\n\n    Parameters\n    ----------\n    mass : float or array\n        Mass of the black hole (in solar masses).\n    spin : float or array\n        Dimensionless spin of the final black hole.\n    l : int or array\n        l-index of the harmonic.\n    m : int or array\n        m-index of the harmonic.\n    nmodes : int\n        The number of overtones to generate.\n\n    Returns\n    -------\n    f0 : float or array\n        The frequency of the QNM(s), in Hz. If only a single mode is requested\n        (and mass, spin, l, and m are not arrays), this will be a float. If\n        multiple modes requested, will be an array with shape\n        ``[input shape x] nmodes``, where ``input shape`` is the broadcasted\n        shape of the inputs.\n    tau : float or array\n        The damping time of the QNM(s), in seconds. Return type is same as f0.\n    \"\"\"\n    # convert to arrays\n    mass, spin, l, m, input_is_array = ensurearray(\n        mass, spin, l, m)\n    # we'll ravel the arrays so we can evaluate each parameter combination\n    # one at a a time\n    origshape = mass.shape\n    if nmodes < 1:\n        raise ValueError(\"nmodes must be >= 1\")\n    if nmodes > 1:\n        newshape = tuple(list(origshape)+[nmodes])\n    else:\n        newshape = origshape\n    f0s = numpy.zeros((mass.size, nmodes))\n    taus = numpy.zeros((mass.size, nmodes))\n    mass = mass.ravel()\n    spin = spin.ravel()\n    l = l.ravel()\n    m = m.ravel()\n    qnmfreq = None\n    modes = range(nmodes)\n    for ii in range(mass.size):\n        qnmfreq = _genqnmfreq(mass[ii], spin[ii], l[ii], m[ii], nmodes,\n                              qnmfreq=qnmfreq)\n        f0s[ii, :] = [qnmfreq.data[n].real/(2 * numpy.pi) for n in modes]\n        taus[ii, :] = [1./qnmfreq.data[n].imag for n in modes]\n    f0s = f0s.reshape(newshape)\n    taus = taus.reshape(newshape)\n    return (formatreturn(f0s, input_is_array),\n            formatreturn(taus, input_is_array))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a dictionary of all of the frequencies and damping times for the requested modes.", "response": "def get_lm_f0tau_allmodes(mass, spin, modes):\n    \"\"\"Returns a dictionary of all of the frequencies and damping times for the\n    requested modes.\n\n    Parameters\n    ----------\n    mass : float or array\n        Mass of the black hole (in solar masses).\n    spin : float or array\n        Dimensionless spin of the final black hole.\n    modes : list of str\n        The modes to get. Each string in the list should be formatted 'lmN',\n        where l (m) is the l (m) index of the harmonic and N is the number of\n        overtones to generate (note, N is not the index of the overtone). For\n        example, '221' will generate the 0th overtone of the l = m = 2 mode.\n\n    Returns\n    -------\n    f0 : dict\n        Dictionary mapping the modes to the frequencies. The dictionary keys\n        are 'lmn' string, where l (m) is the l (m) index of the harmonic and\n        n is the index of the overtone. For example, '220' is the l = m = 2\n        mode and the 0th overtone.\n    tau : dict\n        Dictionary mapping the modes to the damping times. The keys are the\n        same as ``f0``.\n    \"\"\"\n    f0, tau = {}, {}\n    key = '{}{}{}'\n    for lmn in modes:\n        l, m, nmodes = int(lmn[0]), int(lmn[1]), int(lmn[2])\n        tmp_f0, tmp_tau = get_lm_f0tau(mass, spin, l, m, nmodes)\n        if nmodes == 1:\n            # in this case, tmp_f0 and tmp_tau will just be floats\n            f0[key.format(l, m, '0')] = tmp_f0\n            tau[key.format(l, m, '0')] = tmp_tau\n        else:\n            for n in range(nmodes):\n                # we need to wrap tmp_f0 with formatreturn to ensure that if\n                # only a mass, spin pair was requested, the value stored to\n                # the dict is a float\n                f0[key.format(l, m, n)] = formatreturn(tmp_f0[..., n])\n                tau[key.format(l, m, n)] = formatreturn(tmp_tau[..., n])\n    return f0, tau"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef freq_from_final_mass_spin(final_mass, final_spin, l=2, m=2, nmodes=1):\n    return get_lm_f0tau(final_mass, final_spin, l, m, nmodes)[0]", "response": "Returns the QNM frequency for the given mass and spin and mode."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the QNM damping time for the given mass and spin and mode.", "response": "def tau_from_final_mass_spin(final_mass, final_spin, l=2, m=2, nmodes=1):\n    \"\"\"Returns QNM damping time for the given mass and spin and mode.\n\n    Parameters\n    ----------\n    final_mass : float or array\n        Mass of the black hole (in solar masses).\n    final_spin : float or array\n        Dimensionless spin of the final black hole.\n    l : int or array, optional\n        l-index of the harmonic. Default is 2.\n    m : int or array, optional\n        m-index of the harmonic. Default is 2.\n    nmodes : int, optional\n        The number of overtones to generate. Default is 1.\n\n    Returns\n    -------\n    float or array\n        The damping time of the QNM(s), in seconds. If only a single mode is\n        requested (and mass, spin, l, and m are not arrays), this will be a\n        float. If multiple modes requested, will be an array with shape\n        ``[input shape x] nmodes``, where ``input shape`` is the broadcasted\n        shape of the inputs.\n    \"\"\"\n    return get_lm_f0tau(final_mass, final_spin, l, m, nmodes)[1]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef final_spin_from_f0_tau(f0, tau, l=2, m=2):\n    f0, tau, input_is_array = ensurearray(f0, tau)\n    # from Berti et al. 2006\n    a, b, c = _berti_spin_constants[l,m]\n    origshape = f0.shape\n    # flatten inputs for storing results\n    f0 = f0.ravel()\n    tau = tau.ravel()\n    spins = numpy.zeros(f0.size)\n    for ii in range(spins.size):\n        Q = f0[ii] * tau[ii] * numpy.pi\n        try:\n            s = 1. - ((Q-a)/b)**(1./c)\n        except ValueError:\n            s = numpy.nan\n        spins[ii] = s\n    spins = spins.reshape(origshape)\n    return formatreturn(spins, input_is_array)", "response": "Returns the final spin based on the given frequency and damping time."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef final_mass_from_f0_tau(f0, tau, l=2, m=2):\n    # from Berti et al. 2006\n    spin = final_spin_from_f0_tau(f0, tau, l=l, m=m)\n    a, b, c = _berti_mass_constants[l,m]\n    return (a + b*(1-spin)**c)/(2*numpy.pi*f0*lal.MTSUN_SI)", "response": "Returns the final mass based on the given frequency and damping time."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nestimates the final mass and spin from the given initial parameters.", "response": "def get_final_from_initial(mass1, mass2, spin1x=0., spin1y=0., spin1z=0.,\n                           spin2x=0., spin2y=0., spin2z=0.,\n                           approximant='SEOBNRv4'):\n    \"\"\"Estimates the final mass and spin from the given initial parameters.\n\n    This uses the fits used by the EOBNR models for converting from initial\n    parameters to final. Which version used can be controlled by the\n    ``approximant`` argument.\n\n    Parameters\n    ----------\n    mass1 : float\n        The mass of one of the components, in solar masses.\n    mass2 : float\n        The mass of the other component, in solar masses.\n    spin1x : float, optional\n        The dimensionless x-component of the spin of mass1. Default is 0.\n    spin1y : float, optional\n        The dimensionless y-component of the spin of mass1. Default is 0.\n    spin1z : float, optional\n        The dimensionless z-component of the spin of mass1. Default is 0.\n    spin2x : float, optional\n        The dimensionless x-component of the spin of mass2. Default is 0.\n    spin2y : float, optional\n        The dimensionless y-component of the spin of mass2. Default is 0.\n    spin2z : float, optional\n        The dimensionless z-component of the spin of mass2. Default is 0.\n    approximant : str, optional\n        The waveform approximant to use for the fit function. Default is\n        \"SEOBNRv4\".\n\n    Returns\n    -------\n    final_mass : float\n        The final mass, in solar masses.\n    final_spin : float\n        The dimensionless final spin.\n    \"\"\"\n    args = (mass1, mass2, spin1x, spin1y, spin1z, spin2x, spin2y, spin2z)\n    args = ensurearray(*args)\n    input_is_array = args[-1]\n    origshape = args[0].shape\n    # flatten inputs for storing results\n    args = [a.ravel() for a in args[:-1]]\n    mass1, mass2, spin1x, spin1y, spin1z, spin2x, spin2y, spin2z = args\n    final_mass = numpy.zeros(mass1.shape)\n    final_spin = numpy.zeros(mass1.shape)\n    for ii in range(final_mass.size):\n        m1 = mass1[ii]\n        m2 = mass2[ii]\n        spin1 = [spin1x[ii], spin1y[ii], spin1z[ii]]\n        spin2 = [spin2x[ii], spin2y[ii], spin2z[ii]]\n        _, fm, fs = lalsim.SimIMREOBFinalMassSpin(m1, m2, spin1, spin2,\n                                                  getattr(lalsim, approximant))\n        final_mass[ii] = fm * (m1 + m2)\n        final_spin[ii] = fs\n    final_mass = final_mass.reshape(origshape)\n    final_spin = final_spin.reshape(origshape)\n    return (formatreturn(final_mass, input_is_array),\n            formatreturn(final_spin, input_is_array))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef final_mass_from_initial(mass1, mass2, spin1x=0., spin1y=0., spin1z=0.,\n                            spin2x=0., spin2y=0., spin2z=0.,\n                            approximant='SEOBNRv4'):\n    \"\"\"Estimates the final mass from the given initial parameters.\n\n    This uses the fits used by the EOBNR models for converting from initial\n    parameters to final. Which version used can be controlled by the\n    ``approximant`` argument.\n\n    Parameters\n    ----------\n    mass1 : float\n        The mass of one of the components, in solar masses.\n    mass2 : float\n        The mass of the other component, in solar masses.\n    spin1x : float, optional\n        The dimensionless x-component of the spin of mass1. Default is 0.\n    spin1y : float, optional\n        The dimensionless y-component of the spin of mass1. Default is 0.\n    spin1z : float, optional\n        The dimensionless z-component of the spin of mass1. Default is 0.\n    spin2x : float, optional\n        The dimensionless x-component of the spin of mass2. Default is 0.\n    spin2y : float, optional\n        The dimensionless y-component of the spin of mass2. Default is 0.\n    spin2z : float, optional\n        The dimensionless z-component of the spin of mass2. Default is 0.\n    approximant : str, optional\n        The waveform approximant to use for the fit function. Default is\n        \"SEOBNRv4\".\n\n    Returns\n    -------\n    float\n        The final mass, in solar masses.\n    \"\"\"\n    return get_final_from_initial(mass1, mass2, spin1x, spin1y, spin1z,\n                                  spin2x, spin2y, spin2z, approximant)[0]", "response": "Estimates the final mass from the given initial parameters."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the coefficients needed to compute the nltides of the current language.", "response": "def nltides_coefs(amplitude, n, m1, m2):\n    \"\"\"Calculate the coefficents needed to compute the\n    shift in t(f) and phi(f) due to non-linear tides.\n\n    Parameters\n    ----------\n    amplitude: float\n        Amplitude of effect\n    n: float\n        Growth dependence of effect\n    m1: float\n        Mass of component 1\n    m2: float\n        Mass of component 2\n\n    Returns\n    -------\n    f_ref : float\n        Reference frequency used to define A and n\n    t_of_f_factor: float\n        The constant factor needed to compute t(f)\n    phi_of_f_factor: float\n        The constant factor needed to compute phi(f)\n    \"\"\"\n\n    # Use 100.0 Hz as a reference frequency\n    f_ref = 100.0\n\n    # Calculate chirp mass\n    mc = mchirp_from_mass1_mass2(m1, m2)\n    mc *= lal.lal.MSUN_SI\n\n    # Calculate constants in phasing\n    a = (96./5.) * \\\n        (lal.lal.G_SI * lal.lal.PI * mc * f_ref / lal.lal.C_SI**3.)**(5./3.)\n    b = 6. * amplitude\n    t_of_f_factor = -1./(lal.lal.PI*f_ref) * b/(a*a * (n-4.))\n    phi_of_f_factor = -2.*b / (a*a * (n-3.))\n\n    return f_ref, t_of_f_factor, phi_of_f_factor"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef nltides_gw_phase_difference(f, f0, amplitude, n, m1, m2):\n    f, f0, amplitude, n, m1, m2, input_is_array = ensurearray(\n        f, f0, amplitude, n, m1, m2)\n\n    delta_phi = numpy.zeros(m1.shape)\n\n    f_ref, _, phi_of_f_factor = nltides_coefs(amplitude, n, m1, m2)\n\n    mask = f <= f0\n    delta_phi[mask] = - phi_of_f_factor[mask] * (f0[mask]/f_ref)**(n[mask]-3.)\n\n    mask = f > f0\n    delta_phi[mask] = - phi_of_f_factor[mask] * (f[mask]/f_ref)**(n[mask]-3.)\n\n    return formatreturn(delta_phi, input_is_array)", "response": "Compute the gravitational - wave phase shift between f0 and f_coalescence"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef nltides_gw_phase_diff_isco(f_low, f0, amplitude, n, m1, m2):\n    f0, amplitude, n, m1, m2, input_is_array = ensurearray(\n        f0, amplitude, n, m1, m2)\n\n    f_low = numpy.zeros(m1.shape) + f_low\n\n    phi_l = nltides_gw_phase_difference(\n                f_low, f0, amplitude, n, m1, m2)\n\n    f_isco = f_schwarzchild_isco(m1+m2)\n\n    phi_i = nltides_gw_phase_difference(\n                f_isco, f0, amplitude, n, m1, m2)\n\n    return formatreturn(phi_i - phi_l, input_is_array)", "response": "Calculates the gravitational - wave phase shift bwtween\n    f_low f_isco f0 amplitude n m1 and m2"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef model_stats(self):\n        stats = self.model.default_stats\n        return blob_data_to_dict(stats, self._sampler.blobs)", "response": "A dict mapping the model s default_stats to arrays of values."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_state_from_file(self, filename):\n        with self.io(filename, 'r') as fp:\n            rstate = fp.read_random_state()\n        # set the numpy random state\n        numpy.random.set_state(rstate)\n        # set emcee's generator to the same state\n        self._sampler.random_state = rstate", "response": "Sets the state of the sampler to the instance saved in a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_results(self, filename):\n        with self.io(filename, 'a') as fp:\n            # write samples\n            fp.write_samples(self.samples, self.model.variable_params,\n                             last_iteration=self.niterations)\n            # write stats\n            fp.write_samples(self.model_stats,\n                             last_iteration=self.niterations)\n            # write accpetance\n            fp.write_acceptance_fraction(self._sampler.acceptance_fraction)\n            # write random state\n            fp.write_random_state(state=self._sampler.random_state)", "response": "Writes the samples model stats acceptance fraction and random state to the given file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading the sampler from the given config file.", "response": "def from_config(cls, cp, model, nprocesses=1, use_mpi=False):\n        \"\"\"Loads the sampler from the given config file.\"\"\"\n        section = \"sampler\"\n        # check name\n        assert cp.get(section, \"name\") == cls.name, (\n            \"name in section [sampler] must match mine\")\n        # get the number of walkers to use\n        nwalkers = int(cp.get(section, \"nwalkers\"))\n        # get the checkpoint interval, if it's specified\n        checkpoint_interval = cls.checkpoint_from_config(cp, section)\n        checkpoint_signal = cls.ckpt_signal_from_config(cp, section)\n        # get the logpost function\n        lnpost = get_optional_arg_from_config(cp, section, 'logpost-function')\n        obj = cls(model, nwalkers,\n                  checkpoint_interval=checkpoint_interval,\n                  checkpoint_signal=checkpoint_signal,\n                  logpost_function=lnpost, nprocesses=nprocesses,\n                  use_mpi=use_mpi)\n        # set target\n        obj.set_target_from_config(cp, section)\n        # add burn-in if it's specified\n        obj.set_burn_in_from_config(cp)\n        # set prethin options\n        obj.set_thin_interval_from_config(cp, section)\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef lstring_as_obj(true_or_false=None):\n    if true_or_false is not None:\n        _default_types_status['lstring_as_obj'] = true_or_false\n        # update the typeDict\n        numpy.typeDict[u'lstring'] = numpy.object_ \\\n            if _default_types_status['lstring_as_obj'] \\\n            else 'S%i' % _default_types_status['default_strlen']\n    return _default_types_status['lstring_as_obj']", "response": "Toggles whether lstrings should be treated as strings or as objects."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ilwd_as_int(true_or_false=None):\n    if true_or_false is not None:\n        _default_types_status['ilwd_as_int'] = true_or_false\n        numpy.typeDict[u'ilwd:char'] = int \\\n            if _default_types_status['ilwd_as_int'] \\\n            else 'S%i' % default_strlen\n    return _default_types_status['ilwd_as_int']", "response": "Similar to lstring_as_obj sets whether or not ilwd : chars should be\n    treated as strings or as ints. Default is True."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef default_strlen(strlen=None):\n    if strlen is not None:\n        _default_types_status['default_strlen'] = strlen\n        # update the typeDicts as needed\n        lstring_as_obj(_default_types_status['lstring_as_obj'])\n        ilwd_as_int(_default_types_status['ilwd_as_int'])\n    return _default_types_status['default_strlen']", "response": "Sets the default string length for lstring and ilwd : char."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives a FieldArray-like array and a list of names, determines what fields are needed from the array so that using the names does not result in an error. Parameters ---------- arr : instance of a FieldArray or similar The array from which to determine what fields to get. names : (list of) strings A list of the names that are desired. The names may be either a field, a virtualfield, a property, a method of ``arr``, or any function of these. If a virtualfield/property or a method, the source code of that property/method will be analyzed to pull out what fields are used in it. Returns ------- set The set of the fields needed to evaluate the names.", "response": "def get_needed_fieldnames(arr, names):\n    \"\"\"Given a FieldArray-like array and a list of names, determines what\n    fields are needed from the array so that using the names does not result\n    in an error.\n\n    Parameters\n    ----------\n    arr : instance of a FieldArray or similar\n        The array from which to determine what fields to get.\n    names : (list of) strings\n        A list of the names that are desired. The names may be either a field,\n        a virtualfield, a property, a method of ``arr``, or any function of\n        these. If a virtualfield/property or a method, the source code of that\n        property/method will be analyzed to pull out what fields are used in\n        it.\n\n    Returns\n    -------\n    set\n        The set of the fields needed to evaluate the names.\n    \"\"\"\n    fieldnames = set([])\n    # we'll need the class that the array is an instance of to evaluate some\n    # things\n    cls = arr.__class__\n    if isinstance(names, string_types):\n        names = [names]\n    # parse names for variables, incase some of them are functions of fields\n    parsed_names = set([])\n    for name in names:\n        parsed_names.update(get_fields_from_arg(name))\n    # only include things that are in the array's namespace\n    names = list(parsed_names & (set(dir(arr)) | set(arr.fieldnames)))\n    for name in names:\n        if name in arr.fieldnames:\n            # is a field, just add the name\n            fieldnames.update([name])\n        else:\n            # the name is either a virtualfield, a method, or some other\n            # property; we need to evaluate the source code to figure out what\n            # fields we need\n            try:\n                # the underlying functions of properties need to be retrieved\n                # using their fget attribute\n                func = getattr(cls, name).fget\n            except AttributeError:\n                # no fget attribute, assume is an instance method\n                func = getattr(arr, name)\n            # evaluate the source code of the function\n            try:\n                sourcecode = inspect.getsource(func)\n            except TypeError:\n                # not a function, just pass\n                continue\n            # evaluate the source code for the fields\n            possible_fields = get_instance_fields_from_arg(sourcecode)\n            # some of the variables returned by possible fields may themselves\n            # be methods/properties that depend on other fields. For instance,\n            # mchirp relies on eta and mtotal, which each use mass1 and mass2;\n            # we therefore need to anayze each of the possible fields\n            fieldnames.update(get_needed_fieldnames(arr, possible_fields))\n    return fieldnames"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef combine_fields(dtypes):\n    if not isinstance(dtypes, list):\n        dtypes = [dtypes]\n    # Note: incase any of the dtypes have offsets, we won't include any fields\n    # that have no names and are void\n    new_dt = numpy.dtype([dt for dtype in dtypes \\\n        for dt in get_dtype_descr(dtype)])\n    return new_dt", "response": "Combines the fields in the list of given dtypes into a single dtype."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _ensure_array_list(arrays):\n    # Note: the isinstance test is needed below so that instances of FieldArray\n    # are not converted to numpy arrays\n    return [numpy.array(arr, ndmin=1) if not isinstance(arr, numpy.ndarray)\n            else arr for arr in arrays]", "response": "Ensures that every element in a list is an instance of a numpy array."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef merge_arrays(merge_list, names=None, flatten=True, outtype=None):\n    # make sure everything in merge_list is an array\n    merge_list = _ensure_array_list(merge_list)\n    if not all(merge_list[0].shape == arr.shape for arr in merge_list):\n        raise ValueError(\"all of the arrays in merge_list must have the \" +\n            \"same shape\")\n    if flatten:\n        new_dt = combine_fields([arr.dtype for arr in merge_list])\n    else:\n        new_dt = numpy.dtype([('f%i' %ii, arr.dtype.descr) \\\n            for ii,arr in enumerate(merge_list)])\n    new_arr = merge_list[0].__class__(merge_list[0].shape, dtype=new_dt)\n    # ii is a counter to keep track of which fields from the new array\n    # go with which arrays in merge list\n    ii = 0\n    for arr in merge_list:\n        if arr.dtype.names is None:\n            new_arr[new_dt.names[ii]] = arr\n            ii += 1\n        else:\n            for field in arr.dtype.names:\n                new_arr[field] = arr[field]\n                ii += 1\n    # set the names if desired\n    if names is not None:\n        new_arr.dtype.names = names\n    # ditto the outtype\n    if outtype is not None:\n        new_arr = new_arr.view(type=outtype)\n    return new_arr", "response": "Merges the given arrays into a single array."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds the given array or subarray to the given output array.", "response": "def add_fields(input_array, arrays, names=None, assubarray=False):\n    \"\"\"Adds the given array(s) as new field(s) to the given input array.\n    Returns a new instance of the input_array with the new fields added.\n\n    Parameters\n    ----------\n    input_array : instance of a numpy.ndarray or numpy recarray\n        The array to to add the fields to.\n    arrays : (list of) numpy array(s)\n        The arrays to add. If adding multiple arrays, must be a list;\n        if adding a single array, can just be that array.\n    names : (list of) strings\n        Optional, the name(s) of the new fields in the output array. If\n        adding multiple fields, must be a list of strings with the same\n        length as the list of arrays. If None provided, names used will\n        be the same as the name of the datatype in the given arrays.\n        If the datatype has no name, the new field will be ``'fi'`` where\n        i is the index of the array in arrays.\n    assubarray : bool\n        Add the list of arrays as a single subarray field. If True, and names\n        provided, names should be a string or a length-1 sequence. Default is\n        False, in which case each array will be added as a separate field.\n\n    Returns\n    -------\n    new_array : new instance of `input_array`\n        A copy of the `input_array` with the desired fields added.\n    \"\"\"\n    if not isinstance(arrays, list):\n        arrays = [arrays]\n    # ensure that all arrays in arrays are arrays\n    arrays = _ensure_array_list(arrays)\n    # set the names\n    if names is not None:\n        if isinstance(names, string_types):\n            names = [names]\n        # check if any names are subarray names; if so, we have to add them\n        # separately\n        subarray_names = [name for name in names if len(name.split('.')) > 1]\n    else:\n        subarray_names = []\n    if any(subarray_names):\n        subarrays = [arrays[ii] for ii,name in enumerate(names) \\\n            if name in subarray_names]\n        # group together by subarray\n        groups = {}\n        for name,arr in zip(subarray_names, subarrays):\n            key = name.split('.')[0]\n            subkey = '.'.join(name.split('.')[1:])\n            try:\n                groups[key].append((subkey, arr))\n            except KeyError:\n                groups[key] = [(subkey, arr)]\n        # now cycle over the groups, adding all of the fields in each group\n        # as a subarray\n        for group_name in groups:\n            # we'll create a dictionary out of the subarray field names ->\n            # subarrays\n            thisdict = dict(groups[group_name])\n            # check if the input array has this field; if so, remove it, then\n            # add it back with the other new arrays\n            if group_name in input_array.fieldnames:\n                # get the data\n                new_subarray = input_array[group_name]\n                # add the new fields to the subarray\n                new_subarray = add_fields(new_subarray, thisdict.values(),\n                    thisdict.keys())\n                # remove the original from the input array\n                input_array = input_array.without_fields(group_name)\n            else:\n                new_subarray = thisdict.values()\n            # add the new subarray to input_array as a subarray\n            input_array = add_fields(input_array, new_subarray,\n                names=group_name, assubarray=True)\n            # set the subarray names\n            input_array[group_name].dtype.names = thisdict.keys()\n        # remove the subarray names from names\n        keep_idx = [ii for ii,name in enumerate(names) \\\n            if name not in subarray_names]\n        names = [names[ii] for ii in keep_idx]\n        # if there's nothing left, just return\n        if names == []:\n            return input_array\n        # also remove the subarray arrays\n        arrays = [arrays[ii] for ii in keep_idx]\n    if assubarray:\n        # merge all of the arrays into a single array\n        if len(arrays) > 1:\n            arrays = [merge_arrays(arrays, flatten=True)]\n        # now merge all the fields as a single subarray\n        merged_arr = numpy.empty(len(arrays[0]),\n            dtype=[('f0', arrays[0].dtype.descr)])\n        merged_arr['f0'] = arrays[0]\n        arrays = [merged_arr]\n    merge_list = [input_array] + arrays\n    if names is not None:\n        names = list(input_array.dtype.names) + names\n    # merge into a single array\n    return merge_arrays(merge_list, names=names, flatten=True,\n        outtype=type(input_array))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _isstring(dtype):\n    return dtype.type == numpy.unicode_ or dtype.type == numpy.string_", "response": "Determines whether a numpy dtype is a string. Returns True\n    if the dtype is string or unicode."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives a dictionary of fields and a list of names will return a dictionary consisting of the fields specified by names.", "response": "def fields_from_names(fields, names=None):\n    \"\"\"Given a dictionary of fields and a list of names, will return a\n    dictionary consisting of the fields specified by names. Names can be\n    either the names of fields, or their aliases.\n    \"\"\"\n\n    if names is None:\n        return fields\n    if isinstance(names, string_types):\n        names = [names]\n    aliases_to_names = aliases_from_fields(fields)\n    names_to_aliases = dict(zip(aliases_to_names.values(),\n        aliases_to_names.keys()))\n    outfields = {}\n    for name in names:\n        try:\n            outfields[name] = fields[name]\n        except KeyError:\n            if name in aliases_to_names:\n                key = (name, aliases_to_names[name])\n            elif name in names_to_aliases:\n                key = (names_to_aliases[name], name)\n            else:\n                raise KeyError('default fields has no field %s' % name)\n            outfields[key] = fields[key]\n    return outfields"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sort(self, axis=-1, kind='quicksort', order=None):\n        try:\n            numpy.recarray.sort(self, axis=axis, kind=kind, order=order)\n        except ValueError:\n            if isinstance(order, list):\n                raise ValueError(\"Cannot process more than one order field\")\n            self[:] = self[numpy.argsort(self[order])]", "response": "Sort an array along a given axis."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef addattr(self, attrname, value=None, persistent=True):\n        setattr(self, attrname, value)\n        # add as persistent\n        if persistent and attrname not in self.__persistent_attributes__:\n            self.__persistent_attributes__.append(attrname)", "response": "Adds an attribute to the current object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_methods(self, names, methods):\n        if isinstance(names, string_types):\n            names = [names]\n            methods = [methods]\n        for name,method in zip(names, methods):\n            setattr(self, name, types.MethodType(method, self))", "response": "Adds the given methods as instance methods of self."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a view of self with the given methods added as properties.", "response": "def add_properties(self, names, methods):\n        \"\"\"Returns a view of self with the given methods added as properties.\n\n        From: <http://stackoverflow.com/a/2954373/1366472>.\n        \"\"\"\n        cls = type(self)\n        cls = type(cls.__name__, (cls,), dict(cls.__dict__))\n        if isinstance(names, string_types):\n            names = [names]\n            methods = [methods]\n        for name,method in zip(names, methods):\n            setattr(cls, name, property(method))\n        return self.view(type=cls)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a view of this array with the given methods added as virtual fields. Specifically the given names and methods are added to the list of virtual fields that are assumed to operate on one or more of the virtual fields.", "response": "def add_virtualfields(self, names, methods):\n        \"\"\"Returns a view of this array with the given methods added as virtual\n        fields. Specifically, the given methods are added using add_properties\n        and their names are added to the list of virtual fields. Virtual fields\n        are properties that are assumed to operate on one or more of self's\n        fields, thus returning an array of values.\n        \"\"\"\n        if isinstance(names, string_types):\n            names = [names]\n            methods = [methods]\n        out = self.add_properties(names, methods)\n        if out._virtualfields is None:\n            out._virtualfields = []\n        out._virtualfields.extend(names)\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding the given functions to the function library.", "response": "def add_functions(self, names, functions):\n        \"\"\"Adds the given functions to the function library.\n\n        Functions are added to this instance of the array; all copies of\n        and slices of this array will also have the new functions included.\n\n        Parameters\n        ----------\n        names : (list of) string(s)\n            Name or list of names of the functions.\n        functions : (list of) function(s)\n            The function(s) to call.\n        \"\"\"\n        if isinstance(names, string_types):\n            names = [names]\n            functions = [functions]\n        if len(functions) != len(names):\n            raise ValueError(\"number of provided names must be same as number \"\n                             \"of functions\")\n        self._functionlib.update(dict(zip(names, functions)))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef del_functions(self, names):\n        if isinstance(names, string_types):\n            names = [names]\n        for name in names:\n            self._functionlib.pop(name)", "response": "Removes the specified functions from the function library."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new instance of this class from the given numpy arrays.", "response": "def from_arrays(cls, arrays, name=None, **kwargs):\n        \"\"\"Creates a new instance of self from the given (list of) array(s).\n        This is done by calling numpy.rec.fromarrays on the given arrays with\n        the given kwargs. The type of the returned array is cast to this\n        class, and the name (if provided) is set.\n\n        Parameters\n        ----------\n        arrays : (list of) numpy array(s)\n            A list of the arrays to create the FieldArray from.\n        name : {None|str}\n            What the output array should be named.\n\n        For other keyword parameters, see the numpy.rec.fromarrays help.\n\n        Returns\n        -------\n        array : instance of this class\n            An array that is an instance of this class in which the field\n            data is from the given array(s).\n        \"\"\"\n        obj = numpy.rec.fromarrays(arrays, **kwargs).view(type=cls)\n        obj.name = name\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_records(cls, records, name=None, **kwargs):\n        obj = numpy.rec.fromrecords(records, **kwargs).view(\n            type=cls)\n        obj.name = name\n        return obj", "response": "Creates a new array of this class from the given list of record values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_kwargs(cls, **kwargs):\n        arrays = []\n        names = []\n        for p,vals in kwargs.items():\n            if not isinstance(vals, numpy.ndarray):\n                if not isinstance(vals, list):\n                    vals = [vals]\n                vals = numpy.array(vals)\n            arrays.append(vals)\n            names.append(p)\n        return cls.from_arrays(arrays, names=names)", "response": "Creates a new instance of this class from the given keyword arguments."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting the given LIGO_LW table instance into an array of FieldArray.", "response": "def from_ligolw_table(cls, table, columns=None, cast_to_dtypes=None):\n        \"\"\"Converts the given ligolw table into an FieldArray. The `tableName`\n        attribute is copied to the array's `name`.\n\n        Parameters\n        ----------\n        table : LIGOLw table instance\n            The table to convert.\n        columns : {None|list}\n            Optionally specify a list of columns to retrieve. All of the\n            columns must be in the table's validcolumns attribute. If None\n            provided, all the columns in the table will be converted.\n        dtype : {None | dict}\n            Override the columns' dtypes using the given dictionary. The\n            dictionary should be keyed by the column names, with the values\n            a tuple that can be understood by numpy.dtype. For example, to\n            cast a ligolw column called \"foo\" to a field called \"bar\" with\n            type float, cast_to_dtypes would be: ``{\"foo\": (\"bar\", float)}``.\n\n        Returns\n        -------\n        array : FieldArray\n            The input table as an FieldArray.\n        \"\"\"\n        name = table.tableName.split(':')[0]\n        if columns is None:\n            # get all the columns\n            columns = table.validcolumns\n        else:\n            # note: this will raise a KeyError if one or more columns is\n            # not in the table's validcolumns\n            new_columns = {}\n            for col in columns:\n                new_columns[col] = table.validcolumns[col]\n            columns = new_columns\n        if cast_to_dtypes is not None:\n            dtype = [cast_to_dtypes[col] for col in columns]\n        else:\n            dtype = columns.items()\n        # get the values\n        if _default_types_status['ilwd_as_int']:\n            input_array = \\\n                [tuple(getattr(row, col) if dt != 'ilwd:char'\n                       else int(getattr(row, col))\n                       for col,dt in columns.items())\n                 for row in table]\n        else:\n            input_array = \\\n                [tuple(getattr(row, col) for col in columns) for row in table]\n        # return the values as an instance of cls\n        return cls.from_records(input_array, dtype=dtype,\n            name=name)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_array(self, fields=None, axis=0):\n        if fields is None:\n            fields = self.fieldnames\n        if isinstance(fields, string_types):\n            fields = [fields]\n        return numpy.stack([self[f] for f in fields], axis=axis)", "response": "Returns an array of self in which the fields are included\n            as an extra dimension."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef virtualfields(self):\n        if self._virtualfields is None:\n            vfs = tuple()\n        else:\n            vfs = tuple(self._virtualfields)\n        return vfs", "response": "Returns a tuple listing the names of virtual fields in the current object"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef aliases(self):\n        return dict(c[0] for c in self.dtype.descr if isinstance(c[0], tuple))", "response": "Returns a dictionary of the aliases or titles of the field names\n        in self."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd the given arrays as new fields to the current array.", "response": "def add_fields(self, arrays, names=None, assubarray=False):\n        \"\"\"\n        Adds the given arrays as new fields to self.  Returns a new instance\n        with the new fields added. Note: this array does not change; the\n        returned array is a new copy.\n\n        Parameters\n        ----------\n        arrays : (list of) numpy array(s)\n            The arrays to add. If adding multiple arrays, must be a list;\n            if adding a single array, can just be that array.\n        names : (list of) strings\n            Optional, the name(s) of the new fields in the output array. If\n            adding multiple fields, must be a list of strings with the same\n            length as the list of arrays. If None provided, names used will\n            be the same as the name of the datatype in the given arrays.\n            If the datatype has no name, the new field will be ``'fi'`` where\n            i is the index of the array in arrays.\n        assubarray : bool\n            Add the list of arrays as a single subarray field. If True, and\n            names provided, names should be a string or a length-1 sequence.\n            Default is False, in which case each array will be added as a\n            separate field.\n\n        Returns\n        -------\n        new_array : new instance of this array\n            A copy of this array with the desired fields added.\n        \"\"\"\n        newself = add_fields(self, arrays, names=names, assubarray=assubarray)\n        self.__copy_attributes__(newself)\n        return newself"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse the given arguments and return a list of values populated by indices of the given values.", "response": "def parse_boolargs(self, args):\n        \"\"\"Returns an array populated by given values, with the indices of\n        those values dependent on given boolen tests on self.\n\n        The given `args` should be a list of tuples, with the first element the\n        return value and the second argument a string that evaluates to either\n        True or False for each element in self.\n\n        Each boolean argument is evaluated on elements for which every prior\n        boolean argument was False. For example, if array `foo` has a field\n        `bar`, and `args = [(1, 'bar < 10'), (2, 'bar < 20'), (3, 'bar < 30')]`,\n        then the returned array will have `1`s at the indices for\n        which `foo.bar < 10`, `2`s where `foo.bar < 20 and not foo.bar < 10`,\n        and `3`s where `foo.bar < 30 and not (foo.bar < 10 or foo.bar < 20)`.\n\n        The last argument in the list may have \"else\", an empty string, None,\n        or simply list a return value. In any of these cases, any element not\n        yet populated will be assigned the last return value.\n\n        Parameters\n        ----------\n        args : {(list of) tuples, value}\n            One or more return values and boolean argument determining where\n            they should go.\n\n        Returns\n        -------\n        return_values : array\n            An array with length equal to self, with values populated with the\n            return values.\n        leftover_indices : array\n            An array of indices that evaluated to False for all arguments.\n            These indices will not have been popluated with any value,\n            defaulting to whatever numpy uses for a zero for the return\n            values' dtype. If there are no leftovers, an empty array is\n            returned.\n\n        Examples\n        --------\n        Given the following array:\n\n        >>> arr = FieldArray(5, dtype=[('mtotal', float)])\n        >>> arr['mtotal'] = numpy.array([3., 5., 2., 1., 4.])\n\n        Return `\"TaylorF2\"` for all elements with `mtotal < 4` (note that the\n        elements 1 and 4 are leftover):\n\n        >>> arr.parse_boolargs(('TaylorF2', 'mtotal<4'))\n            (array(['TaylorF2', '', 'TaylorF2', 'TaylorF2', ''],\n            dtype='|S8'),\n            array([1, 4]))\n\n        Return `\"TaylorF2\"` for all elements with `mtotal < 4`,\n        `\"SEOBNR_ROM_DoubleSpin\"` otherwise:\n\n        >>> arr.parse_boolargs([('TaylorF2', 'mtotal<4'), ('SEOBNRv2_ROM_DoubleSpin', 'else')])\n            (array(['TaylorF2', 'SEOBNRv2_ROM_DoubleSpin', 'TaylorF2', 'TaylorF2',\n            'SEOBNRv2_ROM_DoubleSpin'],\n            dtype='|S23'),\n            array([], dtype=int64))\n\n        The following will also return the same:\n\n        >>> arr.parse_boolargs([('TaylorF2', 'mtotal<4'), ('SEOBNRv2_ROM_DoubleSpin',)])\n        >>> arr.parse_boolargs([('TaylorF2', 'mtotal<4'), ('SEOBNRv2_ROM_DoubleSpin', '')])\n        >>> arr.parse_boolargs([('TaylorF2', 'mtotal<4'), 'SEOBNRv2_ROM_DoubleSpin'])\n\n        Return `\"TaylorF2\"` for all elements with `mtotal < 3`, `\"IMRPhenomD\"`\n        for all elements with `3 <= mtotal < 4`, `\"SEOBNRv2_ROM_DoubleSpin\"`\n        otherwise:\n\n        >>> arr.parse_boolargs([('TaylorF2', 'mtotal<3'), ('IMRPhenomD', 'mtotal<4'), 'SEOBNRv2_ROM_DoubleSpin'])\n            (array(['IMRPhenomD', 'SEOBNRv2_ROM_DoubleSpin', 'TaylorF2', 'TaylorF2',\n            'SEOBNRv2_ROM_DoubleSpin'],\n            dtype='|S23'),\n            array([], dtype=int64))\n\n        Just return `\"TaylorF2\"` for all elements:\n\n        >>> arr.parse_boolargs('TaylorF2')\n            (array(['TaylorF2', 'TaylorF2', 'TaylorF2', 'TaylorF2', 'TaylorF2'],\n            dtype='|S8'),\n            array([], dtype=int64))\n\n        \"\"\"\n        if not isinstance(args, list):\n            args = [args]\n        # format the arguments\n        return_vals = []\n        bool_args = []\n        for arg in args:\n            if not isinstance(arg, tuple):\n                return_val = arg\n                bool_arg = None\n            elif len(arg) == 1:\n                return_val = arg[0]\n                bool_arg = None\n            elif len(arg) == 2:\n                return_val, bool_arg = arg\n            else:\n                raise ValueError(\"argument not formatted correctly\")\n            return_vals.append(return_val)\n            bool_args.append(bool_arg)\n        # get the output dtype\n        outdtype = numpy.array(return_vals).dtype\n        out = numpy.zeros(self.size, dtype=outdtype)\n        mask = numpy.zeros(self.size, dtype=bool)\n        leftovers = numpy.ones(self.size, dtype=bool)\n        for ii,(boolarg,val) in enumerate(zip(bool_args, return_vals)):\n            if boolarg is None or boolarg == '' or boolarg.lower() == 'else':\n                if ii+1 != len(bool_args):\n                    raise ValueError(\"only the last item may not provide \"\n                        \"any boolean arguments\")\n                mask = leftovers\n            else:\n                mask = leftovers & self[boolarg]\n            out[mask] = val\n            leftovers &= ~mask\n        return out, numpy.where(leftovers)[0]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nappending another array to this array.", "response": "def append(self, other):\n        \"\"\"Appends another array to this array.\n\n        The returned array will have all of the class methods and virutal\n        fields of this array, including any that were added using `add_method`\n        or `add_virtualfield`. If this array and other array have one or more\n        string fields, the dtype for those fields are updated to a string\n        length that can encompass the longest string in both arrays.\n\n        .. note::\n            Increasing the length of strings only works for fields, not\n            sub-fields.\n\n        Parameters\n        ----------\n        other : array\n            The array to append values from. It must have the same fields and\n            dtype as this array, modulo the length of strings. If the other\n            array does not have the same dtype, a TypeError is raised.\n\n        Returns\n        -------\n        array\n            An array with others values appended to this array's values. The\n            returned array is an instance of the same class as this array,\n            including all methods and virtual fields.\n        \"\"\"\n        try:\n            return numpy.append(self, other).view(type=self.__class__)\n        except TypeError:\n            # see if the dtype error was due to string fields having different\n            # lengths; if so, we'll make the joint field the larger of the\n            # two\n            str_fields = [name for name in self.fieldnames\n                          if _isstring(self.dtype[name])]\n            # get the larger of the two\n            new_strlens = dict(\n                [[name,\n                  max(self.dtype[name].itemsize, other.dtype[name].itemsize)]\n                 for name in str_fields]\n            )\n            # cast both to the new string lengths\n            new_dt = []\n            for dt in self.dtype.descr:\n                name = dt[0]\n                if name in new_strlens:\n                    dt = (name, self.dtype[name].type, new_strlens[name])\n                new_dt.append(dt)\n            new_dt = numpy.dtype(new_dt)\n            return numpy.append(\n                self.astype(new_dt),\n                other.astype(new_dt)\n                ).view(type=self.__class__)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a list of parameters to get the list of fields needed in order to evaluate those parameters.", "response": "def parse_parameters(cls, parameters, possible_fields):\n        \"\"\"Parses a list of parameters to get the list of fields needed in\n        order to evaluate those parameters.\n\n        Parameters\n        ----------\n        parameters : (list of) string(s)\n            The list of desired parameters. These can be (functions of) fields\n            or virtual fields.\n        possible_fields : (list of) string(s)\n            The list of possible fields.\n\n        Returns\n        -------\n        list :\n            The list of names of the fields that are needed in order to\n            evaluate the given parameters.\n        \"\"\"\n        if isinstance(possible_fields, string_types):\n            possible_fields = [possible_fields]\n        possible_fields = map(str, possible_fields)\n        # we'll just use float as the dtype, as we just need this for names\n        arr = cls(1, dtype=zip(possible_fields,\n                               len(possible_fields)*[float]))\n        # try to perserve order\n        return list(get_needed_fieldnames(arr, parameters))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a dictionary of the default fields and their dtypes.", "response": "def default_fields(cls, include_virtual=True, **kwargs):\n        \"\"\"The default fields and their dtypes. By default, this returns\n        whatever the class's ``_staticfields`` and ``_virtualfields`` is set\n        to as a dictionary of fieldname, dtype (the dtype of virtualfields is\n        given by VIRTUALFIELD_DTYPE). This function should be overridden by\n        subclasses to add dynamic fields; i.e., fields that require some input\n        parameters at initialization. Keyword arguments can be passed to this\n        to set such dynamic fields.\n        \"\"\"\n        output = cls._staticfields.copy()\n        if include_virtual:\n            output.update({name: VIRTUALFIELD_DTYPE\n                           for name in cls._virtualfields})\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding one or more empty default fields to self.", "response": "def add_default_fields(self, names, **kwargs):\n        \"\"\"\n        Adds one or more empty default fields to self.\n\n        Parameters\n        ----------\n        names : (list of) string(s)\n            The names of the fields to add. Must be a field in self's default\n            fields.\n\n        Other keyword args are any arguments passed to self's default fields.\n\n        Returns\n        -------\n        new array : instance of this array\n            A copy of this array with the field added.\n        \"\"\"\n        if isinstance(names, string_types):\n            names = [names]\n        default_fields = self.default_fields(include_virtual=False, **kwargs)\n        # parse out any virtual fields\n        arr = self.__class__(1, field_kwargs=kwargs)\n        # try to perserve order\n        sortdict = dict([[nm, ii] for ii,nm in enumerate(names)])\n        names = list(get_needed_fieldnames(arr, names))\n        names.sort(key=lambda x: sortdict[x] if x in sortdict\n            else len(names))\n        fields = [(name, default_fields[name]) for name in names]\n        arrays = []\n        names = []\n        for name,dt in fields:\n            arrays.append(default_empty(self.size, dtype=[(name, dt)]))\n            names.append(name)\n        return self.add_fields(arrays, names)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing a list of parameters to get the list of fields needed in the given class.", "response": "def parse_parameters(cls, parameters, possible_fields=None):\n        \"\"\"Parses a list of parameters to get the list of fields needed in\n        order to evaluate those parameters.\n\n        Parameters\n        ----------\n        parameters : (list of) strings\n            The list of desired parameters. These can be (functions of) fields\n            or virtual fields.\n        possible_fields : {None, dict}\n            Specify the list of possible fields. Must be a dictionary given\n            the names, and dtype of each possible field. If None, will use this\n            class's `_staticfields`.\n\n        Returns\n        -------\n        list :\n            The list of names of the fields that are needed in order to\n            evaluate the given parameters.\n        \"\"\"\n        if possible_fields is not None:\n            # make sure field names are strings and not unicode\n            possible_fields = dict([[f, dt]\n                for f,dt in possible_fields.items()])\n            class ModifiedArray(cls):\n                _staticfields = possible_fields\n            cls = ModifiedArray\n        return cls(1, names=parameters).fieldnames"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef chi_eff(self):\n        return conversions.chi_eff(self.mass1, self.mass2, self.spin1z,\n                                   self.spin2z)", "response": "Returns the effective spin."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the x - component of the spin of the primary mass.", "response": "def spin_px(self):\n        \"\"\"Returns the x-component of the spin of the primary mass.\"\"\"\n        return conversions.primary_spin(self.mass1, self.mass2, self.spin1x,\n                                        self.spin2x)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef spin_py(self):\n        return conversions.primary_spin(self.mass1, self.mass2, self.spin1y,\n                                        self.spin2y)", "response": "Returns the y - component of the spin of the primary mass."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef spin_pz(self):\n        return conversions.primary_spin(self.mass1, self.mass2, self.spin1z,\n                                        self.spin2z)", "response": "Returns the z - component of the spin of the primary mass."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef spin_sx(self):\n        return conversions.secondary_spin(self.mass1, self.mass2, self.spin1x,\n                                        self.spin2x)", "response": "Returns the x - component of the spin of the secondary mass."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the y - component of the spin of the secondary mass.", "response": "def spin_sy(self):\n        \"\"\"Returns the y-component of the spin of the secondary mass.\"\"\"\n        return conversions.secondary_spin(self.mass1, self.mass2, self.spin1y,\n                                        self.spin2y)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the z - component of the spin of the secondary mass.", "response": "def spin_sz(self):\n        \"\"\"Returns the z-component of the spin of the secondary mass.\"\"\"\n        return conversions.secondary_spin(self.mass1, self.mass2, self.spin1z,\n                                        self.spin2z)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef spin1_a(self):\n        return coordinates.cartesian_to_spherical_rho(\n                                    self.spin1x, self.spin1y, self.spin1z)", "response": "Returns the dimensionless spin magnitude of mass 1."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef spin1_polar(self):\n        return coordinates.cartesian_to_spherical_polar(\n                                     self.spin1x, self.spin1y, self.spin1z)", "response": "Returns the polar spin angle of mass 1."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef spin2_a(self):\n        return coordinates.cartesian_to_spherical_rho(\n                                    self.spin1x, self.spin1y, self.spin1z)", "response": "Returns the dimensionless spin magnitude of mass 2."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef spin2_polar(self):\n        return coordinates.cartesian_to_spherical_polar(\n                                     self.spin2x, self.spin2y, self.spin2z)", "response": "Returns the polar spin angle of mass 2."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef setup_splittable_workflow(workflow, input_tables, out_dir=None, tags=None):\n    '''\n    This function aims to be the gateway for code that is responsible for taking\n    some input file containing some table, and splitting into multiple files\n    containing different parts of that table. For now the only supported operation\n    is using lalapps_splitbank to split a template bank xml file into multiple\n    template bank xml files.\n\n    Parameters\n    -----------\n    workflow : pycbc.workflow.core.Workflow\n        The Workflow instance that the jobs will be added to.\n    input_tables : pycbc.workflow.core.FileList\n        The input files to be split up.\n    out_dir : path\n        The directory in which output will be written.\n\n    Returns\n    --------\n    split_table_outs : pycbc.workflow.core.FileList\n        The list of split up files as output from this job.\n    '''\n    if tags is None:\n        tags = []\n    logging.info(\"Entering split output files module.\")\n    make_analysis_dir(out_dir)\n    # Parse for options in .ini file\n    splitMethod = workflow.cp.get_opt_tags(\"workflow-splittable\",\n                                           \"splittable-method\", tags)\n\n    if splitMethod == \"IN_WORKFLOW\":\n        # Scope here for choosing different options\n        logging.info(\"Adding split output file jobs to workflow.\")\n        split_table_outs = setup_splittable_dax_generated(workflow,\n                input_tables, out_dir, tags)\n    elif splitMethod == \"NOOP\":\n        # Probably better not to call the module at all, but this option will\n        # return the input file list.\n        split_table_outs = input_tables\n    else:\n        errMsg = \"Splittable method not recognized. Must be one of \"\n        errMsg += \"IN_WORKFLOW or NOOP.\"\n        raise ValueError(errMsg)\n\n    logging.info(\"Leaving split output files module.\")\n    return split_table_outs", "response": "This function is used to setup the splittable workflow for a single template bank xml file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfunction for setting up the splitting jobs as part of the workflow. Parameters ----------- workflow : pycbc.workflow.core.Workflow The Workflow instance that the jobs will be added to. input_tables : pycbc.workflow.core.FileList The input files to be split up. out_dir : path The directory in which output will be written. Returns -------- split_table_outs : pycbc.workflow.core.FileList The list of split up files as output from this job.", "response": "def setup_splittable_dax_generated(workflow, input_tables, out_dir, tags):\n    '''\n    Function for setting up the splitting jobs as part of the workflow.\n\n    Parameters\n    -----------\n    workflow : pycbc.workflow.core.Workflow\n        The Workflow instance that the jobs will be added to.\n    input_tables : pycbc.workflow.core.FileList\n        The input files to be split up.\n    out_dir : path\n        The directory in which output will be written.\n\n    Returns\n    --------\n    split_table_outs : pycbc.workflow.core.FileList\n        The list of split up files as output from this job.\n    '''\n    cp = workflow.cp\n\n    # Get values from ini file\n    try:\n        num_splits = cp.get_opt_tags(\"workflow-splittable\",\n                                     \"splittable-num-banks\", tags)\n    except BaseException:\n        inj_interval = int(cp.get_opt_tags(\"workflow-splittable\",\n                                           \"splitinjtable-interval\", tags))\n        if cp.has_option_tags(\"em_bright_filter\", \"max-keep\", tags) and \\\n                cp.has_option(\"workflow-injections\", \"em-bright-only\"):\n            num_injs = int(cp.get_opt_tags(\"em_bright_filter\", \"max-keep\",\n                                           tags))\n        else:\n            num_injs = int(cp.get_opt_tags(\"workflow-injections\", \"num-injs\",\n                                           tags))\n        inj_tspace = float(abs(workflow.analysis_time)) / num_injs\n        num_splits = int(inj_interval // inj_tspace) + 1\n\n    split_exe_tag = cp.get_opt_tags(\"workflow-splittable\",\n                                    \"splittable-exe-tag\", tags)\n    split_exe = os.path.basename(cp.get(\"executables\", split_exe_tag))\n    # Select the appropriate class\n    exe_class = select_splitfilejob_instance(split_exe)\n\n    # Set up output structure\n    out_file_groups = FileList([])\n\n    # Set up the condorJob class for the current executable\n    curr_exe_job = exe_class(workflow.cp, split_exe_tag, num_splits,\n                             out_dir=out_dir)\n\n    for input in input_tables:\n        node = curr_exe_job.create_node(input, tags=tags)\n        workflow.add_node(node)\n        out_file_groups += node.output_files\n    return out_file_groups"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a string that links to the summary page and aLOG for this detector", "response": "def get_summary_page_link(ifo, utc_time):\n    \"\"\"Return a string that links to the summary page and aLOG for this ifo\n\n    Parameters\n    ----------\n    ifo : string\n        The detector name\n    utc_time : sequence\n        First three elements must be strings giving year, month, day resp.\n\n    Returns\n    -------\n    return_string : string\n        String containing HTML for links to summary page and aLOG search\n    \"\"\"\n    search_form = search_form_string\n    data = {'H1': data_h1_string, 'L1': data_l1_string}\n    if ifo not in data:\n        return ifo\n    else:\n        # alog format is day-month-year\n        alog_utc = '%02d-%02d-%4d' % (utc_time[2], utc_time[1], utc_time[0])\n        # summary page is exactly the reverse\n        ext = '%4d%02d%02d' % (utc_time[0], utc_time[1], utc_time[2])\n        return_string = search_form % (ifo.lower(), ifo.lower(), alog_utc, alog_utc)\n        return return_string + data[ifo] % ext"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nuse pkg - config to query for the location of libraries library directories and header directories.", "response": "def pkg_config(pkg_libraries):\n    \"\"\"Use pkg-config to query for the location of libraries, library directories,\n       and header directories\n\n       Arguments:\n           pkg_libries(list): A list of packages as strings\n\n       Returns:\n           libraries(list), library_dirs(list), include_dirs(list)\n    \"\"\"\n    libraries=[]\n    library_dirs=[]\n    include_dirs=[]\n\n    # Check that we have the packages\n    for pkg in pkg_libraries:\n        if os.system('pkg-config --exists %s 2>/dev/null' % pkg) == 0:\n            pass\n        else:\n            print(\"Could not find library {0}\".format(pkg))\n            sys.exit(1)\n\n    # Get the pck-config flags\n    if len(pkg_libraries)>0 :\n        # PKG_CONFIG_ALLOW_SYSTEM_CFLAGS explicitly lists system paths.\n        # On system-wide LAL installs, this is needed for swig to find lalswig.i\n        for token in getoutput(\"PKG_CONFIG_ALLOW_SYSTEM_CFLAGS=1 pkg-config --libs --cflags %s\" % ' '.join(pkg_libraries)).split():\n            if token.startswith(\"-l\"):\n                libraries.append(token[2:])\n            elif token.startswith(\"-L\"):\n                library_dirs.append(token[2:])\n            elif token.startswith(\"-I\"):\n                include_dirs.append(token[2:])\n\n    return libraries, library_dirs, include_dirs"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pkg_config_header_strings(pkg_libraries):\n    _, _, header_dirs = pkg_config(pkg_libraries)\n\n    header_strings = []\n\n    for header_dir in header_dirs:\n        header_strings.append(\"-I\" + header_dir)\n\n    return header_strings", "response": "Returns a list of header strings that could be passed to a compiler\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pkg_config_libdirs(packages):\n\n    # don't try calling pkg-config if NO_PKGCONFIG is set in environment\n    if os.environ.get(\"NO_PKGCONFIG\", None):\n        return []\n\n    # if calling pkg-config failes, don't continue and don't try again.\n    try:\n        FNULL = open(os.devnull, 'w')\n        subprocess.check_call([\"pkg-config\", \"--version\"], stdout=FNULL, close_fds=True)\n    except:\n        print(\"PyCBC.libutils: pkg-config call failed, setting NO_PKGCONFIG=1\",\n              file=sys.stderr)\n        os.environ['NO_PKGCONFIG'] = \"1\"\n        return []\n\n    # First, check that we can call pkg-config on each package in the list\n    for pkg in packages:\n        if not pkg_config_check_exists(pkg):\n            raise ValueError(\"Package {0} cannot be found on the pkg-config search path\".format(pkg))\n\n    libdirs = []\n    for token in getoutput(\"PKG_CONFIG_ALLOW_SYSTEM_LIBS=1 pkg-config --libs-only-L {0}\".format(' '.join(packages))).split():\n        if token.startswith(\"-L\"):\n            libdirs.append(token[2:])\n    return libdirs", "response": "Returns a list of all library paths that pkg - config says should be included when the packages given as packages are given as packages."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_ctypes_library(libname, packages, mode=None):\n    libdirs = []\n    # First try to get from LD_LIBRARY_PATH\n    if \"LD_LIBRARY_PATH\" in os.environ:\n        libdirs += os.environ[\"LD_LIBRARY_PATH\"].split(\":\")\n    # Next try to append via pkg_config\n    try:\n        libdirs += pkg_config_libdirs(packages)\n    except ValueError:\n        pass\n\n    # Note that the function below can accept an empty list for libdirs, in which case\n    # it will return None\n    fullpath = get_libpath_from_dirlist(libname,libdirs)\n\n    if fullpath is None:\n        # This won't actually return a full-path, but it should be something\n        # that can be found by CDLL\n        fullpath = find_library(libname)\n\n    if fullpath is None:\n        # We got nothin'\n        return None\n    else:\n        if mode is None:\n            return ctypes.CDLL(fullpath)\n        else:\n            return ctypes.CDLL(fullpath,mode=mode)", "response": "This function returns a CDLL object for the given library name and a list of packages."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef thin_samples_for_writing(fp, samples, parameters, last_iteration):\n    if fp.thinned_by > 1:\n        if last_iteration is None:\n            raise ValueError(\"File's thinned_by attribute is > 1 ({}), \"\n                             \"but last_iteration not provided.\"\n                             .format(fp.thinned_by))\n        thinned_samples = {}\n        for param in parameters:\n            data = samples[param]\n            nsamples = data.shape[-1]\n            # To figure out where to start:\n            # the last iteration in the file + the file's thinning interval\n            # gives the iteration of the next sample that should be written;\n            # last_iteration - nsamples gives the iteration of the first\n            # sample in samples. Subtracting the latter from the former - 1\n            # (-1 to convert from iteration to index) therefore gives the index\n            # in the samples data to start using samples.\n            thin_start = fp.last_iteration(param) + fp.thinned_by \\\n                - (last_iteration - nsamples) - 1\n            thinned_samples[param] = data[..., thin_start::fp.thinned_by]\n    else:\n        thinned_samples = samples\n    return thinned_samples", "response": "Thins samples for writing to disk."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nkeeps a list of the number of iterations that were in a file when a checkpoint was resumed from a checkpoint.", "response": "def write_resume_point(self):\n        \"\"\"Keeps a list of the number of iterations that were in a file when a\n        run was resumed from a checkpoint.\"\"\"\n        try:\n            resume_pts = self.attrs[\"resume_points\"].tolist()\n        except KeyError:\n            resume_pts = []\n        try:\n            niterations = self.niterations\n        except KeyError:\n            niterations = 0\n        resume_pts.append(niterations)\n        self.attrs[\"resume_points\"] = resume_pts"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nthinning the samples on disk using the given thinning interval.", "response": "def thin(self, thin_interval):\n        \"\"\"Thins the samples on disk using the given thinning interval.\n\n        Parameters\n        ----------\n        thin_interval : int\n            The interval to thin by.\n        \"\"\"\n        # read thinned samples into memory\n        params = self[self.samples_group].keys()\n        samples = self.read_raw_samples(params, thin_start=0,\n                                        thin_interval=thin_interval,\n                                        thin_end=None,\n                                        flatten=False)\n        # now resize and write the data back to disk\n        group = self[self.samples_group]\n        for param in params:\n            data = samples[param]\n            # resize the arrays on disk\n            group[param].resize(data.shape)\n            # and write\n            group[param][:] = data\n        # store the interval that samples were thinned by\n        self.thinned_by *= thin_interval\n        # If a default thin interval and thin start exist, reduce them by the\n        # thinned interval. If the thin interval is not an integer multiple\n        # of the original, we'll round up, to avoid getting samples from\n        # before the burn in / at an interval less than the ACL.\n        self.thin_start = int(numpy.ceil(self.thin_start/thin_interval))\n        self.thin_interval = int(numpy.ceil(self.thin_interval/thin_interval))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the iteration of the last sample of the given parameter.", "response": "def last_iteration(self, parameter=None):\n        \"\"\"Returns the iteration of the last sample of the given parameter.\n\n        If parameter is ``None``, will just use the first parameter in the\n        samples group.\n        \"\"\"\n        if parameter is None:\n            try:\n                parameter = list(self[self.samples_group].keys())[0]\n            except (IndexError, KeyError):\n                # nothing has been written yet, just return 0\n                return 0\n        try:\n            lastiter = self[self.samples_group][parameter].shape[-1]\n        except KeyError:\n            # no samples have been written, just return 0\n            lastiter = 0\n        # account for thinning\n        return lastiter * self.thinned_by"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef iterations(self, parameter):\n        return numpy.arange(0, self.last_iteration(parameter), self.thinned_by)", "response": "Returns the iteration each sample occurred at."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write_sampler_metadata(self, sampler):\n        self.attrs['sampler'] = sampler.name\n        self[self.sampler_group].attrs['nwalkers'] = sampler.nwalkers\n        # write the model's metadata\n        sampler.model.write_metadata(self)", "response": "Writes the sampler s metadata."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite the given autocorrelation lengths to the file.", "response": "def write_acls(self, acls):\n        \"\"\"Writes the given autocorrelation lengths.\n\n        The ACL of each parameter is saved to\n        ``[sampler_group]/acls/{param}']``.  The maximum over all the\n        parameters is saved to the file's 'acl' attribute.\n\n        Parameters\n        ----------\n        acls : dict\n            A dictionary of ACLs keyed by the parameter.\n\n        Returns\n        -------\n        ACL\n            The maximum of the acls that was written to the file.\n        \"\"\"\n        group = self.sampler_group + '/acls/{}'\n        # write the individual acls\n        for param in acls:\n            try:\n                # we need to use the write_direct function because it's\n                # apparently the only way to update scalars in h5py\n                self[group.format(param)].write_direct(\n                    numpy.array(acls[param]))\n            except KeyError:\n                # dataset doesn't exist yet\n                self[group.format(param)] = acls[param]\n        # write the maximum over all params\n        acl = numpy.array(acls.values()).max()\n        self[self.sampler_group].attrs['acl'] = acl\n        # set the default thin interval to be the acl (if it is finite)\n        if numpy.isfinite(acl):\n            self.thin_interval = int(numpy.ceil(acl))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread the acls of all the parameters. Returns ------- dict A dictionary of the acls keyed by the parameter name.", "response": "def read_acls(self):\n        \"\"\"Reads the acls of all the parameters.\n\n        Returns\n        -------\n        dict\n            A dictionary of the ACLs, keyed by the parameter name.\n        \"\"\"\n        group = self[self.sampler_group]['acls']\n        return {param: group[param].value for param in group.keys()}"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_burn_in(self, burn_in):\n        group = self[self.sampler_group]\n        group.attrs['burn_in_test'] = burn_in.burn_in_test\n        group.attrs['is_burned_in'] = burn_in.is_burned_in\n        group.attrs['burn_in_iteration'] = burn_in.burn_in_iteration\n        # set the defaut thin_start to be the burn_in_index\n        self.thin_start = burn_in.burn_in_index\n        # write individual test data\n        for tst in burn_in.burn_in_data:\n            key = 'burn_in_tests/{}'.format(tst)\n            try:\n                attrs = group[key].attrs\n            except KeyError:\n                group.create_group(key)\n                attrs = group[key].attrs\n            self.write_kwargs_to_attrs(attrs, **burn_in.burn_in_data[tst])", "response": "Write the given burn - in data to the given filename."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a parser to parse sampler - specific arguments for loading a specific set of modules.", "response": "def extra_args_parser(parser=None, skip_args=None, **kwargs):\n        \"\"\"Create a parser to parse sampler-specific arguments for loading\n        samples.\n\n        Parameters\n        ----------\n        parser : argparse.ArgumentParser, optional\n            Instead of creating a parser, add arguments to the given one. If\n            none provided, will create one.\n        skip_args : list, optional\n            Don't parse the given options. Options should be given as the\n            option string, minus the '--'. For example,\n            ``skip_args=['iteration']`` would cause the ``--iteration``\n            argument not to be included.\n        \\**kwargs :\n            All other keyword arguments are passed to the parser that is\n            created.\n\n        Returns\n        -------\n        parser : argparse.ArgumentParser\n            An argument parser with th extra arguments added.\n        actions : list of argparse.Action\n            A list of the actions that were added.\n        \"\"\"\n        if parser is None:\n            parser = argparse.ArgumentParser(**kwargs)\n        elif kwargs:\n            raise ValueError(\"No other keyword arguments should be provded if \"\n                             \"a parser is provided.\")\n        if skip_args is None:\n            skip_args = []\n        actions = []\n        if 'thin-start' not in skip_args:\n            act = parser.add_argument(\n                \"--thin-start\", type=int, default=None,\n                help=\"Sample number to start collecting samples to plot. If \"\n                     \"none provided, will use the input file's `thin_start` \"\n                     \"attribute.\")\n            actions.append(act)\n        if 'thin-interval' not in skip_args:\n            act = parser.add_argument(\n                \"--thin-interval\", type=int, default=None,\n                help=\"Interval to use for thinning samples. If none provided, \"\n                     \"will use the input file's `thin_interval` attribute.\")\n            actions.append(act)\n        if 'thin-end' not in skip_args:\n            act = parser.add_argument(\n                \"--thin-end\", type=int, default=None,\n                help=\"Sample number to stop collecting samples to plot. If \"\n                     \"none provided, will use the input file's `thin_end` \"\n                     \"attribute.\")\n            actions.append(act)\n        if 'iteration' not in skip_args:\n            act = parser.add_argument(\n                \"--iteration\", type=int, default=None,\n                help=\"Only retrieve the given iteration. To load \"\n                     \"the last n-th sampe use -n, e.g., -1 will \"\n                     \"load the last iteration. This overrides \"\n                     \"the thin-start/interval/end options.\")\n            actions.append(act)\n        if 'walkers' not in skip_args:\n            act = parser.add_argument(\n                \"--walkers\", type=int, nargs=\"+\", default=None,\n                help=\"Only retrieve samples from the listed \"\n                     \"walkers. Default is to retrieve from all \"\n                     \"walkers.\")\n            actions.append(act)\n        return parser, actions"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write_samples(self, samples, parameters=None, last_iteration=None):\n        nwalkers, nsamples = list(samples.values())[0].shape\n        assert all(p.shape == (nwalkers, nsamples)\n                   for p in samples.values()), (\n               \"all samples must have the same shape\")\n        group = self.samples_group + '/{name}'\n        if parameters is None:\n            parameters = samples.keys()\n        # thin the samples\n        samples = thin_samples_for_writing(self, samples, parameters, last_iteration)\n        # loop over number of dimensions\n        for param in parameters:\n            dataset_name = group.format(name=param)\n            data = samples[param]\n            # check that there's something to write after thinning\n            if data.shape[1] == 0:\n                # nothing to write, move along\n                continue\n            try:\n                fp_nsamples = self[dataset_name].shape[-1]\n                istart = fp_nsamples\n                istop = istart + data.shape[1]\n                if istop > fp_nsamples:\n                    # resize the dataset\n                    self[dataset_name].resize(istop, axis=1)\n            except KeyError:\n                # dataset doesn't exist yet\n                istart = 0\n                istop = istart + data.shape[1]\n                self.create_dataset(dataset_name, (nwalkers, istop),\n                                    maxshape=(nwalkers, None),\n                                    dtype=data.dtype,\n                                    fletcher32=True)\n            self[dataset_name][:, istart:istop] = data", "response": "Writes samples to the given file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_raw_samples(self, fields,\n                         thin_start=None, thin_interval=None, thin_end=None,\n                         iteration=None, walkers=None, flatten=True):\n        \"\"\"Base function for reading samples.\n\n        Parameters\n        -----------\n        fields : list\n            The list of field names to retrieve. Must be names of datasets in\n            the ``samples_group``.\n        thin_start : int, optional\n            Start reading from the given iteration. Default is to start from\n            the first iteration.\n        thin_interval : int, optional\n            Only read every ``thin_interval`` -th sample. Default is 1.\n        thin_end : int, optional\n            Stop reading at the given iteration. Default is to end at the last\n            iteration.\n        iteration : int, optional\n            Only read the given iteration. If this provided, it overrides\n            the ``thin_(start|interval|end)`` options.\n        walkers : int, optional\n            Only read from the given walkers. Default is to read all.\n        flatten : bool, optional\n            Flatten the samples to 1D arrays before returning. Otherwise, the\n            returned arrays will have shape (requested walkers x\n            requested iteration(s)). Default is True.\n\n        Returns\n        -------\n        dict\n            A dictionary of field name -> numpy array pairs.\n        \"\"\"\n        if isinstance(fields, (str, unicode)):\n            fields = [fields]\n        # walkers to load\n        if walkers is not None:\n            widx = numpy.zeros(self.nwalkers, dtype=bool)\n            widx[walkers] = True\n            nwalkers = widx.sum()\n        else:\n            widx = slice(0, None)\n            nwalkers = self.nwalkers\n        # get the slice to use\n        if iteration is not None:\n            get_index = int(iteration)\n            niterations = 1\n        else:\n            get_index = self.get_slice(thin_start=thin_start,\n                                       thin_end=thin_end,\n                                       thin_interval=thin_interval)\n            # we'll just get the number of iterations from the returned shape\n            niterations = None\n        # load\n        group = self.samples_group + '/{name}'\n        arrays = {}\n        for name in fields:\n            arr = self[group.format(name=name)][widx, get_index]\n            if niterations is None:\n                niterations = arr.shape[-1]\n            if flatten:\n                arr = arr.flatten()\n            else:\n                # ensure that the returned array is 2D\n                arr = arr.reshape((nwalkers, niterations))\n            arrays[name] = arr\n        return arrays", "response": "Read samples from the given fields from the given dataset."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setup_datafind_runtime_frames_single_call_perifo(cp, scienceSegs,\n                                              outputDir, tags=None):\n    \"\"\"\n    This function uses the glue.datafind library to obtain the location of all\n    the frame files that will be needed to cover the analysis of the data\n    given in scienceSegs. This function will not check if the returned frames\n    cover the whole time requested, such sanity checks are done in the\n    pycbc.workflow.setup_datafind_workflow entry function. As opposed to\n    setup_datafind_runtime_generated this call will only run one call to\n    datafind per ifo, spanning the whole time. This function will return a list\n    of files corresponding to the individual frames returned by the datafind\n    query. This will allow pegasus to more easily identify all the files used\n    as input, but may cause problems for codes that need to take frame cache\n    files as input.\n\n    Parameters\n    -----------\n    cp : ConfigParser.ConfigParser instance\n        This contains a representation of the information stored within the\n        workflow configuration files\n    scienceSegs : Dictionary of ifo keyed glue.segment.segmentlist instances\n        This contains the times that the workflow is expected to analyse.\n    outputDir : path\n        All output files written by datafind processes will be written to this\n        directory.\n    tags : list of strings, optional (default=None)\n        Use this to specify tags. This can be used if this module is being\n        called more than once to give call specific configuration (by setting\n        options in [workflow-datafind-${TAG}] rather than [workflow-datafind]).\n        This is also used to tag the Files returned by the class to uniqueify\n        the Files and uniqueify the actual filename.\n        FIXME: Filenames may not be unique with current codes!\n\n    Returns\n    --------\n    datafindcaches : list of glue.lal.Cache instances\n       The glue.lal.Cache representations of the various calls to the datafind\n       server and the returned frame files.\n    datafindOuts : pycbc.workflow.core.FileList\n        List of all the datafind output files for use later in the pipeline.\n\n    \"\"\"\n    datafindcaches, _ = \\\n        setup_datafind_runtime_cache_single_call_perifo(cp, scienceSegs,\n                                                        outputDir, tags=tags)\n\n    datafindouts = convert_cachelist_to_filelist(datafindcaches)\n\n    return datafindcaches, datafindouts", "response": "This function will create a list of files that will be used to cover the whole time requested by datafind. This function will only return one frame per file per ifo."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef convert_cachelist_to_filelist(datafindcache_list):\n    prev_file = None\n    prev_name = None\n    this_name = None\n\n    datafind_filelist = FileList([])\n\n    for cache in datafindcache_list:\n        # sort the cache into time sequential order\n        cache.sort()\n        curr_ifo = cache.ifo\n        for frame in cache:\n            # Pegasus doesn't like \"localhost\" in URLs.\n            frame.url = frame.url.replace('file://localhost','file://')\n\n            # Create one File() object for each unique frame file that we\n            # get back in the cache.\n            if prev_file:\n                prev_name = os.path.basename(prev_file.cache_entry.url)\n                this_name = os.path.basename(frame.url)\n\n            if (prev_file is None) or (prev_name != this_name):\n                currFile = File(curr_ifo, frame.description,\n                    frame.segment, file_url=frame.url, use_tmp_subdirs=True)\n                datafind_filelist.append(currFile)\n                prev_file = currFile\n\n            # Populate the PFNs for the File() we just created\n            if frame.url.startswith('file://'):\n                currFile.PFN(frame.url, site='local')\n                if frame.url.startswith(\n                    'file:///cvmfs/oasis.opensciencegrid.org/ligo/frames'):\n                    # Datafind returned a URL valid on the osg as well\n                    # so add the additional PFNs to allow OSG access.\n                    currFile.PFN(frame.url, site='osg')\n                    currFile.PFN(frame.url.replace(\n                        'file:///cvmfs/oasis.opensciencegrid.org/',\n                        'root://xrootd-local.unl.edu/user/'), site='osg')\n                    currFile.PFN(frame.url.replace(\n                        'file:///cvmfs/oasis.opensciencegrid.org/',\n                        'gsiftp://red-gridftp.unl.edu/user/'), site='osg')\n                    currFile.PFN(frame.url.replace(\n                        'file:///cvmfs/oasis.opensciencegrid.org/',\n                        'gsiftp://ldas-grid.ligo.caltech.edu/hdfs/'), site='osg')\n                elif frame.url.startswith(\n                    'file:///cvmfs/gwosc.osgstorage.org/'):\n                    # Datafind returned a URL valid on the osg as well\n                    # so add the additional PFNs to allow OSG access.\n                    for s in ['osg', 'orangegrid', 'osgconnect']:\n                        currFile.PFN(frame.url, site=s)\n                        currFile.PFN(frame.url, site=\"{}-scratch\".format(s))\n            else:\n                currFile.PFN(frame.url, site='notlocal')\n\n    return datafind_filelist", "response": "Converts a list of cache objects to a list of File objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_science_segs_from_datafind_outs(datafindcaches):\n    newScienceSegs = {}\n    for cache in datafindcaches:\n        if len(cache) > 0:\n            groupSegs = segments.segmentlist(e.segment for e in cache).coalesce()\n            ifo = cache.ifo\n            if ifo not in newScienceSegs:\n                newScienceSegs[ifo] = groupSegs\n            else:\n                newScienceSegs[ifo].extend(groupSegs)\n                newScienceSegs[ifo].coalesce()\n    return newScienceSegs", "response": "This function calculates the science segments that are covered by the frames found in the datafind output files."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_missing_segs_from_frame_file_cache(datafindcaches):\n    missingFrameSegs = {}\n    missingFrames = {}\n    for cache in datafindcaches:\n        if len(cache) > 0:\n            # Don't bother if these are not file:// urls, assume all urls in\n            # one cache file must be the same type\n            if not cache[0].scheme == 'file':\n                warn_msg = \"We have %s entries in the \" %(cache[0].scheme,)\n                warn_msg += \"cache file. I do not check if these exist.\"\n                logging.info(warn_msg)\n                continue\n            _, currMissingFrames = cache.checkfilesexist(on_missing=\"warn\")\n            missingSegs = segments.segmentlist(e.segment \\\n                                         for e in currMissingFrames).coalesce()\n            ifo = cache.ifo\n            if ifo not in missingFrameSegs:\n                missingFrameSegs[ifo] = missingSegs\n                missingFrames[ifo] = lal.Cache(currMissingFrames)\n            else:\n                missingFrameSegs[ifo].extend(missingSegs)\n                # NOTE: This .coalesce probably isn't needed as the segments\n                # should be disjoint. If speed becomes an issue maybe remove it?\n                missingFrameSegs[ifo].coalesce()\n                missingFrames[ifo].extend(currMissingFrames)\n    return missingFrameSegs, missingFrames", "response": "This function will return a list of dicts that contain the list of missing frames for a single segment in the datafind cache."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef setup_datafind_server_connection(cp, tags=None):\n    if tags is None:\n        tags = []\n\n    if cp.has_option_tags(\"workflow-datafind\",\n                          \"datafind-ligo-datafind-server\", tags):\n        datafind_server = cp.get_opt_tags(\"workflow-datafind\",\n                                        \"datafind-ligo-datafind-server\", tags)\n    else:\n        datafind_server = None\n\n    return datafind_connection(datafind_server)", "response": "Setup the connection to the datafind server."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef datafind_keep_unique_backups(backup_outs, orig_outs):\n    # NOTE: This function is not optimized and could be made considerably\n    #       quicker if speed becomes in issue. With 4s frame files this might\n    #       be slow, but for >1000s files I don't foresee any issue, so I keep\n    #       this simple.\n    return_list = FileList([])\n    # We compare the LFNs to determine uniqueness\n    # Is there a way to associate two paths with one LFN??\n    orig_names = [f.name for f in orig_outs]\n    for file in backup_outs:\n        if file.name not in orig_names:\n            return_list.append(file)\n        else:\n            index_num = orig_names.index(file.name)\n            orig_out = orig_outs[index_num]\n            pfns = list(file.pfns)\n            # This shouldn't happen, but catch if it does\n            assert(len(pfns) == 1)\n            orig_out.PFN(pfns[0].url, site='notlocal')\n\n    return return_list", "response": "This function will take a list of backup datafind files and compares them against a list of original datafind files."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef segment_snrs(filters, stilde, psd, low_frequency_cutoff):\n    snrs = []\n    norms = []\n\n    for bank_template in filters:\n        # For every template compute the snr against the stilde segment\n        snr, _, norm = matched_filter_core(\n                bank_template, stilde, h_norm=bank_template.sigmasq(psd),\n                psd=None, low_frequency_cutoff=low_frequency_cutoff)\n        # SNR time series stored here\n        snrs.append(snr)\n        # Template normalization factor stored here\n        norms.append(norm)\n\n    return snrs, norms", "response": "This function calculates the snr of each bank veto template against the current segment of data."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef template_overlaps(bank_filters, template, psd, low_frequency_cutoff):\n    overlaps = []\n    template_ow = template / psd\n    for bank_template in bank_filters:\n        overlap = overlap_cplx(template_ow, bank_template,\n                low_frequency_cutoff=low_frequency_cutoff, normalized=False)\n        norm = sqrt(1 / template.sigmasq(psd) / bank_template.sigmasq(psd))\n        overlaps.append(overlap * norm)\n        if (abs(overlaps[-1]) > 0.99):\n            errMsg = \"Overlap > 0.99 between bank template and filter. \"\n            errMsg += \"This bank template will not be used to calculate \"\n            errMsg += \"bank chisq for this filter template. The expected \"\n            errMsg += \"value will be added to the chisq to account for \"\n            errMsg += \"the removal of this template.\\n\"\n            errMsg += \"Masses of filter template: %e %e\\n\" \\\n                      %(template.params.mass1, template.params.mass2)\n            errMsg += \"Masses of bank filter template: %e %e\\n\" \\\n                      %(bank_template.params.mass1, bank_template.params.mass2)\n            errMsg += \"Overlap: %e\" %(abs(overlaps[-1]))\n            logging.debug(errMsg)\n    return overlaps", "response": "This function calculates the overlaps between the template and the bank veto templates."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef values(self, template, psd, stilde, snrv, norm, indices):\n        if self.do:\n            logging.info(\"...Doing bank veto\")\n            overlaps = self.cache_overlaps(template, psd)\n            bank_veto_snrs, bank_veto_norms = self.cache_segment_snrs(stilde, psd)\n            chisq = bank_chisq_from_filters(snrv, norm, bank_veto_snrs,\n                                            bank_veto_norms, overlaps, indices)\n            dof = numpy.repeat(self.dof, len(chisq))\n            return chisq, dof\n        else:\n            return None, None", "response": "Returns the chisq and dof of a bank veto from the filters."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef start_end_from_segments(segment_file):\n    from glue.ligolw.ligolw import LIGOLWContentHandler as h; lsctables.use_in(h)\n    indoc = ligolw_utils.load_filename(segment_file, False, contenthandler=h)\n    segment_table  = table.get_table(indoc, lsctables.SegmentTable.tableName)\n    start = numpy.array(segment_table.getColumnByName('start_time'))\n    start_ns = numpy.array(segment_table.getColumnByName('start_time_ns'))\n    end = numpy.array(segment_table.getColumnByName('end_time'))\n    end_ns = numpy.array(segment_table.getColumnByName('end_time_ns'))\n    return start + start_ns * 1e-9, end + end_ns * 1e-9", "response": "Return the start and end time arrays from a segment file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn an index array into times that lie within the durations defined by start end arrays", "response": "def indices_within_times(times, start, end):\n    \"\"\"\n    Return an index array into times that lie within the durations defined by start end arrays\n\n    Parameters\n    ----------\n    times: numpy.ndarray\n        Array of times\n    start: numpy.ndarray\n        Array of duration start times\n    end: numpy.ndarray\n        Array of duration end times\n\n    Returns\n    -------\n    indices: numpy.ndarray\n        Array of indices into times\n    \"\"\"\n    # coalesce the start/end segments\n    start, end = segments_to_start_end(start_end_to_segments(start, end).coalesce())\n\n    tsort = times.argsort()\n    times_sorted = times[tsort]\n    left = numpy.searchsorted(times_sorted, start)\n    right = numpy.searchsorted(times_sorted, end)\n\n    if len(left) == 0:\n        return numpy.array([], dtype=numpy.uint32)\n\n    return tsort[numpy.hstack(numpy.r_[s:e] for s, e in zip(left, right))]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning an index array into times that like outside the durations defined by start end arrays", "response": "def indices_outside_times(times, start, end):\n    \"\"\"\n    Return an index array into times that like outside the durations defined by start end arrays\n\n    Parameters\n    ----------\n    times: numpy.ndarray\n        Array of times\n    start: numpy.ndarray\n        Array of duration start times\n    end: numpy.ndarray\n        Array of duration end times\n\n    Returns\n    -------\n    indices: numpy.ndarray\n        Array of indices into times\n    \"\"\"\n    exclude = indices_within_times(times, start, end)\n    indices = numpy.arange(0, len(times))\n    return numpy.delete(indices, exclude)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef select_segments_by_definer(segment_file, segment_name=None, ifo=None):\n    from glue.ligolw.ligolw import LIGOLWContentHandler as h; lsctables.use_in(h)\n    indoc = ligolw_utils.load_filename(segment_file, False, contenthandler=h)\n    segment_table  = table.get_table(indoc, 'segment')\n\n    seg_def_table = table.get_table(indoc, 'segment_definer')\n    def_ifos = seg_def_table.getColumnByName('ifos')\n    def_names = seg_def_table.getColumnByName('name')\n    def_ids = seg_def_table.getColumnByName('segment_def_id')\n\n    valid_id = []\n    for def_ifo, def_name, def_id in zip(def_ifos, def_names, def_ids):\n        if ifo and ifo != def_ifo:\n            continue\n        if segment_name and segment_name != def_name:\n            continue\n        valid_id += [def_id]\n\n    start = numpy.array(segment_table.getColumnByName('start_time'))\n    start_ns = numpy.array(segment_table.getColumnByName('start_time_ns'))\n    end = numpy.array(segment_table.getColumnByName('end_time'))\n    end_ns = numpy.array(segment_table.getColumnByName('end_time_ns'))\n    start, end = start + 1e-9 * start_ns, end + 1e-9 * end_ns\n    did = segment_table.getColumnByName('segment_def_id')\n\n    keep = numpy.array([d in valid_id for d in did])\n    if sum(keep) > 0:\n        return start_end_to_segments(start[keep], end[keep])\n    else:\n        return segmentlist([])", "response": "Select the segments that match the segment name"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef indices_within_segments(times, segment_files, ifo=None, segment_name=None):\n    veto_segs = segmentlist([])\n    indices = numpy.array([], dtype=numpy.uint32)\n    for veto_file in segment_files:\n        veto_segs += select_segments_by_definer(veto_file, segment_name, ifo)\n    veto_segs.coalesce()\n\n    start, end = segments_to_start_end(veto_segs)\n    if len(start) > 0:\n        idx = indices_within_times(times, start, end)\n        indices = numpy.union1d(indices, idx)\n\n    return indices, veto_segs.coalesce()", "response": "Return the list of indices that should be vetoed by the segments in the segment_files."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the list of indices that are outside the segments in the segmentlist of the specified time.", "response": "def indices_outside_segments(times, segment_files, ifo=None, segment_name=None):\n    \"\"\" Return the list of indices that are outside the segments in the\n    list of segment files.\n\n    Parameters\n    ----------\n    times: numpy.ndarray of integer type\n        Array of gps start times\n    segment_files: string or list of strings\n        A string or list of strings that contain the path to xml files that\n        contain a segment table\n    ifo: string, optional\n        The ifo to retrieve segments for from the segment files\n    segment_name: str, optional\n        name of segment\n    Returns\n    --------\n    indices: numpy.ndarray\n        The array of index values outside the segments\n    segmentlist:\n        The segment list corresponding to the selected time.\n    \"\"\"\n    exclude, segs = indices_within_segments(times, segment_files,\n                                         ifo=ifo, segment_name=segment_name)\n    indices = numpy.arange(0, len(times))\n    return numpy.delete(indices, exclude), segs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a dict with the comment column as the value for each segment", "response": "def get_segment_definer_comments(xml_file, include_version=True):\n    \"\"\"Returns a dict with the comment column as the value for each segment\"\"\"\n\n    from glue.ligolw.ligolw import LIGOLWContentHandler as h\n    lsctables.use_in(h)\n\n    # read segment definer table\n    xmldoc, _ = ligolw_utils.load_fileobj(xml_file,\n                                        gz=xml_file.name.endswith(\".gz\"),\n                                        contenthandler=h)\n    seg_def_table = table.get_table(xmldoc,\n                                    lsctables.SegmentDefTable.tableName)\n\n    # put comment column into a dict\n    comment_dict = {}\n    for seg_def in seg_def_table:\n        if include_version:\n            full_channel_name = ':'.join([str(seg_def.ifos),\n                                          str(seg_def.name),\n                                          str(seg_def.version)])\n        else:\n            full_channel_name = ':'.join([str(seg_def.ifos),\n                                          str(seg_def.name)])\n\n        comment_dict[full_channel_name] = seg_def.comment\n\n    return comment_dict"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the median 1d marginalized parameter of the specified key.", "response": "def median1d(self, name, return_errors=False):\n        \"\"\" Return median 1d marginalized parameters\n\n        Parameters\n        ----------\n        name: str\n            The name of the parameter requested\n        return_errors: Optional, {bool, False}\n            If true, return a second and third parameter that represents the\n            lower and upper 90% error on the parameter.\n\n        Returns\n        -------\n        param: float or tuple\n            The requested parameter\n        \"\"\"\n        if return_errors:\n            mid = self.data[name]['best']\n            low, high = self.data[name]['err']\n            return (mid, low, high)\n        else:\n            return self.data[name]['best']"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the time series of the event in the smallest available format.", "response": "def strain(self, ifo, duration=32, sample_rate=4096):\n        \"\"\" Return strain around the event\n\n        Currently this will return the strain around the event in the smallest\n        format available. Selection of other data is not yet available.\n\n        Parameters\n        ----------\n        ifo: str\n            The name of the observatory you want strain for. Ex. H1, L1, V1\n\n        Returns\n        -------\n        strain: pycbc.types.TimeSeries\n            Strain around the event.\n        \"\"\"\n        from astropy.utils.data import download_file\n        from pycbc.frame import read_frame\n\n        # Information is currently wrong on GWOSC!\n        # channels = self.data['files']['FrameChannels']\n        # for channel in channels:\n        #    if ifo in channel:\n        #        break\n\n        length = \"{}sec\".format(duration)\n        if sample_rate == 4096:\n            sampling = \"4KHz\"\n        elif sample_rate == 16384:\n            sampling = \"16KHz\"\n\n        channel = \"{}:GWOSC-{}_R1_STRAIN\".format(ifo, sampling.upper())\n        url = self.data['files'][ifo][length][sampling]['GWF']\n        filename = download_file(url, cache=True)\n        return read_frame(str(filename), str(channel))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the median 1d marginalized parameters for the specified parameter.", "response": "def median1d(self, param, return_errors=False):\n        \"\"\" Return median 1d marginalized parameters\n\n        Parameters\n        ----------\n        name: str\n            The name of the parameter requested\n        return_errors: Optional, {bool, False}\n            If true, return a second and third parameter that represents the\n            lower and upper 90% error on the parameter.\n\n        Returns\n        -------\n        param: nump.ndarray or tuple\n            The requested parameter\n        \"\"\"\n        v = [self.mergers[m].median1d(param, return_errors=return_errors) for m in self.mergers]\n        if return_errors:\n            value, merror, perror = zip(*v)\n            return numpy.array(value), numpy.array(merror), numpy.array(perror)\n        else:\n            return numpy.array(v)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make_inference_prior_plot(workflow, config_file, output_dir,\n                    sections=None, name=\"inference_prior\",\n                    analysis_seg=None, tags=None):\n    \"\"\" Sets up the corner plot of the priors in the workflow.\n\n    Parameters\n    ----------\n    workflow: pycbc.workflow.Workflow\n        The core workflow instance we are populating\n    config_file: pycbc.workflow.File\n        The WorkflowConfigParser parasable inference configuration file..\n    output_dir: str\n        The directory to store result plots and files.\n    sections : list\n        A list of subsections to use.\n    name: str\n        The name in the [executables] section of the configuration file\n        to use.\n    analysis_segs: {None, ligo.segments.Segment}\n       The segment this job encompasses. If None then use the total analysis\n       time from the workflow.\n    tags: {None, optional}\n        Tags to add to the inference executables.\n\n    Returns\n    -------\n    pycbc.workflow.FileList\n        A list of result and output files.\n    \"\"\"\n\n    # default values\n    tags = [] if tags is None else tags\n    analysis_seg = workflow.analysis_time \\\n                       if analysis_seg is None else analysis_seg\n\n    # make the directory that will contain the output files\n    makedir(output_dir)\n\n    # make a node for plotting the posterior as a corner plot\n    node = PlotExecutable(workflow.cp, name, ifos=workflow.ifos,\n                      out_dir=output_dir, universe=\"local\",\n                      tags=tags).create_node()\n\n    # add command line options\n    node.add_input_opt(\"--config-file\", config_file)\n    node.new_output_file_opt(analysis_seg, \".png\", \"--output-file\")\n    if sections is not None:\n        node.add_opt(\"--sections\", \" \".join(sections))\n\n    # add node to workflow\n    workflow += node\n\n    return node.output_files", "response": "Creates a corner plot of the priors in the core workflow."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating the summary table for the posterior samples.", "response": "def make_inference_summary_table(workflow, inference_file, output_dir,\n                    variable_args=None, name=\"inference_table\",\n                    analysis_seg=None, tags=None):\n    \"\"\" Sets up the corner plot of the posteriors in the workflow.\n\n    Parameters\n    ----------\n    workflow: pycbc.workflow.Workflow\n        The core workflow instance we are populating\n    inference_file: pycbc.workflow.File\n        The file with posterior samples.\n    output_dir: str\n        The directory to store result plots and files.\n    variable_args : list\n        A list of parameters to use instead of [variable_args].\n    name: str\n        The name in the [executables] section of the configuration file\n        to use.\n    analysis_segs: {None, ligo.segments.Segment}\n       The segment this job encompasses. If None then use the total analysis\n       time from the workflow.\n    tags: {None, optional}\n        Tags to add to the inference executables.\n\n    Returns\n    -------\n    pycbc.workflow.FileList\n        A list of result and output files.\n    \"\"\"\n\n    # default values\n    tags = [] if tags is None else tags\n    analysis_seg = workflow.analysis_time \\\n                       if analysis_seg is None else analysis_seg\n\n    # make the directory that will contain the output files\n    makedir(output_dir)\n\n    # make a node for plotting the posterior as a corner plot\n    node = PlotExecutable(workflow.cp, name, ifos=workflow.ifos,\n                      out_dir=output_dir, tags=tags).create_node()\n\n    # add command line options\n    node.add_input_opt(\"--input-file\", inference_file)\n    node.new_output_file_opt(analysis_seg, \".html\", \"--output-file\")\n    node.add_opt(\"--parameters\", \" \".join(variable_args))\n\n    # add node to workflow\n    workflow += node\n\n    return node.output_files"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_inference_acceptance_rate_plot(workflow, inference_file, output_dir,\n                    name=\"inference_rate\", analysis_seg=None, tags=None):\n    \"\"\" Sets up the acceptance rate plot in the workflow.\n\n    Parameters\n    ----------\n    workflow: pycbc.workflow.Workflow\n        The core workflow instance we are populating\n    inference_file: pycbc.workflow.File\n        The file with posterior samples.\n    output_dir: str\n        The directory to store result plots and files.\n    name: str\n        The name in the [executables] section of the configuration file\n        to use.\n    analysis_segs: {None, ligo.segments.Segment}\n       The segment this job encompasses. If None then use the total analysis\n       time from the workflow.\n    tags: {None, optional}\n        Tags to add to the inference executables.\n\n    Returns\n    -------\n    pycbc.workflow.FileList\n        A list of result and output files.\n    \"\"\"\n\n    # default values\n    tags = [] if tags is None else tags\n    analysis_seg = workflow.analysis_time \\\n                       if analysis_seg is None else analysis_seg\n\n    # make the directory that will contain the output files\n    makedir(output_dir)\n\n    # make a node for plotting the acceptance rate\n    node = PlotExecutable(workflow.cp, name, ifos=workflow.ifos,\n                      out_dir=output_dir, tags=tags).create_node()\n\n    # add command line options\n    node.add_input_opt(\"--input-file\", inference_file)\n    node.new_output_file_opt(analysis_seg, \".png\", \"--output-file\")\n\n    # add node to workflow\n    workflow += node\n\n    return node.output_files", "response": "Creates the node that will plot the acceptance rate of the given file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_inference_inj_plots(workflow, inference_files, output_dir,\n                             parameters, name=\"inference_recovery\",\n                             analysis_seg=None, tags=None):\n    \"\"\" Sets up the recovered versus injected parameter plot in the workflow.\n\n    Parameters\n    ----------\n    workflow: pycbc.workflow.Workflow\n        The core workflow instance we are populating\n    inference_files: pycbc.workflow.FileList\n        The files with posterior samples.\n    output_dir: str\n        The directory to store result plots and files.\n    parameters : list\n        A ``list`` of parameters. Each parameter gets its own plot.\n    name: str\n        The name in the [executables] section of the configuration file\n        to use.\n    analysis_segs: {None, ligo.segments.Segment}\n       The segment this job encompasses. If None then use the total analysis\n       time from the workflow.\n    tags: {None, optional}\n        Tags to add to the inference executables.\n\n    Returns\n    -------\n    pycbc.workflow.FileList\n        A list of result and output files.\n    \"\"\"\n\n    # default values\n    tags = [] if tags is None else tags\n    analysis_seg = workflow.analysis_time \\\n                       if analysis_seg is None else analysis_seg\n    output_files = FileList([])\n\n    # make the directory that will contain the output files\n    makedir(output_dir)\n\n    # add command line options\n    for (ii, param) in enumerate(parameters):\n        plot_exe = PlotExecutable(workflow.cp, name, ifos=workflow.ifos,\n                                  out_dir=output_dir,\n                                  tags=tags+['param{}'.format(ii)])\n        node = plot_exe.create_node()\n        node.add_input_list_opt(\"--input-file\", inference_files)\n        node.new_output_file_opt(analysis_seg, \".png\", \"--output-file\")\n        node.add_opt(\"--parameters\", param)\n        workflow += node\n        output_files += node.output_files\n\n    return output_files", "response": "Create the result and output plots for the injected parameter plot."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the analyzable segments after applying ini specified vetoes.", "response": "def get_science_segments(workflow, out_dir, tags=None):\n    \"\"\"\n    Get the analyzable segments after applying ini specified vetoes.\n\n    Parameters\n    -----------\n    workflow : Workflow object\n        Instance of the workflow object\n    out_dir : path\n        Location to store output files\n    tags : list of strings\n        Used to retrieve subsections of the ini file for\n        configuration options.\n\n    Returns\n    --------\n    sci_seg_file : workflow.core.SegFile instance\n        The segment file combined from all ifos containing the science segments.\n    sci_segs : Ifo keyed dict of ligo.segments.segmentlist instances\n        The science segs for each ifo, keyed by ifo\n    sci_seg_name : str\n        The name with which science segs are stored in the output XML file.\n    \"\"\"\n    if tags is None:\n        tags = []\n    logging.info('Starting generation of science segments')\n\n    make_analysis_dir(out_dir)\n    start_time = workflow.analysis_time[0]\n    end_time = workflow.analysis_time[1]\n\n    # NOTE: Should this be overrideable in the config file?\n    sci_seg_name = \"SCIENCE\"\n    sci_segs = {}\n    sci_seg_dict = segments.segmentlistdict()\n    sci_seg_summ_dict = segments.segmentlistdict()\n\n    for ifo in workflow.ifos:\n        curr_sci_segs, curr_sci_xml, curr_seg_name = get_sci_segs_for_ifo(ifo,\n                              workflow.cp, start_time, end_time, out_dir, tags)\n        sci_seg_dict[ifo + ':' + sci_seg_name] = curr_sci_segs\n        sci_segs[ifo] = curr_sci_segs\n        sci_seg_summ_dict[ifo + ':' + sci_seg_name] = \\\n                          curr_sci_xml.seg_summ_dict[ifo + ':' + curr_seg_name]\n    sci_seg_file = SegFile.from_segment_list_dict(sci_seg_name,\n                                          sci_seg_dict, extension='xml',\n                                          valid_segment=workflow.analysis_time,\n                                          seg_summ_dict=sci_seg_summ_dict,\n                                          directory=out_dir, tags=tags)\n    logging.info('Done generating science segments')\n    return sci_seg_file, sci_segs, sci_seg_name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the various sets of veto segments that will be used in this analysis. Parameters ----------- workflow : Workflow object Instance of the workflow object out_dir : path Location to store output files runtime_names : list Veto category groups with these names in the [workflow-segment] section of the ini file will be generated now. in_workflow_names : list Veto category groups with these names in the [workflow-segment] section of the ini file will be generated in the workflow. If a veto category appears here and in runtime_names, it will be generated now. tags : list of strings Used to retrieve subsections of the ini file for configuration options. Returns -------- veto_seg_files : FileList List of veto segment files generated", "response": "def get_files_for_vetoes(workflow, out_dir,\n                         runtime_names=None, in_workflow_names=None, tags=None):\n    \"\"\"\n    Get the various sets of veto segments that will be used in this analysis.\n\n    Parameters\n    -----------\n    workflow : Workflow object\n        Instance of the workflow object\n    out_dir : path\n        Location to store output files\n    runtime_names : list\n        Veto category groups with these names in the [workflow-segment] section\n        of the ini file will be generated now.\n    in_workflow_names : list\n        Veto category groups with these names in the [workflow-segment] section\n        of the ini file will be generated in the workflow. If a veto category\n        appears here and in runtime_names, it will be generated now.\n    tags : list of strings\n        Used to retrieve subsections of the ini file for\n        configuration options.\n\n    Returns\n    --------\n    veto_seg_files : FileList\n        List of veto segment files generated\n    \"\"\"\n    if tags is None:\n        tags = []\n    if runtime_names is None:\n        runtime_names = []\n    if in_workflow_names is None:\n        in_workflow_names = []\n    logging.info('Starting generating veto files for analysis')\n    make_analysis_dir(out_dir)\n    start_time = workflow.analysis_time[0]\n    end_time = workflow.analysis_time[1]\n    save_veto_definer(workflow.cp, out_dir, tags)\n\n    now_cat_sets = []\n    for name in runtime_names:\n        cat_sets = parse_cat_ini_opt(workflow.cp.get_opt_tags(\n                                              'workflow-segments', name, tags))\n        now_cat_sets.extend(cat_sets)\n\n    now_cats = set()\n    for cset in now_cat_sets:\n        now_cats = now_cats.union(cset)\n\n    later_cat_sets = []\n    for name in in_workflow_names:\n        cat_sets = parse_cat_ini_opt(workflow.cp.get_opt_tags(\n                                              'workflow-segments', name, tags))\n        later_cat_sets.extend(cat_sets)\n\n    later_cats = set()\n    for cset in later_cat_sets:\n        later_cats = later_cats.union(cset)\n        # Avoid duplication\n        later_cats = later_cats - now_cats\n\n    veto_gen_job = create_segs_from_cats_job(workflow.cp, out_dir,\n                                             workflow.ifo_string, tags=tags)\n\n    cat_files = FileList()\n    for ifo in workflow.ifos:\n        for category in now_cats:\n            cat_files.append(get_veto_segs(workflow, ifo,\n                                        cat_to_veto_def_cat(category),\n                                        start_time, end_time, out_dir,\n                                        veto_gen_job, execute_now=True,\n                                        tags=tags))\n\n        for category in later_cats:\n            cat_files.append(get_veto_segs(workflow, ifo,\n                                        cat_to_veto_def_cat(category),\n                                        start_time, end_time, out_dir,\n                                        veto_gen_job, tags=tags,\n                                        execute_now=False))\n\n    logging.info('Done generating veto segments')\n    return cat_files"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the analyzable segments for the given set of science segments.", "response": "def get_analyzable_segments(workflow, sci_segs, cat_files, out_dir, tags=None):\n    \"\"\"\n    Get the analyzable segments after applying ini specified vetoes and any\n    other restrictions on the science segs, e.g. a minimum segment length, or\n    demanding that only coincident segments are analysed.\n\n    Parameters\n    -----------\n    workflow : Workflow object\n        Instance of the workflow object\n    sci_segs : Ifo-keyed dictionary of glue.segmentlists\n        The science segments for each ifo to which the vetoes, or any other\n        restriction, will be applied.\n    cat_files : FileList of SegFiles\n        The category veto files generated by get_veto_segs\n    out_dir : path\n        Location to store output files\n    tags : list of strings\n        Used to retrieve subsections of the ini file for\n        configuration options.\n\n    Returns\n    --------\n    sci_ok_seg_file : workflow.core.SegFile instance\n        The segment file combined from all ifos containing the analyzable\n        science segments.\n    sci_ok_segs : Ifo keyed dict of ligo.segments.segmentlist instances\n        The analyzable science segs for each ifo, keyed by ifo\n    sci_ok_seg_name : str\n        The name with which analyzable science segs are stored in the output\n        XML file.\n    \"\"\"\n    if tags is None:\n        tags = []\n    logging.info('Starting reducing to analysable science segments')\n\n    make_analysis_dir(out_dir)\n    # NOTE: Should this be overrideable in the config file?\n    sci_ok_seg_name = \"SCIENCE_OK\"\n    sci_ok_seg_dict = segments.segmentlistdict()\n    sci_ok_segs = {}\n\n    cat_sets = parse_cat_ini_opt(workflow.cp.get_opt_tags('workflow-segments',\n                                                'segments-science-veto', tags))\n    if len(cat_sets) > 1:\n        raise ValueError('Provide only 1 category group to determine'\n                         ' analyzable segments')\n    cat_set = cat_sets[0]\n\n    for ifo in workflow.ifos:\n        curr_segs = copy.copy(sci_segs[ifo])\n        files = cat_files.find_output_with_ifo(ifo)\n        for category in cat_set:\n            veto_def_cat = cat_to_veto_def_cat(category)\n            file_list = files.find_output_with_tag('VETO_CAT%d' %(veto_def_cat))\n            if len(file_list) > 1:\n                err_msg = \"Found more than one veto file for %s \" %(ifo,)\n                err_msg += \"and category %s.\" %(category,)\n                raise ValueError(err_msg)\n            if len(file_list) == 0:\n                err_msg = \"Found no veto files for %s \" %(ifo,)\n                err_msg += \"and category %s.\" %(category,)\n                raise ValueError(err_msg)\n            curr_veto_file = file_list[0]\n            cat_segs = curr_veto_file.return_union_seglist()\n            curr_segs -= cat_segs\n            curr_segs.coalesce()\n        sci_ok_seg_dict[ifo + ':' + sci_ok_seg_name] = curr_segs\n\n    sci_ok_seg_file = SegFile.from_segment_list_dict(sci_ok_seg_name,\n                                          sci_ok_seg_dict, extension='xml',\n                                          valid_segment=workflow.analysis_time,\n                                          directory=out_dir, tags=tags)\n\n\n    if workflow.cp.has_option_tags(\"workflow-segments\",\n                          \"segments-minimum-segment-length\", tags):\n        min_seg_length = int( workflow.cp.get_opt_tags(\"workflow-segments\",\n                              \"segments-minimum-segment-length\", tags) )\n        sci_ok_seg_file.remove_short_sci_segs(min_seg_length)\n\n    # FIXME: Another test we can do is limit to coinc time +/- some window\n    #        this should *not* be set through segments-method, but currently\n    #        is not implemented\n    #segments_method = workflow.cp.get_opt_tags(\"workflow-segments\",\n    #                                  \"segments-method\", tags)\n    #if segments_method == 'ALL_SINGLE_IFO_TIME':\n    #    pass\n    #elif segments_method == 'COINC_TIME':\n    #    cum_segs = None\n    #    for ifo in sci_segs:\n    #        if cum_segs is not None:\n    #            cum_segs = (cum_segs & sci_segs[ifo]).coalesce()\n    #        else:\n    #            cum_segs = sci_segs[ifo]\n    #\n    #    for ifo in sci_segs:\n    #        sci_segs[ifo] = cum_segs\n    #else:\n    #    raise ValueError(\"Invalid segments-method, %s. Options are \"\n    #                     \"ALL_SINGLE_IFO_TIME and COINC_TIME\" % segments_method)\n\n    for ifo in workflow.ifos:\n        sci_ok_segs[ifo] = \\\n                      sci_ok_seg_file.segment_dict[ifo + ':' + sci_ok_seg_name]\n\n    logging.info('Done generating analyzable science segments')\n    return sci_ok_seg_file, sci_ok_segs, sci_ok_seg_name"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the cumulative veto files that define different backgrounds for each veto group.", "response": "def get_cumulative_veto_group_files(workflow, option, cat_files,\n                                    out_dir, execute_now=True, tags=None):\n    \"\"\"\n    Get the cumulative veto files that define the different backgrounds\n    we want to analyze, defined by groups of vetos.\n\n    Parameters\n    -----------\n    workflow : Workflow object\n        Instance of the workflow object\n    option : str\n        ini file option to use to get the veto groups\n    cat_files : FileList of SegFiles\n        The category veto files generated by get_veto_segs\n    out_dir : path\n        Location to store output files\n    execute_now : Boolean\n        If true outputs are generated at runtime. Else jobs go into the workflow\n        and are generated then.\n    tags : list of strings\n        Used to retrieve subsections of the ini file for\n        configuration options.\n\n    Returns\n    --------\n    seg_files : workflow.core.FileList instance\n        The cumulative segment files for each veto group.\n    names : list of strings\n        The segment names for the corresponding seg_file\n    cat_files : workflow.core.FileList instance\n        The list of individual category veto files\n    \"\"\"\n    if tags is None:\n        tags = []\n    logging.info(\"Starting generating vetoes for groups in %s\" %(option))\n    make_analysis_dir(out_dir)\n\n    cat_sets = parse_cat_ini_opt(workflow.cp.get_opt_tags('workflow-segments',\n                                            option, tags))\n\n    cum_seg_files = FileList()\n    names = []\n    for cat_set in cat_sets:\n        segment_name = \"CUMULATIVE_CAT_%s\" % (''.join(sorted(cat_set)))\n        logging.info('getting information for %s' % segment_name)\n        categories = [cat_to_veto_def_cat(c) for c in cat_set]\n\n        cum_seg_files += [get_cumulative_segs(workflow, categories, cat_files,\n                          out_dir, execute_now=execute_now,\n                          segment_name=segment_name, tags=tags)]\n        names.append(segment_name)\n\n    logging.info(\"Done generating vetoes for groups in %s\" %(option))\n\n    return cum_seg_files, names, cat_files"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setup_segment_generation(workflow, out_dir, tag=None):\n    logging.info(\"Entering segment generation module\")\n    make_analysis_dir(out_dir)\n\n    cp = workflow.cp\n\n    # Parse for options in ini file\n    segmentsMethod = cp.get_opt_tags(\"workflow-segments\",\n                                     \"segments-method\", [tag])\n    # These only needed if calling setup_segment_gen_mixed\n    if segmentsMethod in ['AT_RUNTIME','CAT2_PLUS_DAG','CAT3_PLUS_DAG',\n                          'CAT4_PLUS_DAG']:\n        veto_cats = cp.get_opt_tags(\"workflow-segments\",\n                                    \"segments-veto-categories\", [tag])\n        max_veto_cat = max([int(c) for c in veto_cats.split(',')])\n        veto_categories = range(1, max_veto_cat + 1)\n        if cp.has_option_tags(\"workflow-segments\",\n                              \"segments-generate-coincident-segments\", [tag]):\n            generate_coincident_segs = True\n        else:\n            generate_coincident_segs = False\n        # Need to curl the veto-definer file\n        vetoDefUrl = cp.get_opt_tags(\"workflow-segments\",\n                                     \"segments-veto-definer-url\", [tag])\n        vetoDefBaseName = os.path.basename(vetoDefUrl)\n        vetoDefNewPath = os.path.join(out_dir, vetoDefBaseName)\n        resolve_url(vetoDefUrl,out_dir)\n        # and update location\n        cp.set(\"workflow-segments\", \"segments-veto-definer-file\",\n                vetoDefNewPath)\n\n\n    if cp.has_option_tags(\"workflow-segments\",\n                          \"segments-minimum-segment-length\", [tag]):\n        minSegLength = int( cp.get_opt_tags(\"workflow-segments\",\n                           \"segments-minimum-segment-length\", [tag]) )\n    else:\n        minSegLength = 0\n\n    if segmentsMethod == \"AT_RUNTIME\":\n        max_veto = 1000\n    elif segmentsMethod == \"CAT2_PLUS_DAG\":\n        max_veto = 1\n    elif segmentsMethod == \"CAT3_PLUS_DAG\":\n        max_veto = 2\n    elif segmentsMethod == \"CAT4_PLUS_DAG\":\n        max_veto = 3\n    else:\n        msg = \"Entry segments-method in [workflow-segments] does not have \"\n        msg += \"expected value. Valid values are AT_RUNTIME, CAT4_PLUS_DAG, \"\n        msg += \"CAT2_PLUS_DAG or CAT3_PLUS_DAG.\"\n        raise ValueError(msg)\n\n    logging.info(\"Generating segments with setup_segment_gen_mixed\")\n    segFilesList = setup_segment_gen_mixed(workflow, veto_categories,\n                             out_dir, max_veto, tag=tag,\n                             generate_coincident_segs=generate_coincident_segs)\n    logging.info(\"Segments obtained\")\n\n    # This creates the segsToAnalyse from the segFilesList. Currently it uses\n    # the 'SCIENCE_OK' segFilesList, which is science - CAT_1 in\n    # setup_segment_gen_mixed.\n    # This also applies the minimum science length\n    segsToAnalyse = {}\n    for ifo in workflow.ifos:\n        analSegs = segFilesList.find_output_with_ifo(ifo)\n        analSegs = analSegs.find_output_with_tag('SCIENCE_OK')\n        assert len(analSegs) == 1\n        analSegs = analSegs[0]\n        if analSegs.segment_list:\n            if minSegLength:\n                analSegs.remove_short_sci_segs(minSegLength)\n                analSegs.to_segment_xml(override_file_if_exists=True)\n            segsToAnalyse[ifo] = analSegs.segment_list\n        else:\n            msg = \"No science segments found for ifo %s. \" %(ifo)\n            msg += \"If this is unexpected check the files that were dumped \"\n            msg += \"in the %s directory. Also the \" %(out_dir)\n            msg += \"commands that can be used to reproduce some of these \"\n            msg += \"in %s/*.sh\" %(os.path.join(out_dir,'logs'))\n            logging.warn(msg)\n\n    logging.info(\"Leaving segment generation module\")\n    return segsToAnalyse, segFilesList", "response": "This function is used to setup the segment generation module. It is used to generate the segment files for the current code and generate the files for the current code."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nobtains science segments for the selected ifo Parameters ----------- ifo : string The string describing the ifo to obtain science times for. start_time : gps time (either int/LIGOTimeGPS) The time at which to begin searching for segments. end_time : gps time (either int/LIGOTimeGPS) The time at which to stop searching for segments. out_dir : path The directory in which output will be stored. tag : string, optional (default=None) Use this to specify a tag. This can be used if this module is being called more than once to give call specific configuration (by setting options in [workflow-datafind-${TAG}] rather than [workflow-datafind]). This is also used to tag the Files returned by the class to uniqueify the Files and uniqueify the actual filename. Returns -------- sci_segs : ligo.segments.segmentlist The segmentlist generated by this call sci_xml_file : pycbc.workflow.core.SegFile The workflow File object corresponding to this science segments file. out_sci_seg_name : string The name of the output segment list in the output XML file.", "response": "def get_sci_segs_for_ifo(ifo, cp, start_time, end_time, out_dir, tags=None):\n    \"\"\"\n    Obtain science segments for the selected ifo\n\n    Parameters\n    -----------\n    ifo : string\n        The string describing the ifo to obtain science times for.\n    start_time : gps time (either int/LIGOTimeGPS)\n        The time at which to begin searching for segments.\n    end_time : gps time (either int/LIGOTimeGPS)\n        The time at which to stop searching for segments.\n    out_dir : path\n        The directory in which output will be stored.\n    tag : string, optional (default=None)\n        Use this to specify a tag. This can be used if this module is being\n        called more than once to give call specific configuration (by setting\n        options in [workflow-datafind-${TAG}] rather than [workflow-datafind]).\n        This is also used to tag the Files returned by the class to uniqueify\n        the Files and uniqueify the actual filename.\n\n    Returns\n    --------\n    sci_segs : ligo.segments.segmentlist\n        The segmentlist generated by this call\n    sci_xml_file : pycbc.workflow.core.SegFile\n        The workflow File object corresponding to this science segments file.\n    out_sci_seg_name : string\n        The name of the output segment list in the output XML file.\n\n    \"\"\"\n    if tags is None:\n        tags = []\n    seg_valid_seg = segments.segment([start_time,end_time])\n    sci_seg_name = cp.get_opt_tags(\n        \"workflow-segments\", \"segments-%s-science-name\" %(ifo.lower()), tags)\n    sci_seg_url = cp.get_opt_tags(\n        \"workflow-segments\", \"segments-database-url\", tags)\n    # NOTE: ligolw_segment_query returns slightly strange output. The output\n    #       segment list is put in with name \"RESULT\". So this is hardcoded here\n    out_sci_seg_name = \"RESULT\"\n    if tags:\n        sci_xml_file_path = os.path.join(\n            out_dir, \"%s-SCIENCE_SEGMENTS_%s.xml\" \\\n                     %(ifo.upper(), '_'.join(tags)))\n        tag_list=tags + ['SCIENCE']\n    else:\n        sci_xml_file_path = os.path.join(\n            out_dir, \"%s-SCIENCE_SEGMENTS.xml\" %(ifo.upper()) )\n        tag_list = ['SCIENCE']\n\n    if file_needs_generating(sci_xml_file_path, cp, tags=tags):\n        seg_find_call = [ resolve_url(cp.get(\"executables\",\"segment_query\"),\n                permissions=stat.S_IRUSR | stat.S_IWUSR | stat.S_IXUSR),\n            \"--query-segments\",\n            \"--segment-url\", sci_seg_url,\n            \"--gps-start-time\", str(start_time),\n            \"--gps-end-time\", str(end_time),\n            \"--include-segments\", sci_seg_name,\n            \"--output-file\", sci_xml_file_path ]\n\n        make_external_call(seg_find_call, out_dir=os.path.join(out_dir,'logs'),\n                                out_basename='%s-science-call' %(ifo.lower()) )\n\n    # Yes its yucky to generate a file and then read it back in.\n    sci_xml_file_path = os.path.abspath(sci_xml_file_path)\n    sci_xml_file = SegFile.from_segment_xml(sci_xml_file_path, tags=tag_list,\n                                        valid_segment=seg_valid_seg)\n    # NOTE: ligolw_segment_query returns slightly strange output. The output\n    #       segment_summary output does not use RESULT. Therefore move the\n    #       segment_summary across.\n    sci_xml_file.seg_summ_dict[ifo.upper() + \":\" + out_sci_seg_name] = \\\n             sci_xml_file.seg_summ_dict[':'.join(sci_seg_name.split(':')[0:2])]\n\n    sci_segs = sci_xml_file.return_union_seglist()\n    return sci_segs, sci_xml_file, out_sci_seg_name"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_veto_segs(workflow, ifo, category, start_time, end_time, out_dir,\n                  veto_gen_job, tags=None, execute_now=False):\n    \"\"\"\n    Obtain veto segments for the selected ifo and veto category and add the job\n    to generate this to the workflow.\n\n    Parameters\n    -----------\n    workflow: pycbc.workflow.core.Workflow\n        An instance of the Workflow class that manages the workflow.\n    ifo : string\n        The string describing the ifo to generate vetoes for.\n    category : int\n        The veto category to generate vetoes for.\n    start_time : gps time (either int/LIGOTimeGPS)\n        The time at which to begin searching for segments.\n    end_time : gps time (either int/LIGOTimeGPS)\n        The time at which to stop searching for segments.\n    out_dir : path\n        The directory in which output will be stored.\n    vetoGenJob : Job\n        The veto generation Job class that will be used to create the Node.\n    tag : string, optional (default=None)\n        Use this to specify a tag. This can be used if this module is being\n        called more than once to give call specific configuration (by setting\n        options in [workflow-datafind-${TAG}] rather than [workflow-datafind]).\n        This is also used to tag the Files returned by the class to uniqueify\n        the Files and uniqueify the actual filename.\n        FIXME: Filenames may not be unique with current codes!\n    execute_now : boolean, optional\n        If true, jobs are executed immediately. If false, they are added to the\n        workflow to be run later.\n\n    Returns\n    --------\n    veto_def_file : pycbc.workflow.core.SegFile\n        The workflow File object corresponding to this DQ veto file.\n    \"\"\"\n    if tags is None:\n        tags = []\n    seg_valid_seg = segments.segment([start_time,end_time])\n    # FIXME: This job needs an internet connection and X509_USER_PROXY\n    #        For internet connection, it may need a headnode (ie universe local)\n    #        For X509_USER_PROXY, I don't know what pegasus is doing\n    node = Node(veto_gen_job)\n    node.add_opt('--veto-categories', str(category))\n    node.add_opt('--ifo-list', ifo)\n    node.add_opt('--gps-start-time', str(start_time))\n    node.add_opt('--gps-end-time', str(end_time))\n    if tags:\n        veto_xml_file_name = \"%s-VETOTIME_CAT%d_%s-%d-%d.xml\" \\\n                               %(ifo, category, '_'.join(tags), start_time,\n                                 end_time-start_time)\n    else:\n        veto_xml_file_name = \"%s-VETOTIME_CAT%d-%d-%d.xml\" \\\n                         %(ifo, category, start_time, end_time-start_time)\n    veto_xml_file_path = os.path.abspath(os.path.join(out_dir,\n                                         veto_xml_file_name))\n    curr_url = urlparse.urlunparse(['file', 'localhost',\n                                   veto_xml_file_path, None, None, None])\n    if tags:\n        curr_tags = tags + ['VETO_CAT%d' %(category)]\n    else:\n        curr_tags = ['VETO_CAT%d' %(category)]\n\n    if file_needs_generating(veto_xml_file_path, workflow.cp, tags=tags):\n        if execute_now:\n            workflow.execute_node(node, verbatim_exe = True)\n            veto_xml_file = SegFile.from_segment_xml(veto_xml_file_path,\n                                                 tags=curr_tags,\n                                                 valid_segment=seg_valid_seg)\n        else:\n            veto_xml_file = SegFile(ifo, 'SEGMENTS', seg_valid_seg,\n                                    file_url=curr_url, tags=curr_tags)\n            node._add_output(veto_xml_file)\n            workflow.add_node(node)\n    else:\n        node.executed = True\n        for fil in node._outputs:\n            fil.node = None\n        veto_xml_file = SegFile.from_segment_xml(veto_xml_file_path,\n                                                 tags=curr_tags,\n                                                 valid_segment=seg_valid_seg)\n    return veto_xml_file", "response": "Returns the veto segments for the selected ifo and veto category."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_segs_from_cats_job(cp, out_dir, ifo_string, tags=None):\n    if tags is None:\n        tags = []\n\n    seg_server_url = cp.get_opt_tags(\"workflow-segments\",\n                                   \"segments-database-url\", tags)\n    veto_def_file = cp.get_opt_tags(\"workflow-segments\",\n                                  \"segments-veto-definer-file\", tags)\n\n    job = Executable(cp, 'segments_from_cats', universe='local',\n                               ifos=ifo_string, out_dir=out_dir, tags=tags)\n    job.add_opt('--separate-categories')\n    job.add_opt('--segment-url', seg_server_url)\n\n    job.add_opt('--veto-file', veto_def_file)\n    # FIXME: Would like the proxy in the Workflow instance\n    # FIXME: Explore using the x509 condor commands\n    # If the user has a proxy set in the environment, add it to the job\n    return job", "response": "This function creates a CondorDAGJob that will run the segments_from_cats command."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfunctioning to generate the cumulative veto files for a given set of veto categories and seg_files_list.", "response": "def get_cumulative_segs(workflow, categories, seg_files_list, out_dir,\n                        tags=None, execute_now=False, segment_name=None):\n    \"\"\"\n    Function to generate one of the cumulative, multi-detector segment files\n    as part of the workflow.\n\n    Parameters\n    -----------\n    workflow: pycbc.workflow.core.Workflow\n        An instance of the Workflow class that manages the workflow.\n    categories : int\n        The veto categories to include in this cumulative veto.\n    seg_files_list : Listionary of SegFiles\n        The list of segment files to be used as input for combining.\n    out_dir : path\n        The directory to write output to.\n    tags : list of strings, optional\n        A list of strings that is used to identify this job\n    execute_now : boolean, optional\n        If true, jobs are executed immediately. If false, they are added to the\n        workflow to be run later.\n    segment_name : str\n        The name of the combined, cumulative segments in the output file.\n    \"\"\"\n    if tags is None:\n        tags = []\n    add_inputs = FileList([])\n    valid_segment = workflow.analysis_time\n    if segment_name is None:\n        segment_name = 'VETO_CAT%d_CUMULATIVE' % (categories[-1])\n    cp = workflow.cp\n    # calculate the cumulative veto files for a given ifo\n    for ifo in workflow.ifos:\n        cum_job = LigoLWCombineSegsExecutable(cp, 'ligolw_combine_segments',\n                       out_dir=out_dir, tags=[segment_name]+tags, ifos=ifo)\n        inputs = []\n        files = seg_files_list.find_output_with_ifo(ifo)\n        for category in categories:\n            file_list = files.find_output_with_tag('VETO_CAT%d' %(category))\n            inputs+=file_list\n\n        cum_node  = cum_job.create_node(valid_segment, inputs, segment_name)\n        if file_needs_generating(cum_node.output_files[0].cache_entry.path,\n                                 workflow.cp, tags=tags):\n            if execute_now:\n                workflow.execute_node(cum_node)\n            else:\n                workflow.add_node(cum_node)\n        else:\n            cum_node.executed = True\n            for fil in cum_node._outputs:\n                fil.node = None\n                fil.PFN(urlparse.urljoin('file:',\n                                         urllib.pathname2url(fil.storage_path)),\n                        site='local')\n        add_inputs += cum_node.output_files\n\n    # add cumulative files for each ifo together\n    name = '%s_VETO_SEGMENTS' %(segment_name)\n    outfile = File(workflow.ifos, name, workflow.analysis_time,\n                                            directory=out_dir, extension='xml',\n                                            tags=[segment_name] + tags)\n    add_job = LigolwAddExecutable(cp, 'llwadd', ifos=ifo, out_dir=out_dir,\n                                  tags=tags)\n    add_node = add_job.create_node(valid_segment, add_inputs, output=outfile)\n    if file_needs_generating(add_node.output_files[0].cache_entry.path,\n                             workflow.cp, tags=tags):\n        if execute_now:\n            workflow.execute_node(add_node)\n        else:\n            workflow.add_node(add_node)\n    else:\n        add_node.executed = True\n        for fil in add_node._outputs:\n            fil.node = None\n            fil.PFN(urlparse.urljoin('file:',\n                                     urllib.pathname2url(fil.storage_path)),\n                    site='local')\n    return outfile"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_cumulative_files(workflow, output_file, input_files, out_dir,\n                         execute_now=False, tags=None):\n    \"\"\"\n    Function to combine a set of segment files into a single one. This function\n    will not merge the segment lists but keep each separate.\n\n    Parameters\n    -----------\n    workflow: pycbc.workflow.core.Workflow\n        An instance of the Workflow class that manages the workflow.\n    output_file: pycbc.workflow.core.File\n        The output file object\n    input_files: pycbc.workflow.core.FileList\n        This list of input segment files\n    out_dir : path\n        The directory to write output to.\n    execute_now : boolean, optional\n        If true, jobs are executed immediately. If false, they are added to the\n        workflow to be run later.\n    tags : list of strings, optional\n        A list of strings that is used to identify this job\n    \"\"\"\n    if tags is None:\n        tags = []\n    llwadd_job = LigolwAddExecutable(workflow.cp, 'llwadd',\n                       ifo=output_file.ifo_list, out_dir=out_dir, tags=tags)\n    add_node = llwadd_job.create_node(output_file.segment, input_files,\n                                   output=output_file)\n    if file_needs_generating(add_node.output_files[0].cache_entry.path,\n                             workflow.cp, tags=tags):\n        if execute_now:\n            workflow.execute_node(add_node)\n        else:\n            workflow.add_node(add_node)\n    else:\n        add_node.executed = True\n        for fil in add_node._outputs:\n            fil.node = None\n            fil.PFN(urlparse.urljoin('file:',\n                                     urllib.pathname2url(fil.storage_path)),\n                    site='local')\n    return add_node.output_files[0]", "response": "This function will combine a set of input segment files into a single set of output segment files."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding playground time in a list of segments.", "response": "def find_playground_segments(segs):\n    '''Finds playground time in a list of segments.\n\n      Playground segments include the first 600s of every 6370s stride starting\n      at GPS time 729273613.\n\n      Parameters\n      ----------\n      segs : segmentfilelist\n          A segmentfilelist to find playground segments.\n\n      Returns\n      -------\n      outlist : segmentfilelist\n          A segmentfilelist with all playground segments during the input\n          segmentfilelist (ie. segs).\n    '''\n\n    # initializations\n    start_s2 = 729273613\n    playground_stride = 6370\n    playground_length = 600\n    outlist = segments.segmentlist()\n\n    # loop over segments\n    for seg in segs:\n        start = seg[0]\n        end = seg[1]\n\n        # the first playground segment whose end is after the start of seg\n        playground_start = start_s2 + playground_stride * ( 1 + \\\n                     int(start-start_s2-playground_length) / playground_stride)\n\n        while playground_start < end:\n            # find start of playground segment\n            if playground_start > start:\n                ostart = playground_start\n            else:\n                ostart = start\n\n            playground_end = playground_start + playground_length\n\n            # find end of playground segment\n            if playground_end < end:\n                oend = playground_end\n            else:\n                oend = end\n\n            # append segment\n            x = segments.segment(ostart, oend)\n            outlist.append(x)\n\n            # increment\n            playground_start = playground_start + playground_stride\n\n    return outlist"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the coherent network on and off source segments for a single IFO search.", "response": "def get_triggered_coherent_segment(workflow, sciencesegs):\n    \"\"\"\n    Construct the coherent network on and off source segments. Can switch to\n    construction of segments for a single IFO search when coherent segments\n    are insufficient for a search.\n\n    Parameters\n    -----------\n    workflow : pycbc.workflow.core.Workflow\n        The workflow instance that the calculated segments belong to.\n    sciencesegs : dict\n        Dictionary of all science segments within analysis time.\n\n    Returns\n    --------\n    onsource : ligo.segments.segmentlistdict\n        A dictionary containing the on source segments for network IFOs\n\n    offsource : ligo.segments.segmentlistdict\n        A dictionary containing the off source segments for network IFOs\n    \"\"\"\n\n    # Load parsed workflow config options\n    cp = workflow.cp\n    triggertime = int(os.path.basename(cp.get('workflow', 'trigger-time')))\n    minduration = int(os.path.basename(cp.get('workflow-exttrig_segments',\n                                              'min-duration')))\n    maxduration = int(os.path.basename(cp.get('workflow-exttrig_segments',\n                                              'max-duration')))\n    onbefore = int(os.path.basename(cp.get('workflow-exttrig_segments',\n                                           'on-before')))\n    onafter = int(os.path.basename(cp.get('workflow-exttrig_segments',\n                                          'on-after')))\n    padding = int(os.path.basename(cp.get('workflow-exttrig_segments',\n                                          'pad-data')))\n    if cp.has_option(\"workflow-condition_strain\", \"do-gating\"):\n        padding += int(os.path.basename(cp.get(\"condition_strain\",\n                                               \"pad-data\")))\n    quanta = int(os.path.basename(cp.get('workflow-exttrig_segments',\n                                         'quanta')))\n\n    # Check available data segments meet criteria specified in arguments\n    commonsegs = sciencesegs.extract_common(sciencesegs.keys())\n    offsrclist = commonsegs[commonsegs.keys()[0]]\n    if len(offsrclist) > 1:\n        logging.info(\"Removing network segments that do not contain trigger \"\n                     \"time\")\n        for seg in offsrclist:\n            if triggertime in seg:\n                offsrc = seg\n    else:\n        offsrc = offsrclist[0]\n\n    if abs(offsrc) < minduration + 2 * padding:\n        fail = segments.segment([triggertime - minduration / 2. - padding,\n                                 triggertime + minduration / 2. + padding])\n        logging.warning(\"Available network segment shorter than minimum \"\n                        \"allowed duration.\")\n        return None, fail\n\n    # Will segment duration be the maximum desired length or not?\n    if abs(offsrc) >= maxduration + 2 * padding:\n        logging.info(\"Available network science segment duration (%ds) is \"\n                     \"greater than the maximum allowed segment length (%ds). \"\n                     \"Truncating...\" % (abs(offsrc), maxduration))\n    else:\n        logging.info(\"Available network science segment duration (%ds) is \"\n                     \"less than the maximum allowed segment length (%ds).\"\n                     % (abs(offsrc), maxduration))\n\n    logging.info(\"%ds of padding applied at beginning and end of segment.\"\n                 % padding)\n\n\n    # Construct on-source\n    onstart = triggertime - onbefore\n    onend = triggertime + onafter\n    oncentre = onstart + ((onbefore + onafter) / 2)\n    onsrc = segments.segment(onstart, onend)\n    logging.info(\"Constructed ON-SOURCE: duration %ds (%ds before to %ds after\"\n                 \" trigger).\"\n                 % (abs(onsrc), triggertime - onsrc[0],\n                    onsrc[1] - triggertime))\n    onsrc = segments.segmentlist([onsrc])\n\n    # Maximal, centred coherent network segment\n    idealsegment = segments.segment(int(oncentre - padding -\n                                    0.5 * maxduration),\n                                    int(oncentre + padding +\n                                    0.5 * maxduration))\n\n    # Construct off-source\n    if (idealsegment in offsrc):\n        offsrc = idealsegment\n\n    elif idealsegment[1] not in offsrc:\n        offsrc &= segments.segment(offsrc[1] - maxduration - 2 * padding,\n                                   offsrc[1])\n\n    elif idealsegment[0] not in offsrc:\n        offsrc &= segments.segment(offsrc[0],\n                                   offsrc[0] + maxduration + 2 * padding)\n\n    # Trimming off-source\n    excess = (abs(offsrc) - 2 * padding) % quanta\n    if excess != 0:\n        logging.info(\"Trimming %ds excess time to make OFF-SOURCE duration a \"\n                     \"multiple of %ds\" % (excess, quanta))\n        offset = (offsrc[0] + abs(offsrc) / 2.) - oncentre\n        if 2 * abs(offset) > excess:\n            if offset < 0:\n                offsrc &= segments.segment(offsrc[0] + excess,\n                                           offsrc[1])\n            elif offset > 0:\n                offsrc &= segments.segment(offsrc[0],\n                                           offsrc[1] - excess)\n            assert abs(offsrc) % quanta == 2 * padding\n        else:\n            logging.info(\"This will make OFF-SOURCE symmetrical about trigger \"\n                         \"time.\")\n            start = int(offsrc[0] - offset + excess / 2)\n            end = int(offsrc[1] - offset - round(float(excess) / 2))\n            offsrc = segments.segment(start, end)\n            assert abs(offsrc) % quanta == 2 * padding\n\n    logging.info(\"Constructed OFF-SOURCE: duration %ds (%ds before to %ds \"\n                 \"after trigger).\"\n                 % (abs(offsrc) - 2 * padding,\n                    triggertime - offsrc[0] - padding,\n                    offsrc[1] - triggertime - padding))\n    offsrc = segments.segmentlist([offsrc])\n\n    # Put segments into segmentlistdicts\n    onsource = segments.segmentlistdict()\n    offsource = segments.segmentlistdict()\n    ifos = ''\n    for iifo in sciencesegs.keys():\n        ifos += str(iifo)\n        onsource[iifo] = onsrc\n        offsource[iifo] = offsrc\n\n    return onsource, offsource"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef save_veto_definer(cp, out_dir, tags=None):\n    if tags is None:\n        tags = []\n    make_analysis_dir(out_dir)\n    veto_def_url = cp.get_opt_tags(\"workflow-segments\",\n                                 \"segments-veto-definer-url\", tags)\n    veto_def_base_name = os.path.basename(veto_def_url)\n    veto_def_new_path = os.path.abspath(os.path.join(out_dir,\n                                        veto_def_base_name))\n    # Don't need to do this if already done\n    resolve_url(veto_def_url,out_dir)\n\n    # and update location\n    cp.set(\"workflow-segments\", \"segments-veto-definer-file\", veto_def_new_path)\n    return veto_def_new_path", "response": "Retrieve the veto definer file and save it locally"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_cat_ini_opt(cat_str):\n    if cat_str == \"\":\n        return []\n\n    cat_groups = cat_str.split(',')\n    cat_sets = []\n    for group in cat_groups:\n        group = group.strip()\n        cat_sets += [set(c for c in group)]\n    return cat_sets", "response": "Parse a cat str from the ini file into a list of sets"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef file_needs_generating(file_path, cp, tags=None):\n    if tags is None:\n        tags = []\n    if cp.has_option_tags(\"workflow-segments\",\n                          \"segments-generate-segment-files\", tags):\n        value = cp.get_opt_tags(\"workflow-segments\",\n                                   \"segments-generate-segment-files\", tags)\n        generate_segment_files = value\n    else:\n        generate_segment_files = 'always'\n\n    # Does the file exist\n    if os.path.isfile(file_path):\n        if generate_segment_files in ['if_not_present', 'never']:\n            return 0\n        elif generate_segment_files == 'always':\n            err_msg = \"File %s already exists. \" %(file_path,)\n            err_msg += \"Regenerating and overwriting.\"\n            logging.warn(err_msg)\n            return 1\n        elif generate_segment_files == 'error_on_duplicate':\n            err_msg = \"File %s already exists. \" %(file_path,)\n            err_msg += \"Refusing to overwrite file and exiting.\"\n            raise ValueError(err_msg)\n        else:\n            err_msg = 'Global variable generate_segment_files must be one of '\n            err_msg += '\"always\", \"if_not_present\", \"error_on_duplicate\", '\n            err_msg += '\"never\". Got %s.' %(generate_segment_files,)\n            raise ValueError(err_msg)\n    else:\n        if generate_segment_files in ['always', 'if_not_present',\n                                      'error_on_duplicate']:\n            return 1\n        elif generate_segment_files == 'never':\n            err_msg = 'File %s does not exist. ' %(file_path,)\n            raise ValueError(err_msg)\n        else:\n            err_msg = 'Global variable generate_segment_files must be one of '\n            err_msg += '\"always\", \"if_not_present\", \"error_on_duplicate\", '\n            err_msg += '\"never\". Got %s.' %(generate_segment_files,)\n            raise ValueError(err_msg)", "response": "This function checks the file location and determines if the file needs generating now or not."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the cumulative segments from the option name syntax for each ifo.", "response": "def get_segments_file(workflow, name, option_name, out_dir):\n    \"\"\"Get cumulative segments from option name syntax for each ifo.\n\n    Use syntax of configparser string to define the resulting segment_file\n    e.x. option_name = +up_flag1,+up_flag2,+up_flag3,-down_flag1,-down_flag2\n    Each ifo may have a different string and is stored separately in the file.\n    Flags which add time must precede flags which subtract time.\n\n    Parameters\n    ----------\n    workflow: pycbc.workflow.Workflow\n    name: string\n        Name of the segment list being created\n    option_name: str\n        Name of option in the associated config parser to get the flag list\n\n    returns\n    --------\n    seg_file: pycbc.workflow.SegFile\n        SegFile intance that points to the segment xml file on disk.\n    \"\"\"\n    from pycbc.dq import query_str\n    make_analysis_dir(out_dir)\n    cp = workflow.cp\n    start = workflow.analysis_time[0]\n    end = workflow.analysis_time[1]\n\n    # Check for veto definer file\n    veto_definer = None\n    if cp.has_option(\"workflow-segments\", \"segments-veto-definer-url\"):\n        veto_definer = save_veto_definer(workflow.cp, out_dir, [])\n\n    # Check for provided server\n    server = \"segments.ligo.org\"\n    if cp.has_option(\"workflow-segments\", \"segments-database-url\"):\n        server = cp.get(\"workflow-segments\",\n                                 \"segments-database-url\")\n\n    segs = {}\n    for ifo in workflow.ifos:\n        flag_str = cp.get_opt_tags(\"workflow-segments\", option_name, [ifo])\n        key = ifo + ':' + name\n        segs[key] = query_str(ifo, flag_str, start, end,\n                              server=server,\n                              veto_definer=veto_definer)\n        logging.info(\"%s: got %s flags\", ifo, option_name)\n\n    return SegFile.from_segment_list_dict(name, segs,\n                                          extension='.xml',\n                                          valid_segment=workflow.analysis_time,\n                                          directory=out_dir)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_swstat_bits(frame_filenames, swstat_channel_name, start_time, end_time):\n    ''' This function just checks the first time in the SWSTAT channel\n    to see if the filter was on, it doesn't check times beyond that.\n\n    This is just for a first test on a small chunck of data.\n\n    To read the SWSTAT bits, reference: https://dcc.ligo.org/DocDB/0107/T1300711/001/LIGO-T1300711-v1.pdf\n\n    Bit 0-9 = Filter on/off switches for the 10 filters in an SFM.\n    Bit 10 = Filter module input switch on/off\n    Bit 11 = Filter module offset switch on/off\n    Bit 12 = Filter module output switch on/off\n    Bit 13 = Filter module limit switch on/off\n    Bit 14 = Filter module history reset momentary switch\n    '''\n\n    # read frames\n    swstat = frame.read_frame(frame_filenames, swstat_channel_name,\n                      start_time=start_time, end_time=end_time)\n\n    # convert number in channel to binary\n    bits = bin(int(swstat[0]))\n\n    # check if filterbank input or output was off\n    filterbank_off = False\n    if len(bits) < 14 or int(bits[-13]) == 0 or int(bits[-11]) == 0:\n        filterbank_off = True\n\n    return bits[-10:], filterbank_off", "response": "This function reads the SWSTAT channel and returns the SWSTAT bits."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfilter the data with the filter module.", "response": "def filter_data(data, filter_name, filter_file, bits, filterbank_off=False,\n                    swstat_channel_name=None):\n    '''\n    A naive function to determine if the filter was on at the time\n    and then filter the data.\n    '''\n\n    # if filterbank is off then return a time series of zeroes\n    if filterbank_off:\n        return numpy.zeros(len(data))\n\n    # loop over the 10 filters in the filterbank\n    for i in range(10):\n\n        # read the filter\n        filter = Filter(filter_file[filter_name][i])\n\n        # if bit is on then filter the data\n        bit = int(bits[-(i+1)])\n        if bit:\n            logging.info('filtering with filter module %d', i)\n\n            # if there are second-order sections then filter with them\n            if len(filter.sections):\n                data = filter.apply(data)\n\n            # else it is a filter with only gain so apply the gain\n            else:\n                coeffs = iir2z(filter_file[filter_name][i])\n                if len(coeffs) > 1:\n                    logging.info('Gain-only filter module return more than one number')\n                    sys.exit()\n                gain = coeffs[0]\n                data = gain * data\n\n    return  data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread the gain from the frames.", "response": "def read_gain_from_frames(frame_filenames, gain_channel_name, start_time, end_time):\n    '''\n    Returns the gain from the file.\n    '''\n\n    # get timeseries from frame\n    gain = frame.read_frame(frame_filenames, gain_channel_name,\n                      start_time=start_time, end_time=end_time)\n\n    return gain[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_from_config(cp, model, **kwargs):\n    name = cp.get('sampler', 'name')\n    return samplers[name].from_config(cp, model, **kwargs)", "response": "Loads a sampler from the given config file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef qplane(qplane_tile_dict, fseries, return_complex=False):\n    # store q-transforms for each q in a dict\n    qplanes = {}\n    max_energy, max_key = None, None\n    for i, q in enumerate(qplane_tile_dict):\n        energies = []\n        for f0 in qplane_tile_dict[q]:\n            energy = qseries(fseries, q, f0, return_complex=return_complex)\n            menergy = abs(energy).max()\n            energies.append(energy)\n\n            if i == 0 or menergy > max_energy:\n                max_energy = menergy\n                max_key = q\n\n        qplanes[q] = energies\n\n    # record q-transform output for peak q\n    plane = qplanes[max_key]\n    frequencies = qplane_tile_dict[max_key]\n    times = plane[0].sample_times.numpy()\n    plane = numpy.array([v.numpy() for v in plane])\n    return max_key, times, frequencies, numpy.array(plane)", "response": "Performs q - transform on each q - tile and selects\n       tile with the maximum energy."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef qtiling(fseries, qrange, frange, mismatch=0.2):\n    qplane_tile_dict = {}\n    qs = list(_iter_qs(qrange, deltam_f(mismatch)))\n    for q in qs:\n        qtilefreq = _iter_frequencies(q, frange, mismatch, fseries.duration)\n        qplane_tile_dict[q] = numpy.array(list(qtilefreq))\n\n    return qplane_tile_dict", "response": "Returns a dictionary containing QTile tuples for a set of Q - planes."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\niterates over the frequency of this QPlane.", "response": "def _iter_frequencies(q, frange, mismatch, dur):\n    \"\"\"Iterate over the frequencies of this 'QPlane'\n\n    Parameters\n    ----------\n    q:\n        q value\n    frange: 'list'\n        upper and lower bounds of frequency range\n    mismatch:\n        percentage of desired fractional mismatch\n    dur:\n        duration of timeseries in seconds\n\n    Returns\n    -------\n    frequencies:\n        Q-Tile frequency\n    \"\"\"\n    # work out how many frequencies we need\n    minf, maxf = frange\n    fcum_mismatch = log(float(maxf) / minf) * (2 + q**2)**(1/2.) / 2.\n    nfreq = int(max(1, ceil(fcum_mismatch / deltam_f(mismatch))))\n    fstep = fcum_mismatch / nfreq\n    fstepmin = 1. / dur\n    # for each frequency, yield a QTile\n    for i in xrange(nfreq):\n        yield (float(minf) *\n               exp(2 / (2 + q**2)**(1/2.) * (i + .5) * fstep) //\n               fstepmin * fstepmin)\n    raise StopIteration()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef qseries(fseries, Q, f0, return_complex=False):\n    # normalize and generate bi-square window\n    qprime = Q / 11**(1/2.)\n    norm = numpy.sqrt(315. * qprime / (128. * f0))\n    window_size = 2 * int(f0 / qprime * fseries.duration) + 1\n    xfrequencies = numpy.linspace(-1., 1., window_size)\n\n    start = int((f0 - (f0 / qprime)) * fseries.duration)\n    end = int(start + window_size)\n    center = (start + end) / 2\n\n    windowed = fseries[start:end] * (1 - xfrequencies ** 2) ** 2 * norm\n\n    tlen = (len(fseries)-1) * 2\n    windowed.resize(tlen)\n    windowed.roll(-center)\n\n    # calculate the time series for this q -value\n    windowed = FrequencySeries(windowed, delta_f=fseries.delta_f,\n                            epoch=fseries.start_time)\n    ctseries = TimeSeries(zeros(tlen, dtype=numpy.complex128),\n                            delta_t=fseries.delta_t)\n    ifft(windowed, ctseries)\n\n    if return_complex:\n        return ctseries\n    else:\n        energy = ctseries.squared_norm()\n        medianenergy = numpy.median(energy.numpy())\n        return  energy / float(medianenergy)", "response": "Calculate the energy time series for the given frequency - series Q value f0"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning the given command and return the output as a string.", "response": "def call(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n         on_error='ignore', returncode=False):\n    \"\"\"Run the given command (with shell=False) and return the output as a\n    string.\n\n    Strips the output of enclosing whitespace.\n\n    If the return code is non-zero, throw GitInvocationError.\n    \"\"\"\n    # start external command process\n    p = subprocess.Popen(command, stdout=subprocess.PIPE,\n                         stderr=subprocess.PIPE)\n\n    # get outputs\n    out, _ = p.communicate()\n\n    # throw exception if process failed\n    if p.returncode != 0 and on_error == 'raise':\n        raise GitInvocationError('Failed to run \"%s\"' % \" \".join(command))\n\n    if returncode:\n        return out.strip(), p.returncode\n    else:\n        return out.strip()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding the username of the current builder", "response": "def get_build_name(git_path='git'):\n    \"\"\"Find the username of the current builder\n    \"\"\"\n    name,retcode = call(('git', 'config', 'user.name'), returncode=True)\n    if retcode:\n        name = \"Unknown User\"\n    email,retcode = call(('git', 'config', 'user.email'), returncode=True)\n    if retcode:\n        email = \"\"\n    return \"%s <%s>\" % (name, email)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_last_commit(git_path='git'):\n    hash_, udate, aname, amail, cname, cmail = (\n        call((git_path, 'log', '-1',\n              '--pretty=format:%H,%ct,%an,%ae,%cn,%ce')).split(\",\"))\n    date = time.strftime('%Y-%m-%d %H:%M:%S +0000', time.gmtime(float(udate)))\n    author = '%s <%s>' % (aname, amail)\n    committer = '%s <%s>' % (cname, cmail)\n    return hash_, date, author, committer", "response": "Returns the details of the last git commit"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the name of the current git branch", "response": "def get_git_branch(git_path='git'):\n    \"\"\"Returns the name of the current git branch\n    \"\"\"\n    branch_match = call((git_path, 'rev-parse', '--symbolic-full-name', 'HEAD'))\n    if branch_match == \"HEAD\":\n        return None\n    else:\n        return os.path.basename(branch_match)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the name of the current git tag", "response": "def get_git_tag(hash_, git_path='git'):\n    \"\"\"Returns the name of the current git tag\n    \"\"\"\n    tag, status = call((git_path, 'describe', '--exact-match',\n                        '--tags', hash_), returncode=True)\n    if status == 0:\n        return tag\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the state of the git working copy", "response": "def get_git_status(git_path='git'):\n    \"\"\"Returns the state of the git working copy\n    \"\"\"\n    status_output = subprocess.call((git_path, 'diff-files', '--quiet'))\n    if status_output != 0:\n        return 'UNCLEAN: Modified working tree'\n    else:\n        # check index for changes\n        status_output = subprocess.call((git_path, 'diff-index', '--cached',\n                                         '--quiet', 'HEAD'))\n        if status_output != 0:\n            return 'UNCLEAN: Modified index'\n        else:\n            return 'CLEAN: All modifications committed'"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nqueries the git repository information to generate a version module.", "response": "def generate_git_version_info():\n    \"\"\"Query the git repository information to generate a version module.\n    \"\"\"\n    info = GitInfo()\n    git_path = call(('which', 'git'))\n\n    # get build info\n    info.builder = get_build_name()\n    info.build_date = get_build_date()\n\n    # parse git ID\n    info.hash, info.date, info.author, info.committer = (\n        get_last_commit(git_path))\n\n    # determine branch\n    info.branch = get_git_branch(git_path)\n\n    # determine tag\n    info.tag = get_git_tag(info.hash, git_path)\n\n    # determine version\n    if info.tag:\n        info.version = info.tag.strip('v')\n        info.release = not re.search('[a-z]', info.version.lower())\n    else:\n        info.version = info.hash[:6]\n        info.release = False\n\n    # Determine *last* stable release\n    info.last_release = determine_latest_release_version()\n\n    # refresh index\n    call((git_path, 'update-index', '-q', '--refresh'))\n\n    # check working copy for changes\n    info.status = get_git_status(git_path)\n\n    return info"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef values(self, stilde, template, psd, snrv, snr_norm,\n                     bchisq, bchisq_dof, indices):\n        \"\"\" Calculate sine-Gaussian chisq\n\n        Parameters\n        ----------\n        stilde: pycbc.types.Frequencyseries\n            The overwhitened strain\n        template: pycbc.types.Frequencyseries\n            The waveform template being analyzed\n        psd: pycbc.types.Frequencyseries\n            The power spectral density of the data\n        snrv: numpy.ndarray\n            The peak unnormalized complex SNR values\n        snr_norm: float\n            The normalization factor for the snr\n        bchisq: numpy.ndarray\n            The Bruce Allen power chisq values for these triggers\n        bchisq_dof: numpy.ndarray\n            The degrees of freedom of the Bruce chisq\n        indics: numpy.ndarray\n            The indices of the snr peaks.\n\n        Returns\n        -------\n        chisq: Array\n            Chisq values, one for each sample index\n        \"\"\"\n        if not self.do:\n            return None\n\n        if template.params.template_hash not in self.params:\n            return numpy.ones(len(snrv))\n        values = self.params[template.params.template_hash].split(',')\n\n        # Get the chisq bins to use as the frequency reference point\n        bins = self.cached_chisq_bins(template, psd)\n\n        # This is implemented slowly, so let's not call it often, OK?\n        chisq = numpy.ones(len(snrv))\n        for i, snrvi in enumerate(snrv):\n            #Skip if newsnr too low\n            snr = abs(snrvi * snr_norm)\n            nsnr = ranking.newsnr(snr, bchisq[i] / bchisq_dof[i])\n            if nsnr < self.snr_threshold:\n                continue\n\n            N = (len(template) - 1) * 2\n            dt = 1.0 / (N * template.delta_f)\n            kmin = int(template.f_lower / psd.delta_f)\n            time = float(template.epoch) + dt * indices[i]\n            # Shift the time of interest to be centered on 0\n            stilde_shift = apply_fseries_time_shift(stilde, -time)\n\n            # Only apply the sine-Gaussian in a +-50 Hz range around the\n            # central frequency\n            qwindow = 50\n            chisq[i] = 0\n\n            # Estimate the maximum frequency up to which the waveform has\n            # power by approximating power per frequency\n            # as constant over the last 2 chisq bins. We cannot use the final\n            # chisq bin edge as it does not have to be where the waveform\n            # terminates.\n            fstep = (bins[-2] - bins[-3])\n            fpeak = (bins[-2] + fstep) * template.delta_f\n\n            # This is 90% of the Nyquist frequency of the data\n            # This allows us to avoid issues near Nyquist due to resample\n            # Filtering\n            fstop = len(stilde) * stilde.delta_f * 0.9\n\n            dof = 0\n            # Calculate the sum of SNR^2 for the sine-Gaussians specified\n            for descr in values:\n                # Get the q and frequency offset from the descriptor\n                q, offset = descr.split('-')\n                q, offset = float(q), float(offset)\n                fcen = fpeak + offset\n                flow = max(kmin * template.delta_f, fcen - qwindow)\n                fhigh = fcen + qwindow\n\n                # If any sine-gaussian tile has an upper frequency near\n                # nyquist return 1 instead.\n                if fhigh > fstop:\n                    return numpy.ones(len(snrv))\n\n                kmin = int(flow / template.delta_f)\n                kmax = int(fhigh / template.delta_f)\n\n                #Calculate sine-gaussian tile\n                gtem = sinegauss.fd_sine_gaussian(1.0, q, fcen, flow,\n                                      len(template) * template.delta_f,\n                                      template.delta_f).astype(numpy.complex64)\n                gsigma = sigma(gtem, psd=psd,\n                                     low_frequency_cutoff=flow,\n                                     high_frequency_cutoff=fhigh)\n                #Calculate the SNR of the tile\n                gsnr = (gtem[kmin:kmax] * stilde_shift[kmin:kmax]).sum()\n                gsnr *= 4.0 * gtem.delta_f / gsigma\n                chisq[i] += abs(gsnr)**2.0\n                dof += 2\n            if dof == 0:\n                chisq[i] = 1\n            else:\n                chisq[i] /= dof\n        return chisq", "response": "Calculates the chisq values for the given template and psd."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef single(self, trigs):\n        newsnr = ranking.get_newsnr(trigs)\n        rchisq = trigs['chisq'][:] / (2. * trigs['chisq_dof'][:] - 2.)\n        newsnr[numpy.logical_and(newsnr < 10, rchisq > 2)] = -1\n        return newsnr", "response": "Calculate the single detector statistic."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the coincident detection statistic.", "response": "def coinc(self, s0, s1, slide, step): # pylint:disable=unused-argument\n        \"\"\"Calculate the coincident detection statistic.\n\n        Parameters\n        ----------\n        s0: numpy.ndarray\n            Single detector ranking statistic for the first detector.\n        s1: numpy.ndarray\n            Single detector ranking statistic for the second detector.\n        slide: (unused in this statistic)\n        step: (unused in this statistic)\n\n        Returns\n        -------\n        cstat: numpy.ndarray\n            Array of coincident ranking statistic values\n        \"\"\"\n        cstat = (s0**2. + s1**2.) ** 0.5\n        cstat[s0==-1] = 0\n        cstat[s1==-1] = 0\n        return cstat"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef logsignalrate(self, s0, s1, slide, step):\n        td = numpy.array(s0['end_time'] - s1['end_time'] - slide*step, ndmin=1)\n        pd = numpy.array((s0['coa_phase'] - s1['coa_phase']) % \\\n                         (2. * numpy.pi), ndmin=1)\n        rd = numpy.array((s0['sigmasq'] / s1['sigmasq']) ** 0.5, ndmin=1)\n        sn0 = numpy.array(s0['snr'], ndmin=1)\n        sn1 = numpy.array(s1['snr'], ndmin=1)\n\n        snr0 = sn0 * 1\n        snr1 = sn1 * 1\n\n        snr0[rd > 1] = sn1[rd > 1]\n        snr1[rd > 1] = sn0[rd > 1]\n        rd[rd > 1] = 1. / rd[rd > 1]\n\n        # Find which bin each coinc falls into\n        tv = numpy.searchsorted(self.tbins, td) - 1\n        pv = numpy.searchsorted(self.pbins, pd) - 1\n        s0v = numpy.searchsorted(self.sbins, snr0) - 1\n        s1v = numpy.searchsorted(self.sbins, snr1) - 1\n        rv = numpy.searchsorted(self.rbins, rd) - 1\n\n        # Enforce that points fits into the bin boundaries: if a point lies\n        # outside the boundaries it is pushed back to the nearest bin.\n        tv[tv < 0] = 0\n        tv[tv >= len(self.tbins) - 1] = len(self.tbins) - 2\n        pv[pv < 0] = 0\n        pv[pv >= len(self.pbins) - 1] = len(self.pbins) - 2\n        s0v[s0v < 0] = 0\n        s0v[s0v >= len(self.sbins) - 1] = len(self.sbins) - 2\n        s1v[s1v < 0] = 0\n        s1v[s1v >= len(self.sbins) - 1] = len(self.sbins) - 2\n        rv[rv < 0] = 0\n        rv[rv >= len(self.rbins) - 1] = len(self.rbins) - 2\n\n        return self.hist[tv, pv, s0v, s1v, rv]", "response": "Calculate the normalized log rate density of signals via lookup"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef coinc(self, s0, s1, slide, step):\n        rstat = s0['snglstat']**2. + s1['snglstat']**2.\n        cstat = rstat + 2. * self.logsignalrate(s0, s1, slide, step)\n        cstat[cstat < 0] = 0\n        return cstat ** 0.5", "response": "Calculates the coincident detection statistic for a set of timeslans."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding the fit coefficients for a specific ifo and template id", "response": "def find_fits(self, trigs):\n        \"\"\"Get fit coeffs for a specific ifo and template id(s)\"\"\"\n        try:\n            tnum = trigs.template_num  # exists if accessed via coinc_findtrigs\n            ifo = trigs.ifo\n        except AttributeError:\n            tnum = trigs['template_id']  # exists for SingleDetTriggers\n            # Should only be one ifo fit file provided\n            assert len(self.ifos) == 1\n            ifo = self.ifos[0]\n        # fits_by_tid is a dictionary of dictionaries of arrays\n        # indexed by ifo / coefficient name / template_id\n        alphai = self.fits_by_tid[ifo]['alpha'][tnum]\n        ratei = self.fits_by_tid[ifo]['rate'][tnum]\n        thresh = self.fits_by_tid[ifo]['thresh']\n        return alphai, ratei, thresh"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate the log noise rate density over a set of single - ifo newsnr trigs", "response": "def lognoiserate(self, trigs):\n        \"\"\"\n        Calculate the log noise rate density over single-ifo newsnr\n\n        Read in single trigger information, make the newsnr statistic\n        and rescale by the fitted coefficients alpha and rate\n        \"\"\"\n        alphai, ratei, thresh = self.find_fits(trigs)\n        newsnr = self.get_newsnr(trigs)\n        # alphai is constant of proportionality between single-ifo newsnr and\n        #  negative log noise likelihood in given template\n        # ratei is rate of trigs in given template compared to average\n        # thresh is stat threshold used in given ifo\n        lognoisel = - alphai * (newsnr - thresh) + numpy.log(alphai) + \\\n                      numpy.log(ratei)\n        return numpy.array(lognoisel, ndmin=1, dtype=numpy.float32)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef coinc(self, s0, s1, slide, step): # pylint:disable=unused-argument\n        # Approximate log likelihood ratio by summing single-ifo negative\n        # log noise likelihoods\n        loglr = - s0 - s1\n        # add squares of threshold stat values via idealized Gaussian formula\n        threshes = [self.fits_by_tid[i]['thresh'] for i in self.ifos]\n        loglr += sum([t**2. / 2. for t in threshes])\n        # convert back to a coinc-SNR-like statistic\n        # via log likelihood ratio \\propto rho_c^2 / 2\n        return (2. * loglr) ** 0.5", "response": "Calculate the final coinc ranking statistic"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef single(self, trigs):\n        chisq_newsnr = ranking.get_newsnr(trigs)\n        rautochisq = trigs['cont_chisq'][:] / trigs['cont_chisq_dof'][:]\n        autochisq_newsnr = ranking.newsnr(trigs['snr'][:], rautochisq)\n        return numpy.array(numpy.minimum(chisq_newsnr, autochisq_newsnr,\n                           dtype=numpy.float32), ndmin=1, copy=False)", "response": "Calculate the single detector statistic."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_table_list(self, cursor):\r\n\r\n        cursor.execute(\"\"\"\r\n            SELECT c.relname, c.relkind\r\n            FROM pg_catalog.pg_class c\r\n            LEFT JOIN pg_catalog.pg_namespace n ON n.oid = c.relnamespace\r\n            WHERE c.relkind IN ('r', 'v', '')\r\n                AND n.nspname = '%s'\r\n                AND pg_catalog.pg_table_is_visible(c.oid)\"\"\" % self.connection.schema_name)\r\n\r\n        return [TableInfo(row[0], {'r': 't', 'v': 'v'}.get(row[1]))\r\n                for row in cursor.fetchall()\r\n                if row[0] not in self.ignored_tables]", "response": "Returns a list of table names in the current database and schema."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a description of the table with the DB - API cursor. description interface.", "response": "def get_table_description(self, cursor, table_name):\r\n        \"Returns a description of the table, with the DB-API cursor.description interface.\"\r\n        # As cursor.description does not return reliably the nullable property,\r\n        # we have to query the information_schema (#7783)\r\n        cursor.execute(\"\"\"\r\n            SELECT column_name, is_nullable, column_default\r\n            FROM information_schema.columns\r\n            WHERE table_schema = %s and table_name = %s\"\"\", [self.connection.schema_name, table_name])\r\n        field_map = {line[0]: line[1:] for line in cursor.fetchall()}\r\n        cursor.execute(\"SELECT * FROM %s LIMIT 1\" % self.connection.ops.quote_name(table_name))\r\n        return [FieldInfo(*((force_text(line[0]),) + line[1:6] +\r\n                            (field_map[force_text(line[0])][0] == 'YES', field_map[force_text(line[0])][1])))\r\n                for line in cursor.description]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _drop_schema(self, force_drop=False):\n        connection = connections[get_tenant_database_alias()]\n        has_schema = hasattr(connection, 'schema_name')\n        if has_schema and connection.schema_name not in (self.schema_name, get_public_schema_name()):\n            raise Exception(\"Can't delete tenant outside it's own schema or \"\n                            \"the public schema. Current schema is %s.\"\n                            % connection.schema_name)\n\n        if has_schema and schema_exists(self.schema_name) and (self.auto_drop_schema or force_drop):\n            self.pre_drop()\n            cursor = connection.cursor()\n            cursor.execute('DROP SCHEMA %s CASCADE' % self.schema_name)", "response": "Drops the schema if it exists."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndeleting this row. Drops the tenant's schema if the attribute auto_drop_schema set to True.", "response": "def delete(self, force_drop=False, *args, **kwargs):\n        \"\"\"\n        Deletes this row. Drops the tenant's schema if the attribute\n        auto_drop_schema set to True.\n        \"\"\"\n        self._drop_schema(force_drop)\n        super().delete(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the primary domain of the tenant", "response": "def get_primary_domain(self):\n        \"\"\"\n        Returns the primary domain of the tenant\n        \"\"\"\n        try:\n            domain = self.domains.get(is_primary=True)\n            return domain\n        except get_tenant_domain_model().DoesNotExist:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the URL of this tenant.", "response": "def reverse(self, request, view_name):\n        \"\"\"\n        Returns the URL of this tenant.\n        \"\"\"\n        http_type = 'https://' if request.is_secure() else 'http://'\n\n        domain = get_current_site(request).domain\n\n        url = ''.join((http_type, self.schema_name, '.', domain, reverse(view_name)))\n\n        return url"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nis app_label present in apps_list?", "response": "def app_in_list(self, app_label, apps_list):\n        \"\"\"\n        Is 'app_label' present in 'apps_list'?\n\n        apps_list is either settings.SHARED_APPS or settings.TENANT_APPS, a\n        list of app names.\n\n        We check the presence of the app's name or the full path to the apps's\n        AppConfig class.\n        https://docs.djangoproject.com/en/1.8/ref/applications/#configuring-applications\n        \"\"\"\n        appconfig = django_apps.get_app_config(app_label)\n        appconfig_full_name = '{}.{}'.format(\n            appconfig.__module__, appconfig.__class__.__name__)\n        return (appconfig.name in apps_list) or (appconfig_full_name in apps_list)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef locations(self):\n        if self._locations.get(connection.schema_name, None) is None:\n            schema_locations = []\n            for root in settings.MULTITENANT_STATICFILES_DIRS:\n                root = utils.parse_tenant_config_path(root)\n\n                if isinstance(root, (list, tuple)):\n                    prefix, root = root\n                else:\n                    prefix = \"\"\n\n                if (prefix, root) not in schema_locations:\n                    schema_locations.append((prefix, root))\n\n            self._locations[connection.schema_name] = schema_locations\n\n        return self._locations[connection.schema_name]", "response": "Lazy retrieval of list of locations with static files based on current tenant schema."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef storages(self):\n        if self._storages.get(connection.schema_name, None) is None:\n            schema_storages = OrderedDict()\n\n            for prefix, root in self.locations:\n                filesystem_storage = TenantStaticFilesStorage(location=root)\n                filesystem_storage.prefix = prefix\n                schema_storages[root] = filesystem_storage\n\n            self._storages[connection.schema_name] = schema_storages\n\n        return self._storages[connection.schema_name]", "response": "Lazy retrieval of list of storage handlers for the current tenant."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_tenant(self, tenant, include_public=True):\n        self.tenant = tenant\n        self.schema_name = tenant.schema_name\n        self.include_public_schema = include_public\n        self.set_settings_schema(self.schema_name)\n        self.search_path_set = False", "response": "This method is used to set the tenant for this object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_schema_to_public(self):\n        self.tenant = FakeTenant(schema_name=get_public_schema_name())\n        self.schema_name = get_public_schema_name()\n        self.set_settings_schema(self.schema_name)\n        self.search_path_set = False", "response": "Sets the schema to public."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cache_key(self, template_name, skip=None):\n        dirs_prefix = ''\n        skip_prefix = ''\n        tenant_prefix = ''\n\n        if skip:\n            matching = [origin.name for origin in skip if origin.template_name == template_name]\n\n            if matching:\n                skip_prefix = self.generate_hash(matching)\n\n        if connection.tenant:\n            tenant_prefix = str(connection.tenant.pk)\n\n        return '-'.join(s for s in (str(template_name), tenant_prefix, skip_prefix, dirs_prefix) if s)", "response": "Generate a cache key for the template name dirs and skip."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of tenants that will be created by cloning an analyzed schema.", "response": "def get_creation_fakes_migrations():\n    \"\"\"\n    If TENANT_CREATION_FAKES_MIGRATIONS, tenants will be created by cloning an\n    existing schema specified by TENANT_CLONE_BASE.\n    \"\"\"\n    faked = getattr(settings, 'TENANT_CREATION_FAKES_MIGRATIONS', False)\n    if faked:\n        if not getattr(settings, 'TENANT_BASE_SCHEMA', False):\n            raise ImproperlyConfigured(\n                'You must specify a schema name in TENANT_BASE_SCHEMA if '\n                'TENANT_CREATION_FAKES_MIGRATIONS is enabled.'\n            )\n    return faked"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the base schema of the tenant.", "response": "def get_tenant_base_schema():\n    \"\"\"\n    If TENANT_CREATION_FAKES_MIGRATIONS, tenants will be created by cloning an\n    existing schema specified by TENANT_CLONE_BASE.\n    \"\"\"\n    schema = getattr(settings, 'TENANT_BASE_SCHEMA', False)\n    if schema:\n        if not getattr(settings, 'TENANT_CREATION_FAKES_MIGRATIONS', False):\n            raise ImproperlyConfigured(\n                'TENANT_CREATION_FAKES_MIGRATIONS setting must be True to use '\n                'TENANT_BASE_SCHEMA for cloning.'\n            )\n    return schema"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_tenant_config_path(config_path):\n    try:\n        # Insert schema name\n        return config_path % connection.schema_name\n    except (TypeError, ValueError):\n        # No %s in string; append schema name at the end\n        return os.path.join(config_path, connection.schema_name)", "response": "Parses the django - tenants path configuration strings."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _create_clone_schema_function(self):\n        cursor = connection.cursor()\n        cursor.execute(CLONE_SCHEMA_FUNCTION)\n        cursor.close()", "response": "Creates a postgres function that copies a schema and its\n        contents. Will replace any existing clone_schema functions owned by the postgres superuser."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a new schema with the given name as a clone of an existing schema with the given name.", "response": "def clone_schema(self, base_schema_name, new_schema_name):\n        \"\"\"\n        Creates a new schema `new_schema_name` as a clone of an existing schema\n        `old_schema_name`.\n        \"\"\"\n        connection.set_schema_to_public()\n        cursor = connection.cursor()\n\n        # check if the clone_schema function already exists in the db\n        try:\n            cursor.execute(\"SELECT 'clone_schema'::regproc\")\n        except ProgrammingError:\n            self._create_clone_schema_function()\n            transaction.commit()\n\n        sql = 'SELECT clone_schema(%(base_schema)s, %(new_schema)s, TRUE)'\n        cursor.execute(\n            sql,\n            {'base_schema': base_schema_name, 'new_schema': new_schema_name}\n        )\n        cursor.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef install_database(name, owner, template='template0', encoding='UTF8', locale='en_US.UTF-8'):\n\n\n    create_database(name, owner, template=template, encoding=encoding,\n                        locale=locale)", "response": "Install a PostgreSQL database."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dirs(self):\n        if self._dirs.get(connection.schema_name, None) is None:\n            try:\n                # Use directories configured via MULTITENANT_TEMPLATE_DIRS\n                dirs = [\n                    utils.parse_tenant_config_path(dir_)\n                    for dir_ in settings.MULTITENANT_TEMPLATE_DIRS\n                ]\n            except AttributeError:\n                raise ImproperlyConfigured(\n                    \"To use {}.{} you must define the MULTITENANT_TEMPLATE_DIRS setting.\".format(\n                        __name__, Loader.__name__\n                    )\n                )\n\n            self.dirs = dirs\n\n        return self._dirs[connection.schema_name]", "response": "Lazy retrieval of list of template file dirs based on current tenant schema."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _read_stream(self, stream):\n        contents = json.load(stream)\n\n        family, containers, volumes = '', contents, []\n\n        if isinstance(contents, dict) and 'containerDefinitions' in contents.keys():\n            family = contents.get('family', None)\n            containers = contents.get('containerDefinitions', [])\n            volumes = self.ingest_volumes_param(contents.get('volumes', []))\n\n        return family, containers, volumes", "response": "Read extra volume\n        data from a file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a volume to the set of available volumes.", "response": "def add_volume(self, volume):\n        \"\"\"\n        Add a volume to self.volumes if it isn't already present\n        \"\"\"\n        for old_vol in self.volumes:\n            if volume == old_vol:\n                return\n        self.volumes.append(volume)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nemit the task definition and sorts containers by name", "response": "def emit_containers(self, containers, verbose=True):\n        \"\"\"\n        Emits the task definition and sorts containers by name\n\n        :param containers: List of the container definitions\n        :type containers: list of dict\n\n        :param verbose: Print out newlines and indented JSON\n        :type verbose: bool\n\n        :returns: The text output\n        :rtype: str\n        \"\"\"\n        containers = sorted(containers, key=lambda c: c.get('name'))\n        task_definition = {\n            'family': self.family,\n            'containerDefinitions': containers,\n            'volumes': self.volumes or []\n        }\n        if verbose:\n            return json.dumps(task_definition, indent=4, sort_keys=True)\n        else:\n            return json.dumps(task_definition)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ningesting the volumes parameter", "response": "def ingest_volumes_param(self, volumes):\n        \"\"\"\n        This is for ingesting the \"volumes\" of a task description\n        \"\"\"\n        data = {}\n\n        for volume in volumes:\n            if volume.get('host', {}).get('sourcePath'):\n                data[volume.get('name')] = {\n                    'path': volume.get('host', {}).get('sourcePath'),\n                    'readonly': volume.get('readOnly', False)\n                }\n            else:\n                data[volume.get('name')] = {\n                    'path': '/tmp/{}'.format(uuid.uuid4().hex[:8]),\n                    'readonly': volume.get('readOnly', False)\n                }\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbuilding the mountPoints element for a generic volume definition", "response": "def _build_mountpoint(self, volume):\n        \"\"\"\n        Given a generic volume definition, create the mountPoints element\n        \"\"\"\n        self.add_volume(self._build_volume(volume))\n        return {\n            'sourceVolume': self.path_to_name(volume.get('host')),\n            'containerPath': volume.get('container')\n        }"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _lookup_parameter(self, container, key, common_type=None):\n        if not container.get('container', {}).get('docker', {}).get('parameters'):\n            return\n        params = container['container']['docker']['parameters']\n\n        # Super hacky - log-opt is a sub option of the logging directive of everything else\n        if key == 'log-driver':\n            return [\n                p\n                for p\n                in params\n                if p['key'] in ['log-opt', 'log-driver']]\n\n        matching_params = [\n            p['value']\n            for p\n            in params\n            if p['key'] == key]\n\n        if matching_params:\n            if common_type == list:\n                return matching_params\n            else:\n                return matching_params[0]", "response": "Lookup the docker run keyword from the container. docker. parameters list"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntakes a marathon container and pulls out the nested values into the top level", "response": "def flatten_container(self, container):\n        \"\"\"\n        Accepts a marathon container and pulls out the nested values into the top level\n        \"\"\"\n        for names in ARG_MAP.values():\n\n            if names[TransformationTypes.MARATHON.value]['name'] and \\\n                            '.' in names[TransformationTypes.MARATHON.value]['name']:\n                marathon_dotted_name = names[TransformationTypes.MARATHON.value]['name']\n                parts = marathon_dotted_name.split('.')\n\n                if parts[-2] == 'parameters':\n                    # Special lookup for docker parameters\n                    common_type = names[TransformationTypes.MARATHON.value].get('type')\n                    result = self._lookup_parameter(container, parts[-1], common_type)\n                    if result:\n                        container[marathon_dotted_name] = result\n                else:\n                    result = lookup_nested_dict(container, *parts)\n                    if result:\n                        container[marathon_dotted_name] = result\n        return container"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nemit the applications and sorts containers by name", "response": "def emit_containers(self, containers, verbose=True):\n        \"\"\"\n        Emits the applications and sorts containers by name\n\n        :param containers: List of the container definitions\n        :type containers: list of dict\n\n        :param verbose: Print out newlines and indented JSON\n        :type verbose: bool\n\n        :returns: The text output\n        :rtype: str\n        \"\"\"\n        containers = sorted(containers, key=lambda c: c.get('id'))\n\n        if len(containers) == 1 and isinstance(containers, list):\n            containers = containers[0]\n\n        if verbose:\n            return json.dumps(containers, indent=4, sort_keys=True)\n        else:\n            return json.dumps(containers)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _convert_volume(self, volume):\n        data = {\n            'host': volume.get('hostPath'),\n            'container': volume.get('containerPath'),\n            'readonly': volume.get('mode') == 'RO',\n        }\n        return data", "response": "Convert a volume to a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds the first instance of a self. pod_types object in the data.", "response": "def _find_convertable_object(self, data):\n        \"\"\"\n        Get the first instance of a `self.pod_types`\n        \"\"\"\n        data = list(data)\n        convertable_object_idxs = [\n            idx\n            for idx, obj\n            in enumerate(data)\n            if obj.get('kind') in self.pod_types.keys()\n        ]\n        if len(convertable_object_idxs) < 1:\n            raise Exception(\"Kubernetes config didn't contain any of {}\".format(\n                ', '.join(self.pod_types.keys())\n            ))\n        return list(data)[convertable_object_idxs[0]]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _read_stream(self, stream):\n        data = yaml.safe_load_all(stream=stream)\n        obj = self._find_convertable_object(data)\n        pod = self.pod_types[obj['kind']](obj)\n        return obj, pod.get('containers'), self.ingest_volumes_param(pod.get('volumes', []))", "response": "Read in the pod stream and return the object and the container list and the volumes list"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ingest_volumes_param(self, volumes):\n        data = {}\n        for volume in volumes:\n            if volume.get('hostPath', {}).get('path'):\n                data[volume.get('name')] = {\n                    'path': volume.get('hostPath', {}).get('path'),\n                }\n            elif volume.get('emptyDir'):\n                data[volume.get('name')] = {}\n            else:\n                data[volume.get('name')] = {}\n            # TODO Support other k8s volume types?\n        return data", "response": "Ingests the volumes parameter into a dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntake a kubernetes container and pulls out the nested values into the top level", "response": "def flatten_container(self, container):\n        \"\"\"\n        Accepts a kubernetes container and pulls out the nested values into the top level\n        \"\"\"\n        for names in ARG_MAP.values():\n            if names[TransformationTypes.KUBERNETES.value]['name'] and \\\n                            '.' in names[TransformationTypes.KUBERNETES.value]['name']:\n                kubernetes_dotted_name = names[TransformationTypes.KUBERNETES.value]['name']\n                parts = kubernetes_dotted_name.split('.')\n                result = lookup_nested_dict(container, *parts)\n                if result:\n                    container[kubernetes_dotted_name] = result\n        return container"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef emit_containers(self, containers, verbose=True):\n        containers = sorted(containers, key=lambda c: c.get('name'))\n\n        output = {\n            'kind': 'Deployment',\n            'apiVersion': 'extensions/v1beta1',\n            'metadata': {\n                'name': None,\n                'namespace': 'default',\n                'labels': {\n                    'app': None,\n                    'version': 'latest',\n                },\n            },\n            'spec': {\n                'replicas': 1,\n                'selector': {\n                    'matchLabels': {\n                        'app': None,\n                        'version': 'latest'\n                    }\n                },\n                'template': {\n                    'metadata': {\n                        'labels': {\n                            'app': None,\n                            'version': 'latest'\n                        }\n                    },\n                    'spec': {\n                        'containers': json.loads(json.dumps(containers))\n                    }\n                }\n            }\n        }\n        if self.volumes:\n            volumes = sorted(self.volumes.values(), key=lambda x: x.get('name'))\n            output['spec']['template']['spec']['volumes'] = volumes\n\n        noalias_dumper = yaml.dumper.SafeDumper\n        noalias_dumper.ignore_aliases = lambda self, data: True\n        return yaml.dump(\n            output,\n            default_flow_style=False,\n            Dumper=noalias_dumper\n        )", "response": "Emits the applications and sorts containers by name and returns the text output"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ingest_memory(self, memory):\n        def lshift(num, shift):\n            return num << shift\n\n        def k(num, thousands):\n            return num * thousands\n\n        # if isinstance(memory, int):\n        #     # Memory was specified as an integer, meaning it is in bytes\n        memory = str(memory)\n\n        bit_shift = {\n            'E': {'func': k, 'shift': 10e17},\n            'P': {'func': k, 'shift': 10e14},\n            'T': {'func': k, 'shift': 10e11},\n            'G': {'func': k, 'shift': 10e8},\n            'M': {'func': k, 'shift': 10e5},\n            'K': {'func': k, 'shift': 10e2},\n            'Ei': {'func': lshift, 'shift': 60},\n            'Pi': {'func': lshift, 'shift': 50},\n            'Ti': {'func': lshift, 'shift': 40},\n            'Gi': {'func': lshift, 'shift': 30},\n            'Mi': {'func': lshift, 'shift': 20},\n            'Ki': {'func': lshift, 'shift': 10},\n        }\n\n        if len(memory) > 2 and memory[-2:] in bit_shift.keys():\n            unit = memory[-2:]\n            number = int(memory[:-2])\n            memory = bit_shift[unit]['func'](number, bit_shift[unit]['shift'])\n        elif len(memory) > 1 and memory[-1:] in bit_shift.keys():\n            unit = memory[-1]\n            number = int(memory[:-1])\n            memory = bit_shift[unit]['func'](number, bit_shift[unit]['shift'])\n        # Cast to a float to properly consume scientific notation\n        return int(float(memory))", "response": "Transform the memory into bytes and return the result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _build_volume(self, volume):\n        self.volumes[self._build_volume_name(volume.get('host'))] = {\n            'name': self._build_volume_name(volume.get('host')),\n            'hostPath': {\n                'path': volume.get('host')\n            }\n        }\n        response = {\n            'name': self._build_volume_name(volume.get('host')),\n            'mountPath': volume.get('container'),\n\n        }\n        if volume.get('readonly', False):\n            response['readOnly'] = bool(volume.get('readonly', False))\n        return response", "response": "Builds the generic volume definition and returns the metadata"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef convert(self, verbose=True):\n        input_transformer = self._input_class(self._filename)\n        output_transformer = self._output_class()\n\n        containers = input_transformer.ingest_containers()\n\n        output_containers = []\n\n        for container in containers:\n            converted_container = self._convert_container(\n                container,\n                input_transformer,\n                output_transformer\n            )\n\n            validated = output_transformer.validate(converted_container)\n\n            output_containers.append(validated)\n\n        return output_transformer.emit_containers(output_containers, verbose)", "response": "Convert the file to a set of related items."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _convert_container(self, container, input_transformer, output_transformer):\n        output = {}\n        for parameter, options in ARG_MAP.items():\n            output_name = options.get(self.output_type, {}).get('name')\n            output_required = options.get(self.output_type, {}).get('required')\n\n            input_name = options.get(self.input_type, {}).get('name')\n\n            if container.get(input_name) and \\\n                    hasattr(input_transformer, 'ingest_{}'.format(parameter)) and \\\n                    output_name and hasattr(output_transformer, 'emit_{}'.format(parameter)):\n                # call transform_{}\n                ingest_func = getattr(input_transformer, 'ingest_{}'.format(parameter))\n                emit_func = getattr(output_transformer, 'emit_{}'.format(parameter))\n\n                output[output_name] = emit_func(ingest_func(container.get(input_name)))\n\n            if not container.get(input_name) and output_required:\n                msg_template = 'Container {name} is missing required parameter \"{output_name}\".'\n                self.messages.add(\n                    msg_template.format(\n                        output_name=output_name,\n                        output_type=self.output_type,\n                        name=container.get('name', container)\n                    )\n                )\n\n        return output", "response": "Converts a given dictionary to an output container definition"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntakes a chronos container and pulls out the nested values into the top level", "response": "def flatten_container(self, container):\n        \"\"\"\n        Accepts a chronos container and pulls out the nested values into the top level\n        \"\"\"\n        for names in ARG_MAP.values():\n            if names[TransformationTypes.CHRONOS.value]['name'] and \\\n                            '.' in names[TransformationTypes.CHRONOS.value]['name']:\n                chronos_dotted_name = names[TransformationTypes.CHRONOS.value]['name']\n                parts = chronos_dotted_name.split('.')\n\n                if parts[-2] == 'parameters':\n                    # Special lookup for docker parameters\n                    common_type = names[TransformationTypes.CHRONOS.value].get('type')\n                    result = self._lookup_parameter(container, parts[-1], common_type)\n                    if result:\n                        container[chronos_dotted_name] = result\n                else:\n                    result = lookup_nested_dict(container, *parts)\n                    if result:\n                        container[chronos_dotted_name] = result\n        return container"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _read_file(self, filename):\n        with open(filename, 'r') as stream:\n            return self._read_stream(stream=stream)", "response": "Reads the file and returns a dict of the data."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntransforms the YAML into a list of dicts with normalized keys", "response": "def ingest_containers(self, containers=None):\n        \"\"\"\n        Transform the YAML into a dict with normalized keys\n        \"\"\"\n        containers = containers or self.stream or {}\n\n        output_containers = []\n\n        for container_name, definition in containers.items():\n            container = definition.copy()\n            container['name'] = container_name\n            output_containers.append(container)\n\n        return output_containers"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ingest_memory(self, memory):\n        def lshift(num, shift):\n            return num << shift\n\n        def rshift(num, shift):\n            return num >> shift\n\n        if isinstance(memory, int):\n            # Memory was specified as an integer, meaning it is in bytes\n            memory = '{}b'.format(memory)\n\n        bit_shift = {\n            'g': {'func': lshift, 'shift': 30},\n            'm': {'func': lshift, 'shift': 20},\n            'k': {'func': lshift, 'shift': 10},\n            'b': {'func': rshift, 'shift': 0}\n        }\n        unit = memory[-1]\n        number = int(memory[:-1])\n        return bit_shift[unit]['func'](number, bit_shift[unit]['shift'])", "response": "Transform the memory into bytes\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn if a condition is true otherwise raise a caller - configurable exception", "response": "def ensure(cond, *args, **kwds):\n    \"\"\"\n    Return if a condition is true, otherwise raise a caller-configurable\n    :py:class:`Exception`\n    :param bool cond: the condition to be checked\n    :param sequence args: the arguments to be passed to the exception's\n                          constructor\n    The only accepted named parameter is `raising` used to configure the\n    exception to be raised if `cond` is not `True`\n    \"\"\"\n    _CHK_UNEXP = 'check_condition() got an unexpected keyword argument {0}'\n\n    raising = kwds.pop('raising', AssertionError)\n    if kwds:\n        raise TypeError(_CHK_UNEXP.format(repr(kwds.popitem()[0])))\n\n    if cond is True:\n        return\n    raise raising(*args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nhashes a message with SHA256.", "response": "def sha256(message, encoder=nacl.encoding.HexEncoder):\n    \"\"\"\n    Hashes ``message`` with SHA256.\n\n    :param message: The message to hash.\n    :type message: bytes\n    :param encoder: A class that is able to encode the hashed message.\n    :returns: The hashed message.\n    :rtype: bytes\n    \"\"\"\n    return encoder.encode(nacl.bindings.crypto_hash_sha256(message))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sha512(message, encoder=nacl.encoding.HexEncoder):\n    return encoder.encode(nacl.bindings.crypto_hash_sha512(message))", "response": "Hashes a message with SHA512."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nhashing ``data`` with blake2b. :param data: the digest input byte sequence :type data: bytes :param digest_size: the requested digest size; must be at most :const:`BLAKE2B_BYTES_MAX`; the default digest size is :const:`BLAKE2B_BYTES` :type digest_size: int :param key: the key to be set for keyed MAC/PRF usage; if set, the key must be at most :data:`~nacl.hash.BLAKE2B_KEYBYTES_MAX` long :type key: bytes :param salt: an initialization salt at most :const:`BLAKE2B_SALTBYTES` long; it will be zero-padded if needed :type salt: bytes :param person: a personalization string at most :const:`BLAKE2B_PERSONALBYTES` long; it will be zero-padded if needed :type person: bytes :param encoder: the encoder to use on returned digest :type encoder: class :returns: The hashed message. :rtype: bytes", "response": "def blake2b(data, digest_size=BLAKE2B_BYTES, key=b'',\n            salt=b'', person=b'',\n            encoder=nacl.encoding.HexEncoder):\n    \"\"\"\n    Hashes ``data`` with blake2b.\n\n    :param data: the digest input byte sequence\n    :type data: bytes\n    :param digest_size: the requested digest size; must be at most\n                        :const:`BLAKE2B_BYTES_MAX`;\n                        the default digest size is\n                        :const:`BLAKE2B_BYTES`\n    :type digest_size: int\n    :param key: the key to be set for keyed MAC/PRF usage; if set, the key\n                must be at most :data:`~nacl.hash.BLAKE2B_KEYBYTES_MAX` long\n    :type key: bytes\n    :param salt: an initialization salt at most\n                 :const:`BLAKE2B_SALTBYTES` long;\n                 it will be zero-padded if needed\n    :type salt: bytes\n    :param person: a personalization string at most\n                   :const:`BLAKE2B_PERSONALBYTES` long;\n                   it will be zero-padded if needed\n    :type person: bytes\n    :param encoder: the encoder to use on returned digest\n    :type encoder: class\n    :returns: The hashed message.\n    :rtype: bytes\n    \"\"\"\n\n    digest = _b2b_hash(data, digest_size=digest_size, key=key,\n                       salt=salt, person=person)\n    return encoder.encode(digest)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a 24 - byte SIP hash of the given message using the short - input - optimized siphash - 2 - 4 construction.", "response": "def siphash24(message, key=b'', encoder=nacl.encoding.HexEncoder):\n    \"\"\"\n    Computes a keyed MAC of ``message`` using the short-input-optimized\n    siphash-2-4 construction.\n\n    :param message: The message to hash.\n    :type message: bytes\n    :param key: the message authentication key for the siphash MAC construct\n    :type key: bytes(:const:`SIPHASH_KEYBYTES`)\n    :param encoder: A class that is able to encode the hashed message.\n    :returns: The hashed message.\n    :rtype: bytes(:const:`SIPHASH_BYTES`)\n    \"\"\"\n    digest = _sip_hash(message, key)\n    return encoder.encode(digest)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef siphashx24(message, key=b'', encoder=nacl.encoding.HexEncoder):\n    digest = _sip_hashx(message, key)\n    return encoder.encode(digest)", "response": "Computes a 128 bit HMAC of a message using the 128 bit SIP hash algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the message message.", "response": "def crypto_hash(message):\n    \"\"\"\n    Hashes and returns the message ``message``.\n\n    :param message: bytes\n    :rtype: bytes\n    \"\"\"\n    digest = ffi.new(\"unsigned char[]\", crypto_hash_BYTES)\n    rc = lib.crypto_hash(digest, message, len(message))\n    ensure(rc == 0,\n           'Unexpected library error',\n           raising=exc.RuntimeError)\n    return ffi.buffer(digest, crypto_hash_BYTES)[:]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef crypto_hash_sha256(message):\n    digest = ffi.new(\"unsigned char[]\", crypto_hash_sha256_BYTES)\n    rc = lib.crypto_hash_sha256(digest, message, len(message))\n    ensure(rc == 0,\n           'Unexpected library error',\n           raising=exc.RuntimeError)\n    return ffi.buffer(digest, crypto_hash_sha256_BYTES)[:]", "response": "Hashes and returns the message message."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef crypto_hash_sha512(message):\n    digest = ffi.new(\"unsigned char[]\", crypto_hash_sha512_BYTES)\n    rc = lib.crypto_hash_sha512(digest, message, len(message))\n    ensure(rc == 0,\n           'Unexpected library error',\n           raising=exc.RuntimeError)\n    return ffi.buffer(digest, crypto_hash_sha512_BYTES)[:]", "response": "Hashes and returns the message message."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a randomly generated public key and secret key.", "response": "def crypto_sign_keypair():\n    \"\"\"\n    Returns a randomly generated public key and secret key.\n\n    :rtype: (bytes(public_key), bytes(secret_key))\n    \"\"\"\n    pk = ffi.new(\"unsigned char[]\", crypto_sign_PUBLICKEYBYTES)\n    sk = ffi.new(\"unsigned char[]\", crypto_sign_SECRETKEYBYTES)\n\n    rc = lib.crypto_sign_keypair(pk, sk)\n    ensure(rc == 0,\n           'Unexpected library error',\n           raising=exc.RuntimeError)\n\n    return (\n        ffi.buffer(pk, crypto_sign_PUBLICKEYBYTES)[:],\n        ffi.buffer(sk, crypto_sign_SECRETKEYBYTES)[:],\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute and returns the public key and secret key using the seed.", "response": "def crypto_sign_seed_keypair(seed):\n    \"\"\"\n    Computes and returns the public key and secret key using the seed ``seed``.\n\n    :param seed: bytes\n    :rtype: (bytes(public_key), bytes(secret_key))\n    \"\"\"\n    if len(seed) != crypto_sign_SEEDBYTES:\n        raise exc.ValueError(\"Invalid seed\")\n\n    pk = ffi.new(\"unsigned char[]\", crypto_sign_PUBLICKEYBYTES)\n    sk = ffi.new(\"unsigned char[]\", crypto_sign_SECRETKEYBYTES)\n\n    rc = lib.crypto_sign_seed_keypair(pk, sk, seed)\n    ensure(rc == 0,\n           'Unexpected library error',\n           raising=exc.RuntimeError)\n\n    return (\n        ffi.buffer(pk, crypto_sign_PUBLICKEYBYTES)[:],\n        ffi.buffer(sk, crypto_sign_SECRETKEYBYTES)[:],\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef crypto_sign(message, sk):\n    signed = ffi.new(\"unsigned char[]\", len(message) + crypto_sign_BYTES)\n    signed_len = ffi.new(\"unsigned long long *\")\n\n    rc = lib.crypto_sign(signed, signed_len, message, len(message), sk)\n    ensure(rc == 0,\n           'Unexpected library error',\n           raising=exc.RuntimeError)\n\n    return ffi.buffer(signed, signed_len[0])[:]", "response": "Signs the message using the secret key sk and returns the resulting message."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nverifying the signature of the signed message and returns the unsigned message.", "response": "def crypto_sign_open(signed, pk):\n    \"\"\"\n    Verifies the signature of the signed message ``signed`` using the public\n    key ``pk`` and returns the unsigned message.\n\n    :param signed: bytes\n    :param pk: bytes\n    :rtype: bytes\n    \"\"\"\n    message = ffi.new(\"unsigned char[]\", len(signed))\n    message_len = ffi.new(\"unsigned long long *\")\n\n    if lib.crypto_sign_open(\n            message, message_len, signed, len(signed), pk) != 0:\n        raise exc.BadSignatureError(\"Signature was forged or corrupt\")\n\n    return ffi.buffer(message, message_len[0])[:]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a public Ed25519 key to a public Curve25519 key.", "response": "def crypto_sign_ed25519_pk_to_curve25519(public_key_bytes):\n    \"\"\"\n    Converts a public Ed25519 key (encoded as bytes ``public_key_bytes``) to\n    a public Curve25519 key as bytes.\n\n    Raises a ValueError if ``public_key_bytes`` is not of length\n    ``crypto_sign_PUBLICKEYBYTES``\n\n    :param public_key_bytes: bytes\n    :rtype: bytes\n    \"\"\"\n    if len(public_key_bytes) != crypto_sign_PUBLICKEYBYTES:\n        raise exc.ValueError(\"Invalid curve public key\")\n\n    curve_public_key_len = crypto_sign_curve25519_BYTES\n    curve_public_key = ffi.new(\"unsigned char[]\", curve_public_key_len)\n\n    rc = lib.crypto_sign_ed25519_pk_to_curve25519(curve_public_key,\n                                                  public_key_bytes)\n    ensure(rc == 0,\n           'Unexpected library error',\n           raising=exc.RuntimeError)\n\n    return ffi.buffer(curve_public_key, curve_public_key_len)[:]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef crypto_sign_ed25519_sk_to_curve25519(secret_key_bytes):\n    if len(secret_key_bytes) != crypto_sign_SECRETKEYBYTES:\n        raise exc.ValueError(\"Invalid curve secret key\")\n\n    curve_secret_key_len = crypto_sign_curve25519_BYTES\n    curve_secret_key = ffi.new(\"unsigned char[]\", curve_secret_key_len)\n\n    rc = lib.crypto_sign_ed25519_sk_to_curve25519(curve_secret_key,\n                                                  secret_key_bytes)\n    ensure(rc == 0,\n           'Unexpected library error',\n           raising=exc.RuntimeError)\n\n    return ffi.buffer(curve_secret_key, curve_secret_key_len)[:]", "response": "Converts a secret Ed25519 key to a secret Curve25519 key."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef crypto_sign_ed25519ph_update(edph, pmsg):\n    ensure(isinstance(edph, crypto_sign_ed25519ph_state),\n           'edph parameter must be a ed25519ph_state object',\n           raising=exc.TypeError)\n    ensure(isinstance(pmsg, bytes),\n           'pmsg parameter must be a bytes object',\n           raising=exc.TypeError)\n    rc = lib.crypto_sign_ed25519ph_update(edph.state,\n                                          pmsg,\n                                          len(pmsg))\n    ensure(rc == 0,\n           'Unexpected library error',\n           raising=exc.RuntimeError)", "response": "Update the state of the ed25519ph state with the partial message pmsg."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a signature for the data hashed in edph using the secret part of the signing key sk.", "response": "def crypto_sign_ed25519ph_final_create(edph,\n                                       sk):\n    \"\"\"\n    Create a signature for the data hashed in edph\n    using the secret key sk\n\n    :param edph: the ed25519ph state for the data\n                 being signed\n    :type edph: crypto_sign_ed25519ph_state\n    :param sk: the ed25519 secret part of the signing key\n    :type sk: bytes\n    :return: ed25519ph signature\n    :rtype: bytes\n    \"\"\"\n    ensure(isinstance(edph, crypto_sign_ed25519ph_state),\n           'edph parameter must be a ed25519ph_state object',\n           raising=exc.TypeError)\n    ensure(isinstance(sk, bytes),\n           'secret key parameter must be a bytes object',\n           raising=exc.TypeError)\n    ensure(len(sk) == crypto_sign_SECRETKEYBYTES,\n           ('secret key must be {0} '\n            'bytes long').format(crypto_sign_SECRETKEYBYTES),\n           raising=exc.TypeError)\n    signature = ffi.new(\"unsigned char[]\", crypto_sign_BYTES)\n    rc = lib.crypto_sign_ed25519ph_final_create(edph.state,\n                                                signature,\n                                                ffi.NULL,\n                                                sk)\n    ensure(rc == 0,\n           'Unexpected library error',\n           raising=exc.RuntimeError)\n\n    return ffi.buffer(signature, crypto_sign_BYTES)[:]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nverifies the signature of a signed message.", "response": "def verify(self, smessage, signature=None, encoder=encoding.RawEncoder):\n        \"\"\"\n        Verifies the signature of a signed message, returning the message\n        if it has not been tampered with else raising\n        :class:`~nacl.signing.BadSignatureError`.\n\n        :param smessage: [:class:`bytes`] Either the original messaged or a\n            signature and message concated together.\n        :param signature: [:class:`bytes`] If an unsigned message is given for\n            smessage then the detached signature must be provided.\n        :param encoder: A class that is able to decode the secret message and\n            signature.\n        :rtype: :class:`bytes`\n        \"\"\"\n        if signature is not None:\n            # If we were given the message and signature separately, combine\n            #   them.\n            smessage = signature + encoder.decode(smessage)\n        else:\n            # Decode the signed message\n            smessage = encoder.decode(smessage)\n\n        return nacl.bindings.crypto_sign_open(smessage, self._key)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_curve25519_public_key(self):\n        raw_pk = nacl.bindings.crypto_sign_ed25519_pk_to_curve25519(self._key)\n        return _Curve25519_PublicKey(raw_pk)", "response": "Converts a ~nacl. signing. VerifyKey to a Curve25519 public key."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a random : class : ~nacl. signing. SigningKey object.", "response": "def generate(cls):\n        \"\"\"\n        Generates a random :class:`~nacl.signing.SigningKey` object.\n\n        :rtype: :class:`~nacl.signing.SigningKey`\n        \"\"\"\n        return cls(\n            random(nacl.bindings.crypto_sign_SEEDBYTES),\n            encoder=encoding.RawEncoder,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a ~nacl. signing. SigningKey to a Curve25519 private key.", "response": "def to_curve25519_private_key(self):\n        \"\"\"\n        Converts a :class:`~nacl.signing.SigningKey` to a\n        :class:`~nacl.public.PrivateKey`\n\n        :rtype: :class:`~nacl.public.PrivateKey`\n        \"\"\"\n        sk = self._signing_key\n        raw_private = nacl.bindings.crypto_sign_ed25519_sk_to_curve25519(sk)\n        return _Curve25519_PrivateKey(raw_private)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a new key derivation tree from a caller - supplied password and salt pair.", "response": "def kdf(size, password, salt,\n        opslimit=OPSLIMIT_SENSITIVE,\n        memlimit=MEMLIMIT_SENSITIVE,\n        encoder=nacl.encoding.RawEncoder):\n    \"\"\"\n    Derive a ``size`` bytes long key from a caller-supplied\n    ``password`` and ``salt`` pair using the argon2i\n    memory-hard construct.\n\n    the enclosing module provides the constants\n\n        - :py:const:`.OPSLIMIT_INTERACTIVE`\n        - :py:const:`.MEMLIMIT_INTERACTIVE`\n        - :py:const:`.OPSLIMIT_MODERATE`\n        - :py:const:`.MEMLIMIT_MODERATE`\n        - :py:const:`.OPSLIMIT_SENSITIVE`\n        - :py:const:`.MEMLIMIT_SENSITIVE`\n\n    as a guidance for correct settings.\n\n    :param size: derived key size, must be between\n                 :py:const:`.BYTES_MIN` and\n                 :py:const:`.BYTES_MAX`\n    :type size: int\n    :param password: password used to seed the key derivation procedure;\n                     it length must be between\n                     :py:const:`.PASSWD_MIN` and\n                     :py:const:`.PASSWD_MAX`\n    :type password: bytes\n    :param salt: **RANDOM** salt used in the key derivation procedure;\n                 its length must be exactly :py:const:`.SALTBYTES`\n    :type salt: bytes\n    :param opslimit: the time component (operation count)\n                     of the key derivation procedure's computational cost;\n                     it must be between\n                     :py:const:`.OPSLIMIT_MIN` and\n                     :py:const:`.OPSLIMIT_MAX`\n    :type opslimit: int\n    :param memlimit: the memory occupation component\n                     of the key derivation procedure's computational cost;\n                     it must be between\n                     :py:const:`.MEMLIMIT_MIN` and\n                     :py:const:`.MEMLIMIT_MAX`\n    :type memlimit: int\n    :rtype: bytes\n\n    .. versionadded:: 1.2\n    \"\"\"\n\n    return encoder.encode(\n        nacl.bindings.crypto_pwhash_alg(size, password, salt,\n                                        opslimit, memlimit,\n                                        ALG)\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a ascii string that has the password hashed with a random salt and the needed info to check against a future password.", "response": "def str(password,\n        opslimit=OPSLIMIT_INTERACTIVE,\n        memlimit=MEMLIMIT_INTERACTIVE):\n    \"\"\"\n    Hashes a password with a random salt, using the memory-hard\n    argon2i construct and returning an ascii string that has all\n    the needed info to check against a future password\n\n\n    The default settings for opslimit and memlimit are those deemed\n    correct for the interactive user login case.\n\n    :param bytes password:\n    :param int opslimit:\n    :param int memlimit:\n    :rtype: bytes\n\n    .. versionadded:: 1.2\n    \"\"\"\n    return nacl.bindings.crypto_pwhash_str_alg(password,\n                                               opslimit,\n                                               memlimit,\n                                               ALG)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nderive a cryptographic key using the scrypt KDF", "response": "def scrypt(password, salt='', n=2**20, r=8, p=1,\n           maxmem=2**25, dklen=64):\n    \"\"\"\n    Derive a cryptographic key using the scrypt KDF.\n\n    Implements the same signature as the ``hashlib.scrypt`` implemented\n    in cpython version 3.6\n    \"\"\"\n    return nacl.bindings.crypto_pwhash_scryptsalsa208sha256_ll(\n        password, salt, n, r, p, maxmem=maxmem, dklen=dklen)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nencrypting and returns the message with the secret key and the nonce nonce.", "response": "def crypto_secretbox(message, nonce, key):\n    \"\"\"\n    Encrypts and returns the message ``message`` with the secret ``key`` and\n    the nonce ``nonce``.\n\n    :param message: bytes\n    :param nonce: bytes\n    :param key: bytes\n    :rtype: bytes\n    \"\"\"\n    if len(key) != crypto_secretbox_KEYBYTES:\n        raise exc.ValueError(\"Invalid key\")\n\n    if len(nonce) != crypto_secretbox_NONCEBYTES:\n        raise exc.ValueError(\"Invalid nonce\")\n\n    padded = b\"\\x00\" * crypto_secretbox_ZEROBYTES + message\n    ciphertext = ffi.new(\"unsigned char[]\", len(padded))\n\n    res = lib.crypto_secretbox(ciphertext, padded, len(padded), nonce, key)\n    ensure(res == 0, \"Encryption failed\", raising=exc.CryptoError)\n\n    ciphertext = ffi.buffer(ciphertext, len(padded))\n    return ciphertext[crypto_secretbox_BOXZEROBYTES:]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef crypto_shorthash_siphash24(data, key):\n    if len(key) != KEYBYTES:\n        raise exc.ValueError(\n            \"Key length must be exactly {0} bytes\".format(KEYBYTES))\n    digest = ffi.new(\"unsigned char[]\", BYTES)\n    rc = lib.crypto_shorthash_siphash24(digest, data, len(data), key)\n\n    ensure(rc == 0, raising=exc.RuntimeError)\n    return ffi.buffer(digest, BYTES)[:]", "response": "Compute a fast cryptographic quality keyed hash of the input data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef crypto_shorthash_siphashx24(data, key):\n    if len(key) != XKEYBYTES:\n        raise exc.ValueError(\n            \"Key length must be exactly {0} bytes\".format(XKEYBYTES))\n    digest = ffi.new(\"unsigned char[]\", XBYTES)\n    rc = lib.crypto_shorthash_siphashx24(digest, data, len(data), key)\n\n    ensure(rc == 0, raising=exc.RuntimeError)\n    return ffi.buffer(digest, XBYTES)[:]", "response": "Compute a fast cryptographic quality keyed hash of the input data"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a new blake2b hash state.", "response": "def generichash_blake2b_init(key=b'', salt=b'',\n                             person=b'',\n                             digest_size=crypto_generichash_BYTES):\n    \"\"\"\n    Create a new initialized blake2b hash state\n\n    :param key: must be at most\n                :py:data:`.crypto_generichash_KEYBYTES_MAX` long\n    :type key: bytes\n    :param salt: must be at most\n                 :py:data:`.crypto_generichash_SALTBYTES` long;\n                 will be zero-padded if needed\n    :type salt: bytes\n    :param person: must be at most\n                   :py:data:`.crypto_generichash_PERSONALBYTES` long:\n                   will be zero-padded if needed\n    :type person: bytes\n    :param digest_size: must be at most\n                        :py:data:`.crypto_generichash_BYTES_MAX`;\n                        the default digest size is\n                        :py:data:`.crypto_generichash_BYTES`\n    :type digest_size: int\n    :return: a initialized :py:class:`.Blake2State`\n    :rtype: object\n    \"\"\"\n\n    _checkparams(digest_size, key, salt, person)\n\n    state = Blake2State(digest_size)\n\n    # both _salt and _personal must be zero-padded to the correct length\n    _salt = ffi.new(\"unsigned char []\", crypto_generichash_SALTBYTES)\n    _person = ffi.new(\"unsigned char []\", crypto_generichash_PERSONALBYTES)\n\n    ffi.memmove(_salt, salt, len(salt))\n    ffi.memmove(_person, person, len(person))\n\n    rc = lib.crypto_generichash_blake2b_init_salt_personal(state._statebuf,\n                                                           key, len(key),\n                                                           digest_size,\n                                                           _salt, _person)\n    ensure(rc == 0, 'Unexpected failure',\n           raising=exc.RuntimeError)\n\n    return state"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating the blake2b hash state", "response": "def generichash_blake2b_update(state, data):\n    \"\"\"Update the blake2b hash state\n\n    :param state: a initialized Blake2bState object as returned from\n                     :py:func:`.crypto_generichash_blake2b_init`\n    :type state: :py:class:`.Blake2State`\n    :param data:\n    :type data: bytes\n    \"\"\"\n\n    ensure(isinstance(state, Blake2State),\n           'State must be a Blake2State object',\n           raising=exc.TypeError)\n\n    ensure(isinstance(data, bytes),\n           'Input data must be a bytes sequence',\n           raising=exc.TypeError)\n\n    rc = lib.crypto_generichash_blake2b_update(state._statebuf,\n                                               data, len(data))\n    ensure(rc == 0, 'Unexpected failure',\n           raising=exc.RuntimeError)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinalizes the blake2b hash state and return the digest.", "response": "def generichash_blake2b_final(state):\n    \"\"\"Finalize the blake2b hash state and return the digest.\n\n    :param state: a initialized Blake2bState object as returned from\n                     :py:func:`.crypto_generichash_blake2b_init`\n    :type state: :py:class:`.Blake2State`\n    :return: the blake2 digest of the passed-in data stream\n    :rtype: bytes\n    \"\"\"\n\n    ensure(isinstance(state, Blake2State),\n           'State must be a Blake2State object',\n           raising=exc.TypeError)\n\n    _digest = ffi.new(\"unsigned char[]\", crypto_generichash_BYTES_MAX)\n    rc = lib.crypto_generichash_blake2b_final(state._statebuf,\n                                              _digest, state.digest_size)\n\n    ensure(rc == 0, 'Unexpected failure',\n           raising=exc.RuntimeError)\n    return ffi.buffer(_digest, state.digest_size)[:]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating a key for use with", "response": "def crypto_secretstream_xchacha20poly1305_keygen():\n    \"\"\"\n    Generate a key for use with\n    :func:`.crypto_secretstream_xchacha20poly1305_init_push`.\n\n    \"\"\"\n    keybuf = ffi.new(\n        \"unsigned char[]\",\n        crypto_secretstream_xchacha20poly1305_KEYBYTES,\n    )\n    lib.crypto_secretstream_xchacha20poly1305_keygen(keybuf)\n    return ffi.buffer(keybuf)[:]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef crypto_secretstream_xchacha20poly1305_init_push(state, key):\n    ensure(\n        isinstance(state, crypto_secretstream_xchacha20poly1305_state),\n        'State must be a crypto_secretstream_xchacha20poly1305_state object',\n        raising=exc.TypeError,\n    )\n    ensure(\n        isinstance(key, bytes),\n        'Key must be a bytes sequence',\n        raising=exc.TypeError,\n    )\n    ensure(\n        len(key) == crypto_secretstream_xchacha20poly1305_KEYBYTES,\n        'Invalid key length',\n        raising=exc.ValueError,\n    )\n\n    headerbuf = ffi.new(\n        \"unsigned char []\",\n        crypto_secretstream_xchacha20poly1305_HEADERBYTES,\n    )\n\n    rc = lib.crypto_secretstream_xchacha20poly1305_init_push(\n        state.statebuf, headerbuf, key)\n    ensure(rc == 0, 'Unexpected failure', raising=exc.RuntimeError)\n\n    return ffi.buffer(headerbuf)[:]", "response": "Initialize a crypto_secretstream_xchacha20poly1305 encryption buffer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef crypto_secretstream_xchacha20poly1305_push(\n    state,\n    m,\n    ad=None,\n    tag=crypto_secretstream_xchacha20poly1305_TAG_MESSAGE,\n):\n    \"\"\"\n    Add an encrypted message to the secret stream.\n\n    :param state: a secretstream state object\n    :type state: crypto_secretstream_xchacha20poly1305_state\n    :param m: the message to encrypt, the maximum length of an individual\n              message is\n              :data:`.crypto_secretstream_xchacha20poly1305_MESSAGEBYTES_MAX`.\n    :type m: bytes\n    :param ad: additional data to include in the authentication tag\n    :type ad: bytes or None\n    :param tag: the message tag, usually\n                :data:`.crypto_secretstream_xchacha20poly1305_TAG_MESSAGE` or\n                :data:`.crypto_secretstream_xchacha20poly1305_TAG_FINAL`.\n    :type tag: int\n    :return: ciphertext\n    :rtype: bytes\n\n    \"\"\"\n    ensure(\n        isinstance(state, crypto_secretstream_xchacha20poly1305_state),\n        'State must be a crypto_secretstream_xchacha20poly1305_state object',\n        raising=exc.TypeError,\n    )\n    ensure(isinstance(m, bytes), 'Message is not bytes', raising=exc.TypeError)\n    ensure(\n        len(m) <= crypto_secretstream_xchacha20poly1305_MESSAGEBYTES_MAX,\n        'Message is too long',\n        raising=exc.ValueError,\n    )\n    ensure(\n        ad is None or isinstance(ad, bytes),\n        'Additional data must be bytes or None',\n        raising=exc.TypeError,\n    )\n\n    clen = len(m) + crypto_secretstream_xchacha20poly1305_ABYTES\n    if state.rawbuf is None or len(state.rawbuf) < clen:\n        state.rawbuf = ffi.new('unsigned char[]', clen)\n\n    if ad is None:\n        ad = ffi.NULL\n        adlen = 0\n    else:\n        adlen = len(ad)\n\n    rc = lib.crypto_secretstream_xchacha20poly1305_push(\n        state.statebuf,\n        state.rawbuf, ffi.NULL,\n        m, len(m),\n        ad, adlen,\n        tag,\n    )\n    ensure(rc == 0, 'Unexpected failure', raising=exc.RuntimeError)\n\n    return ffi.buffer(state.rawbuf, clen)[:]", "response": "Adds a message to the secret stream."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninitializing a crypto_secretstream_xchacha20poly1305 decryption buffer.", "response": "def crypto_secretstream_xchacha20poly1305_init_pull(state, header, key):\n    \"\"\"\n    Initialize a crypto_secretstream_xchacha20poly1305 decryption buffer.\n\n    :param state: a secretstream state object\n    :type state: crypto_secretstream_xchacha20poly1305_state\n    :param header: must be\n                :data:`.crypto_secretstream_xchacha20poly1305_HEADERBYTES` long\n    :type header: bytes\n    :param key: must be\n                :data:`.crypto_secretstream_xchacha20poly1305_KEYBYTES` long\n    :type key: bytes\n\n    \"\"\"\n    ensure(\n        isinstance(state, crypto_secretstream_xchacha20poly1305_state),\n        'State must be a crypto_secretstream_xchacha20poly1305_state object',\n        raising=exc.TypeError,\n    )\n    ensure(\n        isinstance(header, bytes),\n        'Header must be a bytes sequence',\n        raising=exc.TypeError,\n    )\n    ensure(\n        len(header) == crypto_secretstream_xchacha20poly1305_HEADERBYTES,\n        'Invalid header length',\n        raising=exc.ValueError,\n    )\n    ensure(\n        isinstance(key, bytes),\n        'Key must be a bytes sequence',\n        raising=exc.TypeError,\n    )\n    ensure(\n        len(key) == crypto_secretstream_xchacha20poly1305_KEYBYTES,\n        'Invalid key length',\n        raising=exc.ValueError,\n    )\n\n    if state.tagbuf is None:\n        state.tagbuf = ffi.new('unsigned char *')\n\n    rc = lib.crypto_secretstream_xchacha20poly1305_init_pull(\n        state.statebuf, header, key)\n    ensure(rc == 0, 'Unexpected failure', raising=exc.RuntimeError)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef crypto_secretstream_xchacha20poly1305_pull(state, c, ad=None):\n    ensure(\n        isinstance(state, crypto_secretstream_xchacha20poly1305_state),\n        'State must be a crypto_secretstream_xchacha20poly1305_state object',\n        raising=exc.TypeError,\n    )\n    ensure(\n        state.tagbuf is not None,\n        (\n            'State must be initialized using '\n            'crypto_secretstream_xchacha20poly1305_init_pull'\n        ),\n        raising=exc.ValueError,\n    )\n    ensure(\n        isinstance(c, bytes),\n        'Ciphertext is not bytes',\n        raising=exc.TypeError,\n    )\n    ensure(\n        len(c) > crypto_secretstream_xchacha20poly1305_ABYTES,\n        'Ciphertext is too short',\n        raising=exc.ValueError,\n    )\n    ensure(\n        len(c) <= (\n            crypto_secretstream_xchacha20poly1305_MESSAGEBYTES_MAX +\n            crypto_secretstream_xchacha20poly1305_ABYTES\n        ),\n        'Ciphertext is too long',\n        raising=exc.ValueError,\n    )\n    ensure(\n        ad is None or isinstance(ad, bytes),\n        'Additional data must be bytes or None',\n        raising=exc.TypeError,\n    )\n\n    mlen = len(c) - crypto_secretstream_xchacha20poly1305_ABYTES\n    if state.rawbuf is None or len(state.rawbuf) < mlen:\n        state.rawbuf = ffi.new('unsigned char[]', mlen)\n\n    if ad is None:\n        ad = ffi.NULL\n        adlen = 0\n    else:\n        adlen = len(ad)\n\n    rc = lib.crypto_secretstream_xchacha20poly1305_pull(\n        state.statebuf,\n        state.rawbuf, ffi.NULL,\n        state.tagbuf,\n        c, len(c),\n        ad, adlen,\n    )\n    ensure(rc == 0, 'Unexpected failure', raising=exc.RuntimeError)\n\n    return (ffi.buffer(state.rawbuf, mlen)[:], int(state.tagbuf[0]))", "response": "Read a decrypted message from the secret stream."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef crypto_secretstream_xchacha20poly1305_rekey(state):\n    ensure(\n        isinstance(state, crypto_secretstream_xchacha20poly1305_state),\n        'State must be a crypto_secretstream_xchacha20poly1305_state object',\n        raising=exc.TypeError,\n    )\n    lib.crypto_secretstream_xchacha20poly1305_rekey(state.statebuf)", "response": "Change the encryption key in the stream."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef encrypt(self, plaintext, nonce=None, encoder=encoding.RawEncoder):\n        if nonce is None:\n            nonce = random(self.NONCE_SIZE)\n\n        if len(nonce) != self.NONCE_SIZE:\n            raise exc.ValueError(\n                \"The nonce must be exactly %s bytes long\" % self.NONCE_SIZE,\n            )\n\n        ciphertext = nacl.bindings.crypto_secretbox(plaintext,\n                                                    nonce, self._key)\n\n        encoded_nonce = encoder.encode(nonce)\n        encoded_ciphertext = encoder.encode(ciphertext)\n\n        return EncryptedMessage._from_parts(\n            encoded_nonce,\n            encoded_ciphertext,\n            encoder.encode(nonce + ciphertext),\n        )", "response": "Encrypts the given plaintext message using the given nonce and returns the ciphertext encoded with the encoder."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef verify(password_hash, password):\n    if password_hash.startswith(argon2id.STRPREFIX):\n        return argon2id.verify(password_hash, password)\n    elif password_hash.startswith(argon2i.STRPREFIX):\n        return argon2id.verify(password_hash, password)\n    elif password_hash.startswith(scrypt.STRPREFIX):\n        return scrypt.verify(password_hash, password)\n    else:\n        raise(CryptPrefixError(\"given password_hash is not \"\n                               \"in a supported format\"\n                               )\n              )", "response": "Verify the given password hash using one of the modular crypt encoded stored password algorithms supported by libsodium and checks if the user provided password hash is the same string as the given password hash."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nderiving a key from a caller - supplied key pair using the scryptsalsa208sha256 key derivation procedure.", "response": "def kdf(size, password, salt,\n        opslimit=OPSLIMIT_SENSITIVE,\n        memlimit=MEMLIMIT_SENSITIVE,\n        encoder=nacl.encoding.RawEncoder):\n    \"\"\"\n    Derive a ``size`` bytes long key from a caller-supplied\n    ``password`` and ``salt`` pair using the scryptsalsa208sha256\n    memory-hard construct.\n\n\n    the enclosing module provides the constants\n\n        - :py:const:`.OPSLIMIT_INTERACTIVE`\n        - :py:const:`.MEMLIMIT_INTERACTIVE`\n        - :py:const:`.OPSLIMIT_SENSITIVE`\n        - :py:const:`.MEMLIMIT_SENSITIVE`\n        - :py:const:`.OPSLIMIT_MODERATE`\n        - :py:const:`.MEMLIMIT_MODERATE`\n\n    as a guidance for correct settings respectively for the\n    interactive login and the long term key protecting sensitive data\n    use cases.\n\n    :param size: derived key size, must be between\n                 :py:const:`.BYTES_MIN` and\n                 :py:const:`.BYTES_MAX`\n    :type size: int\n    :param password: password used to seed the key derivation procedure;\n                     it length must be between\n                     :py:const:`.PASSWD_MIN` and\n                     :py:const:`.PASSWD_MAX`\n    :type password: bytes\n    :param salt: **RANDOM** salt used in the key derivation procedure;\n                 its length must be exactly :py:const:`.SALTBYTES`\n    :type salt: bytes\n    :param opslimit: the time component (operation count)\n                     of the key derivation procedure's computational cost;\n                     it must be between\n                     :py:const:`.OPSLIMIT_MIN` and\n                     :py:const:`.OPSLIMIT_MAX`\n    :type opslimit: int\n    :param memlimit: the memory occupation component\n                     of the key derivation procedure's computational cost;\n                     it must be between\n                     :py:const:`.MEMLIMIT_MIN` and\n                     :py:const:`.MEMLIMIT_MAX`\n    :type memlimit: int\n    :rtype: bytes\n\n    .. versionadded:: 1.2\n    \"\"\"\n    ensure(\n        len(salt) == SALTBYTES,\n        \"The salt must be exactly %s, not %s bytes long\" % (\n            SALTBYTES,\n            len(salt)\n        ),\n        raising=exc.ValueError\n    )\n\n    n_log2, r, p = nacl.bindings.nacl_bindings_pick_scrypt_params(opslimit,\n                                                                  memlimit)\n    maxmem = memlimit + (2 ** 16)\n\n    return encoder.encode(\n        nacl.bindings.crypto_pwhash_scryptsalsa208sha256_ll(\n            password, salt, 2 ** n_log2, r, p, maxmem=maxmem, dklen=size)\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a ascii string of the password with a random salt using the memory - hardton algorithm.", "response": "def str(password,\n        opslimit=OPSLIMIT_INTERACTIVE,\n        memlimit=MEMLIMIT_INTERACTIVE):\n    \"\"\"\n    Hashes a password with a random salt, using the memory-hard\n    scryptsalsa208sha256 construct and returning an ascii string\n    that has all the needed info to check against a future password\n\n    The default settings for opslimit and memlimit are those deemed\n    correct for the interactive user login case.\n\n    :param bytes password:\n    :param int opslimit:\n    :param int memlimit:\n    :rtype: bytes\n\n    .. versionadded:: 1.2\n    \"\"\"\n\n    return nacl.bindings.crypto_pwhash_scryptsalsa208sha256_str(password,\n                                                                opslimit,\n                                                                memlimit)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef verify(password_hash, password):\n\n    ensure(len(password_hash) == PWHASH_SIZE,\n           \"The password hash must be exactly %s bytes long\" %\n           nacl.bindings.crypto_pwhash_scryptsalsa208sha256_STRBYTES,\n           raising=exc.ValueError)\n\n    return nacl.bindings.crypto_pwhash_scryptsalsa208sha256_str_verify(\n        password_hash, password\n    )", "response": "Verifies that the password hash is the same as the user provided password."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating a PrivateKey from a seed.", "response": "def from_seed(cls, seed, encoder=encoding.RawEncoder):\n        \"\"\"\n        Generate a PrivateKey using a deterministic construction\n        starting from a caller-provided seed\n\n        .. warning:: The seed **must** be high-entropy; therefore,\n            its generator **must** be a cryptographic quality\n            random function like, for example, :func:`~nacl.utils.random`.\n\n        .. warning:: The seed **must** be protected and remain secret.\n            Anyone who knows the seed is really in possession of\n            the corresponding PrivateKey.\n\n        :param seed: The seed used to generate the private key\n        :rtype: :class:`~nacl.public.PrivateKey`\n        \"\"\"\n        # decode the seed\n        seed = encoder.decode(seed)\n        # Verify the given seed type and size are correct\n        if not (isinstance(seed, bytes) and len(seed) == cls.SEED_SIZE):\n            raise exc.TypeError((\"PrivateKey seed must be a {0} bytes long \"\n                                 \"binary sequence\").format(cls.SEED_SIZE)\n                                )\n        # generate a raw keypair from the given seed\n        raw_pk, raw_sk = nacl.bindings.crypto_box_seed_keypair(seed)\n        # construct a instance from the raw secret key\n        return cls(raw_sk)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef decrypt(self, ciphertext, nonce=None, encoder=encoding.RawEncoder):\n        # Decode our ciphertext\n        ciphertext = encoder.decode(ciphertext)\n\n        if nonce is None:\n            # If we were given the nonce and ciphertext combined, split them.\n            nonce = ciphertext[:self.NONCE_SIZE]\n            ciphertext = ciphertext[self.NONCE_SIZE:]\n\n        if len(nonce) != self.NONCE_SIZE:\n            raise exc.ValueError(\"The nonce must be exactly %s bytes long\" %\n                                 self.NONCE_SIZE)\n\n        plaintext = nacl.bindings.crypto_box_open_afternm(\n            ciphertext,\n            nonce,\n            self._shared_key,\n        )\n\n        return plaintext", "response": "Decrypts the ciphertext using the nonce and returns the plaintext message."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef encrypt(self, plaintext, encoder=encoding.RawEncoder):\n\n        ciphertext = nacl.bindings.crypto_box_seal(\n            plaintext,\n            self._public_key\n        )\n\n        encoded_ciphertext = encoder.encode(ciphertext)\n\n        return encoded_ciphertext", "response": "Encrypts the plaintext message using a random - generated ephemeral key - pair and returns a composed ciphertext."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreleases the current version of the Jenkins.", "response": "def release(ctx, version):\n    \"\"\"\n    ``version`` should be a string like '0.4' or '1.0'.\n    \"\"\"\n    invoke.run(\"git tag -s {0} -m '{0} release'\".format(version))\n    invoke.run(\"git push --tags\")\n\n    invoke.run(\"python setup.py sdist\")\n\n    invoke.run(\"twine upload -s dist/PyNaCl-{0}* \".format(version))\n\n    session = requests.Session()\n\n    token = getpass.getpass(\"Input the Jenkins token: \")\n    response = session.post(\n        \"{0}/build\".format(JENKINS_URL),\n        params={\n            \"cause\": \"Building wheels for {0}\".format(version),\n            \"token\": token\n        }\n    )\n    response.raise_for_status()\n    wait_for_build_completed(session)\n    paths = download_artifacts(session)\n    invoke.run(\"twine upload {0}\".format(\" \".join(paths)))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning size number of random bytes from a cryptographically secure random source.", "response": "def randombytes(size):\n    \"\"\"\n    Returns ``size`` number of random bytes from a cryptographically secure\n    random source.\n\n    :param size: int\n    :rtype: bytes\n    \"\"\"\n    buf = ffi.new(\"unsigned char[]\", size)\n    lib.randombytes(buf, size)\n    return ffi.buffer(buf, size)[:]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef crypto_scalarmult_base(n):\n    q = ffi.new(\"unsigned char[]\", crypto_scalarmult_BYTES)\n\n    rc = lib.crypto_scalarmult_base(q, n)\n    ensure(rc == 0,\n           'Unexpected library error',\n           raising=exc.RuntimeError)\n\n    return ffi.buffer(q, crypto_scalarmult_SCALARBYTES)[:]", "response": "Computes and returns the scalar product of a standard group element and an an\n    integer n."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef crypto_scalarmult_ed25519_base(n):\n    ensure(isinstance(n, bytes) and\n           len(n) == crypto_scalarmult_ed25519_SCALARBYTES,\n           'Input must be a {} long bytes sequence'.format(\n           'crypto_scalarmult_ed25519_SCALARBYTES'),\n           raising=exc.TypeError)\n\n    q = ffi.new(\"unsigned char[]\", crypto_scalarmult_ed25519_BYTES)\n\n    rc = lib.crypto_scalarmult_ed25519_base(q, n)\n    ensure(rc == 0,\n           'Unexpected library error',\n           raising=exc.RuntimeError)\n\n    return ffi.buffer(q, crypto_scalarmult_ed25519_BYTES)[:]", "response": "Computes and returns the scalar product of a standard group element and an\n    integer n on the edwards25519 curve."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute and returns the scalar product of a given integer n and a given group element on the edwards25519 curve.", "response": "def crypto_scalarmult_ed25519(n, p):\n    \"\"\"\n    Computes and returns the scalar product of a *clamped* integer ``n``\n    and the given group element on the edwards25519 curve.\n    The scalar is clamped, as done in the public key generation case,\n    by setting to zero the bits in position [0, 1, 2, 255] and setting\n    to one the bit in position 254.\n\n    :param n: a :py:data:`.crypto_scalarmult_ed25519_SCALARBYTES` long bytes\n              sequence representing a scalar\n    :type n: bytes\n    :param p: a :py:data:`.crypto_scalarmult_ed25519_BYTES` long bytes sequence\n              representing a point on the edwards25519 curve\n    :type p: bytes\n    :return: a point on the edwards25519 curve, represented as a\n             :py:data:`.crypto_scalarmult_ed25519_BYTES` long bytes sequence\n    :rtype: bytes\n    \"\"\"\n    ensure(isinstance(n, bytes) and\n           len(n) == crypto_scalarmult_ed25519_SCALARBYTES,\n           'Input must be a {} long bytes sequence'.format(\n           'crypto_scalarmult_ed25519_SCALARBYTES'),\n           raising=exc.TypeError)\n\n    ensure(isinstance(p, bytes) and\n           len(p) == crypto_scalarmult_ed25519_BYTES,\n           'Input must be a {} long bytes sequence'.format(\n           'crypto_scalarmult_ed25519_BYTES'),\n           raising=exc.TypeError)\n\n    q = ffi.new(\"unsigned char[]\", crypto_scalarmult_ed25519_BYTES)\n\n    rc = lib.crypto_scalarmult_ed25519(q, n, p)\n    ensure(rc == 0,\n           'Unexpected library error',\n           raising=exc.RuntimeError)\n\n    return ffi.buffer(q, crypto_scalarmult_ed25519_BYTES)[:]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate a keypair. This is a duplicate crypto_box_keypair, but is included for api consistency. :return: (public_key, secret_key) :rtype: (bytes, bytes)", "response": "def crypto_kx_keypair():\n    \"\"\"\n    Generate a keypair.\n    This is a duplicate crypto_box_keypair, but\n    is included for api consistency.\n    :return: (public_key, secret_key)\n    :rtype: (bytes, bytes)\n    \"\"\"\n    public_key = ffi.new(\"unsigned char[]\", crypto_kx_PUBLIC_KEY_BYTES)\n    secret_key = ffi.new(\"unsigned char[]\", crypto_kx_SECRET_KEY_BYTES)\n    res = lib.crypto_kx_keypair(public_key, secret_key)\n    ensure(res == 0, \"Key generation failed.\", raising=exc.CryptoError)\n\n    return (ffi.buffer(public_key, crypto_kx_PUBLIC_KEY_BYTES)[:],\n            ffi.buffer(secret_key, crypto_kx_SECRET_KEY_BYTES)[:])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a keypair with a given seed.", "response": "def crypto_kx_seed_keypair(seed):\n    \"\"\"\n    Generate a keypair with a given seed.\n    This is functionally the same as crypto_box_seed_keypair, however\n    it uses the blake2b hash primitive instead of sha512.\n    It is included mainly for api consistency when using crypto_kx.\n    :param seed: random seed\n    :type seed: bytes\n    :return: (public_key, secret_key)\n    :rtype: (bytes, bytes)\n    \"\"\"\n    public_key = ffi.new(\"unsigned char[]\", crypto_kx_PUBLIC_KEY_BYTES)\n    secret_key = ffi.new(\"unsigned char[]\", crypto_kx_SECRET_KEY_BYTES)\n    ensure(isinstance(seed, bytes) and\n           len(seed) == crypto_kx_SEED_BYTES,\n           'Seed must be a {0} byte long bytes sequence'.format(\n               crypto_kx_SEED_BYTES),\n           raising=exc.TypeError)\n    res = lib.crypto_kx_seed_keypair(public_key, secret_key, seed)\n    ensure(res == 0, \"Key generation failed.\", raising=exc.CryptoError)\n\n    return (ffi.buffer(public_key, crypto_kx_PUBLIC_KEY_BYTES)[:],\n            ffi.buffer(secret_key, crypto_kx_SECRET_KEY_BYTES)[:])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef crypto_kx_client_session_keys(client_public_key,\n                                  client_secret_key,\n                                  server_public_key):\n    \"\"\"\n    Generate session keys for the client.\n    :param client_public_key:\n    :type client_public_key: bytes\n    :param client_secret_key:\n    :type client_secret_key: bytes\n    :param server_public_key:\n    :type server_public_key: bytes\n    :return: (rx_key, tx_key)\n    :rtype: (bytes, bytes)\n    \"\"\"\n    ensure(isinstance(client_public_key, bytes) and\n           len(client_public_key) == crypto_kx_PUBLIC_KEY_BYTES,\n           'Client public key must be a {0} bytes long bytes sequence'.format(\n               crypto_kx_PUBLIC_KEY_BYTES),\n           raising=exc.TypeError)\n    ensure(isinstance(client_secret_key, bytes) and\n           len(client_secret_key) == crypto_kx_SECRET_KEY_BYTES,\n           'Client secret key must be a {0} bytes long bytes sequence'.format(\n               crypto_kx_PUBLIC_KEY_BYTES),\n           raising=exc.TypeError)\n    ensure(isinstance(server_public_key, bytes) and\n           len(server_public_key) == crypto_kx_PUBLIC_KEY_BYTES,\n           'Server public key must be a {0} bytes long bytes sequence'.format(\n               crypto_kx_PUBLIC_KEY_BYTES),\n           raising=exc.TypeError)\n\n    rx_key = ffi.new(\"unsigned char[]\", crypto_kx_SESSION_KEY_BYTES)\n    tx_key = ffi.new(\"unsigned char[]\", crypto_kx_SESSION_KEY_BYTES)\n    res = lib.crypto_kx_client_session_keys(rx_key,\n                                            tx_key,\n                                            client_public_key,\n                                            client_secret_key,\n                                            server_public_key)\n    ensure(res == 0,\n           \"Client session key generation failed.\",\n           raising=exc.CryptoError)\n\n    return (ffi.buffer(rx_key, crypto_kx_SESSION_KEY_BYTES)[:],\n            ffi.buffer(tx_key, crypto_kx_SESSION_KEY_BYTES)[:])", "response": "Generates the client s session keys."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef nacl_bindings_pick_scrypt_params(opslimit, memlimit):\n\n    if opslimit < 32768:\n        opslimit = 32768\n\n    r = 8\n\n    if opslimit < (memlimit // 32):\n        p = 1\n        maxn = opslimit // (4 * r)\n        for n_log2 in range(1, 63):  # pragma: no branch\n            if (2 ** n_log2) > (maxn // 2):\n                break\n    else:\n        maxn = memlimit // (r * 128)\n        for n_log2 in range(1, 63):  # pragma: no branch\n            if (2 ** n_log2) > maxn // 2:\n                break\n\n        maxrp = (opslimit // 4) // (2 ** n_log2)\n\n        if maxrp > 0x3fffffff:  # pragma: no cover\n            maxrp = 0x3fffffff\n\n        p = maxrp // r\n\n    return n_log2, r, p", "response": "Python implementation of libsodium s pickparams"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nderive a cryptographic key using the passwd and salt.", "response": "def crypto_pwhash_scryptsalsa208sha256_ll(passwd, salt, n, r, p, dklen=64,\n                                          maxmem=SCRYPT_MAX_MEM):\n    \"\"\"\n    Derive a cryptographic key using the ``passwd`` and ``salt``\n    given as input.\n\n    The work factor can be tuned by by picking different\n    values for the parameters\n\n    :param bytes passwd:\n    :param bytes salt:\n    :param bytes salt: *must* be *exactly* :py:const:`.SALTBYTES` long\n    :param int dklen:\n    :param int opslimit:\n    :param int n:\n    :param int r: block size,\n    :param int p: the parallelism factor\n    :param int maxmem: the maximum available memory available for scrypt's\n                       operations\n    :rtype: bytes\n    \"\"\"\n    ensure(isinstance(n, integer_types),\n           raising=TypeError)\n    ensure(isinstance(r, integer_types),\n           raising=TypeError)\n    ensure(isinstance(p, integer_types),\n           raising=TypeError)\n\n    ensure(isinstance(passwd, bytes),\n           raising=TypeError)\n    ensure(isinstance(salt, bytes),\n           raising=TypeError)\n\n    _check_memory_occupation(n, r, p, maxmem)\n\n    buf = ffi.new(\"uint8_t[]\", dklen)\n\n    ret = lib.crypto_pwhash_scryptsalsa208sha256_ll(passwd, len(passwd),\n                                                    salt, len(salt),\n                                                    n, r, p,\n                                                    buf, dklen)\n\n    ensure(ret == 0, 'Unexpected failure in key derivation',\n           raising=exc.RuntimeError)\n\n    return ffi.buffer(ffi.cast(\"char *\", buf), dklen)[:]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nderive a cryptographic key using passwd and salt and return a string representation of the cryptographic key.", "response": "def crypto_pwhash_scryptsalsa208sha256_str(\n        passwd, opslimit=SCRYPT_OPSLIMIT_INTERACTIVE,\n        memlimit=SCRYPT_MEMLIMIT_INTERACTIVE):\n    \"\"\"\n    Derive a cryptographic key using the ``passwd`` and ``salt``\n    given as input, returning a string representation which includes\n    the salt and the tuning parameters.\n\n    The returned string can be directly stored as a password hash.\n\n    See :py:func:`.crypto_pwhash_scryptsalsa208sha256` for a short\n    discussion about ``opslimit`` and ``memlimit`` values.\n\n    :param bytes passwd:\n    :param int opslimit:\n    :param int memlimit:\n    :return: serialized key hash, including salt and tuning parameters\n    :rtype: bytes\n    \"\"\"\n    buf = ffi.new(\"char[]\", SCRYPT_STRBYTES)\n\n    ret = lib.crypto_pwhash_scryptsalsa208sha256_str(buf, passwd,\n                                                     len(passwd),\n                                                     opslimit,\n                                                     memlimit)\n\n    ensure(ret == 0, 'Unexpected failure in password hashing',\n           raising=exc.RuntimeError)\n\n    return ffi.string(buf)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef crypto_pwhash_scryptsalsa208sha256_str_verify(passwd_hash, passwd):\n\n    ensure(len(passwd_hash) == SCRYPT_STRBYTES - 1, 'Invalid password hash',\n           raising=exc.ValueError)\n\n    ret = lib.crypto_pwhash_scryptsalsa208sha256_str_verify(passwd_hash,\n                                                            passwd,\n                                                            len(passwd))\n    ensure(ret == 0,\n           \"Wrong password\",\n           raising=exc.InvalidkeyError)\n    # all went well, therefore:\n    return True", "response": "Verifies the passwd against the passwd_hash that was generated by the encryption key."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef crypto_pwhash_alg(outlen, passwd, salt, opslimit, memlimit, alg):\n    ensure(isinstance(outlen, integer_types),\n           raising=exc.TypeError)\n    ensure(isinstance(opslimit, integer_types),\n           raising=exc.TypeError)\n    ensure(isinstance(memlimit, integer_types),\n           raising=exc.TypeError)\n    ensure(isinstance(alg, integer_types),\n           raising=exc.TypeError)\n    ensure(isinstance(passwd, bytes),\n           raising=exc.TypeError)\n\n    if len(salt) != crypto_pwhash_SALTBYTES:\n        raise exc.ValueError(\"salt must be exactly {0} bytes long\".format(\n            crypto_pwhash_SALTBYTES))\n\n    if outlen < crypto_pwhash_BYTES_MIN:\n        raise exc.ValueError(\n            'derived key must be at least {0} bytes long'.format(\n                crypto_pwhash_BYTES_MIN))\n\n    elif outlen > crypto_pwhash_BYTES_MAX:\n        raise exc.ValueError(\n            'derived key must be at most {0} bytes long'.format(\n                crypto_pwhash_BYTES_MAX))\n\n    _check_argon2_limits_alg(opslimit, memlimit, alg)\n\n    outbuf = ffi.new(\"unsigned char[]\", outlen)\n\n    ret = lib.crypto_pwhash(outbuf, outlen, passwd, len(passwd),\n                            salt, opslimit, memlimit, alg)\n\n    ensure(ret == 0, 'Unexpected failure in key derivation',\n           raising=exc.RuntimeError)\n\n    return ffi.buffer(outbuf, outlen)[:]", "response": "Derive a raw cryptographic key using the passwd and salt."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nderive a cryptographic key using the passwd given as input and a random salt and tuning parameters and the used algorithm.", "response": "def crypto_pwhash_str_alg(passwd, opslimit, memlimit, alg):\n    \"\"\"\n    Derive a cryptographic key using the ``passwd`` given as input\n    and a random ``salt``, returning a string representation which\n    includes the salt, the tuning parameters and the used algorithm.\n\n    :param passwd: The input password\n    :type passwd: bytes\n    :param opslimit: computational cost\n    :type opslimit: int\n    :param memlimit: memory cost\n    :type memlimit: int\n    :param alg: The algorithm to use\n    :type alg: int\n    :return: serialized derived key and parameters\n    :rtype: bytes\n    \"\"\"\n    ensure(isinstance(opslimit, integer_types),\n           raising=TypeError)\n    ensure(isinstance(memlimit, integer_types),\n           raising=TypeError)\n    ensure(isinstance(passwd, bytes),\n           raising=TypeError)\n\n    _check_argon2_limits_alg(opslimit, memlimit, alg)\n\n    outbuf = ffi.new(\"char[]\", 128)\n\n    ret = lib.crypto_pwhash_str_alg(outbuf, passwd, len(passwd),\n                                    opslimit, memlimit, alg)\n\n    ensure(ret == 0, 'Unexpected failure in key derivation',\n           raising=exc.RuntimeError)\n\n    return ffi.string(outbuf)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef crypto_pwhash_str_verify(passwd_hash, passwd):\n    ensure(isinstance(passwd_hash, bytes),\n           raising=TypeError)\n    ensure(isinstance(passwd, bytes),\n           raising=TypeError)\n    ensure(len(passwd_hash) <= 127,\n           \"Hash must be at most 127 bytes long\",\n           raising=exc.ValueError)\n\n    ret = lib.crypto_pwhash_str_verify(passwd_hash, passwd, len(passwd))\n\n    ensure(ret == 0,\n           \"Wrong password\",\n           raising=exc.InvalidkeyError)\n    # all went well, therefore:\n    return True", "response": "Verifies the passwd against a given password hash."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef crypto_aead_chacha20poly1305_ietf_encrypt(message, aad, nonce, key):\n    ensure(isinstance(message, bytes), 'Input message type must be bytes',\n           raising=exc.TypeError)\n\n    mlen = len(message)\n\n    ensure(mlen <= crypto_aead_chacha20poly1305_ietf_MESSAGEBYTES_MAX,\n           'Message must be at most {0} bytes long'.format(\n               crypto_aead_chacha20poly1305_ietf_MESSAGEBYTES_MAX),\n           raising=exc.ValueError)\n\n    ensure(isinstance(aad, bytes) or (aad is None),\n           'Additional data must be bytes or None',\n           raising=exc.TypeError)\n\n    ensure(isinstance(nonce, bytes) and\n           len(nonce) == crypto_aead_chacha20poly1305_ietf_NPUBBYTES,\n           'Nonce must be a {0} bytes long bytes sequence'.format(\n               crypto_aead_chacha20poly1305_ietf_NPUBBYTES),\n           raising=exc.TypeError)\n\n    ensure(isinstance(key, bytes) and\n           len(key) == crypto_aead_chacha20poly1305_ietf_KEYBYTES,\n           'Key must be a {0} bytes long bytes sequence'.format(\n               crypto_aead_chacha20poly1305_ietf_KEYBYTES),\n           raising=exc.TypeError)\n\n    if aad:\n        _aad = aad\n        aalen = len(aad)\n    else:\n        _aad = ffi.NULL\n        aalen = 0\n\n    mxout = mlen + crypto_aead_chacha20poly1305_ietf_ABYTES\n\n    clen = ffi.new(\"unsigned long long *\")\n\n    ciphertext = ffi.new(\"unsigned char[]\", mxout)\n\n    res = lib.crypto_aead_chacha20poly1305_ietf_encrypt(ciphertext,\n                                                        clen,\n                                                        message,\n                                                        mlen,\n                                                        _aad,\n                                                        aalen,\n                                                        ffi.NULL,\n                                                        nonce,\n                                                        key)\n\n    ensure(res == 0, \"Encryption failed.\", raising=exc.CryptoError)\n    return ffi.buffer(ciphertext, clen[0])[:]", "response": "Encrypts the given message using the IETF ratified chacha20poly1305 - 1. 0 key and returns the authenticated ciphertext."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef crypto_aead_chacha20poly1305_encrypt(message, aad, nonce, key):\n    ensure(isinstance(message, bytes), 'Input message type must be bytes',\n           raising=exc.TypeError)\n\n    mlen = len(message)\n\n    ensure(mlen <= crypto_aead_chacha20poly1305_MESSAGEBYTES_MAX,\n           'Message must be at most {0} bytes long'.format(\n               crypto_aead_chacha20poly1305_MESSAGEBYTES_MAX),\n           raising=exc.ValueError)\n\n    ensure(isinstance(aad, bytes) or (aad is None),\n           'Additional data must be bytes or None',\n           raising=exc.TypeError)\n\n    ensure(isinstance(nonce, bytes) and\n           len(nonce) == crypto_aead_chacha20poly1305_NPUBBYTES,\n           'Nonce must be a {0} bytes long bytes sequence'.format(\n               crypto_aead_chacha20poly1305_NPUBBYTES),\n           raising=exc.TypeError)\n\n    ensure(isinstance(key, bytes) and\n           len(key) == crypto_aead_chacha20poly1305_KEYBYTES,\n           'Key must be a {0} bytes long bytes sequence'.format(\n               crypto_aead_chacha20poly1305_KEYBYTES),\n           raising=exc.TypeError)\n\n    if aad:\n        _aad = aad\n        aalen = len(aad)\n    else:\n        _aad = ffi.NULL\n        aalen = 0\n\n    mlen = len(message)\n    mxout = mlen + crypto_aead_chacha20poly1305_ietf_ABYTES\n\n    clen = ffi.new(\"unsigned long long *\")\n\n    ciphertext = ffi.new(\"unsigned char[]\", mxout)\n\n    res = lib.crypto_aead_chacha20poly1305_encrypt(ciphertext,\n                                                   clen,\n                                                   message,\n                                                   mlen,\n                                                   _aad,\n                                                   aalen,\n                                                   ffi.NULL,\n                                                   nonce,\n                                                   key)\n\n    ensure(res == 0, \"Encryption failed.\", raising=exc.CryptoError)\n    return ffi.buffer(ciphertext, clen[0])[:]", "response": "Encrypts the given message using the legacy construction of the Chacha20Poly1305 cipher."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef crypto_aead_xchacha20poly1305_ietf_encrypt(message, aad, nonce, key):\n    ensure(isinstance(message, bytes), 'Input message type must be bytes',\n           raising=exc.TypeError)\n\n    mlen = len(message)\n\n    ensure(mlen <= crypto_aead_xchacha20poly1305_ietf_MESSAGEBYTES_MAX,\n           'Message must be at most {0} bytes long'.format(\n               crypto_aead_xchacha20poly1305_ietf_MESSAGEBYTES_MAX),\n           raising=exc.ValueError)\n\n    ensure(isinstance(aad, bytes) or (aad is None),\n           'Additional data must be bytes or None',\n           raising=exc.TypeError)\n\n    ensure(isinstance(nonce, bytes) and\n           len(nonce) == crypto_aead_xchacha20poly1305_ietf_NPUBBYTES,\n           'Nonce must be a {0} bytes long bytes sequence'.format(\n               crypto_aead_xchacha20poly1305_ietf_NPUBBYTES),\n           raising=exc.TypeError)\n\n    ensure(isinstance(key, bytes) and\n           len(key) == crypto_aead_xchacha20poly1305_ietf_KEYBYTES,\n           'Key must be a {0} bytes long bytes sequence'.format(\n               crypto_aead_xchacha20poly1305_ietf_KEYBYTES),\n           raising=exc.TypeError)\n\n    if aad:\n        _aad = aad\n        aalen = len(aad)\n    else:\n        _aad = ffi.NULL\n        aalen = 0\n\n    mlen = len(message)\n    mxout = mlen + crypto_aead_xchacha20poly1305_ietf_ABYTES\n\n    clen = ffi.new(\"unsigned long long *\")\n\n    ciphertext = ffi.new(\"unsigned char[]\", mxout)\n\n    res = lib.crypto_aead_xchacha20poly1305_ietf_encrypt(ciphertext,\n                                                         clen,\n                                                         message,\n                                                         mlen,\n                                                         _aad,\n                                                         aalen,\n                                                         ffi.NULL,\n                                                         nonce,\n                                                         key)\n\n    ensure(res == 0, \"Encryption failed.\", raising=exc.CryptoError)\n    return ffi.buffer(ciphertext, clen[0])[:]", "response": "Encrypts the given message using the long - nonces xchacha20poly1305 - xchacha20poly1305 - xchacha20poly1305 - xchacha20poly1305 - xchacha20poly1305 - xchacha20poly1305 - xchacha20poly1305 - xchacha20poly1305 - xchacha20poly1305 - xchacha20poly1305."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef crypto_core_ed25519_is_valid_point(p):\n    ensure(isinstance(p, bytes) and len(p) == crypto_core_ed25519_BYTES,\n           'Point must be a crypto_core_ed25519_BYTES long bytes sequence',\n           raising=exc.TypeError)\n\n    rc = lib.crypto_core_ed25519_is_valid_point(p)\n    return rc == 1", "response": "Checks if a point is on the edwards25519 curve and that it doesn t have a small order."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef crypto_core_ed25519_add(p, q):\n    ensure(isinstance(p, bytes) and isinstance(q, bytes) and\n           len(p) == crypto_core_ed25519_BYTES and\n           len(q) == crypto_core_ed25519_BYTES,\n           'Each point must be a {} long bytes sequence'.format(\n           'crypto_core_ed25519_BYTES'),\n           raising=exc.TypeError)\n\n    r = ffi.new(\"unsigned char[]\", crypto_core_ed25519_BYTES)\n\n    rc = lib.crypto_core_ed25519_add(r, p, q)\n    ensure(rc == 0,\n           'Unexpected library error',\n           raising=exc.RuntimeError)\n\n    return ffi.buffer(r, crypto_core_ed25519_BYTES)[:]", "response": "Adds two points on the edwards25519 curve."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef crypto_box_keypair():\n    pk = ffi.new(\"unsigned char[]\", crypto_box_PUBLICKEYBYTES)\n    sk = ffi.new(\"unsigned char[]\", crypto_box_SECRETKEYBYTES)\n\n    rc = lib.crypto_box_keypair(pk, sk)\n    ensure(rc == 0,\n           'Unexpected library error',\n           raising=exc.RuntimeError)\n\n    return (\n        ffi.buffer(pk, crypto_box_PUBLICKEYBYTES)[:],\n        ffi.buffer(sk, crypto_box_SECRETKEYBYTES)[:],\n    )", "response": "Returns a randomly generated public and secret key."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a public and secret keypair deterministically generated from an input seed.", "response": "def crypto_box_seed_keypair(seed):\n    \"\"\"\n    Returns a (public, secret) keypair deterministically generated\n    from an input ``seed``.\n\n    .. warning:: The seed **must** be high-entropy; therefore,\n        its generator **must** be a cryptographic quality\n        random function like, for example, :func:`~nacl.utils.random`.\n\n    .. warning:: The seed **must** be protected and remain secret.\n        Anyone who knows the seed is really in possession of\n        the corresponding PrivateKey.\n\n\n    :param seed: bytes\n    :rtype: (bytes(public_key), bytes(secret_key))\n    \"\"\"\n    ensure(isinstance(seed, bytes),\n           \"seed must be bytes\",\n           raising=TypeError)\n\n    if len(seed) != crypto_box_SEEDBYTES:\n        raise exc.ValueError(\"Invalid seed\")\n\n    pk = ffi.new(\"unsigned char[]\", crypto_box_PUBLICKEYBYTES)\n    sk = ffi.new(\"unsigned char[]\", crypto_box_SECRETKEYBYTES)\n\n    rc = lib.crypto_box_seed_keypair(pk, sk, seed)\n    ensure(rc == 0,\n           'Unexpected library error',\n           raising=exc.RuntimeError)\n\n    return (\n        ffi.buffer(pk, crypto_box_PUBLICKEYBYTES)[:],\n        ffi.buffer(sk, crypto_box_SECRETKEYBYTES)[:],\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef crypto_box(message, nonce, pk, sk):\n    if len(nonce) != crypto_box_NONCEBYTES:\n        raise exc.ValueError(\"Invalid nonce size\")\n\n    if len(pk) != crypto_box_PUBLICKEYBYTES:\n        raise exc.ValueError(\"Invalid public key\")\n\n    if len(sk) != crypto_box_SECRETKEYBYTES:\n        raise exc.ValueError(\"Invalid secret key\")\n\n    padded = (b\"\\x00\" * crypto_box_ZEROBYTES) + message\n    ciphertext = ffi.new(\"unsigned char[]\", len(padded))\n\n    rc = lib.crypto_box(ciphertext, padded, len(padded), nonce, pk, sk)\n    ensure(rc == 0,\n           'Unexpected library error',\n           raising=exc.RuntimeError)\n\n    return ffi.buffer(ciphertext, len(padded))[crypto_box_BOXZEROBYTES:]", "response": "Encrypts and returns a message using the secret key sk pk and the nonce nonce."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef crypto_box_beforenm(pk, sk):\n    if len(pk) != crypto_box_PUBLICKEYBYTES:\n        raise exc.ValueError(\"Invalid public key\")\n\n    if len(sk) != crypto_box_SECRETKEYBYTES:\n        raise exc.ValueError(\"Invalid secret key\")\n\n    k = ffi.new(\"unsigned char[]\", crypto_box_BEFORENMBYTES)\n\n    rc = lib.crypto_box_beforenm(k, pk, sk)\n    ensure(rc == 0,\n           'Unexpected library error',\n           raising=exc.RuntimeError)\n\n    return ffi.buffer(k, crypto_box_BEFORENMBYTES)[:]", "response": "Computes and returns the shared key for the public key pk and the secret key sk."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef crypto_box_afternm(message, nonce, k):\n    if len(nonce) != crypto_box_NONCEBYTES:\n        raise exc.ValueError(\"Invalid nonce\")\n\n    if len(k) != crypto_box_BEFORENMBYTES:\n        raise exc.ValueError(\"Invalid shared key\")\n\n    padded = b\"\\x00\" * crypto_box_ZEROBYTES + message\n    ciphertext = ffi.new(\"unsigned char[]\", len(padded))\n\n    rc = lib.crypto_box_afternm(ciphertext, padded, len(padded), nonce, k)\n    ensure(rc == 0,\n           'Unexpected library error',\n           raising=exc.RuntimeError)\n\n    return ffi.buffer(ciphertext, len(padded))[crypto_box_BOXZEROBYTES:]", "response": "Encrypts and returns the message using the shared key k and the nonce nonce."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef crypto_box_seal(message, pk):\n    ensure(isinstance(message, bytes),\n           \"input message must be bytes\",\n           raising=TypeError)\n\n    ensure(isinstance(pk, bytes),\n           \"public key must be bytes\",\n           raising=TypeError)\n\n    if len(pk) != crypto_box_PUBLICKEYBYTES:\n        raise exc.ValueError(\"Invalid public key\")\n\n    _mlen = len(message)\n    _clen = crypto_box_SEALBYTES + _mlen\n\n    ciphertext = ffi.new(\"unsigned char[]\", _clen)\n\n    rc = lib.crypto_box_seal(ciphertext, message, _mlen, pk)\n    ensure(rc == 0,\n           'Unexpected library error',\n           raising=exc.RuntimeError)\n\n    return ffi.buffer(ciphertext, _clen)[:]", "response": "Encrypts and returns a message using an ephemeral secret key\n    and the public key pk."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef crypto_box_seal_open(ciphertext, pk, sk):\n    ensure(isinstance(ciphertext, bytes),\n           \"input ciphertext must be bytes\",\n           raising=TypeError)\n\n    ensure(isinstance(pk, bytes),\n           \"public key must be bytes\",\n           raising=TypeError)\n\n    ensure(isinstance(sk, bytes),\n           \"secret key must be bytes\",\n           raising=TypeError)\n\n    if len(pk) != crypto_box_PUBLICKEYBYTES:\n        raise exc.ValueError(\"Invalid public key\")\n\n    if len(sk) != crypto_box_SECRETKEYBYTES:\n        raise exc.ValueError(\"Invalid secret key\")\n\n    _clen = len(ciphertext)\n\n    ensure(_clen >= crypto_box_SEALBYTES,\n           (\"Input cyphertext must be \"\n            \"at least {} long\").format(crypto_box_SEALBYTES),\n           raising=exc.TypeError)\n\n    _mlen = _clen - crypto_box_SEALBYTES\n\n    # zero-length malloc results are implementation.dependent\n    plaintext = ffi.new(\"unsigned char[]\", max(1, _mlen))\n\n    res = lib.crypto_box_seal_open(plaintext, ciphertext, _clen, pk, sk)\n    ensure(res == 0, \"An error occurred trying to decrypt the message\",\n           raising=exc.CryptoError)\n\n    return ffi.buffer(plaintext, _mlen)[:]", "response": "Decrypts and returns an encrypted message using the ephemeral public key pk and the sender s ephemeral public key sk and the ephemeral nonce."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncompares contents of two memory regions in constant time", "response": "def sodium_memcmp(inp1, inp2):\n    \"\"\"\n    Compare contents of two memory regions in constant time\n    \"\"\"\n    ensure(isinstance(inp1, bytes),\n           raising=exc.TypeError)\n    ensure(isinstance(inp2, bytes),\n           raising=exc.TypeError)\n\n    ln = max(len(inp1), len(inp2))\n\n    buf1 = ffi.new(\"char []\", ln)\n    buf2 = ffi.new(\"char []\", ln)\n\n    ffi.memmove(buf1, inp1, len(inp1))\n    ffi.memmove(buf2, inp2, len(inp2))\n\n    eqL = len(inp1) == len(inp2)\n    eqC = lib.sodium_memcmp(buf1, buf2, ln) == 0\n\n    return eqL and eqC"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sodium_pad(s, blocksize):\n    ensure(isinstance(s, bytes),\n           raising=exc.TypeError)\n    ensure(isinstance(blocksize, integer_types),\n           raising=exc.TypeError)\n    if blocksize <= 0:\n        raise exc.ValueError\n    s_len = len(s)\n    m_len = s_len + blocksize\n    buf = ffi.new(\"unsigned char []\", m_len)\n    p_len = ffi.new(\"size_t []\", 1)\n    ffi.memmove(buf, s, s_len)\n    rc = lib.sodium_pad(p_len, buf, s_len, blocksize, m_len)\n    ensure(rc == 0, \"Padding failure\", raising=exc.CryptoError)\n    return ffi.buffer(buf, p_len[0])[:]", "response": "Pad the input bytearray s to a multiple of blocksize."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sodium_unpad(s, blocksize):\n    ensure(isinstance(s, bytes),\n           raising=exc.TypeError)\n    ensure(isinstance(blocksize, integer_types),\n           raising=exc.TypeError)\n    s_len = len(s)\n    u_len = ffi.new(\"size_t []\", 1)\n    rc = lib.sodium_unpad(u_len, s, s_len, blocksize)\n    if rc != 0:\n        raise exc.CryptoError(\"Unpadding failure\")\n    return s[:u_len[0]]", "response": "Remove ISO / IEC 7816 - 4 padding from the input byte array s."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sodium_increment(inp):\n    ensure(isinstance(inp, bytes),\n           raising=exc.TypeError)\n\n    ln = len(inp)\n    buf = ffi.new(\"unsigned char []\", ln)\n\n    ffi.memmove(buf, inp, ln)\n\n    lib.sodium_increment(buf, ln)\n\n    return ffi.buffer(buf, ln)[:]", "response": "Increment the value of a byte - sequence interpreted\n    as a little - endian representation of a unsigned big integer."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the modular addition of two unsigned integers.", "response": "def sodium_add(a, b):\n    \"\"\"\n    Given a couple of *same-sized* byte sequences, interpreted as the\n    little-endian representation of two unsigned integers, compute\n    the modular addition of the represented values, in constant time for\n    a given common length of the byte sequences.\n\n    :param a: input bytes buffer\n    :type a: bytes\n    :param b: input bytes buffer\n    :type b: bytes\n    :return: a byte-sequence representing, as a little-endian big integer,\n             the integer value of ``(to_int(a) + to_int(b)) mod 2^(8*len(a))``\n    :rtype: bytes\n    \"\"\"\n    ensure(isinstance(a, bytes),\n           raising=exc.TypeError)\n    ensure(isinstance(b, bytes),\n           raising=exc.TypeError)\n    ln = len(a)\n    ensure(len(b) == ln,\n           raising=exc.TypeError)\n\n    buf_a = ffi.new(\"unsigned char []\", ln)\n    buf_b = ffi.new(\"unsigned char []\", ln)\n\n    ffi.memmove(buf_a, a, ln)\n    ffi.memmove(buf_b, b, ln)\n\n    lib.sodium_add(buf_a, buf_b, ln)\n\n    return ffi.buffer(buf_a, ln)[:]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the line number with which to report violations.", "response": "def error_lineno(self):\n        \"\"\"Get the line number with which to report violations.\"\"\"\n        if isinstance(self.docstring, Docstring):\n            return self.docstring.start\n        return self.start"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef source(self):\n        full_src = self._source[self._slice]\n\n        def is_empty_or_comment(line):\n            return line.strip() == '' or line.strip().startswith('#')\n\n        filtered_src = dropwhile(is_empty_or_comment, reversed(full_src))\n        return ''.join(reversed(list(filtered_src)))", "response": "Return the source code for the definition."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns True iff this function should be considered public.", "response": "def is_public(self):\n        \"\"\"Return True iff this function should be considered public.\"\"\"\n        if self.dunder_all is not None:\n            return self.name in self.dunder_all\n        else:\n            return not self.name.startswith('_')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_magic(self):\n        return (self.name.startswith('__') and\n                self.name.endswith('__') and\n                self.name not in VARIADIC_MAGIC_METHODS)", "response": "Return True iff this method is a magic method ( e. g. __str__."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing the given file - like object and return its Module object.", "response": "def parse(self, filelike, filename):\n        \"\"\"Parse the given file-like object and return its Module object.\"\"\"\n        self.log = log\n        self.source = filelike.readlines()\n        src = ''.join(self.source)\n        try:\n            compile(src, filename, 'exec')\n        except SyntaxError as error:\n            raise ParseError() from error\n        self.stream = TokenStream(StringIO(src))\n        self.filename = filename\n        self.dunder_all = None\n        self.dunder_all_error = None\n        self.future_imports = set()\n        self._accumulated_decorators = []\n        return self.parse_module()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef consume(self, kind):\n        next_token = self.stream.move()\n        if next_token.kind != kind:\n            raise UnexpectedTokenError(token=next_token, expected_kind=kind)", "response": "Consume one token and verify it is of the expected kind."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse decorators into self. _accumulated_decorators.", "response": "def parse_decorators(self):\n        \"\"\"Called after first @ is found.\n\n        Parse decorators into self._accumulated_decorators.\n        Continue to do so until encountering the 'def' or 'class' start token.\n        \"\"\"\n        name = []\n        arguments = []\n        at_arguments = False\n\n        while self.current is not None:\n            self.log.debug(\"parsing decorators, current token is %r (%s)\",\n                           self.current.kind, self.current.value)\n            if (self.current.kind == tk.NAME and\n                    self.current.value in ['def', 'class']):\n                # Done with decorators - found function or class proper\n                break\n            elif self.current.kind == tk.OP and self.current.value == '@':\n                # New decorator found. Store the decorator accumulated so far:\n                self._accumulated_decorators.append(\n                    Decorator(''.join(name), ''.join(arguments)))\n                # Now reset to begin accumulating the new decorator:\n                name = []\n                arguments = []\n                at_arguments = False\n            elif self.current.kind == tk.OP and self.current.value == '(':\n                at_arguments = True\n            elif self.current.kind == tk.OP and self.current.value == ')':\n                # Ignore close parenthesis\n                pass\n            elif self.current.kind == tk.NEWLINE or self.current.kind == tk.NL:\n                # Ignore newlines\n                pass\n            else:\n                # Keep accumulating current decorator's name or argument.\n                if not at_arguments:\n                    name.append(self.current.value)\n                else:\n                    arguments.append(self.current.value)\n            self.stream.move()\n\n        # Add decorator accumulated so far\n        self._accumulated_decorators.append(\n            Decorator(''.join(name), ''.join(arguments)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the __all__ definition in a module.", "response": "def parse_dunder_all(self):\n        \"\"\"Parse the __all__ definition in a module.\"\"\"\n        assert self.current.value == '__all__'\n        self.consume(tk.NAME)\n        # More than one __all__ definition means we ignore all __all__.\n        if self.dunder_all is not None or self.dunder_all_error is not None:\n            self.dunder_all = None\n            self.dunder_all_error = 'Could not evaluate contents of __all__. '\n            return\n        if self.current.value != '=':\n            self.dunder_all_error = 'Could not evaluate contents of __all__. '\n            return\n        self.consume(tk.OP)\n\n        is_surrounded = False\n        if self.current.value in '([':\n            is_surrounded = True\n            self.consume(tk.OP)\n\n        dunder_all_content = \"(\"\n        while True:\n            if is_surrounded and self.current.value in \")]\":\n                break\n            if self.current.kind in (tk.NEWLINE, tk.ENDMARKER):\n                break\n            if self.current.kind in (tk.NL, tk.COMMENT):\n                pass\n            elif (self.current.kind == tk.STRING or self.current.value == ','):\n                dunder_all_content += self.current.value\n            else:\n                self.dunder_all_error = 'Could not evaluate contents of __all__.'\n                return\n            self.stream.move()\n        if is_surrounded:\n            self.consume(tk.OP)\n        if not is_surrounded and ',' not in dunder_all_content:\n            self.dunder_all_error = (\n                'Unexpected token kind in __all__: {!r}. '\n                    .format(self.current.kind))\n            return\n        dunder_all_content += \")\"\n\n        try:\n            self.dunder_all = eval(dunder_all_content, {})\n        except BaseException as e:\n            self.dunder_all_error = (\n                'Could not evaluate contents of __all__.'\n                '\\bThe value was {}. The exception was:\\n{}'\n                    .format(dunder_all_content, e))\n\n        while (self.current.kind not in self.stream.LOGICAL_NEWLINES and\n               self.current.kind != tk.ENDMARKER):\n            if self.current.kind != tk.COMMENT:\n                self.dunder_all = None\n                self.dunder_all_error = 'Could not evaluate contents of __all__. '\n                return"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a definition and return its value in a class_ object.", "response": "def parse_definition(self, class_):\n        \"\"\"Parse a definition and return its value in a `class_` object.\"\"\"\n        start = self.line\n        self.consume(tk.NAME)\n        name = self.current.value\n        self.log.debug(\"parsing %s '%s'\", class_.__name__, name)\n        self.stream.move()\n        if self.current.kind == tk.OP and self.current.value == '(':\n            parenthesis_level = 0\n            while True:\n                if self.current.kind == tk.OP:\n                    if self.current.value == '(':\n                        parenthesis_level += 1\n                    elif self.current.value == ')':\n                        parenthesis_level -= 1\n                        if parenthesis_level == 0:\n                            break\n                self.stream.move()\n        if self.current.kind != tk.OP or self.current.value != ':':\n            self.leapfrog(tk.OP, value=\":\")\n        else:\n            self.consume(tk.OP)\n        if self.current.kind in (tk.NEWLINE, tk.COMMENT):\n            skipped_error_codes = self.parse_skip_comment()\n            self.leapfrog(tk.INDENT)\n            assert self.current.kind != tk.INDENT\n            docstring = self.parse_docstring()\n            decorators = self._accumulated_decorators\n            self.log.debug(\"current accumulated decorators: %s\", decorators)\n            self._accumulated_decorators = []\n            self.log.debug(\"parsing nested definitions.\")\n            children = list(self.parse_definitions(class_))\n            self.log.debug(\"finished parsing nested definitions for '%s'\",\n                           name)\n            end = self.line - 1\n        else:  # one-liner definition\n            skipped_error_codes = ''\n            docstring = self.parse_docstring()\n            decorators = []  # TODO\n            children = []\n            end = self.line\n            self.leapfrog(tk.NEWLINE)\n        definition = class_(name, self.source, start, end,\n                            decorators, docstring, children, None,\n                            skipped_error_codes)\n        for child in definition.children:\n            child.parent = definition\n        self.log.debug(\"finished parsing %s '%s'. Next token is %r\",\n                       class_.__name__, name, self.current)\n        return definition"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a definition comment for noqa skips.", "response": "def parse_skip_comment(self):\n        \"\"\"Parse a definition comment for noqa skips.\"\"\"\n        skipped_error_codes = ''\n        if self.current.kind == tk.COMMENT:\n            if 'noqa: ' in self.current.value:\n                skipped_error_codes = ''.join(\n                     self.current.value.split('noqa: ')[1:])\n            elif self.current.value.startswith('# noqa'):\n                skipped_error_codes = 'all'\n        return skipped_error_codes"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _parse_from_import_source(self):\n        assert self.current.value == 'from', self.current.value\n        self.stream.move()\n        is_future_import = self.current.value == '__future__'\n        self.stream.move()\n        while (self.current is not None and\n               self.current.kind in (tk.DOT, tk.NAME, tk.OP) and\n               self.current.value != 'import'):\n            self.stream.move()\n        if self.current is None or self.current.value != 'import':\n            return False\n        self.check_current(value='import')\n        assert self.current.value == 'import', self.current.value\n        self.stream.move()\n        return is_future_import", "response": "Parse the from statement. Return True iff x is future."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating docstring errors that exist in filenames.", "response": "def check(filenames, select=None, ignore=None, ignore_decorators=None):\n    \"\"\"Generate docstring errors that exist in `filenames` iterable.\n\n    By default, the PEP-257 convention is checked. To specifically define the\n    set of error codes to check for, supply either `select` or `ignore` (but\n    not both). In either case, the parameter should be a collection of error\n    code strings, e.g., {'D100', 'D404'}.\n\n    When supplying `select`, only specified error codes will be reported.\n    When supplying `ignore`, all error codes which were not specified will be\n    reported.\n\n    Note that ignored error code refer to the entire set of possible\n    error codes, which is larger than just the PEP-257 convention. To your\n    convenience, you may use `pydocstyle.violations.conventions.pep257` as\n    a base set to add or remove errors from.\n\n    Examples\n    ---------\n    >>> check(['pydocstyle.py'])\n    <generator object check at 0x...>\n\n    >>> check(['pydocstyle.py'], select=['D100'])\n    <generator object check at 0x...>\n\n    >>> check(['pydocstyle.py'], ignore=conventions.pep257 - {'D100'})\n    <generator object check at 0x...>\n\n    \"\"\"\n    if select is not None and ignore is not None:\n        raise IllegalConfiguration('Cannot pass both select and ignore. '\n                                   'They are mutually exclusive.')\n    elif select is not None:\n        checked_codes = select\n    elif ignore is not None:\n        checked_codes = list(set(violations.ErrorRegistry.get_error_codes()) -\n                             set(ignore))\n    else:\n        checked_codes = violations.conventions.pep257\n\n    for filename in filenames:\n        log.info('Checking file %s.', filename)\n        try:\n            with tk.open(filename) as file:\n                source = file.read()\n            for error in ConventionChecker().check_source(source, filename,\n                                                          ignore_decorators):\n                code = getattr(error, 'code', None)\n                if code in checked_codes:\n                    yield error\n        except (EnvironmentError, AllError, ParseError) as error:\n            log.warning('Error in file %s: %s', filename, error)\n            yield error\n        except tk.TokenError:\n            yield SyntaxError('invalid syntax in file %s' % filename)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks that the docstring is missing from the definition.", "response": "def check_docstring_missing(self, definition, docstring):\n        \"\"\"D10{0,1,2,3}: Public definitions should have docstrings.\n\n        All modules should normally have docstrings.  [...] all functions and\n        classes exported by a module should also have docstrings. Public\n        methods (including the __init__ constructor) should also have\n        docstrings.\n\n        Note: Public (exported) definitions are either those with names listed\n              in __all__ variable (if present), or those that do not start\n              with a single underscore.\n\n        \"\"\"\n        if (not docstring and definition.is_public or\n                docstring and is_blank(ast.literal_eval(docstring))):\n            codes = {Module: violations.D100,\n                     Class: violations.D101,\n                     NestedClass: violations.D106,\n                     Method: (lambda: violations.D105() if definition.is_magic\n                              else (violations.D107() if definition.is_init\n                              else violations.D102())),\n                     Function: violations.D103,\n                     NestedFunction: violations.D103,\n                     Package: violations.D104}\n            return codes[type(definition)]()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_no_blank_before(self, function, docstring):  # def\n        if docstring:\n            before, _, after = function.source.partition(docstring)\n            blanks_before = list(map(is_blank, before.split('\\n')[:-1]))\n            blanks_after = list(map(is_blank, after.split('\\n')[1:]))\n            blanks_before_count = sum(takewhile(bool, reversed(blanks_before)))\n            blanks_after_count = sum(takewhile(bool, blanks_after))\n            if blanks_before_count != 0:\n                yield violations.D201(blanks_before_count)\n            if not all(blanks_after) and blanks_after_count != 0:\n                yield violations.D202(blanks_after_count)", "response": "Check that no blank lines are allowed around function or method docstring."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_blank_before_after_class(self, class_, docstring):\n        # NOTE: this gives false-positive in this case\n        # class Foo:\n        #\n        #     \"\"\"Docstring.\"\"\"\n        #\n        #\n        # # comment here\n        # def foo(): pass\n        if docstring:\n            before, _, after = class_.source.partition(docstring)\n            blanks_before = list(map(is_blank, before.split('\\n')[:-1]))\n            blanks_after = list(map(is_blank, after.split('\\n')[1:]))\n            blanks_before_count = sum(takewhile(bool, reversed(blanks_before)))\n            blanks_after_count = sum(takewhile(bool, blanks_after))\n            if blanks_before_count != 0:\n                yield violations.D211(blanks_before_count)\n            if blanks_before_count != 1:\n                yield violations.D203(blanks_before_count)\n            if not all(blanks_after) and blanks_after_count != 1:\n                yield violations.D204(blanks_after_count)", "response": "Class docstring should have 1 blank line around all methods in the class."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_blank_after_summary(self, definition, docstring):\n        if docstring:\n            lines = ast.literal_eval(docstring).strip().split('\\n')\n            if len(lines) > 1:\n                post_summary_blanks = list(map(is_blank, lines[1:]))\n                blanks_count = sum(takewhile(bool, post_summary_blanks))\n                if blanks_count != 1:\n                    return violations.D205(blanks_count)", "response": "D205 - Put one blank line between summary line and description."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_docstring_indent(definition, docstring):\n        before_docstring, _, _ = definition.source.partition(docstring)\n        _, _, indent = before_docstring.rpartition('\\n')\n        return indent", "response": "Return the indentation of the docstring s opening quotes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if the docstring is indented same as the code.", "response": "def check_indent(self, definition, docstring):\n        \"\"\"D20{6,7,8}: The entire docstring should be indented same as code.\n\n        The entire docstring is indented the same as the quotes at its\n        first line.\n\n        \"\"\"\n        if docstring:\n            indent = self._get_docstring_indent(definition, docstring)\n            lines = docstring.split('\\n')\n            if len(lines) > 1:\n                lines = lines[1:]  # First line does not need indent.\n                indents = [leading_space(l) for l in lines if not is_blank(l)]\n                if set(' \\t') == set(''.join(indents) + indent):\n                    yield violations.D206()\n                if (len(indents) > 1 and min(indents[:-1]) > indent or\n                        indents[-1] > indent):\n                    yield violations.D208()\n                if min(indents) < indent:\n                    yield violations.D207()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_surrounding_whitespaces(self, definition, docstring):\n        if docstring:\n            lines = ast.literal_eval(docstring).split('\\n')\n            if lines[0].startswith(' ') or \\\n                    len(lines) == 1 and lines[0].endswith(' '):\n                return violations.D210()", "response": "D210 : No whitespaces allowed surrounding docstring text."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_multi_line_summary_start(self, definition, docstring):\n        if docstring:\n            start_triple = [\n                '\"\"\"', \"'''\",\n                'u\"\"\"', \"u'''\",\n                'r\"\"\"', \"r'''\",\n                'ur\"\"\"', \"ur'''\"\n            ]\n\n            lines = ast.literal_eval(docstring).split('\\n')\n            if len(lines) > 1:\n                first = docstring.split(\"\\n\")[0].strip().lower()\n                if first in start_triple:\n                    return violations.D212()\n                else:\n                    return violations.D213()", "response": "Check if a multi - line docstring summary starts at the first line of a docstring."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if the docstring contains triple double quotes.", "response": "def check_triple_double_quotes(self, definition, docstring):\n        r'''D300: Use \"\"\"triple double quotes\"\"\".\n\n        For consistency, always use \"\"\"triple double quotes\"\"\" around\n        docstrings. Use r\"\"\"raw triple double quotes\"\"\" if you use any\n        backslashes in your docstrings. For Unicode docstrings, use\n        u\"\"\"Unicode triple-quoted strings\"\"\".\n\n        Note: Exception to this is made if the docstring contains\n              \"\"\" quotes in its body.\n\n        '''\n        if docstring:\n            if '\"\"\"' in ast.literal_eval(docstring):\n                # Allow ''' quotes if docstring contains \"\"\", because\n                # otherwise \"\"\" quotes could not be expressed inside\n                # docstring. Not in PEP 257.\n                regex = re(r\"[uU]?[rR]?'''[^'].*\")\n            else:\n                regex = re(r'[uU]?[rR]?\"\"\"[^\"].*')\n\n            if not regex.match(docstring):\n                illegal_matcher = re(r\"\"\"[uU]?[rR]?(\"+|'+).*\"\"\")\n                illegal_quotes = illegal_matcher.match(docstring).group(1)\n                return violations.D300(illegal_quotes)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks that the docstring contains backslashes.", "response": "def check_backslashes(self, definition, docstring):\n        r'''D301: Use r\"\"\" if any backslashes in a docstring.\n\n        Use r\"\"\"raw triple double quotes\"\"\" if you use any backslashes\n        (\\) in your docstrings.\n\n        '''\n        # Just check that docstring is raw, check_triple_double_quotes\n        # ensures the correct quotes.\n        if docstring and '\\\\' in docstring and not docstring.startswith(\n                ('r', 'ur')):\n            return violations.D301()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck that the docstring is unicode.", "response": "def check_unicode_docstring(self, definition, docstring):\n        r'''D302: Use u\"\"\" for docstrings with Unicode.\n\n        For Unicode docstrings, use u\"\"\"Unicode triple-quoted strings\"\"\".\n\n        '''\n        if 'unicode_literals' in definition.module.future_imports:\n            return\n\n        # Just check that docstring is unicode, check_triple_double_quotes\n        # ensures the correct quotes.\n        if docstring and sys.version_info[0] <= 2:\n            if not is_ascii(docstring) and not docstring.startswith(\n                    ('u', 'ur')):\n                return violations.D302()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_ends_with_period(self, definition, docstring):\n        if docstring:\n            summary_line = ast.literal_eval(docstring).strip().split('\\n')[0]\n            if not summary_line.endswith('.'):\n                return violations.D400(summary_line[-1])", "response": "Check that the first line of a docstring ends with a period."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_imperative_mood(self, function, docstring):  # def context\n        if docstring and not function.is_test:\n            stripped = ast.literal_eval(docstring).strip()\n            if stripped:\n                first_word = stripped.split()[0]\n                check_word = first_word.lower()\n\n                if check_word in IMPERATIVE_BLACKLIST:\n                    return violations.D401b(first_word)\n\n                try:\n                    correct_form = IMPERATIVE_VERBS.get(stem(check_word))\n                except UnicodeDecodeError:\n                    # This is raised when the docstring contains unicode\n                    # characters in the first word, but is not a unicode\n                    # string. In which case D302 will be reported. Ignoring.\n                    return\n\n                if correct_form and correct_form != check_word:\n                    return violations.D401(\n                        correct_form.capitalize(),\n                        first_word\n                    )", "response": "Check if the first line of the docstring is in imperative mood."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_no_signature(self, function, docstring):  # def context\n        if docstring:\n            first_line = ast.literal_eval(docstring).strip().split('\\n')[0]\n            if function.name + '(' in first_line.replace(' ', ''):\n                return violations.D402()", "response": "Check that the function or method signature is not a signature."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck that the first word of the docstring should not be This.", "response": "def check_starts_with_this(self, function, docstring):\n        \"\"\"D404: First word of the docstring should not be `This`.\n\n        Docstrings should use short, simple language. They should not begin\n        with \"This class is [..]\" or \"This module contains [..]\".\n\n        \"\"\"\n        if docstring:\n            first_word = ast.literal_eval(docstring).split()[0]\n            if first_word.lower() == 'this':\n                return violations.D404()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns any leading set of words from line.", "response": "def _get_leading_words(line):\n        \"\"\"Return any leading set of words from `line`.\n\n        For example, if `line` is \"  Hello world!!!\", returns \"Hello world\".\n        \"\"\"\n        result = re(\"[\\w ]+\").match(line.strip())\n        if result is not None:\n            return result.group()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _is_a_docstring_section(context):\n        section_name_suffix = \\\n            context.line.strip().lstrip(context.section_name.strip()).strip()\n\n        section_suffix_is_only_colon = section_name_suffix == ':'\n\n        punctuation = [',', ';', '.', '-', '\\\\', '/', ']', '}', ')']\n        prev_line_ends_with_punctuation = \\\n            any(context.previous_line.strip().endswith(x) for x in punctuation)\n\n        this_line_looks_like_a_section_name = \\\n            is_blank(section_name_suffix) or section_suffix_is_only_colon\n        \n        prev_line_looks_like_end_of_paragraph = \\\n            prev_line_ends_with_punctuation or is_blank(context.previous_line)\n\n        return (this_line_looks_like_a_section_name and\n                prev_line_looks_like_end_of_paragraph)", "response": "Check if the suspected context is really a section header."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck that the section underline is correct.", "response": "def _check_section_underline(cls, section_name, context, indentation):\n        \"\"\"D4{07,08,09,12}, D215: Section underline checks.\n\n        Check for correct formatting for docstring sections. Checks that:\n            * The line that follows the section name contains\n              dashes (D40{7,8}).\n            * The amount of dashes is equal to the length of the section\n              name (D409).\n            * The section's content does not begin in the line that follows\n              the section header (D412).\n            * The indentation of the dashed line is equal to the docstring's\n              indentation (D215).\n        \"\"\"\n        blank_lines_after_header = 0\n\n        for line in context.following_lines:\n            if not is_blank(line):\n                break\n            blank_lines_after_header += 1\n        else:\n            # There are only blank lines after the header.\n            yield violations.D407(section_name)\n            return\n\n        non_empty_line = context.following_lines[blank_lines_after_header]\n        dash_line_found = ''.join(set(non_empty_line.strip())) == '-'\n\n        if not dash_line_found:\n            yield violations.D407(section_name)\n            if blank_lines_after_header > 0:\n                yield violations.D412(section_name)\n        else:\n            if blank_lines_after_header > 0:\n                yield violations.D408(section_name)\n\n            if non_empty_line.strip() != \"-\" * len(section_name):\n                yield violations.D409(len(section_name),\n                                      section_name,\n                                      len(non_empty_line.strip()))\n\n            if leading_space(non_empty_line) > indentation:\n                yield violations.D215(section_name)\n\n            line_after_dashes_index = blank_lines_after_header + 1\n            # If the line index after the dashes is in range (perhaps we have\n            # a header + underline followed by another section header).\n            if line_after_dashes_index < len(context.following_lines):\n                line_after_dashes = \\\n                    context.following_lines[line_after_dashes_index]\n                if is_blank(line_after_dashes):\n                    rest_of_lines = \\\n                        context.following_lines[line_after_dashes_index:]\n                    if not is_blank(''.join(rest_of_lines)):\n                        yield violations.D412(section_name)\n                    else:\n                        yield violations.D414(section_name)\n            else:\n                yield violations.D414(section_name)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck for valid section names.", "response": "def _check_section(cls, docstring, definition, context):\n        \"\"\"D4{05,06,10,11,13}, D214: Section name checks.\n\n        Check for valid section names. Checks that:\n            * The section name is properly capitalized (D405).\n            * The section is not over-indented (D214).\n            * The section name has no superfluous suffix to it (D406).\n            * There's a blank line after the section (D410, D413).\n            * There's a blank line before the section (D411).\n\n        Also yields all the errors from `_check_section_underline`.\n        \"\"\"\n        capitalized_section = context.section_name.title()\n        indentation = cls._get_docstring_indent(definition, docstring)\n\n        if (context.section_name not in cls.SECTION_NAMES and\n                capitalized_section in cls.SECTION_NAMES):\n            yield violations.D405(capitalized_section, context.section_name)\n\n        if leading_space(context.line) > indentation:\n            yield violations.D214(capitalized_section)\n\n        suffix = context.line.strip().lstrip(context.section_name)\n        if suffix:\n            yield violations.D406(capitalized_section, context.line.strip())\n\n        if (not context.following_lines or\n                not is_blank(context.following_lines[-1])):\n            if context.is_last_section:\n                yield violations.D413(capitalized_section)\n            else:\n                yield violations.D410(capitalized_section)\n\n        if not is_blank(context.previous_line):\n            yield violations.D411(capitalized_section)\n\n        for err in cls._check_section_underline(capitalized_section,\n                                                context,\n                                                indentation):\n            yield err"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_docstring_sections(self, definition, docstring):\n        if not docstring:\n            return\n\n        lines = docstring.split(\"\\n\")\n        if len(lines) < 2:\n            return\n\n        lower_section_names = [s.lower() for s in self.SECTION_NAMES]\n\n        def _suspected_as_section(_line):\n            result = self._get_leading_words(_line.lower())\n            return result in lower_section_names\n\n        # Finding our suspects.\n        suspected_section_indices = [i for i, line in enumerate(lines) if\n                                     _suspected_as_section(line)]\n\n        SectionContext = namedtuple('SectionContext', ('section_name',\n                                                       'previous_line',\n                                                       'line',\n                                                       'following_lines',\n                                                       'original_index',\n                                                       'is_last_section'))\n\n        # First - create a list of possible contexts. Note that the\n        # `following_lines` member is until the end of the docstring.\n        contexts = (SectionContext(self._get_leading_words(lines[i].strip()),\n                                   lines[i - 1],\n                                   lines[i],\n                                   lines[i + 1:],\n                                   i,\n                                   False)\n                    for i in suspected_section_indices)\n\n        # Now that we have manageable objects - rule out false positives.\n        contexts = (c for c in contexts if self._is_a_docstring_section(c))\n\n        # Now we shall trim the `following lines` field to only reach the\n        # next section name.\n        for a, b in pairwise(contexts, None):\n            end = -1 if b is None else b.original_index\n            new_ctx = SectionContext(a.section_name,\n                                     a.previous_line,\n                                     a.line,\n                                     lines[a.original_index + 1:end],\n                                     a.original_index,\n                                     b is None)\n            for err in self._check_section(docstring, definition, new_ctx):\n                yield err", "response": "Check the docstring for section names and return a dictionary of the section names."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the source code context for this error.", "response": "def set_context(self, definition: Definition, explanation: str) -> None:\n        \"\"\"Set the source code context for this error.\"\"\"\n        self.definition = definition\n        self.explanation = explanation"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef message(self) -> str:\n        ret = '{}: {}'.format(self.code, self.short_desc)\n        if self.context is not None:\n            specific_error_msg = self.context.format(*self.parameters)\n            ret += ' ({})'.format(specific_error_msg)\n        return ret", "response": "Return the message to print to the user."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef lines(self) -> str:\n        if self.definition is None:\n            return ''\n        source = ''\n        lines = self.definition.source\n        offset = self.definition.start  # type: ignore\n        lines_stripped = list(reversed(list(dropwhile(is_blank,\n                                                      reversed(lines)))))\n        numbers_width = len(str(offset + len(lines_stripped)))\n        line_format = '{{:{}}}:{{}}'.format(numbers_width)\n        for n, line in enumerate(lines_stripped):\n            if line:\n                line = ' ' + line\n            source += line_format.format(n + offset, line)\n            if n > 5:\n                source += '        ...\\n'\n                break\n        return source", "response": "Return the source code lines for this error."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_group(cls, prefix: str, name: str) -> ErrorGroup:\n        group = cls.ErrorGroup(prefix, name)\n        cls.groups.append(group)\n        return group", "response": "Create a new error group and return it."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nyield all registered codes.", "response": "def get_error_codes(cls) -> Iterable[str]:\n        \"\"\"Yield all registered codes.\"\"\"\n        for group in cls.groups:\n            for error in group.errors:\n                yield error.code"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\noutputs the registry as reStructuredText for documentation.", "response": "def to_rst(cls) -> str:\n        \"\"\"Output the registry as reStructuredText, for documentation.\"\"\"\n        sep_line = '+' + 6 * '-' + '+' + '-' * 71 + '+\\n'\n        blank_line = '|' + 78 * ' ' + '|\\n'\n        table = ''\n        for group in cls.groups:\n            table += sep_line\n            table += blank_line\n            table += '|' + '**{}**'.format(group.name).center(78) + '|\\n'\n            table += blank_line\n            for error in group.errors:\n                table += sep_line\n                table += ('|' + error.code.center(6) + '| ' +\n                          error.short_desc.ljust(70) + '|\\n')\n        table += sep_line\n        return table"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks that the configuration object was initialized.", "response": "def check_initialized(method):\n    \"\"\"Check that the configuration object was initialized.\"\"\"\n    def _decorator(self, *args, **kwargs):\n        if self._arguments is None or self._options is None:\n            raise RuntimeError('using an uninitialized configuration')\n        return method(self, *args, **kwargs)\n    return _decorator"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the configuration. If one of `BASE_ERROR_SELECTION_OPTIONS` was selected, overrides all error codes to check and disregards any error code related configurations from the configuration files.", "response": "def parse(self):\n        \"\"\"Parse the configuration.\n\n        If one of `BASE_ERROR_SELECTION_OPTIONS` was selected, overrides all\n        error codes to check and disregards any error code related\n        configurations from the configuration files.\n\n        \"\"\"\n        self._options, self._arguments = self._parse_args()\n        self._arguments = self._arguments or ['.']\n\n        if not self._validate_options(self._options):\n            raise IllegalConfiguration()\n\n        self._run_conf = self._create_run_config(self._options)\n\n        config = self._create_check_config(self._options, use_defaults=False)\n        self._override_by_cli = config"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_files_to_check(self):\n        def _get_matches(conf):\n            \"\"\"Return the `match` and `match_dir` functions for `config`.\"\"\"\n            match_func = re(conf.match + '$').match\n            match_dir_func = re(conf.match_dir + '$').match\n            return match_func, match_dir_func\n\n        def _get_ignore_decorators(conf):\n            \"\"\"Return the `ignore_decorators` as None or regex.\"\"\"\n            return (re(conf.ignore_decorators) if conf.ignore_decorators\n                    else None)\n\n        for name in self._arguments:\n            if os.path.isdir(name):\n                for root, dirs, filenames in os.walk(name):\n                    config = self._get_config(os.path.abspath(root))\n                    match, match_dir = _get_matches(config)\n                    ignore_decorators = _get_ignore_decorators(config)\n\n                    # Skip any dirs that do not match match_dir\n                    dirs[:] = [d for d in dirs if match_dir(d)]\n\n                    for filename in filenames:\n                        if match(filename):\n                            full_path = os.path.join(root, filename)\n                            yield (full_path, list(config.checked_codes),\n                                   ignore_decorators)\n            else:\n                config = self._get_config(os.path.abspath(name))\n                match, _ = _get_matches(config)\n                ignore_decorators = _get_ignore_decorators(config)\n                if match(name):\n                    yield (name, list(config.checked_codes), ignore_decorators)", "response": "Yields a list of files and error codes to check on each one."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_config_by_discovery(self, node):\n        path = self._get_node_dir(node)\n\n        if path in self._cache:\n            return self._cache[path]\n\n        config_file = self._get_config_file_in_folder(path)\n\n        if config_file is None:\n            parent_dir, tail = os.path.split(path)\n            if tail:\n                # No configuration file, simply take the parent's.\n                config = self._get_config(parent_dir)\n            else:\n                # There's no configuration file and no parent directory.\n                # Use the default configuration or the one given in the CLI.\n                config = self._create_check_config(self._options)\n        else:\n            # There's a config file! Read it and merge if necessary.\n            options, inherit = self._read_configuration_file(config_file)\n\n            parent_dir, tail = os.path.split(path)\n            if tail and inherit:\n                # There is a parent dir and we should try to merge.\n                parent_config = self._get_config(parent_dir)\n                config = self._merge_configuration(parent_config, options)\n            else:\n                # No need to merge or parent dir does not exist.\n                config = self._create_check_config(options)\n\n        return config", "response": "Get a configuration for checking a node by config discovery."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting and cache the run configuration for a node.", "response": "def _get_config(self, node):\n        \"\"\"Get and cache the run configuration for `node`.\n\n        If no configuration exists (not local and not for the parent node),\n        returns and caches a default configuration.\n\n        The algorithm:\n        -------------\n        * If the current directory's configuration exists in\n           `self._cache` - return it.\n        * If a configuration file does not exist in this directory:\n          * If the directory is not a root directory:\n            * Cache its configuration as this directory's and return it.\n          * Else:\n            * Cache a default configuration and return it.\n        * Else:\n          * Read the configuration file.\n          * If a parent directory exists AND the configuration file\n            allows inheritance:\n            * Read the parent configuration by calling this function with the\n              parent directory as `node`.\n            * Merge the parent configuration with the current one and\n              cache it.\n        * If the user has specified one of `BASE_ERROR_SELECTION_OPTIONS` in\n          the CLI - return the CLI configuration with the configuration match\n          clauses\n        * Set the `--add-select` and `--add-ignore` CLI configurations.\n\n        \"\"\"\n        if self._run_conf.config is None:\n            log.debug('No config file specified, discovering.')\n            config = self._get_config_by_discovery(node)\n        else:\n            log.debug('Using config file %r', self._run_conf.config)\n            if not os.path.exists(self._run_conf.config):\n                raise IllegalConfiguration('Configuration file {!r} specified '\n                                           'via --config was not found.'\n                                           .format(self._run_conf.config))\n\n            if None in self._cache:\n                return self._cache[None]\n            options, _ = self._read_configuration_file(self._run_conf.config)\n\n            if options is None:\n                log.warning('Configuration file does not contain a '\n                            'pydocstyle section. Using default configuration.')\n                config = self._create_check_config(self._options)\n            else:\n                config = self._create_check_config(options)\n\n        # Make the CLI always win\n        final_config = {}\n        for attr in CheckConfiguration._fields:\n            cli_val = getattr(self._override_by_cli, attr)\n            conf_val = getattr(config, attr)\n            final_config[attr] = cli_val if cli_val is not None else conf_val\n\n        config = CheckConfiguration(**final_config)\n\n        self._set_add_options(config.checked_codes, self._options)\n\n        # Handle caching\n        if self._run_conf.config is not None:\n            self._cache[None] = config\n        else:\n            self._cache[self._get_node_dir(node)] = config\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_node_dir(node):\n        path = os.path.abspath(node)\n        return path if os.path.isdir(path) else os.path.dirname(path)", "response": "Return the absolute path of the directory of a filesystem node."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntry to read and parse a configuration file.", "response": "def _read_configuration_file(self, path):\n        \"\"\"Try to read and parse `path` as a configuration file.\n\n        If the configurations were illegal (checked with\n        `self._validate_options`), raises `IllegalConfiguration`.\n\n        Returns (options, should_inherit).\n\n        \"\"\"\n        parser = RawConfigParser(inline_comment_prefixes=('#', ';'))\n        options = None\n        should_inherit = True\n\n        if parser.read(path) and self._get_section_name(parser):\n            all_options = self._parser.option_list[:]\n            for group in self._parser.option_groups:\n                all_options.extend(group.option_list)\n\n            option_list = {o.dest: o.type or o.action\n                                for o in all_options}\n\n            # First, read the default values\n            new_options, _ = self._parse_args([])\n\n            # Second, parse the configuration\n            section_name = self._get_section_name(parser)\n            for opt in parser.options(section_name):\n                if opt == 'inherit':\n                    should_inherit = parser.getboolean(section_name, opt)\n                    continue\n\n                if opt.replace('_', '-') not in self.CONFIG_FILE_OPTIONS:\n                    log.warning(\"Unknown option '{}' ignored\".format(opt))\n                    continue\n\n                normalized_opt = opt.replace('-', '_')\n                opt_type = option_list[normalized_opt]\n                if opt_type in ('int', 'count'):\n                    value = parser.getint(section_name, opt)\n                elif opt_type == 'string':\n                    value = parser.get(section_name, opt)\n                else:\n                    assert opt_type in ('store_true', 'store_false')\n                    value = parser.getboolean(section_name, opt)\n                setattr(new_options, normalized_opt, value)\n\n            # Third, fix the set-options\n            options = self._fix_set_options(new_options)\n\n        if options is not None:\n            if not self._validate_options(options):\n                raise IllegalConfiguration('in file: {}'.format(path))\n\n        return options, should_inherit"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmerging parent config into the child options.", "response": "def _merge_configuration(self, parent_config, child_options):\n        \"\"\"Merge parent config into the child options.\n\n        The migration process requires an `options` object for the child in\n        order to distinguish between mutually exclusive codes, add-select and\n        add-ignore error codes.\n\n        \"\"\"\n        # Copy the parent error codes so we won't override them\n        error_codes = copy.deepcopy(parent_config.checked_codes)\n        if self._has_exclusive_option(child_options):\n            error_codes = self._get_exclusive_error_codes(child_options)\n\n        self._set_add_options(error_codes, child_options)\n\n        kwargs = dict(checked_codes=error_codes)\n        for key in ('match', 'match_dir', 'ignore_decorators'):\n            kwargs[key] = \\\n                getattr(child_options, key) or getattr(parent_config, key)\n        return CheckConfiguration(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _parse_args(self, args=None, values=None):\n        options, arguments = self._parser.parse_args(args, values)\n        return self._fix_set_options(options), arguments", "response": "Parse the options using self. _parser and reformat the options."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a RunConfiguration object from options.", "response": "def _create_run_config(options):\n        \"\"\"Create a `RunConfiguration` object from `options`.\"\"\"\n        values = {opt: getattr(options, opt) for opt in\n                       RunConfiguration._fields}\n        return RunConfiguration(**values)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a CheckConfiguration object from options.", "response": "def _create_check_config(cls, options, use_defaults=True):\n        \"\"\"Create a `CheckConfiguration` object from `options`.\n\n        If `use_defaults`, any of the match options that are `None` will\n        be replaced with their default value and the default convention will be\n        set for the checked codes.\n\n        \"\"\"\n        checked_codes = None\n\n        if cls._has_exclusive_option(options) or use_defaults:\n            checked_codes = cls._get_checked_errors(options)\n\n        kwargs = dict(checked_codes=checked_codes)\n        for key in ('match', 'match_dir', 'ignore_decorators'):\n            kwargs[key] = getattr(cls, 'DEFAULT_{}_RE'.format(key.upper())) \\\n                if getattr(options, key) is None and use_defaults \\\n                else getattr(options, key)\n        return CheckConfiguration(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_section_name(cls, parser):\n        for section_name in cls.POSSIBLE_SECTION_NAMES:\n            if parser.has_section(section_name):\n                return section_name\n\n        return None", "response": "Parse options from relevant section."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_config_file_in_folder(cls, path):\n        if os.path.isfile(path):\n            path = os.path.dirname(path)\n\n        for fn in cls.PROJECT_CONFIG_FILES:\n            config = RawConfigParser()\n            full_path = os.path.join(path, fn)\n            if config.read(full_path) and cls._get_section_name(config):\n                return full_path", "response": "Look for a configuration file in path."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_exclusive_error_codes(cls, options):\n        codes = set(ErrorRegistry.get_error_codes())\n        checked_codes = None\n\n        if options.ignore is not None:\n            ignored = cls._expand_error_codes(options.ignore)\n            checked_codes = codes - ignored\n        elif options.select is not None:\n            checked_codes = cls._expand_error_codes(options.select)\n        elif options.convention is not None:\n            checked_codes = getattr(conventions, options.convention)\n\n        # To not override the conventions nor the options - copy them.\n        return copy.deepcopy(checked_codes)", "response": "Extract the error codes from the selected exclusive option."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the checked_codes by the add_ignore or add_select options.", "response": "def _set_add_options(cls, checked_codes, options):\n        \"\"\"Set `checked_codes` by the `add_ignore` or `add_select` options.\"\"\"\n        checked_codes |= cls._expand_error_codes(options.add_select)\n        checked_codes -= cls._expand_error_codes(options.add_ignore)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns an expanded set of error codes to ignore.", "response": "def _expand_error_codes(code_parts):\n        \"\"\"Return an expanded set of error codes to ignore.\"\"\"\n        codes = set(ErrorRegistry.get_error_codes())\n        expanded_codes = set()\n\n        try:\n            for part in code_parts:\n                # Dealing with split-lined configurations; The part might begin\n                # with a whitespace due to the newline character.\n                part = part.strip()\n                if not part:\n                    continue\n\n                codes_to_add = {code for code in codes\n                                if code.startswith(part)}\n                if not codes_to_add:\n                    log.warning(\n                        'Error code passed is not a prefix of any '\n                        'known errors: %s', part)\n                expanded_codes.update(codes_to_add)\n        except TypeError as e:\n            raise IllegalConfiguration(e)\n\n        return expanded_codes"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_checked_errors(cls, options):\n        checked_codes = cls._get_exclusive_error_codes(options)\n        if checked_codes is None:\n            checked_codes = cls.DEFAULT_CONVENTION\n\n        cls._set_add_options(checked_codes, options)\n\n        return checked_codes", "response": "Extract the codes needed to be checked from options."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _validate_options(cls, options):\n        for opt1, opt2 in \\\n                itertools.permutations(cls.BASE_ERROR_SELECTION_OPTIONS, 2):\n            if getattr(options, opt1) and getattr(options, opt2):\n                log.error('Cannot pass both {} and {}. They are '\n                          'mutually exclusive.'.format(opt1, opt2))\n                return False\n\n        if options.convention and options.convention not in conventions:\n            log.error(\"Illegal convention '{}'. Possible conventions: {}\"\n                      .format(options.convention,\n                              ', '.join(conventions.keys())))\n            return False\n        return True", "response": "Validate the mutually exclusive options."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True iff one or more exclusive options were selected.", "response": "def _has_exclusive_option(cls, options):\n        \"\"\"Return `True` iff one or more exclusive options were selected.\"\"\"\n        return any([getattr(options, opt) is not None for opt in\n                    cls.BASE_ERROR_SELECTION_OPTIONS])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nalter the set options from None to sets in place.", "response": "def _fix_set_options(cls, options):\n        \"\"\"Alter the set options from None/strings to sets in place.\"\"\"\n        optional_set_options = ('ignore', 'select')\n        mandatory_set_options = ('add_ignore', 'add_select')\n\n        def _get_set(value_str):\n            \"\"\"Split `value_str` by the delimiter `,` and return a set.\n\n            Removes any occurrences of '' in the set.\n            Also expand error code prefixes, to avoid doing this for every\n            file.\n\n            \"\"\"\n            return cls._expand_error_codes(set(value_str.split(',')) - {''})\n\n        for opt in optional_set_options:\n            value = getattr(options, opt)\n            if value is not None:\n                setattr(options, opt, _get_set(value))\n\n        for opt in mandatory_set_options:\n            value = getattr(options, opt)\n            if value is None:\n                value = ''\n\n            if not isinstance(value, Set):\n                value = _get_set(value)\n\n            setattr(options, opt, value)\n\n        return options"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating an option parser to parse the command line arguments.", "response": "def _create_option_parser(cls):\n        \"\"\"Return an option parser to parse the command line arguments.\"\"\"\n        from optparse import OptionParser, OptionGroup\n\n        parser = OptionParser(\n            version=__version__,\n            usage='Usage: pydocstyle [options] [<file|dir>...]')\n\n        option = parser.add_option\n\n        # Run configuration options\n        option('-e', '--explain', action='store_true', default=False,\n               help='show explanation of each error')\n        option('-s', '--source', action='store_true', default=False,\n               help='show source for each error')\n        option('-d', '--debug', action='store_true', default=False,\n               help='print debug information')\n        option('-v', '--verbose', action='store_true', default=False,\n               help='print status information')\n        option('--count', action='store_true', default=False,\n               help='print total number of errors to stdout')\n        option('--config', metavar='<path>', default=None,\n               help='use given config file and disable config discovery')\n\n        parser.add_option_group(OptionGroup(\n            parser,\n            'Note',\n            'When using --match, --match-dir or --ignore-decorators consider '\n            'whether you should use a single quote (\\') or a double quote (\"), '\n            'depending on your OS, Shell, etc.'))\n\n        check_group = OptionGroup(\n            parser,\n            'Error Check Options',\n            'Only one of --select, --ignore or --convention can be '\n            'specified. If none is specified, defaults to '\n            '`--convention=pep257`. These three options select the \"basic '\n            'list\" of error codes to check. If you wish to change that list '\n            '(for example, if you selected a known convention but wish to '\n            'ignore a specific error from it or add a new one) you can '\n            'use `--add-[ignore/select]` in order to do so.')\n        add_check = check_group.add_option\n\n        # Error check options\n        add_check('--select', metavar='<codes>', default=None,\n                  help='choose the basic list of checked errors by '\n                       'specifying which errors to check for (with a list of '\n                       'comma-separated error codes or prefixes). '\n                       'for example: --select=D101,D2')\n        add_check('--ignore', metavar='<codes>', default=None,\n                  help='choose the basic list of checked errors by '\n                       'specifying which errors to ignore out of all of the '\n                       'available error codes (with a list of '\n                       'comma-separated error codes or prefixes). '\n                       'for example: --ignore=D101,D2')\n        add_check('--convention', metavar='<name>', default=None,\n                  help='choose the basic list of checked errors by specifying '\n                       'an existing convention. Possible conventions: {}.'\n                       .format(', '.join(conventions)))\n        add_check('--add-select', metavar='<codes>', default=None,\n                  help='add extra error codes to check to the basic list of '\n                       'errors previously set by --select, --ignore or '\n                       '--convention.')\n        add_check('--add-ignore', metavar='<codes>', default=None,\n                  help='ignore extra error codes by removing them from the '\n                       'basic list previously set by --select, --ignore '\n                       'or --convention.')\n\n        parser.add_option_group(check_group)\n\n        # Match clauses\n        option('--match', metavar='<pattern>', default=None,\n               help=(\"check only files that exactly match <pattern> regular \"\n                     \"expression; default is --match='{}' which matches \"\n                     \"files that don't start with 'test_' but end with \"\n                     \"'.py'\").format(cls.DEFAULT_MATCH_RE))\n        option('--match-dir', metavar='<pattern>', default=None,\n               help=(\"search only dirs that exactly match <pattern> regular \"\n                     \"expression; default is --match-dir='{}', which \"\n                     \"matches all dirs that don't start with \"\n                     \"a dot\").format(cls.DEFAULT_MATCH_DIR_RE))\n\n        # Decorators\n        option('--ignore-decorators', metavar='<decorators>', default=None,\n               help=(\"ignore any functions or methods that are decorated \"\n                     \"by a function with a name fitting the <decorators> \"\n                     \"regular expression; default is --ignore-decorators='{}'\"\n                     \" which does not ignore any decorated functions.\"\n                     .format(cls.DEFAULT_IGNORE_DECORATORS_RE)))\n\n        return parser"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn pairs of items from iterable.", "response": "def pairwise(\n    iterable: Iterable,\n    default_value: Any,\n) -> Iterable[Tuple[Any, Any]]:\n    \"\"\"Return pairs of items from `iterable`.\n\n    pairwise([1, 2, 3], default_value=None) -> (1, 2) (2, 3), (3, None)\n    \"\"\"\n    a, b = tee(iterable)\n    _ = next(b, default_value)\n    return zip_longest(a, b, fillvalue=default_value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\niterate over lines of a wordlist data file.", "response": "def load_wordlist(name: str) -> Iterator[str]:\n    \"\"\"Iterate over lines of a wordlist data file.\n\n    `name` should be the name of a package data file within the data/\n    directory.\n\n    Whitespace and #-prefixed comments are stripped from each line.\n\n    \"\"\"\n    data = pkgutil.get_data('pydocstyle', 'data/' + name)\n    if data is not None:\n        text = data.decode('utf8')\n        for line in text.splitlines():\n            line = COMMENT_RE.sub('', line).strip()\n            if line:\n                yield line"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmaximizes a function in EvolutionarySearchCV.", "response": "def maximize(func, parameter_dict, args={},\n             verbose=False, population_size=50,\n             gene_mutation_prob=0.1, gene_crossover_prob=0.5,\n             tournament_size=3, generations_number=10, gene_type=None,\n             n_jobs=1, error_score='raise'):\n    \"\"\" Same as _fit in EvolutionarySearchCV but without fitting data. More similar to scipy.optimize.\n\n        Parameters\n        ------------------\n        n_jobs : int or map function, default=1\n            Number of jobs to run in parallel.\n            Also accepts custom parallel map functions from Pool or SCOOP.\n\n        Returns\n        ------------------\n        best_params_ : dict\n            A list of parameters for the best learner.\n\n        best_score_ : float\n            The score of the learner described by best_params_\n\n        score_results : tuple of 2-tuples ((dict, float), ...)\n            The score of every individual evaluation indexed by it's parameters.\n\n        hist : deap.tools.History object.\n            Use to get the geneology data of the search.\n\n        logbook: deap.tools.Logbook object.\n            Includes the statistics of the evolution.\n    \"\"\"\n\n    toolbox = base.Toolbox()\n\n    _check_param_grid(parameter_dict)\n    if isinstance(n_jobs, int):\n        # If n_jobs is an int, greater than 1 or less than 0 (indicating to use as\n        # many jobs as possible) then we are going to create a default pool.\n        # Windows users need to be warned of this feature as it only works properly\n        # on linux. They need to encapsulate their pool in an if __name__ == \"__main__\"\n        # wrapper so that pools are not recursively created when the module is reloaded in each map\n        if isinstance(n_jobs, (int, float)):\n            if n_jobs > 1 or n_jobs < 0:\n                from multiprocessing import Pool  # Only imports if needed\n                if os.name == 'nt':               # Checks if we are on Windows\n                    warnings.warn((\"Windows requires Pools to be declared from within \"\n                                   \"an \\'if __name__==\\\"__main__\\\":\\' structure. In this \"\n                                   \"case, n_jobs will accept map functions as well to \"\n                                   \"facilitate custom parallelism. Please check to see \"\n                                   \"that all code is working as expected.\"))\n                pool = Pool(n_jobs)\n                toolbox.register(\"map\", pool.map)\n                warnings.warn(\"Need to create a creator. Run optimize.compile()\")\n            else:\n                compile()\n\n    # If it's not an int, we are going to pass it as the map directly\n    else:\n        try:\n            toolbox.register(\"map\", n_jobs)\n        except Exception:\n            raise TypeError(\"n_jobs must be either an integer or map function. Received: {}\".format(type(n_jobs)))\n\n    name_values, gene_type, maxints = _get_param_types_maxint(parameter_dict)\n\n    if verbose:\n        print(\"Types %s and maxint %s detected\" % (gene_type, maxints))\n\n    toolbox.register(\"individual\", _initIndividual, creator.Individual, maxints=maxints)\n    toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n\n    toolbox.register(\"evaluate\", _evalFunction, func,\n                     name_values=name_values, verbose=verbose,\n                     error_score=error_score, args=args)\n\n    toolbox.register(\"mate\", _cxIndividual, indpb=gene_crossover_prob, gene_type=gene_type)\n\n    toolbox.register(\"mutate\", _mutIndividual, indpb=gene_mutation_prob, up=maxints)\n    toolbox.register(\"select\", tools.selTournament, tournsize=tournament_size)\n\n    # Tools\n    pop = toolbox.population(n=population_size)\n    hof = tools.HallOfFame(1)\n    stats = tools.Statistics(lambda ind: ind.fitness.values)\n    stats.register(\"avg\", np.nanmean)\n    stats.register(\"min\", np.nanmin)\n    stats.register(\"max\", np.nanmax)\n    stats.register(\"std\", np.nanstd)\n\n    # History\n    hist = tools.History()\n    toolbox.decorate(\"mate\", hist.decorator)\n    toolbox.decorate(\"mutate\", hist.decorator)\n    hist.update(pop)\n\n    if verbose:\n        print('--- Evolve in {0} possible combinations ---'.format(np.prod(np.array(maxints) + 1)))\n\n    pop, logbook = algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.2,\n                                       ngen=generations_number, stats=stats,\n                                       halloffame=hof, verbose=verbose)\n\n    current_best_score_ = hof[0].fitness.values[0]\n    current_best_params_ = _individual_to_params(hof[0], name_values)\n\n    # Generate score_cache with real parameters\n    _, individuals, each_scores = zip(*[(idx, indiv, np.mean(indiv.fitness.values))\n                                        for idx, indiv in list(hist.genealogy_history.items())\n                                        if indiv.fitness.valid and not np.all(np.isnan(indiv.fitness.values))])\n    unique_individuals = {str(indiv): (indiv, score) for indiv, score in zip(individuals, each_scores)}\n    score_results = tuple([(_individual_to_params(indiv, name_values), score)\n                           for indiv, score in unique_individuals.values()])\n\n    if verbose:\n        print(\"Best individual is: %s\\nwith fitness: %s\" % (\n            current_best_params_, current_best_score_))\n\n    # Close your pools if you made them\n    if isinstance(n_jobs, int) and (n_jobs > 1 or n_jobs < 0):\n        pool.close()\n        pool.join()\n\n    return current_best_params_, current_best_score_, score_results, hist, logbook"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn characteristics of parameters", "response": "def _get_param_types_maxint(params):\n    \"\"\"\n    Returns characteristics of parameters\n    :param params: dictionary of pairs\n        it must have parameter_name:list of possible values:\n        params = {\"kernel\": [\"rbf\"],\n                 \"C\"     : [1,2,3,4,5,6,7,8],\n                 \"gamma\" : np.logspace(-9, 9, num=25, base=10)}\n    :return: name_values pairs - list of (name,possible_values) tuples for each parameter\n             types - list of types for each parameter\n             maxints - list of maximum integer for each particular gene in chromosome\n    \"\"\"\n    name_values = list(params.items())\n    types = []\n    for _, possible_values in name_values:\n        if isinstance(possible_values[0], float):\n            types.append(param_types.Numerical)\n        else:\n            types.append(param_types.Categorical)\n    maxints = [len(possible_values) - 1 for _, possible_values in name_values]\n    return name_values, types, maxints"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nevaluate the function for the given individual.", "response": "def _evalFunction(individual, name_values, X, y, scorer, cv, iid, fit_params,\n                  verbose=0, error_score='raise', score_cache={}):\n    \"\"\" Developer Note:\n        --------------------\n        score_cache was purposefully moved to parameters, and given a dict reference.\n        It will be modified in-place by _evalFunction based on it's reference.\n        This is to allow for a managed, paralell memoization dict,\n        and also for different memoization per instance of EvolutionaryAlgorithmSearchCV.\n        Remember that dicts created inside function definitions are presistent between calls,\n        So unless it is replaced this function will be memoized each call automatically. \"\"\"\n\n    parameters = _individual_to_params(individual, name_values)\n    score = 0\n    n_test = 0\n\n    paramkey = str(individual)\n    if paramkey in score_cache:\n        score = score_cache[paramkey]\n    else:\n        for train, test in cv.split(X, y):\n            assert len(train) > 0 and len(test) > 0, \"Training and/or testing not long enough for evaluation.\"\n            _score = _fit_and_score(estimator=individual.est, X=X, y=y, scorer=scorer,\n                                    train=train, test=test, verbose=verbose,\n                                    parameters=parameters, fit_params=fit_params,\n                                    error_score=error_score)[0]\n\n            if iid:\n                score += _score * len(test)\n                n_test += len(test)\n            else:\n                score += _score\n                n_test += 1\n\n        assert n_test > 0, \"No fitting was accomplished, check data and cross validation method.\"\n        score /= float(n_test)\n        score_cache[paramkey] = score\n\n    return (score,)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nuses when assuming params is a list.", "response": "def possible_params(self):\n        \"\"\" Used when assuming params is a list. \"\"\"\n        return self.params if isinstance(self.params, list) else [self.params]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef service_account_email(self):\n        if self._service_account_id is None:\n            self._service_account_id = app_identity.get_service_account_name()\n        return self._service_account_id", "response": "The service account email."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting the given credentials to a google. oauth2. credentials. Credentials object.", "response": "def _convert_oauth2_credentials(credentials):\n    \"\"\"Converts to :class:`google.oauth2.credentials.Credentials`.\n\n    Args:\n        credentials (Union[oauth2client.client.OAuth2Credentials,\n            oauth2client.client.GoogleCredentials]): The credentials to\n            convert.\n\n    Returns:\n        google.oauth2.credentials.Credentials: The converted credentials.\n    \"\"\"\n    new_credentials = google.oauth2.credentials.Credentials(\n        token=credentials.access_token,\n        refresh_token=credentials.refresh_token,\n        token_uri=credentials.token_uri,\n        client_id=credentials.client_id,\n        client_secret=credentials.client_secret,\n        scopes=credentials.scopes)\n\n    new_credentials._expires = credentials.token_expiry\n\n    return new_credentials"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _convert_service_account_credentials(credentials):\n    info = credentials.serialization_data.copy()\n    info['token_uri'] = credentials.token_uri\n    return google.oauth2.service_account.Credentials.from_service_account_info(\n        info)", "response": "Converts the given credentials to a Google Cloud Service Account Credentials object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert the given credentials to a new AppAssertionCredentials object.", "response": "def _convert_appengine_app_assertion_credentials(credentials):\n    \"\"\"Converts to :class:`google.auth.app_engine.Credentials`.\n\n    Args:\n        credentials (oauth2client.contrib.app_engine.AppAssertionCredentials):\n            The credentials to convert.\n\n    Returns:\n        google.oauth2.service_account.Credentials: The converted credentials.\n    \"\"\"\n    # pylint: disable=invalid-name\n    return google.auth.app_engine.Credentials(\n        scopes=_helpers.string_to_scopes(credentials.scope),\n        service_account_id=credentials.service_account_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef convert(credentials):\n\n    credentials_class = type(credentials)\n\n    try:\n        return _CLASS_CONVERSION_MAP[credentials_class](credentials)\n    except KeyError as caught_exc:\n        new_exc = ValueError(_CONVERT_ERROR_TMPL.format(credentials_class))\n        six.raise_from(new_exc, caught_exc)", "response": "Convert oauth2client credentials to google - auth credentials."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmake a request to the Google Cloud IAM service for an access token.", "response": "def _make_iam_token_request(request, principal, headers, body):\n    \"\"\"Makes a request to the Google Cloud IAM service for an access token.\n    Args:\n        request (Request): The Request object to use.\n        principal (str): The principal to request an access token for.\n        headers (Mapping[str, str]): Map of headers to transmit.\n        body (Mapping[str, str]): JSON Payload body for the iamcredentials\n            API call.\n\n    Raises:\n        TransportError: Raised if there is an underlying HTTP connection\n        Error\n        DefaultCredentialsError: Raised if the impersonated credentials\n        are not available.  Common reasons are\n        `iamcredentials.googleapis.com` is not enabled or the\n        `Service Account Token Creator` is not assigned\n    \"\"\"\n    iam_endpoint = _IAM_ENDPOINT.format(principal)\n\n    body = json.dumps(body)\n\n    response = request(\n        url=iam_endpoint,\n        method='POST',\n        headers=headers,\n        body=body)\n\n    response_body = response.data.decode('utf-8')\n\n    if response.status != http_client.OK:\n        exceptions.RefreshError(_REFRESH_ERROR, response_body)\n\n    try:\n        token_response = json.loads(response.data.decode('utf-8'))\n        token = token_response['accessToken']\n        expiry = datetime.strptime(\n            token_response['expireTime'], '%Y-%m-%dT%H:%M:%SZ')\n\n        return token, expiry\n\n    except (KeyError, ValueError) as caught_exc:\n        new_exc = exceptions.RefreshError(\n            '{}: No access token or invalid expiration in response.'.format(\n                _REFRESH_ERROR),\n            response_body)\n        six.raise_from(new_exc, caught_exc)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nverify a message using a list of certificates.", "response": "def verify_signature(message, signature, certs):\n    \"\"\"Verify an RSA cryptographic signature.\n\n    Checks that the provided ``signature`` was generated from ``bytes`` using\n    the private key associated with the ``cert``.\n\n    Args:\n        message (Union[str, bytes]): The plaintext message.\n        signature (Union[str, bytes]): The cryptographic signature to check.\n        certs (Union[Sequence, str, bytes]): The certificate or certificates\n            to use to check the signature.\n\n    Returns:\n        bool: True if the signature is valid, otherwise False.\n    \"\"\"\n    if isinstance(certs, (six.text_type, six.binary_type)):\n        certs = [certs]\n\n    for cert in certs:\n        verifier = rsa.RSAVerifier.from_string(cert)\n        if verifier.verify(message, signature):\n            return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmaking a request to the API signBlob API.", "response": "def _make_signing_request(self, message):\n        \"\"\"Makes a request to the API signBlob API.\"\"\"\n        message = _helpers.to_bytes(message)\n\n        method = 'POST'\n        url = _SIGN_BLOB_URI.format(self._service_account_email)\n        headers = {}\n        body = json.dumps({\n            'bytesToSign': base64.b64encode(message).decode('utf-8'),\n        })\n\n        self._credentials.before_request(self._request, method, url, headers)\n        response = self._request(\n            url=url, method=method, body=body, headers=headers)\n\n        if response.status != http_client.OK:\n            raise exceptions.TransportError(\n                'Error calling the IAM signBytes API: {}'.format(\n                    response.data))\n\n        return json.loads(response.data.decode('utf-8'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfetches certificates. cerificate endpoints return JSON in the format of", "response": "def _fetch_certs(request, certs_url):\n    \"\"\"Fetches certificates.\n\n    Google-style cerificate endpoints return JSON in the format of\n    ``{'key id': 'x509 certificate'}``.\n\n    Args:\n        request (google.auth.transport.Request): The object used to make\n            HTTP requests.\n        certs_url (str): The certificate endpoint URL.\n\n    Returns:\n        Mapping[str, str]: A mapping of public key ID to x.509 certificate\n            data.\n    \"\"\"\n    response = request(certs_url, method='GET')\n\n    if response.status != http_client.OK:\n        raise exceptions.TransportError(\n            'Could not fetch certificates at {}'.format(certs_url))\n\n    return json.loads(response.data.decode('utf-8'))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nverifying an ID token and returns the decoded token.", "response": "def verify_token(id_token, request, audience=None,\n                 certs_url=_GOOGLE_OAUTH2_CERTS_URL):\n    \"\"\"Verifies an ID token and returns the decoded token.\n\n    Args:\n        id_token (Union[str, bytes]): The encoded token.\n        request (google.auth.transport.Request): The object used to make\n            HTTP requests.\n        audience (str): The audience that this token is intended for. If None\n            then the audience is not verified.\n        certs_url (str): The URL that specifies the certificates to use to\n            verify the token. This URL should return JSON in the format of\n            ``{'key id': 'x509 certificate'}``.\n\n    Returns:\n        Mapping[str, Any]: The decoded token.\n    \"\"\"\n    certs = _fetch_certs(request, certs_url)\n\n    return jwt.decode(id_token, certs=certs, audience=audience)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef verify_oauth2_token(id_token, request, audience=None):\n    return verify_token(\n        id_token, request, audience=audience,\n        certs_url=_GOOGLE_OAUTH2_CERTS_URL)", "response": "Verifies an ID Token issued by Google s OAuth 2. 0 authorization server."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef verify_firebase_token(id_token, request, audience=None):\n    return verify_token(\n        id_token, request, audience=audience, certs_url=_GOOGLE_APIS_CERTS_URL)", "response": "Verifies an ID Token issued by Firebase Authentication."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nencode a signed JWT.", "response": "def encode(signer, payload, header=None, key_id=None):\n    \"\"\"Make a signed JWT.\n\n    Args:\n        signer (google.auth.crypt.Signer): The signer used to sign the JWT.\n        payload (Mapping[str, str]): The JWT payload.\n        header (Mapping[str, str]): Additional JWT header payload.\n        key_id (str): The key id to add to the JWT header. If the\n            signer has a key id it will be used as the default. If this is\n            specified it will override the signer's key id.\n\n    Returns:\n        bytes: The encoded JWT.\n    \"\"\"\n    if header is None:\n        header = {}\n\n    if key_id is None:\n        key_id = signer.key_id\n\n    header.update({'typ': 'JWT', 'alg': 'RS256'})\n\n    if key_id is not None:\n        header['kid'] = key_id\n\n    segments = [\n        _helpers.unpadded_urlsafe_b64encode(\n            json.dumps(header).encode('utf-8')\n        ),\n        _helpers.unpadded_urlsafe_b64encode(\n            json.dumps(payload).encode('utf-8')\n        ),\n    ]\n\n    signing_input = b'.'.join(segments)\n    signature = signer.sign(signing_input)\n    segments.append(\n        _helpers.unpadded_urlsafe_b64encode(signature)\n    )\n\n    return b'.'.join(segments)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _decode_jwt_segment(encoded_section):\n    section_bytes = _helpers.padded_urlsafe_b64decode(encoded_section)\n    try:\n        return json.loads(section_bytes.decode('utf-8'))\n    except ValueError as caught_exc:\n        new_exc = ValueError('Can\\'t parse segment: {0}'.format(section_bytes))\n        six.raise_from(new_exc, caught_exc)", "response": "Decodes a single JWT segment."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _unverified_decode(token):\n    token = _helpers.to_bytes(token)\n\n    if token.count(b'.') != 2:\n        raise ValueError(\n            'Wrong number of segments in token: {0}'.format(token))\n\n    encoded_header, encoded_payload, signature = token.split(b'.')\n    signed_section = encoded_header + b'.' + encoded_payload\n    signature = _helpers.padded_urlsafe_b64decode(signature)\n\n    # Parse segments\n    header = _decode_jwt_segment(encoded_header)\n    payload = _decode_jwt_segment(encoded_payload)\n\n    return header, payload, signed_section, signature", "response": "Decodes a token and does no verification."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nverify the iat and exp claims in a token .", "response": "def _verify_iat_and_exp(payload):\n    \"\"\"Verifies the ``iat`` (Issued At) and ``exp`` (Expires) claims in a token\n    payload.\n\n    Args:\n        payload (Mapping[str, str]): The JWT payload.\n\n    Raises:\n        ValueError: if any checks failed.\n    \"\"\"\n    now = _helpers.datetime_to_secs(_helpers.utcnow())\n\n    # Make sure the iat and exp claims are present.\n    for key in ('iat', 'exp'):\n        if key not in payload:\n            raise ValueError(\n                'Token does not contain required claim {}'.format(key))\n\n    # Make sure the token wasn't issued in the future.\n    iat = payload['iat']\n    # Err on the side of accepting a token that is slightly early to account\n    # for clock skew.\n    earliest = iat - _helpers.CLOCK_SKEW_SECS\n    if now < earliest:\n        raise ValueError('Token used too early, {} < {}'.format(now, iat))\n\n    # Make sure the token wasn't issued in the past.\n    exp = payload['exp']\n    # Err on the side of accepting a token that is slightly out of date\n    # to account for clow skew.\n    latest = exp + _helpers.CLOCK_SKEW_SECS\n    if latest < now:\n        raise ValueError('Token expired, {} < {}'.format(latest, now))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndecode and verify a JWT.", "response": "def decode(token, certs=None, verify=True, audience=None):\n    \"\"\"Decode and verify a JWT.\n\n    Args:\n        token (str): The encoded JWT.\n        certs (Union[str, bytes, Mapping[str, Union[str, bytes]]]): The\n            certificate used to validate the JWT signature. If bytes or string,\n            it must the the public key certificate in PEM format. If a mapping,\n            it must be a mapping of key IDs to public key certificates in PEM\n            format. The mapping must contain the same key ID that's specified\n            in the token's header.\n        verify (bool): Whether to perform signature and claim validation.\n            Verification is done by default.\n        audience (str): The audience claim, 'aud', that this JWT should\n            contain. If None then the JWT's 'aud' parameter is not verified.\n\n    Returns:\n        Mapping[str, str]: The deserialized JSON payload in the JWT.\n\n    Raises:\n        ValueError: if any verification checks failed.\n    \"\"\"\n    header, payload, signed_section, signature = _unverified_decode(token)\n\n    if not verify:\n        return payload\n\n    # If certs is specified as a dictionary of key IDs to certificates, then\n    # use the certificate identified by the key ID in the token header.\n    if isinstance(certs, collections.Mapping):\n        key_id = header.get('kid')\n        if key_id:\n            if key_id not in certs:\n                raise ValueError(\n                    'Certificate for key id {} not found.'.format(key_id))\n            certs_to_check = [certs[key_id]]\n        # If there's no key id in the header, check against all of the certs.\n        else:\n            certs_to_check = certs.values()\n    else:\n        certs_to_check = certs\n\n    # Verify that the signature matches the message.\n    if not crypt.verify_signature(signed_section, signature, certs_to_check):\n        raise ValueError('Could not verify token signature.')\n\n    # Verify the issued at and created times in the payload.\n    _verify_iat_and_exp(payload)\n\n    # Check audience.\n    if audience is not None:\n        claim_audience = payload.get('aud')\n        if audience != claim_audience:\n            raise ValueError(\n                'Token has wrong audience {}, expected {}'.format(\n                    claim_audience, audience))\n\n    return payload"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a new Credentials instance from a signer and service account info.", "response": "def _from_signer_and_info(cls, signer, info, **kwargs):\n        \"\"\"Creates a Credentials instance from a signer and service account\n        info.\n\n        Args:\n            signer (google.auth.crypt.Signer): The signer used to sign JWTs.\n            info (Mapping[str, str]): The service account info.\n            kwargs: Additional arguments to pass to the constructor.\n\n        Returns:\n            google.auth.jwt.Credentials: The constructed credentials.\n\n        Raises:\n            ValueError: If the info is not in the expected format.\n        \"\"\"\n        kwargs.setdefault('subject', info['client_email'])\n        kwargs.setdefault('issuer', info['client_email'])\n        return cls(signer, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef with_claims(self, issuer=None, subject=None, audience=None,\n                    additional_claims=None):\n        \"\"\"Returns a copy of these credentials with modified claims.\n\n        Args:\n            issuer (str): The `iss` claim. If unspecified the current issuer\n                claim will be used.\n            subject (str): The `sub` claim. If unspecified the current subject\n                claim will be used.\n            audience (str): the `aud` claim. If unspecified the current\n                audience claim will be used.\n            additional_claims (Mapping[str, str]): Any additional claims for\n                the JWT payload. This will be merged with the current\n                additional claims.\n\n        Returns:\n            google.auth.jwt.Credentials: A new credentials instance.\n        \"\"\"\n        new_additional_claims = copy.deepcopy(self._additional_claims)\n        new_additional_claims.update(additional_claims or {})\n\n        return self.__class__(\n            self._signer,\n            issuer=issuer if issuer is not None else self._issuer,\n            subject=subject if subject is not None else self._subject,\n            audience=audience if audience is not None else self._audience,\n            additional_claims=new_additional_claims)", "response": "Returns a copy of this credentials with modified claims."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmaking a signed JWT.", "response": "def _make_jwt(self):\n        \"\"\"Make a signed JWT.\n\n        Returns:\n            Tuple[bytes, datetime]: The encoded JWT and the expiration.\n        \"\"\"\n        now = _helpers.utcnow()\n        lifetime = datetime.timedelta(seconds=self._token_lifetime)\n        expiry = now + lifetime\n\n        payload = {\n            'iss': self._issuer,\n            'sub': self._subject,\n            'iat': _helpers.datetime_to_secs(now),\n            'exp': _helpers.datetime_to_secs(expiry),\n            'aud': self._audience,\n        }\n\n        payload.update(self._additional_claims)\n\n        jwt = encode(self._signer, payload)\n\n        return jwt, expiry"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new instance of this class from a given credentials.", "response": "def from_signing_credentials(cls, credentials, **kwargs):\n        \"\"\"Creates a new :class:`google.auth.jwt.OnDemandCredentials` instance\n        from an existing :class:`google.auth.credentials.Signing` instance.\n\n        The new instance will use the same signer as the existing instance and\n        will use the existing instance's signer email as the issuer and\n        subject by default.\n\n        Example::\n\n            svc_creds = service_account.Credentials.from_service_account_file(\n                'service_account.json')\n            jwt_creds = jwt.OnDemandCredentials.from_signing_credentials(\n                svc_creds)\n\n        Args:\n            credentials (google.auth.credentials.Signing): The credentials to\n                use to construct the new credentials.\n            kwargs: Additional arguments to pass to the constructor.\n\n        Returns:\n            google.auth.jwt.Credentials: A new Credentials instance.\n        \"\"\"\n        kwargs.setdefault('issuer', credentials.signer_email)\n        kwargs.setdefault('subject', credentials.signer_email)\n        return cls(credentials.signer, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef with_claims(self, issuer=None, subject=None, additional_claims=None):\n        new_additional_claims = copy.deepcopy(self._additional_claims)\n        new_additional_claims.update(additional_claims or {})\n\n        return self.__class__(\n            self._signer,\n            issuer=issuer if issuer is not None else self._issuer,\n            subject=subject if subject is not None else self._subject,\n            additional_claims=new_additional_claims,\n            max_cache_size=self._cache.maxsize)", "response": "Returns a copy of this credentials with modified claims."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_jwt_for_audience(self, audience):\n        token, expiry = self._cache.get(audience, (None, None))\n\n        if token is None or expiry < _helpers.utcnow():\n            token, expiry = self._make_jwt_for_audience(audience)\n            self._cache[audience] = token, expiry\n\n        return token", "response": "Get a JWT for a given audience."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef before_request(self, request, method, url, headers):\n        # pylint: disable=unused-argument\n        # (pylint doesn't correctly recognize overridden methods.)\n        parts = urllib.parse.urlsplit(url)\n        # Strip query string and fragment\n        audience = urllib.parse.urlunsplit(\n            (parts.scheme, parts.netloc, parts.path, \"\", \"\"))\n        token = self._get_jwt_for_audience(audience)\n        self.apply(headers, token=token)", "response": "Performs credential - specific before request logic."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_string(cls, public_key):\n        public_key_data = _helpers.to_bytes(public_key)\n\n        if _CERTIFICATE_MARKER in public_key_data:\n            cert = cryptography.x509.load_pem_x509_certificate(\n                public_key_data, _BACKEND)\n            pubkey = cert.public_key()\n\n        else:\n            pubkey = serialization.load_pem_public_key(\n                public_key_data, _BACKEND)\n\n        return cls(pubkey)", "response": "Construct a Verifier instance from a public key or public_key_certificate string."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_string(cls, key, key_id=None):\n        key = _helpers.to_bytes(key)\n        private_key = serialization.load_pem_private_key(\n            key, password=None, backend=_BACKEND)\n        return cls(private_key, key_id=key_id)", "response": "Construct a RSASigner from a private key in PEM format."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _warn_about_problematic_credentials(credentials):\n    from google.auth import _cloud_sdk\n    if credentials.client_id == _cloud_sdk.CLOUD_SDK_CLIENT_ID:\n        warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)", "response": "Warns about problematic credentials."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload credentials from a file.", "response": "def _load_credentials_from_file(filename):\n    \"\"\"Loads credentials from a file.\n\n    The credentials file must be a service account key or stored authorized\n    user credentials.\n\n    Args:\n        filename (str): The full path to the credentials file.\n\n    Returns:\n        Tuple[google.auth.credentials.Credentials, Optional[str]]: Loaded\n            credentials and the project ID. Authorized user credentials do not\n            have the project ID information.\n\n    Raises:\n        google.auth.exceptions.DefaultCredentialsError: if the file is in the\n            wrong format or is missing.\n    \"\"\"\n    if not os.path.exists(filename):\n        raise exceptions.DefaultCredentialsError(\n            'File {} was not found.'.format(filename))\n\n    with io.open(filename, 'r') as file_obj:\n        try:\n            info = json.load(file_obj)\n        except ValueError as caught_exc:\n            new_exc = exceptions.DefaultCredentialsError(\n                'File {} is not a valid json file.'.format(filename),\n                caught_exc)\n            six.raise_from(new_exc, caught_exc)\n\n    # The type key should indicate that the file is either a service account\n    # credentials file or an authorized user credentials file.\n    credential_type = info.get('type')\n\n    if credential_type == _AUTHORIZED_USER_TYPE:\n        from google.auth import _cloud_sdk\n\n        try:\n            credentials = _cloud_sdk.load_authorized_user_credentials(info)\n        except ValueError as caught_exc:\n            msg = 'Failed to load authorized user credentials from {}'.format(\n                filename)\n            new_exc = exceptions.DefaultCredentialsError(msg, caught_exc)\n            six.raise_from(new_exc, caught_exc)\n        # Authorized user credentials do not contain the project ID.\n        _warn_about_problematic_credentials(credentials)\n        return credentials, None\n\n    elif credential_type == _SERVICE_ACCOUNT_TYPE:\n        from google.oauth2 import service_account\n\n        try:\n            credentials = (\n                service_account.Credentials.from_service_account_info(info))\n        except ValueError as caught_exc:\n            msg = 'Failed to load service account credentials from {}'.format(\n                filename)\n            new_exc = exceptions.DefaultCredentialsError(msg, caught_exc)\n            six.raise_from(new_exc, caught_exc)\n        return credentials, info.get('project_id')\n\n    else:\n        raise exceptions.DefaultCredentialsError(\n            'The file {file} does not have a valid type. '\n            'Type is {type}, expected one of {valid_types}.'.format(\n                file=filename, type=credential_type, valid_types=_VALID_TYPES))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_gcloud_sdk_credentials():\n    from google.auth import _cloud_sdk\n\n    # Check if application default credentials exist.\n    credentials_filename = (\n        _cloud_sdk.get_application_default_credentials_path())\n\n    if not os.path.isfile(credentials_filename):\n        return None, None\n\n    credentials, project_id = _load_credentials_from_file(\n        credentials_filename)\n\n    if not project_id:\n        project_id = _cloud_sdk.get_project_id()\n\n    return credentials, project_id", "response": "Gets the credentials and project ID from the Cloud SDK."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_explicit_environ_credentials():\n    explicit_file = os.environ.get(environment_vars.CREDENTIALS)\n\n    if explicit_file is not None:\n        credentials, project_id = _load_credentials_from_file(\n            os.environ[environment_vars.CREDENTIALS])\n\n        return credentials, project_id\n\n    else:\n        return None, None", "response": "Gets credentials from the GOOGLE_APPLICATION_CREDENTIALS environment variable."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets Google App Engine App Identity credentials and project ID.", "response": "def _get_gae_credentials():\n    \"\"\"Gets Google App Engine App Identity credentials and project ID.\"\"\"\n    # While this library is normally bundled with app_engine, there are\n    # some cases where it's not available, so we tolerate ImportError.\n    try:\n        import google.auth.app_engine as app_engine\n    except ImportError:\n        return None, None\n\n    try:\n        credentials = app_engine.Credentials()\n        project_id = app_engine.get_project_id()\n        return credentials, project_id\n    except EnvironmentError:\n        return None, None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_gce_credentials(request=None):\n    # Ping requires a transport, but we want application default credentials\n    # to require no arguments. So, we'll use the _http_client transport which\n    # uses http.client. This is only acceptable because the metadata server\n    # doesn't do SSL and never requires proxies.\n\n    # While this library is normally bundled with compute_engine, there are\n    # some cases where it's not available, so we tolerate ImportError.\n    try:\n        from google.auth import compute_engine\n        from google.auth.compute_engine import _metadata\n    except ImportError:\n        return None, None\n\n    if request is None:\n        request = google.auth.transport._http_client.Request()\n\n    if _metadata.ping(request=request):\n        # Get the project ID.\n        try:\n            project_id = _metadata.get_project_id(request=request)\n        except exceptions.TransportError:\n            project_id = None\n\n        return compute_engine.Credentials(), project_id\n    else:\n        return None, None", "response": "Gets credentials and project ID from the GCE Metadata Service."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the default credentials for the current environment.", "response": "def default(scopes=None, request=None):\n    \"\"\"Gets the default credentials for the current environment.\n\n    `Application Default Credentials`_ provides an easy way to obtain\n    credentials to call Google APIs for server-to-server or local applications.\n    This function acquires credentials from the environment in the following\n    order:\n\n    1. If the environment variable ``GOOGLE_APPLICATION_CREDENTIALS`` is set\n       to the path of a valid service account JSON private key file, then it is\n       loaded and returned. The project ID returned is the project ID defined\n       in the service account file if available (some older files do not\n       contain project ID information).\n    2. If the `Google Cloud SDK`_ is installed and has application default\n       credentials set they are loaded and returned.\n\n       To enable application default credentials with the Cloud SDK run::\n\n            gcloud auth application-default login\n\n       If the Cloud SDK has an active project, the project ID is returned. The\n       active project can be set using::\n\n            gcloud config set project\n\n    3. If the application is running in the `App Engine standard environment`_\n       then the credentials and project ID from the `App Identity Service`_\n       are used.\n    4. If the application is running in `Compute Engine`_ or the\n       `App Engine flexible environment`_ then the credentials and project ID\n       are obtained from the `Metadata Service`_.\n    5. If no credentials are found,\n       :class:`~google.auth.exceptions.DefaultCredentialsError` will be raised.\n\n    .. _Application Default Credentials: https://developers.google.com\\\n            /identity/protocols/application-default-credentials\n    .. _Google Cloud SDK: https://cloud.google.com/sdk\n    .. _App Engine standard environment: https://cloud.google.com/appengine\n    .. _App Identity Service: https://cloud.google.com/appengine/docs/python\\\n            /appidentity/\n    .. _Compute Engine: https://cloud.google.com/compute\n    .. _App Engine flexible environment: https://cloud.google.com\\\n            /appengine/flexible\n    .. _Metadata Service: https://cloud.google.com/compute/docs\\\n            /storing-retrieving-metadata\n\n    Example::\n\n        import google.auth\n\n        credentials, project_id = google.auth.default()\n\n    Args:\n        scopes (Sequence[str]): The list of scopes for the credentials. If\n            specified, the credentials will automatically be scoped if\n            necessary.\n        request (google.auth.transport.Request): An object used to make\n            HTTP requests. This is used to detect whether the application\n            is running on Compute Engine. If not specified, then it will\n            use the standard library http client to make requests.\n\n    Returns:\n        Tuple[~google.auth.credentials.Credentials, Optional[str]]:\n            the current environment's credentials and project ID. Project ID\n            may be None, which indicates that the Project ID could not be\n            ascertained from the environment.\n\n    Raises:\n        ~google.auth.exceptions.DefaultCredentialsError:\n            If no credentials were found, or if the credentials found were\n            invalid.\n    \"\"\"\n    from google.auth.credentials import with_scopes_if_required\n\n    explicit_project_id = os.environ.get(\n        environment_vars.PROJECT,\n        os.environ.get(environment_vars.LEGACY_PROJECT))\n\n    checkers = (\n        _get_explicit_environ_credentials,\n        _get_gcloud_sdk_credentials,\n        _get_gae_credentials,\n        lambda: _get_gce_credentials(request))\n\n    for checker in checkers:\n        credentials, project_id = checker()\n        if credentials is not None:\n            credentials = with_scopes_if_required(credentials, scopes)\n            effective_project_id = explicit_project_id or project_id\n            if not effective_project_id:\n                _LOGGER.warning(\n                    'No project ID could be determined. Consider running '\n                    '`gcloud config set project` or setting the %s '\n                    'environment variable',\n                    environment_vars.PROJECT)\n            return credentials, effective_project_id\n\n    raise exceptions.DefaultCredentialsError(_HELP_MESSAGE)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef secure_authorized_channel(\n        credentials, request, target, ssl_credentials=None, **kwargs):\n    \"\"\"Creates a secure authorized gRPC channel.\n\n    This creates a channel with SSL and :class:`AuthMetadataPlugin`. This\n    channel can be used to create a stub that can make authorized requests.\n\n    Example::\n\n        import google.auth\n        import google.auth.transport.grpc\n        import google.auth.transport.requests\n        from google.cloud.speech.v1 import cloud_speech_pb2\n\n        # Get credentials.\n        credentials, _ = google.auth.default()\n\n        # Get an HTTP request function to refresh credentials.\n        request = google.auth.transport.requests.Request()\n\n        # Create a channel.\n        channel = google.auth.transport.grpc.secure_authorized_channel(\n            credentials, 'speech.googleapis.com:443', request)\n\n        # Use the channel to create a stub.\n        cloud_speech.create_Speech_stub(channel)\n\n    Args:\n        credentials (google.auth.credentials.Credentials): The credentials to\n            add to requests.\n        request (google.auth.transport.Request): A HTTP transport request\n            object used to refresh credentials as needed. Even though gRPC\n            is a separate transport, there's no way to refresh the credentials\n            without using a standard http transport.\n        target (str): The host and port of the service.\n        ssl_credentials (grpc.ChannelCredentials): Optional SSL channel\n            credentials. This can be used to specify different certificates.\n        kwargs: Additional arguments to pass to :func:`grpc.secure_channel`.\n\n    Returns:\n        grpc.Channel: The created gRPC channel.\n    \"\"\"\n    # Create the metadata plugin for inserting the authorization header.\n    metadata_plugin = AuthMetadataPlugin(credentials, request)\n\n    # Create a set of grpc.CallCredentials using the metadata plugin.\n    google_auth_credentials = grpc.metadata_call_credentials(metadata_plugin)\n\n    if ssl_credentials is None:\n        ssl_credentials = grpc.ssl_channel_credentials()\n\n    # Combine the ssl credentials and the authorization credentials.\n    composite_credentials = grpc.composite_channel_credentials(\n        ssl_credentials, google_auth_credentials)\n\n    return grpc.secure_channel(target, composite_credentials, **kwargs)", "response": "Creates a secure authorized gRPC channel."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the authorization headers for a request.", "response": "def _get_authorization_headers(self, context):\n        \"\"\"Gets the authorization headers for a request.\n\n        Returns:\n            Sequence[Tuple[str, str]]: A list of request headers (key, value)\n                to add to the request.\n        \"\"\"\n        headers = {}\n        self._credentials.before_request(\n            self._request,\n            context.method_name,\n            context.service_url,\n            headers)\n\n        return list(six.iteritems(headers))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _from_signer_and_info(cls, signer, info, **kwargs):\n        return cls(\n            signer,\n            service_account_email=info['client_email'],\n            token_uri=info['token_uri'],\n            project_id=info.get('project_id'), **kwargs)", "response": "Creates a new Credentials instance from a signer and service account info."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_service_account_info(cls, info, **kwargs):\n        signer = _service_account_info.from_dict(\n            info, require=['client_email', 'token_uri'])\n        return cls._from_signer_and_info(signer, info, **kwargs)", "response": "Creates a Credentials instance from a service account info dictionary."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a Credentials instance from a service account json file.", "response": "def from_service_account_file(cls, filename, **kwargs):\n        \"\"\"Creates a Credentials instance from a service account json file.\n\n        Args:\n            filename (str): The path to the service account json file.\n            kwargs: Additional arguments to pass to the constructor.\n\n        Returns:\n            google.auth.service_account.Credentials: The constructed\n                credentials.\n        \"\"\"\n        info, signer = _service_account_info.from_filename(\n            filename, require=['client_email', 'token_uri'])\n        return cls._from_signer_and_info(signer, info, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef with_subject(self, subject):\n        return self.__class__(\n            self._signer,\n            service_account_email=self._service_account_email,\n            scopes=self._scopes,\n            token_uri=self._token_uri,\n            subject=subject,\n            project_id=self._project_id,\n            additional_claims=self._additional_claims.copy())", "response": "Returns a copy of these credentials with the specified subject."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef with_claims(self, additional_claims):\n        new_additional_claims = copy.deepcopy(self._additional_claims)\n        new_additional_claims.update(additional_claims or {})\n\n        return self.__class__(\n            self._signer,\n            service_account_email=self._service_account_email,\n            scopes=self._scopes,\n            token_uri=self._token_uri,\n            subject=self._subject,\n            project_id=self._project_id,\n            additional_claims=new_additional_claims)", "response": "Returns a copy of these credentials with modified claims."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate the OAuth 2. 0 authorization grant assertion.", "response": "def _make_authorization_grant_assertion(self):\n        \"\"\"Create the OAuth 2.0 assertion.\n\n        This assertion is used during the OAuth 2.0 grant to acquire an\n        access token.\n\n        Returns:\n            bytes: The authorization grant assertion.\n        \"\"\"\n        now = _helpers.utcnow()\n        lifetime = datetime.timedelta(seconds=_DEFAULT_TOKEN_LIFETIME_SECS)\n        expiry = now + lifetime\n\n        payload = {\n            'iat': _helpers.datetime_to_secs(now),\n            'exp': _helpers.datetime_to_secs(expiry),\n            # The issuer must be the service account email.\n            'iss': self._service_account_email,\n            # The audience must be the auth token endpoint's URI\n            'aud': self._token_uri,\n            'scope': _helpers.scopes_to_string(self._scopes or ())\n        }\n\n        payload.update(self._additional_claims)\n\n        # The subject can be a user email for domain-wide delegation.\n        if self._subject:\n            payload.setdefault('sub', self._subject)\n\n        token = jwt.encode(self._signer, payload)\n\n        return token"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef with_target_audience(self, target_audience):\n        return self.__class__(\n            self._signer,\n            service_account_email=self._service_account_email,\n            token_uri=self._token_uri,\n            target_audience=target_audience,\n            additional_claims=self._additional_claims.copy())", "response": "Returns a copy of this ID TokenCredentials with the specified target_audience."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate the OAuth 2. 0 authorization grant assertion.", "response": "def _make_authorization_grant_assertion(self):\n        \"\"\"Create the OAuth 2.0 assertion.\n\n        This assertion is used during the OAuth 2.0 grant to acquire an\n        ID token.\n\n        Returns:\n            bytes: The authorization grant assertion.\n        \"\"\"\n        now = _helpers.utcnow()\n        lifetime = datetime.timedelta(seconds=_DEFAULT_TOKEN_LIFETIME_SECS)\n        expiry = now + lifetime\n\n        payload = {\n            'iat': _helpers.datetime_to_secs(now),\n            'exp': _helpers.datetime_to_secs(expiry),\n            # The issuer must be the service account email.\n            'iss': self.service_account_email,\n            # The audience must be the auth token endpoint's URI\n            'aud': self._token_uri,\n            # The target audience specifies which service the ID token is\n            # intended for.\n            'target_audience': self._target_audience\n        }\n\n        payload.update(self._additional_claims)\n\n        token = jwt.encode(self._signer, payload)\n\n        return token"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef request(self, method, url, data=None, headers=None, **kwargs):\n        # pylint: disable=arguments-differ\n        # Requests has a ton of arguments to request, but only two\n        # (method, url) are required. We pass through all of the other\n        # arguments to super, so no need to exhaustively list them here.\n\n        # Use a kwarg for this instead of an attribute to maintain\n        # thread-safety.\n        _credential_refresh_attempt = kwargs.pop(\n            '_credential_refresh_attempt', 0)\n\n        # Make a copy of the headers. They will be modified by the credentials\n        # and we want to pass the original headers if we recurse.\n        request_headers = headers.copy() if headers is not None else {}\n\n        self.credentials.before_request(\n            self._auth_request, method, url, request_headers)\n\n        response = super(AuthorizedSession, self).request(\n            method, url, data=data, headers=request_headers, **kwargs)\n\n        # If the response indicated that the credentials needed to be\n        # refreshed, then refresh the credentials and re-attempt the\n        # request.\n        # A stored token may expire between the time it is retrieved and\n        # the time the request is made, so we may need to try twice.\n        if (response.status_code in self._refresh_status_codes\n                and _credential_refresh_attempt < self._max_refresh_attempts):\n\n            _LOGGER.info(\n                'Refreshing credentials due to a %s response. Attempt %s/%s.',\n                response.status_code, _credential_refresh_attempt + 1,\n                self._max_refresh_attempts)\n\n            auth_request_with_timeout = functools.partial(\n                self._auth_request, timeout=self._refresh_timeout)\n            self.credentials.refresh(auth_request_with_timeout)\n\n            # Recurse. Pass in the original headers, not our modified set.\n            return self.request(\n                method, url, data=data, headers=headers,\n                _credential_refresh_attempt=_credential_refresh_attempt + 1,\n                **kwargs)\n\n        return response", "response": "Implementation of Requests request."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a copy of the credentials with scopes if scoping is required.", "response": "def with_scopes_if_required(credentials, scopes):\n    \"\"\"Creates a copy of the credentials with scopes if scoping is required.\n\n    This helper function is useful when you do not know (or care to know) the\n    specific type of credentials you are using (such as when you use\n    :func:`google.auth.default`). This function will call\n    :meth:`Scoped.with_scopes` if the credentials are scoped credentials and if\n    the credentials require scoping. Otherwise, it will return the credentials\n    as-is.\n\n    Args:\n        credentials (google.auth.credentials.Credentials): The credentials to\n            scope if necessary.\n        scopes (Sequence[str]): The list of scopes to use.\n\n    Returns:\n        google.auth.credentials.Credentials: Either a new set of scoped\n            credentials, or the passed in credentials instance if no scoping\n            was required.\n    \"\"\"\n    if isinstance(credentials, Scoped) and credentials.requires_scopes:\n        return credentials.with_scopes(scopes)\n    else:\n        return credentials"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert an iterable of 1s and 0s to bytes.", "response": "def _bit_list_to_bytes(bit_list):\n    \"\"\"Converts an iterable of 1s and 0s to bytes.\n\n    Combines the list 8 at a time, treating each group of 8 bits\n    as a single byte.\n\n    Args:\n        bit_list (Sequence): Sequence of 1s and 0s.\n\n    Returns:\n        bytes: The decoded bytes.\n    \"\"\"\n    num_bits = len(bit_list)\n    byte_vals = bytearray()\n    for start in six.moves.xrange(0, num_bits, 8):\n        curr_bits = bit_list[start:start + 8]\n        char_val = sum(\n            val * digit for val, digit in six.moves.zip(_POW2, curr_bits))\n        byte_vals.append(char_val)\n    return bytes(byte_vals)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconstructing a new Signer instance from a private key in PEM format.", "response": "def from_string(cls, key, key_id=None):\n        \"\"\"Construct an Signer instance from a private key in PEM format.\n\n        Args:\n            key (str): Private key in PEM format.\n            key_id (str): An optional key id used to identify the private key.\n\n        Returns:\n            google.auth.crypt.Signer: The constructed signer.\n\n        Raises:\n            ValueError: If the key cannot be parsed as PKCS#1 or PKCS#8 in\n                PEM format.\n        \"\"\"\n        key = _helpers.from_bytes(key)  # PEM expects str in Python 3\n        marker_id, key_bytes = pem.readPemBlocksFromFile(\n            six.StringIO(key), _PKCS1_MARKER, _PKCS8_MARKER)\n\n        # Key is in pkcs1 format.\n        if marker_id == 0:\n            private_key = rsa.key.PrivateKey.load_pkcs1(\n                key_bytes, format='DER')\n        # Key is in pkcs8.\n        elif marker_id == 1:\n            key_info, remaining = decoder.decode(\n                key_bytes, asn1Spec=_PKCS8_SPEC)\n            if remaining != b'':\n                raise ValueError('Unused bytes', remaining)\n            private_key_info = key_info.getComponentByName('privateKey')\n            private_key = rsa.key.PrivateKey.load_pkcs1(\n                private_key_info.asOctets(), format='DER')\n        else:\n            raise ValueError('No key could be detected.')\n\n        return cls(private_key, key_id=key_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _handle_error_response(response_body):\n    try:\n        error_data = json.loads(response_body)\n        error_details = '{}: {}'.format(\n            error_data['error'],\n            error_data.get('error_description'))\n    # If no details could be extracted, use the response data.\n    except (KeyError, ValueError):\n        error_details = response_body\n\n    raise exceptions.RefreshError(\n        error_details, response_body)", "response": "Translates an error response into an exception."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _parse_expiry(response_data):\n    expires_in = response_data.get('expires_in', None)\n\n    if expires_in is not None:\n        return _helpers.utcnow() + datetime.timedelta(\n            seconds=expires_in)\n    else:\n        return None", "response": "Parses the expiry field from a response into a datetime."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmaking a request to the OAuth 2. 0 authorization server s token endpoint.", "response": "def _token_endpoint_request(request, token_uri, body):\n    \"\"\"Makes a request to the OAuth 2.0 authorization server's token endpoint.\n\n    Args:\n        request (google.auth.transport.Request): A callable used to make\n            HTTP requests.\n        token_uri (str): The OAuth 2.0 authorizations server's token endpoint\n            URI.\n        body (Mapping[str, str]): The parameters to send in the request body.\n\n    Returns:\n        Mapping[str, str]: The JSON-decoded response data.\n\n    Raises:\n        google.auth.exceptions.RefreshError: If the token endpoint returned\n            an error.\n    \"\"\"\n    body = urllib.parse.urlencode(body)\n    headers = {\n        'content-type': _URLENCODED_CONTENT_TYPE,\n    }\n\n    response = request(\n        method='POST', url=token_uri, headers=headers, body=body)\n\n    response_body = response.data.decode('utf-8')\n\n    if response.status != http_client.OK:\n        _handle_error_response(response_body)\n\n    response_data = json.loads(response_body)\n\n    return response_data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nimplementing the JWT Profile for OAuth 2. 0 Authorization Grants.", "response": "def jwt_grant(request, token_uri, assertion):\n    \"\"\"Implements the JWT Profile for OAuth 2.0 Authorization Grants.\n\n    For more details, see `rfc7523 section 4`_.\n\n    Args:\n        request (google.auth.transport.Request): A callable used to make\n            HTTP requests.\n        token_uri (str): The OAuth 2.0 authorizations server's token endpoint\n            URI.\n        assertion (str): The OAuth 2.0 assertion.\n\n    Returns:\n        Tuple[str, Optional[datetime], Mapping[str, str]]: The access token,\n            expiration, and additional data returned by the token endpoint.\n\n    Raises:\n        google.auth.exceptions.RefreshError: If the token endpoint returned\n            an error.\n\n    .. _rfc7523 section 4: https://tools.ietf.org/html/rfc7523#section-4\n    \"\"\"\n    body = {\n        'assertion': assertion,\n        'grant_type': _JWT_GRANT_TYPE,\n    }\n\n    response_data = _token_endpoint_request(request, token_uri, body)\n\n    try:\n        access_token = response_data['access_token']\n    except KeyError as caught_exc:\n        new_exc = exceptions.RefreshError(\n            'No access token in response.', response_data)\n        six.raise_from(new_exc, caught_exc)\n\n    expiry = _parse_expiry(response_data)\n\n    return access_token, expiry, response_data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nimplement the JWT Profile for OAuth 2. 0 Authorization Grants but requests an OpenID Connect ID Token instead of an access token.", "response": "def id_token_jwt_grant(request, token_uri, assertion):\n    \"\"\"Implements the JWT Profile for OAuth 2.0 Authorization Grants, but\n    requests an OpenID Connect ID Token instead of an access token.\n\n    This is a variant on the standard JWT Profile that is currently unique\n    to Google. This was added for the benefit of authenticating to services\n    that require ID Tokens instead of access tokens or JWT bearer tokens.\n\n    Args:\n        request (google.auth.transport.Request): A callable used to make\n            HTTP requests.\n        token_uri (str): The OAuth 2.0 authorization server's token endpoint\n            URI.\n        assertion (str): JWT token signed by a service account. The token's\n            payload must include a ``target_audience`` claim.\n\n    Returns:\n        Tuple[str, Optional[datetime], Mapping[str, str]]:\n            The (encoded) Open ID Connect ID Token, expiration, and additional\n            data returned by the endpoint.\n\n    Raises:\n        google.auth.exceptions.RefreshError: If the token endpoint returned\n            an error.\n    \"\"\"\n    body = {\n        'assertion': assertion,\n        'grant_type': _JWT_GRANT_TYPE,\n    }\n\n    response_data = _token_endpoint_request(request, token_uri, body)\n\n    try:\n        id_token = response_data['id_token']\n    except KeyError as caught_exc:\n        new_exc = exceptions.RefreshError(\n            'No ID token in response.', response_data)\n        six.raise_from(new_exc, caught_exc)\n\n    payload = jwt.decode(id_token, verify=False)\n    expiry = datetime.datetime.utcfromtimestamp(payload['exp'])\n\n    return id_token, expiry, response_data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef refresh_grant(request, token_uri, refresh_token, client_id, client_secret):\n    body = {\n        'grant_type': _REFRESH_GRANT_TYPE,\n        'client_id': client_id,\n        'client_secret': client_secret,\n        'refresh_token': refresh_token,\n    }\n\n    response_data = _token_endpoint_request(request, token_uri, body)\n\n    try:\n        access_token = response_data['access_token']\n    except KeyError as caught_exc:\n        new_exc = exceptions.RefreshError(\n            'No access token in response.', response_data)\n        six.raise_from(new_exc, caught_exc)\n\n    refresh_token = response_data.get('refresh_token', refresh_token)\n    expiry = _parse_expiry(response_data)\n\n    return access_token, refresh_token, expiry, response_data", "response": "Implements the OAuth 2. 0 refresh token grant."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the absolute path the Cloud SDK s configuration directory.", "response": "def get_config_path():\n    \"\"\"Returns the absolute path the the Cloud SDK's configuration directory.\n\n    Returns:\n        str: The Cloud SDK config path.\n    \"\"\"\n    # If the path is explicitly set, return that.\n    try:\n        return os.environ[environment_vars.CLOUD_SDK_CONFIG_DIR]\n    except KeyError:\n        pass\n\n    # Non-windows systems store this at ~/.config/gcloud\n    if os.name != 'nt':\n        return os.path.join(\n            os.path.expanduser('~'), '.config', _CONFIG_DIRECTORY)\n    # Windows systems store config at %APPDATA%\\gcloud\n    else:\n        try:\n            return os.path.join(\n                os.environ[_WINDOWS_CONFIG_ROOT_ENV_VAR],\n                _CONFIG_DIRECTORY)\n        except KeyError:\n            # This should never happen unless someone is really\n            # messing with things, but we'll cover the case anyway.\n            drive = os.environ.get('SystemDrive', 'C:')\n            return os.path.join(\n                drive, '\\\\', _CONFIG_DIRECTORY)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the project ID from the Cloud SDK.", "response": "def get_project_id():\n    \"\"\"Gets the project ID from the Cloud SDK.\n\n    Returns:\n        Optional[str]: The project ID.\n    \"\"\"\n    if os.name == 'nt':\n        command = _CLOUD_SDK_WINDOWS_COMMAND\n    else:\n        command = _CLOUD_SDK_POSIX_COMMAND\n\n    try:\n        output = subprocess.check_output(\n            (command,) + _CLOUD_SDK_CONFIG_COMMAND,\n            stderr=subprocess.STDOUT)\n    except (subprocess.CalledProcessError, OSError, IOError):\n        return None\n\n    try:\n        configuration = json.loads(output.decode('utf-8'))\n    except ValueError:\n        return None\n\n    try:\n        return configuration['configuration']['properties']['core']['project']\n    except KeyError:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving information about the service account.", "response": "def _retrieve_info(self, request):\n        \"\"\"Retrieve information about the service account.\n\n        Updates the scopes and retrieves the full service account email.\n\n        Args:\n            request (google.auth.transport.Request): The object used to make\n                HTTP requests.\n        \"\"\"\n        info = _metadata.get_service_account_info(\n            request,\n            service_account=self._service_account_email)\n\n        self._service_account_email = info['email']\n        self._scopes = info['scopes']"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef refresh(self, request):\n        try:\n            self._retrieve_info(request)\n            self.token, self.expiry = _metadata.get_service_account_token(\n                request,\n                service_account=self._service_account_email)\n        except exceptions.TransportError as caught_exc:\n            new_exc = exceptions.RefreshError(caught_exc)\n            six.raise_from(new_exc, caught_exc)", "response": "Refresh the access token and scopes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_authorized_user_info(cls, info, scopes=None):\n        keys_needed = set(('refresh_token', 'client_id', 'client_secret'))\n        missing = keys_needed.difference(six.iterkeys(info))\n\n        if missing:\n            raise ValueError(\n                'Authorized user info was not in the expected format, missing '\n                'fields {}.'.format(', '.join(missing)))\n\n        return Credentials(\n            None,  # No access token, must be refreshed.\n            refresh_token=info['refresh_token'],\n            token_uri=_GOOGLE_OAUTH2_TOKEN_ENDPOINT,\n            scopes=scopes,\n            client_id=info['client_id'],\n            client_secret=info['client_secret'])", "response": "Creates a Credentials instance from an authorized user info dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_authorized_user_file(cls, filename, scopes=None):\n        with io.open(filename, 'r', encoding='utf-8') as json_file:\n            data = json.load(json_file)\n            return cls.from_authorized_user_info(data, scopes)", "response": "Creates a Credentials instance from an authorized user json file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ping(request, timeout=_METADATA_DEFAULT_TIMEOUT, retry_count=3):\n    # NOTE: The explicit ``timeout`` is a workaround. The underlying\n    #       issue is that resolving an unknown host on some networks will take\n    #       20-30 seconds; making this timeout short fixes the issue, but\n    #       could lead to false negatives in the event that we are on GCE, but\n    #       the metadata resolution was particularly slow. The latter case is\n    #       \"unlikely\".\n    retries = 0\n    while retries < retry_count:\n        try:\n            response = request(\n                url=_METADATA_IP_ROOT, method='GET', headers=_METADATA_HEADERS,\n                timeout=timeout)\n\n            metadata_flavor = response.headers.get(_METADATA_FLAVOR_HEADER)\n            return (response.status == http_client.OK and\n                    metadata_flavor == _METADATA_FLAVOR_VALUE)\n\n        except exceptions.TransportError:\n            _LOGGER.info('Compute Engine Metadata server unavailable on'\n                         'attempt %s of %s', retries+1, retry_count)\n            retries += 1\n\n    return False", "response": "Checks to see if the metadata server is available."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfetch a resource from the metadata server.", "response": "def get(request, path, root=_METADATA_ROOT, recursive=False):\n    \"\"\"Fetch a resource from the metadata server.\n\n    Args:\n        request (google.auth.transport.Request): A callable used to make\n            HTTP requests.\n        path (str): The resource to retrieve. For example,\n            ``'instance/service-accounts/default'``.\n        root (str): The full path to the metadata server root.\n        recursive (bool): Whether to do a recursive query of metadata. See\n            https://cloud.google.com/compute/docs/metadata#aggcontents for more\n            details.\n\n    Returns:\n        Union[Mapping, str]: If the metadata server returns JSON, a mapping of\n            the decoded JSON is return. Otherwise, the response content is\n            returned as a string.\n\n    Raises:\n        google.auth.exceptions.TransportError: if an error occurred while\n            retrieving metadata.\n    \"\"\"\n    base_url = urlparse.urljoin(root, path)\n    query_params = {}\n\n    if recursive:\n        query_params['recursive'] = 'true'\n\n    url = _helpers.update_query(base_url, query_params)\n\n    response = request(url=url, method='GET', headers=_METADATA_HEADERS)\n\n    if response.status == http_client.OK:\n        content = _helpers.from_bytes(response.data)\n        if response.headers['content-type'] == 'application/json':\n            try:\n                return json.loads(content)\n            except ValueError as caught_exc:\n                new_exc = exceptions.TransportError(\n                    'Received invalid JSON from the Google Compute Engine'\n                    'metadata service: {:.20}'.format(content))\n                six.raise_from(new_exc, caught_exc)\n        else:\n            return content\n    else:\n        raise exceptions.TransportError(\n            'Failed to retrieve {} from the Google Compute Engine'\n            'metadata service. Status: {} Response:\\n{}'.format(\n                url, response.status, response.data), response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_service_account_token(request, service_account='default'):\n    token_json = get(\n        request,\n        'instance/service-accounts/{0}/token'.format(service_account))\n    token_expiry = _helpers.utcnow() + datetime.timedelta(\n        seconds=token_json['expires_in'])\n    return token_json['access_token'], token_expiry", "response": "Get the OAuth 2. 0 access token for a service account."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nvalidates a dictionary containing Google service account data and creates and returns a google. auth. crypt. Signer instance from the private key specified in the data.", "response": "def from_dict(data, require=None):\n    \"\"\"Validates a dictionary containing Google service account data.\n\n    Creates and returns a :class:`google.auth.crypt.Signer` instance from the\n    private key specified in the data.\n\n    Args:\n        data (Mapping[str, str]): The service account data\n        require (Sequence[str]): List of keys required to be present in the\n            info.\n\n    Returns:\n        google.auth.crypt.Signer: A signer created from the private key in the\n            service account file.\n\n    Raises:\n        ValueError: if the data was in the wrong format, or if one of the\n            required keys is missing.\n    \"\"\"\n    keys_needed = set(require if require is not None else [])\n\n    missing = keys_needed.difference(six.iterkeys(data))\n\n    if missing:\n        raise ValueError(\n            'Service account info was not in the expected format, missing '\n            'fields {}.'.format(', '.join(missing)))\n\n    # Create a signer.\n    signer = crypt.RSASigner.from_service_account_info(data)\n\n    return signer"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_filename(filename, require=None):\n    with io.open(filename, 'r', encoding='utf-8') as json_file:\n        data = json.load(json_file)\n        return data, from_dict(data, require=require)", "response": "Reads a Google service account. json file and returns its parsed info."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef copy_docstring(source_class):\n    def decorator(method):\n        \"\"\"Decorator implementation.\n\n        Args:\n            method (Callable): The method to copy the docstring to.\n\n        Returns:\n            Callable: the same method passed in with an updated docstring.\n\n        Raises:\n            ValueError: if the method already has a docstring.\n        \"\"\"\n        if method.__doc__:\n            raise ValueError('Method already has a docstring.')\n\n        source_method = getattr(source_class, method.__name__)\n        method.__doc__ = source_method.__doc__\n\n        return method\n    return decorator", "response": "A decorator that copies a method s docstring from another class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting a string value to bytes if necessary.", "response": "def to_bytes(value, encoding='utf-8'):\n    \"\"\"Converts a string value to bytes, if necessary.\n\n    Unfortunately, ``six.b`` is insufficient for this task since in\n    Python 2 because it does not modify ``unicode`` objects.\n\n    Args:\n        value (Union[str, bytes]): The value to be converted.\n        encoding (str): The encoding to use to convert unicode to bytes.\n            Defaults to \"utf-8\".\n\n    Returns:\n        bytes: The original value converted to bytes (if unicode) or as\n            passed in if it started out as bytes.\n\n    Raises:\n        ValueError: If the value could not be converted to bytes.\n    \"\"\"\n    result = (value.encode(encoding)\n              if isinstance(value, six.text_type) else value)\n    if isinstance(result, six.binary_type):\n        return result\n    else:\n        raise ValueError('{0!r} could not be converted to bytes'.format(value))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_bytes(value):\n    result = (value.decode('utf-8')\n              if isinstance(value, six.binary_type) else value)\n    if isinstance(result, six.text_type):\n        return result\n    else:\n        raise ValueError(\n            '{0!r} could not be converted to unicode'.format(value))", "response": "Converts a bytes value to a string value if necessary."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates a URL s query parameters.", "response": "def update_query(url, params, remove=None):\n    \"\"\"Updates a URL's query parameters.\n\n    Replaces any current values if they are already present in the URL.\n\n    Args:\n        url (str): The URL to update.\n        params (Mapping[str, str]): A mapping of query parameter\n            keys to values.\n        remove (Sequence[str]): Parameters to remove from the query string.\n\n    Returns:\n        str: The URL with updated query parameters.\n\n    Examples:\n\n        >>> url = 'http://example.com?a=1'\n        >>> update_query(url, {'a': '2'})\n        http://example.com?a=2\n        >>> update_query(url, {'b': '3'})\n        http://example.com?a=1&b=3\n        >> update_query(url, {'b': '3'}, remove=['a'])\n        http://example.com?b=3\n\n    \"\"\"\n    if remove is None:\n        remove = []\n\n    # Split the URL into parts.\n    parts = urllib.parse.urlparse(url)\n    # Parse the query string.\n    query_params = urllib.parse.parse_qs(parts.query)\n    # Update the query parameters with the new parameters.\n    query_params.update(params)\n    # Remove any values specified in remove.\n    query_params = {\n        key: value for key, value\n        in six.iteritems(query_params)\n        if key not in remove}\n    # Re-encoded the query string.\n    new_query = urllib.parse.urlencode(query_params, doseq=True)\n    # Unsplit the url.\n    new_parts = parts._replace(query=new_query)\n    return urllib.parse.urlunparse(new_parts)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef padded_urlsafe_b64decode(value):\n    b64string = to_bytes(value)\n    padded = b64string + b'=' * (-len(b64string) % 4)\n    return base64.urlsafe_b64decode(padded)", "response": "Decodes base64 strings lacking padding characters."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef configure_processes(agent_metadata_map, logger):\n\n    if not optional_packages_installed:\n        logger.warning(\"\\n#### WARNING ####\\n\"\n                       \"You are missing some optional packages which will become mandatory in the future!\\n\"\n                       \"Please run `pip install -r requirements.txt` to enjoy optimal functionality \"\n                       \"and future-proof yourself!\\n\")\n\n    if not optional_packages_installed:\n        return\n\n    team_pids_map = {}\n\n    for player_index, data in agent_metadata_map.items():\n        team = data.team\n        if team not in team_pids_map:\n            team_pids_map[team] = set()\n        team_pids_map[team].update(data.pids)\n\n    shared_pids = set()\n    cpu_count = psutil.cpu_count()\n    cpus_per_team = cpu_count // 3\n\n    if len(team_pids_map) >= 2 and cpus_per_team > 0:\n        # Sort into three sets of pids: team 0 exclusives, team 1 exclusives, and shared pids\n        # All pids will be assigned high priority\n        # Team exclusive pids will be bound to a subset of cpus so they can't adversely affect the opposite team.\n\n        for team, team_set in team_pids_map.items():\n            if not shared_pids:\n                shared_pids.update(team_set)\n            else:\n                shared_pids.intersection_update(team_set)\n\n        for team, team_set in team_pids_map.items():\n            team_set -= shared_pids\n\n        for team, team_pids in team_pids_map.items():\n            team_cpu_offset = cpus_per_team * team\n            team_cpus = list(range(cpu_count - cpus_per_team - team_cpu_offset, cpu_count - team_cpu_offset))\n            for pid in team_pids:\n                p = psutil.Process(pid)\n                p.cpu_affinity(team_cpus)  # Restrict the process to run on the cpus assigned to the team\n                p.nice(psutil.HIGH_PRIORITY_CLASS)  # Allow the process to run at high priority\n    else:\n        # Consider everything a shared pid, because we are not in a position to split up cpus.\n        for team, team_set in team_pids_map.items():\n            shared_pids.update(team_set)\n\n    for pid in shared_pids:\n        p = psutil.Process(pid)  # Allow the process to run at high priority\n        p.nice(psutil.HIGH_PRIORITY_CLASS)", "response": "Configure processes for the given agent metadata map."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_header_name(self, header_name, is_indexed=False):\n        header = ConfigHeader()\n        header.is_indexed = is_indexed\n        self.headers[header_name] = header\n        return header", "response": "Adds a new header with the name header_name"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_header(self, header_name):\n        if header_name in self.headers:\n            return self.headers[header_name]\n        return self.add_header_name(header_name)", "response": "Returns a header with that name creates it if it does not exist."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_file(self, config: Union[Path, str, RawConfigParser, 'ConfigObject'], max_index=None, config_directory=None):\n        self.config_directory = config_directory\n\n        if isinstance(config, Path):\n            config = str(config)\n\n        if isinstance(config, str):\n            if not os.path.isfile(config):\n                raise FileNotFoundError(config)\n            self.raw_config_parser = RawConfigParser()\n            self.raw_config_parser.read(config, encoding='utf8')\n            config = self.raw_config_parser\n        elif isinstance(config, RawConfigParser):\n            self.raw_config_parser = config\n        elif not isinstance(config, ConfigObject):\n            raise TypeError(\"The config should be a String, RawConfigParser of a ConfigObject\")\n        for header_name, header in self.headers.items():\n            try:\n                header.parse_file(config[header_name], max_index=max_index, config_directory=config_directory)\n            except KeyError:\n                pass  # skip this header as it does not exist\n        return self", "response": "Parses the file and sets the values of the current instance of the class attributes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a new value to the config file.", "response": "def add_value(self, name, value_type, default=None, description=None, value=None):\n        \"\"\"\n        Adds a new value to this config header\n        :param name: The name of the value as it would appear in a config file\n        :param value_type:  The type of value: bool, str, int, float\n        :param default: The value used when the config does not set any value.\n        :param description: The human readable description of the value\n        :param value: An optional value, if this header is indexed then the value needs to be a list.\n        :return: an instance of itself so that you can chain adding values together.\n        \"\"\"\n        if description is None:\n            description = name\n        if value is not None and self.is_indexed and not isinstance(value, list):\n            raise Exception('Indexed values must be a list')\n        self.values[name] = ConfigValue(value_type, default=default, description=description, value=value)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the value of the given option.", "response": "def set_value(self, option, value, index=None):\n        \"\"\"\n        Sets the value on the given option.\n        :param option: The name of the option as it appears in the config file\n        :param value: The value that is being applied. If this section is indexed then the\n        value must be a list (to be applied directly) or you must supply the index parameter,\n        which will cause the value to be inserted into an existing list.\n        :param index: If the attribute is indexed, we will use this index to insert\n        the value you have supplied.\n        :return: an instance of itself so that you can chain setting values together.\n        \"\"\"\n        if self.is_indexed and index is None and not isinstance(value, list):\n            raise TypeError(\"Value should be a list when not giving an index in an indexed header\")\n        self.values[option].set_value(value=value, index=index)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the value of the attribute.", "response": "def get_value(self, index=None):\n        \"\"\"\n        Returns the default if value is none.\n        :param index:\n        :return: A value.\n        \"\"\"\n\n        if self.value is None:\n            return self.default\n        if index is not None:\n            if self.value[index] is None:\n                return self.default\n            else:\n                value = self.value[index]\n        else:\n            value = self.value\n\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef log_warn(message, args):\n    get_logger(DEFAULT_LOGGER, log_creation=False).log(logging.WARNING, message, *args)", "response": "Logs a warning message using the default logger."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates the config_headers_to_widgets and config_widgets_to_headers dicts for the current locale.", "response": "def create_config_headers_dicts(self):\n        \"\"\"\n        Creates the config_headers_to_widgets and config_widgets_to_headers and config_headers_to_categories dicts\n        \"\"\"\n        self.config_headers_to_widgets = {\n            # blue stuff\n            'Bot Loadout': {\n                'team_color_id': (self.blue_primary_spinbox,),\n                'custom_color_id': (self.blue_secondary_spinbox,),\n                'car_id': (self.blue_car_spinbox, self.blue_car_combobox),\n                'decal_id': (self.blue_decal_spinbox, self.blue_decal_combobox),\n                'wheels_id': (self.blue_wheels_spinbox, self.blue_wheels_combobox),\n                'boost_id': (self.blue_boost_spinbox, self.blue_boost_combobox),\n                'antenna_id': (self.blue_antenna_spinbox, self.blue_antenna_combobox),\n                'hat_id': (self.blue_hat_spinbox, self.blue_hat_combobox),\n                'paint_finish_id': (self.blue_paint_finish_spinbox, self.blue_paint_finish_combobox),\n                'custom_finish_id': (self.blue_custom_finish_spinbox, self.blue_custom_finish_combobox),\n                'engine_audio_id': (self.blue_engine_spinbox, self.blue_engine_combobox),\n                'trails_id': (self.blue_trails_spinbox, self.blue_trails_combobox),\n                'goal_explosion_id': (self.blue_goal_explosion_spinbox, self.blue_goal_explosion_combobox)\n            },\n            'Bot Loadout Orange': {\n                'team_color_id': (self.orange_primary_spinbox,),\n                'custom_color_id': (self.orange_secondary_spinbox,),\n                'car_id': (self.orange_car_spinbox, self.orange_car_combobox),\n                'decal_id': (self.orange_decal_spinbox, self.orange_decal_combobox),\n                'wheels_id': (self.orange_wheels_spinbox, self.orange_wheels_combobox),\n                'boost_id': (self.orange_boost_spinbox, self.orange_boost_combobox),\n                'antenna_id': (self.orange_antenna_spinbox, self.orange_antenna_combobox),\n                'hat_id': (self.orange_hat_spinbox, self.orange_hat_combobox),\n                'paint_finish_id': (self.orange_paint_finish_spinbox, self.orange_paint_finish_combobox),\n                'custom_finish_id': (self.orange_custom_finish_spinbox, self.orange_custom_finish_combobox),\n                'engine_audio_id': (self.orange_engine_spinbox, self.orange_engine_combobox),\n                'trails_id': (self.orange_trails_spinbox, self.orange_trails_combobox),\n                'goal_explosion_id': (self.orange_goal_explosion_spinbox, self.orange_goal_explosion_combobox)\n            },\n        }\n        self.config_widgets_to_headers = {}\n        for header_1, _field_dict in self.config_headers_to_widgets.items():\n            for header_2, _widgets in _field_dict.items():\n                for _widget in _widgets:\n                    self.config_widgets_to_headers[_widget] = (header_1, header_2)\n\n        self.config_headers_to_categories = {\n            'car_id': 'Body',\n            'decal_id': 'Decal',\n            'wheels_id': 'Wheels',\n            'boost_id': 'Rocket Boost',\n            'antenna_id': 'Antenna',\n            'hat_id': 'Topper',\n            'paint_finish_id': 'Paint Finish',\n            'custom_finish_id': 'Paint Finish',\n            'engine_audio_id': 'Engine Audio',\n            'trails_id': 'Trail',\n            'goal_explosion_id': 'Goal Explosion'\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_item_dicts():\n\n        json_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'rocket_league_items.json')\n        with open(json_path, 'r', encoding='utf8') as f:\n            sorted_items = json.load(f, parse_int=True).items()\n\n        longlabel_to_id = {}\n        id_to_longlabel = {}\n        for slot, items in sorted_items:\n            type_longlabel_to_id = {item['LongLabel']: int(item['ID']) for item in items}\n            type_id_to_longlabel = {int(item['ID']): item['LongLabel'] for item in items}\n            longlabel_to_id[slot] = type_longlabel_to_id\n            id_to_longlabel[slot] = type_id_to_longlabel\n\n        return longlabel_to_id, id_to_longlabel", "response": "Creates two item dicts and returns them"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the path of the rlbot package directory", "response": "def get_rlbot_directory() -> str:\n    \"\"\"Gets the path of the rlbot package directory\"\"\"\n    return os.path.dirname(os.path.dirname(os.path.realpath(__file__)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef contains_locked_file(directory: str):\n    for root, subdirs, files in os.walk(directory):\n        for filename in files:\n            file_path = os.path.join(root, filename)\n            try:\n                with open(file_path, 'a'):\n                    pass\n            except IOError:\n                logger.debug(f\"Locked file: {file_path}\")\n                return True\n    return False", "response": "Checks if any of the files in the directory are in use."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_match_config_from_file(match_config_path: Path) -> MatchConfig:\n    config_obj = create_bot_config_layout()\n    config_obj.parse_file(match_config_path, max_index=MAX_PLAYERS)\n    return parse_match_config(config_obj, match_config_path, {}, {})", "response": "Read the rlbot. cfg file on disk into a python datastructure."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading the bot config data for a single bot.", "response": "def _load_bot_config(index, config_bundle: BotConfigBundle,\n                    looks_config_object: ConfigObject, overall_config: ConfigObject,\n                    human_index_tracker: IncrementingInteger) -> PlayerConfig:\n    \"\"\"\n    Loads the config data of a single bot\n    :param index: This is the bot index (where it appears in game_cars)\n    :param bot_configuration: A config object that will eventually be transformed and sent to the game.\n    :param config_bundle: A config object for a single bot\n    :param overall_config: This is the config for the entire session not one particular bot\n    :param human_index_tracker: An object of type HumanIndexManager that helps set human_index correctly.\n    :return:\n    \"\"\"\n\n    bot_configuration = PlayerConfig()\n    bot_configuration.config_path = config_bundle.config_path\n\n    team_num = get_team(overall_config, index)\n\n    bot_configuration.team = team_num\n\n    # Setting up data about what type of bot it is\n    bot_type = overall_config.get(PARTICIPANT_CONFIGURATION_HEADER, PARTICIPANT_TYPE_KEY, index)\n    bot_configuration.bot, bot_configuration.rlbot_controlled = get_bot_options(bot_type)\n    bot_configuration.bot_skill = overall_config.getfloat(\n        PARTICIPANT_CONFIGURATION_HEADER, PARTICIPANT_BOT_SKILL_KEY, index)\n\n    if not bot_configuration.bot:\n        bot_configuration.human_index = human_index_tracker.increment()\n\n    # Setting up the bots name\n    bot_configuration.name = config_bundle.name\n\n    loadout_config = load_bot_appearance(looks_config_object, team_num)\n    bot_configuration.loadout_config = loadout_config\n\n    return bot_configuration"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nvalidates bot config file.", "response": "def validate_bot_config(config_bundle) -> None:\n    \"\"\"\n    Checks the config bundle to see whether it has all required attributes.\n    \"\"\"\n    if not config_bundle.name:\n        bot_config = os.path.join(config_bundle.config_directory, config_bundle.config_file_name or '')\n        raise AttributeError(f\"Bot config {bot_config} has no name configured!\")\n\n    # This will raise an exception if we can't find the looks config, or if it's malformed\n    config_bundle.get_looks_config()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets all the config files or config objects.", "response": "def get_bot_config_bundles(num_participants, config: ConfigObject, config_location, config_bundle_overrides):\n    \"\"\"\n    Adds all the config files or config objects.\n    :param num_participants:\n    :param config:\n    :param config_location: The location of the rlbot.cfg file\n    :param config_bundle_overrides: These are configs that have been loaded from the gui, they get assigned a bot index.\n    :return:\n    \"\"\"\n    config_bundles = []\n    for i in range(num_participants):\n        if i in config_bundle_overrides:\n            config_bundles.append(config_bundle_overrides[i])\n            logger.debug(\"Config available\")\n        else:\n            bot_config_relative_path = config.get(PARTICIPANT_CONFIGURATION_HEADER, PARTICIPANT_CONFIG_KEY, i)\n            bot_config_path = os.path.join(os.path.dirname(config_location), bot_config_relative_path)\n            config_bundles.append(get_bot_config_bundle(bot_config_path))\n            logger.debug(\"Reading raw config\")\n\n    return config_bundles"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_extra_pids(self):\n        while not self.is_retired:\n            for proc in psutil.process_iter():\n                for conn in proc.connections():\n                    if conn.laddr.port == self.port:\n                        self.logger.debug(f\".NET socket server for {self.name} appears to have pid {proc.pid}\")\n                        return [proc.pid]\n            if self.is_executable_configured():\n                return []\n            time.sleep(1)\n            if self.dotnet_executable_path is None:\n                self.logger.info(\"Can't auto-start .NET executable because no executable is configured. \"\n                                 \"Please start the .NET bot manually!\")\n            else:\n                self.logger.info(f\"Can't auto-start .NET executable because {self.dotnet_executable_path} \"\n                                 \"is not found. Please start the .NET bot manually!\")", "response": "Gets the list of process ids that should be used by this bot in addition to the ones inside the python process."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrun a bot helper process.", "response": "def run_helper_process(python_file, metadata_queue, quit_event, options):\n    \"\"\"\n    :param python_file: The absolute path of a python file containing the helper process that should be run.\n    It must define a class which is a subclass of BotHelperProcess.\n    :param metadata_queue: A queue from which the helper process will read AgentMetadata updates.\n    :param quit_event: An event which should be set when rlbot is shutting down.\n    :param options: A dict with arbitrary options that will be passed through to the helper process.\n    \"\"\"\n    class_wrapper = import_class_with_base(python_file, BotHelperProcess)\n    helper_class = class_wrapper.get_loaded_class()\n    helper = helper_class(metadata_queue, quit_event, options)\n    helper.start()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstarts or update the helper process.", "response": "def start_or_update_helper_process(self, agent_metadata: AgentMetadata):\n        \"\"\"\n        Examines the agent metadata to see if the agent needs a helper process. If the process is not running yet,\n        create the process. Once the process is running, feed the agent metadata to it.\n\n        If a process is created here, the pid will be added to the agent metadata.\n        \"\"\"\n\n        helper_req = agent_metadata.helper_process_request\n\n        if helper_req is not None:\n            if helper_req.key not in self.helper_process_map:\n                metadata_queue = mp.Queue()\n                if helper_req.python_file_path is not None:\n                    process = mp.Process(target=run_helper_process,\n                                         args=(helper_req.python_file_path, metadata_queue, self.quit_event,\n                                               helper_req.options))\n                    process.daemon = True\n                    process.start()\n                    agent_metadata.pids.add(process.pid)\n\n                    self.helper_process_map[helper_req.key] = metadata_queue\n\n                if helper_req.executable is not None:\n                    # TODO: find a nice way to pass the options dict as arguments\n                    process = subprocess.Popen([helper_req.executable])\n\n                    agent_metadata.pids.add(process.pid)\n\n                    self.helper_process_map[helper_req.key] = metadata_queue\n\n            metadata_queue = self.helper_process_map[helper_req.key]\n            metadata_queue.put(agent_metadata)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a sanitized version of the name that is being used for the bot.", "response": "def get_sanitized_bot_name(dict: Dict[str, int], name: str) -> str:\n    \"\"\"\n    Cut off at 31 characters and handle duplicates.\n    :param dict: Holds the list of names for duplicates\n    :param name: The name that is being sanitized\n    :return: A sanitized version of the name\n    \"\"\"\n    if name not in dict:\n        new_name = name[:31]  # Make sure name does not exceed 31 characters\n        dict[name] = 1\n    else:\n        count = dict[name]\n        new_name = name[:27] + \"(\" + str(count + 1) + \")\"  # Truncate at 27 because we can have up to '(10)' appended\n        assert new_name not in dict  # TODO: Fix collision between [\"foo\", \"foo\", \"foo(1)\"]\n        dict[name] = count + 1\n\n    return new_name"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef bot_config(player_config_path: Path, team: Team) -> 'PlayerConfig':\n        bot_config = PlayerConfig()\n        bot_config.bot = True\n        bot_config.rlbot_controlled = True\n        bot_config.team = team.value\n        bot_config.config_path = str(player_config_path.absolute()) # TODO: Refactor to use Path's\n        config_bundle = get_bot_config_bundle(bot_config.config_path)\n        bot_config.name = config_bundle.name\n        bot_config.loadout_config = load_bot_appearance(config_bundle.get_looks_config(), bot_config.team)\n        return bot_config", "response": "Create a PlayerConfig object for a bot."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setup_manager_context():\n    setup_manager = SetupManager()\n    setup_manager.connect_to_game()\n    try:\n        yield setup_manager\n    finally:\n        setup_manager.shut_down(kill_all_pids=True)", "response": "Context manager which starts up the context manager."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads the bot parameters from the given bundle.", "response": "def load_bot_parameters(config_bundle) -> ConfigObject:\n    \"\"\"\n    Initializes the agent in the bundle's python file and asks it to provide its\n    custom configuration object where its parameters can be set.\n    :return: the parameters as a ConfigObject\n    \"\"\"\n    # Python file relative to the config location.\n    python_file = config_bundle.python_file\n    agent_class_wrapper = import_agent(python_file)\n    bot_parameters = agent_class_wrapper.get_loaded_class().base_create_agent_configurations()\n    bot_parameters.parse_file(config_bundle.config_obj, config_directory=config_bundle.config_directory)\n    return bot_parameters"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_match_config(self, match_config: MatchConfig, bot_config_overrides={}):\n        self.num_participants = match_config.num_players\n        self.names = [bot.name for bot in match_config.player_configs]\n        self.teams = [bot.team for bot in match_config.player_configs]\n\n        bundles = [bot_config_overrides[index] if index in bot_config_overrides else\n                   get_bot_config_bundle(bot.config_path) if bot.config_path else None\n                   for index, bot in enumerate(match_config.player_configs)]\n\n        self.python_files = [bundle.python_file if bundle else None\n                             for bundle in bundles]\n\n        self.parameters = []\n\n        for index, bot in enumerate(match_config.player_configs):\n            python_config = None\n            if bot.rlbot_controlled:\n                python_config = load_bot_parameters(bundles[index])\n            self.parameters.append(python_config)\n            if bot.loadout_config is None and bundles[index]:\n                looks_config = bundles[index].get_looks_config()\n                bot.loadout_config = load_bot_appearance(looks_config, bot.team)\n\n        if match_config.extension_config is not None and match_config.extension_config.python_file_path is not None:\n            self.load_extension(match_config.extension_config.python_file_path)\n\n        self.match_config = match_config\n        self.start_match_configuration = match_config.create_match_settings()\n        self.game_interface.start_match_configuration = self.start_match_configuration", "response": "Loads the match config into internal data structures which prepares us to later launch bot processes and start the match."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_config(self, framework_config: ConfigObject = None, config_location=DEFAULT_RLBOT_CONFIG_LOCATION,\n                    bot_configs=None,\n                    looks_configs=None):\n        \"\"\"\n        Loads the configuration into internal data structures, which prepares us to later\n        launch bot processes and start the match.\n\n        :param framework_config: A config object that indicates what bots to run. May come from parsing a rlbot.cfg.\n        :param config_location: The location of the rlbot.cfg file, which will be used to resolve relative paths.\n        :param bot_configs: Overrides for bot configurations.\n        :param looks_configs: Overrides for looks configurations.\n        \"\"\"\n        self.logger.debug('reading the configs')\n\n        # Set up RLBot.cfg\n        if framework_config is None:\n            framework_config = create_bot_config_layout()\n            framework_config.parse_file(config_location, max_index=MAX_PLAYERS)\n        if bot_configs is None:\n            bot_configs = {}\n        if looks_configs is None:\n            looks_configs = {}\n\n        match_config = parse_match_config(framework_config, config_location, bot_configs, looks_configs)\n        self.load_match_config(match_config, bot_configs)", "response": "Loads the configuration into the internal data structures."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntry to recieve AgentMetadata from the queue and update the helper process manager.", "response": "def try_recieve_agent_metadata(self):\n        \"\"\"\n        Checks whether any of the started bots have posted their AgentMetadata\n        yet. If so, we put them on the agent_metadata_map such that we can\n        kill their process later when we shut_down(kill_agent_process_ids=True)\n\n        Returns how from how many bots we recieved metadata from.\n        \"\"\"\n        num_recieved = 0\n        while True:  # will exit on queue.Empty\n            try:\n                single_agent_metadata = self.agent_metadata_queue.get(timeout=0.1)\n                num_recieved += 1\n                self.helper_process_manager.start_or_update_helper_process(single_agent_metadata)\n                self.agent_metadata_map[single_agent_metadata.index] = single_agent_metadata\n                process_configuration.configure_processes(self.agent_metadata_map, self.logger)\n            except queue.Empty:\n                return num_recieved\n            except Exception as ex:\n                self.logger.error(ex)\n                return num_recieved\n        return num_recieved"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_external_class(python_file, base_class):\n    loaded_module = load_external_module(python_file)\n\n    # Find a class that extends base_class\n    loaded_class = extract_class(loaded_module, base_class)\n    return loaded_class, loaded_module", "response": "Loads a class from a Python file and returns it as a tuple."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_external_module(python_file):\n\n    # There's a special case where python_file may be pointing at the base agent definition here in the framework.\n    # This is sometimes done as a default and we want to allow it. Short-circuit the logic because\n    # loading it as if it's an external class is a real mess.\n    if os.path.abspath(python_file) == os.path.abspath(inspect.getfile(BaseAgent)):\n        return BaseAgent, BaseAgent.__module__\n\n    if not os.path.isfile(python_file):\n        raise FileNotFoundError(f\"Could not find file {python_file}!\")\n\n    dir_name = os.path.dirname(python_file)\n    module_name = os.path.splitext(os.path.basename(python_file))[0]\n    keys_before = set(sys.modules.keys())\n\n    # Temporarily modify the sys.path while we load the module so that the module can use import statements naturally\n    sys.path.insert(0, dir_name)\n    loaded_module = importlib.import_module(module_name)\n\n    # Clean up the changes to sys.path and sys.modules to avoid collisions with other external classes and to\n    # prepare for the next reload.\n    added = set(sys.modules.keys()).difference(keys_before)\n    del sys.path[0]\n    for key in added:\n        del sys.modules[key]\n\n    return loaded_module", "response": "Loads the external module."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_extra_pids(self):\n        while not self.is_retired:\n            for proc in psutil.process_iter():\n                for conn in proc.connections():\n                    if conn.laddr.port == self.port:\n                        self.logger.debug(f'py4j server for {self.name} appears to have pid {proc.pid}')\n                        return [proc.pid]\n            if self.is_executable_configured():\n                # The helper process will start java and report the PID. Nothing to do here.\n                return []\n            time.sleep(1)\n            if self.java_executable_path is None:\n                self.logger.info(\n                    \"Can't auto-start java because no executable is configured. Please start java manually!\")\n            else:\n                self.logger.info(f\"Can't auto-start java because {self.java_executable_path} is not found. \"\n                                 \"Please start java manually!\")", "response": "Gets the list of process ids that should be used by this bot in addition to the ones inside the python process."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the overall index agent config loadout config in that order", "response": "def get_configs(self):\n        \"\"\"\n        :return: overall index, agent config, loadout config in that order\n        \"\"\"\n        loadout_config = self.loadout_preset.config.copy()\n\n        config_path = None\n        if self.agent_preset.config_path is not None:  # Might be none if preset was never saved to disk.\n            config_path = os.path.dirname(self.agent_preset.config_path)\n        config = self.agent_preset.config.copy()\n        config.set_value(BOT_CONFIG_MODULE_HEADER, BOT_NAME_KEY, self.ingame_name)\n        config_bundle = BotConfigBundle(config_path, config, os.path.basename(self.agent_preset.config_path))\n\n        return self.overall_index, config_bundle, loadout_config"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef send_quick_chat_from_agent(self, team_only, quick_chat):\n\n        # Send the quick chat to the game\n        rlbot_status = send_quick_chat_flat(self.game_interface, self.index, self.team, team_only, quick_chat)\n\n        if rlbot_status == RLBotCoreStatus.QuickChatRateExceeded:\n            self.logger.debug('quick chat disabled')\n        else:\n            # Make the quick chat visible to other python bots. Unfortunately other languages can't see it.\n            send_quick_chat(self.quick_chat_queue_holder, self.index, self.team, team_only, quick_chat)", "response": "Send a quick chat from the agents."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_agent(self):\n        agent_class = self.agent_class_wrapper.get_loaded_class()\n        agent = agent_class(self.name, self.team, self.index)\n        agent.init_match_config(self.match_config)\n\n        agent.load_config(self.bot_configuration.get_header(\"Bot Parameters\"))\n\n        self.update_metadata_queue(agent)\n        self.set_render_manager(agent)\n\n        agent_class_file = self.agent_class_wrapper.python_file\n        agent._register_quick_chat(self.send_quick_chat_from_agent)\n        agent._register_field_info(self.get_field_info)\n        agent._register_set_game_state(self.set_game_state)\n        agent._register_ball_prediction(self.get_ball_prediction)\n        agent._register_ball_prediction_struct(self.get_ball_prediction_struct)\n        agent._register_get_rigid_body_tick(self.get_rigid_body_tick)\n        if self.quick_chat_quit_event:\n            self.quick_chat_quit_event.set()\n        self.quick_chat_quit_event = mp.Event()\n        register_for_quick_chat(self.quick_chat_queue_holder, agent.handle_quick_chat, self.quick_chat_quit_event)\n        while not self.is_valid_field_info():\n            time.sleep(0.1)\n\n        # Once all engine setup is done, do the agent-specific initialization, if any:\n        agent.initialize_agent()\n        return agent, agent_class_file", "response": "Loads and initializes an agent using instance variables registers for quick chat and sets render functions."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_render_manager(self, agent: BaseAgent):\n        rendering_manager = self.game_interface.renderer.get_rendering_manager(self.index, self.team)\n        agent._set_renderer(rendering_manager)", "response": "Sets the render manager for the agent."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update_metadata_queue(self, agent: BaseAgent):\n        pids = {os.getpid(), *agent.get_extra_pids()}\n\n        helper_process_request = agent.get_helper_process_request()\n\n        self.agent_metadata_queue.put(AgentMetadata(self.index, self.name, self.team, pids, helper_process_request))", "response": "Adds a new instance of AgentMetadata into the agent_metadata_queue using agent data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreloading the agent. Can throw exceptions.", "response": "def reload_agent(self, agent: BaseAgent, agent_class_file):\n        \"\"\"\n        Reloads the agent. Can throw exceptions. External classes should use reload_event.set() instead.\n        :param agent: An instance of an agent.\n        :param agent_class_file: The agent's class file. TODO: Remove this argument, it only affects logging and may be misleading.\n        :return: The reloaded instance of the agent, and the agent class file.\n        \"\"\"\n        self.logger.debug('Reloading Agent: ' + agent_class_file)\n        self.agent_class_wrapper.reload()\n        old_agent = agent\n        agent, agent_class_file = self.load_agent()\n\n        # Retire after the replacement initialized properly.\n        if hasattr(old_agent, 'retire'):\n            old_agent.retire()\n\n        return agent, agent_class_file"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrun the Agent and returns the result.", "response": "def run(self):\n        \"\"\"\n        Loads interface for RLBot, prepares environment and agent, and calls the update for the agent.\n        \"\"\"\n        self.logger.debug('initializing agent')\n        self.game_interface.load_interface()\n\n        self.prepare_for_run()\n\n        # Create Ratelimiter\n        rate_limit = rate_limiter.RateLimiter(GAME_TICK_PACKET_POLLS_PER_SECOND)\n        last_tick_game_time = None  # What the tick time of the last observed tick was\n        last_call_real_time = datetime.now()  # When we last called the Agent\n\n        # Get bot module\n        agent, agent_class_file = self.load_agent()\n\n        last_module_modification_time = os.stat(agent_class_file).st_mtime\n\n        # Run until main process tells to stop, or we detect Ctrl+C\n        try:\n            while not self.terminate_request_event.is_set():\n                self.pull_data_from_game()\n                # game_tick_packet = self.game_interface.get\n                # Read from game data shared memory\n\n                # Run the Agent only if the game_info has updated.\n                tick_game_time = self.get_game_time()\n                should_call_while_paused = datetime.now() - last_call_real_time >= MAX_AGENT_CALL_PERIOD\n                if tick_game_time != last_tick_game_time or should_call_while_paused:\n                    last_tick_game_time = tick_game_time\n                    last_call_real_time = datetime.now()\n\n                    # Reload the Agent if it has been modified or if reload is requested from outside.\n                    try:\n                        new_module_modification_time = os.stat(agent_class_file).st_mtime\n                        if new_module_modification_time != last_module_modification_time or self.reload_request_event.is_set():\n                            self.reload_request_event.clear()\n                            last_module_modification_time = new_module_modification_time\n                            # Clear the render queue on reload.\n                            if hasattr(agent, 'renderer') and isinstance(agent.renderer, RenderingManager):\n                                agent.renderer.clear_all_touched_render_groups()\n                            agent, agent_class_file = self.reload_agent(agent, agent_class_file)\n                    except FileNotFoundError:\n                        self.logger.error(f\"Agent file {agent_class_file} was not found. Will try again.\")\n                        time.sleep(0.5)\n                    except Exception:\n                        self.logger.error(\"Reloading the agent failed:\\n\" + traceback.format_exc())\n                        time.sleep(0.5)  # Avoid burning CPU / logs if this starts happening constantly\n\n                    # Call agent\n                    try:\n                        self.call_agent(agent, self.agent_class_wrapper.get_loaded_class())\n                    except Exception as e:\n                        self.logger.error(\"Call to agent failed:\\n\" + traceback.format_exc())\n\n                # Ratelimit here\n                rate_limit.acquire()\n        except KeyboardInterrupt:\n            self.terminate_request_event.set()\n\n        # Shut down the bot by calling cleanup functions.\n        if hasattr(agent, 'retire'):\n            try:\n                agent.retire()\n            except Exception as e:\n                self.logger.error(\"Retiring the agent failed:\\n\" + traceback.format_exc())\n        if hasattr(agent, 'renderer') and isinstance(agent.renderer, RenderingManager):\n            agent.renderer.clear_all_touched_render_groups()\n        # Zero out the inputs, so it's more obvious that the bot has stopped.\n        self.game_interface.update_player_input(PlayerInput(), self.index)\n        self.quick_chat_quit_event.set()  # Shut down quick chat.\n\n        # If terminated, send callback\n        self.termination_complete_event.set()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef team_color(self, team=None, alt_color=False):\n        if team is None:\n            team = self.bot_team\n\n        if team == 0:\n            return self.cyan() if alt_color else self.blue()\n        elif team == 1:\n            return self.red() if alt_color else self.orange()\n        else:\n            return self.gray() if alt_color else self.white()", "response": "Returns the color of the team associated with the bot."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef __create_vector(self, *vec) -> Vector3:\n        import numbers\n\n        if len(vec) == 1:\n            if hasattr(vec[0], \"__getitem__\"):  # Support all subscriptable types.\n                try:\n                    x = float(vec[0][0])\n                    y = float(vec[0][1])\n                    try:\n                        z = float(vec[0][2])\n                    except (ValueError, IndexError):\n                        z = 0\n                except ValueError:\n                    raise ValueError(f\"Unexpected type(s) for creating vector: {type(vec[0][0])}, {type(vec[0][1])}\")\n                except IndexError:\n                    raise IndexError(f\"Unexpected IndexError when creating vector from type: {type(vec[0])}\")\n            elif isinstance(vec[0], Vector3.Vector3):\n                x = vec[0].X()\n                y = vec[0].Y()\n                z = vec[0].Z()\n            elif isinstance(vec[0], GameDataStructVector3):\n                x = vec[0].x\n                y = vec[0].y\n                z = vec[0].z\n            else:\n                raise ValueError(f\"Unexpected type for creating vector: {type(vec[0])}\")\n        elif len(vec) == 2 or len(vec) == 3:\n            if isinstance(vec[0], numbers.Number) and isinstance(vec[1], numbers.Number):\n                x = vec[0]\n                y = vec[1]\n                if len(vec) == 2:\n                    z = 0\n                else:\n                    if isinstance(vec[2], numbers.Number):\n                        z = vec[2]\n                    else:\n                        raise ValueError(f\"Unexpected type for creating vector: {type(vec[0])}\")\n            else:\n                raise ValueError(f\"Unexpected type(s) for creating vector: {type(vec[0])}, {type(vec[1])}\")\n        else:\n            raise ValueError(\"Unexpected number of arguments for creating vector\")\n\n        return Vector3.CreateVector3(self.builder, x, y, z)", "response": "Converts a variety of vector types to a flatbuffer Vector3."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nswitching the team for the agent to the other team for the drop event", "response": "def bot_item_drop_event(self, dropped_listwidget, event):\n        \"\"\"\n        Switches the team for the dropped agent to the other team\n        :param dropped_listwidget: The listwidget belonging to the new team for the agent\n        :param event: The QDropEvent containing the source\n        :return:\n        \"\"\"\n        dragged_listwidget = event.source()\n        if dragged_listwidget is dropped_listwidget:  # drops in the same widget\n            return\n        self.current_bot.set_team(0 if dropped_listwidget == self.blue_listwidget else 1)\n        self.update_teams_listwidgets()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fixed_indices(self):\n        config = self.overall_config.copy()\n\n        used_indices = sorted(self.index_manager.numbers)\n        not_used_indices = [e for e in range(MAX_PLAYERS) if e not in used_indices]\n        order = used_indices + not_used_indices\n        header = config[PARTICIPANT_CONFIGURATION_HEADER]\n        for name, config_value in header.values.items():\n            old_values = list(config_value.value)\n            for i in range(MAX_PLAYERS):\n                config_value.set_value(old_values[order[i]], index=i)\n        return config", "response": "Return a new CustomConfig instance with the fixed indices of the overall config."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstart a match with the current configuration.", "response": "def start_match(self):\n        \"\"\"\n        Starts a match with the current configuration\n        :return:\n        \"\"\"\n\n        agent_configs_dict = {}\n        loadout_configs_dict = {}\n        for agent in self.agents:\n            i, agent_config, loadout_config = agent.get_configs()\n            agent_configs_dict[i] = agent_config\n            loadout_configs_dict[i] = loadout_config\n        agent_configs = {}\n        loadout_configs = {}\n        index = 0\n        for i in range(MAX_PLAYERS):\n            if i in agent_configs_dict:\n                agent_configs[index] = agent_configs_dict[i]\n                loadout_configs[index] = loadout_configs_dict[i]\n                index += 1\n        self.setup_manager = SetupManager()\n        self.setup_manager.load_config(self.overall_config, self.overall_config_path, agent_configs, loadout_configs)\n        self.setup_manager.connect_to_game()\n        self.setup_manager.launch_ball_prediction()\n        self.setup_manager.launch_quick_chat_manager()\n        self.setup_manager.launch_bot_processes()\n        self.setup_manager.start_match()\n        self.launch_in_progress = False\n        self.setup_manager.infinite_loop()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef connect_functions(self):\n        # Lambda is sometimes used to prevent passing the event parameter.\n        self.cfg_load_pushbutton.clicked.connect(lambda: self.load_overall_config())\n        self.cfg_save_pushbutton.clicked.connect(lambda: self.save_overall_config())\n\n        self.blue_listwidget.itemSelectionChanged.connect(self.load_selected_bot)\n        self.orange_listwidget.itemSelectionChanged.connect(self.load_selected_bot)\n        self.blue_listwidget.dropEvent = lambda event: self.bot_item_drop_event(self.blue_listwidget, event)\n        self.orange_listwidget.dropEvent = lambda event: self.bot_item_drop_event(self.orange_listwidget, event)\n\n        self.blue_name_lineedit.editingFinished.connect(self.team_settings_edit_event)\n        self.orange_name_lineedit.editingFinished.connect(self.team_settings_edit_event)\n        self.blue_color_spinbox.valueChanged.connect(self.team_settings_edit_event)\n        self.orange_color_spinbox.valueChanged.connect(self.team_settings_edit_event)\n\n        self.blue_minus_toolbutton.clicked.connect(lambda e: self.remove_agent(self.current_bot))\n        self.orange_minus_toolbutton.clicked.connect(lambda e: self.remove_agent(self.current_bot))\n        self.blue_plus_toolbutton.clicked.connect(lambda e: self.add_agent_button(team_index=0))\n        self.orange_plus_toolbutton.clicked.connect(lambda e: self.add_agent_button(team_index=1))\n\n        for child in self.bot_config_groupbox.findChildren(QWidget):\n            if isinstance(child, QLineEdit):\n                child.editingFinished.connect(self.bot_config_edit_event)\n            elif isinstance(child, QSlider):\n                child.valueChanged.connect(self.bot_config_edit_event)\n            elif isinstance(child, QRadioButton):\n                child.toggled.connect(self.bot_config_edit_event)\n            elif isinstance(child, QComboBox):\n                child.currentTextChanged.connect(self.bot_config_edit_event)\n        self.loadout_preset_toolbutton.clicked.connect(self.car_customisation.popup)\n        self.agent_preset_toolbutton.clicked.connect(self.agent_customisation.popup)\n        self.preset_load_toplevel_pushbutton.clicked.connect(self.load_preset_toplevel)\n\n        for child in self.match_settings_groupbox.findChildren(QWidget):\n            if isinstance(child, QComboBox):\n                child.currentTextChanged.connect(self.match_settings_edit_event)\n            elif isinstance(child, QCheckBox):\n                child.toggled.connect(self.match_settings_edit_event)\n\n        self.edit_mutators_pushbutton.clicked.connect(self.mutator_customisation.popup)\n        self.kill_bots_pushbutton.clicked.connect(self.kill_bots)\n        self.run_button.clicked.connect(self.run_button_pressed)", "response": "Connects all events to the functions which should be called\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nhandles the bot configuration edit event.", "response": "def bot_config_edit_event(self, value=None):\n        \"\"\"\n        Handles the events called when editing a value regarding the bot configuration\n        :param value: the new value to store in the config\n        :return:\n        \"\"\"\n        sender = self.sender()\n        if value is None:\n            value = sender.text()\n        agent = self.current_bot\n\n        if sender is self.bot_type_combobox:\n            self.update_bot_type_combobox()\n\n        elif sender is self.blue_radiobutton and value:  # 'and value' check to make sure that one got selected\n            if agent.get_team() != 0:\n                agent.set_team(0)\n                self.update_teams_listwidgets()\n                self.blue_listwidget.setCurrentItem(self.blue_listwidget.findItems(\n                    self.validate_name(agent.get_name(), agent), QtCore.Qt.MatchExactly)[0])\n\n        elif sender is self.orange_radiobutton and value:\n            if agent.get_team() != 1:\n                agent.set_team(1)\n                self.update_teams_listwidgets()\n                self.orange_listwidget.setCurrentItem(self.orange_listwidget.findItems(\n                    self.validate_name(agent.get_name(), agent), QtCore.Qt.MatchExactly)[0])\n\n        elif sender is self.ign_lineedit:\n            if agent not in self.agents:\n                return\n            if not agent.get_team():\n                listwidget = self.blue_listwidget\n            else:\n                listwidget = self.orange_listwidget\n            name = self.validate_name(value, agent)\n            old_name = self.validate_name(agent.ingame_name, agent)\n            row = listwidget.currentRow()\n            del self.bot_names_to_agent_dict[old_name]\n            agent.set_name(value)\n            self.bot_names_to_agent_dict[name] = agent\n            self.update_teams_listwidgets()\n            listwidget.setCurrentRow(row)\n        elif sender is self.loadout_preset_combobox:\n            if self.bot_config_groupbox.isEnabled() and self.current_bot is not None:\n\n                index = self.loadout_preset_combobox.currentIndex()\n                preset = self.loadout_preset_combobox.itemData(index)\n\n                self.current_bot.set_loadout_preset(preset)\n        elif sender is self.agent_preset_combobox:\n            if value and self.bot_config_groupbox.isEnabled() and self.current_bot is not None:\n\n                preset = self.agent_preset_combobox.currentData()\n\n                self.current_bot.set_agent_preset(preset)\n                agent.set_name(agent.agent_preset.config.get(BOT_CONFIG_MODULE_HEADER, BOT_NAME_KEY))\n                self.ign_lineedit.setText(agent.ingame_name)\n                if not agent.get_team():\n                    listwidget = self.blue_listwidget\n                else:\n                    listwidget = self.orange_listwidget\n                row = listwidget.currentRow()\n                self.update_teams_listwidgets()\n                listwidget.setCurrentRow(row)\n\n                loadout_index = index_of_config_path_in_combobox(self.loadout_preset_combobox, preset.looks_path)\n                if loadout_index is not None:\n                    self.loadout_preset_combobox.setCurrentIndex(loadout_index)\n\n        elif sender is self.bot_level_slider:\n            agent.set_bot_skill(value / 100)\n\n        if self.cfg_autosave_checkbutton.isChecked() and os.path.isfile(self.overall_config_path):\n            self.save_overall_config(10)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates the combobox to reflect the current bot type", "response": "def update_bot_type_combobox(self):\n        \"\"\"\n        Handles selecting another bot type in the combobox, hides some frames and shows others depending on the setting\n        Also saves the new type if there is a bot selected\n        :return:\n        \"\"\"\n        bot_type = self.bot_type_combobox.currentText()\n        if bot_type == 'RLBot':\n            self.rlbot_frame.setHidden(False)\n            self.extra_line.setHidden(False)\n            self.psyonix_bot_frame.setHidden(True)\n            self.appearance_frame.setHidden(False)\n            self.label_3.setHidden(False)\n            self.ign_lineedit.setHidden(False)\n        elif bot_type == 'Psyonix':\n            self.psyonix_bot_frame.setHidden(False)\n            self.rlbot_frame.setHidden(True)\n            self.extra_line.setHidden(False)\n            self.appearance_frame.setHidden(False)\n            self.label_3.setHidden(False)\n            self.ign_lineedit.setHidden(False)\n        elif bot_type == 'Human':\n            self.psyonix_bot_frame.setHidden(True)\n            self.rlbot_frame.setHidden(True)\n            self.extra_line.setHidden(True)\n            self.appearance_frame.setHidden(False)\n            self.label_3.setHidden(True)\n            self.ign_lineedit.setHidden(True)\n        elif bot_type == 'Party Member Bot':\n            self.rlbot_frame.setHidden(False)\n            self.extra_line.setHidden(False)\n            self.psyonix_bot_frame.setHidden(True)\n            self.appearance_frame.setHidden(False)\n            self.label_3.setHidden(True)\n            self.ign_lineedit.setHidden(True)\n\n        if self.bot_config_groupbox.isEnabled() and self.current_bot is not None:\n            config_type = bot_type.lower().replace(\" \", \"_\")\n            self.current_bot.set_participant_type(config_type)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nhandling the events when the user changes the value regarding the team settings.", "response": "def team_settings_edit_event(self, value=None):\n        \"\"\"\n        Handles the events when editing a value regarding the team settings\n        :param value: the new value to store in the config\n        :return:\n        \"\"\"\n        sender = self.sender()\n        if value is None:\n            value = sender.text()\n\n        if sender is self.blue_name_lineedit:\n            self.overall_config.set_value(TEAM_CONFIGURATION_HEADER, \"Team Blue Name\", value)\n        elif sender is self.orange_name_lineedit:\n            self.overall_config.set_value(TEAM_CONFIGURATION_HEADER, \"Team Orange Name\", value)\n        elif sender is self.blue_color_spinbox:\n            self.overall_config.set_value(TEAM_CONFIGURATION_HEADER, \"Team Blue Color\", value)\n        elif sender is self.orange_color_spinbox:\n            self.overall_config.set_value(TEAM_CONFIGURATION_HEADER, \"Team Orange Color\", value)\n\n        if self.cfg_autosave_checkbutton.isChecked() and os.path.isfile(self.overall_config_path):\n            self.save_overall_config(10)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update_team_settings(self):\n        self.blue_name_lineedit.setText(self.overall_config.get(TEAM_CONFIGURATION_HEADER, \"Team Blue Name\"))\n        self.orange_name_lineedit.setText(self.overall_config.get(TEAM_CONFIGURATION_HEADER, \"Team Orange Name\"))\n        self.blue_color_spinbox.setValue(self.overall_config.getint(TEAM_CONFIGURATION_HEADER, \"Team Blue Color\"))\n        self.orange_color_spinbox.setValue(self.overall_config.getint(TEAM_CONFIGURATION_HEADER, \"Team Orange Color\"))", "response": "Updates all team settings widgets to the value in the overall config"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_overall_config(self, config_path=None):\n        if self.overall_config is None:\n            self.overall_config = create_bot_config_layout()\n        GUIAgent.overall_config = self.overall_config\n        if config_path is None:\n            config_path = QFileDialog.getOpenFileName(self, \"Load Overall Config\", \"\", \"Config Files (*.cfg)\")[0]\n            if not config_path:\n                self.statusbar.showMessage(\"No file selected, not loading config\", 5000)\n                return\n        if config_path is None or not os.path.isfile(config_path):\n            return\n        if pathlib.Path(config_path).suffix != '.cfg':\n            self.popup_message(\"This file is not a config file!\", \"Invalid File Extension\", QMessageBox.Warning)\n            return\n        raw_parser = configparser.RawConfigParser()\n        raw_parser.read(config_path, encoding='utf8')\n        for section in ['Match Configuration', 'Participant Configuration']:\n            if not raw_parser.has_section(section):\n                self.popup_message(f\"Config file is missing the section {section}, not loading it!\",\n                                   \"Invalid Config File\", QMessageBox.Warning)\n                return\n        self.overall_config_path = config_path\n        self.overall_config.parse_file(raw_parser, MAX_PLAYERS, config_directory=os.path.dirname(self.overall_config_path))\n        self.load_agents()\n        # self.load_bot_directory(\".\")\n        self.update_teams_listwidgets()\n        self.cfg_file_path_lineedit.setText(self.overall_config_path)\n        self.cfg_file_path_lineedit.setStyleSheet(\"\")\n        self.run_button.setEnabled(True)\n        self.update_team_settings()\n        self.car_customisation.update_presets_widgets()\n        self.agent_customisation.update_presets_widgets()\n        self.mutator_customisation.update_comboboxes()", "response": "Load the overall config from the config file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nscheduling a save after given time_out.", "response": "def save_overall_config(self, time_out=0):\n        \"\"\"\n        Schedules a save after given time_out\n        :param time_out: The amount of seconds it should wait before saving\n        :return:\n        \"\"\"\n        def save():\n            if not os.path.exists(self.overall_config_path):\n                return\n            self.overall_config_timer.setInterval(1000)\n            if self.remaining_save_timer > 0:\n                self.statusbar.showMessage(\"Saving Overall Config in \" + str(self.remaining_save_timer) + \" seconds\")\n                self.remaining_save_timer -= 1\n            else:\n                self.clean_overall_config_loadouts()\n                with open(self.overall_config_path, \"w\", encoding='utf8') as f:\n                    f.write(str(self.fixed_indices()))\n                self.statusbar.showMessage(\"Saved Overall Config to \" + self.overall_config_path, 5000)\n                self.overall_config_timer.stop()\n        if self.overall_config_timer is None:\n            self.overall_config_timer = QTimer()\n            self.overall_config_timer.timeout.connect(save)\n        save_path = self.overall_config_path\n        if save_path is None or not os.path.isfile(save_path):\n            save_path = QFileDialog.getSaveFileName(self, \"Save Overall Config\", \"\", \"Config Files (*.cfg)\")[0]\n            if not save_path:\n                self.statusbar.showMessage(\"Unable to save the configuration without a path\", 5000)\n                return\n            self.overall_config_path = save_path\n        self.remaining_save_timer = time_out\n        self.overall_config_timer.start(0)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncleaning up all unusued loadout paths.", "response": "def clean_overall_config_loadouts(self):\n        \"\"\"\n        Set all unusued loadout paths to None. This makes sure agents don't have a custom loadout when new agents\n        are added in the gui.\n        \"\"\"\n        for i in range(MAX_PLAYERS):\n            if i not in self.index_manager.numbers:\n                self.overall_config.set_value(PARTICIPANT_CONFIGURATION_HEADER, PARTICIPANT_LOADOUT_CONFIG_KEY, \"None\", i)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_selected_bot(self):\n        # prevent processing from itself (clearing the other one processes this)\n        if not self.sender().selectedItems():\n            return\n\n        blue = True if self.sender() is self.blue_listwidget else False\n\n        if blue:  # deselect the other listbox\n            self.orange_listwidget.clearSelection()\n        else:\n            self.blue_listwidget.clearSelection()\n        item_name = self.sender().selectedItems()[0].text()\n        agent = self.bot_names_to_agent_dict[item_name]\n        if agent is None:  # something went wrong if agent is None\n            return\n\n        self.current_bot = agent\n\n        self.bot_config_groupbox.setEnabled(True)  # Make sure that you can edit the bot\n        # enable [-] for right listwidget\n        if blue:\n            self.blue_minus_toolbutton.setDisabled(False)\n            self.orange_minus_toolbutton.setDisabled(True)\n        else:\n            self.orange_minus_toolbutton.setDisabled(False)\n            self.blue_minus_toolbutton.setDisabled(True)\n\n        # load the bot parameters into the edit frame\n        agent_type = agent.get_participant_type()\n\n        known_types = ['human', 'psyonix', 'rlbot', 'party_member_bot']\n        assert agent_type in known_types, 'Bot has unknown type: %s' % agent_type\n\n        self.bot_type_combobox.setCurrentIndex(known_types.index(agent_type))\n        if blue:\n            self.blue_radiobutton.setChecked(True)\n        else:\n            self.orange_radiobutton.setChecked(True)\n        self.ign_lineedit.setText(agent.ingame_name)\n\n        loadout_index = index_of_config_path_in_combobox(\n            self.loadout_preset_combobox, agent.get_loadout_preset().config_path)\n        self.loadout_preset_combobox.setCurrentIndex(loadout_index or 0)\n\n        self.agent_preset_combobox.blockSignals(True)\n        self.agent_preset_combobox.setCurrentText(agent.get_agent_preset().get_name())\n        self.agent_preset_combobox.blockSignals(False)\n        self.bot_level_slider.setValue(int(agent.get_bot_skill() * 100))", "response": "Loads all the values belonging to the new selected agent into the bot_config_groupbox."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update_teams_listwidgets(self):\n        self.bot_names_to_agent_dict.clear()\n        self.blue_listwidget.clear()\n        self.orange_listwidget.clear()\n        for agent in self.agents:\n            name = self.validate_name(agent.ingame_name, agent)\n            if not agent.get_team():\n                self.blue_listwidget.addItem(name)\n            else:\n                self.orange_listwidget.addItem(name)\n\n            self.bot_names_to_agent_dict[name] = agent\n\n        self.enable_disable_on_bot_select_deselect()\n\n        # if max bot count reached: disable + button\n        if not self.index_manager.has_free_slots():\n            self.blue_plus_toolbutton.setDisabled(True)\n            self.orange_plus_toolbutton.setDisabled(True)\n        else:\n            self.blue_plus_toolbutton.setDisabled(False)\n            self.orange_plus_toolbutton.setDisabled(False)", "response": "Updates the listwidgets with all items from the self. agents list and adds them to the right team."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef enable_disable_on_bot_select_deselect(self):\n        if not self.blue_listwidget.selectedItems() and not self.orange_listwidget.selectedItems():\n            self.bot_config_groupbox.setDisabled(True)\n            self.blue_minus_toolbutton.setDisabled(True)\n            self.orange_minus_toolbutton.setDisabled(True)\n        else:\n            self.bot_config_groupbox.setDisabled(False)", "response": "Disables the botconfig groupbox and minus buttons when no bot is selected"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nvalidate the name for the agent instance.", "response": "def validate_name(self, name, agent):\n        \"\"\"\n        Finds the modification of name which is not yet in the list\n        :param name: the (new) name for the agent\n        :param agent: the agent instance to allow the same name as the previous one if necessary\n        :return: the best modification of name not yet in a listwidget\n        \"\"\"\n        if name in self.bot_names_to_agent_dict and self.bot_names_to_agent_dict[name] is not agent:\n            i = 0\n            while True:\n                if name + \" (\" + str(i) + \")\" in self.bot_names_to_agent_dict and \\\n                        self.bot_names_to_agent_dict[name + \" (\" + str(i) + \")\"] is not agent:\n                    i += 1\n                else:\n                    value = name + \" (\" + str(i) + \")\"\n                    return value\n        else:\n            return name"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_agents(self, config_file=None):\n        if config_file is not None:\n            self.overall_config = config_file\n        self.agents.clear()\n        num_participants = get_num_players(self.overall_config)\n        try:\n            for i in range(num_participants):\n                self.load_agent(i)\n        except BaseException as e:\n            raise ValueError(f\"{str(e)}\\nPlease check your config files! {self.overall_config_path}\")", "response": "Loads all agents from the config file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_agent(self, overall_index: int=None):\n        if not self.index_manager.has_free_slots():\n            return None\n        if overall_index is None:\n            overall_index = self.index_manager.get_new_index()\n        else:\n            self.index_manager.use_index(overall_index)\n        agent = self.add_agent(overall_index=overall_index)\n\n        path_in_overall_config = agent.get_agent_config_path()\n        if path_in_overall_config is None:\n            # Fall back to the path of the first agent if there's nothing configured.\n            path_in_overall_config = self.overall_config.getpath(PARTICIPANT_CONFIGURATION_HEADER,\n                                                                 PARTICIPANT_CONFIG_KEY, 0)\n        agent_preset = self.add_agent_preset(path_in_overall_config)\n        agent.set_agent_preset(agent_preset)\n        agent.set_name(agent_preset.config.get(BOT_CONFIG_MODULE_HEADER, BOT_NAME_KEY))\n\n        # Add the preset's loadout as a loadout\n        own_loadout = self.add_loadout_preset(agent_preset.looks_path)\n\n        # Agent has a loadout defined in overall config, load that if it is not None\n        loadout_file_in_overall_config = self.overall_config.get(PARTICIPANT_CONFIGURATION_HEADER,\n                                               PARTICIPANT_LOADOUT_CONFIG_KEY, overall_index)\n        if loadout_file_in_overall_config is None or loadout_file_in_overall_config == \"None\":\n            agent.set_loadout_preset(own_loadout)\n        else:\n            directory = get_python_root()\n            file_path = loadout_file_in_overall_config\n            loadout_file_in_overall_config = os.path.realpath(os.path.join(directory, file_path))\n            loadout_preset = self.add_loadout_preset(loadout_file_in_overall_config)\n            agent.set_loadout_preset(loadout_preset)\n\n        return agent", "response": "Loads all data for the given overall index from the overall config and also loads both presets and loadout files."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding an Agent to the index manager.", "response": "def add_agent(self, overall_index=None, team_index=None):\n        \"\"\"\n        Creates the agent using self.agent_class and adds it to the index manager.\n        :param overall_index: The index of the bot in the config file if it already exists.\n        :param team_index: The index of the team to place the agent in\n        :return agent: an Agent (gui_agent) with either given index or a free one, returns None if there is no index given and all indices are occupied\n        \"\"\"\n        if overall_index is None:\n            if not self.index_manager.has_free_slots():\n                return\n            overall_index = self.index_manager.get_new_index()\n        else:\n            self.index_manager.use_index(overall_index)\n        agent = GUIAgent(overall_index=overall_index)\n        if team_index is not None:\n            agent.set_team(team_index)\n\n        self.agents.append(agent)\n        self.overall_config.set_value(MATCH_CONFIGURATION_HEADER, PARTICIPANT_COUNT_KEY, len(self.agents))\n        return agent"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nremove the given agent from the list of agents.", "response": "def remove_agent(self, agent: GUIAgent):\n        \"\"\"\n        Removes the given agent.\n        :param agent: the agent to remove\n        :return:\n        \"\"\"\n        self.index_manager.free_index(agent.overall_index)\n        self.agents.remove(agent)\n        self.update_teams_listwidgets()\n        self.overall_config.set_value(MATCH_CONFIGURATION_HEADER, PARTICIPANT_COUNT_KEY, len(self.agents))\n        self.overall_config.set_value(PARTICIPANT_CONFIGURATION_HEADER, PARTICIPANT_LOADOUT_CONFIG_KEY, \"None\", agent.overall_index)\n        if len(self.agents) == 0:\n            return\n        if agent.get_team() == 0:\n            if self.blue_listwidget.count() != 0:\n                self.blue_listwidget.setCurrentRow(self.blue_listwidget.count() - 1)\n            else:\n                self.orange_listwidget.setCurrentRow(self.orange_listwidget.count() - 1)\n        else:\n            if self.orange_listwidget.count() != 0:\n                self.orange_listwidget.setCurrentRow(self.orange_listwidget.count() - 1)\n            else:\n                self.blue_listwidget.setCurrentRow(self.blue_listwidget.count() - 1)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_loadout_preset(self, file_path: str):\n        if file_path is not None and os.path.isfile(file_path):\n            name = pathlib.Path(file_path).stem\n        else:\n            name = \"new preset\"\n        if file_path in self.loadout_presets:\n            return self.loadout_presets[file_path]\n        preset = LoadoutPreset(name, file_path)\n        self.loadout_presets[preset.config_path] = preset\n        return preset", "response": "Adds a loadout preset using file_path with all values from that path loaded\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_agent_preset(self, file_path):\n        if file_path is not None and os.path.isfile(file_path):\n            name = pathlib.Path(file_path).stem\n        else:\n            raise FileNotFoundError(f\"File path {file_path} is not found!\")\n\n        if name in self.agent_presets:\n            if self.agent_presets[name].config_path == file_path:\n                return self.agent_presets[name]\n            else:\n                i = 1\n                for preset_name in self.agent_presets:\n                    if name in preset_name:\n                        i += 1\n                name = f\"{name} ({i})\"\n        preset = AgentPreset(name, file_path)\n        self.agent_presets[preset.get_name()] = preset\n        return preset", "response": "Loads a new agent preset using the given file path."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding all items to the match settings comboboxes", "response": "def init_match_settings(self):\n        \"\"\"\n        Adds all items to the match settings comboboxes\n        :return:\n        \"\"\"\n        self.mode_type_combobox.addItems(game_mode_types)\n        self.map_type_combobox.addItems(map_types)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update_match_settings(self):\n        self.mode_type_combobox.setCurrentText(self.overall_config.get(MATCH_CONFIGURATION_HEADER, GAME_MODE))\n        self.map_type_combobox.setCurrentText(self.overall_config.get(MATCH_CONFIGURATION_HEADER, GAME_MAP))\n        self.skip_replays_checkbox.setChecked(self.overall_config.getboolean(MATCH_CONFIGURATION_HEADER, SKIP_REPLAYS))\n        self.instant_start_checkbox.setChecked(\n            self.overall_config.getboolean(MATCH_CONFIGURATION_HEADER, INSTANT_START))", "response": "Updates all match settings widgets to the values in the overall config"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nhandle all edits to match settings and sets the config value to the new value", "response": "def match_settings_edit_event(self, value):\n        \"\"\"\n        Handles all edits to match settings and sets the config value to the new value\n        :param value: the value to apply to the overall config\n        :return:\n        \"\"\"\n        sender = self.sender()\n\n        if sender is self.mode_type_combobox:\n            self.overall_config.set_value(MATCH_CONFIGURATION_HEADER, GAME_MODE, value)\n        elif sender is self.map_type_combobox:\n            self.overall_config.set_value(MATCH_CONFIGURATION_HEADER, GAME_MAP, value)\n        elif sender is self.skip_replays_checkbox:\n            self.overall_config.set_value(MATCH_CONFIGURATION_HEADER, SKIP_REPLAYS, value)\n        elif sender is self.instant_start_checkbox:\n            self.overall_config.set_value(MATCH_CONFIGURATION_HEADER, INSTANT_START, value)\n        elif sender is self.match_length_combobox:\n            self.overall_config.set_value(MUTATOR_CONFIGURATION_HEADER, MUTATOR_MATCH_LENGTH, value)\n        elif sender is self.boost_type_combobox:\n            self.overall_config.set_value(MUTATOR_CONFIGURATION_HEADER, MUTATOR_BOOST_AMOUNT, value)\n\n        if self.cfg_autosave_checkbutton.isChecked() and os.path.isfile(self.overall_config_path):\n            self.save_overall_config(10)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef main():\n        app = QApplication(sys.argv)\n        rlbot_icon = QtGui.QIcon(os.path.join(get_rlbot_directory(), 'img', 'rlbot_icon.png'))\n        app.setWindowIcon(rlbot_icon)\n        window = RLBotQTGui()\n        window.show()\n        app.exec_()", "response": "Start the GUI\n        :return:"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_mutator_settings(mutator_settings, config: ConfigObject):\n    mutator_settings.match_length = safe_get_mutator(match_length_types, config, MUTATOR_MATCH_LENGTH)\n    mutator_settings.max_score = safe_get_mutator(max_score_types, config, MUTATOR_MAX_SCORE, {'0': 'Unlimited'})\n    mutator_settings.overtime = safe_get_mutator(overtime_mutator_types, config, MUTATOR_OVERTIME)\n    mutator_settings.series_length = safe_get_mutator(series_length_mutator_types, config, MUTATOR_SERIES_LENGTH)\n    mutator_settings.game_speed = safe_get_mutator(game_speed_mutator_types, config, MUTATOR_GAME_SPEED)\n    mutator_settings.ball_max_speed = safe_get_mutator(\n        ball_max_speed_mutator_types, config, MUTATOR_BALL_MAX_SPEED, {'0': 'Default'})\n    mutator_settings.ball_type = safe_get_mutator(ball_type_mutator_types, config, MUTATOR_BALL_TYPE)\n    mutator_settings.ball_weight = safe_get_mutator(ball_weight_mutator_types, config, MUTATOR_BALL_WEIGHT)\n    mutator_settings.ball_size = safe_get_mutator(\n        ball_size_mutator_types, config, MUTATOR_BALL_SIZE, {'1.0': 'Default'})\n    mutator_settings.ball_bounciness = safe_get_mutator(\n        ball_bounciness_mutator_types, config, MUTATOR_BALL_BOUNCINESS, {'1.0': 'Default'})\n    mutator_settings.boost_amount = safe_get_mutator(boost_amount_mutator_types, config, MUTATOR_BOOST_AMOUNT)\n    mutator_settings.rumble = safe_get_mutator(rumble_mutator_types, config, MUTATOR_RUMBLE)\n    mutator_settings.boost_strength = safe_get_mutator(\n        boost_strength_mutator_types, config, MUTATOR_BOOST_STRENGTH, {'Default': '1x', '1.0': '1x'})\n    mutator_settings.gravity = safe_get_mutator(gravity_mutator_types, config, MUTATOR_GRAVITY)\n    mutator_settings.demolish = safe_get_mutator(demolish_mutator_types, config, MUTATOR_DEMOLISH)\n    mutator_settings.respawn_time = safe_get_mutator(respawn_time_mutator_types, config, MUTATOR_RESPAWN_TIME, {\n                                                            '3.0': '3 Seconds', '3': '3 Seconds'})", "response": "Parse the mutator settings from the config object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the matching settings modifying the match settings object.", "response": "def parse_match_settings(match_settings, config: ConfigObject):\n    \"\"\"\n    Parses the matching settings modifying the match settings object.\n    :param match_settings:\n    :param config:\n    :return:\n    \"\"\"\n\n    match_settings.game_mode = config.get(MATCH_CONFIGURATION_HEADER, GAME_MODE)\n    match_settings.game_map = config.get(MATCH_CONFIGURATION_HEADER, GAME_MAP)\n    match_settings.skip_replays = config.getboolean(MATCH_CONFIGURATION_HEADER, SKIP_REPLAYS)\n    match_settings.instant_start = config.getboolean(MATCH_CONFIGURATION_HEADER, INSTANT_START)\n\n    parse_mutator_settings(match_settings.mutators, config)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_new_index(self):\n        free_index = next(filterfalse(self.numbers.__contains__, count(0, 1)))\n        self.numbers.add(free_index)\n        return free_index", "response": "Gets the next available index and marks it as in use. Returns the index that is not currently in use."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if there is a free value smaller than self. size", "response": "def has_free_slots(self):\n        \"\"\"\n        Checks if there is a free value smaller than self.size\n        :return: True if there is a free value, False if not\n        \"\"\"\n        return next(filterfalse(self.numbers.__contains__, count(1))) < self.size"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend a quick chat to the other bots.", "response": "def send_quick_chat(self, team_only, quick_chat):\n        \"\"\"\n        Sends a quick chat to the other bots.\n        If it is QuickChats.CHAT_NONE or None it does not send a quick chat to other bots.\n        :param team_only: either True or False, this says if the quick chat should only go to team members.\n        :param quick_chat: The quick chat selection, available chats are defined in quick_chats.py\n        \"\"\"\n        if quick_chat == QuickChats.CHAT_NONE or quick_chat is None:\n            return\n        self.__quick_chat_func(team_only, quick_chat)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef convert_output_to_v4(self, controller_input):\n        player_input = SimpleControllerState()\n        player_input.throttle = controller_input[0]\n        player_input.steer = controller_input[1]\n        player_input.pitch = controller_input[2]\n        player_input.yaw = controller_input[3]\n        player_input.roll = controller_input[4]\n        player_input.jump = controller_input[5]\n        player_input.boost = controller_input[6]\n        player_input.handbrake = controller_input[7]\n\n        return player_input", "response": "Converts a v3 output to a v4 controller state"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef base_create_agent_configurations(cls) -> ConfigObject:\n        config = ConfigObject()\n        location_config = config.add_header_name(BOT_CONFIG_MODULE_HEADER)\n        location_config.add_value(LOOKS_CONFIG_KEY, str,\n                                  description='Path to loadout config from runner')\n        location_config.add_value(PYTHON_FILE_KEY, str,\n                                  description=\"Bot's python file.\\nOnly need this if RLBot controlled\")\n        location_config.add_value(BOT_NAME_KEY, str, default='nameless',\n                                  description='The name that will be displayed in game')\n\n        details_config = config.add_header_name(BOT_CONFIG_DETAILS_HEADER)\n        details_config.add_value('developer', str, description=\"Name of the bot's creator/developer\")\n        details_config.add_value('description', str, description=\"Short description of the bot\")\n        details_config.add_value('fun_fact', str, description=\"Fun fact about the bot\")\n        details_config.add_value('github', str, description=\"Link to github repository\")\n        details_config.add_value('language', str, description=\"Programming language\")\n\n        cls.create_agent_configurations(config)\n\n        return config", "response": "This method creates a ConfigObject that can be used by BaseAgent subclasses to create custom agent configurations."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun the exercises in a single iteration.", "response": "def run_exercises(setup_manager: SetupManager, exercises: Iterable[Exercise], seed: int) -> Iterator[Result]:\n    \"\"\"\n    It is recommended to use setup_manager_context() to generate your setup_manager.\n    \"\"\"\n    game_interface = setup_manager.game_interface\n    names = [exercise.get_name() for exercise in exercises]\n    with training_status_renderer_context(names, game_interface.renderer) as ren:\n        for i, exercise in enumerate(exercises):\n            def update_row(status: str, status_color_func):\n                nonlocal i\n                nonlocal exercise\n                ren.update(i, Row(exercise.get_name(), status, status_color_func))\n\n            update_row('config', ren.renderman.white)\n            # Only reload the match if the config has changed.\n            new_match_config = exercise.get_match_config()\n            if new_match_config != setup_manager.match_config:\n                update_row('match', ren.renderman.white)\n                _setup_match(new_match_config, setup_manager)\n                update_row('bots', ren.renderman.white)\n                _wait_until_bots_ready(setup_manager, new_match_config)\n\n            update_row('wait', ren.renderman.white)\n            _wait_until_good_ticks(game_interface)\n\n            update_row('setup', ren.renderman.white)\n            error_result = _setup_exercise(game_interface, exercise, seed)\n            if error_result is not None:\n                update_row('setup', ren.renderman.red)\n                yield error_result\n                continue\n\n            update_row('reload', ren.renderman.white)\n            setup_manager.reload_all_agents(quiet=True)\n\n            # Wait for the set_game_state() to propagate before we start running ex.on_tick()\n            # TODO: wait until the game looks similar.\n            update_row('sleep', ren.renderman.white)\n            time.sleep(0.03)\n\n            update_row('>>>>', ren.renderman.white)\n            result = _grade_exercise(game_interface, exercise, seed)\n\n            if isinstance(result.grade, Pass):\n                update_row('PASS', ren.renderman.green)\n            else:\n                update_row('FAIL', ren.renderman.red)\n\n            yield result"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nblock until we re getting new packets indicating that the match is ready.", "response": "def _wait_until_good_ticks(game_interface: GameInterface, required_new_ticks: int=3):\n    \"\"\"Blocks until we're getting new packets, indicating that the match is ready.\"\"\"\n    rate_limit = rate_limiter.RateLimiter(120)\n    last_tick_game_time = None  # What the tick time of the last observed tick was\n    packet = GameTickPacket()  # We want to do a deep copy for game inputs so people don't mess with em\n    seen_times = 0\n    while seen_times < required_new_ticks:\n        game_interface.update_live_data_packet(packet)\n        def is_good_tick():\n            if packet.game_info.seconds_elapsed == last_tick_game_time: return False\n            if not packet.game_info.is_round_active: return False\n            if any(car.is_demolished for car in packet.game_cars): return False\n            return True\n        if is_good_tick():\n            seen_times += 1\n        last_tick_game_time = packet.game_info.seconds_elapsed\n\n        rate_limit.acquire()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the game state. Only returns a result if there was an error in ex.setup()", "response": "def _setup_exercise(game_interface: GameInterface, ex: Exercise, seed: int) -> Optional[Result]:\n    \"\"\"\n    Set the game state.\n    Only returns a result if there was an error in ex.setup()\n    \"\"\"\n    rng = random.Random()\n    rng.seed(seed)\n    try:\n        game_state = ex.setup(rng)\n    except Exception as e:\n        return Result(ex, seed, FailDueToExerciseException(e, traceback.format_exc()))\n    game_interface.set_game_state(game_state)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef training_status_renderer_context(exercise_names: List[str], renderman: RenderingManager):\n    renderer = TrainingStatusRenderer(exercise_names, renderman)\n    try:\n        yield renderer\n    finally:\n        renderer.clear_screen()", "response": "Context manager that returns a training status renderer."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a game tick packet in the v3 struct format to a legacy one.", "response": "def convert_to_legacy_v3(\n        game_tick_packet: game_data_struct.GameTickPacket,\n        field_info_packet: game_data_struct.FieldInfoPacket = None):\n    \"\"\"\n    Returns a legacy packet from v3\n    :param game_tick_packet a game tick packet in the v4 struct format.\n    :param field_info_packet a field info packet in the v4 struct format. Optional. If this is not supplied,\n    none of the boost locations will be filled in.\n    \"\"\"\n    legacy_packet = GameTickPacket()\n\n    legacy_packet.numBoosts = game_tick_packet.num_boost\n    legacy_packet.numCars = game_tick_packet.num_cars\n\n    for i in range(game_tick_packet.num_cars):\n        convert_player_info(legacy_packet.gamecars[i], game_tick_packet.game_cars[i])\n\n    for i in range(game_tick_packet.num_boost):\n        convert_boost_info(legacy_packet.gameBoosts[i], game_tick_packet.game_boosts[i])\n        if field_info_packet is not None:\n            convert_vector(legacy_packet.gameBoosts[i].Location, field_info_packet.boost_pads[i].location)\n\n    convert_ball_info(legacy_packet.gameball, game_tick_packet.game_ball)\n\n    convert_game_info(legacy_packet.gameInfo, game_tick_packet.game_info)\n\n    return legacy_packet"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_enum_object(list, list_name=None, other_attributes=None, attribute_object=None):\n    result = EmptyClass()\n    for i in range(len(list)):\n        setattr(result, list[i], i if attribute_object is None else getattr(attribute_object, list[i]))\n    if list_name is not None:\n        setattr(result, list_name, list)\n    if other_attributes is not None:\n        for attribute_tuple in other_attributes:\n            setattr(result, attribute_tuple[0], attribute_tuple[1])\n    return result", "response": "Creates an enum object + adds other attributes."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninject the DLL into the system.", "response": "def inject_dll(self):\n        \"\"\"\n        Calling this function will inject the DLL without GUI\n        DLL will return status codes from 0 to 5 which correspond to injector_codes\n        DLL injection is only valid if codes are 0->'INJECTION_SUCCESSFUL' or 3->'RLBOT_DLL_ALREADY_INJECTED'\n        It will print the output code and if it's not valid it will kill runner.py\n        If RL isn't running the Injector will stay hidden waiting for RL to open and inject as soon as it does\n        \"\"\"\n\n        self.logger.info('Injecting DLL')\n        # Inject DLL\n        injector_dir = os.path.join(get_dll_directory(), 'RLBot_Injector.exe')\n\n        for file in ['RLBot_Injector.exe', 'RLBot_Core.dll', 'RLBot_Core_Interface.dll', 'RLBot_Core_Interface_32.dll']:\n            file_path = os.path.join(get_dll_directory(), file)\n            if not os.path.isfile(file_path):\n                raise FileNotFoundError(f'{file} was not found in {get_dll_directory()}. '\n                                        'Please check that the file exists and your antivirus '\n                                        'is not removing it. See https://github.com/RLBot/RLBot/wiki/Antivirus-Notes')\n\n        incode = subprocess.call([injector_dir, 'hidden'])\n        injector_codes = ['INJECTION_SUCCESSFUL',\n                          'INJECTION_FAILED',\n                          'MULTIPLE_ROCKET_LEAGUE_PROCESSES_FOUND',\n                          'RLBOT_DLL_ALREADY_INJECTED',\n                          'RLBOT_DLL_NOT_FOUND',\n                          'MULTIPLE_RLBOT_DLL_FILES_FOUND']\n        injector_valid_codes = ['INJECTION_SUCCESSFUL',\n                                'RLBOT_DLL_ALREADY_INJECTED']\n        injection_status = injector_codes[incode]\n        if injection_status in injector_valid_codes:\n            self.logger.info('Finished Injecting DLL')\n            return injection_status\n        else:\n            self.logger.error('Failed to inject DLL: ' + injection_status)\n            sys.exit()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_status_callback(self, callback=None):\n        if callback is None:\n            return self.callback_func\n\n        def safe_wrapper(id, rlbotstatsus):\n            callback(rlbotstatsus)\n\n        return self.game_status_callback_type(wrap_callback(safe_wrapper))", "response": "Creates a callback function for the rlbot status."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the live data packet in flatbuffer format.", "response": "def get_live_data_flat_binary(self):\n        \"\"\"\n        Gets the live data packet in flatbuffer binary format. You'll need to do something like\n        GameTickPacket.GetRootAsGameTickPacket(binary, 0) to get the data out.\n\n        This is a temporary method designed to keep the integration test working. It returns the raw bytes\n        of the flatbuffer so that it can be stored in a file. We can get rid of this once we have a first-class\n        data recorder that lives inside the core dll.\n        \"\"\"\n        byte_buffer = self.game.UpdateLiveDataPacketFlatbuffer()\n        if byte_buffer.size >= 4:  # GetRootAsGameTickPacket gets angry if the size is less than 4\n            # We're counting on this copying the data over to a new memory location so that the original\n            # pointer can be freed safely.\n            proto_string = ctypes.string_at(byte_buffer.ptr, byte_buffer.size)\n            self.game.Free(byte_buffer.ptr)  # Avoid a memory leak\n            self.game_status(None, RLBotCoreStatus.Success)\n            return proto_string"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_rigid_body_tick(self, rigid_body_tick: RigidBodyTick):\n        rlbot_status = self.game.UpdateRigidBodyTick(rigid_body_tick)\n        self.game_status(None, rlbot_status)\n        return rigid_body_tick", "response": "Update the status of the game with the new rigid body tick."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the field information from the interface.", "response": "def get_field_info(self) -> FieldInfo:\n        \"\"\"\n        Gets the field information from the interface.\n        :return: The field information\n        \"\"\"\n        byte_buffer = self.game.UpdateFieldInfoFlatbuffer()\n\n        if byte_buffer.size >= 4:  # GetRootAsGameTickPacket gets angry if the size is less than 4\n            # We're counting on this copying the data over to a new memory location so that the original\n            # pointer can be freed safely.\n            proto_string = ctypes.string_at(byte_buffer.ptr, byte_buffer.size)\n            self.game.Free(byte_buffer.ptr)  # Avoid a memory leak\n            self.game_status(None, RLBotCoreStatus.Success)\n            return FieldInfo.GetRootAsFieldInfo(proto_string, 0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_ball_prediction(self) -> BallPredictionPacket:\n        byte_buffer = self.game.GetBallPrediction()\n\n        if byte_buffer.size >= 4:  # GetRootAsGameTickPacket gets angry if the size is less than 4\n            # We're counting on this copying the data over to a new memory location so that the original\n            # pointer can be freed safely.\n            proto_string = ctypes.string_at(byte_buffer.ptr, byte_buffer.size)\n            self.game.Free(byte_buffer.ptr)  # Avoid a memory leak\n            self.game_status(None, RLBotCoreStatus.Success)\n            return BallPredictionPacket.GetRootAsBallPrediction(proto_string, 0)", "response": "Gets the latest ball prediction available in shared memory. Only works if BallPrediction. exe is running."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts this object into a flatbuffer representation.", "response": "def convert_to_flat(self, builder):\n        \"\"\"\n        In this conversion, we always want to return a valid flatbuffer pointer even if all the\n        contents are blank because sometimes we need to put empty car states into the car list\n        to make the indices line up.\n        \"\"\"\n        physics_offset = None if self.physics is None else self.physics.convert_to_flat(builder)\n\n        DesiredCarState.DesiredCarStateStart(builder)\n        if physics_offset is not None:\n            DesiredCarState.DesiredCarStateAddPhysics(builder, physics_offset)\n        if self.boost_amount is not None:\n            DesiredCarState.DesiredCarStateAddBoostAmount(builder, Float.CreateFloat(builder, self.boost_amount))\n        if self.jumped is not None:\n            DesiredCarState.DesiredCarStateAddJumped(builder, Bool.CreateBool(builder, self.jumped))\n        if self.double_jumped is not None:\n            DesiredCarState.DesiredCarStateAddDoubleJumped(builder, Bool.CreateBool(builder, self.double_jumped))\n        return DesiredCarState.DesiredCarStateEnd(builder)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef convert_to_flat(self, builder):\n\n        DesiredBoostState.DesiredBoostStateStart(builder)\n        if self.respawn_time is not None:\n            DesiredBoostState.DesiredBoostStateAddRespawnTime(builder, Float.CreateFloat(builder, self.respawn_time))\n        return DesiredBoostState.DesiredBoostStateEnd(builder)", "response": "Convert the current entry into a flatbuffer."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends a quick chat to the general queue for everyone to pull from the message", "response": "def send_quick_chat(queue_holder, index, team, team_only, quick_chat):\n    \"\"\"\n    Sends a quick chat to the general queue for everyone to pull from\n    :param queue_holder:\n    :param index: The index of the player sending the message\n    :param team: The team of the player sending the message\n    :param team_only: if the message is team only\n    :param quick_chat: The contents of the quick chat\n    :return:\n    \"\"\"\n    queue_holder[\"output\"].put((index, team, team_only, quick_chat))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nregister a function to be called anytime this queue gets a quick chat.", "response": "def register_for_quick_chat(queue_holder, called_func, quit_event):\n    \"\"\"\n    Registers a function to be called anytime this queue gets a quick chat.\n    :param queue_holder:  This holds the queues for the bots\n    :param called_func: This is the function that is called when a quick chat is received\n    :param quit_event: This event will be set when rlbot is trying to shut down\n    :return: The newly created thread.\n    \"\"\"\n\n    def threaded_func(chat_queue, called_func, quit_event):\n        while not quit_event.is_set():\n            try:\n                next_message = chat_queue.get(timeout=0.1)\n                index, team, chat = next_message\n                called_func(index, team, chat)\n            except queue.Empty:\n                pass\n            except EOFError as e:\n                # Something else is shutting down - we can no longer communicate.\n                get_logger('chats').debug('quick_chat queue terminated. %s', e)\n                return\n\n    thread = Thread(\n        target=threaded_func,\n        args=(queue_holder[\"input\"], called_func, quit_event),\n        daemon=True,\n    )\n    thread.start()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninitialize the engine. :param app: The app to use :type app: Object :param storage: The blog storage instance that implements the :type storage: Object :param cache: (Optional) A Flask-Cache object to enable caching :type cache: Object ``Storage`` class interface.", "response": "def init_app(self, app, storage=None, cache=None, file_upload=None):\n        \"\"\"\n        Initialize the engine.\n\n        :param app: The app to use\n        :type app: Object\n        :param storage: The blog storage instance that implements the\n        :type storage: Object\n        :param cache: (Optional) A Flask-Cache object to enable caching\n        :type cache: Object\n         ``Storage`` class interface.\n        \"\"\"\n\n        self.app = app\n        self.config = self.app.config\n        self.storage = storage or self.storage\n        self.file_upload = file_upload or self.file_upload\n        self.cache = cache or self.cache\n        self._register_plugins(self.app, self.config)\n\n        from .views import create_blueprint\n        blog_app = create_blueprint(__name__, self)\n        # external urls\n        blueprint_created.send(self.app, engine=self, blueprint=blog_app)\n        self.app.register_blueprint(\n            blog_app, url_prefix=self.config.get(\"BLOGGING_URL_PREFIX\"))\n\n        self.app.extensions[\"FLASK_BLOGGING_ENGINE\"] = self  # duplicate\n        self.app.extensions[\"blogging\"] = self\n        self.principal = Principal(self.app)\n        engine_initialised.send(self.app, engine=self)\n\n        if self.config.get(\"BLOGGING_ALLOW_FILEUPLOAD\", True):\n            self.ffu = self.file_upload or FlaskFileUpload(app)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprocesses a single post.", "response": "def process_post(self, post, render=True):\n        \"\"\"\n        A high level view to create post processing.\n        :param post: Dictionary representing the post\n        :type post: dict\n        :param render: Choice if the markdown text has to be converted or not\n        :type render: bool\n        :return:\n        \"\"\"\n        post_processor = self.post_processor\n        post_processor.process(post, render)\n        try:\n            author = self.user_callback(post[\"user_id\"])\n        except Exception:\n            raise Exception(\"No user_loader has been installed for this \"\n                            \"BloggingEngine. Add one with the \"\n                            \"'BloggingEngine.user_loader' decorator.\")\n        if author is not None:\n            post[\"user_name\"] = self.get_user_name(author)\n        post_processed.send(self.app, engine=self, post=post, render=render)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef process(cls, post, render=True):\n        post[\"slug\"] = cls.create_slug(post[\"title\"])\n        post[\"editable\"] = cls.is_author(post, current_user)\n        post[\"url\"] = cls.construct_url(post)\n        post[\"priority\"] = 0.8\n        if render:\n            cls.render_text(post)\n            post[\"meta\"][\"images\"] = cls.extract_images(post)", "response": "This method takes the post data and renders it\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef save_post(self, title, text, user_id, tags, draft=False,\n                  post_date=None, last_modified_date=None, meta_data=None,\n                  post_id=None):\n        \"\"\"\n        Persist the blog post data. If ``post_id`` is ``None`` or ``post_id``\n        is invalid, the post must be inserted into the storage. If ``post_id``\n        is a valid id, then the data must be updated.\n\n        :param title: The title of the blog post\n        :type title: str\n        :param text: The text of the blog post\n        :type text: str\n        :param user_id: The user identifier\n        :type user_id: str\n        :param tags: A list of tags\n        :type tags: list\n        :param draft: If the post is a draft of if needs to be published.\n        :type draft: bool\n        :param post_date: (Optional) The date the blog was posted (default\n         datetime.datetime.utcnow())\n        :type post_date: datetime.datetime\n        :param last_modified_date: (Optional) The date when blog was last\n         modified  (default datetime.datetime.utcnow())\n        :type last_modified_date: datetime.datetime\n        :param meta_data: The meta data for the blog post\n        :type meta_data: dict\n        :param post_id: The post identifier. This should be ``None`` for an\n         insert call, and a valid value for update.\n        :type post_id: int\n\n        :return: The post_id value, in case of a successful insert or update.\n        Return ``None`` if there were errors.\n        \"\"\"\n        raise NotImplementedError(\"This method needs to be implemented by \"\n                                  \"the inheriting class\")", "response": "Saves the blog post data into the storage."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_posts(self, count=10, offset=0, recent=True,  tag=None,\n                  user_id=None, include_draft=False):\n        \"\"\"\n        Get posts given by filter criteria\n\n        :param count: The number of posts to retrieve (default 10). If count\n         is ``None``, all posts are returned.\n        :type count: int\n        :param offset: The number of posts to offset (default 0)\n        :type offset: int\n        :param recent: Order by recent posts or not\n        :type recent: bool\n        :param tag: Filter by a specific tag\n        :type tag: str\n        :param user_id: Filter by a specific user\n        :type user_id: str\n        :param include_draft: Whether to include posts marked as draft or not\n        :type include_draft: bool\n\n        :return: A list of posts, with each element a dict containing values\n         for the following keys: (title, text, draft, post_date,\n         last_modified_date). If count is ``None``, then all the posts are\n         returned.\n        \"\"\"\n        raise NotImplementedError(\"This method needs to be implemented by the \"\n                                  \"inheriting class\")", "response": "Get the posts given by filter criteria\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_posts(self, count=10, offset=0, recent=True, tag=None,\n                  user_id=None, include_draft=False):\n        \"\"\"TODO: implement cursors support, if it will be needed.\n           But for the regular blog, it is overhead and\n           cost savings are minimal.\n        \"\"\"\n        query = self._client.query(kind='Post')\n\n        if tag:\n            norm_tag = self.normalize_tag(tag)\n            posts_ids = self._filter_posts_by_tag(norm_tag)\n\n            if posts_ids:\n                keys = [self._client.key('Post', id) for id in posts_ids]\n                posts = self._client.get_multi(keys)\n            else:\n                posts = []\n        else:\n            if user_id:\n                query.add_filter('user_id', '=', user_id)\n            if include_draft:\n                query.add_filter('draft', '=', include_draft)\n            if recent:\n                query.order = ['-post_date']\n            posts = list(query.fetch(offset=offset, limit=count))\n\n        if not posts:\n            return []\n\n        res = []\n        for post in posts:\n            p = dict(post)\n            res.append(p)\n\n        if tag and recent:\n            res = sorted(res, key=itemgetter('post_date'), reverse=True)\n        elif tag and not recent:\n            res = sorted(res, key=itemgetter('post_date'))\n\n        if tag:\n            res = res[offset:offset+count]\n\n        return res", "response": "Get a list of posts from the blog."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save_post(self, title, text, user_id, tags, draft=False,\n                  post_date=None, last_modified_date=None, meta_data=None,\n                  post_id=None):\n        \"\"\"\n        Persist the blog post data. If ``post_id`` is ``None`` or ``post_id``\n        is invalid, the post must be inserted into the storage. If ``post_id``\n        is a valid id, then the data must be updated.\n\n        :param title: The title of the blog post\n        :type title: str\n        :param text: The text of the blog post\n        :type text: str\n        :param user_id: The user identifier\n        :type user_id: str\n        :param tags: A list of tags\n        :type tags: list\n        :param draft: (Optional) If the post is a draft of if needs to be\n         published. (default ``False``)\n        :type draft: bool\n        :param post_date: (Optional) The date the blog was posted (default\n         datetime.datetime.utcnow() )\n        :type post_date: datetime.datetime\n        :param last_modified_date: (Optional) The date when blog was last\n         modified  (default datetime.datetime.utcnow() )\n        :type last_modified_date: datetime.datetime\n        :param post_id: (Optional) The post identifier. This should be ``None``\n         for an insert call,\n         and a valid value for update. (default ``None``)\n        :type post_id: str\n\n        :return: The post_id value, in case of a successful insert or update.\n         Return ``None`` if there were errors.\n        \"\"\"\n        new_post = post_id is None\n        post_id = _as_int(post_id)\n        current_datetime = datetime.datetime.utcnow()\n        draft = 1 if draft is True else 0\n        post_date = post_date if post_date is not None else current_datetime\n        last_modified_date = last_modified_date if last_modified_date is not \\\n            None else current_datetime\n\n        with self._engine.begin() as conn:\n            try:\n                if post_id is not None:  # validate post_id\n                    exists_statement = sqla.select([self._post_table]).where(\n                        self._post_table.c.id == post_id)\n                    exists = \\\n                        conn.execute(exists_statement).fetchone() is not None\n                    post_id = post_id if exists else None\n                post_statement = \\\n                    self._post_table.insert() if post_id is None else \\\n                    self._post_table.update().where(\n                        self._post_table.c.id == post_id)\n                post_statement = post_statement.values(\n                    title=title, text=text, post_date=post_date,\n                    last_modified_date=last_modified_date, draft=draft\n                )\n\n                post_result = conn.execute(post_statement)\n                post_id = post_result.inserted_primary_key[0] \\\n                    if post_id is None else post_id\n                self._save_tags(tags, post_id, conn)\n                self._save_user_post(user_id, post_id, conn)\n\n            except Exception as e:\n                self._logger.exception(str(e))\n                post_id = None\n        return post_id", "response": "Save the blog post data."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _serialise_posts_and_tags_from_joined_rows(cls, joined_rows):\n        posts_by_id = OrderedDict()\n        tags_by_post_id = defaultdict(list)\n        for joined_row in joined_rows:\n            post_id = joined_row.post_id\n            post = cls._serialise_post_from_joined_row(joined_row)\n            posts_by_id[post_id] = post\n            tags_by_post_id[post_id].append(joined_row.tag_text)\n\n        for id, post in posts_by_id.items():\n            tags = tags_by_post_id.get(id)\n            if tags:\n                post[\"tags\"] = tags\n\n        return [post for post in posts_by_id.values()]", "response": "Converts multiple rows of joined post and tag information into a list of dicts."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfetch the blog post given by post_id.", "response": "def get_post_by_id(self, post_id):\n        \"\"\"\n        Fetch the blog post given by ``post_id``\n\n        :param post_id: The post identifier for the blog post\n        :type post_id: str\n        :return: If the ``post_id`` is valid, the post data is retrieved, else\n         returns ``None``.\n        \"\"\"\n        r = None\n        post_id = _as_int(post_id)\n        with self._engine.begin() as conn:\n            try:\n                post_statement = sqla.select([self._post_table]) \\\n                    .where(self._post_table.c.id == post_id) \\\n                    .alias('post')\n\n                joined_statement = post_statement.join(self._tag_posts_table) \\\n                    .join(self._tag_table) \\\n                    .join(self._user_posts_table) \\\n                    .alias('join')\n\n                # Note this will retrieve one row per tag\n                all_rows = conn.execute(\n                    sqla.select([joined_statement])\n                ).fetchall()\n                r = self._serialise_posts_and_tags_from_joined_rows(\n                    all_rows\n                )[0]\n\n            except Exception as e:\n                self._logger.exception(str(e))\n                r = None\n        return r"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the list of posts given by filter criteria.", "response": "def get_posts(self, count=10, offset=0, recent=True, tag=None,\n                  user_id=None, include_draft=False):\n        \"\"\"\n        Get posts given by filter criteria\n\n        :param count: The number of posts to retrieve (default 10)\n        :type count: int\n        :param offset: The number of posts to offset (default 0)\n        :type offset: int\n        :param recent: Order by recent posts or not\n        :type recent: bool\n        :param tag: Filter by a specific tag\n        :type tag: str\n        :param user_id: Filter by a specific user\n        :type user_id: str\n        :param include_draft: Whether to include posts marked as draft or not\n        :type include_draft: bool\n\n        :return: A list of posts, with each element a dict containing values\n         for the following keys: (title, text, draft, post_date,\n         last_modified_date). If count is ``None``, then all the posts are\n         returned.\n        \"\"\"\n        user_id = str(user_id) if user_id else user_id\n\n        with self._engine.begin() as conn:\n            try:\n                # post_statement ensures the correct posts are selected\n                # in the correct order\n                post_statement = sqla.select([self._post_table])\n                post_filter = self._get_filter(\n                    tag, user_id, include_draft, conn\n                )\n\n                if post_filter is not None:\n                    post_statement = post_statement.where(post_filter)\n                if count:\n                    post_statement = post_statement.limit(count)\n                if offset:\n                    post_statement = post_statement.offset(offset)\n\n                post_ordering = \\\n                    sqla.desc(self._post_table.c.post_date) if recent \\\n                    else self._post_table.c.post_date\n                post_statement = post_statement.order_by(post_ordering)\n                post_statement = post_statement.alias('post')\n\n                # joined_statement ensures other data is retrieved\n                joined_statement = post_statement.join(self._tag_posts_table) \\\n                    .join(self._tag_table) \\\n                    .join(self._user_posts_table) \\\n                    .alias('join')\n\n                joined_ordering = \\\n                    sqla.desc(joined_statement.c.post_post_date) if recent \\\n                    else joined_statement.c.post_post_date\n\n                joined_statement = sqla.select([joined_statement]) \\\n                    .order_by(joined_ordering)\n                all_rows = conn.execute(joined_statement).fetchall()\n                result = \\\n                    self._serialise_posts_and_tags_from_joined_rows(all_rows)\n            except Exception as e:\n                self._logger.exception(str(e))\n                result = []\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef count_posts(self, tag=None, user_id=None, include_draft=False):\n        result = 0\n        with self._engine.begin() as conn:\n            try:\n                count_statement = sqla.select([sqla.func.count()]). \\\n                    select_from(self._post_table)\n                sql_filter = self._get_filter(tag, user_id, include_draft,\n                                              conn)\n                count_statement = count_statement.where(sql_filter)\n                result = conn.execute(count_statement).scalar()\n            except Exception as e:\n                self._logger.exception(str(e))\n                result = 0\n        return result", "response": "Returns the total number of posts for the given filter."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes the post defined by post_id.", "response": "def delete_post(self, post_id):\n        \"\"\"\n        Delete the post defined by ``post_id``\n\n        :param post_id: The identifier corresponding to a post\n        :type post_id: int\n        :return: Returns True if the post was successfully deleted and False\n         otherwise.\n        \"\"\"\n        status = False\n        success = 0\n        post_id = _as_int(post_id)\n        with self._engine.begin() as conn:\n            try:\n                post_del_statement = self._post_table.delete().where(\n                    self._post_table.c.id == post_id)\n                conn.execute(post_del_statement)\n                success += 1\n            except Exception as e:\n                self._logger.exception(str(e))\n            try:\n                user_posts_del_statement = self._user_posts_table.delete(). \\\n                    where(self._user_posts_table.c.post_id == post_id)\n                conn.execute(user_posts_del_statement)\n                success += 1\n            except Exception as e:\n                self._logger.exception(str(e))\n            try:\n                tag_posts_del_statement = self._tag_posts_table.delete(). \\\n                    where(self._tag_posts_table.c.post_id == post_id)\n                conn.execute(tag_posts_del_statement)\n                success += 1\n            except Exception as e:\n                self._logger.exception(str(e))\n        status = success == 3\n        return status"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate all the required tables by calling the required functions.", "response": "def _create_all_tables(self):\n        \"\"\"\n        Creates all the required tables by calling the required functions.\n        :return:\n        \"\"\"\n        self._create_post_table()\n        self._create_tag_table()\n        self._create_tag_posts_table()\n        self._create_user_posts_table()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating the post table if it does not exist.", "response": "def _create_post_table(self):\n        \"\"\"\n        Creates the table to store the blog posts.\n        :return:\n        \"\"\"\n        with self._engine.begin() as conn:\n            post_table_name = self._table_name(\"post\")\n            if not conn.dialect.has_table(conn, post_table_name):\n\n                self._post_table = sqla.Table(\n                    post_table_name, self._metadata,\n                    sqla.Column(\"id\", sqla.Integer, primary_key=True),\n                    sqla.Column(\"title\", sqla.String(256)),\n                    sqla.Column(\"text\", sqla.Text),\n                    sqla.Column(\"post_date\", sqla.DateTime),\n                    sqla.Column(\"last_modified_date\", sqla.DateTime),\n                    # if 1 then make it a draft\n                    sqla.Column(\"draft\", sqla.SmallInteger, default=0),\n                    info=self._info\n\n                )\n                self._logger.debug(\"Created table with table name %s\" %\n                                   post_table_name)\n            else:\n                self._post_table = self._metadata.tables[post_table_name]\n                self._logger.debug(\"Reflecting to table with table name %s\" %\n                                   post_table_name)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate the table to store blog post tags.", "response": "def _create_tag_table(self):\n        \"\"\"\n        Creates the table to store blog post tags.\n        :return:\n        \"\"\"\n        with self._engine.begin() as conn:\n            tag_table_name = self._table_name(\"tag\")\n            if not conn.dialect.has_table(conn, tag_table_name):\n                self._tag_table = sqla.Table(\n                    tag_table_name, self._metadata,\n                    sqla.Column(\"id\", sqla.Integer, primary_key=True),\n                    sqla.Column(\"text\", sqla.String(128), unique=True,\n                                index=True),\n                    info=self._info\n                )\n                self._logger.debug(\"Created table with table name %s\" %\n                                   tag_table_name)\n            else:\n                self._tag_table = self._metadata.tables[tag_table_name]\n                self._logger.debug(\"Reflecting to table with table name %s\" %\n                                   tag_table_name)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _create_tag_posts_table(self):\n        with self._engine.begin() as conn:\n            tag_posts_table_name = self._table_name(\"tag_posts\")\n            if not conn.dialect.has_table(conn, tag_posts_table_name):\n                tag_id_key = self._table_name(\"tag\") + \".id\"\n                post_id_key = self._table_name(\"post\") + \".id\"\n                self._tag_posts_table = sqla.Table(\n                    tag_posts_table_name, self._metadata,\n                    sqla.Column('tag_id', sqla.Integer,\n                                sqla.ForeignKey(tag_id_key, onupdate=\"CASCADE\",\n                                                ondelete=\"CASCADE\"),\n                                index=True),\n                    sqla.Column('post_id', sqla.Integer,\n                                sqla.ForeignKey(post_id_key,\n                                                onupdate=\"CASCADE\",\n                                                ondelete=\"CASCADE\"),\n                                index=True),\n                    sqla.UniqueConstraint('tag_id', 'post_id', name='uix_1'),\n                    info=self._info\n                )\n                self._logger.debug(\"Created table with table name %s\" %\n                                   tag_posts_table_name)\n            else:\n                self._tag_posts_table = \\\n                    self._metadata.tables[tag_posts_table_name]\n                self._logger.debug(\"Reflecting to table with table name %s\" %\n                                   tag_posts_table_name)", "response": "Creates the table to store association info between blog posts and tags."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates the user posts table.", "response": "def _create_user_posts_table(self):\n        \"\"\"\n        Creates the table to store association info between user and blog\n        posts.\n        :return:\n        \"\"\"\n        with self._engine.begin() as conn:\n            user_posts_table_name = self._table_name(\"user_posts\")\n            if not conn.dialect.has_table(conn, user_posts_table_name):\n                post_id_key = self._table_name(\"post\") + \".id\"\n                self._user_posts_table = sqla.Table(\n                    user_posts_table_name, self._metadata,\n                    sqla.Column(\"user_id\", sqla.String(128), index=True),\n                    sqla.Column(\"post_id\", sqla.Integer,\n                                sqla.ForeignKey(post_id_key,\n                                                onupdate=\"CASCADE\",\n                                                ondelete=\"CASCADE\"),\n                                index=True),\n                    sqla.UniqueConstraint('user_id', 'post_id', name='uix_2'),\n                    info=self._info\n                )\n                self._logger.debug(\"Created table with table name %s\" %\n                                   user_posts_table_name)\n            else:\n                self._user_posts_table = \\\n                    self._metadata.tables[user_posts_table_name]\n                self._logger.debug(\"Reflecting to table with table name %s\" %\n                                   user_posts_table_name)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting input to unicode if necessary.", "response": "def ensureUtf(s, encoding='utf8'):\n    \"\"\"Converts input to unicode if necessary.\n    If `s` is bytes, it will be decoded using the `encoding` parameters.\n    This function is used for preprocessing /source/ and /filename/ arguments\n    to the builtin function `compile`.\n    \"\"\"\n    # In Python2, str == bytes.\n    # In Python3, bytes remains unchanged, but str means unicode\n    # while unicode is not defined anymore\n    if type(s) == bytes:\n        return s.decode(encoding, 'ignore')\n    else:\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nserves the page with a list of blog posts", "response": "def index(count, page):\n    \"\"\"\n    Serves the page with a list of blog posts\n\n    :param count:\n    :param offset:\n    :return:\n    \"\"\"\n    blogging_engine = _get_blogging_engine(current_app)\n    storage = blogging_engine.storage\n    config = blogging_engine.config\n    count = count or config.get(\"BLOGGING_POSTS_PER_PAGE\", 10)\n\n    meta = _get_meta(storage, count, page)\n    offset = meta[\"offset\"]\n    meta[\"is_user_blogger\"] = _is_blogger(blogging_engine.blogger_permission)\n    meta[\"count\"] = count\n    meta[\"page\"] = page\n\n    render = config.get(\"BLOGGING_RENDER_TEXT\", True)\n    posts = storage.get_posts(count=count, offset=offset, include_draft=False,\n                              tag=None, user_id=None, recent=True)\n    index_posts_fetched.send(blogging_engine.app, engine=blogging_engine,\n                             posts=posts, meta=meta)\n    for post in posts:\n        blogging_engine.process_post(post, render=render)\n    index_posts_processed.send(blogging_engine.app, engine=blogging_engine,\n                               posts=posts, meta=meta)\n    return render_template(\"blogging/index.html\", posts=posts, meta=meta,\n                           config=config)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a StripeObject to a regular dict.", "response": "def convert_to_dict(obj):\n    \"\"\"Converts a StripeObject back to a regular dict.\n\n    Nested StripeObjects are also converted back to regular dicts.\n\n    :param obj: The StripeObject to convert.\n\n    :returns: The StripeObject as a dict.\n    \"\"\"\n    if isinstance(obj, list):\n        return [convert_to_dict(i) for i in obj]\n    # This works by virtue of the fact that StripeObjects _are_ dicts. The dict\n    # comprehension returns a regular dict and recursively applies the\n    # conversion to each value.\n    elif isinstance(obj, dict):\n        return {k: convert_to_dict(v) for k, v in six.iteritems(obj)}\n    else:\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef request_raw(self, method, url, params=None, supplied_headers=None):\n\n        if self.api_key:\n            my_api_key = self.api_key\n        else:\n            from stripe import api_key\n\n            my_api_key = api_key\n\n        if my_api_key is None:\n            raise error.AuthenticationError(\n                \"No API key provided. (HINT: set your API key using \"\n                '\"stripe.api_key = <API-KEY>\"). You can generate API keys '\n                \"from the Stripe web interface.  See https://stripe.com/api \"\n                \"for details, or email support@stripe.com if you have any \"\n                \"questions.\"\n            )\n\n        abs_url = \"%s%s\" % (self.api_base, url)\n\n        encoded_params = urlencode(list(_api_encode(params or {})))\n\n        # Don't use strict form encoding by changing the square bracket control\n        # characters back to their literals. This is fine by the server, and\n        # makes these parameter strings easier to read.\n        encoded_params = encoded_params.replace(\"%5B\", \"[\").replace(\"%5D\", \"]\")\n\n        if method == \"get\" or method == \"delete\":\n            if params:\n                abs_url = _build_api_url(abs_url, encoded_params)\n            post_data = None\n        elif method == \"post\":\n            if (\n                supplied_headers is not None\n                and supplied_headers.get(\"Content-Type\")\n                == \"multipart/form-data\"\n            ):\n                generator = MultipartDataGenerator()\n                generator.add_params(params or {})\n                post_data = generator.get_post_data()\n                supplied_headers[\n                    \"Content-Type\"\n                ] = \"multipart/form-data; boundary=%s\" % (generator.boundary,)\n            else:\n                post_data = encoded_params\n        else:\n            raise error.APIConnectionError(\n                \"Unrecognized HTTP method %r.  This may indicate a bug in the \"\n                \"Stripe bindings.  Please contact support@stripe.com for \"\n                \"assistance.\" % (method,)\n            )\n\n        headers = self.request_headers(my_api_key, method)\n        if supplied_headers is not None:\n            for key, value in six.iteritems(supplied_headers):\n                headers[key] = value\n\n        util.log_info(\"Request to Stripe api\", method=method, path=abs_url)\n        util.log_debug(\n            \"Post details\",\n            post_data=encoded_params,\n            api_version=self.api_version,\n        )\n\n        rbody, rcode, rheaders = self._client.request_with_retries(\n            method, abs_url, headers, post_data\n        )\n\n        util.log_info(\"Stripe API response\", path=abs_url, response_code=rcode)\n        util.log_debug(\"API response body\", body=rbody)\n\n        if \"Request-Id\" in rheaders:\n            request_id = rheaders[\"Request-Id\"]\n            util.log_debug(\n                \"Dashboard link for request\",\n                link=util.dashboard_link(request_id),\n            )\n\n        return rbody, rcode, rheaders, my_api_key", "response": "Makes a request to the Stripe API."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a new instance of the given driver_name with the given arguments and keyword arguments.", "response": "def Browser(driver_name=\"firefox\", *args, **kwargs):\n    \"\"\"\n    Returns a driver instance for the given name.\n\n    When working with ``firefox``, it's possible to provide a profile name\n    and a list of extensions.\n\n    If you don't provide any driver_name, then ``firefox`` will be used.\n\n    If there is no driver registered with the provided ``driver_name``, this\n    function will raise a :class:`splinter.exceptions.DriverNotFoundError`\n    exception.\n    \"\"\"\n\n    try:\n        driver = _DRIVERS[driver_name]\n    except KeyError:\n        raise DriverNotFoundError(\"No driver for %s\" % driver_name)\n    return driver(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef title(self):\n        with switch_window(self._browser, self.name):\n            return self._browser.title", "response": "The title of this window."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef prev(self):\n        prev_index = self.index - 1\n        prev_handle = self._browser.driver.window_handles[prev_index]\n        return Window(self._browser, prev_handle)", "response": "Return the previous window"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef next(self):\n        next_index = (self.index + 1) % len(self._browser.driver.window_handles)\n        next_handle = self._browser.driver.window_handles[next_index]\n        return Window(self._browser, next_handle)", "response": "Return the next window"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef close(self):\n        target = self.prev if (self.is_current and self.prev != self) else None\n\n        with switch_window(self._browser, self.name):\n            self._browser.driver.close()\n\n        if target is not None:\n            target.is_current = True", "response": "Close this window. If this window is active switch to previous window."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nperforms a mouse over the element.", "response": "def mouse_over(self):\n        \"\"\"\n        Performs a mouse over the element.\n\n        Currently works only on Chrome driver.\n        \"\"\"\n        self.scroll_to()\n        ActionChains(self.parent.driver).move_to_element(self._element).perform()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mouse_out(self):\n        self.scroll_to()\n        ActionChains(self.parent.driver).move_by_offset(0, 0).click().perform()", "response": "Performs a mouse out the element."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nperforming a double click in the element.", "response": "def double_click(self):\n        \"\"\"\n        Performs a double click in the element.\n\n        Currently works only on Chrome driver.\n        \"\"\"\n        self.scroll_to()\n        ActionChains(self.parent.driver).double_click(self._element).perform()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nperform a right click in the element.", "response": "def right_click(self):\n        \"\"\"\n        Performs a right click in the element.\n\n        Currently works only on Chrome driver.\n        \"\"\"\n        self.scroll_to()\n        ActionChains(self.parent.driver).context_click(self._element).perform()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nperforms drag a element to another elmenet.", "response": "def drag_and_drop(self, droppable):\n        \"\"\"\n        Performs drag a element to another elmenet.\n\n        Currently works only on Chrome driver.\n        \"\"\"\n        self.scroll_to()\n        ActionChains(self.parent.driver).drag_and_drop(self._element, droppable._element).perform()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef force_unicode(value):\n    if IS_PY3:\n        # Python 3.X\n        if isinstance(value, bytes):\n            value = value.decode('utf-8', errors='replace')\n        elif not isinstance(value, str):\n            value = str(value)\n    else:\n        # Python 2.X\n        if isinstance(value, str):\n            value = value.decode('utf-8', 'replace')\n        elif not isinstance(value, basestring):  # NOQA: F821\n            value = unicode(value)  # NOQA: F821\n\n    return value", "response": "Force a bytestring to become a Unicode string."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef force_bytes(value):\n    if IS_PY3:\n        if isinstance(value, str):\n            value = value.encode('utf-8', 'backslashreplace')\n    else:\n        if isinstance(value, unicode):  # NOQA: F821\n            value = value.encode('utf-8')\n\n    return value", "response": "Force a Unicode string to become a bytestring."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef unescape_html(text):\n    def fixup(m):\n        text = m.group(0)\n        if text[:2] == \"&#\":\n            # character reference\n            try:\n                if text[:3] == \"&#x\":\n                    return unicode_char(int(text[3:-1], 16))\n                else:\n                    return unicode_char(int(text[2:-1]))\n            except ValueError:\n                pass\n        else:\n            # named entity\n            try:\n                text = unicode_char(htmlentities.name2codepoint[text[1:-1]])\n            except KeyError:\n                pass\n        return text  # leave as is\n    return re.sub(r\"&#?\\w+;\", fixup, text)", "response": "Unescapes HTML or XML character references and entities from a text string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef safe_urlencode(params, doseq=0):\n    if IS_PY3:\n        return urlencode(params, doseq)\n\n    if hasattr(params, \"items\"):\n        params = params.items()\n\n    new_params = []\n\n    for k, v in params:\n        k = k.encode(\"utf-8\")\n\n        if isinstance(v, (list, tuple)):\n            new_params.append((k, [force_bytes(i) for i in v]))\n        else:\n            new_params.append((k, force_bytes(v)))\n\n    return urlencode(new_params, doseq)", "response": "URL encode a dictionary of parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _select(self, params, handler=None):\n        # specify json encoding of results\n        params['wt'] = 'json'\n        custom_handler = handler or self.search_handler\n        handler = 'select'\n        if custom_handler:\n            if self.use_qt_param:\n                params['qt'] = custom_handler\n            else:\n                handler = custom_handler\n\n        params_encoded = safe_urlencode(params, True)\n\n        if len(params_encoded) < 1024:\n            # Typical case.\n            path = '%s/?%s' % (handler, params_encoded)\n            return self._send_request('get', path)\n        else:\n            # Handles very long queries by submitting as a POST.\n            path = '%s/' % handler\n            headers = {\n                'Content-type': 'application/x-www-form-urlencoded; charset=utf-8',\n            }\n            return self._send_request('post', path, body=params_encoded, headers=headers)", "response": "Select the related object from the database."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _update(self, message, clean_ctrl_chars=True, commit=None, softCommit=False, waitFlush=None, waitSearcher=None,\n                overwrite=None, handler='update'):\n        \"\"\"\n        Posts the given xml message to http://<self.url>/update and\n        returns the result.\n\n        Passing `clean_ctrl_chars` as False will prevent the message from being cleaned\n        of control characters (default True). This is done by default because\n        these characters would cause Solr to fail to parse the XML. Only pass\n        False if you're positive your data is clean.\n        \"\"\"\n\n        # Per http://wiki.apache.org/solr/UpdateXmlMessages, we can append a\n        # ``commit=true`` to the URL and have the commit happen without a\n        # second request.\n        query_vars = []\n\n        path_handler = handler\n        if self.use_qt_param:\n            path_handler = 'select'\n            query_vars.append('qt=%s' % safe_urlencode(handler, True))\n\n        path = '%s/' % path_handler\n\n        if commit is None:\n            commit = self.always_commit\n\n        if commit:\n            query_vars.append('commit=%s' % str(bool(commit)).lower())\n        elif softCommit:\n            query_vars.append('softCommit=%s' % str(bool(softCommit)).lower())\n\n        if waitFlush is not None:\n            query_vars.append('waitFlush=%s' % str(bool(waitFlush)).lower())\n\n        if overwrite is not None:\n            query_vars.append('overwrite=%s' % str(bool(overwrite)).lower())\n\n        if waitSearcher is not None:\n            query_vars.append('waitSearcher=%s' % str(bool(waitSearcher)).lower())\n\n        if query_vars:\n            path = '%s?%s' % (path, '&'.join(query_vars))\n\n        # Clean the message of ctrl characters.\n        if clean_ctrl_chars:\n            message = sanitize(message)\n\n        return self._send_request('post', path, message, {'Content-type': 'text/xml; charset=utf-8'})", "response": "Update the xml message with the given message."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _extract_error(self, resp):\n        reason = resp.headers.get('reason', None)\n        full_response = None\n\n        if reason is None:\n            try:\n                # if response is in json format\n                reason = resp.json()['error']['msg']\n            except KeyError:\n                # if json response has unexpected structure\n                full_response = resp.content\n            except ValueError:\n                # otherwise we assume it's html\n                reason, full_html = self._scrape_response(resp.headers, resp.content)\n                full_response = unescape_html(full_html)\n\n        msg = \"[Reason: %s]\" % reason\n\n        if reason is None:\n            msg += \"\\n%s\" % full_response\n\n        return msg", "response": "Extract the actual error message from a solr response."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nscrapes the response and return the reason and full html.", "response": "def _scrape_response(self, headers, response):\n        \"\"\"\n        Scrape the html response.\n        \"\"\"\n        # identify the responding server\n        server_type = None\n        server_string = headers.get('server', '')\n\n        if server_string and 'jetty' in server_string.lower():\n            server_type = 'jetty'\n\n        if server_string and 'coyote' in server_string.lower():\n            server_type = 'tomcat'\n\n        reason = None\n        full_html = ''\n        dom_tree = None\n\n        # In Python3, response can be made of bytes\n        if IS_PY3 and hasattr(response, 'decode'):\n            response = response.decode()\n        if response.startswith('<?xml'):\n            # Try a strict XML parse\n            try:\n                soup = ElementTree.fromstring(response)\n\n                reason_node = soup.find('lst[@name=\"error\"]/str[@name=\"msg\"]')\n                tb_node = soup.find('lst[@name=\"error\"]/str[@name=\"trace\"]')\n                if reason_node is not None:\n                    full_html = reason = reason_node.text.strip()\n                if tb_node is not None:\n                    full_html = tb_node.text.strip()\n                    if reason is None:\n                        reason = full_html\n\n                # Since we had a precise match, we'll return the results now:\n                if reason and full_html:\n                    return reason, full_html\n            except ElementTree.ParseError:\n                # XML parsing error, so we'll let the more liberal code handle it.\n                pass\n\n        if server_type == 'tomcat':\n            # Tomcat doesn't produce a valid XML response or consistent HTML:\n            m = re.search(r'<(h1)[^>]*>\\s*(.+?)\\s*</\\1>', response, re.IGNORECASE)\n            if m:\n                reason = m.group(2)\n            else:\n                full_html = \"%s\" % response\n        else:\n            # Let's assume others do produce a valid XML response\n            try:\n                dom_tree = ElementTree.fromstring(response)\n                reason_node = None\n\n                # html page might be different for every server\n                if server_type == 'jetty':\n                    reason_node = dom_tree.find('body/pre')\n                else:\n                    reason_node = dom_tree.find('head/title')\n\n                if reason_node is not None:\n                    reason = reason_node.text\n\n                if reason is None:\n                    full_html = ElementTree.tostring(dom_tree)\n            except SyntaxError as err:\n                LOG.warning('Unable to extract error message from invalid XML: %s', err,\n                            extra={'data': {'response': response}})\n                full_html = \"%s\" % response\n\n        full_html = force_unicode(full_html)\n        full_html = full_html.replace('\\n', '')\n        full_html = full_html.replace('\\r', '')\n        full_html = full_html.replace('<br/>', '')\n        full_html = full_html.replace('<br />', '')\n        full_html = full_html.strip()\n        return reason, full_html"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _from_python(self, value):\n        if hasattr(value, 'strftime'):\n            if hasattr(value, 'hour'):\n                offset = value.utcoffset()\n                if offset:\n                    value = value - offset\n                value = value.replace(tzinfo=None).isoformat() + 'Z'\n            else:\n                value = \"%sT00:00:00Z\" % value.isoformat()\n        elif isinstance(value, bool):\n            if value:\n                value = 'true'\n            else:\n                value = 'false'\n        else:\n            if IS_PY3:\n                # Python 3.X\n                if isinstance(value, bytes):\n                    value = str(value, errors='replace')  # NOQA: F821\n            else:\n                # Python 2.X\n                if isinstance(value, str):\n                    value = unicode(value, errors='replace')   # NOQA: F821\n\n            value = \"{0}\".format(value)\n\n        return clean_xml_string(value)", "response": "Converts python values to a form suitable for insertion into solr."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts values from Solr to native Python values.", "response": "def _to_python(self, value):\n        \"\"\"\n        Converts values from Solr to native Python values.\n        \"\"\"\n        if isinstance(value, (int, float, long, complex)):\n            return value\n\n        if isinstance(value, (list, tuple)):\n            value = value[0]\n\n        if value == 'true':\n            return True\n        elif value == 'false':\n            return False\n\n        is_string = False\n\n        if IS_PY3:\n            if isinstance(value, bytes):\n                value = force_unicode(value)\n\n            if isinstance(value, str):\n                is_string = True\n        else:\n            if isinstance(value, str):\n                value = force_unicode(value)\n\n            if isinstance(value, basestring):  # NOQA: F821\n                is_string = True\n\n        if is_string:\n            possible_datetime = DATETIME_REGEX.search(value)\n\n            if possible_datetime:\n                date_values = possible_datetime.groupdict()\n\n                for dk, dv in date_values.items():\n                    date_values[dk] = int(dv)\n\n                return datetime.datetime(date_values['year'],\n                                         date_values['month'],\n                                         date_values['day'],\n                                         date_values['hour'],\n                                         date_values['minute'],\n                                         date_values['second'])\n\n        try:\n            # This is slightly gross but it's hard to tell otherwise what the\n            # string's original type might have been.\n            return ast.literal_eval(value)\n        except (ValueError, SyntaxError):\n            # If it fails, continue on.\n            pass\n\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if a given value is null.", "response": "def _is_null_value(self, value):\n        \"\"\"\n        Check if a given value is ``null``.\n\n        Criteria for this is based on values that shouldn't be included\n        in the Solr ``add`` request at all.\n        \"\"\"\n        if value is None:\n            return True\n\n        if IS_PY3:\n            # Python 3.X\n            if isinstance(value, str) and len(value) == 0:\n                return True\n        else:\n            # Python 2.X\n            if isinstance(value, basestring) and len(value) == 0:  # NOQA: F821\n                return True\n\n        # TODO: This should probably be removed when solved in core Solr level?\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nperforming a search and returns the results.", "response": "def search(self, q, search_handler=None, **kwargs):\n        \"\"\"\n        Performs a search and returns the results.\n\n        Requires a ``q`` for a string version of the query to run.\n\n        Optionally accepts ``**kwargs`` for additional options to be passed\n        through the Solr URL.\n\n        Returns ``self.results_cls`` class object (defaults to\n        ``pysolr.Results``)\n\n        Usage::\n\n            # All docs.\n            results = solr.search('*:*')\n\n            # Search with highlighting.\n            results = solr.search('ponies', **{\n                'hl': 'true',\n                'hl.fragsize': 10,\n            })\n\n        \"\"\"\n        params = {'q': q}\n        params.update(kwargs)\n        response = self._select(params, handler=search_handler)\n        decoded = self.decoder.decode(response)\n\n        self.log.debug(\n            \"Found '%s' search results.\",\n            # cover both cases: there is no response key or value is None\n            (decoded.get('response', {}) or {}).get('numFound', 0)\n        )\n        return self.results_cls(decoded)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef more_like_this(self, q, mltfl, handler='mlt', **kwargs):\n        params = {\n            'q': q,\n            'mlt.fl': mltfl,\n        }\n        params.update(kwargs)\n        response = self._mlt(params, handler=handler)\n        decoded = self.decoder.decode(response)\n\n        self.log.debug(\n            \"Found '%s' MLT results.\",\n            # cover both cases: there is no response key or value is None\n            (decoded.get('response', {}) or {}).get('numFound', 0)\n        )\n        return self.results_cls(decoded)", "response": "Finds and returns results similar to the provided query."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef suggest_terms(self, fields, prefix, handler='terms', **kwargs):\n        params = {\n            'terms.fl': fields,\n            'terms.prefix': prefix,\n        }\n        params.update(kwargs)\n        response = self._suggest_terms(params, handler=handler)\n        result = self.decoder.decode(response)\n        terms = result.get(\"terms\", {})\n        res = {}\n\n        # in Solr 1.x the value of terms is a flat list:\n        #   [\"field_name\", [\"dance\",23,\"dancers\",10,\"dancing\",8,\"dancer\",6]]\n        #\n        # in Solr 3.x the value of terms is a dict:\n        #   {\"field_name\": [\"dance\",23,\"dancers\",10,\"dancing\",8,\"dancer\",6]}\n        if isinstance(terms, (list, tuple)):\n            terms = dict(zip(terms[0::2], terms[1::2]))\n\n        for field, values in terms.items():\n            tmp = []\n\n            while values:\n                tmp.append((values.pop(0), values.pop(0)))\n\n            res[field] = tmp\n\n        self.log.debug(\"Found '%d' Term suggestions results.\", sum(len(j) for i, j in res.items()))\n        return res", "response": "Suggests the terms for a list of field names and a prefix."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add(self, docs, boost=None, fieldUpdates=None, commit=None, softCommit=False, commitWithin=None, waitFlush=None,\n            waitSearcher=None, overwrite=None, handler='update'):\n        \"\"\"\n        Adds or updates documents.\n\n        Requires ``docs``, which is a list of dictionaries. Each key is the\n        field name and each value is the value to index.\n\n        Optionally accepts ``commit``. Default is ``None``. None signals to use default\n\n        Optionally accepts ``softCommit``. Default is ``False``.\n\n        Optionally accepts ``boost``. Default is ``None``.\n\n        Optionally accepts ``fieldUpdates``. Default is ``None``.\n\n        Optionally accepts ``commitWithin``. Default is ``None``.\n\n        Optionally accepts ``waitFlush``. Default is ``None``.\n\n        Optionally accepts ``waitSearcher``. Default is ``None``.\n\n        Optionally accepts ``overwrite``. Default is ``None``.\n\n        Usage::\n\n            solr.add([\n                {\n                    \"id\": \"doc_1\",\n                    \"title\": \"A test document\",\n                },\n                {\n                    \"id\": \"doc_2\",\n                    \"title\": \"The Banana: Tasty or Dangerous?\",\n                },\n            ])\n        \"\"\"\n        start_time = time.time()\n        self.log.debug(\"Starting to build add request...\")\n        message = ElementTree.Element('add')\n\n        if commitWithin:\n            message.set('commitWithin', commitWithin)\n\n        for doc in docs:\n            el = self._build_doc(doc, boost=boost, fieldUpdates=fieldUpdates)\n            message.append(el)\n\n        # This returns a bytestring. Ugh.\n        m = ElementTree.tostring(message, encoding='utf-8')\n        # Convert back to Unicode please.\n        m = force_unicode(m)\n\n        end_time = time.time()\n        self.log.debug(\"Built add request of %s docs in %0.2f seconds.\", len(message), end_time - start_time)\n        return self._update(m, commit=commit, softCommit=softCommit, waitFlush=waitFlush, waitSearcher=waitSearcher,\n                            overwrite=overwrite, handler=handler)", "response": "Adds or updates documents."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting all the related entries in the cache.", "response": "def delete(self, id=None, q=None, commit=None, softCommit=False, waitFlush=None, waitSearcher=None, handler='update'):  # NOQA: A002\n        \"\"\"\n        Deletes documents.\n\n        Requires *either* ``id`` or ``query``. ``id`` is if you know the\n        specific document id to remove. Note that ``id`` can also be a list of\n        document ids to be deleted. ``query`` is a Lucene-style query\n        indicating a collection of documents to delete.\n\n        Optionally accepts ``commit``. Default is ``True``.\n\n        Optionally accepts ``softCommit``. Default is ``False``.\n\n        Optionally accepts ``waitFlush``. Default is ``None``.\n\n        Optionally accepts ``waitSearcher``. Default is ``None``.\n\n        Usage::\n\n            solr.delete(id='doc_12')\n            solr.delete(id=['doc_1', 'doc_3'])\n            solr.delete(q='*:*')\n\n        \"\"\"\n        if id is None and q is None:\n            raise ValueError('You must specify \"id\" or \"q\".')\n        elif id is not None and q is not None:\n            raise ValueError('You many only specify \"id\" OR \"q\", not both.')\n        elif id is not None:\n            if not isinstance(id, (list, set, tuple)):\n                doc_id = [id]\n            else:\n                doc_id = list(filter(None, id))\n            if doc_id:\n                m = '<delete>%s</delete>' % ''.join('<id>%s</id>' % i for i in doc_id)\n            else:\n                raise ValueError('The list of documents to delete was empty.')\n        elif q is not None:\n            m = '<delete><query>%s</query></delete>' % q\n\n        return self._update(m, commit=commit, softCommit=softCommit, waitFlush=waitFlush, waitSearcher=waitSearcher, handler=handler)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef commit(self, softCommit=False, waitFlush=None, waitSearcher=None, expungeDeletes=None, handler='update'):\n        if expungeDeletes is not None:\n            msg = '<commit expungeDeletes=\"%s\" />' % str(bool(expungeDeletes)).lower()\n        else:\n            msg = '<commit />'\n\n        return self._update(msg, commit=not softCommit, softCommit=softCommit, waitFlush=waitFlush, waitSearcher=waitSearcher,\n                            handler=handler)", "response": "Commits the current index data to Solr."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef optimize(self, commit=True, waitFlush=None, waitSearcher=None, maxSegments=None, handler='update'):\n        if maxSegments:\n            msg = '<optimize maxSegments=\"%d\" />' % maxSegments\n        else:\n            msg = '<optimize />'\n\n        return self._update(msg, commit=commit, waitFlush=waitFlush, waitSearcher=waitSearcher, handler=handler)", "response": "Optimize the number of segments used by a single entry in a set of keys."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nextract the contents of a file - like object into a dictionary of the record s metadata.", "response": "def extract(self, file_obj, extractOnly=True, handler='update/extract', **kwargs):\n        \"\"\"\n        POSTs a file to the Solr ExtractingRequestHandler so rich content can\n        be processed using Apache Tika. See the Solr wiki for details:\n\n            http://wiki.apache.org/solr/ExtractingRequestHandler\n\n        The ExtractingRequestHandler has a very simple model: it extracts\n        contents and metadata from the uploaded file and inserts it directly\n        into the index. This is rarely useful as it allows no way to store\n        additional data or otherwise customize the record. Instead, by default\n        we'll use the extract-only mode to extract the data without indexing it\n        so the caller has the opportunity to process it as appropriate; call\n        with ``extractOnly=False`` if you want to insert with no additional\n        processing.\n\n        Returns None if metadata cannot be extracted; otherwise returns a\n        dictionary containing at least two keys:\n\n            :contents:\n                        Extracted full-text content, if applicable\n            :metadata:\n                        key:value pairs of text strings\n        \"\"\"\n        if not hasattr(file_obj, \"name\"):\n            raise ValueError(\"extract() requires file-like objects which have a defined name property\")\n\n        params = {\n            \"extractOnly\": \"true\" if extractOnly else \"false\",\n            \"lowernames\": \"true\",\n            \"wt\": \"json\",\n        }\n        params.update(kwargs)\n        filename = quote(file_obj.name.encode('utf-8'))\n        try:\n            # We'll provide the file using its true name as Tika may use that\n            # as a file type hint:\n            resp = self._send_request('post', handler,\n                                      body=params,\n                                      files={'file': (filename, file_obj)})\n        except (IOError, SolrError):\n            self.log.exception(\"Failed to extract document metadata\")\n            raise\n\n        try:\n            data = json.loads(resp)\n        except ValueError:\n            self.log.exception(\"Failed to load JSON response\")\n            raise\n\n        data['contents'] = data.pop(filename, None)\n        data['metadata'] = metadata = {}\n\n        raw_metadata = data.pop(\"%s_metadata\" % filename, None)\n\n        if raw_metadata:\n            # The raw format is somewhat annoying: it's a flat list of\n            # alternating keys and value lists\n            while raw_metadata:\n                metadata[raw_metadata.pop()] = raw_metadata.pop()\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend a ping request.", "response": "def ping(self, handler='admin/ping', **kwargs):\n        \"\"\"\n        Sends a ping request.\n\n        Usage::\n\n            solr.ping()\n\n        \"\"\"\n        params = kwargs\n        params_encoded = safe_urlencode(params, True)\n\n        if len(params_encoded) < 1024:\n            # Typical case.\n            path = '%s/?%s' % (handler, params_encoded)\n            return self._send_request('get', path)\n        else:\n            # Handles very long queries by submitting as a POST.\n            path = '%s/' % handler\n            headers = {\n                'Content-type': 'application/x-www-form-urlencoded; charset=utf-8',\n            }\n            return self._send_request('post', path, body=params_encoded, headers=headers)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the status of the current user.", "response": "def status(self, core=None):\n        \"\"\"http://wiki.apache.org/solr/CoreAdmin#head-9be76f5a459882c5c093a7a1456e98bea7723953\"\"\"\n        params = {\n            'action': 'STATUS',\n        }\n\n        if core is not None:\n            params.update(core=core)\n\n        return self._get_url(self.url, params=params)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create(self, name, instance_dir=None, config='solrconfig.xml', schema='schema.xml'):\n        params = {\n            'action': 'CREATE',\n            'name': name,\n            'config': config,\n            'schema': schema,\n        }\n\n        if instance_dir is None:\n            params.update(instanceDir=name)\n        else:\n            params.update(instanceDir=instance_dir)\n\n        return self._get_url(self.url, params=params)", "response": "Create a new entry in the system."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreloading the related object for the specified core.", "response": "def reload(self, core):  # NOQA: A003\n        \"\"\"http://wiki.apache.org/solr/CoreAdmin#head-3f125034c6a64611779442539812067b8b430930\"\"\"\n        params = {\n            'action': 'RELOAD',\n            'core': core,\n        }\n        return self._get_url(self.url, params=params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef swap(self, core, other):\n        params = {\n            'action': 'SWAP',\n            'core': core,\n            'other': other,\n        }\n        return self._get_url(self.url, params=params)", "response": "This method returns the url of the SWAP request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nunload a specific object from the solr server.", "response": "def unload(self, core):\n        \"\"\"http://wiki.apache.org/solr/CoreAdmin#head-f5055a885932e2c25096a8856de840b06764d143\"\"\"\n        params = {\n            'action': 'UNLOAD',\n            'core': core,\n        }\n        return self._get_url(self.url, params=params)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef format_response(self, response):\n        conversion = self.shell_ctx.config.BOOLEAN_STATES\n        if response in conversion:\n            if conversion[response]:\n                return 'yes'\n            return 'no'\n        raise ValueError('Invalid response: input should equate to true or false')", "response": "formats a response in a binary"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _retry(self, context, backoff):\n        '''\n        A function which determines whether and how to retry.\n\n        :param ~azure.storage.models.RetryContext context: \n            The retry context. This contains the request, response, and other data \n            which can be used to determine whether or not to retry.\n        :param function() backoff:\n            A function which returns the backoff time if a retry is to be performed.\n        :return: \n            An integer indicating how long to wait before retrying the request, \n            or None to indicate no retry should be performed.\n        :rtype: int or None\n        '''\n        # If the context does not contain a count parameter, this request has not \n        # been retried yet. Add the count parameter to track the number of retries.\n        if not hasattr(context, 'count'):\n            context.count = 0\n\n        # Determine whether to retry, and if so increment the count, modify the \n        # request as desired, and return the backoff.\n        if self._should_retry(context):\n            backoff_interval = backoff(context)\n            context.count += 1\n\n            # If retry to secondary is enabled, attempt to change the host if the \n            # request allows it\n            if self.retry_to_secondary:\n                self._set_next_host_location(context)\n\n            # rewind the request body if it is a stream\n            if hasattr(context.request.body, 'read'):\n                # no position was saved, then retry would not work\n                if context.body_position is None:\n                    return None\n                else:\n                    try:\n                        # attempt to rewind the body to the initial position\n                        context.request.body.seek(context.body_position, SEEK_SET)\n                    except UnsupportedOperation:\n                        # if body is not seekable, then retry would not work\n                        return None\n\n            return backoff_interval\n\n        return None", "response": "Attempts to retry the specified data in the specified context."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmake the file url using the service and returns the file directory and name as a tuple.", "response": "def make_encoded_file_url_and_params(file_service, share, file_dir, file_name, sas_token, safe=SAFE_CHARS):\n    \"\"\"\n    Makes the file url using the service. Converts the file directory and name into byte-strings if needed and returns\n    (url, dir, file) as a tuple. This is needed to account for string encoding differences between python 2 and 3.\n    \"\"\"\n    try:\n        file_url = file_service.make_file_url(share, file_dir, file_name, sas_token=sas_token)\n    except UnicodeEncodeError:\n        file_dir = file_dir.encode('utf-8')\n        file_name = file_name.encode('utf-8')\n        file_url = file_service.make_file_url(share, file_dir, file_name, sas_token=sas_token)\n\n    if not file_dir:\n        sep = file_url.find('://')\n        file_url = file_url[:sep + 3] + file_url[sep + 3:].replace('//', '/')\n    return encode_url_path(file_url, safe), file_dir, file_name"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the dimensions depending on python version and os", "response": "def get_window_dim():\n    \"\"\" gets the dimensions depending on python version and os\"\"\"\n    version = sys.version_info\n\n    if version >= (3, 3):\n        return _size_36()\n    if platform.system() == 'Windows':\n        return _size_windows()\n    return _size_27()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the rows and columns of the last 36 lines of the terminal", "response": "def _size_36():\n    \"\"\" returns the rows, columns of terminal \"\"\"\n    from shutil import get_terminal_size\n    dim = get_terminal_size()\n    if isinstance(dim, list):\n        return dim[0], dim[1]\n    return dim.lines, dim.columns"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating the frequency from files", "response": "def update_frequency(shell_ctx):\n    \"\"\" updates the frequency from files \"\"\"\n    frequency_path = os.path.join(shell_ctx.config.get_config_dir(), shell_ctx.config.get_frequency())\n    if os.path.exists(frequency_path):\n        with open(frequency_path, 'r') as freq:\n            try:\n                frequency = json.load(freq)\n            except ValueError:\n                frequency = {}\n    else:\n        frequency = {}\n\n    with open(frequency_path, 'w') as freq:\n        now = day_format(datetime.datetime.utcnow())\n        val = frequency.get(now)\n        frequency[now] = val + 1 if val else 1\n        json.dump(frequency, freq)\n\n    return frequency"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmeasure how many times a user has used this program in the last calendar week", "response": "def frequency_measurement(shell_ctx):\n    \"\"\" measures how many times a user has used this program in the last calendar week \"\"\"\n    freq = update_frequency(shell_ctx)\n    count = 0\n    base = datetime.datetime.utcnow()\n    date_list = [base - datetime.timedelta(days=x) for x in range(0, DAYS_AGO)]\n    for day in date_list:\n        count += 1 if freq.get(day_format(day), 0) > 0 else 0\n\n    return count"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_public_ip_validator():\n    from msrestazure.tools import is_valid_resource_id, resource_id\n\n    def simple_validator(cmd, namespace):\n        if namespace.public_ip_address:\n            is_list = isinstance(namespace.public_ip_address, list)\n\n            def _validate_name_or_id(public_ip):\n                # determine if public_ip_address is name or ID\n                is_id = is_valid_resource_id(public_ip)\n                return public_ip if is_id else resource_id(\n                    subscription=get_subscription_id(cmd.cli_ctx),\n                    resource_group=namespace.resource_group_name,\n                    namespace='Microsoft.Network',\n                    type='publicIPAddresses',\n                    name=public_ip)\n\n            if is_list:\n                for i, public_ip in enumerate(namespace.public_ip_address):\n                    namespace.public_ip_address[i] = _validate_name_or_id(public_ip)\n            else:\n                namespace.public_ip_address = _validate_name_or_id(namespace.public_ip_address)\n\n    return simple_validator", "response": "Returns a validator function for public IP address."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading all the extra information from help files", "response": "def load_help_files(data):\n    \"\"\" loads all the extra information from help files \"\"\"\n    for command_name, help_yaml in helps.items():\n\n        help_entry = yaml.safe_load(help_yaml)\n        try:\n            help_type = help_entry['type']\n        except KeyError:\n            continue\n\n        # if there is extra help for this command but it's not reflected in the command table\n        if command_name not in data and help_type == 'command':\n            logger.debug('Command: %s not found in command table', command_name)\n            continue\n\n        short_summary = help_entry.get('short-summary')\n        if short_summary and help_type == 'command':\n            data[command_name]['help'] = short_summary\n        else:\n            # must be a command group or sub-group\n            data[command_name] = {'help': short_summary}\n            continue\n\n        if 'parameters' in help_entry:\n            for param in help_entry['parameters']:\n                # this could fail if the help file and options list are not in the same order\n                param_name = param['name'].split()[0]\n\n                if param_name not in data[command_name]['parameters']:\n                    logger.debug('Command %s does not have parameter: %s', command_name, param_name)\n                    continue\n\n                if 'short-summary' in param:\n                    data[command_name]['parameters'][param_name]['help'] = param[\"short-summary\"]\n\n        if 'examples' in help_entry:\n            data[command_name]['examples'] = [[example['name'], example['text']]\n                                              for example in help_entry['examples']]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_cache_dir(shell_ctx):\n    azure_folder = shell_ctx.config.get_config_dir()\n    cache_path = os.path.join(azure_folder, 'cache')\n    if not os.path.exists(azure_folder):\n        os.makedirs(azure_folder)\n    if not os.path.exists(cache_path):\n        os.makedirs(cache_path)\n    return cache_path", "response": "gets the location of the cache"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dump_command_table(self, shell_ctx=None):\n        from azure.cli.core.commands.arm import register_global_subscription_argument, register_ids_argument\n        from knack import events\n        import timeit\n\n        start_time = timeit.default_timer()\n        shell_ctx = shell_ctx or self.shell_ctx\n        main_loader = AzInteractiveCommandsLoader(shell_ctx.cli_ctx)\n\n        main_loader.load_command_table(None)\n        main_loader.load_arguments(None)\n        register_global_subscription_argument(shell_ctx.cli_ctx)\n        register_ids_argument(shell_ctx.cli_ctx)\n        shell_ctx.cli_ctx.raise_event(events.EVENT_INVOKER_POST_CMD_TBL_CREATE, commands_loader=main_loader)\n        cmd_table = main_loader.command_table\n\n        cmd_table_data = {}\n        for command_name, cmd in cmd_table.items():\n\n            try:\n                command_description = cmd.description\n                if callable(command_description):\n                    command_description = command_description()\n\n                # checking all the parameters for a single command\n                parameter_metadata = {}\n                for arg in cmd.arguments.values():\n                    options = {\n                        'name': [name for name in arg.options_list],\n                        'required': REQUIRED_TAG if arg.type.settings.get('required') else '',\n                        'help': arg.type.settings.get('help') or ''\n                    }\n                    # the key is the first alias option\n                    if arg.options_list:\n                        parameter_metadata[arg.options_list[0]] = options\n\n                cmd_table_data[command_name] = {\n                    'parameters': parameter_metadata,\n                    'help': command_description,\n                    'examples': ''\n                }\n            except (ImportError, ValueError):\n                pass\n\n        load_help_files(cmd_table_data)\n        elapsed = timeit.default_timer() - start_time\n        logger.debug('Command table dumped: %s sec', elapsed)\n        FreshTable.loader = main_loader\n\n        # dump into the cache file\n        command_file = shell_ctx.config.get_help_files()\n        with open(os.path.join(get_cache_dir(shell_ctx), command_file), 'w') as help_file:\n            json.dump(cmd_table_data, help_file, default=lambda x: x.target or '', skipkeys=True)", "response": "dumps the command table"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _query_account_key(cli_ctx, account_name):\n    rg, scf = _query_account_rg(cli_ctx, account_name)\n    t_storage_account_keys = get_sdk(\n        cli_ctx, CUSTOM_MGMT_STORAGE, 'models.storage_account_keys#StorageAccountKeys')\n\n    if t_storage_account_keys:\n        return scf.storage_accounts.list_keys(rg, account_name).key1\n    # of type: models.storage_account_list_keys_result#StorageAccountListKeysResult\n    return scf.storage_accounts.list_keys(rg, account_name).keys[0].value", "response": "Query the storage account key. This is used when the customer doesn t offer account key but name."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _query_account_rg(cli_ctx, account_name):\n    scf = get_mgmt_service_client(cli_ctx, CUSTOM_MGMT_STORAGE)\n    acc = next((x for x in scf.storage_accounts.list() if x.name == account_name), None)\n    if acc:\n        from msrestazure.tools import parse_resource_id\n        return parse_resource_id(acc.id)['resource_group'], scf\n    raise ValueError(\"Storage account '{}' not found.\".format(account_name))", "response": "Query the storage account s resource group which the mgmt sdk requires."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process_resource_group(cmd, namespace):\n    if namespace.account_name and not namespace.resource_group_name:\n        namespace.resource_group_name = _query_account_rg(cmd.cli_ctx, namespace.account_name)[0]", "response": "Processes the resource group parameter from the account name"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef validate_client_parameters(cmd, namespace):\n    n = namespace\n\n    def get_config_value(section, key, default):\n        return cmd.cli_ctx.config.get(section, key, default)\n\n    if hasattr(n, 'auth_mode'):\n        auth_mode = n.auth_mode or get_config_value('storage', 'auth_mode', None)\n        del n.auth_mode\n        if not n.account_name:\n            n.account_name = get_config_value('storage', 'account', None)\n        if auth_mode == 'login':\n            n.token_credential = _create_token_credential(cmd.cli_ctx)\n\n            # give warning if there are account key args being ignored\n            account_key_args = [n.account_key and \"--account-key\", n.sas_token and \"--sas-token\",\n                                n.connection_string and \"--connection-string\"]\n            account_key_args = [arg for arg in account_key_args if arg]\n\n            if account_key_args:\n                from knack.log import get_logger\n\n                logger = get_logger(__name__)\n                logger.warning('In \"login\" auth mode, the following arguments are ignored: %s',\n                               ' ,'.join(account_key_args))\n            return\n\n    if not n.connection_string:\n        n.connection_string = get_config_value('storage', 'connection_string', None)\n\n    # if connection string supplied or in environment variables, extract account key and name\n    if n.connection_string:\n        conn_dict = validate_key_value_pairs(n.connection_string)\n        n.account_name = conn_dict.get('AccountName')\n        n.account_key = conn_dict.get('AccountKey')\n        if not n.account_name or not n.account_key:\n            from knack.util import CLIError\n            raise CLIError('Connection-string: %s, is malformed. Some shell environments require the '\n                           'connection string to be surrounded by quotes.' % n.connection_string)\n\n    # otherwise, simply try to retrieve the remaining variables from environment variables\n    if not n.account_name:\n        n.account_name = get_config_value('storage', 'account', None)\n    if not n.account_key:\n        n.account_key = get_config_value('storage', 'key', None)\n    if not n.sas_token:\n        n.sas_token = get_config_value('storage', 'sas_token', None)\n\n    # strip the '?' from sas token. the portal and command line are returns sas token in different\n    # forms\n    if n.sas_token:\n        n.sas_token = n.sas_token.lstrip('?')\n\n    # if account name is specified but no key, attempt to query\n    if n.account_name and not n.account_key and not n.sas_token:\n        n.account_key = _query_account_key(cmd.cli_ctx, n.account_name)", "response": "Validates the client parameters for the given namespace."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef validate_encryption_services(cmd, namespace):\n    if namespace.encryption_services:\n        t_encryption_services, t_encryption_service = get_sdk(cmd.cli_ctx, CUSTOM_MGMT_STORAGE,\n                                                              'EncryptionServices', 'EncryptionService', mod='models')\n        services = {service: t_encryption_service(enabled=True) for service in namespace.encryption_services}\n\n        namespace.encryption_services = t_encryption_services(**services)", "response": "Validate the encryption services passed in."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_file_path_validator(default_file_param=None):\n\n    def validator(namespace):\n        if not hasattr(namespace, 'path'):\n            return\n\n        path = namespace.path\n        dir_name, file_name = os.path.split(path) if path else (None, '')\n\n        if default_file_param and '.' not in file_name:\n            dir_name = path\n            file_name = os.path.split(getattr(namespace, default_file_param))[1]\n        namespace.directory_name = dir_name\n        namespace.file_name = file_name\n        del namespace.path\n\n    return validator", "response": "Creates a namespace validator that splits out the path into directory_name and file_name."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_datetime_type(to_string):\n    from datetime import datetime\n\n    def datetime_type(string):\n        \"\"\" Validates UTC datetime. Examples of accepted forms:\n        2017-12-31T01:11:59Z,2017-12-31T01:11Z or 2017-12-31T01Z or 2017-12-31 \"\"\"\n        accepted_date_formats = ['%Y-%m-%dT%H:%M:%SZ', '%Y-%m-%dT%H:%MZ',\n                                 '%Y-%m-%dT%HZ', '%Y-%m-%d']\n        for form in accepted_date_formats:\n            try:\n                if to_string:\n                    return datetime.strptime(string, form).strftime(form)\n\n                return datetime.strptime(string, form)\n            except ValueError:\n                continue\n        raise ValueError(\"Input '{}' not valid. Valid example: 2000-12-31T12:59:59Z\".format(string))\n\n    return datetime_type", "response": "Returns a function that takes a string and returns the datetime type of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nvalidating an IPv4 address or address range.", "response": "def ipv4_range_type(string):\n    \"\"\" Validates an IPv4 address or address range. \"\"\"\n    import re\n    ip_format = r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}'\n    if not re.match(\"^{}$\".format(ip_format), string):\n        if not re.match(\"^{ip_format}-{ip_format}$\".format(ip_format=ip_format), string):\n            raise ValueError\n    return string"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a function which validates that the string contains only a combination of service c and o.", "response": "def resource_type_type(loader):\n    \"\"\" Returns a function which validates that resource types string contains only a combination of service,\n    container, and object. Their shorthand representations are s, c, and o. \"\"\"\n\n    def impl(string):\n        t_resources = loader.get_models('common.models#ResourceTypes')\n        if set(string) - set(\"sco\"):\n            raise ValueError\n        return t_resources(_str=''.join(set(string)))\n\n    return impl"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef services_type(loader):\n\n    def impl(string):\n        t_services = loader.get_models('common.models#Services')\n        if set(string) - set(\"bqtf\"):\n            raise ValueError\n        return t_services(_str=''.join(set(string)))\n\n    return impl", "response": "Returns a function which validates that services string contains only a combination of blob queue table and file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nvalidates a string as a possible Kubernetes version.", "response": "def validate_k8s_version(namespace):\n    \"\"\"Validates a string as a possible Kubernetes version. An empty string is also valid, which tells the server\n    to use its default version.\"\"\"\n    if namespace.kubernetes_version:\n        k8s_release_regex = re.compile(r'^[v|V]?(\\d+\\.\\d+\\.\\d+.*)$')\n        found = k8s_release_regex.findall(namespace.kubernetes_version)\n        if found:\n            namespace.kubernetes_version = found[0]\n        else:\n            raise CLIError('--kubernetes-version should be the full version number, '\n                           'such as \"1.7.12\" or \"1.8.7\"')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate_linux_host_name(namespace):\n    # https://stackoverflow.com/questions/106179/regular-expression-to-match-dns-hostname-or-ip-address\n    rfc1123_regex = re.compile(r'^([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9])(\\.([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\\-]{0,61}[a-zA-Z0-9]))*$')  # pylint:disable=line-too-long\n    found = rfc1123_regex.findall(namespace.name)\n    if not found:\n        raise CLIError('--name cannot exceed 63 characters and can only contain '\n                       'letters, numbers, or dashes (-).')", "response": "Validates a string as a legal host name component."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nvalidate that max_pods is at least a reasonable minimum number.", "response": "def validate_max_pods(namespace):\n    \"\"\"Validates that max_pods is set to a reasonable minimum number.\"\"\"\n    # kube-proxy and kube-svc reside each nodes,\n    # 2 kube-proxy pods, 1 azureproxy/heapster/dashboard/tunnelfront are in kube-system\n    minimum_pods_required = ceil((namespace.node_count * 2 + 6 + 1) / namespace.node_count)\n    if namespace.max_pods != 0 and namespace.max_pods < minimum_pods_required:\n        raise CLIError('--max-pods must be at least {} for a managed Kubernetes cluster to function.'\n                       .format(minimum_pods_required))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nvalidate that min_count and max_count are set to 1 - 100", "response": "def validate_nodes_count(namespace):\n    \"\"\"Validate that min_count and max_count is set to 1-100\"\"\"\n    if namespace.min_count is not None:\n        if namespace.min_count < 1 or namespace.min_count > 100:\n            raise CLIError('--min-count must be in the range [1,100]')\n    if namespace.max_count is not None:\n        if namespace.max_count < 1 or namespace.max_count > 100:\n            raise CLIError('--max-count must be in the range [1,100]')"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nvalidates a nodepool name to be at most 12 characters alphanumeric only.", "response": "def validate_nodepool_name(namespace):\n    \"\"\"Validates a nodepool name to be at most 12 characters, alphanumeric only.\"\"\"\n    if namespace.nodepool_name != \"\":\n        if len(namespace.nodepool_name) > 12:\n            raise CLIError('--nodepool-name can contain atmost 12 characters')\n        if not namespace.nodepool_name.isalnum():\n            raise CLIError('--nodepool-name should only contain alphanumeric characters')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a BlockBlobService object with the settings specified in the CloudStorageAccount.", "response": "def create_block_blob_service(self):\n        '''\n        Creates a BlockBlobService object with the settings specified in the \n        CloudStorageAccount.\n\n        :return: A service object.\n        :rtype: :class:`~azure.storage.blob.blockblobservice.BlockBlobService`\n        '''\n        try:\n            from azure.storage.blob.blockblobservice import BlockBlobService\n            return BlockBlobService(self.account_name, self.account_key,\n                                    sas_token=self.sas_token,\n                                    is_emulated=self.is_emulated)\n        except ImportError:\n            raise Exception('The package azure-storage-blob is required. '\n                            + 'Please install it using \"pip install azure-storage-blob\"')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a PageBlobService object with the settings specified in the CloudStorageAccount.", "response": "def create_page_blob_service(self):\n        '''\n        Creates a PageBlobService object with the settings specified in the \n        CloudStorageAccount.\n\n        :return: A service object.\n        :rtype: :class:`~azure.storage.blob.pageblobservice.PageBlobService`\n        '''\n        try:\n            from azure.storage.blob.pageblobservice import PageBlobService\n            return PageBlobService(self.account_name, self.account_key,\n                                   sas_token=self.sas_token,\n                                   is_emulated=self.is_emulated)\n        except ImportError:\n            raise Exception('The package azure-storage-blob is required. '\n                            + 'Please install it using \"pip install azure-storage-blob\"')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_append_blob_service(self):\n        '''\n        Creates a AppendBlobService object with the settings specified in the \n        CloudStorageAccount.\n\n        :return: A service object.\n        :rtype: :class:`~azure.storage.blob.appendblobservice.AppendBlobService`\n        '''\n        try:\n            from azure.storage.blob.appendblobservice import AppendBlobService\n            return AppendBlobService(self.account_name, self.account_key,\n                                     sas_token=self.sas_token,\n                                     is_emulated=self.is_emulated)\n        except ImportError:\n            raise Exception('The package azure-storage-blob is required. '\n                            + 'Please install it using \"pip install azure-storage-blob\"')", "response": "Creates a AppendBlobService object with the settings specified in the \n        CloudStorageAccount."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_queue_service(self):\n        '''\n        Creates a QueueService object with the settings specified in the \n        CloudStorageAccount.\n\n        :return: A service object.\n        :rtype: :class:`~azure.storage.queue.queueservice.QueueService`\n        '''\n        try:\n            from azure.storage.queue.queueservice import QueueService\n            return QueueService(self.account_name, self.account_key,\n                                sas_token=self.sas_token,\n                                is_emulated=self.is_emulated)\n        except ImportError:\n            raise Exception('The package azure-storage-queue is required. '\n                            + 'Please install it using \"pip install azure-storage-queue\"')", "response": "Creates a QueueService object with the settings specified in the \n        CloudStorageAccount."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef exists(self, queue_name, timeout=None):\n        '''\n        Returns a boolean indicating whether the queue exists.\n\n        :param str queue_name:\n            The name of queue to check for existence.\n        :param int timeout:\n            The server timeout, expressed in seconds.\n        :return: A boolean indicating whether the queue exists.\n        :rtype: bool\n        '''\n        try:\n            self.get_queue_metadata(queue_name, timeout=timeout)\n            return True\n        except AzureHttpError as ex:\n            _dont_fail_not_exist(ex)\n            return False", "response": "Returns a boolean indicating whether the queue exists."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef available_delegations(self):\n        api_version = self._get_api_version('available_delegations')\n        if api_version == '2018-08-01':\n            from .v2018_08_01.operations import AvailableDelegationsOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))", "response": "Return an instance of the AvailableDelegationsOperations class."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef available_resource_group_delegations(self):\n        api_version = self._get_api_version('available_resource_group_delegations')\n        if api_version == '2018-08-01':\n            from .v2018_08_01.operations import AvailableResourceGroupDelegationsOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))", "response": "Return an instance of the AvailableResourceGroupDelegationsOperations API version 2. 0."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn an instance of the Azure Firewall FQDN Tags API version.", "response": "def azure_firewall_fqdn_tags(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2018-08-01: :class:`AzureFirewallFqdnTagsOperations<azure.mgmt.network.v2018_08_01.operations.AzureFirewallFqdnTagsOperations>`\n        \"\"\"\n        api_version = self._get_api_version('azure_firewall_fqdn_tags')\n        if api_version == '2018-08-01':\n            from .v2018_08_01.operations import AzureFirewallFqdnTagsOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef express_route_connections(self):\n        api_version = self._get_api_version('express_route_connections')\n        if api_version == '2018-08-01':\n            from .v2018_08_01.operations import ExpressRouteConnectionsOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))", "response": "Return an instance of the ExpressRouteConnectionsOperations API version 2. 0. 0."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef express_route_cross_connection_peerings(self):\n        api_version = self._get_api_version('express_route_cross_connection_peerings')\n        if api_version == '2018-02-01':\n            from .v2018_02_01.operations import ExpressRouteCrossConnectionPeeringsOperations as OperationClass\n        elif api_version == '2018-04-01':\n            from .v2018_04_01.operations import ExpressRouteCrossConnectionPeeringsOperations as OperationClass\n        elif api_version == '2018-06-01':\n            from .v2018_06_01.operations import ExpressRouteCrossConnectionPeeringsOperations as OperationClass\n        elif api_version == '2018-07-01':\n            from .v2018_07_01.operations import ExpressRouteCrossConnectionPeeringsOperations as OperationClass\n        elif api_version == '2018-08-01':\n            from .v2018_08_01.operations import ExpressRouteCrossConnectionPeeringsOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))", "response": "Return an instance of the ExpressRouteCrossConnectionPeerings API client."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef express_route_gateways(self):\n        api_version = self._get_api_version('express_route_gateways')\n        if api_version == '2018-08-01':\n            from .v2018_08_01.operations import ExpressRouteGatewaysOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))", "response": "Return an instance of the ExpressRouteGateways API Version 2. 0 Operation class."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns an instance of the ExpressRouteLinksOperations API version 2. 0. 0.", "response": "def express_route_links(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2018-08-01: :class:`ExpressRouteLinksOperations<azure.mgmt.network.v2018_08_01.operations.ExpressRouteLinksOperations>`\n        \"\"\"\n        api_version = self._get_api_version('express_route_links')\n        if api_version == '2018-08-01':\n            from .v2018_08_01.operations import ExpressRouteLinksOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef express_route_ports(self):\n        api_version = self._get_api_version('express_route_ports')\n        if api_version == '2018-08-01':\n            from .v2018_08_01.operations import ExpressRoutePortsOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))", "response": "Return an instance of the ExpressRoutePortsOperations API version 2. 0. 0."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns an instance of the ExpressRoutePortsLocationsOperations API version 2. 0. 0.", "response": "def express_route_ports_locations(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2018-08-01: :class:`ExpressRoutePortsLocationsOperations<azure.mgmt.network.v2018_08_01.operations.ExpressRoutePortsLocationsOperations>`\n        \"\"\"\n        api_version = self._get_api_version('express_route_ports_locations')\n        if api_version == '2018-08-01':\n            from .v2018_08_01.operations import ExpressRoutePortsLocationsOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef interface_endpoints(self):\n        api_version = self._get_api_version('interface_endpoints')\n        if api_version == '2018-08-01':\n            from .v2018_08_01.operations import InterfaceEndpointsOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))", "response": "Return an instance of the InterfaceEndpointsOperations API version 2. 0. 0."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef network_interface_tap_configurations(self):\n        api_version = self._get_api_version('network_interface_tap_configurations')\n        if api_version == '2018-08-01':\n            from .v2018_08_01.operations import NetworkInterfaceTapConfigurationsOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))", "response": "Return an instance of NetworkInterfaceTapConfigurationsOperations class."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef network_profiles(self):\n        api_version = self._get_api_version('network_profiles')\n        if api_version == '2018-08-01':\n            from .v2018_08_01.operations import NetworkProfilesOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))", "response": "Return an instance of the NetworkProfilesOperations API version 2. 0. 0."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a new object containing all the operations in the current API version.", "response": "def operations(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2017-09-01: :class:`Operations<azure.mgmt.network.v2017_09_01.operations.Operations>`\n           * 2017-10-01: :class:`Operations<azure.mgmt.network.v2017_10_01.operations.Operations>`\n           * 2017-11-01: :class:`Operations<azure.mgmt.network.v2017_11_01.operations.Operations>`\n           * 2018-01-01: :class:`Operations<azure.mgmt.network.v2018_01_01.operations.Operations>`\n           * 2018-02-01: :class:`Operations<azure.mgmt.network.v2018_02_01.operations.Operations>`\n           * 2018-04-01: :class:`Operations<azure.mgmt.network.v2018_04_01.operations.Operations>`\n           * 2018-06-01: :class:`Operations<azure.mgmt.network.v2018_06_01.operations.Operations>`\n           * 2018-07-01: :class:`Operations<azure.mgmt.network.v2018_07_01.operations.Operations>`\n           * 2018-08-01: :class:`Operations<azure.mgmt.network.v2018_08_01.operations.Operations>`\n        \"\"\"\n        api_version = self._get_api_version('operations')\n        if api_version == '2017-09-01':\n            from .v2017_09_01.operations import Operations as OperationClass\n        elif api_version == '2017-10-01':\n            from .v2017_10_01.operations import Operations as OperationClass\n        elif api_version == '2017-11-01':\n            from .v2017_11_01.operations import Operations as OperationClass\n        elif api_version == '2018-01-01':\n            from .v2018_01_01.operations import Operations as OperationClass\n        elif api_version == '2018-02-01':\n            from .v2018_02_01.operations import Operations as OperationClass\n        elif api_version == '2018-04-01':\n            from .v2018_04_01.operations import Operations as OperationClass\n        elif api_version == '2018-06-01':\n            from .v2018_06_01.operations import Operations as OperationClass\n        elif api_version == '2018-07-01':\n            from .v2018_07_01.operations import Operations as OperationClass\n        elif api_version == '2018-08-01':\n            from .v2018_08_01.operations import Operations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef public_ip_prefixes(self):\n        api_version = self._get_api_version('public_ip_prefixes')\n        if api_version == '2018-07-01':\n            from .v2018_07_01.operations import PublicIPPrefixesOperations as OperationClass\n        elif api_version == '2018-08-01':\n            from .v2018_08_01.operations import PublicIPPrefixesOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))", "response": "Return an instance of the PublicIPPrefixes API Version 2. 0 operation class."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting all Route Filter rules for the current resource.", "response": "def route_filter_rules(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2016-12-01: :class:`RouteFilterRulesOperations<azure.mgmt.network.v2016_12_01.operations.RouteFilterRulesOperations>`\n           * 2017-03-01: :class:`RouteFilterRulesOperations<azure.mgmt.network.v2017_03_01.operations.RouteFilterRulesOperations>`\n           * 2017-06-01: :class:`RouteFilterRulesOperations<azure.mgmt.network.v2017_06_01.operations.RouteFilterRulesOperations>`\n           * 2017-08-01: :class:`RouteFilterRulesOperations<azure.mgmt.network.v2017_08_01.operations.RouteFilterRulesOperations>`\n           * 2017-09-01: :class:`RouteFilterRulesOperations<azure.mgmt.network.v2017_09_01.operations.RouteFilterRulesOperations>`\n           * 2017-10-01: :class:`RouteFilterRulesOperations<azure.mgmt.network.v2017_10_01.operations.RouteFilterRulesOperations>`\n           * 2017-11-01: :class:`RouteFilterRulesOperations<azure.mgmt.network.v2017_11_01.operations.RouteFilterRulesOperations>`\n           * 2018-01-01: :class:`RouteFilterRulesOperations<azure.mgmt.network.v2018_01_01.operations.RouteFilterRulesOperations>`\n           * 2018-02-01: :class:`RouteFilterRulesOperations<azure.mgmt.network.v2018_02_01.operations.RouteFilterRulesOperations>`\n           * 2018-04-01: :class:`RouteFilterRulesOperations<azure.mgmt.network.v2018_04_01.operations.RouteFilterRulesOperations>`\n           * 2018-06-01: :class:`RouteFilterRulesOperations<azure.mgmt.network.v2018_06_01.operations.RouteFilterRulesOperations>`\n           * 2018-07-01: :class:`RouteFilterRulesOperations<azure.mgmt.network.v2018_07_01.operations.RouteFilterRulesOperations>`\n           * 2018-08-01: :class:`RouteFilterRulesOperations<azure.mgmt.network.v2018_08_01.operations.RouteFilterRulesOperations>`\n        \"\"\"\n        api_version = self._get_api_version('route_filter_rules')\n        if api_version == '2016-12-01':\n            from .v2016_12_01.operations import RouteFilterRulesOperations as OperationClass\n        elif api_version == '2017-03-01':\n            from .v2017_03_01.operations import RouteFilterRulesOperations as OperationClass\n        elif api_version == '2017-06-01':\n            from .v2017_06_01.operations import RouteFilterRulesOperations as OperationClass\n        elif api_version == '2017-08-01':\n            from .v2017_08_01.operations import RouteFilterRulesOperations as OperationClass\n        elif api_version == '2017-09-01':\n            from .v2017_09_01.operations import RouteFilterRulesOperations as OperationClass\n        elif api_version == '2017-10-01':\n            from .v2017_10_01.operations import RouteFilterRulesOperations as OperationClass\n        elif api_version == '2017-11-01':\n            from .v2017_11_01.operations import RouteFilterRulesOperations as OperationClass\n        elif api_version == '2018-01-01':\n            from .v2018_01_01.operations import RouteFilterRulesOperations as OperationClass\n        elif api_version == '2018-02-01':\n            from .v2018_02_01.operations import RouteFilterRulesOperations as OperationClass\n        elif api_version == '2018-04-01':\n            from .v2018_04_01.operations import RouteFilterRulesOperations as OperationClass\n        elif api_version == '2018-06-01':\n            from .v2018_06_01.operations import RouteFilterRulesOperations as OperationClass\n        elif api_version == '2018-07-01':\n            from .v2018_07_01.operations import RouteFilterRulesOperations as OperationClass\n        elif api_version == '2018-08-01':\n            from .v2018_08_01.operations import RouteFilterRulesOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef service_endpoint_policies(self):\n        api_version = self._get_api_version('service_endpoint_policies')\n        if api_version == '2018-07-01':\n            from .v2018_07_01.operations import ServiceEndpointPoliciesOperations as OperationClass\n        elif api_version == '2018-08-01':\n            from .v2018_08_01.operations import ServiceEndpointPoliciesOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))", "response": "Return an instance of the Service Endpoint Policies API Version 2. 0."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef service_endpoint_policy_definitions(self):\n        api_version = self._get_api_version('service_endpoint_policy_definitions')\n        if api_version == '2018-07-01':\n            from .v2018_07_01.operations import ServiceEndpointPolicyDefinitionsOperations as OperationClass\n        elif api_version == '2018-08-01':\n            from .v2018_08_01.operations import ServiceEndpointPolicyDefinitionsOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))", "response": "Return an instance of the ServiceEndpointPolicyDefinitionsOperations API version 2. 0."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn an instance of the Virtual Hubs API version.", "response": "def virtual_hubs(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2018-04-01: :class:`VirtualHubsOperations<azure.mgmt.network.v2018_04_01.operations.VirtualHubsOperations>`\n           * 2018-06-01: :class:`VirtualHubsOperations<azure.mgmt.network.v2018_06_01.operations.VirtualHubsOperations>`\n           * 2018-07-01: :class:`VirtualHubsOperations<azure.mgmt.network.v2018_07_01.operations.VirtualHubsOperations>`\n           * 2018-08-01: :class:`VirtualHubsOperations<azure.mgmt.network.v2018_08_01.operations.VirtualHubsOperations>`\n        \"\"\"\n        api_version = self._get_api_version('virtual_hubs')\n        if api_version == '2018-04-01':\n            from .v2018_04_01.operations import VirtualHubsOperations as OperationClass\n        elif api_version == '2018-06-01':\n            from .v2018_06_01.operations import VirtualHubsOperations as OperationClass\n        elif api_version == '2018-07-01':\n            from .v2018_07_01.operations import VirtualHubsOperations as OperationClass\n        elif api_version == '2018-08-01':\n            from .v2018_08_01.operations import VirtualHubsOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef virtual_network_taps(self):\n        api_version = self._get_api_version('virtual_network_taps')\n        if api_version == '2018-08-01':\n            from .v2018_08_01.operations import VirtualNetworkTapsOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))", "response": "Return an instance of the Virtual Network Taps API version 2. 0."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef virtual_wans(self):\n        api_version = self._get_api_version('virtual_wans')\n        if api_version == '2018-08-01':\n            from .v2018_08_01.operations import VirtualWansOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))", "response": "Return an instance of the VirtualWans API version 2. 0 resource."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmake the interface or refreshes it", "response": "def cli(self):\n        \"\"\" Makes the interface or refreshes it \"\"\"\n        if self._cli is None:\n            self._cli = self.create_interface()\n        return self._cli"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncall when the user clicks on the input timeout.", "response": "def on_input_timeout(self, cli):\n        \"\"\"\n        brings up the metadata for the command if there is a valid command already typed\n        \"\"\"\n        document = cli.current_buffer.document\n        text = document.text\n\n        text = text.replace('az ', '')\n        if self.default_command:\n            text = self.default_command + ' ' + text\n\n        param_info, example = self.generate_help_text()\n\n        self.param_docs = u'{}'.format(param_info)\n        self.example_docs = u'{}'.format(example)\n\n        self._update_default_info()\n\n        cli.buffers['description'].reset(\n            initial_document=Document(self.description_docs, cursor_position=0))\n        cli.buffers['parameter'].reset(\n            initial_document=Document(self.param_docs))\n        cli.buffers['examples'].reset(\n            initial_document=Document(self.example_docs))\n        cli.buffers['default_values'].reset(\n            initial_document=Document(\n                u'{}'.format(self.config_default if self.config_default else 'No Default Values')))\n        self._update_toolbar()\n        cli.request_redraw()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmakes the example text", "response": "def _space_examples(self, list_examples, rows, section_value):\n        \"\"\" makes the example text \"\"\"\n        examples_with_index = []\n\n        for i, _ in list(enumerate(list_examples)):\n            if len(list_examples[i]) > 1:\n                examples_with_index.append(\"[\" + str(i + 1) + \"] \" + list_examples[i][0] +\n                                           list_examples[i][1])\n\n        example = \"\".join(exam for exam in examples_with_index)\n        num_newline = example.count('\\n')\n\n        page_number = ''\n        if num_newline > rows * PART_SCREEN_EXAMPLE and rows > PART_SCREEN_EXAMPLE * 10:\n            len_of_excerpt = math.floor(float(rows) * PART_SCREEN_EXAMPLE)\n\n            group = example.split('\\n')\n            end = int(section_value * len_of_excerpt)\n            begin = int((section_value - 1) * len_of_excerpt)\n\n            if end < num_newline:\n                example = '\\n'.join(group[begin:end]) + \"\\n\"\n            else:\n                # default chops top off\n                example = '\\n'.join(group[begin:]) + \"\\n\"\n                while ((section_value - 1) * len_of_excerpt) > num_newline:\n                    self.example_page -= 1\n            page_number = '\\n' + str(section_value) + \"/\" + str(int(math.ceil(num_newline / len_of_excerpt)))\n\n        return example + page_number + ' CTRL+Y (^) CTRL+N (v)'"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates the help text based on commands typed", "response": "def generate_help_text(self):\n        \"\"\" generates the help text based on commands typed \"\"\"\n        param_descrip = example = \"\"\n        self.description_docs = u''\n\n        rows, _ = get_window_dim()\n        rows = int(rows)\n\n        param_args = self.completer.leftover_args\n        last_word = self.completer.unfinished_word\n        command = self.completer.current_command\n        new_command = ' '.join([command, last_word]).strip()\n\n        if not self.completer.complete_command and new_command in self.completer.command_description:\n            command = new_command\n\n        # get command/group help\n        if self.completer and command in self.completer.command_description:\n            self.description_docs = u'{}'.format(self.completer.command_description[command])\n\n        # get parameter help if full command\n        if self.completer and command in self.completer.command_param_info:\n            param = param_args[-1] if param_args else ''\n            param = last_word if last_word.startswith('-') else param\n\n            if param in self.completer.command_param_info[command] and self.completer.has_description(\n                    command + \" \" + param):\n                param_descrip = ''.join([\n                    param, \":\", '\\n', self.completer.param_description.get(command + \" \" + param, '')])\n\n            if command in self.completer.command_examples:\n                string_example = []\n                for example in self.completer.command_examples[command]:\n                    for part in example:\n                        string_example.append(part)\n                ''.join(string_example)\n                example = self._space_examples(\n                    self.completer.command_examples[command], rows, self.example_page)\n\n        return param_descrip, example"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate the application object and the buffers", "response": "def create_application(self, full_layout=True):\n        \"\"\" makes the application object and the buffers \"\"\"\n        layout_manager = LayoutManager(self)\n        if full_layout:\n            layout = layout_manager.create_layout(ExampleLexer, ToolbarLexer)\n        else:\n            layout = layout_manager.create_tutorial_layout()\n\n        buffers = {\n            DEFAULT_BUFFER: Buffer(is_multiline=True),\n            'description': Buffer(is_multiline=True, read_only=True),\n            'parameter': Buffer(is_multiline=True, read_only=True),\n            'examples': Buffer(is_multiline=True, read_only=True),\n            'bottom_toolbar': Buffer(is_multiline=True),\n            'example_line': Buffer(is_multiline=True),\n            'default_values': Buffer(),\n            'symbols': Buffer(),\n            'progress': Buffer(is_multiline=False)\n        }\n\n        writing_buffer = Buffer(\n            history=self.history,\n            auto_suggest=AutoSuggestFromHistory(),\n            enable_history_search=True,\n            completer=self.completer,\n            complete_while_typing=Always()\n        )\n\n        return Application(\n            mouse_support=False,\n            style=self.style,\n            buffer=writing_buffer,\n            on_input_timeout=self.on_input_timeout,\n            key_bindings_registry=InteractiveKeyBindings(self).registry,\n            layout=layout,\n            buffers=buffers,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_prompt(self, prompt_command=\"\", position=0):\n        self.description_docs = u'{}'.format(prompt_command)\n        self.cli.current_buffer.reset(\n            initial_document=Document(\n                self.description_docs,\n                cursor_position=position))\n        self.cli.request_redraw()", "response": "writes the prompt line"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the scope of the command", "response": "def set_scope(self, value):\n        \"\"\" narrows the scopes the commands \"\"\"\n        if self.default_command:\n            self.default_command += ' ' + value\n        else:\n            self.default_command += value\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef handle_example(self, text, continue_flag):\n        cmd = text.partition(SELECT_SYMBOL['example'])[0].rstrip()\n        num = text.partition(SELECT_SYMBOL['example'])[2].strip()\n        example = \"\"\n        try:\n            num = int(num) - 1\n        except ValueError:\n            print(\"An Integer should follow the colon\", file=self.output)\n            return \"\"\n        if cmd in self.completer.command_examples:\n            if num >= 0 and num < len(self.completer.command_examples[cmd]):\n                example = self.completer.command_examples[cmd][num][1]\n                example = example.replace('\\n', '')\n            else:\n                print('Invalid example number', file=self.output)\n                return '', True\n\n        example = example.replace('az', '')\n\n        starting_index = None\n        counter = 0\n        example_no_fill = \"\"\n        flag_fill = True\n        for word in example.split():\n            if flag_fill:\n                example_no_fill += word + \" \"\n            if word.startswith('-'):\n                example_no_fill += word + \" \"\n                if not starting_index:\n                    starting_index = counter\n                flag_fill = False\n            counter += 1\n\n        return self.example_repl(example_no_fill, example, starting_index, continue_flag)", "response": "Parses the example and returns the text"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef example_repl(self, text, example, start_index, continue_flag):\n        if start_index:\n            start_index = start_index + 1\n            cmd = ' '.join(text.split()[:start_index])\n            example_cli = CommandLineInterface(\n                application=self.create_application(\n                    full_layout=False),\n                eventloop=create_eventloop())\n            example_cli.buffers['example_line'].reset(\n                initial_document=Document(u'{}\\n'.format(\n                    add_new_lines(example)))\n            )\n            while start_index < len(text.split()):\n                if self.default_command:\n                    cmd = cmd.replace(self.default_command + ' ', '')\n                example_cli.buffers[DEFAULT_BUFFER].reset(\n                    initial_document=Document(\n                        u'{}'.format(cmd),\n                        cursor_position=len(cmd)))\n                example_cli.request_redraw()\n                answer = example_cli.run()\n                if not answer:\n                    return \"\", True\n                answer = answer.text\n                if answer.strip('\\n') == cmd.strip('\\n'):\n                    continue\n                else:\n                    if len(answer.split()) > 1:\n                        start_index += 1\n                        cmd += \" \" + answer.split()[-1] + \" \" +\\\n                               u' '.join(text.split()[start_index:start_index + 1])\n            example_cli.exit()\n            del example_cli\n        else:\n            cmd = text\n\n        return cmd, continue_flag", "response": "REPL for interactive tutorials"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nhandle the jmespath query for injection or printing", "response": "def handle_jmespath_query(self, args):\n        \"\"\" handles the jmespath query for injection or printing \"\"\"\n        continue_flag = False\n        query_symbol = SELECT_SYMBOL['query']\n        symbol_len = len(query_symbol)\n        try:\n            if len(args) == 1:\n                # if arguments start with query_symbol, just print query result\n                if args[0] == query_symbol:\n                    result = self.last.result\n                elif args[0].startswith(query_symbol):\n                    result = jmespath.search(args[0][symbol_len:], self.last.result)\n                print(json.dumps(result, sort_keys=True, indent=2), file=self.output)\n            elif args[0].startswith(query_symbol):\n                # print error message, user unsure of query shortcut usage\n                print((\"Usage Error: \" + os.linesep +\n                       \"1. Use {0} stand-alone to display previous result with optional filtering \"\n                       \"(Ex: {0}[jmespath query])\" +\n                       os.linesep + \"OR:\" + os.linesep +\n                       \"2. Use {0} to query the previous result for argument values \"\n                       \"(Ex: group show --name {0}[jmespath query])\").format(query_symbol), file=self.output)\n            else:\n                # query, inject into cmd\n                def jmespath_query(match):\n                    if match.group(0) == query_symbol:\n                        return str(self.last.result)\n                    query_result = jmespath.search(match.group(0)[symbol_len:], self.last.result)\n                    return str(query_result)\n\n                def sub_result(arg):\n                    escaped_symbol = re.escape(query_symbol)\n                    # regex captures query symbol and all characters following it in the argument\n                    return json.dumps(re.sub(r'%s.*' % escaped_symbol, jmespath_query, arg))\n                cmd_base = ' '.join(map(sub_result, args))\n                self.cli_execute(cmd_base)\n            continue_flag = True\n        except (jmespath.exceptions.ParseError, CLIError) as e:\n            print(\"Invalid Query Input: \" + str(e), file=self.output)\n            continue_flag = True\n        return continue_flag"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nhandling what to do with a scoping gesture", "response": "def handle_scoping_input(self, continue_flag, cmd, text):\n        \"\"\" handles what to do with a scoping gesture \"\"\"\n        default_split = text.partition(SELECT_SYMBOL['scope'])[2].split()\n        cmd = cmd.replace(SELECT_SYMBOL['scope'], '')\n\n        continue_flag = True\n\n        if not default_split:\n            self.default_command = \"\"\n            print('unscoping all', file=self.output)\n\n            return continue_flag, cmd\n\n        while default_split:\n            if not text:\n                value = ''\n            else:\n                value = default_split[0]\n\n            tree_path = self.default_command.split()\n            tree_path.append(value)\n\n            if self.completer.command_tree.in_tree(tree_path):\n                self.set_scope(value)\n                print(\"defaulting: \" + value, file=self.output)\n                cmd = cmd.replace(SELECT_SYMBOL['scope'], '')\n            elif SELECT_SYMBOL['unscope'] == default_split[0] and self.default_command.split():\n\n                value = self.default_command.split()[-1]\n                self.default_command = ' ' + ' '.join(self.default_command.split()[:-1])\n\n                if not self.default_command.strip():\n                    self.default_command = self.default_command.strip()\n                print('unscoping: ' + value, file=self.output)\n\n            elif SELECT_SYMBOL['unscope'] not in text:\n                print(\"Scope must be a valid command\", file=self.output)\n\n            default_split = default_split[1:]\n        return continue_flag, cmd"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends the command to the CLI to be executed", "response": "def cli_execute(self, cmd):\n        \"\"\" sends the command to the CLI to be executed \"\"\"\n\n        try:\n            args = parse_quotes(cmd)\n\n            if args and args[0] == 'feedback':\n                self.config.set_feedback('yes')\n                self.user_feedback = False\n\n            azure_folder = get_config_dir()\n            if not os.path.exists(azure_folder):\n                os.makedirs(azure_folder)\n            ACCOUNT.load(os.path.join(azure_folder, 'azureProfile.json'))\n            CONFIG.load(os.path.join(azure_folder, 'az.json'))\n            SESSION.load(os.path.join(azure_folder, 'az.sess'), max_age=3600)\n\n            invocation = self.cli_ctx.invocation_cls(cli_ctx=self.cli_ctx,\n                                                     parser_cls=self.cli_ctx.parser_cls,\n                                                     commands_loader_cls=self.cli_ctx.commands_loader_cls,\n                                                     help_cls=self.cli_ctx.help_cls)\n\n            if '--progress' in args:\n                args.remove('--progress')\n                execute_args = [args]\n                thread = Thread(target=invocation.execute, args=execute_args)\n                thread.daemon = True\n                thread.start()\n                self.threads.append(thread)\n                self.curr_thread = thread\n\n                progress_args = [self]\n                thread = Thread(target=progress_view, args=progress_args)\n                thread.daemon = True\n                thread.start()\n                self.threads.append(thread)\n                result = None\n            else:\n                result = invocation.execute(args)\n\n            self.last_exit = 0\n            if result and result.result is not None:\n                if self.output:\n                    self.output.write(result)\n                    self.output.flush()\n                else:\n                    formatter = self.cli_ctx.output.get_formatter(self.cli_ctx.invocation.data['output'])\n                    self.cli_ctx.output.out(result, formatter=formatter, out_file=sys.stdout)\n                    self.last = result\n\n        except Exception as ex:  # pylint: disable=broad-except\n            self.last_exit = handle_exception(ex)\n        except SystemExit as ex:\n            self.last_exit = int(ex.code)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npatches the Shell Progress controller", "response": "def progress_patch(self, _=False):\n        \"\"\" forces to use the Shell Progress \"\"\"\n        from .progress import ShellProgressView\n        self.cli_ctx.progress_controller.init_progress(ShellProgressView())\n        return self.cli_ctx.progress_controller"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate the view with the current progress message", "response": "def progress_view(shell):\n    \"\"\" updates the view \"\"\"\n    while not ShellProgressView.done:\n        _, col = get_window_dim()\n        col = int(col)\n        progress = get_progress_message()\n        if '\\n' in progress:\n            prog_list = progress.split('\\n')\n            prog_val = len(prog_list[-1])\n        else:\n            prog_val = len(progress)\n        buffer_size = col - prog_val - 4\n\n        if ShellProgressView.progress_bar:\n            doc = u'{}:{}'.format(progress, ShellProgressView.progress_bar)\n            shell.spin_val = -1\n            counter = 0\n            ShellProgressView.heart_bar = ''\n        else:\n            if progress and not ShellProgressView.done:\n                heart_bar = ShellProgressView.heart_bar\n                if shell.spin_val >= 0:\n                    beat = ShellProgressView.heart_beat_values[_get_heart_frequency()]\n                    heart_bar += beat\n                    heart_bar = heart_bar[len(beat):]\n                    len_beat = len(heart_bar)\n                    if len_beat > buffer_size:\n                        heart_bar = heart_bar[len_beat - buffer_size:]\n\n                    while len(heart_bar) < buffer_size:\n                        beat = ShellProgressView.heart_beat_values[_get_heart_frequency()]\n                        heart_bar += beat\n\n                else:\n                    shell.spin_val = 0\n                    counter = 0\n                    while counter < buffer_size:\n                        beat = ShellProgressView.heart_beat_values[_get_heart_frequency()]\n                        heart_bar += beat\n                        counter += len(beat)\n                ShellProgressView.heart_bar = heart_bar\n            doc = u'{}:{}'.format(progress, ShellProgressView.heart_bar)\n        shell.cli.buffers['progress'].reset(\n            initial_document=Document(doc))\n        shell.cli.request_redraw()\n        sleep(shell.intermediate_sleep)\n\n    ShellProgressView.done = False\n    ShellProgressView.progress_bar = ''\n    shell.spin_val = -1\n    sleep(shell.final_sleep)\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a boolean indicating whether the resource exists in the given share.", "response": "def exists(self, share_name, directory_name=None, file_name=None, timeout=None, snapshot=None):\n        '''\n        Returns a boolean indicating whether the share exists if only share name is\n        given. If directory_name is specificed a boolean will be returned indicating\n        if the directory exists. If file_name is specified as well, a boolean will be\n        returned indicating if the file exists.\n\n        :param str share_name:\n            Name of a share.\n        :param str directory_name:\n            The path to a directory.\n        :param str file_name:\n            Name of a file.\n        :param int timeout:\n            The timeout parameter is expressed in seconds.\n        :param str snapshot:\n            A string that represents the snapshot version, if applicable.\n        :return: A boolean indicating whether the resource exists.\n        :rtype: bool\n        '''\n        _validate_not_none('share_name', share_name)\n        try:\n            if file_name is not None:\n                self.get_file_properties(share_name, directory_name, file_name, timeout=timeout, snapshot=snapshot)\n            elif directory_name is not None:\n                self.get_directory_properties(share_name, directory_name, timeout=timeout, snapshot=snapshot)\n            else:\n                self.get_share_properties(share_name, timeout=timeout, snapshot=snapshot)\n            return True\n        except AzureHttpError as ex:\n            _dont_fail_not_exist(ex)\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a function that returns the value of a resource property in the frontdoor resource group.", "response": "def list_frontdoor_resource_property(resource, prop):\n    \"\"\" Factory method for creating list functions. \"\"\"\n\n    def list_func(cmd, resource_group_name, resource_name):\n        client = cf_frontdoor(cmd.cli_ctx, None)\n        return client.get(resource_group_name, resource_name).__getattribute__(prop)\n\n    func_name = 'list_fd_{}_{}'.format(resource, prop)\n    setattr(sys.modules[__name__], func_name, list_func)\n    return func_name"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a delete function that deletes the resource property entry in the frontdoor.", "response": "def delete_frontdoor_resource_property_entry(resource, prop):\n    \"\"\" Factory method for creating delete functions. \"\"\"\n\n    def delete_func(cmd, resource_group_name, resource_name, item_name, no_wait=False):  # pylint: disable=unused-argument\n\n        client = cf_frontdoor(cmd.cli_ctx, None)\n        item = client.get(resource_group_name, resource_name)\n        keep_items = \\\n            [x for x in item.__getattribute__(prop) if x.name.lower() != item_name.lower()]\n        with UpdateContext(item) as c:\n            c.update_param(prop, keep_items, False)\n        if no_wait:\n            sdk_no_wait(no_wait, client.create_or_update, resource_group_name, resource_name, item)\n        else:\n            result = sdk_no_wait(no_wait, client.create_or_update, resource_group_name, resource_name, item).result()\n            if next((x for x in getattr(result, prop) if x.name.lower() == item_name.lower()), None):\n                from knack.util import CLIError\n                raise CLIError(\"Failed to delete '{}' on '{}'\".format(item_name, resource_name))\n\n    func_name = 'delete_fd_{}_{}'.format(resource, prop)\n    setattr(sys.modules[__name__], func_name, delete_func)\n    return func_name"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sqlvm_list(\n        client,\n        resource_group_name=None):\n    '''\n    Lists all SQL virtual machines in a resource group or subscription.\n    '''\n    if resource_group_name:\n        # List all sql vms  in the resource group\n        return client.list_by_resource_group(resource_group_name=resource_group_name)\n\n    # List all sql vms in the subscription\n    return client.list()", "response": "Lists all SQL virtual machines in a resource group or subscription."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlists all SQL virtual machine groups in a resource group or subscription.", "response": "def sqlvm_group_list(\n        client,\n        resource_group_name=None):\n    '''\n    Lists all SQL virtual machine groups in a resource group or subscription.\n    '''\n    if resource_group_name:\n        # List all sql vm groups in the resource group\n        return client.list_by_resource_group(resource_group_name=resource_group_name)\n\n    # List all sql vm groups in the subscription\n    return client.list()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sqlvm_group_create(client, cmd, sql_virtual_machine_group_name, resource_group_name, location, sql_image_offer,\n                       sql_image_sku, domain_fqdn, cluster_operator_account, sql_service_account,\n                       storage_account_url, storage_account_key, cluster_bootstrap_account=None,\n                       file_share_witness_path=None, ou_path=None, tags=None):\n\n    '''\n    Creates a SQL virtual machine group.\n    '''\n    tags = tags or {}\n\n    # Create the windows server failover cluster domain profile object.\n    wsfc_domain_profile_object = WsfcDomainProfile(domain_fqdn=domain_fqdn,\n                                                   ou_path=ou_path,\n                                                   cluster_bootstrap_account=cluster_bootstrap_account,\n                                                   cluster_operator_account=cluster_operator_account,\n                                                   sql_service_account=sql_service_account,\n                                                   file_share_witness_path=file_share_witness_path,\n                                                   storage_account_url=storage_account_url,\n                                                   storage_account_primary_key=storage_account_key)\n\n    sqlvm_group_object = SqlVirtualMachineGroup(sql_image_offer=sql_image_offer,\n                                                sql_image_sku=sql_image_sku,\n                                                wsfc_domain_profile=wsfc_domain_profile_object,\n                                                location=location,\n                                                tags=tags)\n\n    # Since it's a running operation, we will do the put and then the get to display the instance.\n    LongRunningOperation(cmd.cli_ctx)(sdk_no_wait(False, client.create_or_update, resource_group_name,\n                                                  sql_virtual_machine_group_name, sqlvm_group_object))\n\n    return client.get(resource_group_name, sql_virtual_machine_group_name)", "response": "Creates a SQL virtual machine group."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sqlvm_group_update(instance, domain_fqdn=None, sql_image_sku=None, sql_image_offer=None,\n                       cluster_operator_account=None, sql_service_account=None,\n                       storage_account_url=None, storage_account_key=None, cluster_bootstrap_account=None,\n                       file_share_witness_path=None, ou_path=None, tags=None):\n    '''\n    Updates a SQL virtual machine group.\n    '''\n    if sql_image_sku is not None:\n        instance.sql_image_sku = sql_image_sku\n    if sql_image_offer is not None:\n        instance.sql_image_offer = sql_image_offer\n    if domain_fqdn is not None:\n        instance.wsfc_domain_profile.domain_fqdn = domain_fqdn\n    if cluster_operator_account is not None:\n        instance.wsfc_domain_profile.cluster_operator_account = cluster_operator_account\n    if cluster_bootstrap_account is not None:\n        instance.wsfc_domain_profile.cluster_bootstrap_account = cluster_bootstrap_account\n    if sql_service_account is not None:\n        instance.wsfc_domain_profile.sql_service_account = sql_service_account\n    if storage_account_url is not None:\n        instance.wsfc_domain_profile.storage_account_url = storage_account_url\n    if storage_account_key is not None:\n        instance.wsfc_domain_profile.storage_access_key = storage_account_key\n    if file_share_witness_path is not None:\n        instance.wsfc_domain_profile.file_share_witness_path = file_share_witness_path\n    if ou_path is not None:\n        instance.wsfc_domain_profile.ou_path = ou_path\n    if tags is not None:\n        instance.tags = tags\n\n    return instance", "response": "Updates the SQL virtual machine group."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate an availability group listener", "response": "def sqlvm_aglistener_create(client, cmd, availability_group_listener_name, sql_virtual_machine_group_name,\n                            resource_group_name, availability_group_name, ip_address, subnet_resource_id,\n                            load_balancer_resource_id, probe_port, sql_virtual_machine_instances, port=1433,\n                            public_ip_address_resource_id=None):\n    '''\n    Creates an availability group listener\n    '''\n\n    if not is_valid_resource_id(subnet_resource_id):\n        raise CLIError(\"Invalid subnet resource id.\")\n    if not is_valid_resource_id(load_balancer_resource_id):\n        raise CLIError(\"Invalid load balancer resource id.\")\n    if public_ip_address_resource_id and not is_valid_resource_id(public_ip_address_resource_id):\n        raise CLIError(\"Invalid public IP address resource id.\")\n    for sqlvm in sql_virtual_machine_instances:\n        if not is_valid_resource_id(sqlvm):\n            raise CLIError(\"Invalid SQL virtual machine resource id.\")\n\n    # Create the private ip address\n    private_ip_object = PrivateIPAddress(ip_address=ip_address,\n                                         subnet_resource_id=subnet_resource_id\n                                         if is_valid_resource_id(subnet_resource_id) else None)\n\n    # Create the load balancer configurations\n    load_balancer_object = LoadBalancerConfiguration(private_ip_address=private_ip_object,\n                                                     public_ip_address_resource_id=public_ip_address_resource_id,\n                                                     load_balancer_resource_id=load_balancer_resource_id,\n                                                     probe_port=probe_port,\n                                                     sql_virtual_machine_instances=sql_virtual_machine_instances)\n\n    # Create the availability group listener object\n    ag_listener_object = AvailabilityGroupListener(availability_group_name=availability_group_name,\n                                                   load_balancer_configurations=load_balancer_object,\n                                                   port=port)\n\n    LongRunningOperation(cmd.cli_ctx)(sdk_no_wait(False, client.create_or_update, resource_group_name,\n                                                  sql_virtual_machine_group_name, availability_group_listener_name,\n                                                  ag_listener_object))\n\n    return client.get(resource_group_name, sql_virtual_machine_group_name, availability_group_listener_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sqlvm_create(client, cmd, location, sql_virtual_machine_name, resource_group_name,\n                 sql_server_license_type='PAYG', sql_virtual_machine_group_resource_id=None, cluster_bootstrap_account_password=None,\n                 cluster_operator_account_password=None, sql_service_account_password=None, enable_auto_patching=None,\n                 day_of_week=None, maintenance_window_starting_hour=None, maintenance_window_duration=None,\n                 enable_auto_backup=None, enable_encryption=False, retention_period=None, storage_account_url=None,\n                 storage_access_key=None, backup_password=None, backup_system_dbs=False, backup_schedule_type=None,\n                 full_backup_frequency=None, full_backup_start_time=None, full_backup_window_hours=None, log_backup_frequency=None,\n                 enable_key_vault_credential=None, credential_name=None, azure_key_vault_url=None, service_principal_name=None,\n                 service_principal_secret=None, connectivity_type=None, port=None, sql_auth_update_username=None,\n                 sql_auth_update_password=None, sql_workload_type=None, enable_r_services=None, tags=None):\n    '''\n    Creates a SQL virtual machine.\n    '''\n    from azure.cli.core.commands.client_factory import get_subscription_id\n\n    subscription_id = get_subscription_id(cmd.cli_ctx)\n\n    virtual_machine_resource_id = resource_id(\n        subscription=subscription_id, resource_group=resource_group_name,\n        namespace='Microsoft.Compute', type='virtualMachines', name=sql_virtual_machine_name)\n\n    if sql_virtual_machine_group_resource_id and not is_valid_resource_id(sql_virtual_machine_group_resource_id):\n        raise CLIError(\"Invalid SQL virtual machine group resource id.\")\n\n    tags = tags or {}\n\n    wsfc_domain_credentials_object = WsfcDomainCredentials(cluster_bootstrap_account_password=cluster_bootstrap_account_password,\n                                                           cluster_operator_account_password=cluster_operator_account_password,\n                                                           sql_service_account_password=sql_service_account_password)\n\n    # If customer has provided any auto_patching settings, enabling plugin should be True\n    if (day_of_week or maintenance_window_duration or maintenance_window_starting_hour):\n        enable_auto_patching = True\n\n    auto_patching_object = AutoPatchingSettings(enable=enable_auto_patching,\n                                                day_of_week=day_of_week,\n                                                maintenance_window_starting_hour=maintenance_window_starting_hour,\n                                                maintenance_window_duration=maintenance_window_duration)\n\n    # If customer has provided any auto_backup settings, enabling plugin should be True\n    if (enable_encryption or retention_period or storage_account_url or storage_access_key or backup_password or\n            backup_system_dbs or backup_schedule_type or full_backup_frequency or full_backup_start_time or\n            full_backup_window_hours or log_backup_frequency):\n        enable_auto_backup = True\n\n    auto_backup_object = AutoBackupSettings(enable=enable_auto_backup,\n                                            enable_encryption=enable_encryption if enable_auto_backup else None,\n                                            retention_period=retention_period,\n                                            storage_account_url=storage_account_url,\n                                            storage_access_key=storage_access_key,\n                                            password=backup_password,\n                                            backup_system_dbs=backup_system_dbs if enable_auto_backup else None,\n                                            backup_schedule_type=backup_schedule_type,\n                                            full_backup_frequency=full_backup_frequency,\n                                            full_backup_start_time=full_backup_start_time,\n                                            full_backup_window_hours=full_backup_window_hours,\n                                            log_backup_frequency=log_backup_frequency)\n\n    # If customer has provided any key_vault_credential settings, enabling plugin should be True\n    if (credential_name or azure_key_vault_url or service_principal_name or service_principal_secret):\n        enable_key_vault_credential = True\n\n    keyvault_object = KeyVaultCredentialSettings(enable=enable_key_vault_credential,\n                                                 credential_name=credential_name,\n                                                 azure_key_vault_url=azure_key_vault_url,\n                                                 service_principal_name=service_principal_name,\n                                                 service_principal_secret=service_principal_secret)\n\n    connectivity_object = SqlConnectivityUpdateSettings(port=port,\n                                                        connectivity_type=connectivity_type,\n                                                        sql_auth_update_user_name=sql_auth_update_username,\n                                                        sql_auth_update_password=sql_auth_update_password)\n\n    workload_type_object = SqlWorkloadTypeUpdateSettings(sql_workload_type=sql_workload_type)\n\n    additional_features_object = AdditionalFeaturesServerConfigurations(is_rservices_enabled=enable_r_services)\n\n    server_configuration_object = ServerConfigurationsManagementSettings(sql_connectivity_update_settings=connectivity_object,\n                                                                         sql_workload_type_update_settings=workload_type_object,\n                                                                         additional_features_server_configurations=additional_features_object)\n\n    sqlvm_object = SqlVirtualMachine(location=location,\n                                     virtual_machine_resource_id=virtual_machine_resource_id,\n                                     sql_server_license_type=sql_server_license_type,\n                                     sql_virtual_machine_group_resource_id=sql_virtual_machine_group_resource_id,\n                                     wsfc_domain_credentials=wsfc_domain_credentials_object,\n                                     auto_patching_settings=auto_patching_object,\n                                     auto_backup_settings=auto_backup_object,\n                                     key_vault_credential_settings=keyvault_object,\n                                     server_configurations_management_settings=server_configuration_object,\n                                     tags=tags)\n\n    # Since it's a running operation, we will do the put and then the get to display the instance.\n    LongRunningOperation(cmd.cli_ctx)(sdk_no_wait(False, client.create_or_update,\n                                                  resource_group_name, sql_virtual_machine_name, sqlvm_object))\n\n    return client.get(resource_group_name, sql_virtual_machine_name)", "response": "Create a SQL virtual machine."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sqlvm_update(instance, sql_server_license_type=None, enable_auto_patching=None,\n                 day_of_week=None, maintenance_window_starting_hour=None, maintenance_window_duration=None,\n                 enable_auto_backup=None, enable_encryption=False, retention_period=None, storage_account_url=None,\n                 storage_access_key=None, backup_password=None, backup_system_dbs=False, backup_schedule_type=None,\n                 full_backup_frequency=None, full_backup_start_time=None, full_backup_window_hours=None, log_backup_frequency=None,\n                 enable_key_vault_credential=None, credential_name=None, azure_key_vault_url=None, service_principal_name=None,\n                 service_principal_secret=None, connectivity_type=None, port=None, sql_workload_type=None, enable_r_services=None, tags=None):\n    '''\n    Updates a SQL virtual machine.\n    '''\n    if tags is not None:\n        instance.tags = tags\n    if sql_server_license_type is not None:\n        instance.sql_server_license_type = sql_server_license_type\n\n    if (enable_auto_patching is not None or day_of_week is not None or maintenance_window_starting_hour is not None or maintenance_window_duration is not None):\n\n        enable_auto_patching = enable_auto_patching if enable_auto_patching is False else True\n        instance.auto_patching_settings = AutoPatchingSettings(enable=enable_auto_patching,\n                                                               day_of_week=day_of_week,\n                                                               maintenance_window_starting_hour=maintenance_window_starting_hour,\n                                                               maintenance_window_duration=maintenance_window_duration)\n\n    if (enable_auto_backup is not None or enable_encryption or retention_period is not None or storage_account_url is not None or\n            storage_access_key is not None or backup_password is not None or backup_system_dbs or backup_schedule_type is not None or\n            full_backup_frequency is not None or full_backup_start_time is not None or full_backup_window_hours is not None or\n            log_backup_frequency is not None):\n\n        enable_auto_backup = enable_auto_backup if enable_auto_backup is False else True\n        instance.auto_backup_settings = AutoBackupSettings(enable=enable_auto_backup,\n                                                           enable_encryption=enable_encryption if enable_auto_backup else None,\n                                                           retention_period=retention_period,\n                                                           storage_account_url=storage_account_url,\n                                                           storage_access_key=storage_access_key,\n                                                           password=backup_password,\n                                                           backup_system_dbs=backup_system_dbs if enable_auto_backup else None,\n                                                           backup_schedule_type=backup_schedule_type,\n                                                           full_backup_frequency=full_backup_frequency,\n                                                           full_backup_start_time=full_backup_start_time,\n                                                           full_backup_window_hours=full_backup_window_hours,\n                                                           log_backup_frequency=log_backup_frequency)\n\n    if (enable_key_vault_credential is not None or credential_name is not None or azure_key_vault_url is not None or\n            service_principal_name is not None or service_principal_secret is not None):\n\n        enable_key_vault_credential = enable_key_vault_credential if enable_key_vault_credential is False else True\n        instance.key_vault_credential_settings = KeyVaultCredentialSettings(enable=enable_key_vault_credential,\n                                                                            credential_name=credential_name,\n                                                                            service_principal_name=service_principal_name,\n                                                                            service_principal_secret=service_principal_secret,\n                                                                            azure_key_vault_url=azure_key_vault_url)\n\n    instance.server_configurations_management_settings = ServerConfigurationsManagementSettings()\n\n    if (connectivity_type is not None or port is not None):\n        instance.server_configurations_management_settings.sql_connectivity_update_settings = SqlConnectivityUpdateSettings(connectivity_type=connectivity_type,\n                                                                                                                            port=port)\n\n    if sql_workload_type is not None:\n        instance.server_configurations_management_settings.sql_workload_type_update_settings = SqlWorkloadTypeUpdateSettings(sql_workload_type=sql_workload_type)\n\n    if enable_r_services is not None:\n        instance.server_configurations_management_settings.additional_features_server_configurations = AdditionalFeaturesServerConfigurations(is_rservices_enabled=enable_r_services)\n\n    # If none of the settings was modified, reset server_configurations_management_settings to be null\n    if (instance.server_configurations_management_settings.sql_connectivity_update_settings is None and\n            instance.server_configurations_management_settings.sql_workload_type_update_settings is None and\n            instance.server_configurations_management_settings.sql_storage_update_settings is None and\n            instance.server_configurations_management_settings.additional_features_server_configurations is None):\n        instance.server_configurations_management_settings = None\n\n    return instance", "response": "Updates an SQL virtual machine."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_sqlvm_to_group(instance, sql_virtual_machine_group_resource_id, sql_service_account_password,\n                       cluster_operator_account_password, cluster_bootstrap_account_password=None):\n    '''\n    Add a SQL virtual machine to a SQL virtual machine group.\n    '''\n\n    if not is_valid_resource_id(sql_virtual_machine_group_resource_id):\n        raise CLIError(\"Invalid SQL virtual machine group resource id.\")\n\n    instance.sql_virtual_machine_group_resource_id = sql_virtual_machine_group_resource_id\n    instance.wsfc_domain_credentials = WsfcDomainCredentials(cluster_bootstrap_account_password=cluster_bootstrap_account_password,\n                                                             cluster_operator_account_password=cluster_operator_account_password,\n                                                             sql_service_account_password=sql_service_account_password)\n    return instance", "response": "Adds a SQL virtual machine to an SQL virtual machine group."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a SQL virtual machine to an availability group listener.", "response": "def add_sqlvm_to_aglistener(instance, sqlvm_resource_id):\n    '''\n    Add a SQL virtual machine to an availability group listener.\n    '''\n    if not is_valid_resource_id(sqlvm_resource_id):\n        raise CLIError(\"Invalid SQL virtual machine resource id.\")\n\n    vm_list = instance.load_balancer_configurations[0].sql_virtual_machine_instances\n\n    if sqlvm_resource_id not in vm_list:\n        instance.load_balancer_configurations[0].sql_virtual_machine_instances.append(sqlvm_resource_id)\n\n    return instance"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remove_sqlvm_from_aglistener(instance, sqlvm_resource_id):\n    '''\n    Remove a SQL virtual machine from an availability group listener.\n    '''\n    if not is_valid_resource_id(sqlvm_resource_id):\n        raise CLIError(\"Invalid SQL virtual machine resource id.\")\n\n    vm_list = instance.load_balancer_configurations[0].sql_virtual_machine_instances\n\n    if sqlvm_resource_id in vm_list:\n        instance.load_balancer_configurations[0].sql_virtual_machine_instances.remove(sqlvm_resource_id)\n\n    return instance", "response": "Removes a SQL virtual machine from an availability group listener."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef models(cls, api_version=DEFAULT_API_VERSION):\n        if api_version == '2015-06-15':\n            from .v2015_06_15 import models\n            return models\n        elif api_version == '2016-09-01':\n            from .v2016_09_01 import models\n            return models\n        elif api_version == '2016-12-01':\n            from .v2016_12_01 import models\n            return models\n        elif api_version == '2017-03-01':\n            from .v2017_03_01 import models\n            return models\n        elif api_version == '2017-06-01':\n            from .v2017_06_01 import models\n            return models\n        elif api_version == '2017-08-01':\n            from .v2017_08_01 import models\n            return models\n        elif api_version == '2017-09-01':\n            from .v2017_09_01 import models\n            return models\n        elif api_version == '2017-10-01':\n            from .v2017_10_01 import models\n            return models\n        elif api_version == '2017-11-01':\n            from .v2017_11_01 import models\n            return models\n        elif api_version == '2018-01-01':\n            from .v2018_01_01 import models\n            return models\n        elif api_version == '2018-02-01':\n            from .v2018_02_01 import models\n            return models\n        elif api_version == '2018-04-01':\n            from .v2018_04_01 import models\n            return models\n        raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))", "response": "Return a new instance of the models module."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef application_gateways(self):\n        api_version = self._get_api_version('application_gateways')\n        if api_version == '2015-06-15':\n            from .v2015_06_15.operations import ApplicationGatewaysOperations as OperationClass\n        elif api_version == '2016-09-01':\n            from .v2016_09_01.operations import ApplicationGatewaysOperations as OperationClass\n        elif api_version == '2016-12-01':\n            from .v2016_12_01.operations import ApplicationGatewaysOperations as OperationClass\n        elif api_version == '2017-03-01':\n            from .v2017_03_01.operations import ApplicationGatewaysOperations as OperationClass\n        elif api_version == '2017-06-01':\n            from .v2017_06_01.operations import ApplicationGatewaysOperations as OperationClass\n        elif api_version == '2017-08-01':\n            from .v2017_08_01.operations import ApplicationGatewaysOperations as OperationClass\n        elif api_version == '2017-09-01':\n            from .v2017_09_01.operations import ApplicationGatewaysOperations as OperationClass\n        elif api_version == '2017-10-01':\n            from .v2017_10_01.operations import ApplicationGatewaysOperations as OperationClass\n        elif api_version == '2017-11-01':\n            from .v2017_11_01.operations import ApplicationGatewaysOperations as OperationClass\n        elif api_version == '2018-01-01':\n            from .v2018_01_01.operations import ApplicationGatewaysOperations as OperationClass\n        elif api_version == '2018-02-01':\n            from .v2018_02_01.operations import ApplicationGatewaysOperations as OperationClass\n        elif api_version == '2018-04-01':\n            from .v2018_04_01.operations import ApplicationGatewaysOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))", "response": "Get all ApplicationGateways API Version."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the current version of available endpoint services.", "response": "def available_endpoint_services(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2017-06-01: :class:`AvailableEndpointServicesOperations<azure.mgmt.network.v2017_06_01.operations.AvailableEndpointServicesOperations>`\n           * 2017-08-01: :class:`AvailableEndpointServicesOperations<azure.mgmt.network.v2017_08_01.operations.AvailableEndpointServicesOperations>`\n           * 2017-09-01: :class:`AvailableEndpointServicesOperations<azure.mgmt.network.v2017_09_01.operations.AvailableEndpointServicesOperations>`\n           * 2017-10-01: :class:`AvailableEndpointServicesOperations<azure.mgmt.network.v2017_10_01.operations.AvailableEndpointServicesOperations>`\n           * 2017-11-01: :class:`AvailableEndpointServicesOperations<azure.mgmt.network.v2017_11_01.operations.AvailableEndpointServicesOperations>`\n           * 2018-01-01: :class:`AvailableEndpointServicesOperations<azure.mgmt.network.v2018_01_01.operations.AvailableEndpointServicesOperations>`\n           * 2018-02-01: :class:`AvailableEndpointServicesOperations<azure.mgmt.network.v2018_02_01.operations.AvailableEndpointServicesOperations>`\n           * 2018-04-01: :class:`AvailableEndpointServicesOperations<azure.mgmt.network.v2018_04_01.operations.AvailableEndpointServicesOperations>`\n        \"\"\"\n        api_version = self._get_api_version('available_endpoint_services')\n        if api_version == '2017-06-01':\n            from .v2017_06_01.operations import AvailableEndpointServicesOperations as OperationClass\n        elif api_version == '2017-08-01':\n            from .v2017_08_01.operations import AvailableEndpointServicesOperations as OperationClass\n        elif api_version == '2017-09-01':\n            from .v2017_09_01.operations import AvailableEndpointServicesOperations as OperationClass\n        elif api_version == '2017-10-01':\n            from .v2017_10_01.operations import AvailableEndpointServicesOperations as OperationClass\n        elif api_version == '2017-11-01':\n            from .v2017_11_01.operations import AvailableEndpointServicesOperations as OperationClass\n        elif api_version == '2018-01-01':\n            from .v2018_01_01.operations import AvailableEndpointServicesOperations as OperationClass\n        elif api_version == '2018-02-01':\n            from .v2018_02_01.operations import AvailableEndpointServicesOperations as OperationClass\n        elif api_version == '2018-04-01':\n            from .v2018_04_01.operations import AvailableEndpointServicesOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ddos_protection_plans(self):\n        api_version = self._get_api_version('ddos_protection_plans')\n        if api_version == '2018-02-01':\n            from .v2018_02_01.operations import DdosProtectionPlansOperations as OperationClass\n        elif api_version == '2018-04-01':\n            from .v2018_04_01.operations import DdosProtectionPlansOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))", "response": "Return an instance of the Ddos Protection Plans API Version 2. 0."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning an instance of the ExpressRouteCircuitConnections API version 2. 0.", "response": "def express_route_circuit_connections(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2018-02-01: :class:`ExpressRouteCircuitConnectionsOperations<azure.mgmt.network.v2018_02_01.operations.ExpressRouteCircuitConnectionsOperations>`\n           * 2018-04-01: :class:`ExpressRouteCircuitConnectionsOperations<azure.mgmt.network.v2018_04_01.operations.ExpressRouteCircuitConnectionsOperations>`\n        \"\"\"\n        api_version = self._get_api_version('express_route_circuit_connections')\n        if api_version == '2018-02-01':\n            from .v2018_02_01.operations import ExpressRouteCircuitConnectionsOperations as OperationClass\n        elif api_version == '2018-04-01':\n            from .v2018_04_01.operations import ExpressRouteCircuitConnectionsOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef express_route_cross_connection_peerings(self):\n        api_version = self._get_api_version('express_route_cross_connection_peerings')\n        if api_version == '2018-02-01':\n            from .v2018_02_01.operations import ExpressRouteCrossConnectionPeeringsOperations as OperationClass\n        elif api_version == '2018-04-01':\n            from .v2018_04_01.operations import ExpressRouteCrossConnectionPeeringsOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))", "response": "Return an instance of the ExpressRouteCrossConnectionPeerings API Version 2. 0."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef express_route_cross_connections(self):\n        api_version = self._get_api_version('express_route_cross_connections')\n        if api_version == '2018-02-01':\n            from .v2018_02_01.operations import ExpressRouteCrossConnectionsOperations as OperationClass\n        elif api_version == '2018-04-01':\n            from .v2018_04_01.operations import ExpressRouteCrossConnectionsOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))", "response": "Return an instance of the ExpressRouteCrossConnections API version 2. 0."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef hub_virtual_network_connections(self):\n        api_version = self._get_api_version('hub_virtual_network_connections')\n        if api_version == '2018-04-01':\n            from .v2018_04_01.operations import HubVirtualNetworkConnectionsOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))", "response": "Returns an instance of the Hub Virtual Network Connections API version."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning an instance of the Virtual Hubs API version.", "response": "def virtual_hubs(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2018-04-01: :class:`VirtualHubsOperations<azure.mgmt.network.v2018_04_01.operations.VirtualHubsOperations>`\n        \"\"\"\n        api_version = self._get_api_version('virtual_hubs')\n        if api_version == '2018-04-01':\n            from .v2018_04_01.operations import VirtualHubsOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning an instance of the VirtualWANsOperations class.", "response": "def virtual_wa_ns(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2018-04-01: :class:`VirtualWANsOperations<azure.mgmt.network.v2018_04_01.operations.VirtualWANsOperations>`\n        \"\"\"\n        api_version = self._get_api_version('virtual_wa_ns')\n        if api_version == '2018-04-01':\n            from .v2018_04_01.operations import VirtualWANsOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning an instance of the VpnConnectionsOperations API version available for this resource.", "response": "def vpn_connections(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2018-04-01: :class:`VpnConnectionsOperations<azure.mgmt.network.v2018_04_01.operations.VpnConnectionsOperations>`\n        \"\"\"\n        api_version = self._get_api_version('vpn_connections')\n        if api_version == '2018-04-01':\n            from .v2018_04_01.operations import VpnConnectionsOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef vpn_gateways(self):\n        api_version = self._get_api_version('vpn_gateways')\n        if api_version == '2018-04-01':\n            from .v2018_04_01.operations import VpnGatewaysOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))", "response": "Returns an instance of the VpnGateways API version 2. 0 Operation class."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns an instance of the VpnSitesOperations class that provides access to the VPN sites.", "response": "def vpn_sites(self):\n        \"\"\"Instance depends on the API version:\n\n           * 2018-04-01: :class:`VpnSitesOperations<azure.mgmt.network.v2018_04_01.operations.VpnSitesOperations>`\n        \"\"\"\n        api_version = self._get_api_version('vpn_sites')\n        if api_version == '2018-04-01':\n            from .v2018_04_01.operations import VpnSitesOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef vpn_sites_configuration(self):\n        api_version = self._get_api_version('vpn_sites_configuration')\n        if api_version == '2018-04-01':\n            from .v2018_04_01.operations import VpnSitesConfigurationOperations as OperationClass\n        else:\n            raise NotImplementedError(\"APIVersion {} is not available\".format(api_version))\n        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))", "response": "Returns an instance of the VpnSitesConfigurationOperations class that provides access to the VPNSitesConfiguration API."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __install_node_dependencies(kudu_client):\n    if not kudu_client._KuduClient__initialized:  # pylint:disable=protected-access\n        kudu_client._KuduClient__initialize()  # pylint:disable=protected-access\n\n    payload = {\n        'command': 'npm install',\n        'dir': r'site\\wwwroot'\n    }\n    response = requests.post(kudu_client._KuduClient__scm_url + '/api/command', data=json.dumps(payload),  # pylint:disable=protected-access\n                             headers=kudu_client._KuduClient__get_application_json_headers())   # pylint:disable=protected-access\n    HttpResponseValidator.check_response_status(response)\n    return response.json()", "response": "Installs Node. js dependencies at site \\ wwwroot \\"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\npublish local bot code to Azure.", "response": "def publish_app(cmd, client, resource_group_name, resource_name, code_dir=None, proj_name=None, version='v3'):\n    \"\"\"Publish local bot code to Azure.\n\n    This method is directly called via \"bot publish\"\n\n    :param cmd:\n    :param client:\n    :param resource_group_name:\n    :param resource_name:\n    :param code_dir:\n    :param proj_name:\n    :param version:\n    :return:\n    \"\"\"\n    if version == 'v3':\n        return publish_appv3(cmd, client, resource_group_name, resource_name, code_dir)\n\n    # Get the bot information and ensure it's not only a registration bot.\n    bot = client.bots.get(\n        resource_group_name=resource_group_name,\n        resource_name=resource_name\n    )\n    if bot.kind == 'bot':\n        raise CLIError('Bot kind is \\'bot\\', meaning it is a registration bot. '\n                       'Source publish is not supported for registration only bots.')\n\n    # If the user does not pass in a path to the local bot project, get the current working directory.\n    if not code_dir:\n        code_dir = os.getcwd()\n\n        logger.info('Parameter --code-dir not provided, defaulting to current working directory, %s. '\n                    'For more information, run \\'az bot publish -h\\'', code_dir)\n\n    if not os.path.isdir(code_dir):\n        raise CLIError('The path %s is not a valid directory. '\n                       'Please supply a valid directory path containing your source code.' % code_dir)\n\n    # Ensure that the directory contains appropriate post deploy scripts folder\n    if 'PostDeployScripts' not in os.listdir(code_dir):\n        BotPublishPrep.prepare_publish_v4(logger, code_dir, proj_name)\n\n    logger.info('Creating upload zip file.')\n    zip_filepath = BotPublishPrep.create_upload_zip(logger, code_dir, include_node_modules=False)\n    logger.info('Zip file path created, at %s.', zip_filepath)\n\n    kudu_client = KuduClient(cmd, resource_group_name, resource_name, bot)\n    output = kudu_client.publish(zip_filepath)\n    logger.info('Bot source published. Preparing bot application to run the new source.')\n    os.remove('upload.zip')\n    if os.path.exists(os.path.join('.', 'package.json')):\n        logger.info('Detected language javascript. Installing node dependencies in remote bot.')\n        __install_node_dependencies(kudu_client)\n\n    if output.get('active'):\n        logger.info('Deployment successful!')\n\n    if not output.get('active'):\n        scm_url = output.get('url')\n        deployment_id = output.get('id')\n        # Instead of replacing \"latest\", which would could be in the bot name, we replace \"deployments/latest\"\n        deployment_url = scm_url.replace('deployments/latest', 'deployments/%s' % deployment_id)\n        logger.error('Deployment failed. To find out more information about this deployment, please visit %s.'\n                     % deployment_url)\n\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_child(self, child_name):  # pylint: disable=no-self-use\n        child = self.children.get(child_name, None)\n        if child:\n            return child\n        raise ValueError(\"Value {} not in this tree\".format(child_name))", "response": "returns the object with the name supplied"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True if a command is in the tree False otherwise.", "response": "def in_tree(self, cmd_args):\n        \"\"\" if a command is in the tree \"\"\"\n        if not cmd_args:\n            return True\n        tree = self\n        try:\n            for datum in cmd_args:\n                tree = tree.get_child(datum)\n        except ValueError:\n            return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef generate_payload(self):\n        events = []\n        transformation_task = self._get_alias_transformation_properties()\n        transformation_task.update(self._get_based_properties())\n        events.append(transformation_task)\n\n        for exception in self.exceptions:\n            properties = {\n                'Reserved.DataModel.Fault.TypeString': exception.__class__.__name__,\n                'Reserved.DataModel.Fault.Exception.Message': self.get_exception_message(exception),\n                'Reserved.DataModel.Fault.Exception.StackTrace': _get_stack_trace(),\n            }\n            self.set_custom_properties(properties, 'ActionType', 'Exception')\n            self.set_custom_properties(properties, 'Version', VERSION)\n            events.append(properties)\n\n        return events", "response": "Generate a list of telemetry events as payload"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sort_completions(completions_gen):\n    from knack.help import REQUIRED_TAG\n\n    def _get_weight(val):\n        \"\"\" weights the completions with required things first the lexicographically\"\"\"\n        priority = ''\n        if val.display_meta and val.display_meta.startswith(REQUIRED_TAG):\n            priority = ' '  # a space has the lowest ordinance\n        return priority + val.text\n\n    return sorted(completions_gen, key=_get_weight)", "response": "sorts the completions in a list of completions"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nvalidates that a param should be completed", "response": "def validate_param_completion(self, param, leftover_args):\n        \"\"\" validates that a param should be completed \"\"\"\n        # validates param starts with unfinished word\n        completes = self.validate_completion(param)\n\n        # show parameter completions when started\n        full_param = self.unfinished_word.startswith(\"--\") and param.startswith(\"--\")\n        char_param = self.unfinished_word.startswith(\"-\") and not param.startswith(\"--\")\n\n        # show full parameters before any are used\n        new_param = not self.unfinished_word and not leftover_args and param.startswith(\"--\")\n\n        # checks for parameters already in the line as well as aliases\n        no_doubles = True\n        command_doubles = self.command_param_info.get(self.current_command, {})\n        for alias in command_doubles.get(param, []):\n            if alias in leftover_args:\n                no_doubles = False\n\n        return completes and no_doubles and any((full_param, char_param, new_param))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef gen_enum_completions(self, arg_name):\n        try:  # if enum completion\n            for choice in self.cmdtab[self.current_command].arguments[arg_name].choices:\n                if self.validate_completion(choice):\n                    yield Completion(choice, -len(self.unfinished_word))\n\n        except TypeError:  # there is no choices option\n            pass", "response": "generates dynamic enumeration completions"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the name of the argument used in the command table for a parameter", "response": "def get_arg_name(self, param):\n        \"\"\" gets the argument name used in the command table for a parameter \"\"\"\n        if self.current_command in self.cmdtab:\n            for arg in self.cmdtab[self.current_command].arguments:\n\n                for name in self.cmdtab[self.current_command].arguments[arg].options_list:\n                    if name == param:\n                        return arg\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mute_parse_args(self, text):\n        error = AzCliCommandParser.error\n        _check_value = AzCliCommandParser._check_value\n\n        AzCliCommandParser.error = error_pass\n        AzCliCommandParser._check_value = _check_value_muted\n\n        # No exception is expected. However, we add this try-catch block, as this may have far-reaching effects.\n        try:\n            parse_args = self.argsfinder.get_parsed_args(parse_quotes(text, quotes=False, string=False))\n        except Exception:  # pylint: disable=broad-except\n            pass\n\n        AzCliCommandParser.error = error\n        AzCliCommandParser._check_value = _check_value\n        return parse_args", "response": "mutes the parser error when parsing then puts it back"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate the dynamic values like the names of resource groups", "response": "def gen_dynamic_completions(self, text):\n        \"\"\" generates the dynamic values, like the names of resource groups \"\"\"\n        try:  # pylint: disable=too-many-nested-blocks\n            param = self.leftover_args[-1]\n\n            # command table specific name\n            arg_name = self.get_arg_name(param)\n\n            for comp in self.gen_enum_completions(arg_name):\n                yield comp\n\n            parsed_args = self.mute_parse_args(text)\n\n            # there are 3 formats for completers the cli uses\n            # this try catches which format it is\n            if self.cmdtab[self.current_command].arguments[arg_name].completer:\n                completions = []\n                try:\n                    completions = self.cmdtab[self.current_command].arguments[arg_name].completer(\n                        prefix=self.unfinished_word, action=None, parsed_args=parsed_args)\n                except TypeError:\n                    try:\n                        completions = self.cmdtab[self.current_command].arguments[arg_name].completer(\n                            prefix=self.unfinished_word)\n                    except TypeError:\n                        try:\n                            completions = self.cmdtab[self.current_command].arguments[arg_name].completer()\n                        except TypeError:\n                            pass  # other completion method used\n\n                for comp in completions:\n                    for completion in self.process_dynamic_completion(comp):\n                        yield completion\n\n        # if the user isn't logged in\n        except Exception:  # pylint: disable=broad-except\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef gen_cmd_and_param_completions(self):\n        if self.complete_command:\n            for param in self.command_param_info.get(self.current_command, []):\n                if self.validate_param_completion(param, self.leftover_args):\n                    yield self.yield_param_completion(param, self.unfinished_word)\n        elif not self.leftover_args:\n            for child_command in self.subtree.children:\n                if self.validate_completion(child_command):\n                    yield Completion(child_command, -len(self.unfinished_word))", "response": "Generates command and parameter completions"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef has_description(self, param):\n        return param in self.param_description.keys() and \\\n            not self.param_description[param].isspace()", "response": "Returns True if the parameter has a description"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef reformat_cmd(self, text):\n        # remove az if there\n        text = text.replace('az', '')\n        # disregard defaulting symbols\n        if text and SELECT_SYMBOL['scope'] == text[0:2]:\n            text = text.replace(SELECT_SYMBOL['scope'], \"\")\n\n        if self.shell_ctx.default_command:\n            text = self.shell_ctx.default_command + ' ' + text\n        return text", "response": "reformat the text to be stripped of noise"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove some often - empty fields from a list of ManagedClusters so the JSON representation doesn t contain distracting null fields.", "response": "def _remove_nulls(managed_clusters):\n    \"\"\"\n    Remove some often-empty fields from a list of ManagedClusters, so the JSON representation\n    doesn't contain distracting null fields.\n\n    This works around a quirk of the SDK for python behavior. These fields are not sent\n    by the server, but get recreated by the CLI's own \"to_dict\" serialization.\n    \"\"\"\n    attrs = ['tags']\n    ap_attrs = ['os_disk_size_gb', 'vnet_subnet_id']\n    sp_attrs = ['secret']\n    for managed_cluster in managed_clusters:\n        for attr in attrs:\n            if getattr(managed_cluster, attr, None) is None:\n                delattr(managed_cluster, attr)\n        for ap_profile in managed_cluster.agent_pool_profiles:\n            for attr in ap_attrs:\n                if getattr(ap_profile, attr, None) is None:\n                    delattr(ap_profile, attr)\n        for attr in sp_attrs:\n            if getattr(managed_cluster.service_principal_profile, attr, None) is None:\n                delattr(managed_cluster.service_principal_profile, attr)\n    return managed_clusters"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the parsed args from a patched parser", "response": "def get_parsed_args(self, comp_words):\n        \"\"\" gets the parsed args from a patched parser \"\"\"\n        active_parsers = self._patch_argument_parser()\n\n        parsed_args = argparse.Namespace()\n\n        self.completing = True\n        if USING_PYTHON2:\n            # Python 2 argparse only properly works with byte strings.\n            comp_words = [ensure_bytes(word) for word in comp_words]\n\n        try:\n            active_parsers[0].parse_known_args(comp_words, namespace=parsed_args)\n        except BaseException:  # pylint: disable=broad-except\n            pass\n\n        self.completing = False\n        return parsed_args"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_alias_table():\n    try:\n        alias_table = get_config_parser()\n        alias_table.read(azext_alias.alias.GLOBAL_ALIAS_PATH)\n        return alias_table\n    except Exception:  # pylint: disable=broad-except\n        return get_config_parser()", "response": "Get the current alias table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_alias_command(subcommands, args):\n    if not args:\n        return False\n\n    for subcommand in subcommands:\n        if args[:2] == ['alias', subcommand]:\n            return True\n\n    return False", "response": "Checks if the user is invoking an alias command."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef remove_pos_arg_placeholders(alias_command):\n    # Boundary index is the index at which named argument or positional argument starts\n    split_command = shlex.split(alias_command)\n    boundary_index = len(split_command)\n    for i, subcommand in enumerate(split_command):\n        if not re.match('^[a-z]', subcommand.lower()) or i > COLLISION_CHECK_LEVEL_DEPTH:\n            boundary_index = i\n            break\n\n    return ' '.join(split_command[:boundary_index]).lower()", "response": "Removes positional argument placeholders from alias_command."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfiltering aliases that do not have a command field in the configuration file.", "response": "def filter_aliases(alias_table):\n    \"\"\"\n    Filter aliases that does not have a command field in the configuration file.\n\n    Args:\n        alias_table: The alias table.\n\n    Yield:\n        A tuple with [0] being the first word of the alias and\n        [1] being the command that the alias points to.\n    \"\"\"\n    for alias in alias_table.sections():\n        if alias_table.has_option(alias, 'command'):\n            yield (alias.split()[0], remove_pos_arg_placeholders(alias_table.get(alias, 'command')))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbuilding a dictionary where the keys are all the alias commands and the values are all the parent commands of the keys.", "response": "def build_tab_completion_table(alias_table):\n    \"\"\"\n    Build a dictionary where the keys are all the alias commands (without positional argument placeholders)\n    and the values are all the parent commands of the keys. After that, write the table into a file.\n    The purpose of the dictionary is to validate the alias tab completion state.\n\n    For example:\n    {\n        \"group\": [\"\", \"ad\"],\n        \"dns\": [\"network\"]\n    }\n\n    Args:\n        alias_table: The alias table.\n\n    Returns:\n        The tab completion table.\n    \"\"\"\n    alias_commands = [t[1] for t in filter_aliases(alias_table)]\n    tab_completion_table = defaultdict(list)\n    for alias_command in alias_commands:\n        for reserved_command in azext_alias.cached_reserved_commands:\n            # Check if alias_command has no parent command\n            if reserved_command == alias_command or reserved_command.startswith(alias_command + ' ') \\\n                    and '' not in tab_completion_table[alias_command]:\n                tab_completion_table[alias_command].append('')\n            elif ' {} '.format(alias_command) in reserved_command or reserved_command.endswith(' ' + alias_command):\n                # Extract parent commands\n                index = reserved_command.index(alias_command)\n                parent_command = reserved_command[:index - 1]\n                if parent_command not in tab_completion_table[alias_command]:\n                    tab_completion_table[alias_command].append(parent_command)\n\n    with open(GLOBAL_ALIAS_TAB_COMP_TABLE_PATH, 'w') as f:\n        f.write(json.dumps(tab_completion_table))\n\n    return tab_completion_table"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef reduce_alias_table(alias_table):\n    for alias in alias_table.sections():\n        if alias_table.has_option(alias, 'command'):\n            yield (alias, alias_table.get(alias, 'command'))", "response": "Reduce the alias table to a tuple that contains the alias and the command that the alias points to."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving a file from an URL.", "response": "def retrieve_file_from_url(url):\n    \"\"\"\n    Retrieve a file from an URL\n\n    Args:\n        url: The URL to retrieve the file from.\n\n    Returns:\n        The absolute path of the downloaded file.\n    \"\"\"\n    try:\n        alias_source, _ = urlretrieve(url)\n        # Check for HTTPError in Python 2.x\n        with open(alias_source, 'r') as f:\n            content = f.read()\n            if content[:3].isdigit():\n                raise CLIError(ALIAS_FILE_URL_ERROR.format(url, content.strip()))\n    except Exception as exception:\n        if isinstance(exception, CLIError):\n            raise\n\n        # Python 3.x\n        raise CLIError(ALIAS_FILE_URL_ERROR.format(url, exception))\n\n    return alias_source"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef filter_alias_create_namespace(namespace):\n    def filter_string(s):\n        return ' '.join(s.strip().split())\n\n    namespace.alias_name = filter_string(namespace.alias_name)\n    namespace.alias_command = filter_string(namespace.alias_command)\n    return namespace", "response": "Filters alias name and alias command inside alias create namespace to appropriate strings."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_lexers(main_lex, exam_lex, tool_lex):\n    if not main_lex:\n        return None, None, None\n    lexer = None\n    if main_lex:\n        if issubclass(main_lex, PromptLex):\n            lexer = main_lex\n        elif issubclass(main_lex, PygLex):\n            lexer = PygmentsLexer(main_lex)\n\n    if exam_lex:\n        if issubclass(exam_lex, PygLex):\n            exam_lex = PygmentsLexer(exam_lex)\n\n    if tool_lex:\n        if issubclass(tool_lex, PygLex):\n            tool_lex = PygmentsLexer(tool_lex)\n\n    return lexer, exam_lex, tool_lex", "response": "gets all the lexer wrappers"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_anyhline(config):\n    if config.BOOLEAN_STATES[config.config.get('Layout', 'command_description')] or\\\n       config.BOOLEAN_STATES[config.config.get('Layout', 'param_description')]:\n        return Window(\n            width=LayoutDimension.exact(1),\n            height=LayoutDimension.exact(1),\n            content=FillControl('-', token=Token.Line))\n    return get_empty()", "response": "Returns any line between descriptions and example"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a horiztonal line", "response": "def get_hline():\n    \"\"\" gets a horiztonal line \"\"\"\n    return Window(\n        width=LayoutDimension.exact(1),\n        height=LayoutDimension.exact(1),\n        content=FillControl('-', token=Token.Line))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of descriptions for the current language.", "response": "def get_descriptions(config, exam_lex, lexer):\n    \"\"\" based on the configuration settings determines which windows to include \"\"\"\n    if config.BOOLEAN_STATES[config.config.get('Layout', 'command_description')]:\n        if config.BOOLEAN_STATES[config.config.get('Layout', 'param_description')]:\n            return VSplit([\n                get_descript(exam_lex),\n                get_vline(),\n                get_param(lexer),\n            ])\n        return get_descript(exam_lex)\n    if config.BOOLEAN_STATES[config.config.get('Layout', 'param_description')]:\n        return get_param(lexer)\n    return get_empty()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_tutorial_layout(self):\n        lexer, _, _ = get_lexers(self.shell_ctx.lexer, None, None)\n        layout_full = HSplit([\n            FloatContainer(\n                Window(\n                    BufferControl(\n                        input_processors=self.input_processors,\n                        lexer=lexer,\n                        preview_search=Always()),\n                    get_height=get_height),\n                [\n                    Float(xcursor=True,\n                          ycursor=True,\n                          content=CompletionsMenu(\n                              max_height=MAX_COMPLETION,\n                              scroll_offset=1,\n                              extra_filter=(HasFocus(DEFAULT_BUFFER))))]),\n            ConditionalContainer(\n                HSplit([\n                    get_hline(),\n                    get_param(lexer),\n                    get_hline(),\n                    Window(\n                        content=BufferControl(\n                            buffer_name='example_line',\n                            lexer=lexer\n                        ),\n                    ),\n                    Window(\n                        TokenListControl(\n                            get_tutorial_tokens,\n                            default_char=Char(' ', Token.Toolbar)),\n                        height=LayoutDimension.exact(1)),\n                ]),\n                filter=~IsDone() & RendererHeightIsKnown()\n            )\n        ])\n        return layout_full", "response": "Create a layout for example tutorial."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate the layout for the user.", "response": "def create_layout(self, exam_lex, toolbar_lex):\n        \"\"\" creates the layout \"\"\"\n        lexer, exam_lex, toolbar_lex = get_lexers(self.shell_ctx.lexer, exam_lex, toolbar_lex)\n\n        if not any(isinstance(processor, DefaultPrompt) for processor in self.input_processors):\n            self.input_processors.append(DefaultPrompt(self.get_prompt_tokens))\n\n        layout_lower = ConditionalContainer(\n            HSplit([\n                get_anyhline(self.shell_ctx.config),\n                get_descriptions(self.shell_ctx.config, exam_lex, lexer),\n                get_examplehline(self.shell_ctx.config),\n                get_example(self.shell_ctx.config, exam_lex),\n\n                ConditionalContainer(\n                    get_hline(),\n                    filter=self.show_default | self.show_symbol\n                ),\n                ConditionalContainer(\n                    Window(\n                        content=BufferControl(\n                            buffer_name='default_values',\n                            lexer=lexer\n                        )\n                    ),\n                    filter=self.show_default\n                ),\n                ConditionalContainer(\n                    get_hline(),\n                    filter=self.show_default & self.show_symbol\n                ),\n                ConditionalContainer(\n                    Window(\n                        content=BufferControl(\n                            buffer_name='symbols',\n                            lexer=exam_lex\n                        )\n                    ),\n                    filter=self.show_symbol\n                ),\n                ConditionalContainer(\n                    Window(\n                        content=BufferControl(\n                            buffer_name='progress',\n                            lexer=lexer\n                        )\n                    ),\n                    filter=self.show_progress\n                ),\n                Window(\n                    content=BufferControl(\n                        buffer_name='bottom_toolbar',\n                        lexer=toolbar_lex\n                    ),\n                ),\n            ]),\n            filter=~IsDone() & RendererHeightIsKnown()\n        )\n\n        layout_full = HSplit([\n            FloatContainer(\n                Window(\n                    BufferControl(\n                        input_processors=self.input_processors,\n                        lexer=lexer,\n                        preview_search=Always()),\n                    get_height=get_height,\n                ),\n                [\n                    Float(xcursor=True,\n                          ycursor=True,\n                          content=CompletionsMenu(\n                              max_height=MAX_COMPLETION,\n                              scroll_offset=1,\n                              extra_filter=(HasFocus(DEFAULT_BUFFER))))]),\n            layout_lower\n        ])\n\n        return layout_full"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ads_use_dev_spaces(cluster_name, resource_group_name, update=False, space_name=None, do_not_prompt=False):\n\n    azds_cli = _install_dev_spaces_cli(update)\n\n    use_command_arguments = [azds_cli, 'use', '--name', cluster_name,\n                             '--resource-group', resource_group_name]\n\n    if space_name is not None:\n        use_command_arguments.append('--space')\n        use_command_arguments.append(space_name)\n\n    if do_not_prompt:\n        use_command_arguments.append('-y')\n    subprocess.call(\n        use_command_arguments, universal_newlines=True)", "response": "Use Azure Dev Spaces with a managed Kubernetes cluster."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove Azure Dev Spaces from a managed Kubernetes cluster.", "response": "def ads_remove_dev_spaces(cluster_name, resource_group_name, do_not_prompt=False):\n    \"\"\"\n    Remove Azure Dev Spaces from a managed Kubernetes cluster.\n\n    :param cluster_name: Name of the managed cluster.\n    :type cluster_name: String\n    :param resource_group_name: Name of resource group. You can configure the default group. \\\n    Using 'az configure --defaults group=<name>'.\n    :type resource_group_name: String\n    :param do_not_prompt: Do not prompt for confirmation.\n    :type do_not_prompt: bool\n    \"\"\"\n\n    azds_cli = _install_dev_spaces_cli(False)\n\n    remove_command_arguments = [azds_cli, 'remove', '--name', cluster_name,\n                                '--resource-group', resource_group_name]\n    if do_not_prompt:\n        remove_command_arguments.append('-y')\n    subprocess.call(\n        remove_command_arguments, universal_newlines=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_query_targets(cli_ctx, apps, resource_group):\n    if isinstance(apps, list):\n        if resource_group:\n            return [get_id_from_azure_resource(cli_ctx, apps[0], resource_group)]\n        return list(map(lambda x: get_id_from_azure_resource(cli_ctx, x), apps))\n    else:\n        if resource_group:\n            return [get_id_from_azure_resource(cli_ctx, apps, resource_group)]\n        return apps", "response": "Produces a list of uniform GUIDs representing applications to query."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_linked_properties(cli_ctx, app, resource_group, read_properties=None, write_properties=None):\n    roles = {\n        \"ReadTelemetry\": \"api\",\n        \"WriteAnnotations\": \"annotations\",\n        \"AuthenticateSDKControlChannel\": \"agentconfig\"\n    }\n\n    sub_id = get_subscription_id(cli_ctx)\n    tmpl = '/subscriptions/{}/resourceGroups/{}/providers/microsoft.insights/components/{}'.format(\n        sub_id,\n        resource_group,\n        app\n    )\n    linked_read_properties, linked_write_properties = [], []\n\n    if isinstance(read_properties, list):\n        propLen = len(read_properties)\n        linked_read_properties = ['{}/{}'.format(tmpl, roles[read_properties[i]]) for i in range(propLen)]\n    else:\n        linked_read_properties = ['{}/{}'.format(tmpl, roles[read_properties])]\n    if isinstance(write_properties, list):\n        propLen = len(write_properties)\n        linked_write_properties = ['{}/{}'.format(tmpl, roles[write_properties[i]]) for i in range(propLen)]\n    else:\n        linked_write_properties = ['{}/{}'.format(tmpl, roles[write_properties])]\n    return linked_read_properties, linked_write_properties", "response": "Maps user - facing role names to strings used to identify them on resources."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntransforms the result of an SQL virtual machine group to eliminate unnecessary parameters.", "response": "def transform_sqlvm_group_output(result):\n    '''\n    Transforms the result of SQL virtual machine group to eliminate unnecessary parameters.\n    '''\n    from collections import OrderedDict\n    from msrestazure.tools import parse_resource_id\n    try:\n        resource_group = getattr(result, 'resource_group', None) or parse_resource_id(result.id)['resource_group']\n        wsfc_object = format_wsfc_domain_profile(result.wsfc_domain_profile)\n        # Create a dictionary with the relevant parameters\n        output = OrderedDict([('id', result.id),\n                              ('location', result.location),\n                              ('name', result.name),\n                              ('provisioningState', result.provisioning_state),\n                              ('sqlImageOffer', result.sql_image_offer),\n                              ('sqlImageSku', result.sql_image_sku),\n                              ('resourceGroup', resource_group),\n                              ('wsfcDomainProfile', wsfc_object),\n                              ('tags', result.tags)])\n        return output\n    except AttributeError:\n        # Return the response object if the formating fails\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntransform the SQL virtual machine group to eliminate unnecessary parameters.", "response": "def transform_sqlvm_output(result):\n    '''\n    Transforms the result of SQL virtual machine group to eliminate unnecessary parameters.\n    '''\n    from collections import OrderedDict\n    from msrestazure.tools import parse_resource_id\n    try:\n        resource_group = getattr(result, 'resource_group', None) or parse_resource_id(result.id)['resource_group']\n        # Create a dictionary with the relevant parameters\n        output = OrderedDict([('id', result.id),\n                              ('location', result.location),\n                              ('name', result.name),\n                              ('provisioningState', result.provisioning_state),\n                              ('sqlImageOffer', result.sql_image_offer),\n                              ('sqlImageSku', result.sql_image_sku),\n                              ('resourceGroup', resource_group),\n                              ('sqlServerLicenseType', result.sql_server_license_type),\n                              ('virtualMachineResourceId', result.virtual_machine_resource_id),\n                              ('tags', result.tags)])\n\n        # Note, wsfcDomainCredentials will not display\n        if result.sql_virtual_machine_group_resource_id is not None:\n            output['sqlVirtualMachineGroupResourceId'] = result.sql_virtual_machine_group_resource_id\n\n        return output\n    except AttributeError:\n        # Return the response object if the formating fails\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntransform the result of an availability group listener to eliminate unnecessary parameters.", "response": "def transform_aglistener_output(result):\n    '''\n    Transforms the result of Availability Group Listener to eliminate unnecessary parameters.\n    '''\n    from collections import OrderedDict\n    from msrestazure.tools import parse_resource_id\n    try:\n        resource_group = getattr(result, 'resource_group', None) or parse_resource_id(result.id)['resource_group']\n        # Create a dictionary with the relevant parameters\n        output = OrderedDict([('id', result.id),\n                              ('name', result.name),\n                              ('provisioningState', result.provisioning_state),\n                              ('port', result.port),\n                              ('resourceGroup', resource_group)])\n\n        # Note, wsfcDomainCredentials will not display\n        if result.load_balancer_configurations is not None:\n            output['loadBalancerConfigurations'] = format_load_balancer_configuration_list(result.load_balancer_configurations)\n\n        return output\n    except AttributeError:\n        # Return the response object if the formating fails\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nformat the WSFCDomainProfile object removing arguments that are empty", "response": "def format_wsfc_domain_profile(result):\n    '''\n    Formats the WSFCDomainProfile object removing arguments that are empty\n    '''\n    from collections import OrderedDict\n    # Only display parameters that have content\n    order_dict = OrderedDict()\n    if result.cluster_bootstrap_account is not None:\n        order_dict['clusterBootstrapAccount'] = result.cluster_bootstrap_account\n    if result.domain_fqdn is not None:\n        order_dict['domainFqdn'] = result.domain_fqdn\n    if result.ou_path is not None:\n        order_dict['ouPath'] = result.ou_path\n    if result.cluster_operator_account is not None:\n        order_dict['clusterOperatorAccount'] = result.cluster_operator_account\n    if result.file_share_witness_path is not None:\n        order_dict['fileShareWitnessPath'] = result.file_share_witness_path\n    if result.sql_service_account is not None:\n        order_dict['sqlServiceAccount'] = result.sql_service_account\n    if result.storage_account_url is not None:\n        order_dict['storageAccountUrl'] = result.storage_account_url\n\n    return order_dict"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nformatting the AdditionalFeaturesServerConfigurations object removing arguments that are empty", "response": "def format_additional_features_server_configurations(result):\n    '''\n    Formats the AdditionalFeaturesServerConfigurations object removing arguments that are empty\n    '''\n    from collections import OrderedDict\n    # Only display parameters that have content\n    order_dict = OrderedDict()\n    if result.is_rservices_enabled is not None:\n        order_dict['isRServicesEnabled'] = result.is_rservices_enabled\n    if result.backup_permissions_for_azure_backup_svc is not None:\n        order_dict['backupPermissionsForAzureBackupSvc'] = result.backup_permissions_for_azure_backup_svc\n\n    return order_dict"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef format_auto_backup_settings(result):\n    '''\n    Formats the AutoBackupSettings object removing arguments that are empty\n    '''\n    from collections import OrderedDict\n    # Only display parameters that have content\n    order_dict = OrderedDict()\n    if result.enable is not None:\n        order_dict['enable'] = result.enable\n    if result.enable_encryption is not None:\n        order_dict['enableEncryption'] = result.enable_encryption\n    if result.retention_period is not None:\n        order_dict['retentionPeriod'] = result.retention_period\n    if result.storage_account_url is not None:\n        order_dict['storageAccountUrl'] = result.storage_account_url\n    if result.backup_system_dbs is not None:\n        order_dict['backupSystemDbs'] = result.backup_system_dbs\n    if result.backup_schedule_type is not None:\n        order_dict['backupScheduleType'] = result.backup_schedule_type\n    if result.full_backup_frequency is not None:\n        order_dict['fullBackupFrequency'] = result.full_backup_frequency\n    if result.full_backup_start_time is not None:\n        order_dict['fullBackupStartTime'] = result.full_backup_start_time\n    if result.full_backup_window_hours is not None:\n        order_dict['fullBackupWindowHours'] = result.full_backup_window_hours\n    if result.log_backup_frequency is not None:\n        order_dict['logBackupFrequency'] = result.log_backup_frequency\n\n    return order_dict", "response": "Formats the AutoBackupSettings object removing arguments that are empty\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nformat the AutoPatchingSettings object removing arguments that are empty", "response": "def format_auto_patching_settings(result):\n    '''\n    Formats the AutoPatchingSettings object removing arguments that are empty\n    '''\n    from collections import OrderedDict\n    # Only display parameters that have content\n    order_dict = OrderedDict()\n    if result.enable is not None:\n        order_dict['enable'] = result.enable\n    if result.day_of_week is not None:\n        order_dict['dayOfWeek'] = result.day_of_week\n    if result.maintenance_window_starting_hour is not None:\n        order_dict['maintenanceWindowStartingHour'] = result.maintenance_window_starting_hour\n    if result.maintenance_window_duration is not None:\n        order_dict['maintenanceWindowDuration'] = result.maintenance_window_duration\n\n    return order_dict"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef format_key_vault_credential_settings(result):\n    '''\n    Formats the KeyVaultCredentialSettings object removing arguments that are empty\n    '''\n    from collections import OrderedDict\n    # Only display parameters that have content\n    order_dict = OrderedDict()\n    if result.enable is not None:\n        order_dict['enable'] = result.enable\n    if result.credential_name is not None:\n        order_dict['credentialName'] = result.credential_name\n    if result.azure_key_vault_url is not None:\n        order_dict['azureKeyVaultUrl'] = result.azure_key_vault_url\n\n    return order_dict", "response": "Formats the KeyVaultCredentialSettings object removing arguments that are empty\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef format_load_balancer_configuration(result):\n    '''\n    Formats the LoadBalancerConfiguration object removing arguments that are empty\n    '''\n    from collections import OrderedDict\n    # Only display parameters that have content\n    order_dict = OrderedDict()\n    if result.private_ip_address is not None:\n        order_dict['privateIpAddress'] = format_private_ip_address(result.private_ip_address)\n    if result.public_ip_address_resource_id is not None:\n        order_dict['publicIpAddressResourceId'] = result.public_ip_address_resource_id\n    if result.load_balancer_resource_id is not None:\n        order_dict['loadBalancerResourceId'] = result.load_balancer_resource_id\n    if result.probe_port is not None:\n        order_dict['probePort'] = result.probe_port\n    if result.sql_virtual_machine_instances is not None:\n        order_dict['sqlVirtualMachineInstances'] = result.sql_virtual_machine_instances\n\n    return order_dict", "response": "Formats the LoadBalancerConfiguration object removing arguments that are empty\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef format_private_ip_address(result):\n    '''\n    Formats the PrivateIPAddress object removing arguments that are empty\n    '''\n    from collections import OrderedDict\n    # Only display parameters that have content\n    order_dict = OrderedDict()\n    if result.ip_address is not None:\n        order_dict['ipAddress'] = result.ip_address\n    if result.subnet_resource_id is not None:\n        order_dict['subnetResourceId'] = result.subnet_resource_id\n\n    return order_dict", "response": "Formats the PrivateIPAddress object removing arguments that are empty\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nformat the ServerConfigurationsManagementSettings object removing arguments that are empty", "response": "def format_server_configuration_management_settings(result):\n    '''\n    Formats the ServerConfigurationsManagementSettings object removing arguments that are empty\n    '''\n    from collections import OrderedDict\n    order_dict = OrderedDict([('sqlConnectivityUpdateSettings',\n                               format_sql_connectivity_update_settings(result.sql_connectivity_update_settings)),\n                              ('sqlWorkloadTypeUpdateSettings', format_sql_workload_type_update_settings(result.sql_workload_type_update_settings)),\n                              ('sqlStorageUpdateSettings', format_sql_storage_update_settings(result.sql_storage_update_settings)),\n                              ('additionalFeaturesServerConfigurations',\n                               format_additional_features_server_configurations(result.additional_features_server_configurations))])\n\n    return order_dict"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef format_sql_connectivity_update_settings(result):\n    '''\n    Formats the SqlConnectivityUpdateSettings object removing arguments that are empty\n    '''\n    from collections import OrderedDict\n    # Only display parameters that have content\n    order_dict = OrderedDict()\n    if result.connectivity_type is not None:\n        order_dict['connectivityType'] = result.connectivity_type\n    if result.port is not None:\n        order_dict['port'] = result.port\n    if result.sql_auth_update_user_name is not None:\n        order_dict['sqlAuthUpdateUserName'] = result.sql_auth_update_user_name\n\n    return order_dict", "response": "Formats the SqlConnectivityUpdateSettings object removing arguments that are empty\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nformatting the SqlStorageUpdateSettings object removing arguments that are empty", "response": "def format_sql_storage_update_settings(result):\n    '''\n    Formats the SqlStorageUpdateSettings object removing arguments that are empty\n    '''\n    from collections import OrderedDict\n    # Only display parameters that have content\n    order_dict = OrderedDict()\n    if result.disk_count is not None:\n        order_dict['diskCount'] = result.disk_count\n    if result.disk_configuration_type is not None:\n        order_dict['diskConfigurationType'] = result.disk_configuration_type\n\n    return order_dict"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nformat the SqlWorkloadTypeUpdateSettings object removing arguments that are empty", "response": "def format_sql_workload_type_update_settings(result):\n    '''\n    Formats the SqlWorkloadTypeUpdateSettings object removing arguments that are empty\n    '''\n    from collections import OrderedDict\n    # Only display parameters that have content\n    order_dict = OrderedDict()\n    if result.sql_workload_type is not None:\n        order_dict['sqlWorkloadType'] = result.sql_workload_type\n\n    return order_dict"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef aks_upgrades_table_format(result):\n    # pylint: disable=import-error\n    from jmespath import compile as compile_jmes, Options\n\n    # This expression assumes there is one node pool, and that the master and nodes upgrade in lockstep.\n    parsed = compile_jmes(\"\"\"{\n        name: name,\n        resourceGroup: resourceGroup,\n        masterVersion: controlPlaneProfile.kubernetesVersion || `unknown`,\n        nodePoolVersion: agentPoolProfiles[0].kubernetesVersion || `unknown`,\n        upgrades: controlPlaneProfile.upgrades || [`None available`] | sort_versions(@) | join(`, `, @)\n    }\"\"\")\n    # use ordered dicts so headers are predictable\n    return parsed.search(result, Options(dict_cls=OrderedDict, custom_functions=_custom_functions()))", "response": "Format get - upgrades results as a summary for display with \"- o table\"."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nformats get - versions results as a summary for display with \"- o table\".", "response": "def aks_versions_table_format(result):\n    \"\"\"Format get-versions results as a summary for display with \"-o table\".\"\"\"\n    # pylint: disable=import-error\n    from jmespath import compile as compile_jmes, Options\n\n    parsed = compile_jmes(\"\"\"orchestrators[].{\n        kubernetesVersion: orchestratorVersion,\n        upgrades: upgrades[].orchestratorVersion || [`None available`] | sort_versions(@) | join(`, `, @)\n    }\"\"\")\n    # use ordered dicts so headers are predictable\n    results = parsed.search(result, Options(dict_cls=OrderedDict, custom_functions=_custom_functions()))\n    return sorted(results, key=lambda x: version_to_tuple(x.get('kubernetesVersion')), reverse=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nvalidating input arguments when the user invokes az alias create.", "response": "def process_alias_create_namespace(namespace):\n    \"\"\"\n    Validate input arguments when the user invokes 'az alias create'.\n\n    Args:\n        namespace: argparse namespace object.\n    \"\"\"\n    namespace = filter_alias_create_namespace(namespace)\n    _validate_alias_name(namespace.alias_name)\n    _validate_alias_command(namespace.alias_command)\n    _validate_alias_command_level(namespace.alias_name, namespace.alias_command)\n    _validate_pos_args_syntax(namespace.alias_name, namespace.alias_command)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef process_alias_import_namespace(namespace):\n    if is_url(namespace.alias_source):\n        alias_source = retrieve_file_from_url(namespace.alias_source)\n\n        _validate_alias_file_content(alias_source, url=namespace.alias_source)\n    else:\n        namespace.alias_source = os.path.abspath(namespace.alias_source)\n        _validate_alias_file_path(namespace.alias_source)\n        _validate_alias_file_content(namespace.alias_source)", "response": "Validate input arguments when the user invokes az alias import."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nvalidates input arguments when the user invokes az alias export.", "response": "def process_alias_export_namespace(namespace):\n    \"\"\"\n    Validate input arguments when the user invokes 'az alias export'.\n\n    Args:\n        namespace: argparse namespace object.\n    \"\"\"\n    namespace.export_path = os.path.abspath(namespace.export_path)\n    if os.path.isfile(namespace.export_path):\n        raise CLIError(FILE_ALREADY_EXISTS_ERROR.format(namespace.export_path))\n\n    export_path_dir = os.path.dirname(namespace.export_path)\n    if not os.path.isdir(export_path_dir):\n        os.makedirs(export_path_dir)\n\n    if os.path.isdir(namespace.export_path):\n        namespace.export_path = os.path.join(namespace.export_path, ALIAS_FILE_NAME)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if the alias name is valid.", "response": "def _validate_alias_name(alias_name):\n    \"\"\"\n    Check if the alias name is valid.\n\n    Args:\n        alias_name: The name of the alias to validate.\n    \"\"\"\n    if not alias_name:\n        raise CLIError(EMPTY_ALIAS_ERROR)\n\n    if not re.match('^[a-zA-Z]', alias_name):\n        raise CLIError(INVALID_STARTING_CHAR_ERROR.format(alias_name[0]))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _validate_alias_command(alias_command):\n    if not alias_command:\n        raise CLIError(EMPTY_ALIAS_ERROR)\n\n    split_command = shlex.split(alias_command)\n    boundary_index = len(split_command)\n    for i, subcommand in enumerate(split_command):\n        if not re.match('^[a-z]', subcommand.lower()) or i > COLLISION_CHECK_LEVEL_DEPTH:\n            boundary_index = i\n            break\n\n    # Extract possible CLI commands and validate\n    command_to_validate = ' '.join(split_command[:boundary_index]).lower()\n    for command in azext_alias.cached_reserved_commands:\n        if re.match(r'([a-z\\-]*\\s)*{}($|\\s)'.format(command_to_validate), command):\n            return\n\n    _validate_positional_arguments(shlex.split(alias_command))", "response": "Check if the alias command is valid."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _validate_pos_args_syntax(alias_name, alias_command):\n    pos_args_from_alias = get_placeholders(alias_name)\n    # Split by '|' to extract positional argument name from Jinja filter (e.g. {{ arg_name | upper }})\n    # Split by '.' to extract positional argument name from function call (e.g. {{ arg_name.split()[0] }})\n    pos_args_from_command = [x.split('|')[0].split('.')[0].strip() for x in get_placeholders(alias_command)]\n\n    if set(pos_args_from_alias) != set(pos_args_from_command):\n        arg_diff = set(pos_args_from_alias) ^ set(pos_args_from_command)\n        raise CLIError(INCONSISTENT_ARG_ERROR.format('' if len(arg_diff) == 1 else 's',\n                                                     arg_diff,\n                                                     'is' if len(arg_diff) == 1 else 'are'))", "response": "Check if the positional argument syntax is valid in alias name and alias command."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nvalidate that the alias and command are not conflicted with any command.", "response": "def _validate_alias_command_level(alias, command):\n    \"\"\"\n    Make sure that if the alias is a reserved command, the command that the alias points to\n    in the command tree does not conflict in levels.\n\n    e.g. 'dns' -> 'network dns' is valid because dns is a level 2 command and network dns starts at level 1.\n    However, 'list' -> 'show' is not valid because list and show are both reserved commands at level 2.\n\n    Args:\n        alias: The name of the alias.\n        command: The command that the alias points to.\n    \"\"\"\n    alias_collision_table = AliasManager.build_collision_table([alias])\n\n    # Alias is not a reserved command, so it can point to any command\n    if not alias_collision_table:\n        return\n\n    command_collision_table = AliasManager.build_collision_table([command])\n    alias_collision_levels = alias_collision_table.get(alias.split()[0], [])\n    command_collision_levels = command_collision_table.get(command.split()[0], [])\n\n    # Check if there is a command level conflict\n    if set(alias_collision_levels) & set(command_collision_levels):\n        raise CLIError(COMMAND_LVL_ERROR.format(alias, command))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nvalidates that the alias file path is non - existant and a directory.", "response": "def _validate_alias_file_path(alias_file_path):\n    \"\"\"\n    Make sure the alias file path is neither non-existant nor a directory\n\n    Args:\n        The alias file path to import aliases from.\n    \"\"\"\n    if not os.path.exists(alias_file_path):\n        raise CLIError(ALIAS_FILE_NOT_FOUND_ERROR)\n\n    if os.path.isdir(alias_file_path):\n        raise CLIError(ALIAS_FILE_DIR_ERROR.format(alias_file_path))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _validate_alias_file_content(alias_file_path, url=''):\n    alias_table = get_config_parser()\n    try:\n        alias_table.read(alias_file_path)\n        for alias_name, alias_command in reduce_alias_table(alias_table):\n            _validate_alias_name(alias_name)\n            _validate_alias_command(alias_command)\n            _validate_alias_command_level(alias_name, alias_command)\n            _validate_pos_args_syntax(alias_name, alias_command)\n    except Exception as exception:  # pylint: disable=broad-except\n        error_msg = CONFIG_PARSING_ERROR % AliasManager.process_exception_message(exception)\n        error_msg = error_msg.replace(alias_file_path, url or alias_file_path)\n        raise CLIError(error_msg)", "response": "Validate the content of the alias file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nvalidate that the arguments that are positional are valid.", "response": "def _validate_positional_arguments(args):\n    \"\"\"\n    To validate the positional argument feature - https://github.com/Azure/azure-cli/pull/6055.\n    Assuming that unknown commands are positional arguments immediately\n    led by words that only appear at the end of the commands\n\n    Slight modification of\n    https://github.com/Azure/azure-cli/blob/dev/src/azure-cli-core/azure/cli/core/commands/__init__.py#L356-L373\n\n    Args:\n        args: The arguments that the user inputs in the terminal.\n\n    Returns:\n        Rudimentary parsed arguments.\n    \"\"\"\n    nouns = []\n    for arg in args:\n        if not arg.startswith('-') or not arg.startswith('{{'):\n            nouns.append(arg)\n        else:\n            break\n\n    while nouns:\n        search = ' '.join(nouns)\n        # Since the command name may be immediately followed by a positional arg, strip those off\n        if not next((x for x in azext_alias.cached_reserved_commands if x.endswith(search)), False):\n            del nouns[-1]\n        else:\n            return\n\n    raise CLIError(INVALID_ALIAS_COMMAND_ERROR.format(' '.join(args)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninitialize Log Analytics data client for use with CLI.", "response": "def loganalytics_data_plane_client(cli_ctx, _):\n    \"\"\"Initialize Log Analytics data client for use with CLI.\"\"\"\n    from .vendored_sdks.loganalytics import LogAnalyticsDataClient\n    from azure.cli.core._profile import Profile\n    profile = Profile(cli_ctx=cli_ctx)\n    cred, _, _ = profile.get_login_credentials(\n        resource=\"https://api.loganalytics.io\")\n    return LogAnalyticsDataClient(cred)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexecutes a query against the provided Application Insights application.", "response": "def execute_query(cmd, client, application, analytics_query, start_time=None, end_time=None, offset='1h', resource_group_name=None):\n    \"\"\"Executes a query against the provided Application Insights application.\"\"\"\n    from .vendored_sdks.applicationinsights.models import QueryBody\n    targets = get_query_targets(cmd.cli_ctx, application, resource_group_name)\n    return client.query.execute(targets[0], QueryBody(query=analytics_query, timespan=get_timespan(cmd.cli_ctx, start_time, end_time, offset), applications=targets[1:]))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd newlines to the end of the long_phrase", "response": "def add_new_lines(long_phrase, line_min=None, tolerance=TOLERANCE):\n    \"\"\" not everything fits on the screen, based on the size, add newlines \"\"\"\n    if line_min is None:\n        line_min = math.floor(int(_get_window_columns()) / 2 - 15)\n\n    if long_phrase is None:\n        return long_phrase\n    line_min = int(line_min)\n    nl_loc = []\n    skip = False\n    index = 0\n    if len(long_phrase) > line_min:\n\n        for _ in range(int(math.floor(len(long_phrase) / line_min))):\n            previous = index\n            index += line_min\n            if skip:\n                index += 1\n                skip = False\n            while index < len(long_phrase) and \\\n                    not long_phrase[index].isspace() and \\\n                    index < tolerance + previous + line_min:\n                index += 1\n            if index < len(long_phrase):\n                if long_phrase[index].isspace():\n                    index += 1\n                    skip = True\n                nl_loc.append(index)\n\n    counter = 0\n    for loc in nl_loc:\n        long_phrase = long_phrase[:loc + counter] + '\\n' + long_phrase[loc + counter:]\n        counter += 1\n    return long_phrase + \"\\n\""}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd the exits from the application to the command tree", "response": "def add_exit(self):\n        \"\"\" adds the exits from the application \"\"\"\n        self.completable.append(\"quit\")\n        self.completable.append(\"exit\")\n\n        self.descrip[\"quit\"] = \"Exits the program\"\n        self.descrip[\"exit\"] = \"Exits the program\"\n\n        self.command_tree.add_child(CommandBranch(\"quit\"))\n        self.command_tree.add_child(CommandBranch(\"exit\"))\n\n        self.command_param[\"quit\"] = \"\"\n        self.command_param[\"exit\"] = \"\""}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _gather_from_files(self, config):\n        command_file = config.get_help_files()\n        cache_path = os.path.join(config.get_config_dir(), 'cache')\n        cols = _get_window_columns()\n\n        with open(os.path.join(cache_path, command_file), 'r') as help_file:\n            data = json.load(help_file)\n        self.add_exit()\n        commands = data.keys()\n\n        for command in commands:\n            branch = self.command_tree\n            for word in command.split():\n                if word not in self.completable:\n                    self.completable.append(word)\n                if not branch.has_child(word):\n                    branch.add_child(CommandBranch(word))\n                branch = branch.get_child(word)\n\n            description = data[command]['help']\n            self.descrip[command] = add_new_lines(description, line_min=int(cols) - 2 * TOLERANCE)\n\n            if 'examples' in data[command]:\n                examples = []\n                for example in data[command]['examples']:\n                    examples.append([\n                        add_new_lines(example[0], line_min=int(cols) - 2 * TOLERANCE),\n                        add_new_lines(example[1], line_min=int(cols) - 2 * TOLERANCE)])\n                self.command_example[command] = examples\n\n            command_params = data[command].get('parameters', {})\n            for param in command_params:\n                if '==SUPPRESS==' not in command_params[param]['help']:\n                    param_aliases = set()\n\n                    for par in command_params[param]['name']:\n                        param_aliases.add(par)\n\n                        self.param_descript[command + \" \" + par] = \\\n                            add_new_lines(\n                                command_params[param]['required'] +\n                                \" \" + command_params[param]['help'],\n                                line_min=int(cols) - 2 * TOLERANCE)\n                        if par not in self.completable_param:\n                            self.completable_param.append(par)\n\n                    param_doubles = self.command_param_info.get(command, {})\n                    for alias in param_aliases:\n                        param_doubles[alias] = param_aliases\n                    self.command_param_info[command] = param_doubles", "response": "gathers from the files in a way that is convienent to use"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_all_subcommands(self):\n        subcommands = []\n        for command in self.descrip:\n            for word in command.split():\n                for kid in self.command_tree.children:\n                    if word != kid and word not in subcommands:\n                        subcommands.append(word)\n        return subcommands", "response": "returns all the subcommands"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating an alias. Args: alias_name: The name of the alias. alias_command: The command that the alias points to.", "response": "def create_alias(alias_name, alias_command):\n    \"\"\"\n    Create an alias.\n\n    Args:\n        alias_name: The name of the alias.\n        alias_command: The command that the alias points to.\n    \"\"\"\n    alias_name, alias_command = alias_name.strip(), alias_command.strip()\n    alias_table = get_alias_table()\n    if alias_name not in alias_table.sections():\n        alias_table.add_section(alias_name)\n\n    alias_table.set(alias_name, 'command', alias_command)\n    _commit_change(alias_table)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef export_aliases(export_path=None, exclusions=None):\n    if not export_path:\n        export_path = os.path.abspath(ALIAS_FILE_NAME)\n\n    alias_table = get_alias_table()\n    for exclusion in exclusions or []:\n        if exclusion not in alias_table.sections():\n            raise CLIError(ALIAS_NOT_FOUND_ERROR.format(exclusion))\n        alias_table.remove_section(exclusion)\n\n    _commit_change(alias_table, export_path=export_path, post_commit=False)\n    logger.warning(POST_EXPORT_ALIAS_MSG, export_path)", "response": "Exports all registered aliases to a given path."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef import_aliases(alias_source):\n    alias_table = get_alias_table()\n    if is_url(alias_source):\n        alias_source = retrieve_file_from_url(alias_source)\n        alias_table.read(alias_source)\n        os.remove(alias_source)\n    else:\n        alias_table.read(alias_source)\n    _commit_change(alias_table)", "response": "Imports aliases from a file or an URL."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef list_alias():\n    alias_table = get_alias_table()\n    output = []\n    for alias in alias_table.sections():\n        if alias_table.has_option(alias, 'command'):\n            output.append({\n                'alias': alias,\n                # Remove unnecessary whitespaces\n                'command': ' '.join(alias_table.get(alias, 'command').split())\n            })\n\n    return output", "response": "List all registered aliases."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving an alias from the resource tree.", "response": "def remove_alias(alias_names):\n    \"\"\"\n    Remove an alias.\n\n    Args:\n        alias_name: The name of the alias to be removed.\n    \"\"\"\n    alias_table = get_alias_table()\n    for alias_name in alias_names:\n        if alias_name not in alias_table.sections():\n            raise CLIError(ALIAS_NOT_FOUND_ERROR.format(alias_name))\n        alias_table.remove_section(alias_name)\n    _commit_change(alias_table)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _commit_change(alias_table, export_path=None, post_commit=True):\n    with open(export_path or GLOBAL_ALIAS_PATH, 'w+') as alias_config_file:\n        alias_table.write(alias_config_file)\n        if post_commit:\n            alias_config_file.seek(0)\n            alias_config_hash = hashlib.sha1(alias_config_file.read().encode('utf-8')).hexdigest()\n            AliasManager.write_alias_config_hash(alias_config_hash)\n            collided_alias = AliasManager.build_collision_table(alias_table.sections())\n            AliasManager.write_collided_alias(collided_alias)\n            build_tab_completion_table(alias_table)", "response": "Commits changes to the alias table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef applicationinsights_data_plane_client(cli_ctx, _, subscription=None):\n    from .vendored_sdks.applicationinsights import ApplicationInsightsDataClient\n    from azure.cli.core._profile import Profile\n    profile = Profile(cli_ctx=cli_ctx)\n    cred, _, _ = profile.get_login_credentials(\n        resource=\"https://api.applicationinsights.io\",\n        subscription_id=subscription\n    )\n    return ApplicationInsightsDataClient(cred)", "response": "Initialize Log Analytics data client for use with CLI."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninitializes Log Analytics mgmt client for use with CLI.", "response": "def applicationinsights_mgmt_plane_client(cli_ctx, _, subscription=None):\n    \"\"\"Initialize Log Analytics mgmt client for use with CLI.\"\"\"\n    from .vendored_sdks.mgmt_applicationinsights import ApplicationInsightsManagementClient\n    from azure.cli.core._profile import Profile\n    profile = Profile(cli_ctx=cli_ctx)\n    # Use subscription from resource_id where possible, otherwise use login.\n    if subscription:\n        cred, _, _ = profile.get_login_credentials(subscription_id=subscription)\n        return ApplicationInsightsManagementClient(\n            cred,\n            subscription\n        )\n    cred, sub_id, _ = profile.get_login_credentials()\n    return ApplicationInsightsManagementClient(\n        cred,\n        sub_id\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a new volume.", "response": "def create_volume(client, resource_group_name,\n                  name, location,\n                  template_file=None, template_uri=None):\n    \"\"\"Create a volume. \"\"\"\n    volume_properties = None\n\n    if template_uri:\n        volume_properties = shell_safe_json_parse(_urlretrieve(template_uri).decode('utf-8'), preserve_order=True)\n    elif template_file:\n        volume_properties = get_file_json(template_file, preserve_order=True)\n        volume_properties = json.loads(json.dumps(volume_properties))\n    else:\n        raise CLIError('One of --template-file or --template-uri has to be specified')\n\n    volume_properties['location'] = location\n    return client.create(resource_group_name, name, volume_properties)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef enable_aliases_autocomplete(_, **kwargs):\n    external_completions = kwargs.get('external_completions', [])\n    prefix = kwargs.get('cword_prefix', [])\n    cur_commands = kwargs.get('comp_words', [])\n    alias_table = get_alias_table()\n    # Transform aliases if they are in current commands,\n    # so parser can get the correct subparser when chaining aliases\n    _transform_cur_commands(cur_commands, alias_table=alias_table)\n\n    for alias, alias_command in filter_aliases(alias_table):\n        if alias.startswith(prefix) and alias.strip() != prefix and _is_autocomplete_valid(cur_commands, alias_command):\n            # Only autocomplete the first word because alias is space-delimited\n            external_completions.append(alias)\n\n    # Append spaces if necessary (https://github.com/kislyuk/argcomplete/blob/master/argcomplete/__init__.py#L552-L559)\n    prequote = kwargs.get('cword_prequote', '')\n    continuation_chars = \"=/:\"\n    if len(external_completions) == 1 and external_completions[0][-1] not in continuation_chars and not prequote:\n        external_completions[0] += ' '", "response": "Enable aliases autocomplete by injecting aliases into Azure CLI tab completion list."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntransforms any aliases in current commands in interactive into their respective commands.", "response": "def transform_cur_commands_interactive(_, **kwargs):\n    \"\"\"\n    Transform any aliases in current commands in interactive into their respective commands.\n    \"\"\"\n    event_payload = kwargs.get('event_payload', {})\n    # text_split = current commands typed in the interactive shell without any unfinished word\n    # text = current commands typed in the interactive shell\n    cur_commands = event_payload.get('text', '').split(' ')\n    _transform_cur_commands(cur_commands)\n\n    event_payload.update({\n        'text': ' '.join(cur_commands)\n    })"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nenabling aliases autocomplete on interactive mode by injecting aliases in the command tree.", "response": "def enable_aliases_autocomplete_interactive(_, **kwargs):\n    \"\"\"\n    Enable aliases autocomplete on interactive mode by injecting aliases in the command tree.\n    \"\"\"\n    subtree = kwargs.get('subtree', None)\n    if not subtree or not hasattr(subtree, 'children'):\n        return\n\n    for alias, alias_command in filter_aliases(get_alias_table()):\n        # Only autocomplete the first word because alias is space-delimited\n        if subtree.in_tree(alias_command.split()):\n            subtree.add_child(CommandBranch(alias))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if autocomplete can be performed at the current state.", "response": "def _is_autocomplete_valid(cur_commands, alias_command):\n    \"\"\"\n    Determine whether autocomplete can be performed at the current state.\n\n    Args:\n        parser: The current CLI parser.\n        cur_commands: The current commands typed in the console.\n        alias_command: The alias command.\n\n    Returns:\n        True if autocomplete can be performed.\n    \"\"\"\n    parent_command = ' '.join(cur_commands[1:])\n    with open(GLOBAL_ALIAS_TAB_COMP_TABLE_PATH, 'r') as tab_completion_table_file:\n        try:\n            tab_completion_table = json.loads(tab_completion_table_file.read())\n            return alias_command in tab_completion_table and parent_command in tab_completion_table[alias_command]\n        except Exception:  # pylint: disable=broad-except\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _transform_cur_commands(cur_commands, alias_table=None):\n    transformed = []\n    alias_table = alias_table if alias_table else get_alias_table()\n    for cmd in cur_commands:\n        if cmd in alias_table.sections() and alias_table.has_option(cmd, 'command'):\n            transformed += alias_table.get(cmd, 'command').split()\n        else:\n            transformed.append(cmd)\n    cur_commands[:] = transformed", "response": "Transform any aliases in cur_commands into their respective commands."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef help_text(values):\n    result = \"\"\n    for key in values:\n        result += key + ' '.join('' for x in range(GESTURE_LENGTH - len(key))) +\\\n                        ': ' + values[key] + '\\n'\n    return result", "response": "reformats the help text"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ask_user_for_telemetry():\n    answer = \" \"\n    while answer.lower() != 'yes' and answer.lower() != 'no':\n        answer = prompt(u'\\nDo you agree to sending telemetry (yes/no)? Default answer is yes: ')\n\n        if answer == '':\n            answer = 'yes'\n\n    return answer", "response": "asks the user for if we can collect telemetry"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef firsttime(self):\n        self.config.set('DEFAULT', 'firsttime', 'no')\n        if self.cli_config.getboolean('core', 'collect_telemetry', fallback=False):\n            print(PRIVACY_STATEMENT)\n        else:\n            self.cli_config.set_value('core', 'collect_telemetry', ask_user_for_telemetry())\n\n        self.update()", "response": "sets it as already done"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the value of the config values", "response": "def set_val(self, direct, section, val):\n        \"\"\" set the config values \"\"\"\n        if val is not None:\n            self.config.set(direct, section, val)\n            self.update()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the configuration settings", "response": "def update(self):\n        \"\"\" updates the configuration settings \"\"\"\n        with open(os.path.join(self.config_dir, CONFIG_FILE_NAME), 'w') as config_file:\n            self.config.write(config_file)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nexecute a query against the provided Log Analytics workspace.", "response": "def execute_query(client, workspace, analytics_query, timespan=None, workspaces=None):\n    \"\"\"Executes a query against the provided Log Analytics workspace.\"\"\"\n    from .vendored_sdks.loganalytics.models import QueryBody\n    return client.query(workspace, QueryBody(query=analytics_query, timespan=timespan, workspaces=workspaces))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload the alias table.", "response": "def load_alias_table(self):\n        \"\"\"\n        Load (create, if not exist) the alias config file.\n        \"\"\"\n        try:\n            # w+ creates the alias config file if it does not exist\n            open_mode = 'r+' if os.path.exists(GLOBAL_ALIAS_PATH) else 'w+'\n            with open(GLOBAL_ALIAS_PATH, open_mode) as alias_config_file:\n                self.alias_config_str = alias_config_file.read()\n            self.alias_table.read(GLOBAL_ALIAS_PATH)\n            telemetry.set_number_of_aliases_registered(len(self.alias_table.sections()))\n        except Exception as exception:  # pylint: disable=broad-except\n            logger.warning(CONFIG_PARSING_ERROR, AliasManager.process_exception_message(exception))\n            self.alias_table = get_config_parser()\n            telemetry.set_exception(exception)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_alias_hash(self):\n        # w+ creates the alias hash file if it does not exist\n        open_mode = 'r+' if os.path.exists(GLOBAL_ALIAS_HASH_PATH) else 'w+'\n        with open(GLOBAL_ALIAS_HASH_PATH, open_mode) as alias_config_hash_file:\n            self.alias_config_hash = alias_config_hash_file.read()", "response": "Load the alias hash file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading the collided alias file.", "response": "def load_collided_alias(self):\n        \"\"\"\n        Load (create, if not exist) the collided alias file.\n        \"\"\"\n        # w+ creates the alias config file if it does not exist\n        open_mode = 'r+' if os.path.exists(GLOBAL_COLLIDED_ALIAS_PATH) else 'w+'\n        with open(GLOBAL_COLLIDED_ALIAS_PATH, open_mode) as collided_alias_file:\n            collided_alias_str = collided_alias_file.read()\n            try:\n                self.collided_alias = json.loads(collided_alias_str if collided_alias_str else '{}')\n            except Exception:  # pylint: disable=broad-except\n                self.collided_alias = {}"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndetects if the alias configuration file has changed since the last run.", "response": "def detect_alias_config_change(self):\n        \"\"\"\n        Change if the alias configuration has changed since the last run.\n\n        Returns:\n            False if the alias configuration file has not been changed since the last run.\n            Otherwise, return True.\n        \"\"\"\n        # Do not load the entire command table if there is a parse error\n        if self.parse_error():\n            return False\n\n        alias_config_sha1 = hashlib.sha1(self.alias_config_str.encode('utf-8')).hexdigest()\n        if alias_config_sha1 != self.alias_config_hash:\n            # Overwrite the old hash with the new one\n            self.alias_config_hash = alias_config_sha1\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntransform any aliases in args to their respective commands.", "response": "def transform(self, args):\n        \"\"\"\n        Transform any aliases in args to their respective commands.\n\n        Args:\n            args: A list of space-delimited command input extracted directly from the console.\n\n        Returns:\n            A list of transformed commands according to the alias configuration file.\n        \"\"\"\n        if self.parse_error():\n            # Write an empty hash so next run will check the config file against the entire command table again\n            AliasManager.write_alias_config_hash(empty_hash=True)\n            return args\n\n        # Only load the entire command table if it detects changes in the alias config\n        if self.detect_alias_config_change():\n            self.load_full_command_table()\n            self.collided_alias = AliasManager.build_collision_table(self.alias_table.sections())\n            build_tab_completion_table(self.alias_table)\n        else:\n            self.load_collided_alias()\n\n        transformed_commands = []\n        alias_iter = enumerate(args, 1)\n        for alias_index, alias in alias_iter:\n            is_collided_alias = alias in self.collided_alias and alias_index in self.collided_alias[alias]\n            # Check if the current alias is a named argument\n            # index - 2 because alias_iter starts counting at index 1\n            is_named_arg = alias_index > 1 and args[alias_index - 2].startswith('-')\n            is_named_arg_flag = alias.startswith('-')\n            excluded_commands = is_alias_command(['remove', 'export'], transformed_commands)\n            if not alias or is_collided_alias or is_named_arg or is_named_arg_flag or excluded_commands:\n                transformed_commands.append(alias)\n                continue\n\n            full_alias = self.get_full_alias(alias)\n\n            if self.alias_table.has_option(full_alias, 'command'):\n                cmd_derived_from_alias = self.alias_table.get(full_alias, 'command')\n                telemetry.set_alias_hit(full_alias)\n            else:\n                transformed_commands.append(alias)\n                continue\n\n            pos_args_table = build_pos_args_table(full_alias, args, alias_index)\n            if pos_args_table:\n                logger.debug(POS_ARG_DEBUG_MSG, full_alias, cmd_derived_from_alias, pos_args_table)\n                transformed_commands += render_template(cmd_derived_from_alias, pos_args_table)\n\n                # Skip the next arg(s) because they have been already consumed as a positional argument above\n                for pos_arg in pos_args_table:  # pylint: disable=unused-variable\n                    next(alias_iter)\n            else:\n                logger.debug(DEBUG_MSG, full_alias, cmd_derived_from_alias)\n                transformed_commands += shlex.split(cmd_derived_from_alias)\n\n        return self.post_transform(transformed_commands)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_full_alias(self, query):\n        if query in self.alias_table.sections():\n            return query\n\n        return next((section for section in self.alias_table.sections() if section.split()[0] == query), '')", "response": "Get the full alias given a search query."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nperforms a full load of the command table to get all the reserved command words.", "response": "def load_full_command_table(self):\n        \"\"\"\n        Perform a full load of the command table to get all the reserved command words.\n        \"\"\"\n        load_cmd_tbl_func = self.kwargs.get('load_cmd_tbl_func', lambda _: {})\n        cache_reserved_commands(load_cmd_tbl_func)\n        telemetry.set_full_command_table_loaded()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef post_transform(self, args):\n        # Ignore 'az' if it is the first command\n        args = args[1:] if args and args[0] == 'az' else args\n\n        post_transform_commands = []\n        for i, arg in enumerate(args):\n            # Do not translate environment variables for command argument\n            if is_alias_command(['create'], args) and i > 0 and args[i - 1] in ['-c', '--command']:\n                post_transform_commands.append(arg)\n            else:\n                post_transform_commands.append(os.path.expandvars(arg))\n\n        AliasManager.write_alias_config_hash(self.alias_config_hash)\n        AliasManager.write_collided_alias(self.collided_alias)\n\n        return post_transform_commands", "response": "Inject environment variables and write hash to alias hash file after transforming alias to commands."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build_collision_table(aliases, levels=COLLISION_CHECK_LEVEL_DEPTH):\n        collided_alias = defaultdict(list)\n        for alias in aliases:\n            # Only care about the first word in the alias because alias\n            # cannot have spaces (unless they have positional arguments)\n            word = alias.split()[0]\n            for level in range(1, levels + 1):\n                collision_regex = r'^{}{}($|\\s)'.format(r'([a-z\\-]*\\s)' * (level - 1), word.lower())\n                if list(filter(re.compile(collision_regex).match, azext_alias.cached_reserved_commands)) \\\n                        and level not in collided_alias[word]:\n                    collided_alias[word].append(level)\n\n        telemetry.set_collided_aliases(list(collided_alias.keys()))\n        return collided_alias", "response": "Build the collision table according to the command table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write_alias_config_hash(alias_config_hash='', empty_hash=False):\n        with open(GLOBAL_ALIAS_HASH_PATH, 'w') as alias_config_hash_file:\n            alias_config_hash_file.write('' if empty_hash else alias_config_hash)", "response": "Writes the given alias config hash to the alias hash file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites the collided aliases string into the collided alias file.", "response": "def write_collided_alias(collided_alias_dict):\n        \"\"\"\n        Write the collided aliases string into the collided alias file.\n        \"\"\"\n        # w+ creates the alias config file if it does not exist\n        open_mode = 'r+' if os.path.exists(GLOBAL_COLLIDED_ALIAS_PATH) else 'w+'\n        with open(GLOBAL_COLLIDED_ALIAS_PATH, open_mode) as collided_alias_file:\n            collided_alias_file.truncate()\n            collided_alias_file.write(json.dumps(collided_alias_dict))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprocess an exception message.", "response": "def process_exception_message(exception):\n        \"\"\"\n        Process an exception message.\n\n        Args:\n            exception: The exception to process.\n\n        Returns:\n            A filtered string summarizing the exception.\n        \"\"\"\n        exception_message = str(exception)\n        for replace_char in ['\\t', '\\n', '\\\\n']:\n            exception_message = exception_message.replace(replace_char, '' if replace_char != '\\t' else ' ')\n        return exception_message.replace('section', 'alias')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef list_network_resource_property(resource, prop):\n\n    def list_func(cmd, resource_group_name, resource_name):\n        client = getattr(network_client_factory(cmd.cli_ctx), resource)\n        return client.get(resource_group_name, resource_name).__getattribute__(prop)\n\n    func_name = 'list_network_resource_property_{}_{}'.format(resource, prop)\n    setattr(sys.modules[__name__], func_name, list_func)\n    return func_name", "response": "Creates a function that returns the value of a network resource property."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a delete function that deletes a network resource property entry.", "response": "def delete_network_resource_property_entry(resource, prop):\n    \"\"\" Factory method for creating delete functions. \"\"\"\n\n    def delete_func(cmd, resource_group_name, resource_name, item_name, no_wait=False):  # pylint: disable=unused-argument\n        client = getattr(network_client_factory(cmd.cli_ctx), resource)\n        item = client.get(resource_group_name, resource_name)\n        keep_items = \\\n            [x for x in item.__getattribute__(prop) if x.name.lower() != item_name.lower()]\n        _set_param(item, prop, keep_items)\n        if no_wait:\n            sdk_no_wait(no_wait, client.create_or_update, resource_group_name, resource_name, item)\n        else:\n            result = sdk_no_wait(no_wait, client.create_or_update, resource_group_name, resource_name, item).result()\n            if next((x for x in getattr(result, prop) if x.name.lower() == item_name.lower()), None):\n                raise CLIError(\"Failed to delete '{}' on '{}'\".format(item_name, resource_name))\n\n    func_name = 'delete_network_resource_property_entry_{}_{}'.format(resource, prop)\n    setattr(sys.modules[__name__], func_name, delete_func)\n    return func_name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a get function that returns the object that represents the specified property of the specified resource.", "response": "def get_network_resource_property_entry(resource, prop):\n    \"\"\" Factory method for creating get functions. \"\"\"\n\n    def get_func(cmd, resource_group_name, resource_name, item_name):\n        client = getattr(network_client_factory(cmd.cli_ctx), resource)\n        items = getattr(client.get(resource_group_name, resource_name), prop)\n\n        result = next((x for x in items if x.name.lower() == item_name.lower()), None)\n        if not result:\n            raise CLIError(\"Item '{}' does not exist on {} '{}'\".format(\n                item_name, resource, resource_name))\n        else:\n            return result\n\n    func_name = 'get_network_resource_property_entry_{}_{}'.format(resource, prop)\n    setattr(sys.modules[__name__], func_name, get_func)\n    return func_name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntransform to convert SDK file list output to something that can be used in a SDK file directory list.", "response": "def transform_file_output(result):\n    \"\"\" Transform to convert SDK file/dir list output to something that\n    more clearly distinguishes between files and directories. \"\"\"\n    from collections import OrderedDict\n    new_result = []\n\n    iterable = result if isinstance(result, list) else result.get('items', result)\n    for item in iterable:\n        new_entry = OrderedDict()\n\n        entity_type = item['type']  # type property is added by transform_file_directory_result\n        is_dir = entity_type == 'dir'\n\n        new_entry['Name'] = item['name'] + '/' if is_dir else item['name']\n        new_entry['Content Length'] = ' ' if is_dir else item['properties']['contentLength']\n        new_entry['Type'] = item['type']\n        new_entry['Last Modified'] = item['properties']['lastModified'] or ' '\n        new_result.append(new_entry)\n    return sorted(new_result, key=lambda k: k['Name'])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntransform a result returned from file and directory listing API.", "response": "def transform_file_directory_result(cli_ctx):\n    \"\"\"\n    Transform a the result returned from file and directory listing API.\n\n    This transformer add and remove properties from File and Directory objects in the given list\n    in order to align the object's properties so as to offer a better view to the file and dir\n    list.\n    \"\"\"\n    def transformer(result):\n        t_file, t_dir = get_sdk(cli_ctx, CUSTOM_DATA_STORAGE, 'File', 'Directory', mod='file.models')\n        return_list = []\n        for each in result:\n            if isinstance(each, t_file):\n                delattr(each, 'content')\n                setattr(each, 'type', 'file')\n            elif isinstance(each, t_dir):\n                setattr(each, 'type', 'dir')\n\n            return_list.append(each)\n\n        return return_list\n    return transformer"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_subscription_in_enrollment_account(\n            self, enrollment_account_name, body, custom_headers=None, raw=False, polling=True, **operation_config):\n        \"\"\"Creates an Azure subscription.\n\n        :param enrollment_account_name: The name of the enrollment account to\n         which the subscription will be billed.\n        :type enrollment_account_name: str\n        :param body: The subscription creation parameters.\n        :type body:\n         ~azure.mgmt.subscription.models.SubscriptionCreationParameters\n        :param dict custom_headers: headers that will be added to the request\n        :param bool raw: The poller return type is ClientRawResponse, the\n         direct response alongside the deserialized response\n        :param polling: True for ARMPolling, False for no polling, or a\n         polling object for personal polling strategy\n        :return: An instance of LROPoller that returns\n         SubscriptionCreationResult or\n         ClientRawResponse<SubscriptionCreationResult> if raw==True\n        :rtype:\n         ~msrestazure.azure_operation.AzureOperationPoller[~azure.mgmt.subscription.models.SubscriptionCreationResult]\n         or\n         ~msrestazure.azure_operation.AzureOperationPoller[~msrest.pipeline.ClientRawResponse[~azure.mgmt.subscription.models.SubscriptionCreationResult]]\n        :raises:\n         :class:`ErrorResponseException<azure.mgmt.subscription.models.ErrorResponseException>`\n        \"\"\"\n        raw_result = self._create_subscription_in_enrollment_account_initial(\n            enrollment_account_name=enrollment_account_name,\n            body=body,\n            custom_headers=custom_headers,\n            raw=True,\n            **operation_config\n        )\n\n        def get_long_running_output(response):\n            header_dict = {\n                'Location': 'str',\n                'Retry-After': 'str',\n            }\n            deserialized = self._deserialize('SubscriptionCreationResult', response)\n\n            if raw:\n                client_raw_response = ClientRawResponse(deserialized, response)\n                client_raw_response.add_headers(header_dict)\n                return client_raw_response\n\n            return deserialized\n\n        lro_delay = operation_config.get(\n            'long_running_operation_timeout',\n            self.config.long_running_operation_timeout)\n        if polling is True: polling_method = ARMPolling(lro_delay, **operation_config)\n        elif polling is False: polling_method = NoPolling()\n        else: polling_method = polling\n        return LROPoller(self._client, raw_result, get_long_running_output, polling_method)", "response": "Creates an Azure subscription in an Azure Account."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef transform_gateway(result):\n    return OrderedDict([('Name', result.get('name')),\n                        ('ResourceGroup', result.get('resourceGroup')),\n                        ('Location', result.get('location')),\n                        ('ProvisioningState', result.get('provisioningState')),\n                        ('Status', result.get('status')),\n                        ('PublicIP', result.get('ipAddress'))])", "response": "Transform a gateway list to table output."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of all blobs in the given blob container that match the given pattern.", "response": "def collect_blobs(blob_service, container, pattern=None):\n    \"\"\"\n    List the blobs in the given blob container, filter the blob by comparing their path to the given pattern.\n    \"\"\"\n    if not blob_service:\n        raise ValueError('missing parameter blob_service')\n\n    if not container:\n        raise ValueError('missing parameter container')\n\n    if not _pattern_has_wildcards(pattern):\n        return [pattern] if blob_service.exists(container, pattern) else []\n\n    results = []\n    for blob in blob_service.list_blobs(container):\n        try:\n            blob_name = blob.name.encode(\n                'utf-8') if isinstance(blob.name, unicode) else blob.name\n        except NameError:\n            blob_name = blob.name\n\n        if not pattern or _match_path(blob_name, pattern):\n            results.append(blob_name)\n\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncollects files in the given file share recursively.", "response": "def collect_files(cmd, file_service, share, pattern=None):\n    \"\"\"\n    Search files in the the given file share recursively. Filter the files by matching their path to the given pattern.\n    Returns a iterable of tuple (dir, name).\n    \"\"\"\n    if not file_service:\n        raise ValueError('missing parameter file_service')\n\n    if not share:\n        raise ValueError('missing parameter share')\n\n    if not _pattern_has_wildcards(pattern):\n        return [pattern]\n\n    return glob_files_remotely(cmd, file_service, share, pattern)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef glob_files_locally(folder_path, pattern):\n\n    pattern = os.path.join(\n        folder_path, pattern.lstrip('/')) if pattern else None\n\n    len_folder_path = len(folder_path) + 1\n    for root, _, files in os.walk(folder_path):\n        for f in files:\n            full_path = os.path.join(root, f)\n            if not pattern or _match_path(full_path, pattern):\n                yield (full_path, full_path[len_folder_path:])", "response": "glob files in local folder based on the given pattern"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef glob_files_remotely(cmd, client, share_name, pattern):\n    from collections import deque\n    t_dir, t_file = cmd.get_models('file.models#Directory', 'file.models#File')\n\n    queue = deque([\"\"])\n    while queue:\n        current_dir = queue.pop()\n        for f in client.list_directories_and_files(share_name, current_dir):\n            if isinstance(f, t_file):\n                if not pattern or _match_path(os.path.join(current_dir, f.name), pattern):\n                    yield current_dir, f.name\n            elif isinstance(f, t_dir):\n                queue.appendleft(os.path.join(current_dir, f.name))", "response": "glob the files in remote file share based on the given pattern"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef storage_command(self, name, method_name=None, command_type=None, oauth=False, generic_update=None, **kwargs):\n        if generic_update:\n            command_name = '{} {}'.format(self.group_name, name) if self.group_name else name\n            self.generic_update_command(name, **kwargs)\n        elif command_type:\n            command_name = self.command(name, method_name, command_type=command_type, **kwargs)\n        else:\n            command_name = self.command(name, method_name, **kwargs)\n        self._register_data_plane_account_arguments(command_name)\n        if oauth:\n            self._register_data_plane_oauth_arguments(command_name)", "response": "Registers an Azure CLI Storage Data Plane command."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds the required parameters required to create a data plane account.", "response": "def _register_data_plane_account_arguments(self, command_name):\n        \"\"\" Add parameters required to create a storage client \"\"\"\n        from azure.cli.core.commands.parameters import get_resource_name_completion_list\n        from ._validators import validate_client_parameters\n        command = self.command_loader.command_table.get(command_name, None)\n        if not command:\n            return\n\n        group_name = 'Storage Account'\n\n        command.add_argument('account_name', '--account-name', required=False, default=None,\n                             arg_group=group_name,\n                             completer=get_resource_name_completion_list('Microsoft.Storage/storageAccounts'),\n                             help='Storage account name. Related environment variable: AZURE_STORAGE_ACCOUNT. Must be '\n                                  'used in conjunction with either storage account key or a SAS token. If neither are '\n                                  'present, the command will try to query the storage account key using the '\n                                  'authenticated Azure account. If a large number of storage commands are executed the '\n                                  'API quota may be hit')\n        command.add_argument('account_key', '--account-key', required=False, default=None,\n                             arg_group=group_name,\n                             help='Storage account key. Must be used in conjunction with storage account name. '\n                                  'Environment variable: AZURE_STORAGE_KEY')\n        command.add_argument('connection_string', '--connection-string', required=False, default=None,\n                             validator=validate_client_parameters, arg_group=group_name,\n                             help='Storage account connection string. Environment variable: '\n                                  'AZURE_STORAGE_CONNECTION_STRING')\n        command.add_argument('sas_token', '--sas-token', required=False, default=None,\n                             arg_group=group_name,\n                             help='A Shared Access Signature (SAS). Must be used in conjunction with storage account '\n                                  'name. Environment variable: AZURE_STORAGE_SAS_TOKEN')"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns Kubernetes versions available for upgrading an existing cluster.", "response": "def get_k8s_upgrades_completion_list(cmd, prefix, namespace, **kwargs):  # pylint: disable=unused-argument\n    \"\"\"Return Kubernetes versions available for upgrading an existing cluster.\"\"\"\n    resource_group = getattr(namespace, 'resource_group_name', None)\n    name = getattr(namespace, 'name', None)\n    return get_k8s_upgrades(cmd.cli_ctx, resource_group, name) if resource_group and name else None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_k8s_versions_completion_list(cmd, prefix, namespace, **kwargs):  # pylint: disable=unused-argument\n    location = _get_location(cmd.cli_ctx, namespace)\n    return get_k8s_versions(cmd.cli_ctx, location) if location else None", "response": "Return Kubernetes versions available for provisioning a new cluster."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of Kubernetes versions available for a new cluster.", "response": "def get_k8s_versions(cli_ctx, location):\n    \"\"\"Return a list of Kubernetes versions available for a new cluster.\"\"\"\n    from ._client_factory import cf_container_services\n    from jmespath import search  # pylint: disable=import-error\n\n    results = cf_container_services(cli_ctx).list_orchestrators(location, resource_type='managedClusters').as_dict()\n    # Flatten all the \"orchestrator_version\" fields into one array\n    return search('orchestrators[*].orchestrator_version', results)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_vm_size_completion_list(cmd, prefix, namespace, **kwargs):  # pylint: disable=unused-argument\n\n    location = _get_location(cmd.cli_ctx, namespace)\n    result = get_vm_sizes(cmd.cli_ctx, location)\n    return set(r.name for r in result) & set(c.value for c in ContainerServiceVMSizeTypes)", "response": "Return the intersection of the VM sizes allowed by the ACS SDK with those returned by the Compute Service."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the location of the Azure resource group or subscription.", "response": "def _get_location(cli_ctx, namespace):\n    \"\"\"\n    Return an Azure location by using an explicit `--location` argument, then by `--resource-group`, and\n    finally by the subscription if neither argument was provided.\n    \"\"\"\n    location = None\n    if getattr(namespace, 'location', None):\n        location = namespace.location\n    elif getattr(namespace, 'resource_group_name', None):\n        location = _get_location_from_resource_group(cli_ctx, namespace.resource_group_name)\n    if not location:\n        location = get_one_of_subscription_locations(cli_ctx)\n    return location"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of all the placeholders names in order.", "response": "def get_placeholders(arg, check_duplicates=False):\n    \"\"\"\n    Get all the placeholders' names in order.\n    Use the regex below to locate all the opening ({{) and closing brackets (}}).\n    After that, extract \"stuff\" inside the brackets.\n\n    Args:\n        arg: The word which this function performs searching on.\n        check_duplicates: True if we want to check for duplicated positional arguments.\n\n    Returns:\n        A list of positional arguments in order.\n    \"\"\"\n    placeholders = []\n    last_match = None\n    arg = normalize_placeholders(arg)\n    for cur_match in re.finditer(r'\\s*{{|}}\\s*', arg):\n        matched_text = cur_match.group().strip()\n        if not last_match and matched_text == '{{':\n            last_match = cur_match\n            continue\n\n        last_matched_text = '' if not last_match else last_match.group().strip()\n        # Check if the positional argument is enclosed with {{ }} properly\n        if (not last_matched_text and matched_text == '}}') or (last_matched_text == '{{' and matched_text != '}}'):\n            raise CLIError(PLACEHOLDER_BRACKETS_ERROR.format(arg))\n        elif last_matched_text == '{{' and matched_text == '}}':\n            # Extract start and end index of the placeholder name\n            start_index, end_index = last_match.span()[1], cur_match.span()[0]\n            placeholders.append(arg[start_index: end_index].strip())\n            last_match = None\n\n    # last_match did not reset - that means brackets are not enclosed properly\n    if last_match:\n        raise CLIError(PLACEHOLDER_BRACKETS_ERROR.format(arg))\n\n    # Make sure there is no duplicated placeholder names\n    if check_duplicates and len(placeholders) != len(set(placeholders)):\n        raise CLIError(DUPLICATED_PLACEHOLDER_ERROR.format(arg))\n\n    return placeholders"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef normalize_placeholders(arg, inject_quotes=False):\n    number_placeholders = re.findall(r'{{\\s*\\d+\\s*}}', arg)\n    for number_placeholder in number_placeholders:\n        number = re.search(r'\\d+', number_placeholder).group()\n        arg = arg.replace(number_placeholder, '{{_' + number + '}}')\n\n    return arg.replace('{{', '\"{{').replace('}}', '}}\"') if inject_quotes else arg", "response": "Normalizes placeholders names so that the template engine can be ingested into Jinja template engine."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build_pos_args_table(full_alias, args, start_index):\n    pos_args_placeholder = get_placeholders(full_alias, check_duplicates=True)\n    pos_args = args[start_index: start_index + len(pos_args_placeholder)]\n\n    if len(pos_args_placeholder) != len(pos_args):\n        error_msg = INSUFFICIENT_POS_ARG_ERROR.format(full_alias,\n                                                      len(pos_args_placeholder),\n                                                      '' if len(pos_args_placeholder) == 1 else 's',\n                                                      len(pos_args))\n        raise CLIError(error_msg)\n\n    # Escape '\"' because we are using \"\" to surround placeholder expressions\n    for i, pos_arg in enumerate(pos_args):\n        pos_args[i] = pos_arg.replace('\"', '\\\\\"')\n\n    return dict(zip(pos_args_placeholder, pos_args))", "response": "Builds a dictionary where the key is the placeholder name and the value is the respective positional argument value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef render_template(cmd_derived_from_alias, pos_args_table):\n    try:\n        cmd_derived_from_alias = normalize_placeholders(cmd_derived_from_alias, inject_quotes=True)\n        template = jinja.Template(cmd_derived_from_alias)\n\n        # Shlex.split allows us to split a string by spaces while preserving quoted substrings\n        # (positional arguments in this case)\n        rendered = shlex.split(template.render(pos_args_table))\n\n        # Manually check if there is any runtime error (such as index out of range)\n        # since Jinja template engine only checks for compile time error.\n        # Only check for runtime errors if there is an empty string in rendered.\n        if '' in rendered:\n            check_runtime_errors(cmd_derived_from_alias, pos_args_table)\n\n        return rendered\n    except Exception as exception:\n        # Exception raised from runtime error\n        if isinstance(exception, CLIError):\n            raise\n\n        # The template has some sort of compile time errors\n        split_exception_message = str(exception).split()\n\n        # Check if the error message provides the index of the erroneous character\n        error_index = split_exception_message[-1]\n        if error_index.isdigit():\n            split_exception_message.insert(-1, 'index')\n            error_msg = RENDER_TEMPLATE_ERROR.format(' '.join(split_exception_message), cmd_derived_from_alias)\n\n            # Calculate where to put an arrow (^) char so that it is exactly below the erroneous character\n            # e.g. ... \"{{a.split('|)}}\"\n            #                       ^\n            error_msg += '\\n{}^'.format(' ' * (len(error_msg) - len(cmd_derived_from_alias) + int(error_index) - 1))\n        else:\n            exception_str = str(exception).replace('\"{{', '}}').replace('}}\"', '}}')\n            error_msg = RENDER_TEMPLATE_ERROR.format(cmd_derived_from_alias, exception_str)\n\n        raise CLIError(error_msg)", "response": "Render a Jinja template with positional arguments as the arguments."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_runtime_errors(cmd_derived_from_alias, pos_args_table):\n    for placeholder, value in pos_args_table.items():\n        exec('{} = \"{}\"'.format(placeholder, value))  # pylint: disable=exec-used\n\n    expressions = get_placeholders(cmd_derived_from_alias)\n    for expression in expressions:\n        try:\n            exec(expression)  # pylint: disable=exec-used\n        except Exception as exception:  # pylint: disable=broad-except\n            error_msg = PLACEHOLDER_EVAL_ERROR.format(expression, exception)\n            raise CLIError(error_msg)", "response": "Validate placeholders and expressions in cmd_derived_from_alias to make sure that there is no runtime error."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef export_rule_data(variables, actions):\n    from . import operators\n    actions_data = actions.get_all_actions()\n    variables_data = variables.get_all_variables()\n    variable_type_operators = {}\n    for variable_class in inspect.getmembers(operators, lambda x: getattr(x, 'export_in_rule_data', False)):\n        variable_type = variable_class[1] # getmembers returns (name, value)\n        variable_type_operators[variable_type.name] = variable_type.get_all_operators()\n\n    return {\"variables\": variables_data,\n            \"actions\": actions_data,\n            \"variable_type_operators\": variable_type_operators}", "response": "This function returns all information about the variables actions and operators in the a\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef float_to_decimal(f):\n    n, d = f.as_integer_ratio()\n    numerator, denominator = Decimal(n), Decimal(d)\n    ctx = Context(prec=60)\n    result = ctx.divide(numerator, denominator)\n    while ctx.flags[Inexact]:\n        ctx.flags[Inexact] = False\n        ctx.prec *= 2\n        result = ctx.divide(numerator, denominator)\n    return result", "response": "Convert a floating point number to a Decimal."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef type_operator(input_type, label=None,\n                  assert_type_for_arguments=True):\n    \"\"\" Decorator to make a function into a type operator.\n\n    - assert_type_for_arguments - if True this patches the operator function\n      so that arguments passed to it will have _assert_valid_value_and_cast\n      called on them to make type errors explicit.\n    \"\"\"\n    def wrapper(func):\n        func.is_operator = True\n        func.label = label \\\n            or fn_name_to_pretty_label(func.__name__)\n        func.input_type = input_type\n\n        @wraps(func)\n        def inner(self, *args, **kwargs):\n            if assert_type_for_arguments:\n                args = [self._assert_valid_value_and_cast(arg) for arg in args]\n                kwargs = dict((k, self._assert_valid_value_and_cast(v))\n                              for k, v in kwargs.items())\n            return func(self, *args, **kwargs)\n        return inner\n    return wrapper", "response": "Decorator to make a function into a type operator."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _validate_action_parameters(func, params):\n    if params is not None:\n        # Verify field name is valid\n        valid_fields = [getattr(fields, f) for f in dir(fields) \\\n                if f.startswith(\"FIELD_\")]\n        for param in params:\n            param_name, field_type = param['name'], param['fieldType']\n            if param_name not in func.__code__.co_varnames:\n                raise AssertionError(\"Unknown parameter name {0} specified for\"\\\n                        \" action {1}\".format(\n                        param_name, func.__name__))\n\n            if field_type not in valid_fields:\n                raise AssertionError(\"Unknown field type {0} specified for\"\\\n                        \" action {1} param {2}\".format(\n                        field_type, func.__name__, param_name))", "response": "Verifies that the parameters specified are valid for the the\nInsights action func."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rule_action(label=None, params=None):\n    def wrapper(func):\n        params_ = params\n        if isinstance(params, dict):\n            params_ = [dict(label=fn_name_to_pretty_label(name),\n                           name=name,\n                           fieldType=field_type) \\\n                      for name, field_type in params.items()]\n        _validate_action_parameters(func, params_)\n        func.is_rule_action = True\n        func.label = label \\\n                or fn_name_to_pretty_label(func.__name__)\n        func.params = params_\n        return func\n    return wrapper", "response": "Decorator to make a function into a rule action"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_condition(condition, defined_variables):\n    name, op, value = condition['name'], condition['operator'], condition['value']\n    operator_type = _get_variable_value(defined_variables, name)\n    return _do_operator_comparison(operator_type, op, value)", "response": "Checks a single rule condition."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_variable_value(defined_variables, name):\n    def fallback(*args, **kwargs):\n        raise AssertionError(\"Variable {0} is not defined in class {1}\".format(\n                name, defined_variables.__class__.__name__))\n    method = getattr(defined_variables, name, fallback)\n    val = method()\n    return method.field_type(val)", "response": "Get the value of a variable in the\n   ."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _do_operator_comparison(operator_type, operator_name, comparison_value):\n    def fallback(*args, **kwargs):\n        raise AssertionError(\"Operator {0} does not exist for type {1}\".format(\n            operator_name, operator_type.__class__.__name__))\n    method = getattr(operator_type, operator_name, fallback)\n    if getattr(method, 'input_type', '') == FIELD_NO_INPUT:\n        return method()\n    return method(comparison_value)", "response": "Returns a boolean that indicates whether the given operator_value is equal to the given operator_type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndisabling automatic corrections and completions.", "response": "def build_attrs(self, *args, **kwargs):\n        \"\"\"Disable automatic corrections and completions.\"\"\"\n        attrs = super(CaptchaAnswerInput, self).build_attrs(*args, **kwargs)\n        attrs['autocapitalize'] = 'off'\n        attrs['autocomplete'] = 'off'\n        attrs['autocorrect'] = 'off'\n        attrs['spellcheck'] = 'false'\n        return attrs"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fetch_captcha_store(self, name, value, attrs=None, generator=None):\n        try:\n            reverse('captcha-image', args=('dummy',))\n        except NoReverseMatch:\n            raise ImproperlyConfigured('Make sure you\\'ve included captcha.urls as explained in the INSTALLATION section on http://readthedocs.org/docs/django-simple-captcha/en/latest/usage.html#installation')\n\n        if settings.CAPTCHA_GET_FROM_POOL:\n            key = CaptchaStore.pick()\n        else:\n            key = CaptchaStore.generate_key(generator)\n\n        # these can be used by format_output and render\n        self._value = [key, u('')]\n        self._key = key\n        self.id_ = self.build_attrs(attrs).get('id', None)", "response": "This method is called by render_captcha_store to fetch a new CaptchaStore object"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_context(self, name, value, attrs):\n        context = super(CaptchaTextInput, self).get_context(name, value, attrs)\n        context['image'] = self.image_url()\n        context['audio'] = self.audio_url()\n        return context", "response": "Add captcha specific variables to context."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrender the widget the old way - using field_template or output_format.", "response": "def _direct_render(self, name, attrs):\n        \"\"\"Render the widget the old way - using field_template or output_format.\"\"\"\n        context = {\n            'image': self.image_url(),\n            'name': name,\n            'key': self._key,\n            'id': u'%s_%s' % (self.id_prefix, attrs.get('id')) if self.id_prefix else attrs.get('id'),\n            'audio': self.audio_url(),\n        }\n        self.image_and_audio = render_to_string(settings.CAPTCHA_IMAGE_TEMPLATE, context)\n        self.hidden_field = render_to_string(settings.CAPTCHA_HIDDEN_FIELD_TEMPLATE, context)\n        self.text_field = render_to_string(settings.CAPTCHA_TEXT_FIELD_TEMPLATE, context)\n        return self.format_output(None)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning json with new captcha for ajax refresh request", "response": "def captcha_refresh(request):\n    \"\"\"  Return json with new captcha for ajax refresh request \"\"\"\n    if not request.is_ajax():\n        raise Http404\n\n    new_key = CaptchaStore.pick()\n    to_json_response = {\n        'key': new_key,\n        'image_url': captcha_image_url(new_key),\n        'audio_url': captcha_audio_url(new_key) if settings.CAPTCHA_FLITE_PATH else None\n    }\n    return HttpResponse(json.dumps(to_json_response), content_type='application/json')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make_discord_blueprint(\n    client_id=None,\n    client_secret=None,\n    scope=None,\n    redirect_url=None,\n    redirect_to=None,\n    login_url=None,\n    authorized_url=None,\n    session_class=None,\n    storage=None,\n):\n    \"\"\"\n    Make a blueprint for authenticating with Discord using OAuth 2. This requires\n    a client ID and client secret from Discord. You should either pass them to\n    this constructor, or make sure that your Flask application config defines\n    them, using the variables :envvar:`DISCORD_OAUTH_CLIENT_ID` and\n    :envvar:`DISCORD_OAUTH_CLIENT_SECRET`.\n\n    Args:\n        client_id (str): The client ID for your application on Discord.\n        client_secret (str): The client secret for your application on Discord\n        scope (list, optional): list of scopes (str) for the OAuth token\n        redirect_url (str): the URL to redirect to after the authentication\n            dance is complete\n        redirect_to (str): if ``redirect_url`` is not defined, the name of the\n            view to redirect to after the authentication dance is complete.\n            The actual URL will be determined by :func:`flask.url_for`\n        login_url (str, optional): the URL path for the ``login`` view.\n            Defaults to ``/discord``\n        authorized_url (str, optional): the URL path for the ``authorized`` view.\n            Defaults to ``/discord/authorized``.\n        session_class (class, optional): The class to use for creating a\n            Requests session. Defaults to\n            :class:`~flask_dance.consumer.requests.OAuth2Session`.\n        storage: A token storage class, or an instance of a token storage\n                class, to use for this blueprint. Defaults to\n                :class:`~flask_dance.consumer.storage.session.SessionStorage`.\n\n    :rtype: :class:`~flask_dance.consumer.OAuth2ConsumerBlueprint`\n    :returns: A :ref:`blueprint <flask:blueprints>` to attach to your Flask app.\n    \"\"\"\n    scope = scope or [\"identify\"]\n    discord_bp = OAuth2ConsumerBlueprint(\n        \"discord\",\n        __name__,\n        client_id=client_id,\n        client_secret=client_secret,\n        scope=scope,\n        base_url=\"https://discordapp.com/\",\n        token_url=\"https://discordapp.com/api/oauth2/token\",\n        authorization_url=\"https://discordapp.com/api/oauth2/authorize\",\n        redirect_url=redirect_url,\n        redirect_to=redirect_to,\n        login_url=login_url,\n        authorized_url=authorized_url,\n        session_class=session_class,\n        storage=storage,\n    )\n    discord_bp.from_config[\"client_id\"] = \"DISCORD_OAUTH_CLIENT_ID\"\n    discord_bp.from_config[\"client_secret\"] = \"DISCORD_OAUTH_CLIENT_SECRET\"\n\n    @discord_bp.before_app_request\n    def set_applocal_session():\n        ctx = stack.top\n        ctx.discord_oauth = discord_bp.session\n\n    return discord_bp", "response": "Creates a Discord Blueprint for authenticating with Discord using OAuth 2."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make_authentiq_blueprint(\n    client_id=None,\n    client_secret=None,\n    scope=\"openid profile\",\n    redirect_url=None,\n    redirect_to=None,\n    login_url=None,\n    authorized_url=None,\n    session_class=None,\n    storage=None,\n    hostname=\"connect.authentiq.io\",\n):\n    \"\"\"\n    Make a blueprint for authenticating with authentiq using OAuth 2. This requires\n    a client ID and client secret from authentiq. You should either pass them to\n    this constructor, or make sure that your Flask application config defines\n    them, using the variables :envvar:`AUTHENTIQ_OAUTH_CLIENT_ID` and\n    :envvar:`AUTHENTIQ_OAUTH_CLIENT_SECRET`.\n\n    Args:\n        client_id (str): The client ID for your application on Authentiq.\n        client_secret (str): The client secret for your application on Authentiq.\n        scope (str, optional): comma-separated list of scopes for the OAuth token.\n        redirect_url (str): the URL to redirect to after the authentication\n            dance is complete.\n        redirect_to (str): if ``redirect_url`` is not defined, the name of the\n            view to redirect to after the authentication dance is complete.\n            The actual URL will be determined by :func:`flask.url_for`.\n        login_url (str, optional): the URL path for the ``login`` view.\n            Defaults to ``/authentiq``.\n        authorized_url (str, optional): the URL path for the ``authorized`` view.\n            Defaults to ``/authentiq/authorized``.\n        session_class (class, optional): The class to use for creating a\n            Requests session. Defaults to\n            :class:`~flask_dance.consumer.requests.OAuth2Session`.\n        storage: A token storage class, or an instance of a token storage\n                class, to use for this blueprint. Defaults to\n                :class:`~flask_dance.consumer.storage.session.SessionStorage`.\n        hostname (str, optional): If using a private instance of authentiq CE/EE,\n            specify the hostname, default is ``connect.authentiq.io``\n\n    :rtype: :class:`~flask_dance.consumer.OAuth2ConsumerBlueprint`\n    :returns: A :ref:`blueprint <flask:blueprints>` to attach to your Flask app.\n    \"\"\"\n    authentiq_bp = OAuth2ConsumerBlueprint(\n        \"authentiq\",\n        __name__,\n        client_id=client_id,\n        client_secret=client_secret,\n        scope=scope,\n        base_url=\"https://{hostname}/\".format(hostname=hostname),\n        authorization_url=\"https://{hostname}/authorize\".format(hostname=hostname),\n        token_url=\"https://{hostname}/token\".format(hostname=hostname),\n        redirect_url=redirect_url,\n        redirect_to=redirect_to,\n        login_url=login_url,\n        authorized_url=authorized_url,\n        session_class=session_class,\n        storage=storage,\n    )\n    authentiq_bp.from_config[\"client_id\"] = \"AUTHENTIQ_OAUTH_CLIENT_ID\"\n    authentiq_bp.from_config[\"client_secret\"] = \"AUTHENTIQ_OAUTH_CLIENT_SECRET\"\n\n    @authentiq_bp.before_app_request\n    def set_applocal_session():\n        ctx = stack.top\n        ctx.authentiq_oauth = authentiq_bp.session\n\n    return authentiq_bp", "response": "Returns a Flask - Dance authentiq blueprint."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_meetup_blueprint(\n    key=None,\n    secret=None,\n    scope=None,\n    redirect_url=None,\n    redirect_to=None,\n    login_url=None,\n    authorized_url=None,\n    session_class=None,\n    storage=None,\n):\n    \"\"\"\n    Make a blueprint for authenticating with Meetup using OAuth 2. This requires\n    an OAuth consumer from Meetup. You should either pass the key and secret to\n    this constructor, or make sure that your Flask application config defines\n    them, using the variables :envvar:`MEETUP_OAUTH_CLIENT_ID` and\n    :envvar:`MEETUP_OAUTH_CLIENT_SECRET`.\n\n    Args:\n        key (str): The OAuth consumer key for your application on Meetup\n        secret (str): The OAuth consumer secret for your application on Meetup\n        scope (str, optional): comma-separated list of scopes for the OAuth token\n        redirect_url (str): the URL to redirect to after the authentication\n            dance is complete\n        redirect_to (str): if ``redirect_url`` is not defined, the name of the\n            view to redirect to after the authentication dance is complete.\n            The actual URL will be determined by :func:`flask.url_for`\n        login_url (str, optional): the URL path for the ``login`` view.\n            Defaults to ``/meetup``\n        authorized_url (str, optional): the URL path for the ``authorized`` view.\n            Defaults to ``/meetup/authorized``.\n        session_class (class, optional): The class to use for creating a\n            Requests session. Defaults to\n            :class:`~flask_dance.consumer.requests.OAuth2Session`.\n        storage: A token storage class, or an instance of a token storage\n                class, to use for this blueprint. Defaults to\n                :class:`~flask_dance.consumer.storage.session.SessionStorage`.\n\n    :rtype: :class:`~flask_dance.consumer.OAuth2ConsumerBlueprint`\n    :returns: A :ref:`blueprint <flask:blueprints>` to attach to your Flask app.\n    \"\"\"\n    scope = scope or [\"basic\"]\n    meetup_bp = OAuth2ConsumerBlueprint(\n        \"meetup\",\n        __name__,\n        client_id=key,\n        client_secret=secret,\n        scope=scope,\n        base_url=\"https://api.meetup.com/2/\",\n        authorization_url=\"https://secure.meetup.com/oauth2/authorize\",\n        token_url=\"https://secure.meetup.com/oauth2/access\",\n        redirect_url=redirect_url,\n        redirect_to=redirect_to,\n        login_url=login_url,\n        authorized_url=authorized_url,\n        session_class=session_class,\n        storage=storage,\n    )\n    meetup_bp.from_config[\"client_id\"] = \"MEETUP_OAUTH_CLIENT_ID\"\n    meetup_bp.from_config[\"client_secret\"] = \"MEETUP_OAUTH_CLIENT_SECRET\"\n\n    @meetup_bp.before_app_request\n    def set_applocal_session():\n        ctx = stack.top\n        ctx.meetup_oauth = meetup_bp.session\n\n    return meetup_bp", "response": "Creates a Meetup OAuth 2 consumer blueprint."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a Dropbox blueprint for authenticating with Dropbox using OAuth 2.", "response": "def make_dropbox_blueprint(\n    app_key=None,\n    app_secret=None,\n    scope=None,\n    force_reapprove=False,\n    disable_signup=False,\n    require_role=None,\n    redirect_url=None,\n    redirect_to=None,\n    login_url=None,\n    authorized_url=None,\n    session_class=None,\n    storage=None,\n):\n    \"\"\"\n    Make a blueprint for authenticating with Dropbox using OAuth 2. This requires\n    a client ID and client secret from Dropbox. You should either pass them to\n    this constructor, or make sure that your Flask application config defines\n    them, using the variables :envvar:`DROPBOX_OAUTH_CLIENT_ID` and\n    :envvar:`DROPBOX_OAUTH_CLIENT_SECRET`.\n\n    For more information about the ``force_reapprove``, ``disable_signup``,\n    and ``require_role`` arguments, `check the Dropbox API documentation\n    <https://www.dropbox.com/developers-v1/core/docs#oa2-authorize>`_.\n\n    Args:\n        app_key (str): The client ID for your application on Dropbox.\n        app_secret (str): The client secret for your application on Dropbox\n        scope (str, optional): Comma-separated list of scopes for the OAuth token\n        force_reapprove (bool): Force the user to approve the app again\n            if they've already done so.\n        disable_signup (bool): Prevent users from seeing a sign-up link\n            on the authorization page.\n        require_role (str): Pass the string ``work`` to require a Dropbox\n            for Business account, or the string ``personal`` to require a\n            personal account.\n        redirect_url (str): the URL to redirect to after the authentication\n            dance is complete\n        redirect_to (str): if ``redirect_url`` is not defined, the name of the\n            view to redirect to after the authentication dance is complete.\n            The actual URL will be determined by :func:`flask.url_for`\n        login_url (str, optional): the URL path for the ``login`` view.\n            Defaults to ``/dropbox``\n        authorized_url (str, optional): the URL path for the ``authorized`` view.\n            Defaults to ``/dropbox/authorized``.\n        session_class (class, optional): The class to use for creating a\n            Requests session. Defaults to\n            :class:`~flask_dance.consumer.requests.OAuth2Session`.\n        storage: A token storage class, or an instance of a token storage\n                class, to use for this blueprint. Defaults to\n                :class:`~flask_dance.consumer.storage.session.SessionStorage`.\n\n    :rtype: :class:`~flask_dance.consumer.OAuth2ConsumerBlueprint`\n    :returns: A :ref:`blueprint <flask:blueprints>` to attach to your Flask app.\n    \"\"\"\n    authorization_url_params = {}\n    if force_reapprove:\n        authorization_url_params[\"force_reapprove\"] = \"true\"\n    if disable_signup:\n        authorization_url_params[\"disable_signup\"] = \"true\"\n    if require_role:\n        authorization_url_params[\"require_role\"] = require_role\n\n    dropbox_bp = OAuth2ConsumerBlueprint(\n        \"dropbox\",\n        __name__,\n        client_id=app_key,\n        client_secret=app_secret,\n        scope=scope,\n        base_url=\"https://api.dropbox.com/2/\",\n        authorization_url=\"https://www.dropbox.com/oauth2/authorize\",\n        token_url=\"https://api.dropbox.com/oauth2/token\",\n        redirect_url=redirect_url,\n        redirect_to=redirect_to,\n        login_url=login_url,\n        authorized_url=authorized_url,\n        authorization_url_params=authorization_url_params,\n        session_class=session_class,\n        storage=storage,\n    )\n    dropbox_bp.from_config[\"client_id\"] = \"DROPBOX_OAUTH_CLIENT_ID\"\n    dropbox_bp.from_config[\"client_secret\"] = \"DROPBOX_OAUTH_CLIENT_SECRET\"\n\n    @dropbox_bp.before_app_request\n    def set_applocal_session():\n        ctx = stack.top\n        ctx.dropbox_oauth = dropbox_bp.session\n\n    return dropbox_bp"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a blueprint for authenticating with Azure AD using OAuth 2.", "response": "def make_azure_blueprint(\n    client_id=None,\n    client_secret=None,\n    scope=None,\n    redirect_url=None,\n    redirect_to=None,\n    login_url=None,\n    authorized_url=None,\n    session_class=None,\n    storage=None,\n    tenant=\"common\",\n):\n    \"\"\"\n    Make a blueprint for authenticating with Azure AD using OAuth 2. This requires\n    a client ID and client secret from Azure AD. You should either pass them to\n    this constructor, or make sure that your Flask application config defines\n    them, using the variables :envvar:`AZURE_OAUTH_CLIENT_ID` and\n    :envvar:`AZURE_OAUTH_CLIENT_SECRET`.\n\n    Args:\n        client_id (str): The client ID for your application on Azure AD.\n        client_secret (str): The client secret for your application on Azure AD\n        scope (str, optional): comma-separated list of scopes for the OAuth token\n        redirect_url (str): the URL to redirect to after the authentication\n            dance is complete\n        redirect_to (str): if ``redirect_url`` is not defined, the name of the\n            view to redirect to after the authentication dance is complete.\n            The actual URL will be determined by :func:`flask.url_for`\n        login_url (str, optional): the URL path for the ``login`` view.\n            Defaults to ``/azure``\n        authorized_url (str, optional): the URL path for the ``authorized`` view.\n            Defaults to ``/azure/authorized``.\n        session_class (class, optional): The class to use for creating a\n            Requests session. Defaults to\n            :class:`~flask_dance.consumer.requests.OAuth2Session`.\n        storage: A token storage class, or an instance of a token storage\n                class, to use for this blueprint. Defaults to\n                :class:`~flask_dance.consumer.storage.session.SessionStorage`.\n        tenant: Determine which accounts are allowed to authenticate with Azure.\n                `See the Azure documentation for more information about this parameter.\n                <https://docs.microsoft.com/en-us/azure/active-directory/develop/active-directory-v2-protocols#endpoints>`_\n                Defaults to ``common``.\n\n\n    :rtype: :class:`~flask_dance.consumer.OAuth2ConsumerBlueprint`\n    :returns: A :ref:`blueprint <flask:blueprints>` to attach to your Flask app.\n    \"\"\"\n    scope = scope or [\"openid\", \"email\", \"profile\", \"User.Read\"]\n    azure_bp = OAuth2ConsumerBlueprint(\n        \"azure\",\n        __name__,\n        client_id=client_id,\n        client_secret=client_secret,\n        scope=scope,\n        base_url=\"https://graph.microsoft.com\",\n        authorization_url=\"https://login.microsoftonline.com/{tenant}/oauth2/v2.0/authorize\".format(\n            tenant=tenant\n        ),\n        token_url=\"https://login.microsoftonline.com/{tenant}/oauth2/v2.0/token\".format(\n            tenant=tenant\n        ),\n        redirect_url=redirect_url,\n        redirect_to=redirect_to,\n        login_url=login_url,\n        authorized_url=authorized_url,\n        session_class=session_class,\n        storage=storage,\n    )\n    azure_bp.from_config[\"client_id\"] = \"AZURE_OAUTH_CLIENT_ID\"\n    azure_bp.from_config[\"client_secret\"] = \"AZURE_OAUTH_CLIENT_SECRET\"\n\n    @azure_bp.before_app_request\n    def set_applocal_session():\n        ctx = stack.top\n        ctx.azure_oauth = azure_bp.session\n\n    return azure_bp"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_google_blueprint(\n    client_id=None,\n    client_secret=None,\n    scope=None,\n    offline=False,\n    reprompt_consent=False,\n    reprompt_select_account=False,\n    redirect_url=None,\n    redirect_to=None,\n    login_url=None,\n    authorized_url=None,\n    session_class=None,\n    storage=None,\n    hosted_domain=None,\n):\n    \"\"\"\n    Make a blueprint for authenticating with Google using OAuth 2. This requires\n    a client ID and client secret from Google. You should either pass them to\n    this constructor, or make sure that your Flask application config defines\n    them, using the variables :envvar:`GOOGLE_OAUTH_CLIENT_ID` and\n    :envvar:`GOOGLE_OAUTH_CLIENT_SECRET`.\n\n    Args:\n        client_id (str): The client ID for your application on Google\n        client_secret (str): The client secret for your application on Google\n        scope (str, optional): comma-separated list of scopes for the OAuth token.\n            Defaults to the \"https://www.googleapis.com/auth/userinfo.profile\" scope.\n        offline (bool): Whether to request `offline access\n            <https://developers.google.com/accounts/docs/OAuth2WebServer#offline>`_\n            for the OAuth token. Defaults to False\n        reprompt_consent (bool): If True, force Google to re-prompt the user\n            for their consent, even if the user has already given their\n            consent. Defaults to False\n        reprompt_select_account (bool): If True, force Google to re-prompt the select account page,\n            even if there is a single logged-in user. Defaults to False\n        redirect_url (str): the URL to redirect to after the authentication\n            dance is complete\n        redirect_to (str): if ``redirect_url`` is not defined, the name of the\n            view to redirect to after the authentication dance is complete.\n            The actual URL will be determined by :func:`flask.url_for`\n        login_url (str, optional): the URL path for the ``login`` view.\n            Defaults to ``/google``\n        authorized_url (str, optional): the URL path for the ``authorized`` view.\n            Defaults to ``/google/authorized``.\n        session_class (class, optional): The class to use for creating a\n            Requests session. Defaults to\n            :class:`~flask_dance.consumer.requests.OAuth2Session`.\n        storage: A token storage class, or an instance of a token storage\n                class, to use for this blueprint. Defaults to\n                :class:`~flask_dance.consumer.storage.session.SessionStorage`.\n        hosted_domain (str, optional): The domain of the G Suite user. Used to indicate that the account selection UI\n            should be optimized for accounts at this domain. Note that this only provides UI optimization, and requires\n            response validation (see warning).\n\n    .. _google_hosted_domain_warning:\n    .. warning::\n       The ``hosted_domain`` argument **only provides UI optimization**. Don't rely on this argument to control\n       who can access your application. You must verify that the ``hd`` claim of the response ID token matches the\n       ``hosted_domain`` argument passed to ``make_google_blueprint``. For example:\n\n       .. code-block:: python\n\n            from flask import session, abort\n            from flask_dance.consumer import oauth_authorized\n            from flask_dance.contrib.google import make_google_blueprint, google\n            import requests\n\n            google_bp = make_google_blueprint(\n                client_id=\"foo\",\n                client_secret=\"bar\",\n                scope=[\"profile\", \"email\"],\n                hosted_domain=\"example.com\"\n            )\n\n            @oauth_authorized.connect_via(google_bp)\n            def logged_in(blueprint, token):\n                resp_json = google.get(\"/oauth2/v2/userinfo\").json()\n                if resp_json[\"hd\"] != blueprint.authorization_url_params[\"hd\"]:\n                    requests.post(\n                        \"https://accounts.google.com/o/oauth2/revoke\",\n                        params={\"token\": token[\"access_token\"]}\n                    )\n                    session.clear()\n                    abort(403)\n\n\n    :rtype: :class:`~flask_dance.consumer.OAuth2ConsumerBlueprint`\n    :returns: A :ref:`blueprint <flask:blueprints>` to attach to your Flask app.\n    \"\"\"\n    scope = scope or [\"https://www.googleapis.com/auth/userinfo.profile\"]\n    authorization_url_params = {}\n    prompt_params = []\n    auto_refresh_url = None\n    if offline:\n        authorization_url_params[\"access_type\"] = \"offline\"\n        auto_refresh_url = \"https://accounts.google.com/o/oauth2/token\"\n    if reprompt_consent:\n        prompt_params.append(\"consent\")\n    if reprompt_select_account:\n        prompt_params.append(\"select_account\")\n    if prompt_params:\n        prompt_params = \" \".join(prompt_params)\n        authorization_url_params[\"prompt\"] = prompt_params\n    if hosted_domain:\n        authorization_url_params[\"hd\"] = hosted_domain\n    google_bp = OAuth2ConsumerBlueprint(\n        \"google\",\n        __name__,\n        client_id=client_id,\n        client_secret=client_secret,\n        scope=scope,\n        base_url=\"https://www.googleapis.com/\",\n        authorization_url=\"https://accounts.google.com/o/oauth2/auth\",\n        token_url=\"https://accounts.google.com/o/oauth2/token\",\n        auto_refresh_url=auto_refresh_url,\n        redirect_url=redirect_url,\n        redirect_to=redirect_to,\n        login_url=login_url,\n        authorized_url=authorized_url,\n        authorization_url_params=authorization_url_params,\n        session_class=session_class,\n        storage=storage,\n    )\n    google_bp.from_config[\"client_id\"] = \"GOOGLE_OAUTH_CLIENT_ID\"\n    google_bp.from_config[\"client_secret\"] = \"GOOGLE_OAUTH_CLIENT_SECRET\"\n\n    @google_bp.before_app_request\n    def set_applocal_session():\n        ctx = stack.top\n        ctx.google_oauth = google_bp.session\n\n    return google_bp", "response": "Creates a google Blueprint for authenticating with Google using OAuth 2."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_config(self):\n        for local_var, config_var in self.from_config.items():\n            value = flask.current_app.config.get(config_var)\n            if value:\n                if \".\" in local_var:\n                    # this is a dotpath -- needs special handling\n                    body, tail = local_var.rsplit(\".\", 1)\n                    obj = getattrd(self, body)\n                    setattr(obj, tail, value)\n                else:\n                    # just use a normal setattr call\n                    setattr(self, local_var, value)", "response": "Loads the configuration from the Flask application into the blueprint object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a Slack blueprint for authenticating with Slack using OAuth 2.", "response": "def make_slack_blueprint(\n    client_id=None,\n    client_secret=None,\n    scope=None,\n    redirect_url=None,\n    redirect_to=None,\n    login_url=None,\n    authorized_url=None,\n    session_class=None,\n    storage=None,\n):\n    \"\"\"\n    Make a blueprint for authenticating with Slack using OAuth 2. This requires\n    a client ID and client secret from Slack. You should either pass them to\n    this constructor, or make sure that your Flask application config defines\n    them, using the variables :envvar:`SLACK_OAUTH_CLIENT_ID` and\n    :envvar:`SLACK_OAUTH_CLIENT_SECRET`.\n\n    Args:\n        client_id (str): The client ID for your application on Slack.\n        client_secret (str): The client secret for your application on Slack\n        scope (str, optional): comma-separated list of scopes for the OAuth token\n        redirect_url (str): the URL to redirect to after the authentication\n            dance is complete\n        redirect_to (str): if ``redirect_url`` is not defined, the name of the\n            view to redirect to after the authentication dance is complete.\n            The actual URL will be determined by :func:`flask.url_for`\n        login_url (str, optional): the URL path for the ``login`` view.\n            Defaults to ``/slack``\n        authorized_url (str, optional): the URL path for the ``authorized`` view.\n            Defaults to ``/slack/authorized``.\n        session_class (class, optional): The class to use for creating a\n            Requests session. Defaults to\n            :class:`~flask_dance.consumer.requests.OAuth2Session`.\n        storage: A token storage class, or an instance of a token storage\n                class, to use for this blueprint. Defaults to\n                :class:`~flask_dance.consumer.storage.session.SessionStorage`.\n\n    :rtype: :class:`~flask_dance.consumer.OAuth2ConsumerBlueprint`\n    :returns: A :ref:`blueprint <flask:blueprints>` to attach to your Flask app.\n    \"\"\"\n    scope = scope or [\"identify\", \"chat:write:bot\"]\n    slack_bp = SlackBlueprint(\n        \"slack\",\n        __name__,\n        client_id=client_id,\n        client_secret=client_secret,\n        scope=scope,\n        base_url=\"https://slack.com/api/\",\n        authorization_url=\"https://slack.com/oauth/authorize\",\n        token_url=\"https://slack.com/api/oauth.access\",\n        redirect_url=redirect_url,\n        redirect_to=redirect_to,\n        login_url=login_url,\n        authorized_url=authorized_url,\n        session_class=session_class,\n        storage=storage,\n    )\n    slack_bp.from_config[\"client_id\"] = \"SLACK_OAUTH_CLIENT_ID\"\n    slack_bp.from_config[\"client_secret\"] = \"SLACK_OAUTH_CLIENT_SECRET\"\n\n    @slack_bp.before_app_request\n    def set_applocal_session():\n        ctx = stack.top\n        ctx.slack_oauth = slack_bp.session\n\n    return slack_bp"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_reddit_blueprint(\n    client_id=None,\n    client_secret=None,\n    scope=\"identity\",\n    permanent=False,\n    redirect_url=None,\n    redirect_to=None,\n    login_url=None,\n    authorized_url=None,\n    session_class=None,\n    storage=None,\n    user_agent=None,\n):\n    \"\"\"\n    Make a blueprint for authenticating with Reddit using OAuth 2. This requires\n    a client ID and client secret from Reddit. You should either pass them to\n    this constructor, or make sure that your Flask application config defines\n    them, using the variables :envvar:`REDDIT_OAUTH_CLIENT_ID` and\n    :envvar:`REDDIT_OAUTH_CLIENT_SECRET`.\n\n    Args:\n        client_id (str): The client ID for your application on Reddit.\n        client_secret (str): The client secret for your application on Reddit\n        scope (str, optional): space-separated list of scopes for the OAuth token\n            Defaults to ``identity``\n        permanent (bool, optional): Whether to request permanent access token.\n            Defaults to False, access will be valid for 1 hour\n        redirect_url (str): the URL to redirect to after the authentication\n            dance is complete\n        redirect_to (str): if ``redirect_url`` is not defined, the name of the\n            view to redirect to after the authentication dance is complete.\n            The actual URL will be determined by :func:`flask.url_for`\n        login_url (str, optional): the URL path for the ``login`` view.\n            Defaults to ``/reddit``\n        authorized_url (str, optional): the URL path for the ``authorized`` view.\n            Defaults to ``/reddit/authorized``.\n        session_class (class, optional): The class to use for creating a\n            Requests session. Defaults to\n            :class:`~flask_dance.contrib.reddit.RedditOAuth2Session`.\n        storage: A token storage class, or an instance of a token storage\n            class, to use for this blueprint. Defaults to\n            :class:`~flask_dance.consumer.storage.session.SessionStorage`.\n        user_agent (str, optional): User agent for the requests to Reddit API.\n            Defaults to ``Flask-Dance/{{version}}``\n\n    :rtype: :class:`~flask_dance.consumer.OAuth2ConsumerBlueprint`\n    :returns: A :ref:`blueprint <flask:blueprints>` to attach to your Flask app.\n    \"\"\"\n    authorization_url_params = {}\n    if permanent:\n        authorization_url_params[\"duration\"] = \"permanent\"\n\n    reddit_bp = OAuth2ConsumerBlueprint(\n        \"reddit\",\n        __name__,\n        client_id=client_id,\n        client_secret=client_secret,\n        scope=scope,\n        base_url=\"https://oauth.reddit.com/\",\n        authorization_url=\"https://www.reddit.com/api/v1/authorize\",\n        authorization_url_params=authorization_url_params,\n        token_url=\"https://www.reddit.com/api/v1/access_token\",\n        auto_refresh_url=\"https://www.reddit.com/api/v1/access_token\",\n        redirect_url=redirect_url,\n        redirect_to=redirect_to,\n        login_url=login_url,\n        authorized_url=authorized_url,\n        session_class=session_class or RedditOAuth2Session,\n        storage=storage,\n    )\n\n    reddit_bp.from_config[\"client_id\"] = \"REDDIT_OAUTH_CLIENT_ID\"\n    reddit_bp.from_config[\"client_secret\"] = \"REDDIT_OAUTH_CLIENT_SECRET\"\n\n    reddit_bp.user_agent = user_agent\n\n    @reddit_bp.before_app_request\n    def set_applocal_session():\n        ctx = stack.top\n        ctx.reddit_oauth = reddit_bp.session\n\n    return reddit_bp", "response": "Creates a Reddit blueprint for authenticating with Reddit using OAuth 2."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_twitter_blueprint(\n    api_key=None,\n    api_secret=None,\n    redirect_url=None,\n    redirect_to=None,\n    login_url=None,\n    authorized_url=None,\n    session_class=None,\n    storage=None,\n):\n    \"\"\"\n    Make a blueprint for authenticating with Twitter using OAuth 1. This requires\n    an API key and API secret from Twitter. You should either pass them to\n    this constructor, or make sure that your Flask application config defines\n    them, using the variables :envvar:`TWITTER_OAUTH_CLIENT_KEY` and\n    :envvar:`TWITTER_OAUTH_CLIENT_SECRET`.\n\n    Args:\n        api_key (str): The API key for your Twitter application\n        api_secret (str): The API secret for your Twitter application\n        redirect_url (str): the URL to redirect to after the authentication\n            dance is complete\n        redirect_to (str): if ``redirect_url`` is not defined, the name of the\n            view to redirect to after the authentication dance is complete.\n            The actual URL will be determined by :func:`flask.url_for`\n        login_url (str, optional): the URL path for the ``login`` view.\n            Defaults to ``/twitter``\n        authorized_url (str, optional): the URL path for the ``authorized`` view.\n            Defaults to ``/twitter/authorized``.\n        session_class (class, optional): The class to use for creating a\n            Requests session. Defaults to\n            :class:`~flask_dance.consumer.requests.OAuth1Session`.\n        storage: A token storage class, or an instance of a token storage\n                class, to use for this blueprint. Defaults to\n                :class:`~flask_dance.consumer.storage.session.SessionStorage`.\n\n    :rtype: :class:`~flask_dance.consumer.OAuth1ConsumerBlueprint`\n    :returns: A :ref:`blueprint <flask:blueprints>` to attach to your Flask app.\n    \"\"\"\n    twitter_bp = OAuth1ConsumerBlueprint(\n        \"twitter\",\n        __name__,\n        client_key=api_key,\n        client_secret=api_secret,\n        base_url=\"https://api.twitter.com/1.1/\",\n        request_token_url=\"https://api.twitter.com/oauth/request_token\",\n        access_token_url=\"https://api.twitter.com/oauth/access_token\",\n        authorization_url=\"https://api.twitter.com/oauth/authorize\",\n        redirect_url=redirect_url,\n        redirect_to=redirect_to,\n        login_url=login_url,\n        authorized_url=authorized_url,\n        session_class=session_class,\n        storage=storage,\n    )\n    twitter_bp.from_config[\"client_key\"] = \"TWITTER_OAUTH_CLIENT_KEY\"\n    twitter_bp.from_config[\"client_secret\"] = \"TWITTER_OAUTH_CLIENT_SECRET\"\n\n    @twitter_bp.before_app_request\n    def set_applocal_session():\n        ctx = stack.top\n        ctx.twitter_oauth = twitter_bp.session\n\n    return twitter_bp", "response": "Creates a Twitter OAuth 1 consumer blueprint for authenticating with Twitter using OAuth 1."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a new instance of the session class that is used to access the current user s data.", "response": "def session(self):\n        \"\"\"\n        This is a session between the consumer (your website) and the provider\n        (e.g. Twitter). It is *not* a session between a user of your website\n        and your website.\n        :return:\n        \"\"\"\n        return self.session_class(\n            client_key=self.client_key,\n            client_secret=self.client_secret,\n            signature_method=self.signature_method,\n            signature_type=self.signature_type,\n            rsa_key=self.rsa_key,\n            client_class=self.client_class,\n            force_include_body=self.force_include_body,\n            blueprint=self,\n            base_url=self.base_url,\n            **self.kwargs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef authorized(self):\n        if self.redirect_url:\n            next_url = self.redirect_url\n        elif self.redirect_to:\n            next_url = url_for(self.redirect_to)\n        else:\n            next_url = \"/\"\n\n        try:\n            self.session.parse_authorization_response(request.url)\n        except TokenMissing as err:\n            message = err.args[0]\n            response = getattr(err, \"response\", None)\n            log.warning(\"OAuth 1 access token error: %s\", message)\n            oauth_error.send(self, message=message, response=response)\n            return redirect(next_url)\n\n        try:\n            token = self.session.fetch_access_token(\n                self.access_token_url, should_load_token=False\n            )\n        except ValueError as err:\n            # can't proceed with OAuth, have to just redirect to next_url\n            message = err.args[0]\n            response = getattr(err, \"response\", None)\n            log.warning(\"OAuth 1 access token error: %s\", message)\n            oauth_error.send(self, message=message, response=response)\n            return redirect(next_url)\n\n        results = oauth_authorized.send(self, token=token) or []\n        set_token = True\n        for func, ret in results:\n            if isinstance(ret, (Response, current_app.response_class)):\n                return ret\n            if ret == False:\n                set_token = False\n\n        if set_token:\n            self.token = token\n        return redirect(next_url)", "response": "This method is used to check if the user is authorized to access the provider."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmakes a blueprint for authenticating with JIRA using OAuth 1. This requires a consumer key and RSA key for the JIRA application link. You should either pass them to this constructor, or make sure that your Flask application config defines them, using the variables :envvar:`JIRA_OAUTH_CONSUMER_KEY` and :envvar:`JIRA_OAUTH_RSA_KEY`. Args: base_url (str): The base URL of your JIRA installation. For example, for Atlassian's hosted Cloud JIRA, the base_url would be ``https://jira.atlassian.com`` consumer_key (str): The consumer key for your Application Link on JIRA rsa_key (str or path): The RSA private key for your Application Link on JIRA. This can be the contents of the key as a string, or a path to the key file on disk. redirect_url (str): the URL to redirect to after the authentication dance is complete redirect_to (str): if ``redirect_url`` is not defined, the name of the view to redirect to after the authentication dance is complete. The actual URL will be determined by :func:`flask.url_for` login_url (str, optional): the URL path for the ``login`` view. Defaults to ``/jira`` authorized_url (str, optional): the URL path for the ``authorized`` view. Defaults to ``/jira/authorized``. session_class (class, optional): The class to use for creating a Requests session. Defaults to :class:`~flask_dance.contrib.jira.JsonOAuth1Session`. storage: A token storage class, or an instance of a token storage class, to use for this blueprint. Defaults to :class:`~flask_dance.consumer.storage.session.SessionStorage`. :rtype: :class:`~flask_dance.consumer.OAuth1ConsumerBlueprint` :returns: A :ref:`blueprint <flask:blueprints>` to attach to your Flask app.", "response": "def make_jira_blueprint(\n    base_url,\n    consumer_key=None,\n    rsa_key=None,\n    redirect_url=None,\n    redirect_to=None,\n    login_url=None,\n    authorized_url=None,\n    session_class=None,\n    storage=None,\n):\n    \"\"\"\n    Make a blueprint for authenticating with JIRA using OAuth 1. This requires\n    a consumer key and RSA key for the JIRA application link. You should either\n    pass them to this constructor, or make sure that your Flask application\n    config defines them, using the variables :envvar:`JIRA_OAUTH_CONSUMER_KEY`\n    and :envvar:`JIRA_OAUTH_RSA_KEY`.\n\n    Args:\n        base_url (str): The base URL of your JIRA installation. For example,\n            for Atlassian's hosted Cloud JIRA, the base_url would be\n            ``https://jira.atlassian.com``\n        consumer_key (str): The consumer key for your Application Link on JIRA\n        rsa_key (str or path): The RSA private key for your Application Link\n            on JIRA. This can be the contents of the key as a string, or a path\n            to the key file on disk.\n        redirect_url (str): the URL to redirect to after the authentication\n            dance is complete\n        redirect_to (str): if ``redirect_url`` is not defined, the name of the\n            view to redirect to after the authentication dance is complete.\n            The actual URL will be determined by :func:`flask.url_for`\n        login_url (str, optional): the URL path for the ``login`` view.\n            Defaults to ``/jira``\n        authorized_url (str, optional): the URL path for the ``authorized`` view.\n            Defaults to ``/jira/authorized``.\n        session_class (class, optional): The class to use for creating a\n            Requests session. Defaults to\n            :class:`~flask_dance.contrib.jira.JsonOAuth1Session`.\n        storage: A token storage class, or an instance of a token storage\n                class, to use for this blueprint. Defaults to\n                :class:`~flask_dance.consumer.storage.session.SessionStorage`.\n\n    :rtype: :class:`~flask_dance.consumer.OAuth1ConsumerBlueprint`\n    :returns: A :ref:`blueprint <flask:blueprints>` to attach to your Flask app.\n    \"\"\"\n    if rsa_key and os.path.isfile(rsa_key):\n        with open(rsa_key) as f:\n            rsa_key = f.read()\n    base_url = URLObject(base_url)\n\n    jira_bp = OAuth1ConsumerBlueprint(\n        \"jira\",\n        __name__,\n        client_key=consumer_key,\n        rsa_key=rsa_key,\n        signature_method=SIGNATURE_RSA,\n        base_url=base_url,\n        request_token_url=base_url.relative(\"plugins/servlet/oauth/request-token\"),\n        access_token_url=base_url.relative(\"plugins/servlet/oauth/access-token\"),\n        authorization_url=base_url.relative(\"plugins/servlet/oauth/authorize\"),\n        redirect_url=redirect_url,\n        redirect_to=redirect_to,\n        login_url=login_url,\n        authorized_url=authorized_url,\n        session_class=session_class or JsonOAuth1Session,\n        storage=storage,\n    )\n    jira_bp.from_config[\"client_key\"] = \"JIRA_OAUTH_CONSUMER_KEY\"\n    jira_bp.from_config[\"rsa_key\"] = \"JIRA_OAUTH_RSA_KEY\"\n\n    @jira_bp.before_app_request\n    def set_applocal_session():\n        ctx = stack.top\n        ctx.jira_oauth = jira_bp.session\n\n    return jira_bp"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a spotify blueprint for authenticating with Spotify using OAuth 2.", "response": "def make_spotify_blueprint(\n    client_id=None,\n    client_secret=None,\n    scope=None,\n    redirect_url=None,\n    redirect_to=None,\n    login_url=None,\n    authorized_url=None,\n    session_class=None,\n    storage=None,\n):\n    \"\"\"\n    Make a blueprint for authenticating with Spotify using OAuth 2. This requires\n    a client ID and client secret from Spotify. You should either pass them to\n    this constructor, or make sure that your Flask application config defines\n    them, using the variables :envvar:`SPOTIFY_OAUTH_CLIENT_ID` and\n    :envvar:`SPOTIFY_OAUTH_CLIENT_SECRET`.\n\n    Args:\n        client_id (str): The client ID for your application on Spotify.\n        client_secret (str): The client secret for your application on Spotify\n        scope (str, optional): comma-separated list of scopes for the OAuth token\n        redirect_url (str): the URL to redirect to after the authentication\n            dance is complete\n        redirect_to (str): if ``redirect_url`` is not defined, the name of the\n            view to redirect to after the authentication dance is complete.\n            The actual URL will be determined by :func:`flask.url_for`\n        login_url (str, optional): the URL path for the ``login`` view.\n            Defaults to ``/spotify``\n        authorized_url (str, optional): the URL path for the ``authorized`` view.\n            Defaults to ``/spotify/authorized``.\n        session_class (class, optional): The class to use for creating a\n            Requests session. Defaults to\n            :class:`~flask_dance.consumer.requests.OAuth2Session`.\n        storage: A token storage class, or an instance of a token storage\n                class, to use for this blueprint. Defaults to\n                :class:`~flask_dance.consumer.storage.session.SessionStorage`.\n\n    :rtype: :class:`~flask_dance.consumer.OAuth2ConsumerBlueprint`\n    :returns: A :ref:`blueprint <flask:blueprints>` to attach to your Flask app.\n    \"\"\"\n    spotify_bp = OAuth2ConsumerBlueprint(\n        \"spotify\",\n        __name__,\n        client_id=client_id,\n        client_secret=client_secret,\n        scope=scope,\n        base_url=\"https://api.spotify.com\",\n        authorization_url=\"https://accounts.spotify.com/authorize\",\n        token_url=\"https://accounts.spotify.com/api/token\",\n        redirect_url=redirect_url,\n        redirect_to=redirect_to,\n        login_url=login_url,\n        authorized_url=authorized_url,\n        session_class=session_class,\n        storage=storage,\n    )\n    spotify_bp.from_config[\"client_id\"] = \"SPOTIFY_OAUTH_CLIENT_ID\"\n    spotify_bp.from_config[\"client_secret\"] = \"SPOTIFY_OAUTH_CLIENT_SECRET\"\n\n    @spotify_bp.before_app_request\n    def set_applocal_session():\n        ctx = stack.top\n        ctx.spotify_oauth = spotify_bp.session\n\n    return spotify_bp"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a blueprint for authenticating with Okta using OAuth 2.", "response": "def make_okta_blueprint(\n    client_id=None,\n    client_secret=None,\n    base_url=None,\n    scope=None,\n    redirect_url=None,\n    token_url=None,\n    redirect_to=None,\n    login_url=None,\n    authorization_url=None,\n    session_class=None,\n    storage=None,\n):\n    \"\"\"\n    Make a blueprint for authenticating with Okta using OAuth 2. This requires\n    a client ID and client secret from OKta. You should either pass them to\n    this constructor, or make sure that your Flask application config defines\n    them, using the variables :envvar:`OKTA_OAUTH_CLIENT_ID` and\n    :envvar:`OKTA_OAUTH_CLIENT_SECRET`.\n\n    Args:\n        client_id (str): The client ID for your application on Okta.\n        client_secret (str): The client secret for your application on Okta\n        scope (list, optional): list of scopes (str) for the OAuth token\n        redirect_url (str): the URL to redirect to after the authentication\n            dance is complete\n        redirect_to (str): if ``redirect_url`` is not defined, the name of the\n            view to redirect to after the authentication dance is complete.\n            The actual URL will be determined by :func:`flask.url_for`\n        login_url (str, optional): the URL path for the ``login`` view.\n            Defaults to ``/okta``\n        authorized_url (str, optional): the URL path for the ``authorized`` view.\n            Defaults to ``/okta/authorized``.\n        session_class (class, optional): The class to use for creating a\n            Requests session. Defaults to\n            :class:`~flask_dance.consumer.requests.OAuth2Session`.\n        storage: A token storage class, or an instance of a token storage\n                class, to use for this blueprint. Defaults to\n                :class:`~flask_dance.consumer.storage.session.SessionStorage`.\n\n    :rtype: :class:`~flask_dance.consumer.OAuth2ConsumerBlueprint`\n    :returns: A :ref:`blueprint <flask:blueprints>` to attach to your Flask app.\n    \"\"\"\n    scope = scope or [\"openid\", \"email\", \"profile\"]\n    okta_bp = OAuth2ConsumerBlueprint(\n        \"okta\",\n        __name__,\n        client_id=client_id,\n        client_secret=client_secret,\n        scope=scope,\n        base_url=base_url,\n        token_url=token_url,\n        authorization_url=authorization_url,\n        redirect_url=redirect_url,\n        redirect_to=redirect_to,\n        login_url=login_url,\n        session_class=session_class,\n        storage=storage,\n    )\n    okta_bp.from_config[\"client_id\"] = \"OKTA_OAUTH_CLIENT_ID\"\n    okta_bp.from_config[\"client_secret\"] = \"OKTA_OAUTH_CLIENT_SECRET\"\n\n    @okta_bp.before_app_request\n    def set_applocal_session():\n        ctx = stack.top\n        ctx.okta_oauth = okta_bp.session\n\n    return okta_bp"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a GitHub blueprint for authenticating with GitHub using OAuth 2.", "response": "def make_github_blueprint(\n    client_id=None,\n    client_secret=None,\n    scope=None,\n    redirect_url=None,\n    redirect_to=None,\n    login_url=None,\n    authorized_url=None,\n    session_class=None,\n    storage=None,\n):\n    \"\"\"\n    Make a blueprint for authenticating with GitHub using OAuth 2. This requires\n    a client ID and client secret from GitHub. You should either pass them to\n    this constructor, or make sure that your Flask application config defines\n    them, using the variables :envvar:`GITHUB_OAUTH_CLIENT_ID` and\n    :envvar:`GITHUB_OAUTH_CLIENT_SECRET`.\n\n    Args:\n        client_id (str): The client ID for your application on GitHub.\n        client_secret (str): The client secret for your application on GitHub\n        scope (str, optional): comma-separated list of scopes for the OAuth token\n        redirect_url (str): the URL to redirect to after the authentication\n            dance is complete\n        redirect_to (str): if ``redirect_url`` is not defined, the name of the\n            view to redirect to after the authentication dance is complete.\n            The actual URL will be determined by :func:`flask.url_for`\n        login_url (str, optional): the URL path for the ``login`` view.\n            Defaults to ``/github``\n        authorized_url (str, optional): the URL path for the ``authorized`` view.\n            Defaults to ``/github/authorized``.\n        session_class (class, optional): The class to use for creating a\n            Requests session. Defaults to\n            :class:`~flask_dance.consumer.requests.OAuth2Session`.\n        storage: A token storage class, or an instance of a token storage\n                class, to use for this blueprint. Defaults to\n                :class:`~flask_dance.consumer.storage.session.SessionStorage`.\n\n    :rtype: :class:`~flask_dance.consumer.OAuth2ConsumerBlueprint`\n    :returns: A :ref:`blueprint <flask:blueprints>` to attach to your Flask app.\n    \"\"\"\n    github_bp = OAuth2ConsumerBlueprint(\n        \"github\",\n        __name__,\n        client_id=client_id,\n        client_secret=client_secret,\n        scope=scope,\n        base_url=\"https://api.github.com/\",\n        authorization_url=\"https://github.com/login/oauth/authorize\",\n        token_url=\"https://github.com/login/oauth/access_token\",\n        redirect_url=redirect_url,\n        redirect_to=redirect_to,\n        login_url=login_url,\n        authorized_url=authorized_url,\n        session_class=session_class,\n        storage=storage,\n    )\n    github_bp.from_config[\"client_id\"] = \"GITHUB_OAUTH_CLIENT_ID\"\n    github_bp.from_config[\"client_secret\"] = \"GITHUB_OAUTH_CLIENT_SECRET\"\n\n    @github_bp.before_app_request\n    def set_applocal_session():\n        ctx = stack.top\n        ctx.github_oauth = github_bp.session\n\n    return github_bp"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a Zoho blueprint for authenticating with Zoho.", "response": "def make_zoho_blueprint(\n    client_id=None,\n    client_secret=None,\n    scope=None,\n    redirect_url=None,\n    offline=False,\n    redirect_to=None,\n    login_url=None,\n    session_class=None,\n    storage=None,\n    reprompt_consent=False,\n):\n    \"\"\"\n    Make a blueprint for authenticating with Zoho using OAuth 2. This requires\n    a client ID and client secret from Zoho. You should either pass them to\n    this constructor, or make sure that your Flask application config defines\n    them, using the variables :envvar:`ZOHO_OAUTH_CLIENT_ID` and\n    :envvar:`ZOHO_OAUTH_CLIENT_SECRET`.\n    IMPORTANT: Configuring the base_url is not supported in this config.\n\n    Args:\n        client_id (str): The client ID for your application on Zoho.\n        client_secret (str): The client secret for your application on Zoho\n        scope (list, optional): list of scopes (str) for the OAuth token\n        redirect_url (str): the URL to redirect to after the authentication\n            dance is complete\n        redirect_to (str): if ``redirect_url`` is not defined, the name of the\n            view to redirect to after the authentication dance is complete.\n            The actual URL will be determined by :func:`flask.url_for`\n        login_url (str, optional): the URL path for the ``login`` view.\n            Defaults to ``/zoho``\n        authorized_url (str, optional): the URL path for the ``authorized`` view.\n            Defaults to ``/zoho/authorized``.\n        session_class (class, optional): The class to use for creating a\n            Requests session. Defaults to\n            :class:`~flask_dance.consumer.requests.OAuth2Session`.\n        storage: A token storage class, or an instance of a token storage\n                class, to use for this blueprint. Defaults to\n                :class:`~flask_dance.consumer.storage.session.SessionStorage`.\n        offline (bool): Whether to request `offline access`\n            for the OAuth token. Defaults to False\n        reprompt_consent (bool): If True, force Zoho to re-prompt the user\n            for their consent, even if the user has already given their\n            consent. Defaults to False\n    :rtype: :class:`~flask_dance.consumer.OAuth2ConsumerBlueprint`\n    :returns: A :ref:`blueprint <flask:blueprints>` to attach to your Flask app.\n    \"\"\"\n    scope = scope or [\"ZohoCRM.users.all\"]\n    base_url = \"https://www.zohoapis.com/\"\n    client = ZohoWebClient(client_id, token_type=ZOHO_TOKEN_HEADER)\n    authorization_url_params = {}\n    authorization_url_params[\"access_type\"] = \"offline\" if offline else \"online\"\n    if reprompt_consent:\n        authorization_url_params[\"prompt\"] = \"consent\"\n    zoho_bp = OAuth2ConsumerBlueprint(\n        \"zoho\",\n        __name__,\n        client_id=client_id,\n        client_secret=client_secret,\n        client=client,\n        scope=scope,\n        base_url=base_url,\n        token_url=\"https://accounts.zoho.com/oauth/v2/token\",\n        authorization_url=\"https://accounts.zoho.com/oauth/v2/auth\",\n        authorization_url_params=authorization_url_params,\n        redirect_url=redirect_url,\n        redirect_to=redirect_to,\n        login_url=login_url,\n        session_class=session_class,\n        storage=storage,\n    )\n\n    zoho_bp.from_config[\"client_id\"] = \"ZOHO_OAUTH_CLIENT_ID\"\n    zoho_bp.from_config[\"client_secret\"] = \"ZOHO_OAUTH_CLIENT_SECRET\"\n\n    @zoho_bp.before_app_request\n    def set_applocal_session():\n        ctx = stack.top\n        ctx.zoho_oauth = zoho_bp.session\n\n    return zoho_bp"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a zoho token to the request uri body or authorization header. follows bearer pattern", "response": "def _add_zoho_token(\n        self, uri, http_method=\"GET\", body=None, headers=None, token_placement=None\n    ):\n        \"\"\"Add a zoho token to the request uri, body or authorization header. follows bearer pattern\"\"\"\n        headers = self.prepare_zoho_headers(self.access_token, headers)\n        return uri, headers, body"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a Zoho Token to the request URI.", "response": "def prepare_zoho_headers(token, headers=None):\n        \"\"\"Add a `Zoho Token`_ to the request URI.\n        Recommended method of passing bearer tokens.\n\n        Authorization: Zoho-oauthtoken h480djs93hd8\n\n        .. _`Zoho-oauthtoken Token`: custom zoho token\n        \"\"\"\n        headers = headers or {}\n        headers[\"Authorization\"] = \"{token_header} {token}\".format(\n            token_header=ZOHO_TOKEN_HEADER, token=token\n        )\n        return headers"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef authorized(self):\n        if self.redirect_url:\n            next_url = self.redirect_url\n        elif self.redirect_to:\n            next_url = url_for(self.redirect_to)\n        else:\n            next_url = \"/\"\n        log.debug(\"next_url = %s\", next_url)\n\n        # check for error in request args\n        error = request.args.get(\"error\")\n        if error:\n            error_desc = request.args.get(\"error_description\")\n            error_uri = request.args.get(\"error_uri\")\n            log.warning(\n                \"OAuth 2 authorization error: %s description: %s uri: %s\",\n                error,\n                error_desc,\n                error_uri,\n            )\n            oauth_error.send(\n                self, error=error, error_description=error_desc, error_uri=error_uri\n            )\n            return redirect(next_url)\n\n        state_key = \"{bp.name}_oauth_state\".format(bp=self)\n        if state_key not in flask.session:\n            # can't validate state, so redirect back to login view\n            log.info(\"state not found, redirecting user to login\")\n            return redirect(url_for(\".login\"))\n\n        state = flask.session[state_key]\n        log.debug(\"state = %s\", state)\n        self.session._state = state\n        del flask.session[state_key]\n\n        self.session.redirect_uri = url_for(\".authorized\", _external=True)\n\n        log.debug(\"client_id = %s\", self.client_id)\n        log.debug(\"client_secret = %s\", self.client_secret)\n        try:\n            token = self.session.fetch_token(\n                self.token_url,\n                authorization_response=request.url,\n                client_secret=self.client_secret,\n                **self.token_url_params\n            )\n        except MissingCodeError as e:\n            e.args = (\n                e.args[0],\n                \"The redirect request did not contain the expected parameters. Instead I got: {}\".format(\n                    json.dumps(request.args)\n                ),\n            )\n            raise\n\n        results = oauth_authorized.send(self, token=token) or []\n        set_token = True\n        for func, ret in results:\n            if isinstance(ret, (Response, current_app.response_class)):\n                return ret\n            if ret == False:\n                set_token = False\n\n        if set_token:\n            try:\n                self.token = token\n            except ValueError as error:\n                log.warning(\"OAuth 2 authorization error: %s\", str(error))\n                oauth_error.send(self, error=error)\n        return redirect(next_url)", "response": "This function is called by the application to check if the user is authorized to access the provider s website."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a GitLab Blueprint for authenticating with GitLab using OAuth 2.", "response": "def make_gitlab_blueprint(\n    client_id=None,\n    client_secret=None,\n    scope=None,\n    redirect_url=None,\n    redirect_to=None,\n    login_url=None,\n    authorized_url=None,\n    session_class=None,\n    storage=None,\n    hostname=\"gitlab.com\",\n):\n    \"\"\"\n    Make a blueprint for authenticating with GitLab using OAuth 2. This requires\n    a client ID and client secret from GitLab. You should either pass them to\n    this constructor, or make sure that your Flask application config defines\n    them, using the variables :envvar:`GITLAB_OAUTH_CLIENT_ID` and\n    :envvar:`GITLAB_OAUTH_CLIENT_SECRET`.\n\n    Args:\n        client_id (str): The client ID for your application on GitLab.\n        client_secret (str): The client secret for your application on GitLab\n        scope (str, optional): comma-separated list of scopes for the OAuth token\n        redirect_url (str): the URL to redirect to after the authentication\n            dance is complete\n        redirect_to (str): if ``redirect_url`` is not defined, the name of the\n            view to redirect to after the authentication dance is complete.\n            The actual URL will be determined by :func:`flask.url_for`\n        login_url (str, optional): the URL path for the ``login`` view.\n            Defaults to ``/gitlab``\n        authorized_url (str, optional): the URL path for the ``authorized`` view.\n            Defaults to ``/gitlab/authorized``.\n        session_class (class, optional): The class to use for creating a\n            Requests session. Defaults to\n            :class:`~flask_dance.consumer.requests.OAuth2Session`.\n        storage: A token storage class, or an instance of a token storage\n                class, to use for this blueprint. Defaults to\n                :class:`~flask_dance.consumer.storage.session.SessionStorage`.\n        hostname (str, optional): If using a private instance of GitLab CE/EE,\n            specify the hostname, default is ``gitlab.com``\n\n    :rtype: :class:`~flask_dance.consumer.OAuth2ConsumerBlueprint`\n    :returns: A :ref:`blueprint <flask:blueprints>` to attach to your Flask app.\n    \"\"\"\n    gitlab_bp = OAuth2ConsumerBlueprint(\n        \"gitlab\",\n        __name__,\n        client_id=client_id,\n        client_secret=client_secret,\n        scope=scope,\n        base_url=\"https://{hostname}/api/v4/\".format(hostname=hostname),\n        authorization_url=\"https://{hostname}/oauth/authorize\".format(\n            hostname=hostname\n        ),\n        token_url=\"https://{hostname}/oauth/token\".format(hostname=hostname),\n        redirect_url=redirect_url,\n        redirect_to=redirect_to,\n        login_url=login_url,\n        authorized_url=authorized_url,\n        session_class=session_class,\n        storage=storage,\n    )\n    gitlab_bp.from_config[\"client_id\"] = \"GITLAB_OAUTH_CLIENT_ID\"\n    gitlab_bp.from_config[\"client_secret\"] = \"GITLAB_OAUTH_CLIENT_SECRET\"\n\n    @gitlab_bp.before_app_request\n    def set_applocal_session():\n        ctx = stack.top\n        ctx.gitlab_oauth = gitlab_bp.session\n\n    return gitlab_bp"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_nylas_blueprint(\n    client_id=None,\n    client_secret=None,\n    scope=\"email\",\n    redirect_url=None,\n    redirect_to=None,\n    login_url=None,\n    authorized_url=None,\n    session_class=None,\n    storage=None,\n):\n    \"\"\"\n    Make a blueprint for authenticating with Nylas using OAuth 2. This requires\n    an API ID and API secret from Nylas. You should either pass them to\n    this constructor, or make sure that your Flask application config defines\n    them, using the variables :envvar:`NYLAS_OAUTH_CLIENT_ID` and\n    :envvar:`NYLAS_OAUTH_CLIENT_SECRET`.\n\n    Args:\n        client_id (str): The client ID for your developer account on Nylas.\n        client_secret (str): The client secret for your developer account\n            on Nylas.\n        scope (str, optional): comma-separated list of scopes for the OAuth\n            token. Defaults to \"email\".\n        redirect_url (str): the URL to redirect to after the authentication\n            dance is complete\n        redirect_to (str): if ``redirect_url`` is not defined, the name of the\n            view to redirect to after the authentication dance is complete.\n            The actual URL will be determined by :func:`flask.url_for`\n        login_url (str, optional): the URL path for the ``login`` view.\n            Defaults to ``/nylas``\n        authorized_url (str, optional): the URL path for the ``authorized`` view.\n            Defaults to ``/nylas/authorized``.\n        session_class (class, optional): The class to use for creating a\n            Requests session. Defaults to\n            :class:`~flask_dance.consumer.requests.OAuth2Session`.\n        storage: A token storage class, or an instance of a token storage\n                class, to use for this blueprint. Defaults to\n                :class:`~flask_dance.consumer.storage.session.SessionStorage`.\n\n    :rtype: :class:`~flask_dance.consumer.OAuth2ConsumerBlueprint`\n    :returns: A :ref:`blueprint <flask:blueprints>` to attach to your Flask app.\n    \"\"\"\n    nylas_bp = OAuth2ConsumerBlueprint(\n        \"nylas\",\n        __name__,\n        client_id=client_id,\n        client_secret=client_secret,\n        scope=scope,\n        base_url=\"https://api.nylas.com/\",\n        authorization_url=\"https://api.nylas.com/oauth/authorize\",\n        token_url=\"https://api.nylas.com/oauth/token\",\n        redirect_url=redirect_url,\n        redirect_to=redirect_to,\n        login_url=login_url,\n        authorized_url=authorized_url,\n        session_class=session_class,\n        storage=storage,\n    )\n    nylas_bp.from_config[\"client_id\"] = \"NYLAS_OAUTH_CLIENT_ID\"\n    nylas_bp.from_config[\"client_secret\"] = \"NYLAS_OAUTH_CLIENT_SECRET\"\n\n    @nylas_bp.before_app_request\n    def set_applocal_session():\n        ctx = stack.top\n        ctx.nylas_oauth = nylas_bp.session\n\n    return nylas_bp", "response": "Creates a Nylas OAuth 2 Consumer Blueprint."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_facebook_blueprint(\n    client_id=None,\n    client_secret=None,\n    scope=None,\n    redirect_url=None,\n    redirect_to=None,\n    login_url=None,\n    authorized_url=None,\n    rerequest_declined_permissions=False,\n    session_class=None,\n    storage=None,\n):\n    \"\"\"\n    Make a blueprint for authenticating with Facebook using OAuth 2. This requires\n    a client ID and client secret from Facebook. You should either pass them to\n    this constructor, or make sure that your Flask application config defines\n    them, using the variables :envvar:`FACEBOOK_OAUTH_CLIENT_ID` and\n    :envvar:`FACEBOOK_OAUTH_CLIENT_SECRET`.\n\n    Args:\n        client_id (str): The client ID for your application on Facebook.\n        client_secret (str): The client secret for your application on Facebook\n        scope (str, optional): comma-separated list of scopes for the OAuth token\n        redirect_url (str): the URL to redirect to after the authentication\n            dance is complete\n        redirect_to (str): if ``redirect_url`` is not defined, the name of the\n            view to redirect to after the authentication dance is complete.\n            The actual URL will be determined by :func:`flask.url_for`\n        login_url (str, optional): the URL path for the ``login`` view.\n            Defaults to ``/facebook``\n        authorized_url (str, optional): the URL path for the ``authorized`` view.\n            Defaults to ``/facebook/authorized``.\n        rerequest_declined_permissions (bool, optional): should the blueprint ask again for declined permissions.\n            Defaults to ``False``\n        session_class (class, optional): The class to use for creating a\n            Requests session. Defaults to\n            :class:`~flask_dance.consumer.requests.OAuth2Session`.\n        storage: A token storage class, or an instance of a token storage\n                class, to use for this blueprint. Defaults to\n                :class:`~flask_dance.consumer.storage.session.SessionStorage`.\n\n    :rtype: :class:`~flask_dance.consumer.OAuth2ConsumerBlueprint`\n    :returns: A :ref:`blueprint <flask:blueprints>` to attach to your Flask app.\n    \"\"\"\n    authorization_url_params = {}\n    if rerequest_declined_permissions:\n        authorization_url_params[\"auth_type\"] = \"rerequest\"\n    facebook_bp = OAuth2ConsumerBlueprint(\n        \"facebook\",\n        __name__,\n        client_id=client_id,\n        client_secret=client_secret,\n        scope=scope,\n        base_url=\"https://graph.facebook.com/\",\n        authorization_url=\"https://www.facebook.com/dialog/oauth\",\n        authorization_url_params=authorization_url_params,\n        token_url=\"https://graph.facebook.com/oauth/access_token\",\n        redirect_url=redirect_url,\n        redirect_to=redirect_to,\n        login_url=login_url,\n        authorized_url=authorized_url,\n        session_class=session_class,\n        storage=storage,\n    )\n    facebook_bp.from_config[\"client_id\"] = \"FACEBOOK_OAUTH_CLIENT_ID\"\n    facebook_bp.from_config[\"client_secret\"] = \"FACEBOOK_OAUTH_CLIENT_SECRET\"\n\n    @facebook_bp.before_app_request\n    def set_applocal_session():\n        ctx = stack.top\n        ctx.facebook_oauth = facebook_bp.session\n\n    return facebook_bp", "response": "Creates a Facebook blueprint for authenticating with Facebook using OAuth 2."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive a user object returns a real user object", "response": "def _get_real_user(user, anon_user=None):\n    \"\"\"\n    Given a \"user\" that could be:\n\n    * a real user object\n    * a function that returns a real user object\n    * a LocalProxy to a real user object (like Flask-Login's ``current_user``)\n\n    This function returns the real user object, regardless of which we have.\n    \"\"\"\n    if hasattr(user, \"_get_current_object\"):\n        # this is a proxy\n        user = user._get_current_object()\n    if callable(user):\n        # this is a function\n        user = user()\n    if anon_user and isinstance(user, anon_user):\n        return None\n    return user"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the OAuth token associated with the user.", "response": "def get(self, blueprint, user=None, user_id=None):\n        \"\"\" When you have a statement in your code that says\n        \"if <provider>.authorized:\" (for example \"if twitter.authorized:\"),\n        a long string of function calls result in this function being used to\n        check the Flask server's cache and database for any records associated\n        with the current_user. The `user` and `user_id` parameters are actually\n        not set in that case (see base.py:token(), that's what calls this\n        function), so the user information is instead loaded from the\n        current_user (if that's what you specified when you created the\n        blueprint) with blueprint.config.get('user_id').\n\n        :param blueprint:\n        :param user:\n        :param user_id:\n        :return:\n        \"\"\"\n        # check cache\n        cache_key = self.make_cache_key(blueprint=blueprint, user=user, user_id=user_id)\n        token = self.cache.get(cache_key)\n        if token:\n            return token\n\n        # if not cached, make database queries\n        query = self.session.query(self.model).filter_by(provider=blueprint.name)\n        uid = first([user_id, self.user_id, blueprint.config.get(\"user_id\")])\n        u = first(\n            _get_real_user(ref, self.anon_user)\n            for ref in (user, self.user, blueprint.config.get(\"user\"))\n        )\n\n        if self.user_required and not u and not uid:\n            raise ValueError(\"Cannot get OAuth token without an associated user\")\n\n        # check for user ID\n        if hasattr(self.model, \"user_id\") and uid:\n            query = query.filter_by(user_id=uid)\n        # check for user (relationship property)\n        elif hasattr(self.model, \"user\") and u:\n            query = query.filter_by(user=u)\n        # if we have the property, but not value, filter by None\n        elif hasattr(self.model, \"user_id\"):\n            query = query.filter_by(user_id=None)\n        # run query\n        try:\n            token = query.one().token\n        except NoResultFound:\n            token = None\n\n        # cache the result\n        self.cache.set(cache_key, token)\n\n        return token"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getattrd(obj, name, default=sentinel):\n    try:\n        return functools.reduce(getattr, name.split(\".\"), obj)\n    except AttributeError as e:\n        if default is not sentinel:\n            return default\n        raise", "response": "Get the value of a key from an object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef timestamp_from_datetime(dt):\n    dt = dt.replace(tzinfo=utc)\n    if hasattr(dt, \"timestamp\") and callable(dt.timestamp):\n        return dt.replace(tzinfo=utc).timestamp()\n    return (dt - datetime(1970, 1, 1, tzinfo=utc)).total_seconds()", "response": "Given a datetime in UTC return a float that represents the timestamp for\n    that datetime."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning an absolute value for a timedelta always representing a time distance.", "response": "def abs_timedelta(delta):\n    \"\"\"Returns an \"absolute\" value for a timedelta, always representing a\n    time distance.\"\"\"\n    if delta.days < 0:\n        now = _now()\n        return now - (now + delta)\n    return delta"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nturns a value into a date and a timedelta which represents how long ago it was. If that s not possible return None.", "response": "def date_and_delta(value):\n    \"\"\"Turn a value into a date and a timedelta which represents how long ago\n    it was.  If that's not possible, return (None, value).\"\"\"\n    now = _now()\n    if isinstance(value, datetime):\n        date = value\n        delta = now - value\n    elif isinstance(value, timedelta):\n        date = now - value\n        delta = value\n    else:\n        try:\n            value = int(value)\n            delta = timedelta(seconds=value)\n            date = now - delta\n        except (ValueError, TypeError):\n            return (None, value)\n    return date, abs_timedelta(delta)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngiving a timedelta or a number of seconds return a naturaltime representation of the amount of time elapsed.", "response": "def naturaldelta(value, months=True):\n    \"\"\"Given a timedelta or a number of seconds, return a natural\n    representation of the amount of time elapsed.  This is similar to\n    ``naturaltime``, but does not add tense to the result.  If ``months``\n    is True, then a number of months (based on 30.5 days) will be used\n    for fuzziness between years.\"\"\"\n    now = _now()\n    date, delta = date_and_delta(value)\n    if date is None:\n        return value\n\n    use_months = months\n\n    seconds = abs(delta.seconds)\n    days = abs(delta.days)\n    years = days // 365\n    days = days % 365\n    months = int(days // 30.5)\n\n    if not years and days < 1:\n        if seconds == 0:\n            return _(\"a moment\")\n        elif seconds == 1:\n            return _(\"a second\")\n        elif seconds < 60:\n            return ngettext(\"%d second\", \"%d seconds\", seconds) % seconds\n        elif 60 <= seconds < 120:\n            return _(\"a minute\")\n        elif 120 <= seconds < 3600:\n            minutes = seconds // 60\n            return ngettext(\"%d minute\", \"%d minutes\", minutes) % minutes\n        elif 3600 <= seconds < 3600 * 2:\n            return _(\"an hour\")\n        elif 3600 < seconds:\n            hours = seconds // 3600\n            return ngettext(\"%d hour\", \"%d hours\", hours) % hours\n    elif years == 0:\n        if days == 1:\n            return _(\"a day\")\n        if not use_months:\n            return ngettext(\"%d day\", \"%d days\", days) % days\n        else:\n            if not months:\n                return ngettext(\"%d day\", \"%d days\", days) % days\n            elif months == 1:\n                return _(\"a month\")\n            else:\n                return ngettext(\"%d month\", \"%d months\", months) % months\n    elif years == 1:\n        if not months and not days:\n            return _(\"a year\")\n        elif not months:\n            return ngettext(\"1 year, %d day\", \"1 year, %d days\", days) % days\n        elif use_months:\n            if months == 1:\n                return _(\"1 year, 1 month\")\n            else:\n                return ngettext(\"1 year, %d month\",\n                                \"1 year, %d months\", months) % months\n        else:\n            return ngettext(\"1 year, %d day\", \"1 year, %d days\", days) % days\n    else:\n        return ngettext(\"%d year\", \"%d years\", years) % years"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef naturaltime(value, future=False, months=True):\n    now = _now()\n    date, delta = date_and_delta(value)\n    if date is None:\n        return value\n    # determine tense by value only if datetime/timedelta were passed\n    if isinstance(value, (datetime, timedelta)):\n        future = date > now\n\n    ago = _('%s from now') if future else _('%s ago')\n    delta = naturaldelta(delta, months)\n\n    if delta == _(\"a moment\"):\n        return _(\"now\")\n\n    return ago % delta", "response": "Given a datetime or timedelta object return a natural representation of that time in a resolution that makes sense."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef naturalday(value, format='%b %d'):\n    try:\n        value = date(value.year, value.month, value.day)\n    except AttributeError:\n        # Passed value wasn't date-ish\n        return value\n    except (OverflowError, ValueError):\n        # Date arguments out of range\n        return value\n    delta = value - date.today()\n    if delta.days == 0:\n        return _('today')\n    elif delta.days == 1:\n        return _('tomorrow')\n    elif delta.days == -1:\n        return _('yesterday')\n    return value.strftime(format)", "response": "Returns a string representing the natural day of the given date. If the given value is out of date range returns a string containing the string according to format. Otherwise returns a string containing the string according to format."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef naturaldate(value):\n    try:\n        value = date(value.year, value.month, value.day)\n    except AttributeError:\n        # Passed value wasn't date-ish\n        return value\n    except (OverflowError, ValueError):\n        # Date arguments out of range\n        return value\n    delta = abs_timedelta(value - date.today())\n    if delta.days >= 365:\n        return naturalday(value, '%b %d %Y')\n    return naturalday(value)", "response": "Like naturalday but will append a year for dates that are a year\n    ago or more."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef activate(locale, path=None):\n    if path is None:\n        path = _DEFAULT_LOCALE_PATH\n    if locale not in _TRANSLATIONS:\n        translation = gettext_module.translation('humanize', path, [locale])\n        _TRANSLATIONS[locale] = translation\n    _CURRENT.locale = locale\n    return _TRANSLATIONS[locale]", "response": "Set 'locale' as current locale. Search for locale in directory 'path'\n    @param locale: language name, eg 'en_GB"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef naturalsize(value, binary=False, gnu=False, format='%.1f'):\n    if gnu: suffix = suffixes['gnu']\n    elif binary: suffix = suffixes['binary']\n    else: suffix = suffixes['decimal']\n\n    base = 1024 if (gnu or binary) else 1000\n    bytes = float(value)\n\n    if bytes == 1 and not gnu: return '1 Byte'\n    elif bytes < base and not gnu: return '%d Bytes' % bytes\n    elif bytes < base and gnu: return '%dB' % bytes\n\n    for i,s in enumerate(suffix):\n        unit = base ** (i+2)\n        if bytes < unit and not gnu:\n            return (format + ' %s') % ((base * bytes / unit), s)\n        elif bytes < unit and gnu:\n            return (format + '%s') % ((base * bytes / unit), s)\n    if gnu:\n        return (format + '%s') % ((base * bytes / unit), s)\n    return (format + ' %s') % ((base * bytes / unit), s)", "response": "Format a number of byteslike a human readable filesize."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts an integer to its ordinal as a string.", "response": "def ordinal(value):\n    \"\"\"Converts an integer to its ordinal as a string. 1 is '1st', 2 is '2nd',\n    3 is '3rd', etc. Works for any integer or anything int() will turn into an\n    integer.  Anything other value will have nothing done to it.\"\"\"\n    try:\n        value = int(value)\n    except (TypeError, ValueError):\n        return value\n    t = (P_('0', 'th'),\n         P_('1', 'st'),\n         P_('2', 'nd'),\n         P_('3', 'rd'),\n         P_('4', 'th'),\n         P_('5', 'th'),\n         P_('6', 'th'),\n         P_('7', 'th'),\n         P_('8', 'th'),\n         P_('9', 'th'))\n    if value % 100 in (11, 12, 13):  # special case\n        return \"%d%s\" % (value, t[0])\n    return '%d%s' % (value, t[value % 10])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef intcomma(value):\n    try:\n        if isinstance(value, compat.string_types):\n            float(value.replace(',', ''))\n        else:\n            float(value)\n    except (TypeError, ValueError):\n        return value\n    orig = str(value)\n    new = re.sub(\"^(-?\\d+)(\\d{3})\", '\\g<1>,\\g<2>', orig)\n    if orig == new:\n        return new\n    else:\n        return intcomma(new)", "response": "Converts an integer to a string containing commas every three digits."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef intword(value, format='%.1f'):\n    try:\n        value = int(value)\n    except (TypeError, ValueError):\n        return value\n\n    if value < powers[0]:\n        return str(value)\n    for ordinal, power in enumerate(powers[1:], 1):\n        if value < power:\n            chopped = value / float(powers[ordinal - 1])\n            return (' '.join([format, _(human_powers[ordinal - 1])])) % chopped\n    return str(value)", "response": "Converts a large integer to a friendly text representation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the number spelled out.", "response": "def apnumber(value):\n    \"\"\"For numbers 1-9, returns the number spelled out. Otherwise, returns the\n    number. This follows Associated Press style.  This always returns a string\n    unless the value was not int-able, unlike the Django filter.\"\"\"\n    try:\n        value = int(value)\n    except (TypeError, ValueError):\n        return value\n    if not 0 < value < 10:\n        return str(value)\n    return (_('one'), _('two'), _('three'), _('four'), _('five'), _('six'),\n            _('seven'), _('eight'), _('nine'))[value - 1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fractional(value):\n    '''\n    There will be some cases where one might not want to show\n        ugly decimal places for floats and decimals.\n    This function returns a human readable fractional number\n        in form of fractions and mixed fractions.\n    Pass in a string, or a number or a float, and this function returns\n        a string representation of a fraction\n        or whole number\n        or a mixed fraction\n    Examples:\n        fractional(0.3) will return '1/3'\n        fractional(1.3) will return '1 3/10'\n        fractional(float(1/3)) will return '1/3'\n        fractional(1) will return '1'\n    This will always return a string.\n    '''\n    try:\n        number = float(value)\n    except (TypeError, ValueError):\n        return value\n    wholeNumber = int(number)\n    frac = Fraction(number - wholeNumber).limit_denominator(1000)\n    numerator = frac._numerator\n    denominator = frac._denominator\n    if wholeNumber and not numerator and denominator == 1:\n        return '%.0f' % wholeNumber  # this means that an integer was passed in (or variants of that integer like 1.0000)\n    elif not wholeNumber:\n        return '%.0f/%.0f' % (numerator, denominator)\n    else:\n        return '%.0f %.0f/%.0f' % (wholeNumber, numerator, denominator)", "response": "Returns a human readable fractional number for the given value."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the top 10 keywords and their frequency scores", "response": "def keywords(text):\n    \"\"\"get the top 10 keywords and their frequency scores\n    ignores blacklisted words in stopWords,\n    counts the number of occurrences of each word\n    \"\"\"\n    text = split_words(text)\n    numWords = len(text)  # of words before removing blacklist words\n    freq = Counter(x for x in text if x not in stopWords)\n\n    minSize = min(10, len(freq))  # get first 10\n    keywords = {x: y for x, y in freq.most_common(minSize)}  # recreate a dict\n\n    for k in keywords:\n        articleScore = keywords[k]*1.0 / numWords\n        keywords[k] = articleScore * 1.5 + 1\n\n    return keywords"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsplits a text into a list of sentences.", "response": "def split_sentences(text):\n    '''\n    The regular expression matches all sentence ending punctuation and splits the string at those points.\n    At this point in the code, the list looks like this [\"Hello, world\", \"!\" ... ]. The punctuation and all quotation marks\n    are separated from the actual text. The first s_iter line turns each group of two items in the list into a tuple,\n    excluding the last item in the list (the last item in the list does not need to have this performed on it). Then,\n    the second s_iter line combines each tuple in the list into a single item and removes any whitespace at the beginning\n    of the line. Now, the s_iter list is formatted correctly but it is missing the last item of the sentences list. The\n    second to last line adds this item to the s_iter list and the last line returns the full list.\n    '''\n    \n    sentences = regex_split(u'(?<![A-Z\u0410-\u042f\u0401])([.!?]\"?)(?=\\s+\\\"?[A-Z\u0410-\u042f\u0401])', text, flags=REGEX_UNICODE)\n    s_iter = zip(*[iter(sentences[:-1])] * 2)\n    s_iter = [''.join(map(unicode,y)).lstrip() for y in s_iter]\n    s_iter.append(sentences[-1])\n    return s_iter"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sentence_position(i, size):\n\n    normalized = i*1.0 / size\n    if 0 < normalized <= 0.1:\n        return 0.17\n    elif 0.1 < normalized <= 0.2:\n        return 0.23\n    elif 0.2 < normalized <= 0.3:\n        return 0.14\n    elif 0.3 < normalized <= 0.4:\n        return 0.08\n    elif 0.4 < normalized <= 0.5:\n        return 0.05\n    elif 0.5 < normalized <= 0.6:\n        return 0.04\n    elif 0.6 < normalized <= 0.7:\n        return 0.06\n    elif 0.7 < normalized <= 0.8:\n        return 0.04\n    elif 0.8 < normalized <= 0.9:\n        return 0.04\n    elif 0.9 < normalized <= 1.0:\n        return 0.15\n    else:\n        return 0", "response": "return the probability of being an important sentence i - > size"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_html(self, config, url):\n        if isinstance(url, unicode):\n            url = url.encode('utf-8')\n        \n        cookiejar = cookielib.LWPCookieJar()\n        opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cookiejar))\n        urllib2.install_opener(opener)\n\n        headers = {'User-agent': config.browser_user_agent}\n        request = urllib2.Request(url, headers=headers)\n\n        try:\n            result = urllib2.urlopen(request).read()\n        except:\n            return None\n\n        return result", "response": "\\ Returns the html content of the url."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fetch_images(self, images, depth_level):\n        image_results = {}\n        initial_area = float(0.0)\n        total_score = float(0.0)\n        cnt = float(1.0)\n        MIN_WIDTH = 50\n        for image in images[:30]:\n            src = self.parser.getAttribute(image, attr='src')\n            src = self.build_image_path(src)\n            local_image = self.get_local_image(src)\n            width = local_image.width\n            height = local_image.height\n            src = local_image.src\n            file_extension = local_image.file_extension\n\n            if file_extension != '.gif' or file_extension != 'NA':\n                if (depth_level >= 1 and local_image.width > 300) or depth_level < 1:\n                    if not self.is_banner_dimensions(width, height):\n                        if width > MIN_WIDTH:\n                            sequence_score = float(1.0 / cnt)\n                            area = float(width * height)\n                            total_score = float(0.0)\n\n                            if initial_area == 0:\n                                initial_area = area * float(1.48)\n                                total_score = 1\n                            else:\n                                area_difference = float(area / initial_area)\n                                total_score = sequence_score * area_difference\n\n                            image_results.update({local_image: total_score})\n                            cnt += 1\n                            cnt += 1\n        return image_results", "response": "fetch the images from the local disk and set the dimensions of the images to the maximal area and the score of the images that are smaller than the maximal area."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsplitting the title to best part possible", "response": "def split_title(self, title, splitter):\n        \"\"\"\\\n        Split the title to best part possible\n        \"\"\"\n        large_text_length = 0\n        large_text_index = 0\n        title_pieces = splitter.split(title)\n\n        # find the largest title piece\n        for i in range(len(title_pieces)):\n            current = title_pieces[i]\n            if len(current) > large_text_length:\n                large_text_length = len(current)\n                large_text_index = i\n\n        # replace content\n        title = title_pieces[large_text_index]\n        return TITLE_REPLACEMENTS.replaceAll(title).strip()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if the node is boostable.", "response": "def is_boostable(self, node):\n        \"\"\"\\\n        alot of times the first paragraph might be the caption under an image\n        so we'll want to make sure if we're going to boost a parent node that\n        it should be connected to other paragraphs,\n        at least for the first n paragraphs so we'll want to make sure that\n        the next sibling is a paragraph and has at\n        least some substatial weight to it\n        \"\"\"\n        para = \"p\"\n        steps_away = 0\n        minimum_stopword_count = 5\n        max_stepsaway_from_node = 3\n\n        nodes = self.walk_siblings(node)\n        for current_node in nodes:\n            # p\n            current_node_tag = self.parser.getTag(current_node)\n            if current_node_tag == para:\n                if steps_away >= max_stepsaway_from_node:\n                    return False\n                paraText = self.parser.getText(current_node)\n                word_stats = self.stopwords_class(language=self.language).get_stopword_count(paraText)\n                if word_stats.get_stopword_count() > minimum_stopword_count:\n                    return True\n                steps_away += 1\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef connection_made(self, transport):\n        self._transport = transport\n        self._raw_transport = transport\n        if isinstance(transport, asyncio.SubprocessTransport):\n            self._transport = transport.get_pipe_transport(0)", "response": "Used to signal asyncio. Protocol of a successful connection."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nusing to signal asyncio. Protocol of incoming data.", "response": "def data_received(self, data):\n        \"\"\"Used to signal `asyncio.Protocol` of incoming data.\"\"\"\n        if self._on_data:\n            self._on_data(data)\n            return\n        self._queued_data.append(data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nuses to signal asynchronous. SubprocessProtocol of incoming data.", "response": "def pipe_data_received(self, fd, data):\n        \"\"\"Used to signal `asyncio.SubprocessProtocol` of incoming data.\"\"\"\n        if fd == 2:  # stderr fd number\n            self._on_stderr(data)\n        elif self._on_data:\n            self._on_data(data)\n        else:\n            self._queued_data.append(data)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nliking traceback. format_exc but allow skipping the first frames.", "response": "def format_exc_skip(skip, limit=None):\n    \"\"\"Like traceback.format_exc but allow skipping the first frames.\"\"\"\n    etype, val, tb = sys.exc_info()\n    for i in range(skip):\n        tb = tb.tb_next\n    return (''.join(format_exception(etype, val, tb, limit))).rstrip()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsending a msgpack - rpc request to Nvim.", "response": "def request(self, method, args, response_cb):\n        \"\"\"Send a msgpack-rpc request to Nvim.\n\n        A msgpack-rpc with method `method` and argument `args` is sent to\n        Nvim. The `response_cb` function is called with when the response\n        is available.\n        \"\"\"\n        request_id = self._next_request_id\n        self._next_request_id = request_id + 1\n        self._msgpack_stream.send([0, request_id, method, args])\n        self._pending_requests[request_id] = response_cb"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run(self, request_cb, notification_cb):\n        self._request_cb = request_cb\n        self._notification_cb = notification_cb\n        self._msgpack_stream.run(self._on_message)\n        self._request_cb = None\n        self._notification_cb = None", "response": "Run the event loop to receive requests and notifications from Nvim."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef send(self, value, error=False):\n        if error:\n            resp = [1, self._request_id, value, None]\n        else:\n            resp = [1, self._request_id, None, value]\n        debug('sending response to request %d: %s', self._request_id, resp)\n        self._msgpack_stream.send(resp)", "response": "Send the response to the server."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef start_host(session=None):\n    plugins = []\n    for arg in sys.argv:\n        _, ext = os.path.splitext(arg)\n        if ext == '.py':\n            plugins.append(arg)\n        elif os.path.isdir(arg):\n            init = os.path.join(arg, '__init__.py')\n            if os.path.isfile(init):\n                plugins.append(arg)\n\n    # This is a special case to support the old workaround of\n    # adding an empty .py file to make a package directory\n    # visible, and it should be removed soon.\n    for path in list(plugins):\n        dup = path + \".py\"\n        if os.path.isdir(path) and dup in plugins:\n            plugins.remove(dup)\n\n    # Special case: the legacy scripthost receives a single relative filename\n    # while the rplugin host will receive absolute paths.\n    if plugins == [\"script_host.py\"]:\n        name = \"script\"\n    else:\n        name = \"rplugin\"\n\n    setup_logging(name)\n\n    if not session:\n        session = stdio_session()\n    nvim = Nvim.from_session(session)\n\n    if nvim.version.api_level < 1:\n        sys.stderr.write(\"This version of pynvim \"\n                         \"requires nvim 0.1.6 or later\")\n        sys.exit(1)\n\n    host = Host(nvim)\n    host.start(plugins)", "response": "Promote the current process into python plugin host for Nvim."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a nicer interface to create python api sessions.", "response": "def attach(session_type, address=None, port=None,\n           path=None, argv=None, decode=None):\n    \"\"\"Provide a nicer interface to create python api sessions.\n\n    Previous machinery to create python api sessions is still there. This only\n    creates a facade function to make things easier for the most usual cases.\n    Thus, instead of:\n        from pynvim import socket_session, Nvim\n        session = tcp_session(address=<address>, port=<port>)\n        nvim = Nvim.from_session(session)\n    You can now do:\n        from pynvim import attach\n        nvim = attach('tcp', address=<address>, port=<port>)\n    And also:\n        nvim = attach('socket', path=<path>)\n        nvim = attach('child', argv=<argv>)\n        nvim = attach('stdio')\n\n    When the session is not needed anymore, it is recommended to explicitly\n    close it:\n       nvim.close()\n    It is also possible to use the session as a context mangager:\n       with attach('socket', path=thepath) as nvim:\n           print(nvim.funcs.getpid())\n           print(nvim.current.line)\n    This will automatically close the session when you're done with it, or\n    when an error occured.\n\n\n    \"\"\"\n    session = (tcp_session(address, port) if session_type == 'tcp' else\n               socket_session(path) if session_type == 'socket' else\n               stdio_session() if session_type == 'stdio' else\n               child_session(argv) if session_type == 'child' else\n               None)\n\n    if not session:\n        raise Exception('Unknown session type \"%s\"' % session_type)\n\n    if decode is None:\n        decode = IS_PYTHON3\n\n    return Nvim.from_session(session).with_decode(decode)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef main(argv=sys.argv[1:]):\n    usage = 'usage: %prog [options] FILE\\n\\n' + __doc__\n    parser = OptionParser(usage)\n\n    # options\n    parser.add_option(\"-f\", \"--force\",\n                      action='store_true', default=False,\n                      help=\"make changes even if they cannot undone before saving the new file\")\n    parser.add_option(\"-m\", \"--min_level\",\n                      default='NONE',\n                      help=\"minimum level of logging statements to modify [default: no minimum]\")\n    parser.add_option(\"-M\", \"--max_level\",\n                      default='NONE',\n                      help=\"maximum level of logging statements to modify [default: no maximum]\")\n    parser.add_option(\"-o\", \"--output-file\",\n                      default=None,\n                      help=\"where to output the result [default: overwrite the input file]\")\n    parser.add_option(\"-r\", \"--restore\",\n                      action='store_true', default=False,\n                      help=\"restore logging statements previously commented out and replaced with pass statements\")\n    parser.add_option(\"-v\", \"--verbose\",\n                      action='store_true', default=False,\n                      help=\"print informational messages about changes made\")\n\n    (options, args) = parser.parse_args(argv)\n    if len(args) != 1:\n        parser.error(\"expected 1 argument but got %d arguments: %s\" % (len(args), ' '.join(args)))\n    input_fn = args[0]\n    if not options.output_file:\n        options.output_file = input_fn\n\n    # validate min/max level\n    LEVEL_CHOICES = LEVELS + ['NONE']\n    min_level_value = 0 if options.min_level == 'NONE' else get_level_value(options.min_level)\n    if options.min_level is None:\n        parser.error(\"min level must be an integer or one of these values: %s\" % ', '.join(LEVEL_CHOICES))\n    max_level_value = sys.maxint if options.max_level == 'NONE' else get_level_value(options.max_level)\n    if options.max_level is None:\n        parser.error(\"max level must be an integer or one of these values: %s\" % ', '.join(LEVEL_CHOICES))\n\n    if options.verbose:\n        logging.getLogger().setLevel(logging.INFO)\n\n    try:\n        return modify_logging(input_fn, options.output_file,\n                              min_level_value, max_level_value,\n                              options.restore, options.force)\n    except IOError as e:\n        logging.error(str(e))\n        return -1", "response": "Parse the command line comments and execute the main function."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncomment out the given list of lines and return them.", "response": "def comment_lines(lines):\n    \"\"\"Comment out the given list of lines and return them.  The hash mark will\n    be inserted before the first non-whitespace character on each line.\"\"\"\n    ret = []\n    for line in lines:\n        ws_prefix, rest, ignore = RE_LINE_SPLITTER_COMMENT.match(line).groups()\n        ret.append(ws_prefix + '#' + rest)\n    return ''.join(ret)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef first_arg_to_level_name(arg):\n    try:\n        return int(arg)\n    except ValueError:\n        arg = arg.upper()\n        for level in LEVELS:\n            if level in arg:\n                return level\n        return None", "response": "Decide what level the argument specifies and return it."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the value associated with a particular level name.", "response": "def get_level_value(level):\n    \"\"\"Returns the logging value associated with a particular level name.  The\n    argument must be present in LEVELS_DICT or be an integer constant.\n    Otherwise None will be returned.\"\"\"\n    try:\n        # integral constants also work: they are the level value\n        return int(level)\n    except ValueError:\n        try:\n            return LEVELS_DICT[level.upper()]\n        except KeyError:\n            logging.warning(\"level '%s' cannot be translated to a level value (not present in LEVELS_DICT)\" % level)\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndetermines the level of logging in a given logging statement.", "response": "def get_logging_level(logging_stmt, commented_out=False):\n    \"\"\"Determines the level of logging in a given logging statement.  The string\n    representing this level is returned.  False is returned if the method is\n    not a logging statement and thus has no level.  None is returned if a level\n    should have been found but wasn't.\"\"\"\n    regexp = RE_LOGGING_START_IN_COMMENT if commented_out else RE_LOGGING_START\n    ret = regexp.match(logging_stmt)\n    _, method_name, _, first_arg = ret.groups()\n    if method_name not in LOGGING_METHODS_OF_INTEREST:\n        logging.debug('skipping uninteresting logging call: %s' % method_name)\n        return False\n\n    if method_name != 'log':\n        return method_name\n\n    # if the method name did not specify the level, we must have a first_arg to extract the level from\n    if not first_arg:\n        logging.warning(\"logging.log statement found but we couldn't extract the first argument\")\n        return None\n\n    # extract the level of logging from the first argument to the log() call\n    level = first_arg_to_level_name(first_arg)\n    if level is None:\n        logging.warning(\"arg does not contain any known level '%s'\\n\" % first_arg)\n        return None\n    return level"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef level_is_between(level, min_level_value, max_level_value):\n    level_value = get_level_value(level)\n    if level_value is None:\n        # unknown level value\n        return False\n    return level_value >= min_level_value and level_value <= max_level_value", "response": "Returns True if level is between the specified min or max inclusive."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsplitting a list of lines into two elements.", "response": "def split_call(lines, open_paren_line=0):\n    \"\"\"Returns a 2-tuple where the first element is the list of lines from the\n    first open paren in lines to the matching closed paren.  The second element\n    is all remaining lines in a list.\"\"\"\n    num_open = 0\n    num_closed = 0\n    for i, line in enumerate(lines):\n        c = line.count('(')\n        num_open += c\n        if not c and i==open_paren_line:\n            raise Exception('Exception open parenthesis in line %d but there is not one there: %s' % (i, str(lines)))\n        num_closed += line.count(')')\n\n        if num_open == num_closed:\n            return (lines[:i+1], lines[i+1:])\n\n    print(''.join(lines))\n    raise Exception('parenthesis are mismatched (%d open, %d closed found)' % (num_open, num_closed))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmodifies logging statements in the specified file.", "response": "def modify_logging(input_fn, output_fn, min_level_value, max_level_value, restore, force):\n    \"\"\"Modifies logging statements in the specified file.\"\"\"\n    # read in all the lines\n    logging.info('reading in %s' % input_fn)\n    fh = open(input_fn, 'r')\n    lines = fh.readlines()\n    fh.close()\n    original_contents = ''.join(lines)\n\n    if restore:\n        forwards = restore_logging\n        backwards = disable_logging\n    else:\n        forwards = disable_logging\n        backwards = restore_logging\n\n    # apply the requested action\n    new_contents = forwards(lines, min_level_value, max_level_value)\n\n    # quietly check to see if we can undo what we just did (if not, the text\n    # contains something we cannot translate [bug or limitation with this code])\n    logging.disable(logging.CRITICAL)\n    new_contents_undone = backwards(new_contents.splitlines(True), min_level_value, max_level_value)\n    logging.disable(logging.DEBUG)\n    if original_contents != new_contents_undone:\n        base_str = 'We are unable to revert this action as expected'\n        if force:\n            logging.warning(base_str + \" but -f was specified so we'll do it anyway.\")\n        else:\n            logging.error(base_str + ', so we will not do it in the first place.  Pass -f to override this and make the change anyway.')\n            return -1\n\n    logging.info('writing the new contents to %s' % output_fn)\n    fh = open(output_fn, 'w')\n    fh.write(new_contents)\n    fh.close()\n    logging.info('done!')\n    return 0"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nextracting the level of the logging statement and returns True if the level falls betwen min and max_level_value.", "response": "def check_level(logging_stmt, logging_stmt_is_commented_out, min_level_value, max_level_value):\n    \"\"\"Extracts the level of the logging statement and returns True if the\n    level falls betwen min and max_level_value.  If the level cannot be\n    extracted, then a warning is logged.\"\"\"\n    level = get_logging_level(logging_stmt, logging_stmt_is_commented_out)\n    if level is None:\n        logging.warning('skipping logging statement because the level could not be extracted: %s' % logging_stmt.strip())\n        return False\n    elif level is False:\n        return False\n    elif level_is_between(level, min_level_value, max_level_value):\n        return True\n    else:\n        logging.debug('keep this one as is (not in the specified level range): %s' % logging_stmt.strip())\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef disable_logging(lines, min_level_value, max_level_value):\n    output = ''\n    while lines:\n        line = lines[0]\n        ret = RE_LOGGING_START.match(line)\n        if not ret:\n            # no logging statement here, so just leave the line as-is and keep going\n            output += line\n            lines = lines[1:]\n        else:\n            # a logging call has started: find all the lines it includes and those it does not\n            logging_lines, remaining_lines = split_call(lines)\n            lines = remaining_lines\n            logging_stmt = ''.join(logging_lines)\n\n            # replace the logging statement if its level falls b/w min and max\n            if not check_level(logging_stmt, False, min_level_value, max_level_value):\n                output += logging_stmt\n            else:\n                # comment out this logging statement and replace it with pass\n                prefix_ws = ret.group(1)\n                pass_stmt = prefix_ws + PASS_LINE_CONTENTS\n                commented_out_logging_lines = comment_lines(logging_lines)\n                new_lines = pass_stmt + commented_out_logging_lines\n                logging.info('replacing:\\n%s\\nwith this:\\n%s' % (logging_stmt.rstrip(), new_lines.rstrip()))\n                output += new_lines\n    return output", "response": "Disables logging statements in these lines whose logging level falls between the specified minimum and maximum levels."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconnects to tcp address and port. Delegated to _connect_tcp.", "response": "def connect_tcp(self, address, port):\n        \"\"\"Connect to tcp/ip `address`:`port`. Delegated to `_connect_tcp`.\"\"\"\n        info('Connecting to TCP address: %s:%d', address, port)\n        self._connect_tcp(address, port)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run(self, data_cb):\n        if self._error:\n            err = self._error\n            if isinstance(self._error, KeyboardInterrupt):\n                # KeyboardInterrupt is not destructive(it may be used in\n                # the REPL).\n                # After throwing KeyboardInterrupt, cleanup the _error field\n                # so the loop may be started again\n                self._error = None\n            raise err\n        self._on_data = data_cb\n        if threading.current_thread() == main_thread:\n            self._setup_signals([signal.SIGINT, signal.SIGTERM])\n        debug('Entering event loop')\n        self._run()\n        debug('Exited event loop')\n        if threading.current_thread() == main_thread:\n            self._teardown_signals()\n            signal.signal(signal.SIGINT, default_int_handler)\n        self._on_data = None", "response": "Run the event loop."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_async(async_, kwargs, default):\n    if async_ is not None:\n        return async_\n    elif 'async' in kwargs:\n        warnings.warn(\n            '\"async\" attribute is deprecated. Use \"async_\" instead.',\n            DeprecationWarning,\n        )\n        return kwargs.pop('async')\n    else:\n        return default", "response": "Return a value of async in kwargs or default when async_ is None."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntag a class as a plugin.", "response": "def plugin(cls):\n    \"\"\"Tag a class as a plugin.\n\n    This decorator is required to make the class methods discoverable by the\n    plugin_load method of the host.\n    \"\"\"\n    cls._nvim_plugin = True\n    # the _nvim_bind attribute is set to True by default, meaning that\n    # decorated functions have a bound Nvim instance as first argument.\n    # For methods in a plugin-decorated class this is not required, because\n    # the class initializer will already receive the nvim object.\n    predicate = lambda fn: hasattr(fn, '_nvim_bind')\n    for _, fn in inspect.getmembers(cls, predicate):\n        if IS_PYTHON3:\n            fn._nvim_bind = False\n        else:\n            fn.im_func._nvim_bind = False\n    return cls"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nexport a function or plugin method as a msgpack - rpc request handler.", "response": "def rpc_export(rpc_method_name, sync=False):\n    \"\"\"Export a function or plugin method as a msgpack-rpc request handler.\"\"\"\n    def dec(f):\n        f._nvim_rpc_method_name = rpc_method_name\n        f._nvim_rpc_sync = sync\n        f._nvim_bind = True\n        f._nvim_prefix_plugin_path = False\n        return f\n    return dec"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef command(name, nargs=0, complete=None, range=None, count=None, bang=False,\n            register=False, sync=False, allow_nested=False, eval=None):\n    \"\"\"Tag a function or plugin method as a Nvim command handler.\"\"\"\n    def dec(f):\n        f._nvim_rpc_method_name = 'command:{}'.format(name)\n        f._nvim_rpc_sync = sync\n        f._nvim_bind = True\n        f._nvim_prefix_plugin_path = True\n\n        opts = {}\n\n        if range is not None:\n            opts['range'] = '' if range is True else str(range)\n        elif count is not None:\n            opts['count'] = count\n\n        if bang:\n            opts['bang'] = ''\n\n        if register:\n            opts['register'] = ''\n\n        if nargs:\n            opts['nargs'] = nargs\n\n        if complete:\n            opts['complete'] = complete\n\n        if eval:\n            opts['eval'] = eval\n\n        if not sync and allow_nested:\n            rpc_sync = \"urgent\"\n        else:\n            rpc_sync = sync\n\n        f._nvim_rpc_spec = {\n            'type': 'command',\n            'name': name,\n            'sync': rpc_sync,\n            'opts': opts\n        }\n        return f\n    return dec", "response": "Tag a function or plugin method as a Nvim command handler."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntag a function or plugin method as a Nvim autocommand handler.", "response": "def autocmd(name, pattern='*', sync=False, allow_nested=False, eval=None):\n    \"\"\"Tag a function or plugin method as a Nvim autocommand handler.\"\"\"\n    def dec(f):\n        f._nvim_rpc_method_name = 'autocmd:{}:{}'.format(name, pattern)\n        f._nvim_rpc_sync = sync\n        f._nvim_bind = True\n        f._nvim_prefix_plugin_path = True\n\n        opts = {\n            'pattern': pattern\n        }\n\n        if eval:\n            opts['eval'] = eval\n\n        if not sync and allow_nested:\n            rpc_sync = \"urgent\"\n        else:\n            rpc_sync = sync\n\n        f._nvim_rpc_spec = {\n            'type': 'autocmd',\n            'name': name,\n            'sync': rpc_sync,\n            'opts': opts\n        }\n        return f\n    return dec"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef function(name, range=False, sync=False, allow_nested=False, eval=None):\n    def dec(f):\n        f._nvim_rpc_method_name = 'function:{}'.format(name)\n        f._nvim_rpc_sync = sync\n        f._nvim_bind = True\n        f._nvim_prefix_plugin_path = True\n\n        opts = {}\n\n        if range:\n            opts['range'] = '' if range is True else str(range)\n\n        if eval:\n            opts['eval'] = eval\n\n        if not sync and allow_nested:\n            rpc_sync = \"urgent\"\n        else:\n            rpc_sync = sync\n\n        f._nvim_rpc_spec = {\n            'type': 'function',\n            'name': name,\n            'sync': rpc_sync,\n            'opts': opts\n        }\n        return f\n    return dec", "response": "Tag a function or plugin method as a Nvim function handler."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef decode(mode=unicode_errors_default):\n    def dec(f):\n        f._nvim_decode = mode\n        return f\n    return dec", "response": "Configure automatic encoding of strings."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef encoding(encoding=True):\n    if isinstance(encoding, str):\n        encoding = True\n\n    def dec(f):\n        f._nvim_decode = encoding\n        return f\n    return dec", "response": "Deprecated use pynvim. decode"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new Nvim instance from a Session instance.", "response": "def from_session(cls, session):\n        \"\"\"Create a new Nvim instance for a Session instance.\n\n        This method must be called to create the first Nvim instance, since it\n        queries Nvim metadata for type information and sets a SessionHook for\n        creating specialized objects from Nvim remote handles.\n        \"\"\"\n        session.error_wrapper = lambda e: NvimError(e[1])\n        channel_id, metadata = session.request(b'vim_get_api_info')\n\n        if IS_PYTHON3:\n            # decode all metadata strings for python3\n            metadata = walk(decode_if_bytes, metadata)\n\n        types = {\n            metadata['types']['Buffer']['id']: Buffer,\n            metadata['types']['Window']['id']: Window,\n            metadata['types']['Tabpage']['id']: Tabpage,\n        }\n\n        return cls(session, channel_id, metadata, types)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_nvim(cls, nvim):\n        return cls(nvim._session, nvim.channel_id, nvim.metadata,\n                   nvim.types, nvim._decode, nvim._err_cb)", "response": "Create a new Nvim instance from an existing instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nblocking until a message is available.", "response": "def next_message(self):\n        \"\"\"Block until a message(request or notification) is available.\n\n        If any messages were previously enqueued, return the first in queue.\n        If not, run the event loop until one is received.\n        \"\"\"\n        msg = self._session.next_message()\n        if msg:\n            return walk(self._from_nvim, msg)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run_loop(self, request_cb, notification_cb,\n                 setup_cb=None, err_cb=None):\n        \"\"\"Run the event loop to receive requests and notifications from Nvim.\n\n        This should not be called from a plugin running in the host, which\n        already runs the loop and dispatches events to plugins.\n        \"\"\"\n        if err_cb is None:\n            err_cb = sys.stderr.write\n        self._err_cb = err_cb\n\n        def filter_request_cb(name, args):\n            name = self._from_nvim(name)\n            args = walk(self._from_nvim, args)\n            try:\n                result = request_cb(name, args)\n            except Exception:\n                msg = (\"error caught in request handler '{} {}'\\n{}\\n\\n\"\n                       .format(name, args, format_exc_skip(1)))\n                self._err_cb(msg)\n                raise\n            return walk(self._to_nvim, result)\n\n        def filter_notification_cb(name, args):\n            name = self._from_nvim(name)\n            args = walk(self._from_nvim, args)\n            try:\n                notification_cb(name, args)\n            except Exception:\n                msg = (\"error caught in notification handler '{} {}'\\n{}\\n\\n\"\n                       .format(name, args, format_exc_skip(1)))\n                self._err_cb(msg)\n                raise\n\n        self._session.run(filter_request_cb, filter_notification_cb, setup_cb)", "response": "Runs the event loop to receive requests and notifications from Nvim."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef with_decode(self, decode=True):\n        return Nvim(self._session, self.channel_id,\n                    self.metadata, self.types, decode, self._err_cb)", "response": "Initialize a new Nvim instance with decode = False."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nregister as a remote UI.", "response": "def ui_attach(self, width, height, rgb=None, **kwargs):\n        \"\"\"Register as a remote UI.\n\n        After this method is called, the client will receive redraw\n        notifications.\n        \"\"\"\n        options = kwargs\n        if rgb is not None:\n            options['rgb'] = rgb\n        return self.request('nvim_ui_attach', width, height, options)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncall a vimscript function.", "response": "def call(self, name, *args, **kwargs):\n        \"\"\"Call a vimscript function.\"\"\"\n        return self.request('nvim_call_function', name, args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexecutes lua code. Additional parameters are available as `...` inside the lua chunk. Only statements are executed. To evaluate an expression, prefix it with `return`: `return my_function(...)` There is a shorthand syntax to call lua functions with arguments: nvim.lua.func(1,2) nvim.lua.mymod.myfunction(data, async_=True) is equivalent to nvim.exec_lua(\"return func(...)\", 1, 2) nvim.exec_lua(\"mymod.myfunction(...)\", data, async_=True) Note that with `async_=True` there is no return value.", "response": "def exec_lua(self, code, *args, **kwargs):\n        \"\"\"Execute lua code.\n\n        Additional parameters are available as `...` inside the lua chunk.\n        Only statements are executed.  To evaluate an expression, prefix it\n        with `return`: `return my_function(...)`\n\n        There is a shorthand syntax to call lua functions with arguments:\n\n            nvim.lua.func(1,2)\n            nvim.lua.mymod.myfunction(data, async_=True)\n\n        is equivalent to\n\n            nvim.exec_lua(\"return func(...)\", 1, 2)\n            nvim.exec_lua(\"mymod.myfunction(...)\", data, async_=True)\n\n        Note that with `async_=True` there is no return value.\n        \"\"\"\n        return self.request('nvim_execute_lua', code, args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npushes keys to Nvim user input buffer.", "response": "def feedkeys(self, keys, options='', escape_csi=True):\n        \"\"\"Push `keys` to Nvim user input buffer.\n\n        Options can be a string with the following character flags:\n        - 'm': Remap keys. This is default.\n        - 'n': Do not remap keys.\n        - 't': Handle keys as if typed; otherwise they are handled as if coming\n               from a mapping. This matters for undo, opening folds, etc.\n        \"\"\"\n        return self.request('nvim_feedkeys', keys, options, escape_csi)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nscheduling fn to be called by the event loop soon.", "response": "def async_call(self, fn, *args, **kwargs):\n        \"\"\"Schedule `fn` to be called by the event loop soon.\n\n        This function is thread-safe, and is the only way code not\n        on the main thread could interact with nvim api objects.\n\n        This function can also be called in a synchronous\n        event handler, just before it returns, to defer execution\n        that shouldn't block neovim.\n        \"\"\"\n        call_point = ''.join(format_stack(None, 5)[:-1])\n\n        def handler():\n            try:\n                fn(*args, **kwargs)\n            except Exception as err:\n                msg = (\"error caught while executing async callback:\\n\"\n                       \"{!r}\\n{}\\n \\nthe call was requested at\\n{}\"\n                       .format(err, format_exc_skip(1), call_point))\n                self._err_cb(msg)\n                raise\n        self._session.threadsafe_call(handler)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef append(self, lines, index=-1):\n        if isinstance(lines, (basestring, bytes)):\n            lines = [lines]\n        return self.request('nvim_buf_set_lines', index, index, True, lines)", "response": "Append a string or list of lines to the buffer."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a highlight to the buffer.", "response": "def add_highlight(self, hl_group, line, col_start=0,\n                      col_end=-1, src_id=-1, async_=None,\n                      **kwargs):\n        \"\"\"Add a highlight to the buffer.\"\"\"\n        async_ = check_async(async_, kwargs, src_id != 0)\n        return self.request('nvim_buf_add_highlight', src_id, hl_group,\n                            line, col_start, col_end, async_=async_)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef clear_highlight(self, src_id, line_start=0, line_end=-1, async_=None,\n                        **kwargs):\n        \"\"\"Clear highlights from the buffer.\"\"\"\n        async_ = check_async(async_, kwargs, True)\n        self.request('nvim_buf_clear_highlight', src_id,\n                     line_start, line_end, async_=async_)", "response": "Clear highlights from the buffer."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_highlights(self, src_id, hls, clear_start=0, clear_end=-1,\n                          clear=False, async_=True):\n        \"\"\"Add or update highlights in batch to avoid unnecessary redraws.\n\n        A `src_id` must have been allocated prior to use of this function. Use\n        for instance `nvim.new_highlight_source()` to get a src_id for your\n        plugin.\n\n        `hls` should be a list of highlight items. Each item should be a list\n        or tuple on the form `(\"GroupName\", linenr, col_start, col_end)` or\n        `(\"GroupName\", linenr)` to highlight an entire line.\n\n        By default existing highlights are preserved. Specify a line range with\n        clear_start and clear_end to replace highlights in this range. As a\n        shorthand, use clear=True to clear the entire buffer before adding the\n        new highlights.\n        \"\"\"\n        if clear and clear_start is None:\n            clear_start = 0\n        lua = self._session._get_lua_private()\n        lua.update_highlights(self, src_id, hls, clear_start, clear_end,\n                              async_=async_)", "response": "Add or update highlights in batch to avoid unnecessary redraws."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstarting listening for msgpack - rpc requests and notifications.", "response": "def start(self, plugins):\n        \"\"\"Start listening for msgpack-rpc requests and notifications.\"\"\"\n        self.nvim.run_loop(self._on_request,\n                           self._on_notification,\n                           lambda: self._load(plugins),\n                           err_cb=self._on_async_err)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _on_request(self, name, args):\n        if IS_PYTHON3:\n            name = decode_if_bytes(name)\n        handler = self._request_handlers.get(name, None)\n        if not handler:\n            msg = self._missing_handler_error(name, 'request')\n            error(msg)\n            raise ErrorResponse(msg)\n\n        debug('calling request handler for \"%s\", args: \"%s\"', name, args)\n        rv = handler(*args)\n        debug(\"request handler for '%s %s' returns: %s\", name, args, rv)\n        return rv", "response": "Handle a msgpack - rpc request."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nhandling a msgpack - rpc notification.", "response": "def _on_notification(self, name, args):\n        \"\"\"Handle a msgpack-rpc notification.\"\"\"\n        if IS_PYTHON3:\n            name = decode_if_bytes(name)\n        handler = self._notification_handlers.get(name, None)\n        if not handler:\n            msg = self._missing_handler_error(name, 'notification')\n            error(msg)\n            self._on_async_err(msg + \"\\n\")\n            return\n\n        debug('calling notification handler for \"%s\", args: \"%s\"', name, args)\n        handler(*args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndecodes obj if it is bytes.", "response": "def decode_if_bytes(obj, mode=True):\n    \"\"\"Decode obj if it is bytes.\"\"\"\n    if mode is True:\n        mode = unicode_errors_default\n    if isinstance(obj, bytes):\n        return obj.decode(\"utf-8\", errors=mode)\n    return obj"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrap for nvim. request.", "response": "def request(self, name, *args, **kwargs):\n        \"\"\"Wrapper for nvim.request.\"\"\"\n        return self._session.request(name, self, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nqueue a message to Nvim.", "response": "def send(self, msg):\n        \"\"\"Queue `msg` for sending to Nvim.\"\"\"\n        debug('sent %s', msg)\n        self.loop.send(self._packer.pack(msg))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run(self, message_cb):\n        self._message_cb = message_cb\n        self.loop.run(self._on_data)\n        self._message_cb = None", "response": "Run the event loop to receive messages from Nvim."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef threadsafe_call(self, fn, *args, **kwargs):\n        def handler():\n            try:\n                fn(*args, **kwargs)\n            except Exception:\n                warn(\"error caught while excecuting async callback\\n%s\\n\",\n                     format_exc())\n\n        def greenlet_wrapper():\n            gr = greenlet.greenlet(handler)\n            gr.switch()\n\n        self._async_session.threadsafe_call(greenlet_wrapper)", "response": "Wrapper around AsyncSession. threadsafe_call."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nblocking until a message is available.", "response": "def next_message(self):\n        \"\"\"Block until a message(request or notification) is available.\n\n        If any messages were previously enqueued, return the first in queue.\n        If not, run the event loop until one is received.\n        \"\"\"\n        if self._is_running:\n            raise Exception('Event loop already running')\n        if self._pending_messages:\n            return self._pending_messages.popleft()\n        self._async_session.run(self._enqueue_request_and_stop,\n                                self._enqueue_notification_and_stop)\n        if self._pending_messages:\n            return self._pending_messages.popleft()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsends a msgpack - rpc request and block until a response is received.", "response": "def request(self, method, *args, **kwargs):\n        \"\"\"Send a msgpack-rpc request and block until as response is received.\n\n        If the event loop is running, this method must have been called by a\n        request or notification handler running on a greenlet. In that case,\n        send the quest and yield to the parent greenlet until a response is\n        available.\n\n        When the event loop is not running, it will perform a blocking request\n        like this:\n        - Send the request\n        - Run the loop until the response is available\n        - Put requests/notifications received while waiting into a queue\n\n        If the `async_` flag is present and True, a asynchronous notification\n        is sent instead. This will never block, and the return value or error\n        is ignored.\n        \"\"\"\n        async_ = check_async(kwargs.pop('async_', None), kwargs, False)\n        if async_:\n            self._async_session.notify(method, args)\n            return\n\n        if kwargs:\n            raise ValueError(\"request got unsupported keyword argument(s): {}\"\n                             .format(', '.join(kwargs.keys())))\n\n        if self._is_running:\n            v = self._yielding_request(method, args)\n        else:\n            v = self._blocking_request(method, args)\n        if not v:\n            # EOF\n            raise IOError('EOF')\n        err, rv = v\n        if err:\n            info(\"'Received error: %s\", err)\n            raise self.error_wrapper(err)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run(self, request_cb, notification_cb, setup_cb=None):\n        self._request_cb = request_cb\n        self._notification_cb = notification_cb\n        self._is_running = True\n        self._setup_exception = None\n        self._loop_thread = threading.current_thread()\n\n        def on_setup():\n            try:\n                setup_cb()\n            except Exception as e:\n                self._setup_exception = e\n                self.stop()\n\n        if setup_cb:\n            # Create a new greenlet to handle the setup function\n            gr = greenlet.greenlet(on_setup)\n            gr.switch()\n\n        if self._setup_exception:\n            error('Setup error: {}'.format(self._setup_exception))\n            raise self._setup_exception\n\n        # Process all pending requests and notifications\n        while self._pending_messages:\n            msg = self._pending_messages.popleft()\n            getattr(self, '_on_{}'.format(msg[0]))(*msg[1:])\n        self._async_session.run(self._on_request, self._on_notification)\n        self._is_running = False\n        self._request_cb = None\n        self._notification_cb = None\n        self._loop_thread = None\n\n        if self._setup_exception:\n            raise self._setup_exception", "response": "Run the event loop to receive requests and notifications from Nvim."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Clone(self):\n        return AccountState(self.ScriptHash, self.IsFrozen, self.Votes, self.Balances)", "response": "Returns a copy of the account state."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns an AccountState object from a replica.", "response": "def FromReplica(self, replica):\n        \"\"\"\n        Get AccountState object from a replica.\n        Args:\n            replica (obj): must have ScriptHash, IsFrozen, Votes and Balances members.\n\n        Returns:\n            AccountState:\n        \"\"\"\n        return AccountState(replica.ScriptHash, replica.IsFrozen, replica.Votes, replica.Balances)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef Size(self):\n        return super(AccountState, self).Size() + s.uint160 + s.uint8 + GetVarSize(self.Votes) + GetVarSize(len(self.Balances)) + (len(self.Balances) * (32 + 8))", "response": "Returns the size of the object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef Deserialize(self, reader):\n        super(AccountState, self).Deserialize(reader)\n        self.ScriptHash = reader.ReadUInt160()\n        self.IsFrozen = reader.ReadBool()\n        num_votes = reader.ReadVarInt()\n        for i in range(0, num_votes):\n            self.Votes.append(reader.ReadBytes(33))\n\n        num_balances = reader.ReadVarInt()\n        self.Balances = {}\n        for i in range(0, num_balances):\n            assetid = reader.ReadUInt256()\n            amount = reader.ReadFixed8()\n            self.Balances[assetid] = amount", "response": "Deserialize full object.\n\n        Args:\n            reader (neocore.IO.BinaryReader):"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nserialize full object. Args: writer (neo.IO.BinaryWriter):", "response": "def Serialize(self, writer):\n        \"\"\"\n        Serialize full object.\n\n        Args:\n            writer (neo.IO.BinaryWriter):\n        \"\"\"\n        super(AccountState, self).Serialize(writer)\n        writer.WriteUInt160(self.ScriptHash)\n        writer.WriteBool(self.IsFrozen)\n        writer.WriteVarInt(len(self.Votes))\n        for vote in self.Votes:\n            writer.WriteBytes(vote)\n\n        blen = len(self.Balances)\n        writer.WriteVarInt(blen)\n\n        for key, fixed8 in self.Balances.items():\n            writer.WriteUInt256(key)\n            writer.WriteFixed8(fixed8)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef HasBalance(self, assetId):\n        for key, fixed8 in self.Balances.items():\n            if key == assetId:\n                return True\n        return False", "response": "Returns a bool indicating if the asset has a balance."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the balance value for a given asset id.", "response": "def BalanceFor(self, assetId):\n        \"\"\"\n        Get the balance for a given asset id.\n\n        Args:\n            assetId (UInt256):\n\n        Returns:\n            Fixed8: balance value.\n        \"\"\"\n        for key, fixed8 in self.Balances.items():\n            if key == assetId:\n                return fixed8\n        return Fixed8(0)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef SetBalanceFor(self, assetId, fixed8_val):\n        found = False\n        for key, val in self.Balances.items():\n            if key == assetId:\n                self.Balances[key] = fixed8_val\n                found = True\n\n        if not found:\n            self.Balances[assetId] = fixed8_val", "response": "Sets the balance for an asset id."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef AddToBalance(self, assetId, fixed8_val):\n        found = False\n        for key, balance in self.Balances.items():\n            if key == assetId:\n                self.Balances[assetId] = self.Balances[assetId] + fixed8_val\n                found = True\n        if not found:\n            self.Balances[assetId] = fixed8_val", "response": "Adds amount to the specified balance."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef SubtractFromBalance(self, assetId, fixed8_val):\n        found = False\n        for key, balance in self.Balances.items():\n            if key == assetId:\n                self.Balances[assetId] = self.Balances[assetId] - fixed8_val\n                found = True\n        if not found:\n            self.Balances[assetId] = fixed8_val * Fixed8(-1)", "response": "Subtract amount from the specified balance."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True if all balances are 0 or less False otherwise.", "response": "def AllBalancesZeroOrLess(self):\n        \"\"\"\n        Flag indicating if all balances are 0 or less.\n\n        Returns:\n            bool: True if all balances are <= 0. False, otherwise.\n        \"\"\"\n        for key, fixed8 in self.Balances.items():\n            if fixed8.value > 0:\n                return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ToByteArray(self):\n        ms = StreamManager.GetStream()\n        writer = BinaryWriter(ms)\n        self.Serialize(writer)\n\n        retval = ms.ToArray()\n        StreamManager.ReleaseStream(ms)\n\n        return retval", "response": "Serialize self and get the byte stream."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ToJson(self):\n        json = super(AccountState, self).ToJson()\n        addr = Crypto.ToAddress(self.ScriptHash)\n\n        json['address'] = addr\n        json['script_hash'] = str(self.ScriptHash)\n        json['frozen'] = self.IsFrozen\n        json['votes'] = [v.hex() for v in self.Votes]\n\n        balances = []\n        for key, value in self.Balances.items():\n            balances.append({'asset': key.To0xString(), 'value': value.ToString()})\n\n        json['balances'] = balances\n        return json", "response": "Convert object members to a dictionary that can be parsed as JSON."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the total size in bytes of the object.", "response": "def Size(self):\n        \"\"\"\n        Get the total size in bytes of the object.\n\n        Returns:\n            int: size.\n        \"\"\"\n        if len(self.Hashes) > 0:\n            if not isinstance(self.Hashes[0], UInt256):\n                corrected_hashes = list(map(lambda i: UInt256(data=binascii.unhexlify(i)), self.Hashes))\n        return s.uint8 + GetVarSize(corrected_hashes)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef Deserialize(self, reader):\n        self.Type = reader.ReadByte()\n        self.Hashes = reader.ReadHashes()", "response": "Deserialize full object.\n\n        Args:\n            reader (neo.IO.BinaryReader):"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef Serialize(self, writer):\n        try:\n            writer.WriteByte(self.Type)\n            writer.WriteHashes(self.Hashes)\n        except Exception as e:\n            logger.error(f\"COULD NOT WRITE INVENTORY HASHES ({self.Type} {self.Hashes}) {e}\")", "response": "Serialize object.\n\n        Raises:\n            Exception: if hash writing fails.\n\n        Args:\n            writer (neo.IO.BinaryWriter):"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef instance():\n        if not NotificationDB.__instance:\n            if settings.NOTIFICATION_DB_PATH:\n                NotificationDB.__instance = NotificationDB(settings.notification_leveldb_path)\n            else:\n                logger.info(\"Notification DB Path not configured in settings\")\n        return NotificationDB.__instance", "response": "Returns the current instance of the NotificationDB class"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstarts the eventhub for the current SmartContract class.", "response": "def start(self):\n        \"\"\"\n        Handle EventHub events for SmartContract decorators\n        \"\"\"\n        self._events_to_write = []\n        self._new_contracts_to_write = []\n\n        @events.on(SmartContractEvent.CONTRACT_CREATED)\n        @events.on(SmartContractEvent.CONTRACT_MIGRATED)\n        def call_on_success_event(sc_event: SmartContractEvent):\n            self.on_smart_contract_created(sc_event)\n\n        @events.on(SmartContractEvent.RUNTIME_NOTIFY)\n        def call_on_event(sc_event: NotifyEvent):\n            self.on_smart_contract_event(sc_event)\n\n        Blockchain.Default().PersistCompleted.on_change += self.on_persist_completed"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef on_smart_contract_event(self, sc_event: NotifyEvent):\n        if not isinstance(sc_event, NotifyEvent):\n            logger.info(\"Not Notify Event instance\")\n            return\n        if sc_event.ShouldPersist:\n            if sc_event.notify_type in [NotifyType.TRANSFER, NotifyType.REFUND, NotifyType.MINT]:\n                self._events_to_write.append(sc_event)", "response": "Called when a smart contract event is received."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalls when a block has been successfully persisted to disk.", "response": "def on_persist_completed(self, block):\n        \"\"\"\n        Called when a block has been persisted to disk.  Used as a hook to persist notification data.\n        Args:\n            block (neo.Core.Block): the currently persisting block\n        \"\"\"\n        if len(self._events_to_write):\n\n            addr_db = self.db.prefixed_db(NotificationPrefix.PREFIX_ADDR)\n            block_db = self.db.prefixed_db(NotificationPrefix.PREFIX_BLOCK)\n            contract_db = self.db.prefixed_db(NotificationPrefix.PREFIX_CONTRACT)\n\n            block_write_batch = block_db.write_batch()\n            contract_write_batch = contract_db.write_batch()\n\n            block_count = 0\n            block_bytes = self._events_to_write[0].block_number.to_bytes(4, 'little')\n\n            for evt in self._events_to_write:  # type:NotifyEvent\n\n                # write the event for both or one of the addresses involved in the transfer\n                write_both = True\n                hash_data = evt.ToByteArray()\n\n                bytes_to = bytes(evt.addr_to.Data)\n                bytes_from = bytes(evt.addr_from.Data)\n\n                if bytes_to == bytes_from:\n                    write_both = False\n\n                total_bytes_to = addr_db.get(bytes_to + NotificationPrefix.PREFIX_COUNT)\n                total_bytes_from = addr_db.get(bytes_from + NotificationPrefix.PREFIX_COUNT)\n\n                if not total_bytes_to:\n                    total_bytes_to = b'\\x00'\n\n                if not total_bytes_from:\n                    total_bytes_from = b'x\\00'\n\n                addr_to_key = bytes_to + total_bytes_to\n                addr_from_key = bytes_from + total_bytes_from\n\n                with addr_db.write_batch() as b:\n                    b.put(addr_to_key, hash_data)\n                    if write_both:\n                        b.put(addr_from_key, hash_data)\n                    total_bytes_to = int.from_bytes(total_bytes_to, 'little') + 1\n                    total_bytes_from = int.from_bytes(total_bytes_from, 'little') + 1\n                    new_bytes_to = total_bytes_to.to_bytes(4, 'little')\n                    new_bytes_from = total_bytes_from.to_bytes(4, 'little')\n                    b.put(bytes_to + NotificationPrefix.PREFIX_COUNT, new_bytes_to)\n                    if write_both:\n                        b.put(bytes_from + NotificationPrefix.PREFIX_COUNT, new_bytes_from)\n\n                # write the event to the per-block database\n                per_block_key = block_bytes + block_count.to_bytes(4, 'little')\n                block_write_batch.put(per_block_key, hash_data)\n                block_count += 1\n\n                # write the event to the per-contract database\n                contract_bytes = bytes(evt.contract_hash.Data)\n                count_for_contract = contract_db.get(contract_bytes + NotificationPrefix.PREFIX_COUNT)\n                if not count_for_contract:\n                    count_for_contract = b'\\x00'\n                contract_event_key = contract_bytes + count_for_contract\n                contract_count_int = int.from_bytes(count_for_contract, 'little') + 1\n                new_contract_count = contract_count_int.to_bytes(4, 'little')\n                contract_write_batch.put(contract_bytes + NotificationPrefix.PREFIX_COUNT, new_contract_count)\n                contract_write_batch.put(contract_event_key, hash_data)\n\n            # finish off the per-block write batch and contract write batch\n            block_write_batch.write()\n            contract_write_batch.write()\n\n        self._events_to_write = []\n\n        if len(self._new_contracts_to_write):\n\n            token_db = self.db.prefixed_db(NotificationPrefix.PREFIX_TOKEN)\n\n            token_write_batch = token_db.write_batch()\n\n            for token_event in self._new_contracts_to_write:\n                try:\n                    hash_data = token_event.ToByteArray()  # used to fail here\n                    hash_key = token_event.contract.Code.ScriptHash().ToBytes()\n                    token_write_batch.put(hash_key, hash_data)\n                except Exception as e:\n                    logger.debug(f\"Failed to write new contract, reason: {e}\")\n\n            token_write_batch.write()\n\n        self._new_contracts_to_write = []"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlooking up notifications for a block_number", "response": "def get_by_block(self, block_number):\n        \"\"\"\n        Look up notifications for a block\n        Args:\n            block_number (int): height of block to search for notifications\n\n        Returns:\n            list: a list of notifications\n        \"\"\"\n        blocklist_snapshot = self.db.prefixed_db(NotificationPrefix.PREFIX_BLOCK).snapshot()\n        block_bytes = block_number.to_bytes(4, 'little')\n        results = []\n        for val in blocklist_snapshot.iterator(prefix=block_bytes, include_key=False):\n            event = SmartContractEvent.FromByteArray(val)\n            results.append(event)\n\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlooks up a set of notifications by the contract they are associated with", "response": "def get_by_contract(self, contract_hash):\n        \"\"\"\n        Look up a set of notifications by the contract they are associated with\n        Args:\n            contract_hash (UInt160 or str): hash of contract for notifications to be retreived\n\n        Returns:\n            list: a list of notifications\n        \"\"\"\n        hash = contract_hash\n        if isinstance(contract_hash, str) and len(contract_hash) == 40:\n            hash = UInt160.ParseString(contract_hash)\n\n        if not isinstance(hash, UInt160):\n            raise Exception(\"Incorrect address format\")\n\n        contractlist_snapshot = self.db.prefixed_db(NotificationPrefix.PREFIX_CONTRACT).snapshot()\n        results = []\n\n        for val in contractlist_snapshot.iterator(prefix=bytes(hash.Data), include_key=False):\n            if len(val) > 4:\n                try:\n                    event = SmartContractEvent.FromByteArray(val)\n                    results.append(event)\n                except Exception as e:\n                    logger.error(\"could not parse event: %s %s\" % (e, val))\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_tokens(self):\n        tokens_snapshot = self.db.prefixed_db(NotificationPrefix.PREFIX_TOKEN).snapshot()\n        results = []\n        for val in tokens_snapshot.iterator(include_key=False):\n            event = SmartContractEvent.FromByteArray(val)\n            results.append(event)\n        return results", "response": "Look up all tokens in the database Returns a list of SmartContractEvents with contracts that are NEP5 Tokens\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_token(self, hash):\n        tokens_snapshot = self.db.prefixed_db(NotificationPrefix.PREFIX_TOKEN).snapshot()\n\n        try:\n            val = tokens_snapshot.get(hash.ToBytes())\n            if val:\n                event = SmartContractEvent.FromByteArray(val)\n                return event\n        except Exception as e:\n            logger.error(\"Smart contract event with contract hash %s not found: %s \" % (hash.ToString(), e))\n        return None", "response": "Look up a token by hash"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef Deserialize(self, reader):\n        super(Header, self).Deserialize(reader)\n        if reader.ReadByte() != 0:\n            raise Exception('Incorrect Header Format')", "response": "Deserialize full object.\n\n        Args:\n            reader (neo.IO.BinaryReader):"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef FromTrimmedData(data, index):\n        header = Header()\n\n        ms = StreamManager.GetStream(data)\n\n        reader = BinaryReader(ms)\n        header.DeserializeUnsigned(reader)\n        reader.ReadByte()\n\n        witness = Witness()\n        witness.Deserialize(reader)\n        header.Script = witness\n\n        StreamManager.ReleaseStream(ms)\n\n        return header", "response": "Deserialize a Header object from the provided data."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nserialize full object. Args: writer (neo.IO.BinaryWriter):", "response": "def Serialize(self, writer):\n        \"\"\"\n        Serialize full object.\n\n        Args:\n            writer (neo.IO.BinaryWriter):\n        \"\"\"\n        super(Header, self).Serialize(writer)\n        writer.WriteByte(0)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nimporting a contract from wallet to a new wallet", "response": "def ImportContractAddr(wallet, contract_hash, pubkey_script_hash):\n    \"\"\"\n    Args:\n        wallet (Wallet): a UserWallet instance\n        contract_hash (UInt160): hash of the contract to import\n        pubkey_script_hash (UInt160):\n\n    Returns:\n        neo.SmartContract.Contract.Contract\n    \"\"\"\n\n    contract = Blockchain.Default().GetContract(contract_hash)\n    if not contract or not pubkey_script_hash:\n        print(\"Could not find contract\")\n        return\n\n    reedeem_script = contract.Code.Script.hex()\n\n    # there has to be at least 1 param, and the first one needs to be a signature param\n    param_list = bytearray(b'\\x00')\n\n    # if there's more than one param\n    # we set the first parameter to be the signature param\n    if len(contract.Code.ParameterList) > 1:\n        param_list = bytearray(contract.Code.ParameterList)\n        param_list[0] = 0\n\n    verification_contract = Contract.Create(reedeem_script, param_list, pubkey_script_hash)\n\n    address = verification_contract.Address\n\n    wallet.AddContract(verification_contract)\n\n    print(f\"Added contract address {address} to wallet\")\n    return verification_contract"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef Size(self):\n        script_size = GetVarSize(self.Code.Script)\n        parameterlist_size = GetVarSize(self.Code.ParameterList)\n        parameterreturntype_size = s.uint8\n\n        return super(ContractState, self).Size() + script_size + parameterlist_size + parameterreturntype_size + s.uint8 + GetVarSize(self.Name) + GetVarSize(self.CodeVersion) + GetVarSize(self.Author) + GetVarSize(self.Email) + GetVarSize(self.Description)", "response": "Returns the size in bytes of the object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef DeserializeFromDB(buffer):\n        m = StreamManager.GetStream(buffer)\n        reader = BinaryReader(m)\n        c = ContractState()\n        c.Deserialize(reader)\n\n        StreamManager.ReleaseStream(m)\n\n        return c", "response": "Deserialize a full object from a byte string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef Serialize(self, writer):\n        super(ContractState, self).Serialize(writer)\n\n        self.Code.Serialize(writer)\n        writer.WriteUInt8(self.ContractProperties)\n        writer.WriteVarString(self.Name)\n        writer.WriteVarString(self.CodeVersion)\n        writer.WriteVarString(self.Author)\n        writer.WriteVarString(self.Email)\n        writer.WriteVarString(self.Description)", "response": "Serialize full object.\n\n        Args:\n            writer (neo.IO.BinaryWriter):"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef DetermineIsNEP5(self):\n        from neo.Wallets.NEP5Token import NEP5Token\n\n        self._is_nep5 = False\n        token = NEP5Token(binascii.hexlify(self.Code.Script))\n        if token.Query():\n            self._nep_token = token\n            self._is_nep5 = True\n        return self._is_nep5", "response": "Determines if this Smart contract is an NEP5 Token or not."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ToJson(self):\n        name = 'Contract'\n\n        try:\n            name = self.Name.decode('utf-8')\n        except Exception as e:\n            pass\n\n        jsn = {'version': self.StateVersion}\n\n        jsn_code = self.Code.ToJson()\n\n        jsn_contract = {\n            'name': name,\n            'code_version': self.CodeVersion.decode('utf-8'),\n            'author': self.Author.decode('utf-8'),\n            'email': self.Email.decode('utf-8'),\n            'description': self.Description.decode('utf-8'),\n            'properties': {\n                'storage': self.HasStorage,\n                'dynamic_invoke': self.HasDynamicInvoke,\n                'payable': self.Payable\n            }\n        }\n\n        jsn.update(jsn_code)\n        jsn.update(jsn_contract)\n\n        if self._nep_token:\n            jsn['token'] = self._nep_token.ToJson()\n\n        return jsn", "response": "Converts object members to a dictionary that can be parsed as JSON."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a ContractParameterType to a name based on its value", "response": "def ToName(param_type):\n    \"\"\"\n    Gets the name of a ContractParameterType based on its value\n    Args:\n        param_type (ContractParameterType): type to get the name of\n\n    Returns:\n        str\n    \"\"\"\n    items = inspect.getmembers(ContractParameterType)\n\n    if type(param_type) is bytes:\n        param_type = int.from_bytes(param_type, 'little')\n\n    for item in items:\n        name = item[0]\n        val = int(item[1].value)\n\n        if val == param_type:\n            return name\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a ContractParameterType object from a string representation of the ISO - 8601 ISO - 8601 ISO - 8601 ISO - 8601 ISO - 8601 ISO - 8601 ISO - 8601 ISO - 8601 ISO - 8601 string.", "response": "def FromString(val):\n        \"\"\"\n        Create a ContractParameterType object from a str\n\n        Args:\n            val (str): the value to be converted to a ContractParameterType.\n            val can be hex encoded (b'07'), int (7), string int (\"7\"), or string literal (\"String\")\n\n        Returns:\n            ContractParameterType\n        \"\"\"\n        # first, check if the value supplied is the string literal of the enum (e.g. \"String\")\n\n        if isinstance(val, bytes):\n            val = val.decode('utf-8')\n\n        try:\n            return ContractParameterType[val]\n        except Exception as e:\n            # ignore a KeyError if the val isn't found in the Enum\n            pass\n\n        # second, check if the value supplied is bytes or hex-encoded (e.g. b'07')\n        try:\n            if isinstance(val, (bytearray, bytes)):\n                int_val = int.from_bytes(val, 'little')\n            else:\n                int_val = int.from_bytes(binascii.unhexlify(val), 'little')\n        except (binascii.Error, TypeError) as e:\n            # if it's not hex-encoded, then convert as int (e.g. \"7\" or 7)\n            int_val = int(val)\n\n        return ContractParameterType(int_val)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef DeserializeExclusiveData(self, reader):\n        self.Nonce = reader.ReadUInt32()\n        self.Type = TransactionType.MinerTransaction", "response": "Deserialize the object from a binary reader."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef SerializeExclusiveDataAlternative(self, writer):\n        byt = int.to_bytes(self.Nonce, 4, 'little')\n        ba = bytearray(byt)\n        byts = binascii.hexlify(ba)\n        writer.WriteBytes(byts)", "response": "Serialize the object as an exclusive data alternative."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert object members to a dictionary that can be parsed as JSON.", "response": "def ToJson(self):\n        \"\"\"\n        Convert object members to a dictionary that can be parsed as JSON.\n\n        Returns:\n             dict:\n        \"\"\"\n        jsn = super(MinerTransaction, self).ToJson()\n        jsn['nonce'] = self.Nonce\n        return jsn"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef address_to_scripthash(address: str) -> UInt160:\n    AddressVersion = 23  # fixed at this point\n    data = b58decode(address)\n    if len(data) != 25:\n        raise ValueError('Not correct Address, wrong length.')\n    if data[0] != AddressVersion:\n        raise ValueError('Not correct Coin Version')\n\n    checksum_data = data[:21]\n    checksum = hashlib.sha256(hashlib.sha256(checksum_data).digest()).digest()[:4]\n    if checksum != data[21:]:\n        raise Exception('Address format error')\n    return UInt160(data=data[1:21])", "response": "Convert an address to a scripthash"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef HasStorage(self):\n        from neo.Core.State.ContractState import ContractPropertyState\n        return self.ContractProperties & ContractPropertyState.HasStorage > 0", "response": "Returns True if storage is available. False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef HasDynamicInvoke(self):\n        from neo.Core.State.ContractState import ContractPropertyState\n        return self.ContractProperties & ContractPropertyState.HasDynamicInvoke > 0", "response": "Returns True if dynamic invocation is supported. False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn True if the contract accepts payments False otherwise.", "response": "def IsPayable(self):\n        \"\"\"\n        Flag indicating if the contract accepts payments.\n\n        Returns:\n            bool: True if supported. False otherwise.\n        \"\"\"\n        from neo.Core.State.ContractState import ContractPropertyState\n        return self.ContractProperties & ContractPropertyState.Payable > 0"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ScriptHash(self):\n        if self._scriptHash is None:\n            self._scriptHash = Crypto.ToScriptHash(self.Script, unhex=False)\n\n        return self._scriptHash", "response": "Returns the script hash of the message."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nserializing full object. Args: writer (neo.IO.BinaryWriter):", "response": "def Serialize(self, writer):\n        \"\"\"\n        Serialize full object.\n\n        Args:\n            writer (neo.IO.BinaryWriter):\n        \"\"\"\n        writer.WriteVarBytes(self.Script)\n        writer.WriteVarBytes(self.ParameterList)\n        writer.WriteByte(self.ReturnType)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert object members to a dictionary that can be parsed as JSON.", "response": "def ToJson(self):\n        \"\"\"\n        Convert object members to a dictionary that can be parsed as JSON.\n\n        Returns:\n             dict:\n        \"\"\"\n        parameters = self.ParameterList.hex()\n        paramlist = [ToName(ContractParameterType.FromString(parameters[i:i + 2]).value) for i in range(0, len(parameters), 2)]\n        return {\n            'hash': self.ScriptHash().To0xString(),\n            'script': self.Script.hex(),\n            'parameters': paramlist,\n            'returntype': ToName(self.ReturnType) if type(self.ReturnType) is int else ToName(int(self.ReturnType))\n        }"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef Serialize(self, writer):\n        writer.WriteVarInt(len(self.NetworkAddressesWithTime))\n        for address in self.NetworkAddressesWithTime:\n            address.Serialize(writer)", "response": "Serialize object.\n\n        Args:\n            writer (neo.IO.BinaryWriter):"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef SerializeExclusiveData(self, writer):\n        writer.WriteVarBytes(self.Script)\n        if self.Version >= 1:\n            writer.WriteFixed8(self.Gas)", "response": "Serialize object.\n\n        Args:\n            writer (neo.IO.BinaryWriter):"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nverify the transaction. Args: mempool: Returns: bool: True if verified. False otherwise.", "response": "def Verify(self, mempool):\n        \"\"\"\n        Verify the transaction.\n\n        Args:\n            mempool:\n\n        Returns:\n            bool: True if verified. False otherwise.\n        \"\"\"\n        if self.Gas.value % 100000000 != 0:\n            return False\n        return super(InvocationTransaction, self).Verify(mempool)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ToJson(self):\n        jsn = super(InvocationTransaction, self).ToJson()\n        jsn['script'] = self.Script.hex()\n        jsn['gas'] = self.Gas.ToNeoJsonString()\n        return jsn", "response": "Convert object members to a dictionary that can be parsed as JSON."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef DeserializeExclusiveData(self, reader):\n        self.Type = TransactionType.StateTransaction\n\n        self.Descriptors = reader.ReadSerializableArray('neo.Core.State.StateDescriptor.StateDescriptor')", "response": "Deserialize the exclusive data of the state."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ToJson(self):\n\n        json = super(StateTransaction, self).ToJson()\n        descriptors = [d.ToJson() for d in self.Descriptors]\n\n        json['descriptors'] = descriptors\n\n        return json", "response": "Convert the state transaction to a dictionary that can be parsed as JSON."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nverify the transaction. Args: mempool: Returns: bool: True if verified. False otherwise.", "response": "def Verify(self, mempool):\n        \"\"\"\n        Verify the transaction.\n\n        Args:\n            mempool:\n\n        Returns:\n            bool: True if verified. False otherwise.\n        \"\"\"\n\n        for descriptor in self.Descriptors:\n            if not descriptor.Verify():\n                return False\n\n        return super(StateTransaction, self).Verify(mempool)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef GetBlockHash(self, height):\n        if self._current_block_height < height:\n            return\n\n        if len(self._header_index) <= height:\n            return\n\n        return self._header_index[height]", "response": "Get the block hash by its height."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef GetBlockByHeight(self, height):\n        hash = self.GetBlockHash(height)\n        if hash is not None:\n            return self.GetBlockByHash(hash)", "response": "Get a block instance by its height."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert object members to a dictionary that can be parsed as JSON.", "response": "def ToJson(self):\n        \"\"\"\n        Convert object members to a dictionary that can be parsed as JSON.\n\n        Returns:\n             dict:\n        \"\"\"\n        jsn = super(EnrollmentTransaction, self).ToJson()\n        jsn['pubkey'] = self.PublicKey.ToString()\n        return jsn"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_depdendencies():\n    # Get installed packages\n    installed_packages = pip.get_installed_distributions(local_only=False)\n    installed_packages_list = sorted([\"%s==%s\" % (i.key, i.version) for i in installed_packages])\n\n    # Now check if each package specified in requirements.txt is actually installed\n    deps_filename = os.path.join(ROOT_INSTALL_PATH, \"requirements.txt\")\n    with open(deps_filename, \"r\") as f:\n        for dep in f.read().split():\n            if not dep.lower() in installed_packages_list:\n                raise SystemCheckError(\"Required dependency %s is not installed. Please run 'pip install -e .'\" % dep)", "response": "Checks that all required dependencies are installed in the exact version\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloads settings from the privnet JSON config file Args: host (string, optional): if supplied, uses this IP or domain as neo nodes. The host must use these standard ports: P2P 20333, RPC 30333.", "response": "def setup_privnet(self, host=None):\n        \"\"\"\n        Load settings from the privnet JSON config file\n\n        Args:\n            host (string, optional): if supplied, uses this IP or domain as neo nodes. The host must\n                                     use these standard ports: P2P 20333, RPC 30333.\n        \"\"\"\n        self.setup(FILENAME_SETTINGS_PRIVNET)\n        if isinstance(host, str):\n            if \":\" in host:\n                raise Exception(\"No protocol prefix or port allowed in host, use just the IP or domain.\")\n            print(\"Using custom privatenet host:\", host)\n            self.SEED_LIST = [\"%s:20333\" % host]\n            self.RPC_LIST = [\"http://%s:30333\" % host]\n            print(\"- P2P:\", \", \".join(self.SEED_LIST))\n            print(\"- RPC:\", \", \".join(self.RPC_LIST))\n        self.check_privatenet()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the loglevel for all components of the current log level.", "response": "def set_loglevel(self, level):\n        \"\"\"\n        Set the minimum loglevel for all components\n\n        Args:\n            level (int): eg. logging.DEBUG or logging.ERROR. See also https://docs.python.org/2/library/logging.html#logging-levels\n        \"\"\"\n        self.log_level = level\n        log_manager.config_stdio(default_level=level)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck to make sure that the chain directory exists and creates it if it doesn t exist.", "response": "def check_chain_dir_exists(self, warn_migration=False):\n        \"\"\"\n        Checks to make sure there is a directory called ``Chains`` at the root of DATA_DIR_PATH\n        and creates it if it doesn't exist yet\n        \"\"\"\n        chain_path = os.path.join(self.DATA_DIR_PATH, 'Chains')\n\n        if not os.path.exists(chain_path):\n            try:\n                os.makedirs(chain_path)\n                logger.info(\"Created 'Chains' directory at %s \" % chain_path)\n            except Exception as e:\n                logger.error(\"Could not create 'Chains' directory at %s %s\" % (chain_path, e))\n\n        warn_migration = False\n        # Add a warning for migration purposes if we created a chain dir\n        if warn_migration and ROOT_INSTALL_PATH != self.DATA_DIR_PATH:\n            if os.path.exists(os.path.join(ROOT_INSTALL_PATH, 'Chains')):\n                logger.warning(\"[MIGRATION] You are now using the blockchain data at %s, but it appears you have existing data at %s/Chains\" % (\n                    chain_path, ROOT_INSTALL_PATH))\n                logger.warning(\n                    \"[MIGRATION] If you would like to use your existing data, please move any data at %s/Chains to %s \" % (ROOT_INSTALL_PATH, chain_path))\n                logger.warning(\"[MIGRATION] Or you can continue using your existing data by starting your script with the `--datadir=.` flag\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_privatenet(self):\n        rpc_settings.setup(self.RPC_LIST)\n        client = RPCClient()\n\n        try:\n            version = client.get_version()\n        except NEORPCException:\n            raise PrivnetConnectionError(\"Error: private network container doesn't seem to be running, or RPC is not enabled.\")\n\n        print(\"Privatenet useragent '%s', nonce: %s\" % (version[\"useragent\"], version[\"nonce\"]))\n\n        # Now check if nonce is the same as in the chain path\n        nonce_container = str(version[\"nonce\"])\n        neopy_chain_meta_filename = os.path.join(self.chain_leveldb_path, \".privnet-nonce\")\n        if os.path.isfile(neopy_chain_meta_filename):\n            nonce_chain = open(neopy_chain_meta_filename, \"r\").read()\n            if nonce_chain != nonce_container:\n                raise PrivnetConnectionError(\n                    \"Chain database in Chains/privnet is for a different private network than the current container. \"\n                    \"Consider deleting the Chain directory with 'rm -rf %s*'.\" % self.chain_leveldb_path\n                )\n        else:\n            # When the Chains/privnet folder is removed, we need to create the directory\n            if not os.path.isdir(self.chain_leveldb_path):\n                os.mkdir(self.chain_leveldb_path)\n\n            # Write the nonce to the meta file\n            with open(neopy_chain_meta_filename, \"w\") as f:\n                f.write(nonce_container)", "response": "Check if the current privatenet is running and if the nonce is the same as the current chain."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsplitting unspent asset vins into several vouts Args: wallet (neo.Wallet): wallet to show unspent coins from. asset_id (UInt256): a bytearray (len 32) representing an asset on the blockchain. from_addr (UInt160): a bytearray (len 20) representing an address. index (int): index of the unspent vin to split divisions (int): number of vouts to create fee (Fixed8): A fee to be attached to the Transaction for network processing purposes. prompt_passwd (bool): prompt password before processing the transaction Returns: neo.Core.TX.Transaction.ContractTransaction: contract transaction created", "response": "def SplitUnspentCoin(wallet, asset_id, from_addr, index, divisions, fee=Fixed8.Zero(), prompt_passwd=True):\n    \"\"\"\n    Split unspent asset vins into several vouts\n\n    Args:\n        wallet (neo.Wallet): wallet to show unspent coins from.\n        asset_id (UInt256): a bytearray (len 32) representing an asset on the blockchain.\n        from_addr (UInt160): a bytearray (len 20) representing an address.\n        index (int): index of the unspent vin to split\n        divisions (int): number of vouts to create\n        fee (Fixed8): A fee to be attached to the Transaction for network processing purposes.\n        prompt_passwd (bool): prompt password before processing the transaction\n\n    Returns:\n        neo.Core.TX.Transaction.ContractTransaction: contract transaction created\n    \"\"\"\n\n    if wallet is None:\n        print(\"Please open a wallet.\")\n        return\n\n    unspent_items = wallet.FindUnspentCoinsByAsset(asset_id, from_addr=from_addr)\n    if not unspent_items:\n        print(f\"No unspent assets matching the arguments.\")\n        return\n\n    if index < len(unspent_items):\n        unspent_item = unspent_items[index]\n    else:\n        print(f\"unspent-items: {unspent_items}\")\n        print(f\"Could not find unspent item for asset {asset_id} with index {index}\")\n        return\n\n    outputs = split_to_vouts(asset_id, from_addr, unspent_item.Output.Value, divisions)\n\n    # subtract a fee from the first vout\n    if outputs[0].Value > fee:\n        outputs[0].Value -= fee\n    else:\n        print(\"Fee could not be subtracted from outputs.\")\n        return\n\n    contract_tx = ContractTransaction(outputs=outputs, inputs=[unspent_item.Reference])\n\n    ctx = ContractParametersContext(contract_tx)\n    wallet.Sign(ctx)\n\n    print(\"Splitting: %s \" % json.dumps(contract_tx.ToJson(), indent=4))\n    if prompt_passwd:\n        passwd = prompt(\"[Password]> \", is_password=True)\n        if not wallet.ValidatePassword(passwd):\n            print(\"incorrect password\")\n            return\n\n    if ctx.Completed:\n        contract_tx.scripts = ctx.GetScripts()\n\n        relayed = NodeLeader.Instance().Relay(contract_tx)\n\n        if relayed:\n            wallet.SaveTransaction(contract_tx)\n            print(\"Relayed Tx: %s \" % contract_tx.Hash.ToString())\n            return contract_tx\n        else:\n            print(\"Could not relay tx %s \" % contract_tx.Hash.ToString())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef Deserialize(self, reader):\n        super(StorageItem, self).Deserialize(reader)\n        self.Value = reader.ReadVarBytes()", "response": "Deserialize full object.\n\n        Args:\n            reader (neocore.IO.BinaryReader):"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef DeserializeFromDB(buffer):\n        m = StreamManager.GetStream(buffer)\n        reader = BinaryReader(m)\n        v = StorageItem()\n        v.Deserialize(reader)\n        StreamManager.ReleaseStream(m)\n        return v", "response": "Deserialize a full object from a byte string."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef Serialize(self, writer):\n        super(StorageItem, self).Serialize(writer)\n        writer.WriteVarBytes(self.Value)", "response": "Serialize full object.\n\n        Args:\n            writer (neo.IO.BinaryWriter):"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef SystemFee(self):\n        if self.Version >= 1:\n            return Fixed8.Zero()\n\n        # if all outputs are NEO or gas, return 0\n        all_neo_gas = True\n        for output in self.outputs:\n            if output.AssetId != GetSystemCoin().Hash and output.AssetId != GetSystemShare().Hash:\n                all_neo_gas = False\n        if all_neo_gas:\n            return Fixed8.Zero()\n\n        return super(IssueTransaction, self).SystemFee()", "response": "Returns the system fee of the issue transaction."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef DeserializeExclusiveData(self, reader):\n\n        self.Type = TransactionType.IssueTransaction\n\n        if self.Version > 1:\n            raise Exception('Invalid TX Type')", "response": "Deserialize an exclusive transaction."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef register_sub_command(self, sub_command, additional_ids=[]):\n        self.__register_sub_command(sub_command, sub_command.command_desc().command)\n        self.__additional_ids.update(additional_ids)\n        for id in additional_ids:\n            self.__register_sub_command(sub_command, id)", "response": "Register a command as a subcommand."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_class_from_path(path_and_class: str):\n    try:\n        module_path = '.'.join(path_and_class.split('.')[:-1])\n        module = importlib.import_module(module_path)\n    except ImportError as err:\n        raise ValueError(f\"Failed to import module {module_path} with error: {err}\")\n\n    try:\n        class_name = path_and_class.split('.')[-1]\n        class_obj = getattr(module, class_name)\n        return class_obj\n    except AttributeError as err:\n        raise ValueError(f\"Failed to get class {class_name} with error: {err}\")", "response": "Dynamically loads a class from a module at the specified path"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun a script in a test invoke environment", "response": "def Run(script, container=None, exit_on_error=False, gas=Fixed8.Zero(), test_mode=True):\n        \"\"\"\n        Runs a script in a test invoke environment\n\n        Args:\n            script (bytes): The script to run\n            container (neo.Core.TX.Transaction): [optional] the transaction to use as the script container\n\n        Returns:\n            ApplicationEngine\n        \"\"\"\n\n        from neo.Core.Blockchain import Blockchain\n        from neo.SmartContract.StateMachine import StateMachine\n        from neo.EventHub import events\n\n        bc = Blockchain.Default()\n\n        accounts = DBCollection(bc._db, DBPrefix.ST_Account, AccountState)\n        assets = DBCollection(bc._db, DBPrefix.ST_Asset, AssetState)\n        validators = DBCollection(bc._db, DBPrefix.ST_Validator, ValidatorState)\n        contracts = DBCollection(bc._db, DBPrefix.ST_Contract, ContractState)\n        storages = DBCollection(bc._db, DBPrefix.ST_Storage, StorageItem)\n\n        script_table = CachedScriptTable(contracts)\n        service = StateMachine(accounts, validators, assets, contracts, storages, None)\n\n        engine = ApplicationEngine(\n            trigger_type=TriggerType.Application,\n            container=container,\n            table=script_table,\n            service=service,\n            gas=gas,\n            testMode=test_mode,\n            exit_on_error=exit_on_error\n        )\n\n        script = binascii.unhexlify(script)\n\n        engine.LoadScript(script)\n\n        try:\n            success = engine.Execute()\n            engine.testMode = True\n            service.ExecutionCompleted(engine, success)\n        except Exception as e:\n            engine.testMode = True\n            service.ExecutionCompleted(engine, False, e)\n\n        for event in service.events_to_dispatch:\n            events.emit(event.event_type, event)\n\n        return engine"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadvances the iterator forward 1 step.", "response": "def Next(self):\n        \"\"\"\n        Advances the iterator forward 1 step.\n\n        Returns:\n              bool: True if another item exists in the iterator, False otherwise.\n        \"\"\"\n        try:\n            self.key, self.value = next(self.current)\n        except StopIteration:\n\n            if self.current != self.second:\n                self.current = self.second\n                return self.Next()\n\n            return False\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the system fee of the asset.", "response": "def SystemFee(self):\n        \"\"\"\n        Get the system fee.\n\n        Returns:\n            Fixed8:\n        \"\"\"\n        if self.AssetType == AssetType.GoverningToken or self.AssetType == AssetType.UtilityToken:\n            return Fixed8.Zero()\n\n        return super(RegisterTransaction, self).SystemFee()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef SerializeExclusiveData(self, writer):\n        writer.WriteByte(self.AssetType)\n        writer.WriteVarString(self.Name)\n        writer.WriteFixed8(self.Amount)\n        writer.WriteByte(self.Precision)\n\n        self.Owner.Serialize(writer)\n\n        writer.WriteUInt160(self.Admin)", "response": "Serialize object.\n\n        Args:\n            writer (neo.IO.BinaryWriter):"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert object members to a dictionary that can be parsed as JSON.", "response": "def ToJson(self):\n        \"\"\"\n        Convert object members to a dictionary that can be parsed as JSON.\n\n        Returns:\n             dict:\n        \"\"\"\n        jsn = super(RegisterTransaction, self).ToJson()\n\n        asset = {\n            'type': self.AssetType,\n            'name': self.Name.decode('utf-8'),\n            'amount': self.Amount.value,\n            'precision': self.Precision if type(self.Precision) is int else self.Precision.decode('utf-8'),\n            'owner': self.Owner.ToString(),\n            'admin': Crypto.ToAddress(self.Admin)\n        }\n        jsn['asset'] = asset\n\n        return jsn"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of all available asset types.", "response": "def AllTypes():\n        \"\"\"\n        Get a list of all available asset types.\n\n        Returns:\n            list: of AssetType items.\n        \"\"\"\n        return [AssetType.CreditFlag, AssetType.DutyFlag, AssetType.GoverningToken,\n                AssetType.UtilityToken, AssetType.Currency, AssetType.Share,\n                AssetType.Invoice, AssetType.Token]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef custom_background_code():\n    while True:\n        logger.info(\"Block %s / %s\", str(Blockchain.Default().Height), str(Blockchain.Default().HeaderHeight))\n        sleep(15)", "response": "Custom background code run in a daemonized thread."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef DeserializeExclusiveData(self, reader):\n        self.Type = TransactionType.ClaimTransaction\n        if self.Version != 0:\n            raise Exception('Format Exception')\n\n        numrefs = reader.ReadVarInt()\n\n        claims = []\n        for i in range(0, numrefs):\n            c = CoinReference()\n            c.Deserialize(reader)\n            claims.append(c)\n\n        self.Claims = claims\n        if len(self.Claims) == 0:\n            raise Exception('Format Exception')", "response": "Deserialize an exclusive transaction."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of script hashes for verifying transactions.", "response": "def GetScriptHashesForVerifying(self):\n        \"\"\"\n        Get a list of script hashes for verifying transactions.\n\n        Raises:\n            Exception: if there are no valid transactions to claim from.\n\n        Returns:\n            list: of UInt160 type script hashes.\n        \"\"\"\n        hashes = super(ClaimTransaction, self).GetScriptHashesForVerifying()\n\n        for hash, group in groupby(self.Claims, lambda x: x.PrevHash):\n            tx, height = Blockchain.Default().GetTransaction(hash)\n\n            if tx is None:\n                raise Exception(\"Invalid Claim Operation\")\n\n            for claim in group:\n                if len(tx.outputs) <= claim.PrevIndex:\n                    raise Exception(\"Invalid Claim Operation\")\n\n                script_hash = tx.outputs[claim.PrevIndex].ScriptHash\n\n                if script_hash not in hashes:\n                    hashes.append(script_hash)\n\n        hashes.sort()\n\n        return hashes"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ToJson(self):\n        json = super(ClaimTransaction, self).ToJson()\n\n        json['claims'] = [claim.ToJson() for claim in self.Claims]\n\n        return json", "response": "Convert object members to a dictionary that can be parsed as JSON."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nverifying the transaction. Args: mempool: Returns: bool: True if verified. False otherwise.", "response": "def Verify(self, mempool):\n        \"\"\"\n        Verify the transaction.\n\n        Args:\n            mempool:\n\n        Returns:\n            bool: True if verified. False otherwise.\n        \"\"\"\n        if not super(ClaimTransaction, self).Verify(mempool):\n            return False\n\n        # wat does this do\n        # get all claim transactions from mempool list\n        # that are not this claim\n        # and gather all the claims of those claim transactions\n        # and see if they intersect the claims of this transaction\n        # and if that number is greater than zero that we do not verify\n        # (now, to do that in python)\n        # if (mempool.OfType < ClaimTransaction > ().Where(p => p != this).SelectMany(p= > p.Claims).Intersect(Claims).Count() > 0)\n        # return false;\n\n        # im sorry about the below\n        otherclaimTxs = [tx for tx in mempool if tx is ClaimTransaction and tx is not self]\n        for other in otherclaimTxs:\n            # check to see if the length of the intersection between this objects claim's and the other txs claims is > 0\n            if len([list(filter(lambda x: x in self.Claims, otherClaims)) for otherClaims in other.Claims]):\n                return False\n\n        txResult = None\n        for tx in self.GetTransactionResults():\n            if tx.AssetId == Blockchain.SystemCoin().Hash:\n                txResult = tx\n                break\n\n        if txResult is None or txResult.Amount > Fixed8(0):\n            return False\n\n        try:\n            return Blockchain.CalculateBonusIgnoreClaimed(self.Claims, False) == -txResult.Amount\n\n        except Exception as e:\n            logger.error('Could not calculate bonus: %s ' % e)\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwait for a transaction to show up on blockchain.", "response": "def wait_for_tx(self, tx, max_seconds=120):\n    \"\"\" Wait for tx to show up on blockchain\n\n    Args:\n        tx (Transaction or UInt256 or str): Transaction or just the hash\n        max_seconds (float): maximum seconds to wait for tx to show up. default: 120\n\n    Returns:\n        True: if transaction was found\n\n    Raises:\n        AttributeError: if supplied tx is not Transaction or UInt256 or str\n        TxNotFoundInBlockchainError: if tx is not found in blockchain after max_seconds\n    \"\"\"\n    tx_hash = None\n    if isinstance(tx, (str, UInt256)):\n        tx_hash = str(tx)\n    elif isinstance(tx, Transaction):\n        tx_hash = tx.Hash.ToString()\n    else:\n        raise AttributeError(\"Supplied tx is type '%s', but must be Transaction or UInt256 or str\" % type(tx))\n\n    wait_event = Event()\n    time_start = time.time()\n\n    while True:\n        # Try to find transaction in blockchain\n        _tx, height = Blockchain.Default().GetTransaction(tx_hash)\n        if height > -1:\n            return True\n\n        # Using a wait event for the delay because it is not blocking like time.sleep()\n        wait_event.wait(3)\n\n        seconds_passed = time.time() - time_start\n        if seconds_passed > max_seconds:\n            raise TxNotFoundInBlockchainError(\"Transaction with hash %s not found after %s seconds\" % (tx_hash, int(seconds_passed)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a contract to the wallet.", "response": "def AddContract(self, contract):\n        \"\"\"\n        Add a contract to the wallet.\n\n        Args:\n            contract (Contract): a contract of type neo.SmartContract.Contract.\n\n        Raises:\n            Exception: Invalid operation - public key mismatch.\n        \"\"\"\n        if not contract.PublicKeyHash.ToBytes() in self._keys.keys():\n            raise Exception('Invalid operation - public key mismatch')\n\n        self._contracts[contract.ScriptHash.ToBytes()] = contract\n        if contract.ScriptHash in self._watch_only:\n            self._watch_only.remove(contract.ScriptHash)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a watch only address to the wallet.", "response": "def AddWatchOnly(self, script_hash):\n        \"\"\"\n        Add a watch only address to the wallet.\n\n        Args:\n            script_hash (UInt160): a bytearray (len 20) representing the public key.\n\n        Note:\n            Prints a warning to the console if the address already exists in the wallet.\n        \"\"\"\n        if script_hash in self._contracts:\n            logger.error(\"Address already in contracts\")\n            return\n\n        self._watch_only.append(script_hash)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a NEP - 5 compliant token to the wallet.", "response": "def AddNEP5Token(self, token):\n        \"\"\"\n        Add a NEP-5 compliant token to the wallet.\n\n        Args:\n            token (NEP5Token): an instance of type neo.Wallets.NEP5Token.\n\n        Note:\n            Prints a warning to the console if the token already exists in the wallet.\n        \"\"\"\n        if token.ScriptHash.ToBytes() in self._tokens.keys():\n            logger.error(\"Token already in wallet\")\n            return\n        self._tokens[token.ScriptHash.ToBytes()] = token"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nchange the password used to protect the private key.", "response": "def ChangePassword(self, password_old, password_new):\n        \"\"\"\n        Change the password used to protect the private key.\n\n        Args:\n            password_old (str): the current password used to encrypt the private key.\n            password_new (str): the new to be used password to encrypt the private key.\n\n        Returns:\n            bool: whether the password has been changed\n        \"\"\"\n        if not self.ValidatePassword(password_old):\n            return False\n\n        if isinstance(password_new, str):\n            password_new = password_new.encode('utf-8')\n\n        password_key = hashlib.sha256(password_new)\n        self.SaveStoredData(\"PasswordHash\", password_key)\n        self.SaveStoredData(\"MasterKey\", AES.new(self._master_key, AES.MODE_CBC, self._iv))\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ContainsKey(self, public_key):\n        return self.ContainsKeyHash(Crypto.ToScriptHash(public_key.encode_point(True), unhex=True))", "response": "Tests if the wallet contains the supplied public key."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndetermines if the wallet contains the given address.", "response": "def ContainsAddressStr(self, address):\n        \"\"\"\n        Determine if the wallet contains the address.\n\n        Args:\n            address (str): a string representing the public key.\n\n        Returns:\n            bool: True, if the address is present in the wallet. False otherwise.\n        \"\"\"\n        for key, contract in self._contracts.items():\n            if contract.Address == address:\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a KeyPair instance with the given private key.", "response": "def CreateKey(self, private_key=None):\n        \"\"\"\n        Create a KeyPair\n\n        Args:\n            private_key (iterable_of_ints): (optional) 32 byte private key\n\n        Returns:\n            KeyPair: a KeyPair instance\n        \"\"\"\n        if private_key is None:\n            private_key = bytes(Random.get_random_bytes(32))\n\n        key = KeyPair(priv_key=private_key)\n        self._keys[key.PublicKeyHash.ToBytes()] = key\n        return key"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef EncryptPrivateKey(self, decrypted):\n        aes = AES.new(self._master_key, AES.MODE_CBC, self._iv)\n        return aes.encrypt(decrypted)", "response": "Encrypts the provided plaintext with the initialized private key."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndelete an address from the wallet.", "response": "def DeleteAddress(self, script_hash):\n        \"\"\"\n        Deletes an address from the wallet (includes watch-only addresses).\n\n        Args:\n            script_hash (UInt160): a bytearray (len 20) representing the public key.\n\n        Returns:\n            tuple:\n                bool: True if address removed, False otherwise.\n                list: a list of any ``neo.Wallet.Coin`` objects to be removed from the wallet.\n        \"\"\"\n        coin_keys_toremove = []\n        coins_to_remove = []\n        for key, coinref in self._coins.items():\n            if coinref.Output.ScriptHash.ToBytes() == script_hash.ToBytes():\n                coin_keys_toremove.append(key)\n                coins_to_remove.append(coinref)\n\n        for k in coin_keys_toremove:\n            del self._coins[k]\n\n        ok = False\n        if script_hash.ToBytes() in self._contracts.keys():\n            ok = True\n            del self._contracts[script_hash.ToBytes()]\n        elif script_hash in self._watch_only:\n            ok = True\n            self._watch_only.remove(script_hash)\n\n        return ok, coins_to_remove"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsearch through the current collection of coins in a wallet and chooses coins that match the specified list of vins.", "response": "def FindCoinsByVins(self, vins):\n        \"\"\"\n        Looks through the current collection of coins in a wallet\n        and chooses coins that match the specified CoinReference objects.\n\n        Args:\n            vins: A list of ``neo.Core.CoinReference`` objects.\n\n        Returns:\n            list: A list of ``neo.Wallet.Coin`` objects.\n        \"\"\"\n        ret = []\n        for coin in self.GetCoins():\n            coinref = coin.Reference\n            for vin in vins:\n                if coinref.PrevIndex == vin.PrevIndex and \\\n                        coinref.PrevHash == vin.PrevHash:\n                    ret.append(coin)\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef FindUnspentCoins(self, from_addr=None, use_standard=False, watch_only_val=0):\n        ret = []\n        for coin in self.GetCoins():\n            if coin.State & CoinState.Confirmed > 0 and \\\n                    coin.State & CoinState.Spent == 0 and \\\n                    coin.State & CoinState.Locked == 0 and \\\n                    coin.State & CoinState.Frozen == 0 and \\\n                    coin.State & CoinState.WatchOnly == watch_only_val:\n\n                do_exclude = False\n                if self._vin_exclude:\n                    for to_exclude in self._vin_exclude:\n\n                        if coin.Reference.PrevIndex == to_exclude.PrevIndex and \\\n                                coin.Reference.PrevHash == to_exclude.PrevHash:\n                            do_exclude = True\n\n                if do_exclude:\n                    continue\n\n                if from_addr is not None:\n                    if coin.Output.ScriptHash == from_addr:\n                        ret.append(coin)\n                elif use_standard:\n\n                    contract = self._contracts[coin.Output.ScriptHash.ToBytes()]\n                    if contract.IsStandard:\n                        ret.append(coin)\n                else:\n                    ret.append(coin)\n\n        return ret", "response": "Returns a list of unspent coins in the wallet."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef FindUnspentCoinsByAsset(self, asset_id, from_addr=None, use_standard=False, watch_only_val=0):\n        coins = self.FindUnspentCoins(from_addr=from_addr, use_standard=use_standard, watch_only_val=watch_only_val)\n\n        return [coin for coin in coins if coin.Output.AssetId == asset_id]", "response": "Returns a list of unspent coin objects in the wallet limited to those of a certain asset type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding unspent coin objects totalling a requested value in the wallet limited to those of a certain asset type. Args: asset_id (UInt256): a bytearray (len 32) representing an asset on the blockchain. amount (int): the amount of unspent coins that are being requested. from_addr (UInt160): a bytearray (len 20) representing an address. use_standard (bool): whether or not to only include standard contracts ( i.e not a smart contract addr ). watch_only_val (int): a flag ( 0 or 64 ) indicating whether or not to find coins that are in 'watch only' addresses. Returns: list: a list of ``neo.Wallet.Coin`` in the wallet that are not spent. this list is empty if there are not enough coins to satisfy the request.", "response": "def FindUnspentCoinsByAssetAndTotal(self, asset_id, amount, from_addr=None, use_standard=False, watch_only_val=0, reverse=False):\n        \"\"\"\n        Finds unspent coin objects totalling a requested value in the wallet limited to those of a certain asset type.\n\n        Args:\n            asset_id (UInt256): a bytearray (len 32) representing an asset on the blockchain.\n            amount (int): the amount of unspent coins that are being requested.\n            from_addr (UInt160): a bytearray (len 20) representing an address.\n            use_standard (bool): whether or not to only include standard contracts ( i.e not a smart contract addr ).\n            watch_only_val (int): a flag ( 0 or 64 ) indicating whether or not to find coins that are in 'watch only' addresses.\n\n        Returns:\n            list: a list of ``neo.Wallet.Coin`` in the wallet that are not spent. this list is empty if there are not enough coins to satisfy the request.\n        \"\"\"\n        coins = self.FindUnspentCoinsByAsset(asset_id, from_addr=from_addr,\n                                             use_standard=use_standard, watch_only_val=watch_only_val)\n\n        sum = Fixed8(0)\n\n        for coin in coins:\n            sum = sum + coin.Output.Value\n\n        if sum < amount:\n            return None\n\n        coins = sorted(coins, key=lambda coin: coin.Output.Value.value)\n\n        if reverse:\n            coins.reverse()\n\n        total = Fixed8(0)\n\n        # go through all coins, see if one is an exact match. then we'll use that\n        for coin in coins:\n            if coin.Output.Value == amount:\n                return [coin]\n\n        to_ret = []\n        for coin in coins:\n            total = total + coin.Output.Value\n            to_ret.append(coin)\n            if total >= amount:\n                break\n\n        return to_ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of unclaimed coins in the wallet that have not been claimed or redeemed for their gas value on the blockchain.", "response": "def GetUnclaimedCoins(self):\n        \"\"\"\n        Gets coins in the wallet that have not been 'claimed', or redeemed for their gas value on the blockchain.\n\n        Returns:\n            list: a list of ``neo.Wallet.Coin`` that have 'claimable' value\n        \"\"\"\n        unclaimed = []\n\n        neo = Blockchain.SystemShare().Hash\n\n        for coin in self.GetCoins():\n            if coin.Output.AssetId == neo and \\\n                    coin.State & CoinState.Confirmed > 0 and \\\n                    coin.State & CoinState.Spent > 0 and \\\n                    coin.State & CoinState.Claimed == 0 and \\\n                    coin.State & CoinState.Frozen == 0 and \\\n                    coin.State & CoinState.WatchOnly == 0:\n                unclaimed.append(coin)\n\n        return unclaimed"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the total amount of Gas that this wallet can claim at a given moment.", "response": "def GetAvailableClaimTotal(self):\n        \"\"\"\n        Gets the total amount of Gas that this wallet is able to claim at a given moment.\n\n        Returns:\n            Fixed8: the amount of Gas available to claim as a Fixed8 number.\n        \"\"\"\n        coinrefs = [coin.Reference for coin in self.GetUnclaimedCoins()]\n        bonus = Blockchain.CalculateBonusIgnoreClaimed(coinrefs, True)\n        return bonus"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef GetUnavailableBonus(self):\n        height = Blockchain.Default().Height + 1\n        unspents = self.FindUnspentCoinsByAsset(Blockchain.SystemShare().Hash)\n        refs = [coin.Reference for coin in unspents]\n        try:\n            unavailable_bonus = Blockchain.CalculateBonus(refs, height_end=height)\n            return unavailable_bonus\n        except Exception as e:\n            pass\n        return Fixed8(0)", "response": "Gets the total claimable amount of Gas that is not available to claim to claim\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the KeyPair object for the given public key hash.", "response": "def GetKey(self, public_key_hash):\n        \"\"\"\n        Get the KeyPair belonging to the public key hash.\n\n        Args:\n            public_key_hash (UInt160): a public key hash to get the KeyPair for.\n\n        Returns:\n            KeyPair: If successful, the KeyPair belonging to the public key hash, otherwise None\n        \"\"\"\n        if public_key_hash.ToBytes() in self._keys.keys():\n            return self._keys[public_key_hash.ToBytes()]\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef GetKeyByScriptHash(self, script_hash):\n        contract = self.GetContract(script_hash)\n        if contract:\n            return self.GetKey(contract.PublicKeyHash)\n        return None", "response": "Returns the KeyPair object corresponding to the script hash."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the balance of the specified token.", "response": "def GetTokenBalance(self, token, watch_only=0):\n        \"\"\"\n        Get the balance of the specified token.\n\n        Args:\n            token (NEP5Token): an instance of type neo.Wallets.NEP5Token to get the balance from.\n            watch_only (bool): True, to limit to watch only wallets.\n\n        Returns:\n            Decimal: total balance for `token`.\n        \"\"\"\n        total = Decimal(0)\n\n        if watch_only > 0:\n            for addr in self._watch_only:\n                balance = token.GetBalance(self, addr)\n                total += balance\n        else:\n            for contract in self._contracts.values():\n                balance = token.GetBalance(self, contract.Address)\n                total += balance\n        return total"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the balance of a specific token by its asset id.", "response": "def GetBalance(self, asset_id, watch_only=0):\n        \"\"\"\n        Get the balance of a specific token by its asset id.\n\n        Args:\n            asset_id (NEP5Token|TransactionOutput): an instance of type neo.Wallets.NEP5Token or neo.Core.TX.Transaction.TransactionOutput to get the balance from.\n            watch_only (bool): True, to limit to watch only wallets.\n\n        Returns:\n            Fixed8: total balance.\n        \"\"\"\n        total = Fixed8(0)\n\n        if type(asset_id) is NEP5Token.NEP5Token:\n            return self.GetTokenBalance(asset_id, watch_only)\n\n        for coin in self.GetCoins():\n            if coin.Output.AssetId == asset_id:\n                if coin.State & CoinState.Confirmed > 0 and \\\n                        coin.State & CoinState.Spent == 0 and \\\n                        coin.State & CoinState.Locked == 0 and \\\n                        coin.State & CoinState.Frozen == 0 and \\\n                        coin.State & CoinState.WatchOnly == watch_only:\n                    total = total + coin.Output.Value\n\n        return total"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ProcessBlocks(self, block_limit=1000):\n        self._lock.acquire()\n        try:\n            blockcount = 0\n            while self._current_height <= Blockchain.Default().Height and (block_limit == 0 or blockcount < block_limit):\n\n                block = Blockchain.Default().GetBlockByHeight(self._current_height)\n\n                if block is not None:\n                    self.ProcessNewBlock(block)\n                else:\n                    self._current_height += 1\n\n                blockcount += 1\n\n            self.SaveStoredData(\"Height\", self._current_height)\n        except Exception as e:\n            logger.warn(\"Could not process ::: %s \" % e)\n        finally:\n            self._lock.release()", "response": "This method processes the blocks in the wallet."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ProcessNewBlock(self, block):\n        added = set()\n        changed = set()\n        deleted = set()\n\n        try:\n            # go through the list of transactions in the block and enumerate\n            # over their outputs\n            for tx in block.FullTransactions:\n\n                for index, output in enumerate(tx.outputs):\n\n                    # check to see if the outputs in the tx are in this wallet\n                    state = self.CheckAddressState(output.ScriptHash)\n\n                    if state & AddressState.InWallet > 0:\n\n                        # if it's in the wallet, check to see if the coin exists yet\n                        key = CoinReference(tx.Hash, index)\n\n                        # if it exists, update it, otherwise create a new one\n                        if key in self._coins.keys():\n                            coin = self._coins[key]\n                            coin.State |= CoinState.Confirmed\n                            changed.add(coin)\n                        else:\n                            newcoin = Coin.CoinFromRef(coin_ref=key, tx_output=output, state=CoinState.Confirmed, transaction=tx)\n                            self._coins[key] = newcoin\n                            added.add(newcoin)\n\n                        if state & AddressState.WatchOnly > 0:\n                            self._coins[key].State |= CoinState.WatchOnly\n                            changed.add(self._coins[key])\n\n            # now iterate over the inputs of the tx and do the same\n            for tx in block.FullTransactions:\n\n                for input in tx.inputs:\n\n                    if input in self._coins.keys():\n                        if self._coins[input].Output.AssetId == Blockchain.SystemShare().Hash:\n                            coin = self._coins[input]\n                            coin.State |= CoinState.Spent | CoinState.Confirmed\n                            changed.add(coin)\n\n                        else:\n                            deleted.add(self._coins[input])\n                            del self._coins[input]\n\n            for claimTx in [tx for tx in block.Transactions if tx.Type == TransactionType.ClaimTransaction]:\n\n                for ref in claimTx.Claims:\n                    if ref in self._coins.keys():\n                        deleted.add(self._coins[ref])\n                        del self._coins[ref]\n\n            # update the current height of the wallet\n            self._current_height += 1\n\n            # in the case that another wallet implementation needs to do something\n            # with the coins that have been changed ( ie persist to db ) this\n            # method is called\n            self.OnProcessNewBlock(block, added, changed, deleted)\n\n            # this is not necessary at the moment, but any outside process\n            # that wants to subscribe to the balance changed event could do\n            # so from the BalanceChanged method\n            if len(added) + len(deleted) + len(changed) > 0:\n                self.BalanceChanged()\n\n        except Exception as e:\n            traceback.print_stack()\n            traceback.print_exc()\n            logger.error(\"could not process %s \" % e)", "response": "Processes a new block and returns a set of related objects."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nverifies if a transaction belongs to the wallet.", "response": "def IsWalletTransaction(self, tx):\n        \"\"\"\n        Verifies if a transaction belongs to the wallet.\n\n        Args:\n            tx (TransactionOutput):an instance of type neo.Core.TX.Transaction.TransactionOutput to verify.\n\n        Returns:\n            bool: True, if transaction belongs to wallet. False, if not.\n        \"\"\"\n        for key, contract in self._contracts.items():\n\n            for output in tx.outputs:\n                if output.ScriptHash.ToBytes() == contract.ScriptHash.ToBytes():\n                    return True\n\n            for script in tx.scripts:\n\n                if script.VerificationScript:\n                    if bytes(contract.Script) == script.VerificationScript:\n                        return True\n\n        for watch_script_hash in self._watch_only:\n            for output in tx.outputs:\n                if output.ScriptHash == watch_script_hash:\n                    return True\n            for script in tx.scripts:\n                if Crypto.ToScriptHash(script.VerificationScript, unhex=False) == watch_script_hash:\n                    return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks the address state of the provided script hash.", "response": "def CheckAddressState(self, script_hash):\n        \"\"\"\n        Determine the address state of the provided script hash.\n\n        Args:\n            script_hash (UInt160): a script hash to determine the address state of.\n\n        Returns:\n            AddressState: the address state.\n        \"\"\"\n        for key, contract in self._contracts.items():\n            if contract.ScriptHash.ToBytes() == script_hash.ToBytes():\n                return AddressState.InWallet\n        for watch in self._watch_only:\n            if watch == script_hash:\n                return AddressState.InWallet | AddressState.WatchOnly\n        return AddressState.NoState"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ToScriptHash(self, address):\n        if len(address) == 34:\n            if address[0] == 'A':\n                data = b58decode(address)\n                if data[0] != self.AddressVersion:\n                    raise ValueError('Not correct Coin Version')\n\n                checksum = Crypto.Default().Hash256(data[:21])[:4]\n                if checksum != data[21:]:\n                    raise Exception('Address format error')\n                return UInt160(data=data[1:21])\n            else:\n                raise Exception('Address format error')\n        else:\n            raise ValueError('Not correct Address, wrong length.')", "response": "Converts an address string to a script hash."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nvalidating if the provided password matches with the stored password.", "response": "def ValidatePassword(self, password):\n        \"\"\"\n        Validates if the provided password matches with the stored password.\n\n        Args:\n            password (string): a password.\n\n        Returns:\n            bool: the provided password matches with the stored password.\n        \"\"\"\n        password = to_aes_key(password)\n        return hashlib.sha256(password).digest() == self.LoadStoredData('PasswordHash')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef GetStandardAddress(self):\n        for contract in self._contracts.values():\n            if contract.IsStandard:\n                return contract.ScriptHash\n\n        raise Exception(\"Could not find a standard contract address\")", "response": "Get the Wallet s default address."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the address where a change is sent to.", "response": "def GetChangeAddress(self, from_addr=None):\n        \"\"\"\n        Get the address where change is send to.\n\n        Args:\n            from_address (UInt160): (optional) from address script hash.\n\n        Raises:\n            Exception: if change address could not be found.\n\n        Returns:\n            UInt160: script hash.\n        \"\"\"\n        if from_addr is not None:\n            for contract in self._contracts.values():\n                if contract.ScriptHash == from_addr:\n                    return contract.ScriptHash\n\n        for contract in self._contracts.values():\n            if contract.IsStandard:\n                return contract.ScriptHash\n\n        if len(self._contracts.values()):\n            for k, v in self._contracts.items():\n                return v\n\n        raise Exception(\"Could not find change address\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef GetDefaultContract(self):\n        try:\n            return self.GetContracts()[0]\n        except Exception as e:\n            logger.error(\"Could not find default contract: %s\" % str(e))\n            raise", "response": "Get the default contract."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef GetCoinAssets(self):\n        assets = set()\n        for coin in self.GetCoins():\n            assets.add(coin.Output.AssetId)\n        return list(assets)", "response": "Get asset ids of all coins present in the wallet."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a Contract object for the given script hash.", "response": "def GetContract(self, script_hash):\n        \"\"\"\n        Get contract for specified script_hash.\n\n        Args:\n            script_hash (UInt160): a bytearray (len 20).\n\n        Returns:\n            Contract: if a contract was found matching the provided script hash, otherwise None\n        \"\"\"\n        if script_hash.ToBytes() in self._contracts.keys():\n            return self._contracts[script_hash.ToBytes()]\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef SaveTransaction(self, tx):\n        coins = self.GetCoins()\n        changed = []\n        added = []\n        deleted = []\n        found_coin = False\n        for input in tx.inputs:\n            coin = None\n\n            for coinref in coins:\n                test_coin = coinref.Reference\n                if test_coin == input:\n                    coin = coinref\n\n            if coin is None:\n                return False\n            if coin.State & CoinState.Spent > 0:\n                return False\n            elif coin.State & CoinState.Confirmed == 0:\n                return False\n\n            coin.State |= CoinState.Spent\n            coin.State &= ~CoinState.Confirmed\n            changed.append(coin)\n\n        for index, output in enumerate(tx.outputs):\n\n            state = self.CheckAddressState(output.ScriptHash)\n\n            key = CoinReference(tx.Hash, index)\n\n            if state & AddressState.InWallet > 0:\n                newcoin = Coin.CoinFromRef(coin_ref=key, tx_output=output, state=CoinState.Unconfirmed)\n                self._coins[key] = newcoin\n\n                if state & AddressState.WatchOnly > 0:\n                    newcoin.State |= CoinState.WatchOnly\n\n                added.append(newcoin)\n\n        if isinstance(tx, ClaimTransaction):\n            # do claim stuff\n            for claim in tx.Claims:\n                claim_coin = self._coins[claim]\n                claim_coin.State |= CoinState.Claimed\n                claim_coin.State &= ~CoinState.Confirmed\n                changed.append(claim_coin)\n\n        self.OnSaveTransaction(tx, added, changed, deleted)\n\n        return True", "response": "This method is used to update the state of the coins that have been made by this wallet."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsigning the verifiable items in the context with the Keypairs in this wallet.", "response": "def Sign(self, context):\n        \"\"\"\n        Sign the verifiable items ( Transaction, Block, etc ) in the context with the Keypairs in this wallet.\n\n        Args:\n            context (ContractParameterContext): the context to sign.\n\n        Returns:\n            bool: if signing is successful for all contracts in this wallet.\n        \"\"\"\n        success = False\n\n        for hash in context.ScriptHashes:\n\n            contract = self.GetContract(hash)\n            if contract is None:\n                logger.info(\n                    f\"Cannot find key belonging to script_hash {hash}. Make sure the source address you're trying to sign the transaction for is imported in the wallet.\")\n                continue\n\n            key = self.GetKeyByScriptHash(hash)\n\n            if key is None:\n                continue\n\n            signature = Helper.Sign(context.Verifiable, key)\n\n            res = context.AddSignature(contract, key.PublicKey, signature)\n\n            success |= res\n\n        return success"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef SignMessage(self, message, script_hash):\n\n        keypair = self.GetKeyByScriptHash(script_hash)\n        prikey = bytes(keypair.PrivateKey)\n        res = Crypto.Default().Sign(message, prikey)\n        return res, keypair.PublicKey", "response": "Signs a message with a specified script_hash."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of synced balances.", "response": "def GetSyncedBalances(self):\n        \"\"\"\n        Returns a list of synced balances. The list looks like this:\n        [('NEO', 100.0), ('NEOGas', 100.0)]\n\n        Returns\n            list: [(asset_name, amount), ...]\n        \"\"\"\n        assets = self.GetCoinAssets()\n        balances = []\n        for asset in assets:\n            if type(asset) is UInt256:\n                bc_asset = Blockchain.Default().GetAssetState(asset.ToBytes())\n                total = self.GetBalance(asset).value / Fixed8.D\n                balances.append((bc_asset.GetName(), total))\n            elif type(asset) is NEP5Token.NEP5Token:\n                balances.append((asset.symbol, self.GetBalance(asset)))\n        return balances"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef IsSynced(self):\n        if Blockchain.Default().Height == 0:\n            return False\n\n        if (int(100 * self._current_height / Blockchain.Default().Height)) < 100:\n            return False\n        else:\n            return True", "response": "Check if wallet is synced."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef GetStream(data=None):\n        if len(__mstreams_available__) == 0:\n            if data:\n                mstream = MemoryStream(data)\n                mstream.seek(0)\n            else:\n                mstream = MemoryStream()\n            __mstreams__.append(mstream)\n            return mstream\n\n        mstream = __mstreams_available__.pop()\n\n        if data is not None and len(data):\n            mstream.Cleanup()\n            mstream.write(data)\n\n        mstream.seek(0)\n\n        return mstream", "response": "Get a MemoryStream instance from the given data."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef Create(path, password, generate_default_key=True):\n        wallet = UserWallet(path=path, passwordKey=password, create=True)\n        if generate_default_key:\n            wallet.CreateKey()\n        return wallet", "response": "Create a new user wallet."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef CreateKey(self, prikey=None):\n        account = super(UserWallet, self).CreateKey(private_key=prikey)\n        self.OnCreateAccount(account)\n        contract = WalletContract.CreateSignatureContract(account.PublicKey)\n        self.AddContract(contract)\n        return account", "response": "Create a KeyPair and store it encrypted in the database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncall when a new account is created.", "response": "def OnCreateAccount(self, account):\n        \"\"\"\n        Save a KeyPair in encrypted form into the database.\n\n        Args:\n            account (KeyPair):\n        \"\"\"\n        pubkey = account.PublicKey.encode_point(False)\n        pubkeyunhex = binascii.unhexlify(pubkey)\n        pub = pubkeyunhex[1:65]\n\n        priv = bytearray(account.PrivateKey)\n        decrypted = pub + priv\n        encrypted_pk = self.EncryptPrivateKey(bytes(decrypted))\n\n        db_account, created = Account.get_or_create(\n            PrivateKeyEncrypted=encrypted_pk, PublicKeyHash=account.PublicKeyHash.ToBytes())\n        db_account.save()\n        self.__dbaccount = db_account"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef AddContract(self, contract):\n        super(UserWallet, self).AddContract(contract)\n\n        try:\n            db_contract = Contract.get(ScriptHash=contract.ScriptHash.ToBytes())\n            db_contract.delete_instance()\n        except Exception as e:\n            logger.debug(\"contract does not exist yet\")\n\n        sh = bytes(contract.ScriptHash.ToArray())\n        address, created = Address.get_or_create(ScriptHash=sh)\n        address.IsWatchOnly = False\n        address.save()\n        db_contract = Contract.create(RawData=contract.ToArray(),\n                                      ScriptHash=contract.ScriptHash.ToBytes(),\n                                      PublicKeyHash=contract.PublicKeyHash.ToBytes(),\n                                      Address=address,\n                                      Account=self.__dbaccount)\n\n        logger.debug(\"Creating db contract %s \" % db_contract)\n\n        db_contract.save()", "response": "Add a contract to the database."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef Disconnect(self, reason=None, isDead=True):\n        self.disconnecting = True\n        self.expect_verack_next = False\n        if reason:\n            logger.debug(f\"Disconnecting with reason: {reason}\")\n        self.stop_block_loop()\n        self.stop_header_loop()\n        self.stop_peerinfo_loop()\n        if isDead:\n            self.leader.AddDeadAddress(self.address, reason=f\"{self.prefix} Forced disconnect by us\")\n\n        self.leader.forced_disconnect_by_us += 1\n\n        self.disconnect_deferred = defer.Deferred()\n        self.disconnect_deferred.debug = True\n        # force disconnection without waiting on the other side\n        # calling later to give func caller time to add callbacks to the deferred\n        reactor.callLater(1, self.transport.abortConnection)\n        return self.disconnect_deferred", "response": "Disconnects from the remote node."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the peer name.", "response": "def Name(self):\n        \"\"\"\n        Get the peer name.\n\n        Returns:\n            str:\n        \"\"\"\n        name = \"\"\n        if self.Version:\n            name = self.Version.UserAgent\n        return name"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef GetNetworkAddressWithTime(self):\n        if self.port is not None and self.host is not None and self.Version is not None:\n            return NetworkAddressWithTime(self.host, self.port, self.Version.Services)\n        return None", "response": "Returns a NetworkAddress object with the time of the connection."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef connectionLost(self, reason=None):\n        try:\n            self.connected = False\n            self.stop_block_loop()\n            self.stop_peerinfo_loop()\n            self.stop_header_loop()\n\n            self.ReleaseBlockRequests()\n            self.leader.RemoveConnectedPeer(self)\n\n            time_expired = self.time_expired(HEARTBEAT_BLOCKS)\n            # some NEO-cli versions have a 30s timeout to receive block/consensus or tx messages. By default neo-python doesn't respond to these requests\n            if time_expired > 20:\n                self.address.last_connection = Address.Now()\n                self.leader.AddDeadAddress(self.address, reason=f\"{self.prefix} Premature disconnect\")\n\n            if reason and reason.check(twisted_error.ConnectionDone):\n                # this might happen if they close our connection because they've reached max peers or something similar\n                logger.debug(f\"{self.prefix} disconnected normally with reason:{reason.value}\")\n                self._check_for_consecutive_disconnects(\"connection done\")\n\n            elif reason and reason.check(twisted_error.ConnectionLost):\n                # Can be due to a timeout. Only if this happened again within 5 minutes do we label the node as bad\n                # because then it clearly doesn't want to talk to us or we have a bad connection to them.\n                # Otherwise allow for the node to be queued again by NodeLeader.\n                logger.debug(f\"{self.prefix} disconnected with connectionlost reason: {reason.value}\")\n                self._check_for_consecutive_disconnects(\"connection lost\")\n\n            else:\n                logger.debug(f\"{self.prefix} disconnected with reason: {reason.value}\")\n        except Exception as e:\n            logger.error(\"Error with connection lost: %s \" % e)\n\n        def try_me(err):\n            err.check(error.ConnectionAborted)\n\n        if self.disconnect_deferred:\n            d, self.disconnect_deferred = self.disconnect_deferred, None  # type: defer.Deferred\n            d.addErrback(try_me)\n            if len(d.callbacks) > 0:\n                d.callback(reason)\n            else:\n                print(\"connLost, disconnect_deferred cancelling!\")\n                d.cancel()", "response": "Callback handler from twisted when a connection was lost."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dataReceived(self, data):\n        self.bytes_in += (len(data))\n        self.buffer_in = self.buffer_in + data\n\n        while self.CheckDataReceived():\n            pass", "response": "Called from Twisted when data is received."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntry to extract a Message from the data buffer and process it.", "response": "def CheckDataReceived(self):\n        \"\"\"Tries to extract a Message from the data buffer and process it.\"\"\"\n        currentLength = len(self.buffer_in)\n        if currentLength < 24:\n            return False\n        # Extract the message header from the buffer, and return if not enough\n        # buffer to fully deserialize the message object.\n\n        try:\n            # Construct message\n            mstart = self.buffer_in[:24]\n            ms = StreamManager.GetStream(mstart)\n            reader = BinaryReader(ms)\n            m = Message()\n\n            # Extract message metadata\n            m.Magic = reader.ReadUInt32()\n            m.Command = reader.ReadFixedString(12).decode('utf-8')\n            m.Length = reader.ReadUInt32()\n            m.Checksum = reader.ReadUInt32()\n\n            # Return if not enough buffer to fully deserialize object.\n            messageExpectedLength = 24 + m.Length\n            if currentLength < messageExpectedLength:\n                return False\n\n        except Exception as e:\n            logger.debug(f\"{self.prefix} Error: could not read message header from stream {e}\")\n            # self.Log('Error: Could not read initial bytes %s ' % e)\n            return False\n\n        finally:\n            StreamManager.ReleaseStream(ms)\n            del reader\n\n        # The message header was successfully extracted, and we have enough enough buffer\n        # to extract the full payload\n        try:\n            # Extract message bytes from buffer and truncate buffer\n            mdata = self.buffer_in[:messageExpectedLength]\n            self.buffer_in = self.buffer_in[messageExpectedLength:]\n\n            # Deserialize message with payload\n            stream = StreamManager.GetStream(mdata)\n            reader = BinaryReader(stream)\n            message = Message()\n            message.Deserialize(reader)\n\n            if self.incoming_client and self.expect_verack_next:\n                if message.Command != 'verack':\n                    self.Disconnect(\"Expected 'verack' got {}\".format(message.Command))\n\n            # Propagate new message\n            self.MessageReceived(message)\n\n        except Exception as e:\n            logger.debug(f\"{self.prefix} Could not extract message {e}\")\n            # self.Log('Error: Could not extract message: %s ' % e)\n            return False\n\n        finally:\n            StreamManager.ReleaseStream(stream)\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef MessageReceived(self, m):\n        if m.Command == 'verack':\n            # only respond with a verack when we connect to another client, not when a client connected to us or\n            # we might end up in a verack loop\n            if self.incoming_client:\n                if self.expect_verack_next:\n                    self.expect_verack_next = False\n            else:\n                self.HandleVerack()\n        elif m.Command == 'version':\n            self.HandleVersion(m.Payload)\n        elif m.Command == 'getaddr':\n            self.SendPeerInfo()\n        elif m.Command == 'getdata':\n            self.HandleGetDataMessageReceived(m.Payload)\n        elif m.Command == 'getblocks':\n            self.HandleGetBlocksMessageReceived(m.Payload)\n        elif m.Command == 'inv':\n            self.HandleInvMessage(m.Payload)\n        elif m.Command == 'block':\n            self.HandleBlockReceived(m.Payload)\n        elif m.Command == 'getheaders':\n            self.HandleGetHeadersMessageReceived(m.Payload)\n        elif m.Command == 'headers':\n            self.HandleBlockHeadersReceived(m.Payload)\n        elif m.Command == 'addr':\n            self.HandlePeerInfoReceived(m.Payload)\n        else:\n            logger.debug(f\"{self.prefix} Command not implemented: {m.Command}\")", "response": "Process a message received from the broker."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprocessing response of self. RequestPeerInfo.", "response": "def HandlePeerInfoReceived(self, payload):\n        \"\"\"Process response of `self.RequestPeerInfo`.\"\"\"\n        addrs = IOHelper.AsSerializableWithType(payload, 'neo.Network.Payloads.AddrPayload.AddrPayload')\n\n        if not addrs:\n            return\n\n        for nawt in addrs.NetworkAddressesWithTime:\n            self.leader.RemoteNodePeerReceived(nawt.Address, nawt.Port, self.prefix)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef SendVersion(self):\n        m = Message(\"version\", VersionPayload(settings.NODE_PORT, self.remote_nodeid, settings.VERSION_NAME))\n        self.SendSerializedMessage(m)", "response": "Send our client version."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef HandleVersion(self, payload):\n        self.Version = IOHelper.AsSerializableWithType(payload, \"neo.Network.Payloads.VersionPayload.VersionPayload\")\n\n        if not self.Version:\n            return\n\n        if self.incoming_client:\n            if self.Version.Nonce == self.nodeid:\n                self.Disconnect()\n            self.SendVerack()\n        else:\n            self.nodeid = self.Version.Nonce\n            self.SendVersion()", "response": "Process the response of self. RequestVersion."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nhandle the verack response.", "response": "def HandleVerack(self):\n        \"\"\"Handle the `verack` response.\"\"\"\n        m = Message('verack')\n        self.SendSerializedMessage(m)\n        self.leader.NodeCount += 1\n        self.identifier = self.leader.NodeCount\n        logger.debug(f\"{self.prefix} Handshake complete!\")\n        self.handshake_complete = True\n        self.ProtocolReady()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef HandleInvMessage(self, payload):\n\n        if self.sync_mode != MODE_MAINTAIN:\n            return\n\n        inventory = IOHelper.AsSerializableWithType(payload, 'neo.Network.Payloads.InvPayload.InvPayload')\n        if not inventory:\n            return\n\n        if inventory.Type == InventoryType.BlockInt:\n\n            ok_hashes = []\n            for hash in inventory.Hashes:\n                hash = hash.encode('utf-8')\n                if hash not in self.myblockrequests and hash not in BC.Default().BlockRequests:\n                    ok_hashes.append(hash)\n                    BC.Default().BlockRequests.add(hash)\n                    self.myblockrequests.add(hash)\n            if len(ok_hashes):\n                message = Message(\"getdata\", InvPayload(InventoryType.Block, ok_hashes))\n                self.SendSerializedMessage(message)\n\n        elif inventory.Type == InventoryType.TXInt:\n            pass\n        elif inventory.Type == InventoryType.ConsensusInt:\n            pass", "response": "Process an inv message."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef SendSerializedMessage(self, message):\n        try:\n            ba = Helper.ToArray(message)\n            ba2 = binascii.unhexlify(ba)\n            self.bytes_out += len(ba2)\n            self.transport.write(ba2)\n        except Exception as e:\n            logger.debug(f\"Could not send serialized message {e}\")", "response": "Send the serialized message to the remote client."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprocessing a block header inventory payload.", "response": "def HandleBlockHeadersReceived(self, inventory):\n        \"\"\"\n        Process a block header inventory payload.\n\n        Args:\n            inventory (neo.Network.Inventory):\n        \"\"\"\n        try:\n            inventory = IOHelper.AsSerializableWithType(inventory, 'neo.Network.Payloads.HeadersPayload.HeadersPayload')\n            if inventory is not None:\n                logger.debug(f\"{self.prefix} received headers\")\n                self.heart_beat(HEARTBEAT_HEADERS)\n                BC.Default().AddHeaders(inventory.Headers)\n\n        except Exception as e:\n            logger.debug(f\"Error handling Block headers {e}\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprocessing a received block from the broker.", "response": "def HandleBlockReceived(self, inventory):\n        \"\"\"\n        Process a Block inventory payload.\n\n        Args:\n            inventory (neo.Network.Inventory):\n        \"\"\"\n        block = IOHelper.AsSerializableWithType(inventory, 'neo.Core.Block.Block')\n        if not block:\n            return\n\n        blockhash = block.Hash.ToBytes()\n        try:\n            if blockhash in BC.Default().BlockRequests:\n                BC.Default().BlockRequests.remove(blockhash)\n        except KeyError:\n            pass\n        try:\n            if blockhash in self.myblockrequests:\n                # logger.debug(f\"{self.prefix} received block: {block.Index}\")\n                self.heart_beat(HEARTBEAT_BLOCKS)\n                self.myblockrequests.remove(blockhash)\n        except KeyError:\n            pass\n        self.leader.InventoryReceived(block)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprocessing a get data message received from the leader.", "response": "def HandleGetDataMessageReceived(self, payload):\n        \"\"\"\n        Process a InvPayload payload.\n\n        Args:\n            payload (neo.Network.Inventory):\n        \"\"\"\n        inventory = IOHelper.AsSerializableWithType(payload, 'neo.Network.Payloads.InvPayload.InvPayload')\n        if not inventory:\n            return\n\n        for hash in inventory.Hashes:\n            hash = hash.encode('utf-8')\n\n            item = None\n            # try to get the inventory to send from relay cache\n\n            if hash in self.leader.RelayCache.keys():\n                item = self.leader.RelayCache[hash]\n\n            if inventory.Type == InventoryType.TXInt:\n                if not item:\n                    item, index = BC.Default().GetTransaction(hash)\n                if not item:\n                    item = self.leader.GetTransaction(hash)\n                if item:\n                    message = Message(command='tx', payload=item, print_payload=False)\n                    self.SendSerializedMessage(message)\n\n            elif inventory.Type == InventoryType.BlockInt:\n                if not item:\n                    item = BC.Default().GetBlock(hash)\n                if item:\n                    message = Message(command='block', payload=item, print_payload=False)\n                    self.SendSerializedMessage(message)\n\n            elif inventory.Type == InventoryType.ConsensusInt:\n                if item:\n                    self.SendSerializedMessage(Message(command='consensus', payload=item, print_payload=False))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef HandleGetBlocksMessageReceived(self, payload):\n        if not self.leader.ServiceEnabled:\n            return\n\n        inventory = IOHelper.AsSerializableWithType(payload, 'neo.Network.Payloads.GetBlocksPayload.GetBlocksPayload')\n        if not inventory:\n            return\n\n        blockchain = BC.Default()\n        hash = inventory.HashStart[0]\n        if not blockchain.GetHeader(hash):\n            return\n\n        hashes = []\n        hcount = 0\n        while hash != inventory.HashStop and hcount < 500:\n            hash = blockchain.GetNextBlockHash(hash)\n            if hash is None:\n                break\n            hashes.append(hash)\n            hcount += 1\n        if hcount > 0:\n            self.SendSerializedMessage(Message('inv', InvPayload(type=InventoryType.Block, hashes=hashes)))", "response": "Process a GetBlocksPayload payload."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrap the inventory in an InvPayload object and send it over the remote node.", "response": "def Relay(self, inventory):\n        \"\"\"\n        Wrap the inventory in a InvPayload object and send it over the write to the remote node.\n\n        Args:\n            inventory:\n\n        Returns:\n            bool: True (fixed)\n        \"\"\"\n        inventory = InvPayload(type=inventory.InventoryType, hashes=[inventory.Hash.ToBytes()])\n        m = Message(\"inv\", inventory)\n        self.SendSerializedMessage(m)\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef Size(self):\n        return s.uint32 + 12 + s.uint32 + s.uint32 + len(self.Payload)", "response": "Returns the total size in bytes of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef Deserialize(self, reader):\n        self.Magic = reader.ReadUInt32()\n        self.Command = reader.ReadFixedString(12).decode('utf-8')\n        self.Length = reader.ReadUInt32()\n\n        if self.Length > self.PayloadMaxSizeInt:\n            raise Exception(\"invalid format- payload too large\")\n\n        self.Checksum = reader.ReadUInt32()\n        self.Payload = reader.ReadBytes(self.Length)\n\n        checksum = Message.GetChecksum(self.Payload)\n\n        if checksum != self.Checksum:\n            raise ChecksumException(\"checksum mismatch\")", "response": "Deserialize full object.\n\n        Args:\n            reader (neo.IO.BinaryReader):"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef Serialize(self, writer):\n        writer.WriteUInt32(self.Magic)\n        writer.WriteFixedString(self.Command, 12)\n        writer.WriteUInt32(len(self.Payload))\n        writer.WriteUInt32(self.Checksum)\n        writer.WriteBytes(self.Payload)", "response": "Serialize object.\n\n        Args:\n            writer (neo.IO.BinaryWriter):"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfiltering out duplicate Script TransactionAttributeUsage types.", "response": "def make_unique_script_attr(attributes):\n    \"\"\"\n    Filter out duplicate `Script` TransactionAttributeUsage types.\n    Args:\n        attributes: a list of TransactionAttribute's\n\n    Returns:\n        list:\n    \"\"\"\n    filtered_attr = []\n    script_list = []\n    for attr in attributes:\n        if attr.Usage != TransactionAttributeUsage.Script:\n            filtered_attr.append(attr)\n        else:\n            data = attr.Data\n            if isinstance(data, UInt160):\n                # convert it to equal type\n                data = attr.Data.ToArray()\n\n            # only add if it's not already in the list\n            if data not in script_list:\n                script_list.append(data)\n                filtered_attr.append(attr)\n\n    return filtered_attr"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef Deserialize(self, reader):\n        usage = reader.ReadByte()\n        self.Usage = usage\n\n        if usage == TransactionAttributeUsage.ContractHash or usage == TransactionAttributeUsage.Vote or \\\n                (usage >= TransactionAttributeUsage.Hash1 and usage <= TransactionAttributeUsage.Hash15):\n            self.Data = reader.ReadBytes(32)\n\n        elif usage == TransactionAttributeUsage.ECDH02 or usage == TransactionAttributeUsage.ECDH03:\n            self.Data = bytearray(usage) + bytearray(reader.ReadBytes(32))\n\n        elif usage == TransactionAttributeUsage.Script:\n            self.Data = reader.ReadBytes(20)\n\n        elif usage == TransactionAttributeUsage.DescriptionUrl:\n\n            self.Data = reader.ReadBytes(reader.ReadByte())\n\n        elif usage == TransactionAttributeUsage.Description or usage >= TransactionAttributeUsage.Remark:\n            self.Data = reader.ReadVarBytes(max=self.MAX_ATTR_DATA_SIZE)\n        else:\n            logger.error(\"format error!!!\")", "response": "Deserialize full object.\n\n        Args:\n            reader (neo.IO.BinaryReader):"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nserialize the object to a binary file.", "response": "def Serialize(self, writer):\n        \"\"\"\n        Serialize object.\n\n        Args:\n            writer (neo.IO.BinaryWriter):\n\n        Raises:\n            Exception: if the length exceeds the maximum allowed number of attributes in a transaction.\n        \"\"\"\n        writer.WriteByte(self.Usage)\n\n        if isinstance(self.Data, UIntBase):\n            self.Data = self.Data.Data\n\n        length = len(self.Data)\n\n        if length > self.MAX_ATTR_DATA_SIZE:\n            raise Exception(\"Invalid transaction attribute\")\n\n        if self.Usage == TransactionAttributeUsage.ContractHash or self.Usage == TransactionAttributeUsage.Vote or \\\n                (self.Usage >= TransactionAttributeUsage.Hash1 and self.Usage <= TransactionAttributeUsage.Hash15):\n            writer.WriteBytes(self.Data)\n\n        elif self.Usage == TransactionAttributeUsage.ECDH02 or self.Usage == TransactionAttributeUsage.ECDH03:\n            writer.WriteBytes(self.Data[1:33])\n\n        elif self.Usage == TransactionAttributeUsage.Script:\n            writer.WriteBytes(self.Data)\n\n        elif self.Usage == TransactionAttributeUsage.DescriptionUrl:\n            writer.WriteVarString(self.Data)\n\n        elif self.Usage == TransactionAttributeUsage.Description or self.Usage >= TransactionAttributeUsage.Remark:\n            writer.WriteVarString(self.Data)\n        else:\n            logger.error(\"format error!!!\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert object members to a dictionary that can be parsed as JSON.", "response": "def ToJson(self):\n        \"\"\"\n        Convert object members to a dictionary that can be parsed as JSON.\n\n        Returns:\n             dict:\n        \"\"\"\n        obj = {\n            'usage': self.Usage,\n            'data': '' if not self.Data else self.Data.hex()\n        }\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget unspent outputs from a list of transaction outputs.", "response": "def FromTXOutputsConfirmed(outputs):\n        \"\"\"\n        Get unspent outputs from a list of transaction outputs.\n\n        Args:\n            outputs (list): of neo.Core.TX.Transaction.TransactionOutput items.\n\n        Returns:\n            UnspentCoinState:\n        \"\"\"\n        uns = UnspentCoinState()\n        uns.Items = [0] * len(outputs)\n        for i in range(0, len(outputs)):\n            uns.Items[i] = int(CoinState.Confirmed)\n        return uns"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef Size(self):\n        # Items should be an array of type CoinState, not of ints!\n        corrected_items = list(map(lambda i: CoinState(i), self.Items))\n        return super(UnspentCoinState, self).Size() + GetVarSize(corrected_items)", "response": "Get the total size in bytes of the object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn True if all balance is spend.", "response": "def IsAllSpent(self):\n        \"\"\"\n        Flag indicating if all balance is spend.\n\n        Returns:\n            bool:\n        \"\"\"\n        for item in self.Items:\n            if item == CoinState.Confirmed:\n                return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef Serialize(self, writer):\n        super(UnspentCoinState, self).Serialize(writer)\n\n        writer.WriteVarInt(len(self.Items))\n\n        for item in self.Items:\n            byt = item.to_bytes(1, 'little')\n            writer.WriteByte(byt)", "response": "Serialize full object.\n\n        Args:\n            writer (neo.IO.BinaryWriter):"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nserializes object. Args: writer (neo.IO.BinaryWriter):", "response": "def Serialize(self, writer):\n        \"\"\"\n        Serialize object.\n\n        Args:\n            writer (neo.IO.BinaryWriter):\n        \"\"\"\n        writer.WriteUInt256(self.AssetId)\n        writer.WriteFixed8(self.Value)\n        writer.WriteUInt160(self.ScriptHash)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts the object members to a dictionary that can be parsed as JSON.", "response": "def ToJson(self, index):\n        \"\"\"\n        Convert object members to a dictionary that can be parsed as JSON.\n        Args:\n            index (int): The index of the output in a transaction\n\n        Returns:\n             dict:\n        \"\"\"\n        return {\n            'n': index,\n            'asset': self.AssetId.To0xString(),\n            'value': self.Value.ToNeoJsonString(),\n            'address': self.Address\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nserializes object. Args: writer (neo.IO.BinaryWriter):", "response": "def Serialize(self, writer):\n        \"\"\"\n        Serialize object.\n\n        Args:\n            writer (neo.IO.BinaryWriter):\n        \"\"\"\n        writer.WriteUInt256(self.PrevHash)\n        writer.WriteUInt16(self.PrevIndex)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef Hash(self):\n        if not self.__hash:\n            ba = bytearray(binascii.unhexlify(self.GetHashData()))\n            hash = Crypto.Hash256(ba)\n            self.__hash = UInt256(data=hash)\n        return self.__hash", "response": "Get the hash of the transaction."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef References(self):\n        if self.__references is None:\n\n            refs = {}\n\n            # group by the input prevhash\n            for hash, group in groupby(self.inputs, lambda x: x.PrevHash):\n\n                tx, height = GetBlockchain().GetTransaction(hash.ToBytes())\n                if tx is not None:\n                    for input in group:\n                        refs[input] = tx.outputs[input.PrevIndex]\n\n            self.__references = refs\n\n        return self.__references", "response": "Get all references.\n\n        Returns:\n            dict:\n                Key (UInt256): input PrevHash\n                Value (TransactionOutput): object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the total size in bytes of the object.", "response": "def Size(self):\n        \"\"\"\n        Get the total size in bytes of the object.\n\n        Returns:\n            int: size.\n        \"\"\"\n        return s.uint8 + s.uint8 + GetVarSize(self.Attributes) + GetVarSize(self.inputs) + GetVarSize(self.outputs) + GetVarSize(self.Scripts)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef SystemFee(self):\n        tx_name = TransactionType.ToName(self.Type)\n        return Fixed8.FromDecimal(settings.ALL_FEES.get(tx_name, 0))", "response": "Get the system fee."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the network fee.", "response": "def NetworkFee(self):\n        \"\"\"\n        Get the network fee.\n\n        Returns:\n            Fixed8:\n        \"\"\"\n        if self._network_fee is None:\n\n            input = Fixed8(0)\n\n            for coin_ref in self.References.values():\n                if coin_ref.AssetId == GetBlockchain().SystemCoin().Hash:\n                    input = input + coin_ref.Value\n\n            output = Fixed8(0)\n\n            for tx_output in self.outputs:\n                if tx_output.AssetId == GetBlockchain().SystemCoin().Hash:\n                    output = output + tx_output.Value\n\n            self._network_fee = input - output - self.SystemFee()\n\n        #            logger.info(\"Determined network fee to be %s \" % (self.__network_fee.value))\n\n        return self._network_fee"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef Deserialize(self, reader):\n        self.DeserializeUnsigned(reader)\n\n        self.scripts = reader.ReadSerializableArray()\n        self.OnDeserialized()", "response": "Deserialize full object.\n\n        Args:\n            reader (neo.IO.BinaryReader):"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef DeserializeFromBufer(buffer, offset=0):\n        mstream = StreamManager.GetStream(buffer)\n        reader = BinaryReader(mstream)\n        tx = Transaction.DeserializeFrom(reader)\n\n        StreamManager.ReleaseStream(mstream)\n        return tx", "response": "Deserialize object instance from a binary buffer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef DeserializeFrom(reader):\n        ttype = reader.ReadByte()\n        tx = None\n\n        from neo.Core.TX.RegisterTransaction import RegisterTransaction\n        from neo.Core.TX.IssueTransaction import IssueTransaction\n        from neo.Core.TX.ClaimTransaction import ClaimTransaction\n        from neo.Core.TX.MinerTransaction import MinerTransaction\n        from neo.Core.TX.PublishTransaction import PublishTransaction\n        from neo.Core.TX.InvocationTransaction import InvocationTransaction\n        from neo.Core.TX.EnrollmentTransaction import EnrollmentTransaction\n        from neo.Core.TX.StateTransaction import StateTransaction\n\n        if ttype == int.from_bytes(TransactionType.RegisterTransaction, 'little'):\n            tx = RegisterTransaction()\n        elif ttype == int.from_bytes(TransactionType.MinerTransaction, 'little'):\n            tx = MinerTransaction()\n        elif ttype == int.from_bytes(TransactionType.IssueTransaction, 'little'):\n            tx = IssueTransaction()\n        elif ttype == int.from_bytes(TransactionType.ClaimTransaction, 'little'):\n            tx = ClaimTransaction()\n        elif ttype == int.from_bytes(TransactionType.PublishTransaction, 'little'):\n            tx = PublishTransaction()\n        elif ttype == int.from_bytes(TransactionType.InvocationTransaction, 'little'):\n            tx = InvocationTransaction()\n        elif ttype == int.from_bytes(TransactionType.EnrollmentTransaction, 'little'):\n            tx = EnrollmentTransaction()\n        elif ttype == int.from_bytes(TransactionType.StateTransaction, 'little'):\n            tx = StateTransaction()\n        else:\n            tx = Transaction()\n            tx.Type = ttype\n\n        tx.DeserializeUnsignedWithoutType(reader)\n\n        tx.scripts = []\n        byt = reader.ReadVarInt()\n\n        if byt > 0:\n            for i in range(0, byt):\n                witness = Witness()\n                witness.Deserialize(reader)\n\n                tx.scripts.append(witness)\n\n        tx.OnDeserialized()\n\n        return tx", "response": "Deserialize a full object from a binary reader."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef Serialize(self, writer):\n        self.SerializeUnsigned(writer)\n        writer.WriteSerializableArray(self.scripts)", "response": "Serialize object.\n\n        Args:\n            writer (neo.IO.BinaryWriter):"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef SerializeUnsigned(self, writer):\n        writer.WriteByte(self.Type)\n        writer.WriteByte(self.Version)\n        self.SerializeExclusiveData(writer)\n\n        if len(self.Attributes) > self.MAX_TX_ATTRIBUTES:\n            raise Exception(\"Cannot have more than %s transaction attributes\" % self.MAX_TX_ATTRIBUTES)\n\n        writer.WriteSerializableArray(self.Attributes)\n        writer.WriteSerializableArray(self.inputs)\n        writer.WriteSerializableArray(self.outputs)", "response": "Serialize object.\n\n        Args:\n            writer (neo.IO.BinaryWriter):"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert this object to a dictionary that can be parsed as JSON.", "response": "def ToJson(self):\n        \"\"\"\n        Convert object members to a dictionary that can be parsed as JSON.\n\n        Returns:\n             dict:\n        \"\"\"\n        jsn = {}\n        jsn[\"txid\"] = self.Hash.To0xString()\n        jsn[\"size\"] = self.Size()\n        jsn[\"type\"] = TransactionType.ToName(self.Type)\n        jsn[\"version\"] = self.Version\n        jsn[\"attributes\"] = [attr.ToJson() for attr in self.Attributes]\n        jsn[\"vout\"] = [out.ToJson(i) for i, out in enumerate(self.outputs)]\n        jsn[\"vin\"] = [input.ToJson() for input in self.inputs]\n        jsn[\"sys_fee\"] = self.SystemFee().ToNeoJsonString()\n        jsn[\"net_fee\"] = self.NetworkFee().ToNeoJsonString()\n        jsn[\"scripts\"] = [script.ToJson() for script in self.scripts]\n        return jsn"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef Verify(self, mempool):\n        logger.info(\"Verifying transaction: %s \" % self.Hash.ToBytes())\n\n        return Helper.VerifyScripts(self)", "response": "Verify the transaction.\n\n        Args:\n            mempool:\n\n        Returns:\n            bool: True if verified. False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef GetScriptHashesForVerifying(self):\n        if not self.References and len(self.Attributes) < 1:\n            return []\n\n        hashes = set()\n        for coinref, output in self.References.items():\n            hashes.add(output.ScriptHash)\n\n        for attr in self.Attributes:\n            if attr.Usage == TransactionAttributeUsage.Script:\n                if type(attr.Data) is UInt160:\n                    hashes.add(attr.Data)\n                else:\n                    hashes.add(UInt160(data=attr.Data))\n\n        for key, group in groupby(self.outputs, lambda p: p.AssetId):\n            if self.raw_tx:\n                asset = Helper.StaticAssetState(key)\n            else:\n                asset = GetBlockchain().GetAssetState(key.ToBytes())\n            if asset is None:\n                raise Exception(\"Invalid operation\")\n\n            if asset.AssetType == AssetType.DutyFlag:\n                for p in group:\n                    hashes.add(p.ScriptHash)\n\n        hashlist = list(hashes)\n        hashlist.sort()\n        return hashlist", "response": "Get a list of script hashes for verifying transactions."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef GetTransactionResults(self):\n        if self.References is None:\n            return None\n\n        results = []\n        realresults = []\n        for ref_output in self.References.values():\n            results.append(TransactionResult(ref_output.AssetId, ref_output.Value))\n\n        for output in self.outputs:\n            results.append(TransactionResult(output.AssetId, output.Value * Fixed8(-1)))\n\n        for key, group in groupby(results, lambda x: x.AssetId):\n            sum = Fixed8(0)\n            for item in group:\n                sum = sum + item.Amount\n\n            if sum != Fixed8.Zero():\n                realresults.append(TransactionResult(key, sum))\n\n        return realresults", "response": "Retrieves the execution results of the transaction."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a StackItem to a ContractParameter object", "response": "def ToParameter(item: StackItem):\n        \"\"\"\n        Convert a StackItem to a ContractParameter object\n\n        Args:\n            item (neo.VM.InteropService.StackItem) The item to convert to a ContractParameter object\n\n        Returns:\n            ContractParameter\n\n        \"\"\"\n        if isinstance(item, Array) or isinstance(item, Struct):\n            items = item.GetArray()\n            output = [ContractParameter.ToParameter(subitem) for subitem in items]\n            return ContractParameter(type=ContractParameterType.Array, value=output)\n\n        elif isinstance(item, Boolean):\n            return ContractParameter(type=ContractParameterType.Boolean, value=item.GetBoolean())\n\n        elif isinstance(item, ByteArray):\n            return ContractParameter(type=ContractParameterType.ByteArray, value=item.GetByteArray())\n\n        elif isinstance(item, Integer):\n            return ContractParameter(type=ContractParameterType.Integer, value=str(item.GetBigInteger()))\n\n        elif isinstance(item, InteropInterface):\n            return ContractParameter(type=ContractParameterType.InteropInterface, value=item.GetInterface())"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a StackItem to a ContractParameter object of a specified ContractParameterType", "response": "def AsParameterType(type: ContractParameterType, item: StackItem):\n        \"\"\"\n        Convert a StackItem to a ContractParameter object of a specified ContractParameterType\n        Args:\n            type (neo.SmartContract.ContractParameterType): The ContractParameterType to convert to\n            item (neo.VM.InteropService.StackItem): The item to convert to a ContractParameter object\n\n        Returns:\n\n        \"\"\"\n        if type == ContractParameterType.Integer:\n            return ContractParameter(type, value=item.GetBigInteger())\n        elif type == ContractParameterType.Boolean:\n            return ContractParameter(type, value=item.GetBoolean())\n        elif type == ContractParameterType.Array:\n            output = [ContractParameter.ToParameter(subitem) for subitem in item.GetArray()]\n            return ContractParameter(type, value=output)\n        elif type == ContractParameterType.String:\n            return ContractParameter(type, value=item.GetString())\n        elif type == ContractParameterType.InteropInterface:\n            return ContractParameter(type, value=item.GetInterface())\n        # all other types return a byte array\n        else:\n            return ContractParameter(type, value=item.GetByteArray())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a ContractParameter instance to a json representation of the contract parameter.", "response": "def ToJson(self, auto_hex=True):\n        \"\"\"\n        Converts a ContractParameter instance to a json representation\n\n        Returns:\n            dict: a dictionary representation of the contract parameter\n        \"\"\"\n        jsn = {}\n        jsn['type'] = str(ContractParameterType(self.Type))\n\n        if self.Type == ContractParameterType.Signature:\n            jsn['value'] = self.Value.hex()\n\n        elif self.Type == ContractParameterType.ByteArray:\n            if auto_hex:\n                jsn['value'] = self.Value.hex()\n            else:\n                jsn['value'] = self.Value\n        elif self.Type == ContractParameterType.Boolean:\n            jsn['value'] = self.Value\n\n        elif self.Type == ContractParameterType.String:\n            jsn['value'] = str(self.Value)\n\n        elif self.Type == ContractParameterType.Integer:\n            jsn['value'] = self.Value\n\n        # @TODO, see ``FromJson``, not sure if this is working properly\n        elif self.Type == ContractParameterType.PublicKey:\n            jsn['value'] = self.Value.ToString()\n\n        elif self.Type in [ContractParameterType.Hash160,\n                           ContractParameterType.Hash256]:\n            jsn['value'] = self.Value.ToString()\n\n        elif self.Type == ContractParameterType.Array:\n\n            res = []\n            for item in self.Value:\n                if item:\n                    res.append(item.ToJson(auto_hex=auto_hex))\n            jsn['value'] = res\n\n        elif self.Type == ContractParameterType.InteropInterface:\n            try:\n                jsn['value'] = self.Value.ToJson()\n            except Exception as e:\n                pass\n\n        return jsn"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ToVM(self):\n        if self.Type == ContractParameterType.String:\n            return str(self.Value).encode('utf-8').hex()\n        elif self.Type == ContractParameterType.Integer and isinstance(self.Value, int):\n            return BigInteger(self.Value)\n        return self.Value", "response": "Converts a ContractParameter item into a VM object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef FromJson(json):\n        type = ContractParameterType.FromString(json['type'])\n\n        value = json['value']\n        param = ContractParameter(type=type, value=None)\n\n        if type == ContractParameterType.Signature or type == ContractParameterType.ByteArray:\n            param.Value = bytearray.fromhex(value)\n\n        elif type == ContractParameterType.Boolean:\n            param.Value = bool(value)\n\n        elif type == ContractParameterType.Integer:\n            param.Value = int(value)\n\n        elif type == ContractParameterType.Hash160:\n            param.Value = UInt160.ParseString(value)\n\n        elif type == ContractParameterType.Hash256:\n            param.Value = UInt256.ParseString(value)\n\n        # @TODO Not sure if this is working...\n        elif type == ContractParameterType.PublicKey:\n            param.Value = ECDSA.decode_secp256r1(value).G\n\n        elif type == ContractParameterType.String:\n            param.Value = str(value)\n\n        elif type == ContractParameterType.Array:\n            val = [ContractParameter.FromJson(item) for item in value]\n            param.Value = val\n\n        return param", "response": "Converts a json object to a ContractParameter object"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a Coin object using a CoinReference.", "response": "def CoinFromRef(coin_ref, tx_output, state=CoinState.Unconfirmed, transaction=None):\n        \"\"\"\n        Get a Coin object using a CoinReference.\n\n        Args:\n            coin_ref (neo.Core.CoinReference): an object representing a single UTXO / transaction input.\n            tx_output (neo.Core.Transaction.TransactionOutput): an object representing a transaction output.\n            state (neo.Core.State.CoinState):\n\n        Returns:\n            Coin: self.\n        \"\"\"\n        coin = Coin(coin_reference=coin_ref, tx_output=tx_output, state=state)\n        coin._transaction = transaction\n        return coin"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef Equals(self, other):\n        if other is None:\n            return False\n        if other.PrevHash.ToBytes() == self.PrevHash.ToBytes() and other.PrevIndex == self.PrevIndex:\n            return True\n        return False", "response": "Test for equality.\n\n        Args:\n            other (obj):\n\n        Returns:\n            bool: True `other` equals self."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef Deserialize(self, reader):\n        self.Timestamp = reader.ReadUInt32()\n        self.Services = reader.ReadUInt64()\n        addr = bytearray(reader.ReadFixedString(16))\n        addr.reverse()\n        addr.strip(b'\\x00')\n        nums = []\n        for i in range(0, 4):\n            nums.append(str(addr[i]))\n        nums.reverse()\n        adddd = '.'.join(nums)\n        self.Address = adddd\n        self.Port = reader.ReadUInt16(endian='>')", "response": "Deserialize full object.\n\n        Args:\n            reader (neo.IO.BinaryReader):"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef Serialize(self, writer):\n        writer.WriteUInt32(self.Timestamp)\n        writer.WriteUInt64(self.Services)\n        # turn ip address into bytes\n        octets = bytearray(map(lambda oct: int(oct), self.Address.split('.')))\n        # pad to fixed length 16\n        octets += bytearray(12)\n        # and finally write to stream\n        writer.WriteBytes(octets)\n        writer.WriteUInt16(self.Port, endian='>')", "response": "Serialize object to binary stream."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef GetHashData(hashable):\n        ms = StreamManager.GetStream()\n        writer = BinaryWriter(ms)\n        hashable.SerializeUnsigned(writer)\n        ms.flush()\n        retVal = ms.ToArray()\n        StreamManager.ReleaseStream(ms)\n        return retVal", "response": "Get the data used for hashing."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef Sign(verifiable, keypair):\n        prikey = bytes(keypair.PrivateKey)\n        hashdata = verifiable.GetHashData()\n        res = Crypto.Default().Sign(hashdata, prikey)\n        return res", "response": "Signs the verifiable object with the private key from keypair."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ToArray(value):\n        ms = StreamManager.GetStream()\n        writer = BinaryWriter(ms)\n\n        value.Serialize(writer)\n\n        retVal = ms.ToArray()\n        StreamManager.ReleaseStream(ms)\n\n        return retVal", "response": "Serialize the given value to an array of bytes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ToStream(value):\n        ms = StreamManager.GetStream()\n        writer = BinaryWriter(ms)\n\n        value.Serialize(writer)\n\n        retVal = ms.getvalue()\n        StreamManager.ReleaseStream(ms)\n\n        return retVal", "response": "Serialize the given value to a an array of bytes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a public address to a script hash.", "response": "def AddrStrToScriptHash(address):\n        \"\"\"\n        Convert a public address to a script hash.\n\n        Args:\n            address (str): base 58 check encoded public address.\n\n        Raises:\n            ValueError: if the address length of address version is incorrect.\n            Exception: if the address checksum fails.\n\n        Returns:\n            UInt160:\n        \"\"\"\n        data = b58decode(address)\n        if len(data) != 25:\n            raise ValueError('Not correct Address, wrong length.')\n        if data[0] != settings.ADDRESS_VERSION:\n            raise ValueError('Not correct Coin Version')\n\n        checksum = Crypto.Default().Hash256(data[:21])[:4]\n        if checksum != data[21:]:\n            raise Exception('Address format error')\n        return UInt160(data=data[1:21])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef RawBytesToScriptHash(raw):\n        rawh = binascii.unhexlify(raw)\n        rawhashstr = binascii.unhexlify(bytes(Crypto.Hash160(rawh), encoding='utf-8'))\n        return UInt160(data=rawhashstr)", "response": "This function returns a script hash of the provided raw bytes."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nverifies the scripts of the provided object.", "response": "def VerifyScripts(verifiable):\n        \"\"\"\n        Verify the scripts of the provided `verifiable` object.\n\n        Args:\n            verifiable (neo.IO.Mixins.VerifiableMixin):\n\n        Returns:\n            bool: True if verification is successful. False otherwise.\n        \"\"\"\n        try:\n            hashes = verifiable.GetScriptHashesForVerifying()\n        except Exception as e:\n            logger.debug(\"couldn't get script hashes %s \" % e)\n            return False\n\n        if len(hashes) != len(verifiable.Scripts):\n            logger.debug(f\"hash - verification script length mismatch ({len(hashes)}/{len(verifiable.Scripts)})\")\n            return False\n\n        blockchain = GetBlockchain()\n\n        for i in range(0, len(hashes)):\n            verification = verifiable.Scripts[i].VerificationScript\n\n            if len(verification) == 0:\n                sb = ScriptBuilder()\n                sb.EmitAppCall(hashes[i].Data)\n                verification = sb.ms.getvalue()\n            else:\n                verification_hash = Crypto.ToScriptHash(verification, unhex=False)\n                if hashes[i] != verification_hash:\n                    logger.debug(f\"hash {hashes[i]} does not match verification hash {verification_hash}\")\n                    return False\n\n            state_reader = GetStateReader()\n            script_table = CachedScriptTable(DBCollection(blockchain._db, DBPrefix.ST_Contract, ContractState))\n\n            engine = ApplicationEngine(TriggerType.Verification, verifiable, script_table, state_reader, Fixed8.Zero())\n            engine.LoadScript(verification)\n            invocation = verifiable.Scripts[i].InvocationScript\n            engine.LoadScript(invocation)\n\n            try:\n                success = engine.Execute()\n                state_reader.ExecutionCompleted(engine, success)\n            except Exception as e:\n                state_reader.ExecutionCompleted(engine, False, e)\n\n            if engine.ResultStack.Count != 1 or not engine.ResultStack.Pop().GetBoolean():\n                Helper.EmitServiceEvents(state_reader)\n                if engine.ResultStack.Count > 0:\n                    logger.debug(f\"Result stack failure! Count: {engine.ResultStack.Count} bool value: {engine.ResultStack.Pop().GetBoolean()}\")\n                else:\n                    logger.debug(f\"Result stack failure! Count: {engine.ResultStack.Count}\")\n                return False\n\n            Helper.EmitServiceEvents(state_reader)\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef Serialize(self, writer):\n        super(AssetState, self).Serialize(writer)\n        writer.WriteUInt256(self.AssetId)\n        writer.WriteByte(self.AssetType)\n        writer.WriteVarString(self.Name)\n\n        if self.Amount.value > -1:\n            writer.WriteFixed8(self.Amount, unsigned=True)\n        else:\n            writer.WriteFixed8(self.Amount)\n\n        if type(self.Available) is not Fixed8:\n            raise Exception(\"AVAILABLE IS NOT FIXED 8!\")\n        writer.WriteFixed8(self.Available, unsigned=True)\n        writer.WriteByte(self.Precision)\n        writer.WriteByte(b'\\x00')\n        writer.WriteFixed8(self.Fee)\n        writer.WriteUInt160(self.FeeAddress)\n        self.Owner.Serialize(writer)\n        writer.WriteUInt160(self.Admin)\n        writer.WriteUInt160(self.Issuer)\n        writer.WriteUInt32(self.Expiration)\n        writer.WriteBool(self.IsFrozen)", "response": "Serialize full object.\n\n        Args:\n            writer (neo.IO.BinaryWriter):"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the asset name based on its type.", "response": "def GetName(self):\n        \"\"\"\n        Get the asset name based on its type.\n\n        Returns:\n            str: 'NEO' or 'NEOGas'\n        \"\"\"\n        if self.AssetType == AssetType.GoverningToken:\n            return \"NEO\"\n        elif self.AssetType == AssetType.UtilityToken:\n            return \"NEOGas\"\n\n        if type(self.Name) is bytes:\n            return self.Name.decode('utf-8')\n        return self.Name"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ToJson(self):\n        return {\n            'assetId': self.AssetId.To0xString(),\n            'assetType': self.AssetType,\n            'name': self.GetName(),\n            'amount': self.Amount.value,\n            'available': self.Available.value,\n            'precision': self.Precision,\n            'fee': self.Fee.value,\n            'address': self.FeeAddress.ToString(),\n            'owner': self.Owner.ToString(),\n            'admin': Crypto.ToAddress(self.Admin),\n            'issuer': Crypto.ToAddress(self.Issuer),\n            'expiration': self.Expiration,\n            'is_frozen': self.IsFrozen\n        }", "response": "Converts the object members to a dictionary that can be parsed as JSON."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef SystemShare():\n        amount = Fixed8.FromDecimal(sum(Blockchain.GENERATION_AMOUNT) * Blockchain.DECREMENT_INTERVAL)\n        owner = ECDSA.secp256r1().Curve.Infinity\n        admin = Crypto.ToScriptHash(PUSHT)\n        return RegisterTransaction([], [], AssetType.GoverningToken,\n                                   \"[{\\\"lang\\\":\\\"zh-CN\\\",\\\"name\\\":\\\"\u5c0f\u8681\u80a1\\\"},{\\\"lang\\\":\\\"en\\\",\\\"name\\\":\\\"AntShare\\\"}]\",\n                                   amount, 0, owner, admin)", "response": "Register AntShare.\n            is a function that returns a RegisterTransaction object with the amount of time to generate the resource."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef SystemCoin():\n        amount = Fixed8.FromDecimal(sum(Blockchain.GENERATION_AMOUNT) * Blockchain.DECREMENT_INTERVAL)\n\n        owner = ECDSA.secp256r1().Curve.Infinity\n\n        precision = 8\n        admin = Crypto.ToScriptHash(PUSHF)\n\n        return RegisterTransaction([], [], AssetType.UtilityToken,\n                                   \"[{\\\"lang\\\":\\\"zh-CN\\\",\\\"name\\\":\\\"\u5c0f\u8681\u5e01\\\"},{\\\"lang\\\":\\\"en\\\",\\\"name\\\":\\\"AntCoin\\\"}]\",\n                                   amount, precision, owner, admin)", "response": "Register AntCoin\n\n        Returns:\n            RegisterTransaction:"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef GenesisBlock() -> Block:\n        prev_hash = UInt256(data=bytearray(32))\n        timestamp = int(datetime(2016, 7, 15, 15, 8, 21, tzinfo=pytz.utc).timestamp())\n        index = 0\n        consensus_data = 2083236893  # Pay tribute To Bitcoin\n        next_consensus = Blockchain.GetConsensusAddress(Blockchain.StandbyValidators())\n        script = Witness(bytearray(0), bytearray(PUSHT))\n\n        mt = MinerTransaction()\n        mt.Nonce = 2083236893\n\n        output = TransactionOutput(\n            Blockchain.SystemShare().Hash,\n            Blockchain.SystemShare().Amount,\n            Crypto.ToScriptHash(Contract.CreateMultiSigRedeemScript(int(len(Blockchain.StandbyValidators()) / 2) + 1,\n                                                                    Blockchain.StandbyValidators()))\n        )\n\n        it = IssueTransaction([], [output], [], [script])\n\n        return Block(prev_hash, timestamp, index, consensus_data,\n                     next_consensus, script,\n                     [mt, Blockchain.SystemShare(), Blockchain.SystemCoin(), it],\n                     True)", "response": "Create the GenesisBlock.\n\n        Returns:\n            BLock:"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef Default() -> 'Blockchain':\n        if Blockchain._instance is None:\n            Blockchain._instance = Blockchain()\n            Blockchain.GenesisBlock().RebuildMerkleRoot()\n\n        return Blockchain._instance", "response": "Get the default blockchain instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the script hash of the consensus node.", "response": "def GetConsensusAddress(validators):\n        \"\"\"\n        Get the script hash of the consensus node.\n\n        Args:\n            validators (list): of Ellipticcurve.ECPoint's\n\n        Returns:\n            UInt160:\n        \"\"\"\n        vlen = len(validators)\n        script = Contract.CreateMultiSigRedeemScript(vlen - int((vlen - 1) / 3), validators)\n        return Crypto.ToScriptHash(script)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef GetSysFeeAmountByHeight(self, height):\n        hash = self.GetBlockHash(height)\n        return self.GetSysFeeAmount(hash)", "response": "Retrieves the system fee for the specified block height."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef DeregisterBlockchain():\n        Blockchain.SECONDS_PER_BLOCK = 15\n        Blockchain.DECREMENT_INTERVAL = 2000000\n        Blockchain.GENERATION_AMOUNT = [8, 7, 6, 5, 4, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n        Blockchain._blockchain = None\n        Blockchain._validators = []\n        Blockchain._genesis_block = None\n        Blockchain._instance = None\n        Blockchain._blockrequests = set()\n        Blockchain._paused = False\n        Blockchain.BlockSearchTries = 0\n        Blockchain.CACHELIM = 4000\n        Blockchain.CMISSLIM = 5\n        Blockchain.LOOPTIME = .1\n        Blockchain.PersistCompleted = Events()\n        Blockchain.Notify = Events()\n        Blockchain._instance = None", "response": "Deregisters the default blockchain instance."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconfigures the stdio handlers of all loggers.", "response": "def config_stdio(self, log_configurations: Optional[List[LogConfiguration]] = None, default_level=logging.INFO) -> None:\n        \"\"\"\n        Configure the stdio `StreamHandler` levels on the specified loggers.\n        If no log configurations are specified then the `default_level` will be applied to all handlers.\n\n        Args:\n            log_configurations: a list of (component name, log level) tuples\n            default_level: logging level to apply when no log_configurations are specified\n        \"\"\"\n        # no configuration specified, apply `default_level` to the stdio handler of all known loggers\n        if not log_configurations:\n            for logger in self.loggers.values():\n                self._restrict_output(logger, default_level)\n        # only apply specified configuration to the stdio `StreamHandler` of the specific component\n        else:\n            for component, level in log_configurations:\n                try:\n                    logger = self.loggers[self.root + component]\n                except KeyError:\n                    raise ValueError(\"Failed to configure component. Invalid name: {}\".format(component))\n                self._restrict_output(logger, level)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef mute_stdio(self) -> None:\n\n        # The benefit of using a Filter here for disabling messages is that we do not have to restore old logging levels.\n        for logger in self.loggers.values():\n            if logger.hasHandlers():\n                logger.handlers[0].addFilter(self.block_all_filter)", "response": "Mute all standard messages."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nintend to re-store the temporarily disabled logging of `mute_stdio()` by removing the `BlockAll` filter.", "response": "def unmute_stdio(self) -> None:\n        \"\"\"\n        Intended to re-store the temporarily disabled logging of `mute_stdio()` by removing the `BlockAll` filter.\n        \"\"\"\n        for logger in self.loggers.values():\n            if logger.hasHandlers():\n                logger.handlers[0].removeFilter(self.block_all_filter)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a logger instance for the specified component name.", "response": "def getLogger(self, component_name: str = None) -> logging.Logger:\n        \"\"\"\n        Get the logger instance matching ``component_name`` or create a new one if non-existent.\n\n        Args:\n            component_name: a neo-python component name. e.g. network, vm, db\n\n        Returns:\n            a logger for the specified component.\n        \"\"\"\n        logger_name = self.root + (component_name if component_name else 'generic')\n        _logger = self.loggers.get(logger_name)\n        if not _logger:\n            _logger = logging.getLogger(logger_name)\n\n            stdio_handler = logging.StreamHandler()\n            stdio_handler.setFormatter(LogFormatter())\n            stdio_handler.setLevel(logging.INFO)\n            _logger.addHandler(stdio_handler)\n            _logger.setLevel(logging.DEBUG)\n            self.loggers[logger_name] = _logger\n        return _logger"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites a line to the VM instruction log file.", "response": "def write_log(self, message):\n        \"\"\"\n        Write a line to the VM instruction log file.\n\n        Args:\n            message (str): string message to write to file.\n        \"\"\"\n        if self._is_write_log and self.log_file and not self.log_file.closed:\n            self.log_file.write(message + '\\n')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ClaimGas(wallet, require_password=True, from_addr_str=None, to_addr_str=None):\n    if not wallet:\n        print(\"Please open a wallet\")\n        return None, False\n\n    unclaimed_coins = wallet.GetUnclaimedCoins()\n\n    unclaimed_count = len(unclaimed_coins)\n    if unclaimed_count == 0:\n        print(\"No claims to process\")\n        return None, False\n\n    unclaimed_coin_refs = [coin.Reference for coin in unclaimed_coins]\n\n    available_bonus = Blockchain.Default().CalculateBonusIgnoreClaimed(unclaimed_coin_refs)\n\n    if available_bonus == Fixed8.Zero():\n        print(\"No gas to claim\")\n        return None, False\n\n    claim_tx = ClaimTransaction()\n    claim_tx.Claims = unclaimed_coin_refs\n    claim_tx.Attributes = []\n    claim_tx.inputs = []\n\n    script_hash = wallet.GetChangeAddress()\n\n    # the following can be used to claim gas that is in an imported contract_addr\n    # example, wallet claim --from-addr={smart contract addr}\n    if from_addr_str:\n        script_hash = None\n        script_hash = PromptUtils.lookup_addr_str(wallet, from_addr_str)\n        if script_hash is None:\n            logger.debug(\"invalid source address\")\n            return None, False\n        standard_contract = wallet.GetStandardAddress()\n        claim_tx.Attributes = [TransactionAttribute(usage=TransactionAttributeUsage.Script,\n                                                    data=standard_contract.Data)]\n\n    if to_addr_str:\n        script_hash = None\n        script_hash = PromptUtils.lookup_addr_str(wallet, to_addr_str)\n        if script_hash is None:\n            logger.debug(\"invalid destination address\")\n            return None, False\n\n    claim_tx.outputs = [\n        TransactionOutput(AssetId=Blockchain.SystemCoin().Hash, Value=available_bonus, script_hash=script_hash)\n    ]\n\n    context = ContractParametersContext(claim_tx)\n    wallet.Sign(context)\n\n    print(\"\\n---------------------------------------------------------------\")\n    print(f\"Will make claim for {available_bonus.ToString()} GAS\")\n    print(\"------------------------------------------------------------------\\n\")\n\n    if require_password:\n        print(\"Enter your password to complete this claim\")\n\n        passwd = prompt(\"[Password]> \", is_password=True)\n\n        if not wallet.ValidatePassword(passwd):\n            print(\"Incorrect password\")\n            return None, False\n\n    if context.Completed:\n\n        claim_tx.scripts = context.GetScripts()\n\n        print(\"claim tx: %s \" % json.dumps(claim_tx.ToJson(), indent=4))\n\n        relayed = NodeLeader.Instance().Relay(claim_tx)\n\n        if relayed:\n            print(\"Relayed Tx: %s \" % claim_tx.Hash.ToString())\n            wallet.SaveTransaction(claim_tx)\n        else:\n            print(\"Could not relay tx %s \" % claim_tx.Hash.ToString())\n        return claim_tx, relayed\n\n    else:\n        print(\"could not sign tx\")\n\n    return None, False", "response": "This function claims the gas of the unclaimed contract in a single block."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ShowUnspentCoins(wallet, asset_id=None, from_addr=None, watch_only=False, do_count=False):\n\n    if wallet is None:\n        print(\"Please open a wallet.\")\n        return\n\n    watch_only_flag = 64 if watch_only else 0\n    if asset_id:\n        unspents = wallet.FindUnspentCoinsByAsset(asset_id, from_addr=from_addr, watch_only_val=watch_only_flag)\n    else:\n        unspents = wallet.FindUnspentCoins(from_addr=from_addr, watch_only_val=watch_only)\n\n    if do_count:\n        print('\\n-----------------------------------------------')\n        print('Total Unspent: %s' % len(unspents))\n        return unspents\n\n    for unspent in unspents:\n        print('\\n-----------------------------------------------')\n        print(json.dumps(unspent.ToJson(), indent=4))\n\n    if not unspents:\n        print(\"No unspent assets matching the arguments.\")\n\n    return unspents", "response": "Show unspent coins in the wallet."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef Size(self):\n\n        return s.uint8 + GetVarSize(self.Key) + GetVarSize(self.Field) + GetVarSize(self.Value)", "response": "Returns the total size in bytes of the object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef Serialize(self, writer: BinaryWriter):\n        byt = None\n        if self.Type == StateType.Account:\n            byt = b'\\x40'\n        elif self.Type == StateType.Validator:\n            byt = b'\\x48'\n        writer.WriteByte(byt)\n        writer.WriteVarBytes(self.Key)\n        writer.WriteVarString(self.Field)\n        writer.WriteVarBytes(self.Value)", "response": "Serialize full object.\n\n        Args:\n            writer (neo.IO.BinaryWriter):"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert object members to a dictionary that can be parsed as JSON.", "response": "def ToJson(self):\n        \"\"\"\n        Convert object members to a dictionary that can be parsed as JSON.\n\n        Returns:\n             dict:\n        \"\"\"\n\n        type = ''\n        if self.Type == StateType.Validator:\n            type = 'Validator'\n        elif self.Type == StateType.Account:\n            type = 'Account'\n\n        return {\n            'type': type,\n            'key': self.Key.hex(),\n            'field': self.Field,\n            'value': self.Value.hex()\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef FromDBInstance(db_token):\n        hash_ar = bytearray(binascii.unhexlify(db_token.ContractHash))\n        hash_ar.reverse()\n        hash = UInt160(data=hash_ar)\n        token = NEP5Token(script=None)\n        token.SetScriptHash(hash)\n        token.name = db_token.Name\n        token.symbol = db_token.Symbol\n        token.decimals = db_token.Decimals\n        return token", "response": "Returns a NEP5Token instance from a database token."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the wallet address associated with the token.", "response": "def Address(self):\n        \"\"\"\n        Get the wallet address associated with the token.\n\n        Returns:\n            str: base58 encoded string representing the wallet address.\n        \"\"\"\n        if self._address is None:\n            self._address = Crypto.ToAddress(self.ScriptHash)\n        return self._address"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef Query(self):\n        if self.name is not None:\n            # don't query twice\n            return\n\n        sb = ScriptBuilder()\n        sb.EmitAppCallWithOperation(self.ScriptHash, 'name')\n        sb.EmitAppCallWithOperation(self.ScriptHash, 'symbol')\n        sb.EmitAppCallWithOperation(self.ScriptHash, 'decimals')\n\n        engine = None\n        try:\n            engine = ApplicationEngine.Run(sb.ToArray(), exit_on_error=True, gas=Fixed8.FromDecimal(10.0), test_mode=False)\n        except Exception as e:\n            pass\n\n        if engine and len(engine.ResultStack.Items) == 3:\n            results = engine.ResultStack.Items\n\n            try:\n                self.name = results[0].GetString()\n                self.symbol = results[1].GetString()\n                self.decimals = results[2].GetBigInteger()\n                if len(self.name) > 1 and self.name != 'Stack Item' \\\n                        and len(self.symbol) > 1 and self.symbol != 'Stack Item' \\\n                        and self.decimals < 10:\n                    return True\n            except Exception as e:\n                pass\n        return False", "response": "Query the smart contract for its token information."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the token balance of a public address.", "response": "def GetBalance(self, wallet, address, as_string=False):\n        \"\"\"\n        Get the token balance.\n\n        Args:\n            wallet (neo.Wallets.Wallet): a wallet instance.\n            address (str): public address of the account to get the token balance of.\n            as_string (bool): whether the return value should be a string. Default is False, returning an integer.\n\n        Returns:\n            int/str: token balance value as int (default), token balanace as string if `as_string` is set to True. 0 if balance retrieval failed.\n        \"\"\"\n        addr = PromptUtils.parse_param(address, wallet)\n        if isinstance(addr, UInt160):\n            addr = addr.Data\n        sb = ScriptBuilder()\n        sb.EmitAppCallWithOperationAndArgs(self.ScriptHash, 'balanceOf', [addr])\n\n        tx, fee, results, num_ops, engine_success = test_invoke(sb.ToArray(), wallet, [])\n        if engine_success:\n            try:\n                val = results[0].GetBigInteger()\n                precision_divisor = pow(10, self.decimals)\n                balance = Decimal(val) / Decimal(precision_divisor)\n                if as_string:\n                    formatter_str = '.%sf' % self.decimals\n                    balance_str = format(balance, formatter_str)\n                    return balance_str\n                return balance\n            except Exception as e:\n                logger.error(\"could not get balance: %s \" % e)\n                traceback.print_stack()\n        else:\n            addr_str = Crypto.ToAddress(UInt160(data=addr))\n            logger.error(\n                f\"Could not get balance of address {addr_str} for token contract {self.ScriptHash}. VM execution failed. Make sure the contract exists on the network and that it adheres to the NEP-5 standard\")\n\n        return 0"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Transfer(self, wallet, from_addr, to_addr, amount, tx_attributes=None):\n        if not tx_attributes:\n            tx_attributes = []\n\n        sb = ScriptBuilder()\n        sb.EmitAppCallWithOperationAndArgs(self.ScriptHash, 'transfer',\n                                           [PromptUtils.parse_param(from_addr, wallet), PromptUtils.parse_param(to_addr, wallet),\n                                            PromptUtils.parse_param(amount)])\n\n        tx, fee, results, num_ops, engine_success = test_invoke(sb.ToArray(), wallet, [], from_addr=from_addr, invoke_attrs=tx_attributes)\n\n        return tx, fee, results", "response": "Transfers a amount of the NEP5Token from one address to another."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntransferring a amount of a token from one account to another account.", "response": "def TransferFrom(self, wallet, from_addr, to_addr, amount):\n        \"\"\"\n        Transfer a specified amount of a token from the wallet specified in the `from_addr` to the `to_addr`\n        if the originator `wallet` has been approved to do so.\n\n        Args:\n            wallet (neo.Wallets.Wallet): a wallet instance.\n            from_addr (str): public address of the account to transfer the given amount from.\n            to_addr (str): public address of the account to transfer the given amount to.\n            amount (int): quantity to send.\n\n        Returns:\n            tuple:\n                InvocationTransaction: the transaction.\n                int: the transaction fee.\n                list: the neo VM evaluation stack results.\n        \"\"\"\n        invoke_args = [self.ScriptHash.ToString(), 'transferFrom',\n                       [PromptUtils.parse_param(from_addr, wallet), PromptUtils.parse_param(to_addr, wallet), PromptUtils.parse_param(amount)]]\n\n        tx, fee, results, num_ops, engine_success = TestInvokeContract(wallet, invoke_args, None, True)\n\n        return tx, fee, results"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the amount of tokens that the owner_addr account can transfer from the requestor_addr account.", "response": "def Allowance(self, wallet, owner_addr, requestor_addr):\n        \"\"\"\n        Return the amount of tokens that the `requestor_addr` account can transfer from the `owner_addr` account.\n\n        Args:\n            wallet (neo.Wallets.Wallet): a wallet instance.\n            owner_addr (str): public address of the account to transfer the given amount from.\n            requestor_addr (str): public address of the account that requests the transfer.\n\n        Returns:\n            tuple:\n                InvocationTransaction: the transaction.\n                int: the transaction fee.\n                list: the neo VM evaluation stack results.\n        \"\"\"\n        invoke_args = [self.ScriptHash.ToString(), 'allowance',\n                       [PromptUtils.parse_param(owner_addr, wallet), PromptUtils.parse_param(requestor_addr, wallet)]]\n\n        tx, fee, results, num_ops, engine_success = TestInvokeContract(wallet, invoke_args, None, True)\n\n        return tx, fee, results"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmint the tokens to the neo account.", "response": "def Mint(self, wallet, mint_to_addr, attachment_args, invoke_attrs=None):\n        \"\"\"\n        Call the \"mintTokens\" function of the smart contract.\n\n        Args:\n            wallet (neo.Wallets.Wallet): a wallet instance.\n            mint_to_addr (str): public address of the account to mint the tokens to.\n            attachment_args: (list): a list of arguments used to attach neo and/or gas to an invoke, eg ['--attach-gas=10.0','--attach-neo=3']\n            invoke_attrs: (list): a list of TransactionAttributes to be attached to the mint transaction\n        Returns:\n            tuple:\n                InvocationTransaction: the transaction.\n                int: the transaction fee.\n                list: the neo VM evaluation stack results.\n        \"\"\"\n        invoke_args = [self.ScriptHash.ToString(), 'mintTokens', []]\n\n        invoke_args = invoke_args + attachment_args\n\n        tx, fee, results, num_ops, engine_success = TestInvokeContract(wallet, invoke_args, None, True, from_addr=mint_to_addr, invoke_attrs=invoke_attrs)\n\n        return tx, fee, results"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef CrowdsaleRegister(self, wallet, register_addresses, from_addr=None):\n        invoke_args = [self.ScriptHash.ToString(), 'crowdsale_register',\n                       [PromptUtils.parse_param(p, wallet) for p in register_addresses]]\n\n        tx, fee, results, num_ops, engine_success = TestInvokeContract(wallet, invoke_args, None, True, from_addr)\n\n        return tx, fee, results", "response": "This method is used to register a crowd sale."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nserialize this token data to bytes.", "response": "def Serialize(self, writer):\n        \"\"\"\n        Serialize this token data to bytes\n        Args:\n            writer (neocore.IO.BinaryWriter): binary writer to write serialization data to\n\n        \"\"\"\n        writer.WriteVarString(self.name)\n        writer.WriteVarString(self.symbol)\n        writer.WriteUInt8(self.decimals)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef Deserialize(self, reader):\n        self.name = reader.ReadVarString().decode('utf-8')\n        self.symbol = reader.ReadVarString().decode('utf-8')\n        self.decimals = reader.ReadUInt8()", "response": "Deserialize a nova class entry from a byte stream."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ToJson(self):\n        jsn = {\n            'name': self.name,\n            'symbol': self.symbol,\n            'decimals': self.decimals,\n            'script_hash': self.ScriptHash.To0xString(),\n            'contract_address': self.Address\n        }\n        return jsn", "response": "Converts object members to a dictionary that can be parsed as JSON."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a flag indicating if the index exists in any of the spent coin items.", "response": "def HasIndex(self, index):\n        \"\"\"\n        Flag indicating the index exists in any of the spent coin items.\n        Args:\n            index (int):\n\n        Returns:\n\n        \"\"\"\n        for i in self.Items:\n            if i.index == index:\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef DeleteIndex(self, index):\n        to_remove = None\n        for i in self.Items:\n            if i.index == index:\n                to_remove = i\n\n        if to_remove:\n            self.Items.remove(to_remove)", "response": "Deletes a spent coin based on its index."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef Deserialize(self, reader):\n        super(SpentCoinState, self).Deserialize(reader)\n\n        self.TransactionHash = reader.ReadUInt256()\n        self.TransactionHeight = reader.ReadUInt32()\n\n        count = reader.ReadVarInt()\n\n        items = [0] * count\n        for i in range(0, count):\n            index = reader.ReadUInt16()\n            height = reader.ReadUInt32()\n            items[i] = SpentCoinItem(index=index, height=height)\n\n        self.Items = items", "response": "Deserialize full object.\n\n        Args:\n            reader (neocore.IO.BinaryReader):"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef Serialize(self, writer):\n        super(SpentCoinState, self).Serialize(writer)\n\n        writer.WriteUInt256(self.TransactionHash)\n        writer.WriteUInt32(self.TransactionHeight)\n        writer.WriteVarInt(len(self.Items))\n\n        for item in self.Items:\n            writer.WriteUInt16(item.index)\n            writer.WriteUInt32(item.height)", "response": "Serialize full object.\n\n        Args:\n            writer (neo.IO.BinaryWriter):"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts object members to a dictionary that can be parsed as JSON.", "response": "def ToJson(self):\n        \"\"\"\n        Convert object members to a dictionary that can be parsed as JSON.\n\n        Returns:\n             dict:\n        \"\"\"\n        items = []\n\n        for i in self.Items:\n            items.append({'index': i.index, 'height': i.height})\n\n        return {\n            'version': self.StateVersion,\n            'txHash': self.TransactionHash.ToString(),\n            'txHeight': self.TransactionHeight,\n            'items': items\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget all known nodes and their state", "response": "def get_peers(self):\n        \"\"\"Get all known nodes and their 'state' \"\"\"\n        node = NodeLeader.Instance()\n        result = {\"connected\": [], \"unconnected\": [], \"bad\": []}\n        connected_peers = []\n\n        for peer in node.Peers:\n            result['connected'].append({\"address\": peer.host,\n                                        \"port\": peer.port})\n            connected_peers.append(\"{}:{}\".format(peer.host, peer.port))\n\n        for addr in node.DEAD_ADDRS:\n            host, port = addr.rsplit(':', 1)\n            result['bad'].append({\"address\": host, \"port\": port})\n\n        # \"UnconnectedPeers\" is never used. So a check is needed to\n        # verify that a given address:port does not belong to a connected peer\n        for addr in node.KNOWN_ADDRS:\n            host, port = addr.rsplit(':', 1)\n            if addr not in connected_peers:\n                result['unconnected'].append({\"address\": host,\n                                              \"port\": int(port)})\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget information about all the addresses present on the open wallet", "response": "def list_address(self):\n        \"\"\"Get information about all the addresses present on the open wallet\"\"\"\n        result = []\n        for addrStr in self.wallet.Addresses:\n            addr = self.wallet.GetAddress(addrStr)\n            result.append({\n                \"address\": addrStr,\n                \"haskey\": not addr.IsWatchOnly,\n                \"label\": None,\n                \"watchonly\": addr.IsWatchOnly,\n            })\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_aes_key(password):\n    password_hash = hashlib.sha256(password.encode('utf-8')).digest()\n    return hashlib.sha256(password_hash).digest()", "response": "Compute a key to be used in AES encryption from the password"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef Size(self):\n        corrected_hashes = list(map(lambda i: UInt256(data=binascii.unhexlify(i)), self.HashStart))\n        return GetVarSize(corrected_hashes) + self.hash_stop.Size", "response": "Get the total size in bytes of the object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Serialize(self, writer):\n        writer.WriteHashes(self.HashStart)\n        if self.HashStop is not None:\n            writer.WriteUInt256(self.HashStop)", "response": "Serialize object.\n\n        Args:\n            writer (neo.IO.BinaryWriter):"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef CreateSignatureContract(publicKey):\n        script = Contract.CreateSignatureRedeemScript(publicKey)\n        params = b'\\x00'\n        encoded = publicKey.encode_point(True)\n        pubkey_hash = Crypto.ToScriptHash(encoded, unhex=True)\n\n        return Contract(script, params, pubkey_hash)", "response": "Create a signature contract."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the hash value of the data.", "response": "def Hash(self):\n        \"\"\"\n        Get the hash value of the Blockbase.\n\n        Returns:\n            UInt256: containing the hash of the data.\n        \"\"\"\n        if not self.__hash:\n            hashdata = self.RawData()\n            ba = bytearray(binascii.unhexlify(hashdata))\n            hash = bin_dbl_sha256(ba)\n            self.__hash = UInt256(data=hash)\n\n        return self.__hash"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef Size(self):\n        scriptsize = 0\n        if self.Script is not None:\n            scriptsize = self.Script.Size()\n        return s.uint32 + s.uint256 + s.uint256 + s.uint32 + s.uint32 + s.uint64 + s.uint160 + 1 + scriptsize", "response": "Get the total size in bytes of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef SerializeUnsigned(self, writer):\n        writer.WriteUInt32(self.Version)\n        writer.WriteUInt256(self.PrevHash)\n        writer.WriteUInt256(self.MerkleRoot)\n        writer.WriteUInt32(self.Timestamp)\n        writer.WriteUInt32(self.Index)\n        writer.WriteUInt64(self.ConsensusData)\n        writer.WriteUInt160(self.NextConsensus)", "response": "Serialize unsigned data only."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the script hash used for verification.", "response": "def GetScriptHashesForVerifying(self):\n        \"\"\"\n        Get the script hash used for verification.\n\n        Raises:\n            Exception: if the verification script is invalid, or no header could be retrieved from the Blockchain.\n\n        Returns:\n            list: with a single UInt160 representing the next consensus node.\n        \"\"\"\n        # if this is the genesis block, we dont have a prev hash!\n        if self.PrevHash.Data == bytearray(32):\n            #            logger.info(\"verificiation script %s\"  %(self.Script.ToJson()))\n            if type(self.Script.VerificationScript) is bytes:\n                return [bytearray(self.Script.VerificationScript)]\n            elif type(self.Script.VerificationScript) is bytearray:\n                return [self.Script.VerificationScript]\n            else:\n                raise Exception('Invalid Verification script')\n\n        prev_header = GetBlockchain().GetHeader(self.PrevHash.ToBytes())\n        if prev_header is None:\n            raise Exception('Invalid operation')\n        return [prev_header.NextConsensus]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef Serialize(self, writer):\n        self.SerializeUnsigned(writer)\n        writer.WriteByte(1)\n        self.Script.Serialize(writer)", "response": "Serialize the full object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ToJson(self):\n        json = {}\n        json[\"hash\"] = self.Hash.To0xString()\n        json[\"size\"] = self.Size()\n        json[\"version\"] = self.Version\n        json[\"previousblockhash\"] = self.PrevHash.To0xString()\n        json[\"merkleroot\"] = self.MerkleRoot.To0xString()\n        json[\"time\"] = self.Timestamp\n        json[\"index\"] = self.Index\n        nonce = bytearray(self.ConsensusData.to_bytes(8, 'little'))\n        nonce.reverse()\n        json[\"nonce\"] = nonce.hex()\n        json['nextconsensus'] = Crypto.ToAddress(self.NextConsensus)\n        # json[\"consensus data\"] = self.ConsensusData\n        json[\"script\"] = '' if not self.Script else self.Script.ToJson()\n        return json", "response": "Converts the object members to a dictionary that can be parsed as JSON."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef Verify(self):\n        if not self.Hash.ToBytes() == GetGenesis().Hash.ToBytes():\n            return False\n\n        bc = GetBlockchain()\n\n        if not bc.ContainsBlock(self.Index):\n            return False\n\n        if self.Index > 0:\n            prev_header = GetBlockchain().GetHeader(self.PrevHash.ToBytes())\n\n            if prev_header is None:\n                return False\n\n            if prev_header.Index + 1 != self.Index:\n                return False\n\n            if prev_header.Timestamp >= self.Timestamp:\n                return False\n\n        # this should be done to actually verify the block\n        if not Helper.VerifyScripts(self):\n            return False\n\n        return True", "response": "Verify the block using the verification script."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef FullTransactions(self):\n        is_trimmed = False\n        try:\n            tx = self.Transactions[0]\n            if type(tx) is str:\n                is_trimmed = True\n        except Exception as e:\n            pass\n\n        if not is_trimmed:\n            return self.Transactions\n\n        txs = []\n        for hash in self.Transactions:\n            tx, height = GetBlockchain().GetTransaction(hash)\n            txs.append(tx)\n\n        self.Transactions = txs\n\n        return self.Transactions", "response": "Get the list of full Transactions."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef Header(self):\n        if not self._header:\n            self._header = Header(self.PrevHash, self.MerkleRoot, self.Timestamp,\n                                  self.Index, self.ConsensusData, self.NextConsensus, self.Script)\n\n        return self._header", "response": "Get the block header."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef Size(self):\n        s = super(Block, self).Size() + GetVarSize(self.Transactions)\n        return s", "response": "Get the total size in bytes of the object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the total fees in the block.", "response": "def TotalFees(self):\n        \"\"\"\n        Get the total transaction fees in the block.\n\n        Returns:\n            Fixed8:\n        \"\"\"\n        amount = Fixed8.Zero()\n        for tx in self.Transactions:\n            amount += tx.SystemFee()\n        return amount"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef RebuildMerkleRoot(self):\n        logger.debug(\"Rebuilding merkle root!\")\n        if self.Transactions is not None and len(self.Transactions) > 0:\n            self.MerkleRoot = MerkleTree.ComputeRoot([tx.Hash for tx in self.Transactions])", "response": "Rebuild the merkle root of the block"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef Serialize(self, writer):\n        super(Block, self).Serialize(writer)\n        writer.WriteSerializableArray(self.Transactions)", "response": "Serialize full object.\n\n        Args:\n            writer (neo.IO.BinaryWriter):"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ToJson(self):\n        json = super(Block, self).ToJson()\n        if self.Transactions[0] and isinstance(self.Transactions[0], str):\n            json['tx'] = ['0x%s' % tx for tx in self.Transactions]\n        else:\n            json['tx'] = [tx.ToJson() for tx in self.Transactions]\n\n        # json['sys_fee'] = GetBlockchain().GetSysFeeAmount(self.Hash)\n        return json", "response": "Convert object members to a dictionary that can be parsed as JSON."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Trim(self):\n        ms = StreamManager.GetStream()\n        writer = BinaryWriter(ms)\n        self.SerializeUnsigned(writer)\n        writer.WriteByte(1)\n        self.Script.Serialize(writer)\n\n        writer.WriteHashes([tx.Hash.ToBytes() for tx in self.Transactions])\n        retVal = ms.ToArray()\n        StreamManager.ReleaseStream(ms)\n        return retVal", "response": "Returns a byte array that contains only the block header and transaction hash."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nverifying the integrity of the block.", "response": "def Verify(self, completely=False):\n        \"\"\"\n        Verify the integrity of the block.\n\n        Args:\n            completely: (Not functional at this time).\n\n        Returns:\n            bool: True if valid. False otherwise.\n        \"\"\"\n        res = super(Block, self).Verify()\n        if not res:\n            return False\n\n        from neo.Blockchain import GetBlockchain, GetConsensusAddress\n\n        # first TX has to be a miner transaction. other tx after that cant be miner tx\n        if self.Transactions[0].Type != TransactionType.MinerTransaction:\n            return False\n        for tx in self.Transactions[1:]:\n            if tx.Type == TransactionType.MinerTransaction:\n                return False\n\n        if completely:\n            bc = GetBlockchain()\n\n            if self.NextConsensus != GetConsensusAddress(bc.GetValidators(self.Transactions).ToArray()):\n                return False\n\n            for tx in self.Transactions:\n                if not tx.Verify():\n                    pass\n            logger.error(\"Blocks cannot be fully validated at this moment.  please pass completely=False\")\n            raise NotImplementedError()\n            # do this below!\n            # foreach(Transaction tx in Transactions)\n            # if (!tx.Verify(Transactions.Where(p = > !p.Hash.Equals(tx.Hash)))) return false;\n            # Transaction tx_gen = Transactions.FirstOrDefault(p= > p.Type == TransactionType.MinerTransaction);\n            # if (tx_gen?.Outputs.Sum(p = > p.Value) != CalculateNetFee(Transactions)) return false;\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntry to get a NEP - 5 token based on the symbol or script_hash of the given token string.", "response": "def get_token(wallet: 'Wallet', token_str: str) -> 'NEP5Token.NEP5Token':\n    \"\"\"\n    Try to get a NEP-5 token based on the symbol or script_hash\n\n    Args:\n        wallet: wallet instance\n        token_str: symbol or script_hash (accepts script hash with or without 0x prefix)\n    Raises:\n        ValueError: if token is not found\n\n    Returns:\n        NEP5Token instance if found.\n    \"\"\"\n    if token_str.startswith('0x'):\n        token_str = token_str[2:]\n\n    token = None\n    for t in wallet.GetTokens().values():\n        if token_str in [t.symbol, t.ScriptHash.ToString()]:\n            token = t\n            break\n\n    if not isinstance(token, NEP5Token.NEP5Token):\n        raise ValueError(\"The given token argument does not represent a known NEP5 token\")\n    return token"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the total size in bytes of the object.", "response": "def Size(self):\n        \"\"\"\n        Get the total size in bytes of the object.\n\n        Returns:\n            int: size.\n        \"\"\"\n        return super(ValidatorState, self).Size() + self.PublicKey.Size() + s.uint8 + self.Votes.Size()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef Serialize(self, writer: BinaryWriter):\n        super(ValidatorState, self).Serialize(writer)\n        self.PublicKey.Serialize(writer)\n        writer.WriteBool(self.Registered)\n        writer.WriteFixed8(self.Votes)", "response": "Serialize full object.\n\n        Args:\n            writer (neo.IO.BinaryWriter):"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef SerializeExclusiveData(self, writer):\n        self.Code.Serialize(writer)\n\n        if self.Version >= 1:\n            writer.WriteBool(self.NeedStorage)\n\n        writer.WriteVarString(self.Name)\n        writer.WriteVarString(self.CodeVersion)\n        writer.WriteVarString(self.Author)\n        writer.WriteVarString(self.Email)\n        writer.WriteVarString(self.Description)", "response": "Serialize object.\n\n        Args:\n            writer (neo.IO.BinaryWriter):"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ToJson(self):\n        jsn = super(PublishTransaction, self).ToJson()\n        jsn['contract'] = {}\n        jsn['contract']['code'] = self.Code.ToJson()\n        jsn['contract']['needstorage'] = self.NeedStorage\n        jsn['contract']['name'] = self.Name.decode('utf-8')\n        jsn['contract']['version'] = self.CodeVersion.decode('utf-8')\n        jsn['contract']['author'] = self.Author.decode('utf-8')\n        jsn['contract']['email'] = self.Email.decode('utf-8')\n        jsn['contract']['description'] = self.Description.decode('utf-8')\n        return jsn", "response": "Converts the PublishTransaction object to a dictionary that can be parsed as JSON."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef Size(self):\n        return s.uint32 + s.uint64 + s.uint32 + s.uint16 + s.uint32 + GetVarSize(self.UserAgent) + s.uint32 + s.uint8", "response": "Returns the total size in bytes of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef Serialize(self, writer):\n        writer.WriteUInt32(self.Version)\n        writer.WriteUInt64(self.Services)\n        writer.WriteUInt32(self.Timestamp)\n        writer.WriteUInt16(self.Port)\n        writer.WriteUInt32(self.Nonce)\n        writer.WriteVarString(self.UserAgent)\n        writer.WriteUInt32(self.StartHeight)\n        writer.WriteBool(self.Relay)", "response": "Serialize object to binary format."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef Instance(reactor=None):\n        if NodeLeader._LEAD is None:\n            NodeLeader._LEAD = NodeLeader(reactor)\n        return NodeLeader._LEAD", "response": "Returns the local node instance."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Setup(self):\n        self.Peers = []  # active nodes that we're connected to\n        self.KNOWN_ADDRS = []  # node addresses that we've learned about from other nodes\n        self.DEAD_ADDRS = []  # addresses that were performing poorly or we could not establish a connection to\n        self.MissionsGlobal = []\n        self.NodeId = random.randint(1294967200, 4294967200)", "response": "Initializes the local node."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_bcr_catchup(self):\n        logger.debug(f\"Checking if BlockRequests has caught up {len(BC.Default().BlockRequests)}\")\n\n        # test, perhaps there's some race condition between slow startup and throttle sync, otherwise blocks will never go down\n        for peer in self.Peers:  # type: NeoNode\n            peer.stop_block_loop(cancel=False)\n            peer.stop_peerinfo_loop(cancel=False)\n            peer.stop_header_loop(cancel=False)\n\n        if len(BC.Default().BlockRequests) > 0:\n            for peer in self.Peers:\n                peer.keep_alive()\n                peer.health_check(HEARTBEAT_BLOCKS)\n                peer_bcr_len = len(peer.myblockrequests)\n                # if a peer has cleared its queue then reset heartbeat status to avoid timing out when resuming from \"check_bcr\" if there's 1 or more really slow peer(s)\n                if peer_bcr_len == 0:\n                    peer.start_outstanding_data_request[HEARTBEAT_BLOCKS] = 0\n\n                print(f\"{peer.prefix} request count: {peer_bcr_len}\")\n                if peer_bcr_len == 1:\n                    next_hash = BC.Default().GetHeaderHash(self.CurrentBlockheight + 1)\n                    print(f\"{peer.prefix} {peer.myblockrequests} {next_hash}\")\n        else:\n            # we're done catching up. Stop own loop and restart peers\n            self.stop_check_bcr_loop()\n            self.check_bcr_loop = None\n            logger.debug(\"BlockRequests have caught up...resuming sync\")\n            for peer in self.Peers:\n                peer.ProtocolReady()  # this starts all loops again\n                # give a little bit of time between startup of peers\n                time.sleep(2)", "response": "check if we have exceeded data request speed vs receive + process"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstarts connecting to the seed list.", "response": "def Start(self, seed_list: List[str] = None, skip_seeds: bool = False) -> None:\n        \"\"\"\n        Start connecting to the seed list.\n\n        Args:\n            seed_list: a list of host:port strings if not supplied use list from `protocol.xxx.json`\n            skip_seeds: skip connecting to seed list\n        \"\"\"\n        if not seed_list:\n            seed_list = settings.SEED_LIST\n\n        logger.debug(\"Starting up nodeleader\")\n        if not skip_seeds:\n            logger.debug(\"Attempting to connect to seed list...\")\n            for bootstrap in seed_list:\n                if not is_ip_address(bootstrap):\n                    host, port = bootstrap.split(':')\n                    bootstrap = f\"{hostname_to_ip(host)}:{port}\"\n                addr = Address(bootstrap)\n                self.KNOWN_ADDRS.append(addr)\n                self.SetupConnection(addr)\n\n        logger.debug(\"Starting up nodeleader: starting peer, mempool, and blockheight check loops\")\n        # check in on peers every 10 seconds\n        self.start_peer_check_loop()\n        self.start_memcheck_loop()\n        self.start_blockheight_loop()\n\n        if settings.ACCEPT_INCOMING_PEERS and not self.incoming_server_running:\n            class OneShotFactory(Factory):\n                def __init__(self, leader):\n                    self.leader = leader\n\n                def buildProtocol(self, addr):\n                    print(f\"building new protocol for addr: {addr}\")\n                    self.leader.AddKnownAddress(Address(f\"{addr.host}:{addr.port}\"))\n                    p = NeoNode(incoming_client=True)\n                    p.factory = self\n                    return p\n\n            def listen_err(err):\n                print(f\"Failed start listening server for reason: {err.value}\")\n\n            def listen_ok(value):\n                self.incoming_server_running = True\n\n            logger.debug(f\"Starting up nodeleader: setting up listen server on port: {settings.NODE_PORT}\")\n            server_endpoint = TCP4ServerEndpoint(self.reactor, settings.NODE_PORT)\n            listenport_deferred = server_endpoint.listen(OneShotFactory(leader=self))\n            listenport_deferred.addCallback(listen_ok)\n            listenport_deferred.addErrback(listen_err)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef Shutdown(self):\n        logger.debug(\"Nodeleader shutting down\")\n\n        self.stop_peer_check_loop()\n        self.peer_check_loop_deferred = None\n\n        self.stop_check_bcr_loop()\n        self.check_bcr_loop_deferred = None\n\n        self.stop_memcheck_loop()\n        self.memcheck_loop_deferred = None\n\n        self.stop_blockheight_loop()\n        self.blockheight_loop_deferred = None\n\n        for p in self.Peers:\n            p.Disconnect()", "response": "Disconnect all connected peers."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a new connect peer to the known peers list.", "response": "def AddConnectedPeer(self, peer):\n        \"\"\"\n        Add a new connect peer to the known peers list.\n\n        Args:\n            peer (NeoNode): instance.\n        \"\"\"\n        # if present\n        self.RemoveFromQueue(peer.address)\n        self.AddKnownAddress(peer.address)\n\n        if len(self.Peers) > settings.CONNECTED_PEER_MAX:\n            peer.Disconnect(\"Max connected peers reached\", isDead=False)\n\n        if peer not in self.Peers:\n            self.Peers.append(peer)\n        else:\n            # either peer is already in the list and it has reconnected before it timed out on our side\n            # or it's trying to connect multiple times\n            # or we hit the max connected peer count\n            self.RemoveKnownAddress(peer.address)\n            peer.Disconnect()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove a connected peer from the known peers list.", "response": "def RemoveConnectedPeer(self, peer):\n        \"\"\"\n        Remove a connected peer from the known peers list.\n\n        Args:\n            peer (NeoNode): instance.\n        \"\"\"\n        if peer in self.Peers:\n            self.Peers.remove(peer)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove an address from the connection queue.", "response": "def RemoveFromQueue(self, addr):\n        \"\"\"\n        Remove an address from the connection queue\n        Args:\n            addr:\n\n        Returns:\n\n        \"\"\"\n        if addr in self.connection_queue:\n            self.connection_queue.remove(addr)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _monitor_for_zero_connected_peers(self):\n        if len(self.Peers) == 0 and len(self.connection_queue) == 0:\n            if self.peer_zero_count > 2:\n                logger.debug(\"Peer count 0 exceeded max retries threshold, restarting...\")\n                self.Restart()\n            else:\n                logger.debug(\n                    f\"Peer count is 0, allow for retries or queued connections to be established {self.peer_zero_count}\")\n                self.peer_zero_count += 1", "response": "Monitor if there are no peers that are still connected to any other peer."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprocess a received inventory.", "response": "def InventoryReceived(self, inventory):\n        \"\"\"\n        Process a received inventory.\n\n        Args:\n            inventory (neo.Network.Inventory): expect a Block type.\n\n        Returns:\n            bool: True if processed and verified. False otherwise.\n        \"\"\"\n        if inventory.Hash.ToBytes() in self._MissedBlocks:\n            self._MissedBlocks.remove(inventory.Hash.ToBytes())\n\n        if inventory is MinerTransaction:\n            return False\n\n        if type(inventory) is Block:\n            if BC.Default() is None:\n                return False\n\n            if BC.Default().ContainsBlock(inventory.Index):\n                return False\n\n            if not BC.Default().AddBlock(inventory):\n                return False\n\n        else:\n            if not inventory.Verify(self.MemPool.values()):\n                return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrelaying the inventory to the remote client.", "response": "def RelayDirectly(self, inventory):\n        \"\"\"\n        Relay the inventory to the remote client.\n\n        Args:\n            inventory (neo.Network.Inventory):\n\n        Returns:\n            bool: True if relayed successfully. False otherwise.\n        \"\"\"\n        relayed = False\n\n        self.RelayCache[inventory.Hash.ToBytes()] = inventory\n\n        for peer in self.Peers:\n            relayed |= peer.Relay(inventory)\n\n        if len(self.Peers) == 0:\n            if type(BC.Default()) is TestLevelDBBlockchain:\n                # mock a true result for tests\n                return True\n\n            logger.info(\"no connected peers\")\n\n        return relayed"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrelaying the inventory to the remote client.", "response": "def Relay(self, inventory):\n        \"\"\"\n        Relay the inventory to the remote client.\n\n        Args:\n            inventory (neo.Network.Inventory):\n\n        Returns:\n            bool: True if relayed successfully. False otherwise.\n        \"\"\"\n        if type(inventory) is MinerTransaction:\n            return False\n\n        if inventory.Hash.ToBytes() in self.KnownHashes:\n            return False\n\n        self.KnownHashes.append(inventory.Hash.ToBytes())\n\n        if type(inventory) is Block:\n            pass\n\n        elif type(inventory) is Transaction or issubclass(type(inventory), Transaction):\n            if not self.AddTransaction(inventory):\n                # if we fail to add the transaction for whatever reason, remove it from the known hashes list or we cannot retry the same transaction again\n                try:\n                    self.KnownHashes.remove(inventory.Hash.ToBytes())\n                except ValueError:\n                    # it not found\n                    pass\n                return False\n        else:\n            # consensus\n            pass\n\n        relayed = self.RelayDirectly(inventory)\n        return relayed"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef AddTransaction(self, tx):\n        if BC.Default() is None:\n            return False\n\n        if tx.Hash.ToBytes() in self.MemPool.keys():\n            return False\n\n        if BC.Default().ContainsTransaction(tx.Hash):\n            return False\n\n        if not tx.Verify(self.MemPool.values()):\n            logger.error(\"Verifying tx result... failed\")\n            return False\n\n        self.MemPool[tx.Hash.ToBytes()] = tx\n\n        return True", "response": "Adds a transaction to the memory pool."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef RemoveTransaction(self, tx):\n        if BC.Default() is None:\n            return False\n\n        if not BC.Default().ContainsTransaction(tx.Hash):\n            return False\n\n        if tx.Hash.ToBytes() in self.MemPool:\n            del self.MemPool[tx.Hash.ToBytes()]\n            return True\n\n        return False", "response": "Removes a transaction from the memory pool if it is found on the blockchain."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef MempoolCheck(self):\n        txs = []\n        values = self.MemPool.values()\n        for tx in values:\n            txs.append(tx)\n\n        for tx in txs:\n            res = self.RemoveTransaction(tx)\n            if res:\n                logger.debug(\"found tx 0x%s on the blockchain ...removed from mempool\" % tx.Hash)", "response": "Checks the Mempool and removes any tx found on the Blockchain\n           "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef BlockheightCheck(self):\n        if self.CurrentBlockheight == BC.Default().Height:\n            if len(self.Peers) > 0:\n                logger.debug(\"Blockheight is not advancing ...\")\n                next_hash = BC.Default().GetHeaderHash(self.CurrentBlockheight + 1)\n                culprit_found = False\n                for peer in self.Peers:\n                    if next_hash in peer.myblockrequests:\n                        culprit_found = True\n                        peer.Disconnect()\n                        break\n\n                # this happens when we're connecting to other nodes that are stuck themselves\n                if not culprit_found:\n                    for peer in self.Peers:\n                        peer.Disconnect()\n        else:\n            self.CurrentBlockheight = BC.Default().Height", "response": "Checks the current blockheight and finds the peer that prevents advancing the blockheight."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalls when we fail to connect to an endpoint", "response": "def clientConnectionFailed(self, err, address: Address):\n        \"\"\"\n        Called when we fail to connect to an endpoint\n        Args:\n            err: Twisted Failure instance\n            address: the address we failed to connect to\n        \"\"\"\n        if type(err.value) == error.TimeoutError:\n            logger.debug(f\"Failed connecting to {address} connection timed out\")\n        elif type(err.value) == error.ConnectError:\n            ce = err.value\n            if len(ce.args) > 0:\n                logger.debug(f\"Failed connecting to {address} {ce.args[0].value}\")\n            else:\n                logger.debug(f\"Failed connecting to {address}\")\n        else:\n            logger.debug(f\"Failed connecting to {address} {err.value}\")\n        self.peers_connecting -= 1\n        self.RemoveKnownAddress(address)\n        self.RemoveFromQueue(address)\n        # if we failed to connect to new addresses, we should always add them to the DEAD_ADDRS list\n        self.AddDeadAddress(address)\n\n        # for testing\n        return err.type"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads a SSH private key from a string.", "response": "def _load_private_key(self, private_key, passphrase=None):\n        \"\"\" Load a SSH private key (DSA or RSA) from a string\n\n        The private key may be encrypted. In that case, a passphrase\n        must be supplied.\n        \"\"\"\n        key = None\n        last_exception = None\n        for pkey_class in (RSAKey, DSSKey):\n            try:\n                key = pkey_class.from_private_key(StringIO(private_key),\n                    passphrase)\n            except PasswordRequiredException as e:\n                # The key file is encrypted and no passphrase was provided.\n                # There's no point to continue trying\n                raise\n            except SSHException as e:\n                last_exception = e\n                continue\n            else:\n                break\n        if key is None and last_exception:\n            raise last_exception\n        return key"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef open(self, hostname, port=22, username=None, password=None,\n                    private_key=None, key_passphrase=None,\n                    allow_agent=False, timeout=None):\n        \"\"\" Open a connection to a remote SSH server\n\n        In order to connect, either one of these credentials must be\n        supplied:\n            * Password\n                Password-based authentication\n            * Private Key\n                Authenticate using SSH Keys.\n                If the private key is encrypted, it will attempt to\n                load it using the passphrase\n            * Agent\n                Authenticate using the *local* SSH agent. This is the\n                one running alongside wsshd on the server side.\n        \"\"\"\n        try:\n            pkey = None\n            if private_key:\n                pkey = self._load_private_key(private_key, key_passphrase)\n            self._ssh.connect(\n                hostname=hostname,\n                port=port,\n                username=username,\n                password=password,\n                pkey=pkey,\n                timeout=timeout,\n                allow_agent=allow_agent,\n                look_for_keys=False)\n        except socket.gaierror as e:\n            self._websocket.send(json.dumps({'error':\n                'Could not resolve hostname {0}: {1}'.format(\n                    hostname, e.args[1])}))\n            raise\n        except Exception as e:\n            self._websocket.send(json.dumps({'error': e.message or str(e)}))\n            raise", "response": "Open a connection to a remote SSH server."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nforwards inbound traffic to the specified channel.", "response": "def _forward_inbound(self, channel):\n        \"\"\" Forward inbound traffic (websockets -> ssh) \"\"\"\n        try:\n            while True:\n                data = self._websocket.receive()\n                if not data:\n                    return\n                data = json.loads(str(data))\n                if 'resize' in data:\n                    channel.resize_pty(\n                        data['resize'].get('width', 80),\n                        data['resize'].get('height', 24))\n                if 'data' in data:\n                    channel.send(data['data'])\n        finally:\n            self.close()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _forward_outbound(self, channel):\n        try:\n            while True:\n                wait_read(channel.fileno())\n                data = channel.recv(1024)\n                if not len(data):\n                    return\n                self._websocket.send(json.dumps({'data': data}))\n        finally:\n            self.close()", "response": "Forward outbound traffic to the websockets."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _bridge(self, channel):\n        channel.setblocking(False)\n        channel.settimeout(0.0)\n        self._tasks = [\n            gevent.spawn(self._forward_inbound, channel),\n            gevent.spawn(self._forward_outbound, channel)\n        ]\n        gevent.joinall(self._tasks)", "response": "Full - duplex bridge between a websocket and an SSH channel."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef close(self):\n        gevent.killall(self._tasks, block=True)\n        self._tasks = []\n        self._ssh.close()", "response": "Terminate a bridge session"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexecute a command on the remote server", "response": "def execute(self, command, term='xterm'):\n        \"\"\" Execute a command on the remote server\n\n        This method will forward traffic from the websocket to the SSH server\n        and the other way around.\n\n        You must connect to a SSH server using ssh_connect()\n        prior to starting the session.\n        \"\"\"\n        transport = self._ssh.get_transport()\n        channel = transport.open_session()\n        channel.get_pty(term)\n        channel.exec_command(command)\n        self._bridge(channel)\n        channel.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstarts an interactive shell session", "response": "def shell(self, term='xterm'):\n        \"\"\" Start an interactive shell session\n\n        This method invokes a shell on the remote SSH server and proxies\n        traffic to/from both peers.\n\n        You must connect to a SSH server using ssh_connect()\n        prior to starting the session.\n        \"\"\"\n        channel = self._ssh.invoke_shell(term)\n        self._bridge(channel)\n        channel.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef command(engine, format, filepath=None, renderer=None, formatter=None):\n    if formatter is not None and renderer is None:\n        raise RequiredArgumentError('formatter given without renderer')\n\n    if engine not in ENGINES:\n        raise ValueError('unknown engine: %r' % engine)\n    if format not in FORMATS:\n        raise ValueError('unknown format: %r' % format)\n    if renderer is not None and renderer not in RENDERERS:\n        raise ValueError('unknown renderer: %r' % renderer)\n    if formatter is not None and formatter not in FORMATTERS:\n        raise ValueError('unknown formatter: %r' % formatter)\n\n    format_arg = [s for s in (format, renderer, formatter) if s is not None]\n    suffix = '.'.join(reversed(format_arg))\n    format_arg = ':'.join(format_arg)\n\n    cmd = [engine, '-T%s' % format_arg]\n    rendered = None\n    if filepath is not None:\n        cmd.extend(['-O', filepath])\n        rendered = '%s.%s' % (filepath, suffix)\n\n    return cmd, rendered", "response": "Return args list for subprocess. Popen and name of the rendered file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun the command described by cmd and return its stdout and stderr.", "response": "def run(cmd, input=None, capture_output=False, check=False, quiet=False, **kwargs):\n    \"\"\"Run the command described by cmd and return its (stdout, stderr) tuple.\"\"\"\n    if input is not None:\n        kwargs['stdin'] = subprocess.PIPE\n    if capture_output:\n        kwargs['stdout'] = kwargs['stderr'] = subprocess.PIPE\n\n    try:\n        proc = subprocess.Popen(cmd, startupinfo=get_startupinfo(), **kwargs)\n    except OSError as e:\n        if e.errno == errno.ENOENT:\n            raise ExecutableNotFound(cmd)\n        else:\n            raise\n\n    out, err = proc.communicate(input)\n\n    if not quiet and err:\n        stderr_write_bytes(err, flush=True)\n    if check and proc.returncode:\n        raise CalledProcessError(proc.returncode, cmd, output=out, stderr=err)\n\n    return out, err"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrender a DOT source file with Graphviz engine into format.", "response": "def render(engine, format, filepath, renderer=None, formatter=None, quiet=False):\n    \"\"\"Render file with Graphviz ``engine`` into ``format``,  return result filename.\n\n    Args:\n        engine: The layout commmand used for rendering (``'dot'``, ``'neato'``, ...).\n        format: The output format used for rendering (``'pdf'``, ``'png'``, ...).\n        filepath: Path to the DOT source file to render.\n        renderer: The output renderer used for rendering (``'cairo'``, ``'gd'``, ...).\n        formatter: The output formatter used for rendering (``'cairo'``, ``'gd'``, ...).\n        quiet (bool): Suppress ``stderr`` output.\n    Returns:\n        The (possibly relative) path of the rendered file.\n    Raises:\n        ValueError: If ``engine``, ``format``, ``renderer``, or ``formatter`` are not known.\n        graphviz.RequiredArgumentError: If ``formatter`` is given but ``renderer`` is None.\n        graphviz.ExecutableNotFound: If the Graphviz executable is not found.\n        subprocess.CalledProcessError: If the exit status is non-zero.\n    \"\"\"\n    cmd, rendered = command(engine, format, filepath, renderer, formatter)\n    run(cmd, capture_output=True, check=True, quiet=quiet)\n    return rendered"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning data piped through Graphviz engine into format.", "response": "def pipe(engine, format, data, renderer=None, formatter=None, quiet=False):\n    \"\"\"Return ``data`` piped through Graphviz ``engine`` into ``format``.\n\n    Args:\n        engine: The layout commmand used for rendering (``'dot'``, ``'neato'``, ...).\n        format: The output format used for rendering (``'pdf'``, ``'png'``, ...).\n        data: The binary (encoded) DOT source string to render.\n        renderer: The output renderer used for rendering (``'cairo'``, ``'gd'``, ...).\n        formatter: The output formatter used for rendering (``'cairo'``, ``'gd'``, ...).\n        quiet (bool): Suppress ``stderr`` output.\n    Returns:\n        Binary (encoded) stdout of the layout command.\n    Raises:\n        ValueError: If ``engine``, ``format``, ``renderer``, or ``formatter`` are not known.\n        graphviz.RequiredArgumentError: If ``formatter`` is given but ``renderer`` is None.\n        graphviz.ExecutableNotFound: If the Graphviz executable is not found.\n        subprocess.CalledProcessError: If the exit status is non-zero.\n    \"\"\"\n    cmd, _ = command(engine, format, None, renderer, formatter)\n    out, _ = run(cmd, input=data, capture_output=True, check=True, quiet=quiet)\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the version number tuple from the stderr output of graphviz - V.", "response": "def version():\n    \"\"\"Return the version number tuple from the ``stderr`` output of ``dot -V``.\n\n    Returns:\n        Two or three ``int`` version ``tuple``.\n    Raises:\n        graphviz.ExecutableNotFound: If the Graphviz executable is not found.\n        subprocess.CalledProcessError: If the exit status is non-zero.\n        RuntimmeError: If the output cannot be parsed into a version number.\n    \"\"\"\n    cmd = ['dot', '-V']\n    out, _ = run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n\n    info = out.decode('ascii')\n    ma = re.search(r'graphviz version (\\d+\\.\\d+(?:\\.\\d+)?) ', info)\n    if ma is None:\n        raise RuntimeError\n    return tuple(int(d) for d in ma.group(1).split('.'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef view(filepath):\n    try:\n        view_func = getattr(view, PLATFORM)\n    except AttributeError:\n        raise RuntimeError('platform %r not supported' % PLATFORM)\n    view_func(filepath)", "response": "Open filepath with its default viewing application."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pipe(self, format=None, renderer=None, formatter=None):\n        if format is None:\n            format = self._format\n\n        data = text_type(self.source).encode(self._encoding)\n\n        out = backend.pipe(self._engine, format, data, renderer, formatter)\n\n        return out", "response": "Return the source piped through the Graphviz layout command."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsaving the DOT source to file.", "response": "def save(self, filename=None, directory=None):\n        \"\"\"Save the DOT source to file. Ensure the file ends with a newline.\n\n        Args:\n            filename: Filename for saving the source (defaults to ``name`` + ``'.gv'``)\n            directory: (Sub)directory for source saving and rendering.\n        Returns:\n            The (possibly relative) path of the saved source file.\n        \"\"\"\n        if filename is not None:\n            self.filename = filename\n        if directory is not None:\n            self.directory = directory\n\n        filepath = self.filepath\n        tools.mkdirs(filepath)\n\n        data = text_type(self.source)\n\n        with io.open(filepath, 'w', encoding=self.encoding) as fd:\n            fd.write(data)\n            if not data.endswith(u'\\n'):\n                fd.write(u'\\n')\n\n        return filepath"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef render(self, filename=None, directory=None, view=False, cleanup=False,\n               format=None, renderer=None, formatter=None):\n        \"\"\"Save the source to file and render with the Graphviz engine.\n\n        Args:\n            filename: Filename for saving the source (defaults to ``name`` + ``'.gv'``)\n            directory: (Sub)directory for source saving and rendering.\n            view (bool): Open the rendered result with the default application.\n            cleanup (bool): Delete the source file after rendering.\n            format: The output format used for rendering (``'pdf'``, ``'png'``, etc.).\n            renderer: The output renderer used for rendering (``'cairo'``, ``'gd'``, ...).\n            formatter: The output formatter used for rendering (``'cairo'``, ``'gd'``, ...).\n        Returns:\n            The (possibly relative) path of the rendered file.\n        Raises:\n            ValueError: If ``format``, ``renderer``, or ``formatter`` are not known.\n            graphviz.RequiredArgumentError: If ``formatter`` is given but ``renderer`` is None.\n            graphviz.ExecutableNotFound: If the Graphviz executable is not found.\n            subprocess.CalledProcessError: If the exit status is non-zero.\n            RuntimeError: If viewer opening is requested but not supported.\n        \"\"\"\n        filepath = self.save(filename, directory)\n\n        if format is None:\n            format = self._format\n\n        rendered = backend.render(self._engine, format, filepath, renderer, formatter)\n\n        if cleanup:\n            os.remove(filepath)\n\n        if view:\n            self._view(rendered, self._format)\n\n        return rendered", "response": "Save the source to file and render it with Graphviz."}
